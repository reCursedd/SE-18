{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16757", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16757/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16757/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16757/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16757", "id": 294225262, "node_id": "MDU6SXNzdWUyOTQyMjUyNjI=", "number": 16757, "title": "tf.contrib.layers.optimize_loss() to support mixed precision training", "user": {"login": "aisuni", "id": 35795681, "node_id": "MDQ6VXNlcjM1Nzk1Njgx", "avatar_url": "https://avatars3.githubusercontent.com/u/35795681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aisuni", "html_url": "https://github.com/aisuni", "followers_url": "https://api.github.com/users/aisuni/followers", "following_url": "https://api.github.com/users/aisuni/following{/other_user}", "gists_url": "https://api.github.com/users/aisuni/gists{/gist_id}", "starred_url": "https://api.github.com/users/aisuni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aisuni/subscriptions", "organizations_url": "https://api.github.com/users/aisuni/orgs", "repos_url": "https://api.github.com/users/aisuni/repos", "events_url": "https://api.github.com/users/aisuni/events{/privacy}", "received_events_url": "https://api.github.com/users/aisuni/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-02-04T19:58:16Z", "updated_at": "2018-02-23T22:26:14Z", "closed_at": "2018-02-23T22:26:14Z", "author_association": "NONE", "body_html": "<p>ISSUE: Referring to <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/layers/python/layers/optimizers.py\">source code</a>,  it is evident that mixed precision gradients is not supported in <code>tf.contrib.layers.optimize_loss</code>.<br>\nHere is the <code>snip</code> of assertion\u2026</p>\n<pre><code>opt = tf.contrib.layers.optimize_loss(\n    base_loss, global_step=global_step,\n    clip_gradients=clip_grad, increment_global_step=True, **train_params)\n\nTypeError: Tensors in list passed to 'values' of 'Pack' Op have types [float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16] that don't all match.\n</code></pre>\n<p>Description:<br>\nThis was observed during training resnet50 (this involves mixed precision batch_norm). Just curious to know whether there is a roadmap to have mixed precision gradients support in <code>tf.contrib.layers.optimize_loss</code>.</p>\n<p>System information<br>\n\u2022\t**OS Platform and Distribution *: Linux Centos 7.2<br>\n\u2022\tTensorFlow installed from (source or binary): 1.5.0<br>\n\u2022\tTensorFlow version (use command below):  v1.5.0-0-g37aa430d84 1.5.0<br>\n\u2022\tPython version: 3.4.5<br>\n\u2022\tBazel version (if compiling from source): No<br>\n\u2022\tCUDA/CUDAnn version: CUDA 9.1 and CUDAnn 7.0 with latest Nvidia driver<br>\n\u2022\tGPU model and memory: Volta 100, 16GiB</p>", "body_text": "ISSUE: Referring to source code,  it is evident that mixed precision gradients is not supported in tf.contrib.layers.optimize_loss.\nHere is the snip of assertion\u2026\nopt = tf.contrib.layers.optimize_loss(\n    base_loss, global_step=global_step,\n    clip_gradients=clip_grad, increment_global_step=True, **train_params)\n\nTypeError: Tensors in list passed to 'values' of 'Pack' Op have types [float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16] that don't all match.\n\nDescription:\nThis was observed during training resnet50 (this involves mixed precision batch_norm). Just curious to know whether there is a roadmap to have mixed precision gradients support in tf.contrib.layers.optimize_loss.\nSystem information\n\u2022\t**OS Platform and Distribution *: Linux Centos 7.2\n\u2022\tTensorFlow installed from (source or binary): 1.5.0\n\u2022\tTensorFlow version (use command below):  v1.5.0-0-g37aa430d84 1.5.0\n\u2022\tPython version: 3.4.5\n\u2022\tBazel version (if compiling from source): No\n\u2022\tCUDA/CUDAnn version: CUDA 9.1 and CUDAnn 7.0 with latest Nvidia driver\n\u2022\tGPU model and memory: Volta 100, 16GiB", "body": "ISSUE: Referring to [source code](https://github.com/tensorflow/tensorflow/blob/r1.5/tensorflow/contrib/layers/python/layers/optimizers.py),  it is evident that mixed precision gradients is not supported in `tf.contrib.layers.optimize_loss`. \r\nHere is the `snip` of assertion\u2026\r\n```\r\nopt = tf.contrib.layers.optimize_loss(\r\n    base_loss, global_step=global_step,\r\n    clip_gradients=clip_grad, increment_global_step=True, **train_params)\r\n\r\nTypeError: Tensors in list passed to 'values' of 'Pack' Op have types [float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float32, float32, float16, float16] that don't all match.\r\n```\r\nDescription:\r\nThis was observed during training resnet50 (this involves mixed precision batch_norm). Just curious to know whether there is a roadmap to have mixed precision gradients support in `tf.contrib.layers.optimize_loss`.\r\n\r\nSystem information\r\n\t\u2022\t**OS Platform and Distribution *: Linux Centos 7.2\r\n\t\u2022\tTensorFlow installed from (source or binary): 1.5.0\r\n\t\u2022\tTensorFlow version (use command below):  v1.5.0-0-g37aa430d84 1.5.0\r\n\t\u2022\tPython version: 3.4.5\r\n\t\u2022\tBazel version (if compiling from source): No\r\n\t\u2022\tCUDA/CUDAnn version: CUDA 9.1 and CUDAnn 7.0 with latest Nvidia driver\r\n\t\u2022\tGPU model and memory: Volta 100, 16GiB\r\n"}