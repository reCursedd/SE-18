{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175527031", "pull_request_review_id": 105066762, "id": 175527031, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE3NTUyNzAzMQ==", "diff_hunk": "@@ -163,29 +285,87 @@ template <class InputDataT,\n   for (int i = 0; i < rank; ++i) {\n     output_shape.AddDim(target_dimensions[i]);\n   }\n-  const auto new_size =\n-      new_sliced_size * target_dimensions[adjustable_dimension];\n \n   // Create an output tensor and attach it to the current context\n   tensorflow::Tensor* output_tensor = nullptr;\n   OP_REQUIRES_OK(context,\n                  context->allocate_output(0, output_shape, &output_tensor));\n   auto output = output_tensor->flat<InputDataT>();\n \n-  // memory is allocated for these variables outside the inner loop for\n-  // efficiency (although, I could create a separate class scope for\n-  // this purpose instead)\n-  tensorflow::int64 result = 0;\n-  std::vector<tensorflow::int64> output_indices(target_dimensions.size());\n+  // Fill output tensor with periodically resampled input tensor values\n+  InputIndexer input_indexer(target_dimensions, input_tensor.shape(),\n+                             adjustable_dimension);\n+\n+  auto worker_threads = *(context->device()->tensorflow_cpu_worker_threads());\n+  auto fill_output_tensor = [&input_indexer, &output, &input](\n+      tensorflow::int64 start, tensorflow::int64 limit) {\n+    InputIndexer local_indexer(input_indexer);\n+    local_indexer.MoveToOutputIndex(start);\n+    for (tensorflow::int64 output_index = start; output_index < limit;\n+         ++output_index) {\n+      output(output_index) = input(local_indexer.linear_input_index());\n+      local_indexer.IncrementOutputIndex();\n+    }\n+  };\n+  ::tensorflow::Shard(worker_threads.num_threads, worker_threads.workers,\n+                      new_size, costPerFillIndex, fill_output_tensor);\n+}\n+\n+template <class InputDataT>\n+void fill_grad_tensor(tensorflow::OpKernelContext* context,", "path": "tensorflow/contrib/periodic_resample/kernels/periodic_resample_op.h", "position": null, "original_position": 371, "commit_id": "5916a8ea00719e389ed9c667ddc83c73ca6cd4aa", "original_commit_id": "1677dea2161195bee199a5c310ad0e6b6e02aa98", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "It seems like this function could share most of the code with fill_prediodic_tensor. Can you try to consolidate?", "created_at": "2018-03-19T17:48:40Z", "updated_at": "2018-05-09T11:46:55Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16520#discussion_r175527031", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16520", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/175527031"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16520#discussion_r175527031"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16520"}}, "body_html": "<p>It seems like this function could share most of the code with fill_prediodic_tensor. Can you try to consolidate?</p>", "body_text": "It seems like this function could share most of the code with fill_prediodic_tensor. Can you try to consolidate?"}