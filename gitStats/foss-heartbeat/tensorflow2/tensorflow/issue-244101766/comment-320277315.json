{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320277315", "html_url": "https://github.com/tensorflow/tensorflow/issues/11613#issuecomment-320277315", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11613", "id": 320277315, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDI3NzMxNQ==", "user": {"login": "raoconn", "id": 30728600, "node_id": "MDQ6VXNlcjMwNzI4NjAw", "avatar_url": "https://avatars0.githubusercontent.com/u/30728600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raoconn", "html_url": "https://github.com/raoconn", "followers_url": "https://api.github.com/users/raoconn/followers", "following_url": "https://api.github.com/users/raoconn/following{/other_user}", "gists_url": "https://api.github.com/users/raoconn/gists{/gist_id}", "starred_url": "https://api.github.com/users/raoconn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raoconn/subscriptions", "organizations_url": "https://api.github.com/users/raoconn/orgs", "repos_url": "https://api.github.com/users/raoconn/repos", "events_url": "https://api.github.com/users/raoconn/events{/privacy}", "received_events_url": "https://api.github.com/users/raoconn/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-04T15:23:15Z", "updated_at": "2017-08-04T15:23:15Z", "author_association": "NONE", "body_html": "<p>I have encountered the same error, running TensorFlow 1.2.0 on centos 7 with 4 GeForce GTX 980 Ti GPUs .  I am experimenting with a number of different recurrent neural network designs using <code>tf.contrib.training.batch_sequences_with_states</code> and <code>tf.nn.static_state_saving_rnn</code> and get the InvalidArgumentError only for some of them.  For instance, the RNN could have one or two layers and it could use GRU or LSTM cells.  With one layer and GRU cells, I have run 500 epochs (which results in about 536000 steps) without getting the error.  With two layers and GRU cells or one layer and LSTM cells, running 500 epochs results in the Invalid Argument Error at some point along the way (usually after at least 100000 and up to 400000 steps, depending on other parameters that I am modifying such as the number of cells in each layer).  With two layers and LSTM cells, the InvalidArgumentError occurs almost immediately (within the first 100 steps) and happens even if I have the queue set to read the file only once (1 epoch).  I have used different input files (with similar data, all in tfrecords format) and encountered the same error regardless of the file.</p>\n<p>Relevant code:</p>\n<pre><code>num_unroll = 100\nbatch_size = 100\nnum_layers = config.num_layers\nsize = config.size\nuse_lstm = config.use_lstm\ndef single_cell():\n    \"\"\"Sets the basic cell to be either LSTM or GRU.\"\"\"\n    if use_lstm:\n        return tf.nn.rnn_cell.BasicLSTMCell(size)\n    return tf.nn.rnn_cell.GRUCell(size)\nif num_layers &gt; 1:\n    stacked_cells = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\nelse:\n    stacked_cells = single_cell()\nstate_name = 'rnn_state'\nif num_layers &gt; 1 and use_lstm:\n    initial_state = tf.zeros((stacked_cells.state_size[0][0],), dtype=data_type())\n    state_names = ((state_name, state_name),) * num_layers\nelif num_layers &gt; 1:\n    initial_state = tf.zeros((stacked_cells.state_size[0],), dtype=data_type())\n    state_names = (state_name,) * num_layers\nelif use_lstm:\n    initial_state = tf.zeros((stacked_cells.state_size[0],), dtype=data_type())\n    state_names = (state_name, state_name)\nelse:\n    initial_state = tf.zeros((stacked_cells.state_size,), dtype=data_type())\n    state_names = state_name\nrnn_initial_state = {'rnn_state': initial_state}\nbatch = tf.contrib.training.batch_sequences_with_states(\n    input_key=inputs[0]['key'],\n    input_sequences=inputs[1],\n    input_context=inputs[0],\n    input_length=tf.cast(inputs[0]['length'], tf.int32),\n    initial_states=rnn_initial_state,\n    num_unroll=num_unroll,\n    batch_size=batch_size,\n    allow_small_batch=True,\n    num_threads=1,\n    capacity=batch_size*4,\n    pad=True,\n    make_keys_unique=True)\ninputs = data = batch.sequences['alert_id']\nlabels = batch.context['label']\nlengths = batch.length\nembedder = tf.get_variable('embedder', [config.max_alert_id, config.embedding_size])\nvectors = tf.nn.embedding_lookup([embedder], data, name='embed')\nvectors = tf.unstack(\n    value=vectors,\n    num=num_unroll,\n    axis=1)\noutputs, state = tf.nn.static_state_saving_rnn(\n    stacked_cells,\n    vectors,\n    state_saver=batch,\n    state_name=state_names)\n</code></pre>\n<p>All of the parameters involved in the batching are unchanged across my experiments, except <code>initial_states</code> which has to change according to the number of layers and type of RNN cell used, but I don't see why that should affect the reading of the data.</p>\n<p>I don't know if it's relevant, but the error for my data always corresponds to component 13.</p>", "body_text": "I have encountered the same error, running TensorFlow 1.2.0 on centos 7 with 4 GeForce GTX 980 Ti GPUs .  I am experimenting with a number of different recurrent neural network designs using tf.contrib.training.batch_sequences_with_states and tf.nn.static_state_saving_rnn and get the InvalidArgumentError only for some of them.  For instance, the RNN could have one or two layers and it could use GRU or LSTM cells.  With one layer and GRU cells, I have run 500 epochs (which results in about 536000 steps) without getting the error.  With two layers and GRU cells or one layer and LSTM cells, running 500 epochs results in the Invalid Argument Error at some point along the way (usually after at least 100000 and up to 400000 steps, depending on other parameters that I am modifying such as the number of cells in each layer).  With two layers and LSTM cells, the InvalidArgumentError occurs almost immediately (within the first 100 steps) and happens even if I have the queue set to read the file only once (1 epoch).  I have used different input files (with similar data, all in tfrecords format) and encountered the same error regardless of the file.\nRelevant code:\nnum_unroll = 100\nbatch_size = 100\nnum_layers = config.num_layers\nsize = config.size\nuse_lstm = config.use_lstm\ndef single_cell():\n    \"\"\"Sets the basic cell to be either LSTM or GRU.\"\"\"\n    if use_lstm:\n        return tf.nn.rnn_cell.BasicLSTMCell(size)\n    return tf.nn.rnn_cell.GRUCell(size)\nif num_layers > 1:\n    stacked_cells = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\nelse:\n    stacked_cells = single_cell()\nstate_name = 'rnn_state'\nif num_layers > 1 and use_lstm:\n    initial_state = tf.zeros((stacked_cells.state_size[0][0],), dtype=data_type())\n    state_names = ((state_name, state_name),) * num_layers\nelif num_layers > 1:\n    initial_state = tf.zeros((stacked_cells.state_size[0],), dtype=data_type())\n    state_names = (state_name,) * num_layers\nelif use_lstm:\n    initial_state = tf.zeros((stacked_cells.state_size[0],), dtype=data_type())\n    state_names = (state_name, state_name)\nelse:\n    initial_state = tf.zeros((stacked_cells.state_size,), dtype=data_type())\n    state_names = state_name\nrnn_initial_state = {'rnn_state': initial_state}\nbatch = tf.contrib.training.batch_sequences_with_states(\n    input_key=inputs[0]['key'],\n    input_sequences=inputs[1],\n    input_context=inputs[0],\n    input_length=tf.cast(inputs[0]['length'], tf.int32),\n    initial_states=rnn_initial_state,\n    num_unroll=num_unroll,\n    batch_size=batch_size,\n    allow_small_batch=True,\n    num_threads=1,\n    capacity=batch_size*4,\n    pad=True,\n    make_keys_unique=True)\ninputs = data = batch.sequences['alert_id']\nlabels = batch.context['label']\nlengths = batch.length\nembedder = tf.get_variable('embedder', [config.max_alert_id, config.embedding_size])\nvectors = tf.nn.embedding_lookup([embedder], data, name='embed')\nvectors = tf.unstack(\n    value=vectors,\n    num=num_unroll,\n    axis=1)\noutputs, state = tf.nn.static_state_saving_rnn(\n    stacked_cells,\n    vectors,\n    state_saver=batch,\n    state_name=state_names)\n\nAll of the parameters involved in the batching are unchanged across my experiments, except initial_states which has to change according to the number of layers and type of RNN cell used, but I don't see why that should affect the reading of the data.\nI don't know if it's relevant, but the error for my data always corresponds to component 13.", "body": "I have encountered the same error, running TensorFlow 1.2.0 on centos 7 with 4 GeForce GTX 980 Ti GPUs .  I am experimenting with a number of different recurrent neural network designs using `tf.contrib.training.batch_sequences_with_states` and `tf.nn.static_state_saving_rnn` and get the InvalidArgumentError only for some of them.  For instance, the RNN could have one or two layers and it could use GRU or LSTM cells.  With one layer and GRU cells, I have run 500 epochs (which results in about 536000 steps) without getting the error.  With two layers and GRU cells or one layer and LSTM cells, running 500 epochs results in the Invalid Argument Error at some point along the way (usually after at least 100000 and up to 400000 steps, depending on other parameters that I am modifying such as the number of cells in each layer).  With two layers and LSTM cells, the InvalidArgumentError occurs almost immediately (within the first 100 steps) and happens even if I have the queue set to read the file only once (1 epoch).  I have used different input files (with similar data, all in tfrecords format) and encountered the same error regardless of the file.\r\n\r\nRelevant code:\r\n```\r\nnum_unroll = 100\r\nbatch_size = 100\r\nnum_layers = config.num_layers\r\nsize = config.size\r\nuse_lstm = config.use_lstm\r\ndef single_cell():\r\n    \"\"\"Sets the basic cell to be either LSTM or GRU.\"\"\"\r\n    if use_lstm:\r\n        return tf.nn.rnn_cell.BasicLSTMCell(size)\r\n    return tf.nn.rnn_cell.GRUCell(size)\r\nif num_layers > 1:\r\n    stacked_cells = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(num_layers)])\r\nelse:\r\n    stacked_cells = single_cell()\r\nstate_name = 'rnn_state'\r\nif num_layers > 1 and use_lstm:\r\n    initial_state = tf.zeros((stacked_cells.state_size[0][0],), dtype=data_type())\r\n    state_names = ((state_name, state_name),) * num_layers\r\nelif num_layers > 1:\r\n    initial_state = tf.zeros((stacked_cells.state_size[0],), dtype=data_type())\r\n    state_names = (state_name,) * num_layers\r\nelif use_lstm:\r\n    initial_state = tf.zeros((stacked_cells.state_size[0],), dtype=data_type())\r\n    state_names = (state_name, state_name)\r\nelse:\r\n    initial_state = tf.zeros((stacked_cells.state_size,), dtype=data_type())\r\n    state_names = state_name\r\nrnn_initial_state = {'rnn_state': initial_state}\r\nbatch = tf.contrib.training.batch_sequences_with_states(\r\n    input_key=inputs[0]['key'],\r\n    input_sequences=inputs[1],\r\n    input_context=inputs[0],\r\n    input_length=tf.cast(inputs[0]['length'], tf.int32),\r\n    initial_states=rnn_initial_state,\r\n    num_unroll=num_unroll,\r\n    batch_size=batch_size,\r\n    allow_small_batch=True,\r\n    num_threads=1,\r\n    capacity=batch_size*4,\r\n    pad=True,\r\n    make_keys_unique=True)\r\ninputs = data = batch.sequences['alert_id']\r\nlabels = batch.context['label']\r\nlengths = batch.length\r\nembedder = tf.get_variable('embedder', [config.max_alert_id, config.embedding_size])\r\nvectors = tf.nn.embedding_lookup([embedder], data, name='embed')\r\nvectors = tf.unstack(\r\n    value=vectors,\r\n    num=num_unroll,\r\n    axis=1)\r\noutputs, state = tf.nn.static_state_saving_rnn(\r\n    stacked_cells,\r\n    vectors,\r\n    state_saver=batch,\r\n    state_name=state_names)\r\n```\r\nAll of the parameters involved in the batching are unchanged across my experiments, except `initial_states` which has to change according to the number of layers and type of RNN cell used, but I don't see why that should affect the reading of the data.\r\n\r\nI don't know if it's relevant, but the error for my data always corresponds to component 13."}