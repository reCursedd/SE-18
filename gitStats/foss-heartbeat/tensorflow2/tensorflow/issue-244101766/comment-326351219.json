{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/326351219", "html_url": "https://github.com/tensorflow/tensorflow/issues/11613#issuecomment-326351219", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11613", "id": 326351219, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjM1MTIxOQ==", "user": {"login": "raoconn", "id": 30728600, "node_id": "MDQ6VXNlcjMwNzI4NjAw", "avatar_url": "https://avatars0.githubusercontent.com/u/30728600?v=4", "gravatar_id": "", "url": "https://api.github.com/users/raoconn", "html_url": "https://github.com/raoconn", "followers_url": "https://api.github.com/users/raoconn/followers", "following_url": "https://api.github.com/users/raoconn/following{/other_user}", "gists_url": "https://api.github.com/users/raoconn/gists{/gist_id}", "starred_url": "https://api.github.com/users/raoconn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/raoconn/subscriptions", "organizations_url": "https://api.github.com/users/raoconn/orgs", "repos_url": "https://api.github.com/users/raoconn/repos", "events_url": "https://api.github.com/users/raoconn/events{/privacy}", "received_events_url": "https://api.github.com/users/raoconn/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T16:31:26Z", "updated_at": "2017-08-31T16:31:26Z", "author_association": "NONE", "body_html": "<p>The counting key generator did not solve the problem for me, nor did combining the counting key with <code>make_keys_unique=True</code>.  I did learn that the error is happening (when I run with LSTM cells, 2 layers, and <code>num_epochs=1</code>, anyway) on the 111th item, but I don't think that's relevant.</p>\n<p>I continue to find it odd that changing parameters that have no effect on the arguments passed to <code>batch_sequences_with_states</code> (e.g. learning rate parameters) affect whether or not this error occurs.  Any given parameter set has a consistent effect across repeated attempts (down to the key, except for the part generated by <code>make_keys_unique</code>, and component information in the error message), but changing the parameter set changes when or even if the error is triggered.  (Or, at least, some parameter sets won't trigger it within 500 epochs.)</p>\n<p>For the most part I can work around this because training occurs for some time before the error, and I can simply restore the last checkpoint and continue until the error happens again.  However, that doesn't actually solve the problem.</p>", "body_text": "The counting key generator did not solve the problem for me, nor did combining the counting key with make_keys_unique=True.  I did learn that the error is happening (when I run with LSTM cells, 2 layers, and num_epochs=1, anyway) on the 111th item, but I don't think that's relevant.\nI continue to find it odd that changing parameters that have no effect on the arguments passed to batch_sequences_with_states (e.g. learning rate parameters) affect whether or not this error occurs.  Any given parameter set has a consistent effect across repeated attempts (down to the key, except for the part generated by make_keys_unique, and component information in the error message), but changing the parameter set changes when or even if the error is triggered.  (Or, at least, some parameter sets won't trigger it within 500 epochs.)\nFor the most part I can work around this because training occurs for some time before the error, and I can simply restore the last checkpoint and continue until the error happens again.  However, that doesn't actually solve the problem.", "body": "The counting key generator did not solve the problem for me, nor did combining the counting key with `make_keys_unique=True`.  I did learn that the error is happening (when I run with LSTM cells, 2 layers, and `num_epochs=1`, anyway) on the 111th item, but I don't think that's relevant.\r\n\r\nI continue to find it odd that changing parameters that have no effect on the arguments passed to `batch_sequences_with_states` (e.g. learning rate parameters) affect whether or not this error occurs.  Any given parameter set has a consistent effect across repeated attempts (down to the key, except for the part generated by `make_keys_unique`, and component information in the error message), but changing the parameter set changes when or even if the error is triggered.  (Or, at least, some parameter sets won't trigger it within 500 epochs.)\r\n\r\nFor the most part I can work around this because training occurs for some time before the error, and I can simply restore the last checkpoint and continue until the error happens again.  However, that doesn't actually solve the problem."}