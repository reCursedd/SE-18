{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426879077", "html_url": "https://github.com/tensorflow/tensorflow/issues/19731#issuecomment-426879077", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19731", "id": 426879077, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjg3OTA3Nw==", "user": {"login": "p890040", "id": 25840475, "node_id": "MDQ6VXNlcjI1ODQwNDc1", "avatar_url": "https://avatars2.githubusercontent.com/u/25840475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/p890040", "html_url": "https://github.com/p890040", "followers_url": "https://api.github.com/users/p890040/followers", "following_url": "https://api.github.com/users/p890040/following{/other_user}", "gists_url": "https://api.github.com/users/p890040/gists{/gist_id}", "starred_url": "https://api.github.com/users/p890040/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/p890040/subscriptions", "organizations_url": "https://api.github.com/users/p890040/orgs", "repos_url": "https://api.github.com/users/p890040/repos", "events_url": "https://api.github.com/users/p890040/events{/privacy}", "received_events_url": "https://api.github.com/users/p890040/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-04T04:11:01Z", "updated_at": "2018-10-04T04:11:01Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Hi I'm new to GitHub and have been trying to get Tensorflow running in Python.</p>\n<p>In principle I have the thing up and running, but the GPU memory is not released, causing an OOM error at some point. These few lines already clutter the memory.</p>\n<p>import tensorflow as tf<br>\nsess = tf.Session()<br>\nsess.close()</p>\n<p>I've been googling the problem but so far I have only managed to solve it by either</p>\n<ul>\n<li>Clossing the console (and thus losing all varibales which is what I am actually trying to avoid)</li>\n<li>Using numba to kill the cuda device (however, this only works once) - see above. In that case my code looked something like this:</li>\n</ul>\n<p>from numba import cuda<br>\ncuda.select_device(0)<br>\n#do tf stuff<br>\ncuda.close()<br>\n#the memory was released here!<br>\ncuda.select_device(0)<br>\n#to tf stuff -&gt; caused an OOM<br>\n(compare to <a href=\"https://numba.pydata.org/numba-doc/dev/cuda/device-management.html\" rel=\"nofollow\">https://numba.pydata.org/numba-doc/dev/cuda/device-management.html</a>)</p>\n<p>As mentioned above I would like to avoid killing the session and thus losing my varibales in memory (used to train a NN).</p>\n<ul>\n<li>I am aware that I can alocate only a fraction of the memory (cfg.gpu_options.per_process_gpu_memory_fraction = 0.1) or let the memory grow (cfg.gpu_options.allow_growth=True) and this both works fine, but afterwards I simply am unable to release the memory.</li>\n<li>I have tried to put it into threads and pools following ideas and workaounds people suggested.</li>\n<li>Also the K.session_close() and/or gc.collect() combination does not work.</li>\n<li>I also downgraded tf to 1.9 and Keras to 2.1.9 (second not really relevant for the above code).</li>\n<li>I have also upgraded my graphics card driver to the newest release (see below and note the memory which is not released after the call from above).</li>\n</ul>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/43452440/45842749-e6e0e600-bd1d-11e8-8023-387c219b1bbf.png\"><img src=\"https://user-images.githubusercontent.com/43452440/45842749-e6e0e600-bd1d-11e8-8023-387c219b1bbf.png\" alt=\"grafik\" style=\"max-width:100%;\"></a></p>\n<p>I'd be very thankful for any suggestions what to do with the code snippet from above to ensure that the GPU memory is free in the end</p>\n</blockquote>\n<p>Exactly same problem for me.<br>\nVery hope for some suggestion about this.</p>", "body_text": "Hi I'm new to GitHub and have been trying to get Tensorflow running in Python.\nIn principle I have the thing up and running, but the GPU memory is not released, causing an OOM error at some point. These few lines already clutter the memory.\nimport tensorflow as tf\nsess = tf.Session()\nsess.close()\nI've been googling the problem but so far I have only managed to solve it by either\n\nClossing the console (and thus losing all varibales which is what I am actually trying to avoid)\nUsing numba to kill the cuda device (however, this only works once) - see above. In that case my code looked something like this:\n\nfrom numba import cuda\ncuda.select_device(0)\n#do tf stuff\ncuda.close()\n#the memory was released here!\ncuda.select_device(0)\n#to tf stuff -> caused an OOM\n(compare to https://numba.pydata.org/numba-doc/dev/cuda/device-management.html)\nAs mentioned above I would like to avoid killing the session and thus losing my varibales in memory (used to train a NN).\n\nI am aware that I can alocate only a fraction of the memory (cfg.gpu_options.per_process_gpu_memory_fraction = 0.1) or let the memory grow (cfg.gpu_options.allow_growth=True) and this both works fine, but afterwards I simply am unable to release the memory.\nI have tried to put it into threads and pools following ideas and workaounds people suggested.\nAlso the K.session_close() and/or gc.collect() combination does not work.\nI also downgraded tf to 1.9 and Keras to 2.1.9 (second not really relevant for the above code).\nI have also upgraded my graphics card driver to the newest release (see below and note the memory which is not released after the call from above).\n\n\nI'd be very thankful for any suggestions what to do with the code snippet from above to ensure that the GPU memory is free in the end\n\nExactly same problem for me.\nVery hope for some suggestion about this.", "body": "> Hi I'm new to GitHub and have been trying to get Tensorflow running in Python.\r\n> \r\n> In principle I have the thing up and running, but the GPU memory is not released, causing an OOM error at some point. These few lines already clutter the memory.\r\n> \r\n> import tensorflow as tf\r\n> sess = tf.Session()\r\n> sess.close()\r\n> \r\n> I've been googling the problem but so far I have only managed to solve it by either\r\n> \r\n> * Clossing the console (and thus losing all varibales which is what I am actually trying to avoid)\r\n> * Using numba to kill the cuda device (however, this only works once) - see above. In that case my code looked something like this:\r\n> \r\n> from numba import cuda\r\n> cuda.select_device(0)\r\n> #do tf stuff\r\n> cuda.close()\r\n> #the memory was released here!\r\n> cuda.select_device(0)\r\n> #to tf stuff -> caused an OOM\r\n> (compare to https://numba.pydata.org/numba-doc/dev/cuda/device-management.html)\r\n> \r\n> As mentioned above I would like to avoid killing the session and thus losing my varibales in memory (used to train a NN).\r\n> \r\n> * I am aware that I can alocate only a fraction of the memory (cfg.gpu_options.per_process_gpu_memory_fraction = 0.1) or let the memory grow (cfg.gpu_options.allow_growth=True) and this both works fine, but afterwards I simply am unable to release the memory.\r\n> * I have tried to put it into threads and pools following ideas and workaounds people suggested.\r\n> * Also the K.session_close() and/or gc.collect() combination does not work.\r\n> * I also downgraded tf to 1.9 and Keras to 2.1.9 (second not really relevant for the above code).\r\n> * I have also upgraded my graphics card driver to the newest release (see below and note the memory which is not released after the call from above).\r\n> \r\n> ![grafik](https://user-images.githubusercontent.com/43452440/45842749-e6e0e600-bd1d-11e8-8023-387c219b1bbf.png)\r\n> \r\n> I'd be very thankful for any suggestions what to do with the code snippet from above to ensure that the GPU memory is free in the end\r\n\r\nExactly same problem for me.\r\nVery hope for some suggestion about this."}