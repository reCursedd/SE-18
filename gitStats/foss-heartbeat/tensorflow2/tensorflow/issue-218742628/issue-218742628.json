{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8903", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8903/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8903/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8903/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8903", "id": 218742628, "node_id": "MDU6SXNzdWUyMTg3NDI2Mjg=", "number": 8903, "title": "How to change  a classification model to a regression model ? ", "user": {"login": "sfarkya", "id": 6245078, "node_id": "MDQ6VXNlcjYyNDUwNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6245078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sfarkya", "html_url": "https://github.com/sfarkya", "followers_url": "https://api.github.com/users/sfarkya/followers", "following_url": "https://api.github.com/users/sfarkya/following{/other_user}", "gists_url": "https://api.github.com/users/sfarkya/gists{/gist_id}", "starred_url": "https://api.github.com/users/sfarkya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sfarkya/subscriptions", "organizations_url": "https://api.github.com/users/sfarkya/orgs", "repos_url": "https://api.github.com/users/sfarkya/repos", "events_url": "https://api.github.com/users/sfarkya/events{/privacy}", "received_events_url": "https://api.github.com/users/sfarkya/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-02T05:11:35Z", "updated_at": "2017-04-02T16:24:25Z", "closed_at": "2017-04-02T16:24:25Z", "author_association": "NONE", "body_html": "<p>I am using pre-trained Alexnet as shown below. I want to use that model for regression with 6 outputs (Xcoordinate (range (0,227),Ycoordinate (range (0,227),height (range (20,50), width (range (20,50), sine(theta), cos(theta)). (range of theta is -180 to 180 degrees)<br>\nThese are the following things I change -</p>\n<ol>\n<li>changed loss function to MSE.</li>\n<li>changed the output layer from 1000 to 6.</li>\n<li>changed from RELU to linear activation function last layer.<br>\nNow, I am not getting proper valued of sine and cosine above (it should be in the range of (-1 to 1)), I am getting out of bound values. What should I do, How should I keep a bound one the values. Also, should I keep a bout on other parameters as well. What should I do incorporate those changes?<br>\nWhat are the other changes should I make to use this model for regression.?</li>\n</ol>\n<pre><code>\nimport tensorflow as tf\nimport numpy as np\n\nclass AlexNet(object):\n\n  def __init__(self, x, keep_prob, num_classes, skip_layer,\n               weights_path = 'DEFAULT'):\n\n    # Parse input arguments into class variables\n    self.X = x\n    self.NUM_CLASSES = num_classes\n    self.KEEP_PROB = keep_prob\n    self.SKIP_LAYER = skip_layer\n\n    if weights_path == 'DEFAULT':\n      self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\n    else:\n      self.WEIGHTS_PATH = weights_path\n\n    # Call the create function to build the computational graph of AlexNet\n    self.create()\n\n  def create(self):\n\n    # 1st Layer: Conv (w ReLu) -&gt; Pool -&gt; Lrn\n    conv1 = conv(self.X, 11, 11, 96, 4, 4, padding = 'VALID', name = 'conv1')\n    pool1 = max_pool(conv1, 3, 3, 2, 2, padding = 'VALID', name = 'pool1')\n    norm1 = lrn(pool1, 2, 2e-05, 0.75, name = 'norm1')\n\n        # 2nd Layer: Conv (w ReLu) -&gt; Pool -&gt; Lrn with 2 groups\n    conv2 = conv(norm1, 5, 5, 256, 1, 1, groups = 2, name = 'conv2')\n    pool2 = max_pool(conv2, 3, 3, 2, 2, padding = 'VALID', name ='pool2')\n    norm2 = lrn(pool2, 2, 2e-05, 0.75, name = 'norm2')\n\n        # 3rd Layer: Conv (w ReLu)\n    conv3 = conv(norm2, 3, 3, 384, 1, 1, name = 'conv3')\n\n        # 4th Layer: Conv (w ReLu) splitted into two groups\n    conv4 = conv(conv3, 3, 3, 384, 1, 1, groups = 2, name = 'conv4')\n\n        # 5th Layer: Conv (w ReLu) -&gt; Pool splitted into two groups\n    conv5 = conv(conv4, 3, 3, 256, 1, 1, groups = 2, name = 'conv5')\n    pool5 = max_pool(conv5, 3, 3, 2, 2, padding = 'VALID', name = 'pool5')\n\n        # 6th Layer: Flatten -&gt; FC (w ReLu) -&gt; Dropout\n    flattened = tf.reshape(pool5, [-1, 6*6*256])\n    fc6 = fc(flattened, 6*6*256, 4096, name='fc6',relu =True)\n    dropout6 = dropout(fc6, self.KEEP_PROB)\n\n        # 7th Layer: FC (w ReLu) -&gt; Dropout\n    fc7 = fc(dropout6, 4096, 4096, name = 'fc7',relu =False)\n    dropout7 = dropout(fc7, self.KEEP_PROB)\n\n        # 8th Layer: FC and return unscaled activations (for tf.nn.softmax_cross_entropy_with_logits)\n    self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu = False, name='fc8')\n\n\n\n  def load_initial_weights(self, session):\n    \"\"\"\n    As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/ come\n    as a dict of lists (e.g. weights['conv1'] is a list) and not as dict of\n    dicts (e.g. weights['conv1'] is a dict with keys 'weights' &amp; 'biases') we\n    need a special load function\n    \"\"\"\n\n    # Load the weights into memory\n    weights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\n\n    # Loop over all layer names stored in the weights dict\n    for op_name in weights_dict:\n\n      # Check if the layer is one of the layers that should be reinitialized\n      if op_name not in self.SKIP_LAYER:\n\n        with tf.variable_scope(op_name, reuse = True):\n\n          # Loop over list of weights/biases and assign them to their corresponding tf variable\n          for data in weights_dict[op_name]:\n\n            # Biases\n            if len(data.shape) == 1:\n\n              var = tf.get_variable('biases', trainable = False)\n              session.run(var.assign(data))\n\n            # Weights\n            else:\n\n              var = tf.get_variable('weights', trainable = False)\n              session.run(var.assign(data))\n\n\n\n\"\"\"\nPredefine all necessary layer for the AlexNet\n\"\"\"\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n         padding='SAME', groups=1):\n  \"\"\"\n  Adapted from: https://github.com/ethereon/caffe-tensorflow\n  \"\"\"\n  # Get number of input channels\n  input_channels = int(x.get_shape()[-1])\n\n  # Create lambda function for the convolution\n  convolve = lambda i, k: tf.nn.conv2d(i, k,\n                                       strides = [1, stride_y, stride_x, 1],\n                                       padding = padding)\n\n  with tf.variable_scope(name) as scope:\n    # Create tf variables for the weights and biases of the conv layer\n    weights = tf.get_variable('weights', shape = [filter_height, filter_width, input_channels/groups, num_filters])\n    biases = tf.get_variable('biases', shape = [num_filters])\n\n\n    if groups == 1:\n      conv = convolve(x, weights)\n\n    # In the cases of multiple groups, split inputs &amp; weights and\n    else:\n      # Split input and weights and convolve them separately\n      #input_groups = tf.split(value=x, num_split= groups, split_dim=3)\n      #input_groups = tf.split(split_dim=3, num_split= groups,value=x)\n      input_groups = tf.split(axis = 3, num_or_size_splits=groups, value=x)\n     # weight_groups = tf.split(value =weights, num_split=groups, split_dim=3)\n      weight_groups = tf.split(axis = 3, num_or_size_splits=groups, value=weights)\n\n      output_groups = [convolve(i, k) for i,k in zip(input_groups, weight_groups)]\n\n      # Concat the convolved output together again\n      #conv = tf.concat( values = output_groups,concat_dim = 3)\n      conv = tf.concat(axis = 3, values = output_groups)\n\n\n    # Add biases\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\n\n    # Apply relu function\n    relu = tf.nn.relu(bias, name = scope.name)\n\n    return relu\n\n#def fc(x, num_in, num_out, name, relu = True):\ndef fc(x, num_in, num_out, name, relu):\n  with tf.variable_scope(name) as scope:\n\n    # Create tf variables for the weights and biases\n    weights = tf.get_variable('weights', shape=[num_in, num_out], trainable=True)\n    biases = tf.get_variable('biases', [num_out], trainable=True)\n\n    # Matrix multiply weights and inputs and add bias\n    act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n\n    if relu == True:\n      # Apply ReLu non linearity\n      relu = tf.nn.relu(act)\n      return relu\n    else:\n      return act\n\n\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name, padding='SAME'):\n  return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n                        strides = [1, stride_y, stride_x, 1],\n                        padding = padding, name = name)\n\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\n  return tf.nn.local_response_normalization(x, depth_radius = radius, alpha = alpha,\n                                            beta = beta, bias = bias, name = name)\n\ndef dropout(x, keep_prob):\n  return tf.nn.dropout(x, keep_prob)\n\n</code></pre>\n<p>Now the code for the loss function ad optimizer is</p>\n<pre><code>    # Op for calculating the loss\nwith tf.name_scope(\"cross_ent\"):\n    loss = tf.reduce_mean(tf.squared_difference(score, y))\n\n    # Train op\nwith tf.name_scope(\"train\"):\n      # Get gradients of all trainable variables\n    gradients = tf.gradients(loss, var_list)\n    gradients = list(zip(gradients, var_list))\n\n      # Create optimizer and apply gradient descent to the trainable variables\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n\n</code></pre>\n<p>Anything I should change in this part?<br>\nOr any comments, or anything I should take care of to change the model from classifcation to regression.<br>\nI am new to tensorflow and deep learning</p>", "body_text": "I am using pre-trained Alexnet as shown below. I want to use that model for regression with 6 outputs (Xcoordinate (range (0,227),Ycoordinate (range (0,227),height (range (20,50), width (range (20,50), sine(theta), cos(theta)). (range of theta is -180 to 180 degrees)\nThese are the following things I change -\n\nchanged loss function to MSE.\nchanged the output layer from 1000 to 6.\nchanged from RELU to linear activation function last layer.\nNow, I am not getting proper valued of sine and cosine above (it should be in the range of (-1 to 1)), I am getting out of bound values. What should I do, How should I keep a bound one the values. Also, should I keep a bout on other parameters as well. What should I do incorporate those changes?\nWhat are the other changes should I make to use this model for regression.?\n\n\nimport tensorflow as tf\nimport numpy as np\n\nclass AlexNet(object):\n\n  def __init__(self, x, keep_prob, num_classes, skip_layer,\n               weights_path = 'DEFAULT'):\n\n    # Parse input arguments into class variables\n    self.X = x\n    self.NUM_CLASSES = num_classes\n    self.KEEP_PROB = keep_prob\n    self.SKIP_LAYER = skip_layer\n\n    if weights_path == 'DEFAULT':\n      self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\n    else:\n      self.WEIGHTS_PATH = weights_path\n\n    # Call the create function to build the computational graph of AlexNet\n    self.create()\n\n  def create(self):\n\n    # 1st Layer: Conv (w ReLu) -> Pool -> Lrn\n    conv1 = conv(self.X, 11, 11, 96, 4, 4, padding = 'VALID', name = 'conv1')\n    pool1 = max_pool(conv1, 3, 3, 2, 2, padding = 'VALID', name = 'pool1')\n    norm1 = lrn(pool1, 2, 2e-05, 0.75, name = 'norm1')\n\n        # 2nd Layer: Conv (w ReLu) -> Pool -> Lrn with 2 groups\n    conv2 = conv(norm1, 5, 5, 256, 1, 1, groups = 2, name = 'conv2')\n    pool2 = max_pool(conv2, 3, 3, 2, 2, padding = 'VALID', name ='pool2')\n    norm2 = lrn(pool2, 2, 2e-05, 0.75, name = 'norm2')\n\n        # 3rd Layer: Conv (w ReLu)\n    conv3 = conv(norm2, 3, 3, 384, 1, 1, name = 'conv3')\n\n        # 4th Layer: Conv (w ReLu) splitted into two groups\n    conv4 = conv(conv3, 3, 3, 384, 1, 1, groups = 2, name = 'conv4')\n\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n    conv5 = conv(conv4, 3, 3, 256, 1, 1, groups = 2, name = 'conv5')\n    pool5 = max_pool(conv5, 3, 3, 2, 2, padding = 'VALID', name = 'pool5')\n\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n    flattened = tf.reshape(pool5, [-1, 6*6*256])\n    fc6 = fc(flattened, 6*6*256, 4096, name='fc6',relu =True)\n    dropout6 = dropout(fc6, self.KEEP_PROB)\n\n        # 7th Layer: FC (w ReLu) -> Dropout\n    fc7 = fc(dropout6, 4096, 4096, name = 'fc7',relu =False)\n    dropout7 = dropout(fc7, self.KEEP_PROB)\n\n        # 8th Layer: FC and return unscaled activations (for tf.nn.softmax_cross_entropy_with_logits)\n    self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu = False, name='fc8')\n\n\n\n  def load_initial_weights(self, session):\n    \"\"\"\n    As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/ come\n    as a dict of lists (e.g. weights['conv1'] is a list) and not as dict of\n    dicts (e.g. weights['conv1'] is a dict with keys 'weights' & 'biases') we\n    need a special load function\n    \"\"\"\n\n    # Load the weights into memory\n    weights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\n\n    # Loop over all layer names stored in the weights dict\n    for op_name in weights_dict:\n\n      # Check if the layer is one of the layers that should be reinitialized\n      if op_name not in self.SKIP_LAYER:\n\n        with tf.variable_scope(op_name, reuse = True):\n\n          # Loop over list of weights/biases and assign them to their corresponding tf variable\n          for data in weights_dict[op_name]:\n\n            # Biases\n            if len(data.shape) == 1:\n\n              var = tf.get_variable('biases', trainable = False)\n              session.run(var.assign(data))\n\n            # Weights\n            else:\n\n              var = tf.get_variable('weights', trainable = False)\n              session.run(var.assign(data))\n\n\n\n\"\"\"\nPredefine all necessary layer for the AlexNet\n\"\"\"\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n         padding='SAME', groups=1):\n  \"\"\"\n  Adapted from: https://github.com/ethereon/caffe-tensorflow\n  \"\"\"\n  # Get number of input channels\n  input_channels = int(x.get_shape()[-1])\n\n  # Create lambda function for the convolution\n  convolve = lambda i, k: tf.nn.conv2d(i, k,\n                                       strides = [1, stride_y, stride_x, 1],\n                                       padding = padding)\n\n  with tf.variable_scope(name) as scope:\n    # Create tf variables for the weights and biases of the conv layer\n    weights = tf.get_variable('weights', shape = [filter_height, filter_width, input_channels/groups, num_filters])\n    biases = tf.get_variable('biases', shape = [num_filters])\n\n\n    if groups == 1:\n      conv = convolve(x, weights)\n\n    # In the cases of multiple groups, split inputs & weights and\n    else:\n      # Split input and weights and convolve them separately\n      #input_groups = tf.split(value=x, num_split= groups, split_dim=3)\n      #input_groups = tf.split(split_dim=3, num_split= groups,value=x)\n      input_groups = tf.split(axis = 3, num_or_size_splits=groups, value=x)\n     # weight_groups = tf.split(value =weights, num_split=groups, split_dim=3)\n      weight_groups = tf.split(axis = 3, num_or_size_splits=groups, value=weights)\n\n      output_groups = [convolve(i, k) for i,k in zip(input_groups, weight_groups)]\n\n      # Concat the convolved output together again\n      #conv = tf.concat( values = output_groups,concat_dim = 3)\n      conv = tf.concat(axis = 3, values = output_groups)\n\n\n    # Add biases\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\n\n    # Apply relu function\n    relu = tf.nn.relu(bias, name = scope.name)\n\n    return relu\n\n#def fc(x, num_in, num_out, name, relu = True):\ndef fc(x, num_in, num_out, name, relu):\n  with tf.variable_scope(name) as scope:\n\n    # Create tf variables for the weights and biases\n    weights = tf.get_variable('weights', shape=[num_in, num_out], trainable=True)\n    biases = tf.get_variable('biases', [num_out], trainable=True)\n\n    # Matrix multiply weights and inputs and add bias\n    act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n\n    if relu == True:\n      # Apply ReLu non linearity\n      relu = tf.nn.relu(act)\n      return relu\n    else:\n      return act\n\n\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name, padding='SAME'):\n  return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n                        strides = [1, stride_y, stride_x, 1],\n                        padding = padding, name = name)\n\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\n  return tf.nn.local_response_normalization(x, depth_radius = radius, alpha = alpha,\n                                            beta = beta, bias = bias, name = name)\n\ndef dropout(x, keep_prob):\n  return tf.nn.dropout(x, keep_prob)\n\n\nNow the code for the loss function ad optimizer is\n    # Op for calculating the loss\nwith tf.name_scope(\"cross_ent\"):\n    loss = tf.reduce_mean(tf.squared_difference(score, y))\n\n    # Train op\nwith tf.name_scope(\"train\"):\n      # Get gradients of all trainable variables\n    gradients = tf.gradients(loss, var_list)\n    gradients = list(zip(gradients, var_list))\n\n      # Create optimizer and apply gradient descent to the trainable variables\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n\n\nAnything I should change in this part?\nOr any comments, or anything I should take care of to change the model from classifcation to regression.\nI am new to tensorflow and deep learning", "body": "I am using pre-trained Alexnet as shown below. I want to use that model for regression with 6 outputs (Xcoordinate (range (0,227),Ycoordinate (range (0,227),height (range (20,50), width (range (20,50), sine(theta), cos(theta)). (range of theta is -180 to 180 degrees)\r\nThese are the following things I change - \r\n1. changed loss function to MSE. \r\n2. changed the output layer from 1000 to 6. \r\n3. changed from RELU to linear activation function last layer. \r\nNow, I am not getting proper valued of sine and cosine above (it should be in the range of (-1 to 1)), I am getting out of bound values. What should I do, How should I keep a bound one the values. Also, should I keep a bout on other parameters as well. What should I do incorporate those changes?  \r\nWhat are the other changes should I make to use this model for regression.? \r\n\r\n```\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nclass AlexNet(object):\r\n\r\n  def __init__(self, x, keep_prob, num_classes, skip_layer,\r\n               weights_path = 'DEFAULT'):\r\n\r\n    # Parse input arguments into class variables\r\n    self.X = x\r\n    self.NUM_CLASSES = num_classes\r\n    self.KEEP_PROB = keep_prob\r\n    self.SKIP_LAYER = skip_layer\r\n\r\n    if weights_path == 'DEFAULT':\r\n      self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\r\n    else:\r\n      self.WEIGHTS_PATH = weights_path\r\n\r\n    # Call the create function to build the computational graph of AlexNet\r\n    self.create()\r\n\r\n  def create(self):\r\n\r\n    # 1st Layer: Conv (w ReLu) -> Pool -> Lrn\r\n    conv1 = conv(self.X, 11, 11, 96, 4, 4, padding = 'VALID', name = 'conv1')\r\n    pool1 = max_pool(conv1, 3, 3, 2, 2, padding = 'VALID', name = 'pool1')\r\n    norm1 = lrn(pool1, 2, 2e-05, 0.75, name = 'norm1')\r\n\r\n        # 2nd Layer: Conv (w ReLu) -> Pool -> Lrn with 2 groups\r\n    conv2 = conv(norm1, 5, 5, 256, 1, 1, groups = 2, name = 'conv2')\r\n    pool2 = max_pool(conv2, 3, 3, 2, 2, padding = 'VALID', name ='pool2')\r\n    norm2 = lrn(pool2, 2, 2e-05, 0.75, name = 'norm2')\r\n\r\n        # 3rd Layer: Conv (w ReLu)\r\n    conv3 = conv(norm2, 3, 3, 384, 1, 1, name = 'conv3')\r\n\r\n        # 4th Layer: Conv (w ReLu) splitted into two groups\r\n    conv4 = conv(conv3, 3, 3, 384, 1, 1, groups = 2, name = 'conv4')\r\n\r\n        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\r\n    conv5 = conv(conv4, 3, 3, 256, 1, 1, groups = 2, name = 'conv5')\r\n    pool5 = max_pool(conv5, 3, 3, 2, 2, padding = 'VALID', name = 'pool5')\r\n\r\n        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\r\n    flattened = tf.reshape(pool5, [-1, 6*6*256])\r\n    fc6 = fc(flattened, 6*6*256, 4096, name='fc6',relu =True)\r\n    dropout6 = dropout(fc6, self.KEEP_PROB)\r\n\r\n        # 7th Layer: FC (w ReLu) -> Dropout\r\n    fc7 = fc(dropout6, 4096, 4096, name = 'fc7',relu =False)\r\n    dropout7 = dropout(fc7, self.KEEP_PROB)\r\n\r\n        # 8th Layer: FC and return unscaled activations (for tf.nn.softmax_cross_entropy_with_logits)\r\n    self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu = False, name='fc8')\r\n\r\n\r\n\r\n  def load_initial_weights(self, session):\r\n    \"\"\"\r\n    As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/ come\r\n    as a dict of lists (e.g. weights['conv1'] is a list) and not as dict of\r\n    dicts (e.g. weights['conv1'] is a dict with keys 'weights' & 'biases') we\r\n    need a special load function\r\n    \"\"\"\r\n\r\n    # Load the weights into memory\r\n    weights_dict = np.load(self.WEIGHTS_PATH, encoding = 'bytes').item()\r\n\r\n    # Loop over all layer names stored in the weights dict\r\n    for op_name in weights_dict:\r\n\r\n      # Check if the layer is one of the layers that should be reinitialized\r\n      if op_name not in self.SKIP_LAYER:\r\n\r\n        with tf.variable_scope(op_name, reuse = True):\r\n\r\n          # Loop over list of weights/biases and assign them to their corresponding tf variable\r\n          for data in weights_dict[op_name]:\r\n\r\n            # Biases\r\n            if len(data.shape) == 1:\r\n\r\n              var = tf.get_variable('biases', trainable = False)\r\n              session.run(var.assign(data))\r\n\r\n            # Weights\r\n            else:\r\n\r\n              var = tf.get_variable('weights', trainable = False)\r\n              session.run(var.assign(data))\r\n\r\n\r\n\r\n\"\"\"\r\nPredefine all necessary layer for the AlexNet\r\n\"\"\"\r\ndef conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\r\n         padding='SAME', groups=1):\r\n  \"\"\"\r\n  Adapted from: https://github.com/ethereon/caffe-tensorflow\r\n  \"\"\"\r\n  # Get number of input channels\r\n  input_channels = int(x.get_shape()[-1])\r\n\r\n  # Create lambda function for the convolution\r\n  convolve = lambda i, k: tf.nn.conv2d(i, k,\r\n                                       strides = [1, stride_y, stride_x, 1],\r\n                                       padding = padding)\r\n\r\n  with tf.variable_scope(name) as scope:\r\n    # Create tf variables for the weights and biases of the conv layer\r\n    weights = tf.get_variable('weights', shape = [filter_height, filter_width, input_channels/groups, num_filters])\r\n    biases = tf.get_variable('biases', shape = [num_filters])\r\n\r\n\r\n    if groups == 1:\r\n      conv = convolve(x, weights)\r\n\r\n    # In the cases of multiple groups, split inputs & weights and\r\n    else:\r\n      # Split input and weights and convolve them separately\r\n      #input_groups = tf.split(value=x, num_split= groups, split_dim=3)\r\n      #input_groups = tf.split(split_dim=3, num_split= groups,value=x)\r\n      input_groups = tf.split(axis = 3, num_or_size_splits=groups, value=x)\r\n     # weight_groups = tf.split(value =weights, num_split=groups, split_dim=3)\r\n      weight_groups = tf.split(axis = 3, num_or_size_splits=groups, value=weights)\r\n\r\n      output_groups = [convolve(i, k) for i,k in zip(input_groups, weight_groups)]\r\n\r\n      # Concat the convolved output together again\r\n      #conv = tf.concat( values = output_groups,concat_dim = 3)\r\n      conv = tf.concat(axis = 3, values = output_groups)\r\n\r\n\r\n    # Add biases\r\n    bias = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape().as_list())\r\n\r\n    # Apply relu function\r\n    relu = tf.nn.relu(bias, name = scope.name)\r\n\r\n    return relu\r\n\r\n#def fc(x, num_in, num_out, name, relu = True):\r\ndef fc(x, num_in, num_out, name, relu):\r\n  with tf.variable_scope(name) as scope:\r\n\r\n    # Create tf variables for the weights and biases\r\n    weights = tf.get_variable('weights', shape=[num_in, num_out], trainable=True)\r\n    biases = tf.get_variable('biases', [num_out], trainable=True)\r\n\r\n    # Matrix multiply weights and inputs and add bias\r\n    act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\r\n\r\n    if relu == True:\r\n      # Apply ReLu non linearity\r\n      relu = tf.nn.relu(act)\r\n      return relu\r\n    else:\r\n      return act\r\n\r\n\r\ndef max_pool(x, filter_height, filter_width, stride_y, stride_x, name, padding='SAME'):\r\n  return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\r\n                        strides = [1, stride_y, stride_x, 1],\r\n                        padding = padding, name = name)\r\n\r\ndef lrn(x, radius, alpha, beta, name, bias=1.0):\r\n  return tf.nn.local_response_normalization(x, depth_radius = radius, alpha = alpha,\r\n                                            beta = beta, bias = bias, name = name)\r\n\r\ndef dropout(x, keep_prob):\r\n  return tf.nn.dropout(x, keep_prob)\r\n\r\n```\r\n\r\nNow the code for the loss function ad optimizer is\r\n\r\n```\r\n    # Op for calculating the loss\r\nwith tf.name_scope(\"cross_ent\"):\r\n    loss = tf.reduce_mean(tf.squared_difference(score, y))\r\n\r\n    # Train op\r\nwith tf.name_scope(\"train\"):\r\n      # Get gradients of all trainable variables\r\n    gradients = tf.gradients(loss, var_list)\r\n    gradients = list(zip(gradients, var_list))\r\n\r\n      # Create optimizer and apply gradient descent to the trainable variables\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\r\n\r\n``` \r\nAnything I should change in this part? \r\nOr any comments, or anything I should take care of to change the model from classifcation to regression. \r\nI am new to tensorflow and deep learning"}