{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13388", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13388/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13388/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13388/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/13388", "id": 261778287, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQzOTg2MzYz", "number": 13388, "title": "Add fp16 support to fused batchnorm op", "user": {"login": "nluehr", "id": 1873655, "node_id": "MDQ6VXNlcjE4NzM2NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1873655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nluehr", "html_url": "https://github.com/nluehr", "followers_url": "https://api.github.com/users/nluehr/followers", "following_url": "https://api.github.com/users/nluehr/following{/other_user}", "gists_url": "https://api.github.com/users/nluehr/gists{/gist_id}", "starred_url": "https://api.github.com/users/nluehr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nluehr/subscriptions", "organizations_url": "https://api.github.com/users/nluehr/orgs", "repos_url": "https://api.github.com/users/nluehr/repos", "events_url": "https://api.github.com/users/nluehr/events{/privacy}", "received_events_url": "https://api.github.com/users/nluehr/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 419840263, "node_id": "MDU6TGFiZWw0MTk4NDAyNjM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20testing%20(then%20merge)", "name": "awaiting testing (then merge)", "color": "c2e0c6", "default": false}, {"id": 300136613, "node_id": "MDU6TGFiZWwzMDAxMzY2MTM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20no", "name": "cla: no", "color": "eb6420", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-09-29T21:40:21Z", "updated_at": "2017-11-09T06:53:20Z", "closed_at": "2017-11-09T06:53:20Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13388", "html_url": "https://github.com/tensorflow/tensorflow/pull/13388", "diff_url": "https://github.com/tensorflow/tensorflow/pull/13388.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/13388.patch"}, "body_html": "<p>Attention <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a></p>\n<ul>\n<li>This commit adds a mixed-precision fused_batch_norm_v2 op.<br>\nThe inputs and outputs are fp16, while the scale, offset, mean<br>\nand variance are kept in fp32.</li>\n<li>The tf.nn.fused_batch_norm op has been modified to use the v2<br>\nfused batchnorm whenever inputs are fp16 (this does not affect<br>\ncompatibility because fp16 was not previously supported).</li>\n<li>The high-level layers API has also been updated to store the<br>\nscale, offset, mean and variance variables as fp32.</li>\n</ul>", "body_text": "Attention @zheng-xq\n\nThis commit adds a mixed-precision fused_batch_norm_v2 op.\nThe inputs and outputs are fp16, while the scale, offset, mean\nand variance are kept in fp32.\nThe tf.nn.fused_batch_norm op has been modified to use the v2\nfused batchnorm whenever inputs are fp16 (this does not affect\ncompatibility because fp16 was not previously supported).\nThe high-level layers API has also been updated to store the\nscale, offset, mean and variance variables as fp32.", "body": "Attention @zheng-xq \r\n\r\n- This commit adds a mixed-precision fused_batch_norm_v2 op.\r\n  The inputs and outputs are fp16, while the scale, offset, mean\r\n  and variance are kept in fp32.\r\n- The tf.nn.fused_batch_norm op has been modified to use the v2\r\n  fused batchnorm whenever inputs are fp16 (this does not affect\r\n  compatibility because fp16 was not previously supported).\r\n- The high-level layers API has also been updated to store the\r\n  scale, offset, mean and variance variables as fp32."}