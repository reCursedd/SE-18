{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/435612770", "html_url": "https://github.com/tensorflow/tensorflow/issues/12876#issuecomment-435612770", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12876", "id": 435612770, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNTYxMjc3MA==", "user": {"login": "Bengt", "id": 11575, "node_id": "MDQ6VXNlcjExNTc1", "avatar_url": "https://avatars0.githubusercontent.com/u/11575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bengt", "html_url": "https://github.com/Bengt", "followers_url": "https://api.github.com/users/Bengt/followers", "following_url": "https://api.github.com/users/Bengt/following{/other_user}", "gists_url": "https://api.github.com/users/Bengt/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bengt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bengt/subscriptions", "organizations_url": "https://api.github.com/users/Bengt/orgs", "repos_url": "https://api.github.com/users/Bengt/repos", "events_url": "https://api.github.com/users/Bengt/events{/privacy}", "received_events_url": "https://api.github.com/users/Bengt/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-03T19:02:16Z", "updated_at": "2018-11-03T19:02:16Z", "author_association": "NONE", "body_html": "<p>I just ran into this, too. I wanted to make sure my importer code is correct, so I also implemented an exporter. Surprisingly, the results were not the same as the inputs. For my use case, the loss in precision that is described in this issue actually matters. My examples are drawn from a problem with a fractal result space. So deviating in the problem space by losing some precision actually makes my examples wrong, as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22581048\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/quaeler\">@quaeler</a> noted. This means that beyond 32 bit precision, my estimator would be learning garbage. However, I found that the work around of storing the data as strings and casting them back and forth is actually common.</p>\n<p>Here is a blog post from the future (May 2019), encoding floats as bytes:</p>\n<p><a href=\"http://jrmeyer.github.io/machinelearning/2019/05/29/tensorflow-dataset-estimator-api.html\" rel=\"nofollow\">http://jrmeyer.github.io/machinelearning/2019/05/29/tensorflow-dataset-estimator-api.html</a></p>\n<p>The money bit for serialization:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_float64_feature</span>(<span class=\"pl-smi\">float64_value</span>):\n    float64_bytes <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">str</span>(float64_value).encode()]\n    bytes_list <span class=\"pl-k\">=</span> tf.train.BytesList(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span>float64_bytes)\n    bytes_list_feature <span class=\"pl-k\">=</span> tf.train.Feature(<span class=\"pl-v\">bytes_list</span><span class=\"pl-k\">=</span>bytes_list)\n\n    <span class=\"pl-k\">return</span> bytes_list_feature</pre></div>\n<p>... and for deserialization:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">deserialize</span>(<span class=\"pl-smi\">serialized_example</span>):\n    features <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>float_value<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], tf.string),\n    }\n    features <span class=\"pl-k\">=</span> tf.parse_single_example(\n        serialized_example,\n        <span class=\"pl-v\">features</span><span class=\"pl-k\">=</span>features\n    )\n\n    features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float_value<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> tf.strings.to_number(<span class=\"pl-v\">string_tensor</span><span class=\"pl-k\">=</span>features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float_value<span class=\"pl-pds\">'</span></span>], <span class=\"pl-v\">out_type</span><span class=\"pl-k\">=</span>tf.float64)\n\n    <span class=\"pl-k\">return</span> features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float_value<span class=\"pl-pds\">'</span></span>]</pre></div>", "body_text": "I just ran into this, too. I wanted to make sure my importer code is correct, so I also implemented an exporter. Surprisingly, the results were not the same as the inputs. For my use case, the loss in precision that is described in this issue actually matters. My examples are drawn from a problem with a fractal result space. So deviating in the problem space by losing some precision actually makes my examples wrong, as @quaeler noted. This means that beyond 32 bit precision, my estimator would be learning garbage. However, I found that the work around of storing the data as strings and casting them back and forth is actually common.\nHere is a blog post from the future (May 2019), encoding floats as bytes:\nhttp://jrmeyer.github.io/machinelearning/2019/05/29/tensorflow-dataset-estimator-api.html\nThe money bit for serialization:\ndef _float64_feature(float64_value):\n    float64_bytes = [str(float64_value).encode()]\n    bytes_list = tf.train.BytesList(value=float64_bytes)\n    bytes_list_feature = tf.train.Feature(bytes_list=bytes_list)\n\n    return bytes_list_feature\n... and for deserialization:\ndef deserialize(serialized_example):\n    features = {\n        'float_value': tf.FixedLenFeature([], tf.string),\n    }\n    features = tf.parse_single_example(\n        serialized_example,\n        features=features\n    )\n\n    features['float_value'] = tf.strings.to_number(string_tensor=features['float_value'], out_type=tf.float64)\n\n    return features['float_value']", "body": "I just ran into this, too. I wanted to make sure my importer code is correct, so I also implemented an exporter. Surprisingly, the results were not the same as the inputs. For my use case, the loss in precision that is described in this issue actually matters. My examples are drawn from a problem with a fractal result space. So deviating in the problem space by losing some precision actually makes my examples wrong, as @quaeler noted. This means that beyond 32 bit precision, my estimator would be learning garbage. However, I found that the work around of storing the data as strings and casting them back and forth is actually common.\r\n\r\nHere is a blog post from the future (May 2019), encoding floats as bytes:\r\n\r\nhttp://jrmeyer.github.io/machinelearning/2019/05/29/tensorflow-dataset-estimator-api.html\r\n\r\nThe money bit for serialization:\r\n\r\n```python\r\ndef _float64_feature(float64_value):\r\n    float64_bytes = [str(float64_value).encode()]\r\n    bytes_list = tf.train.BytesList(value=float64_bytes)\r\n    bytes_list_feature = tf.train.Feature(bytes_list=bytes_list)\r\n\r\n    return bytes_list_feature\r\n```\r\n\r\n... and for deserialization:\r\n\r\n```python\r\ndef deserialize(serialized_example):\r\n    features = {\r\n        'float_value': tf.FixedLenFeature([], tf.string),\r\n    }\r\n    features = tf.parse_single_example(\r\n        serialized_example,\r\n        features=features\r\n    )\r\n\r\n    features['float_value'] = tf.strings.to_number(string_tensor=features['float_value'], out_type=tf.float64)\r\n\r\n    return features['float_value']\r\n```"}