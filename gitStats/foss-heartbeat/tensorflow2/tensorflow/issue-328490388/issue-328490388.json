{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19688", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19688/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19688/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19688/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19688", "id": 328490388, "node_id": "MDU6SXNzdWUzMjg0OTAzODg=", "number": 19688, "title": "why new defined op always runs on PS host, not in worker host?", "user": {"login": "yuanwujun", "id": 2764923, "node_id": "MDQ6VXNlcjI3NjQ5MjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/2764923?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanwujun", "html_url": "https://github.com/yuanwujun", "followers_url": "https://api.github.com/users/yuanwujun/followers", "following_url": "https://api.github.com/users/yuanwujun/following{/other_user}", "gists_url": "https://api.github.com/users/yuanwujun/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanwujun/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanwujun/subscriptions", "organizations_url": "https://api.github.com/users/yuanwujun/orgs", "repos_url": "https://api.github.com/users/yuanwujun/repos", "events_url": "https://api.github.com/users/yuanwujun/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanwujun/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-01T11:52:22Z", "updated_at": "2018-06-05T18:15:03Z", "closed_at": "2018-06-05T18:15:03Z", "author_association": "NONE", "body_html": "<p>i have refactored word2vec code\uff08from models/tutorials/embedding/\uff09, my problem is new defined OP \"train_op = word2vec.neg_train_word2vec \"  runs on PS,  I don't know where I went wrong.  my code :<br>\nwith tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,<br>\ncluster=cluster, ps_strategy=ps_strategy, merge_devices=False)):</p>\n<pre><code>        # Declare all variables we need.\n        embedding = tf.get_variable(name=\"word_embedding\", shape = [71291,FLAGS.embedding_size],\n                    initializer = tf.random_uniform_initializer(-0.5 / FLAGS.embedding_size, 0.5 / FLAGS.embedding_size))\n\n        weights =   tf.get_variable(name=\"word_weights\",shape = [71291, FLAGS.embedding_size],\n                    initializer = tf.random_normal_initializer() )\n\n\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n\n        # The training data\n        ( words,\n          counts,\n          words_per_epoch,\n          current_epoch,\n          total_words_processed,examples,\n          labels) = word2vec.skipgram_word2vec(filename=FLAGS.train_data,\n                                       batch_size=FLAGS.batch_size,\n                                       window_size=FLAGS.window_size,\n                                       min_count=FLAGS.min_count,\n                                       subsample=FLAGS.subsample)\n\n        # Linear learning rate decay.\n        words_to_train = tf.cast(words_per_epoch * FLAGS.epochs_to_train, tf.float32)\n        lr = FLAGS.learning_rate * tf.maximum(\n                    0.0001,\n                    1.0 - tf.cast(total_words_processed, tf.float32) / words_to_train)\n\n        train_op = word2vec.neg_train_word2vec(embedding,\n                                      weights,\n                                      examples,\n                                      labels,\n                                      lr,\n                                      #vocab_count=counts.tolist(),\n                                       num_negative_samples=FLAGS.num_neg_samples)\n</code></pre>", "body_text": "i have refactored word2vec code\uff08from models/tutorials/embedding/\uff09, my problem is new defined OP \"train_op = word2vec.neg_train_word2vec \"  runs on PS,  I don't know where I went wrong.  my code :\nwith tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\ncluster=cluster, ps_strategy=ps_strategy, merge_devices=False)):\n        # Declare all variables we need.\n        embedding = tf.get_variable(name=\"word_embedding\", shape = [71291,FLAGS.embedding_size],\n                    initializer = tf.random_uniform_initializer(-0.5 / FLAGS.embedding_size, 0.5 / FLAGS.embedding_size))\n\n        weights =   tf.get_variable(name=\"word_weights\",shape = [71291, FLAGS.embedding_size],\n                    initializer = tf.random_normal_initializer() )\n\n\n        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n\n        # The training data\n        ( words,\n          counts,\n          words_per_epoch,\n          current_epoch,\n          total_words_processed,examples,\n          labels) = word2vec.skipgram_word2vec(filename=FLAGS.train_data,\n                                       batch_size=FLAGS.batch_size,\n                                       window_size=FLAGS.window_size,\n                                       min_count=FLAGS.min_count,\n                                       subsample=FLAGS.subsample)\n\n        # Linear learning rate decay.\n        words_to_train = tf.cast(words_per_epoch * FLAGS.epochs_to_train, tf.float32)\n        lr = FLAGS.learning_rate * tf.maximum(\n                    0.0001,\n                    1.0 - tf.cast(total_words_processed, tf.float32) / words_to_train)\n\n        train_op = word2vec.neg_train_word2vec(embedding,\n                                      weights,\n                                      examples,\n                                      labels,\n                                      lr,\n                                      #vocab_count=counts.tolist(),\n                                       num_negative_samples=FLAGS.num_neg_samples)", "body": "i have refactored word2vec code\uff08from models/tutorials/embedding/\uff09, my problem is new defined OP \"train_op = word2vec.neg_train_word2vec \"  runs on PS,  I don't know where I went wrong.  my code :           \r\nwith tf.device(tf.train.replica_device_setter(worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                                                      cluster=cluster, ps_strategy=ps_strategy, merge_devices=False)):\r\n\r\n            # Declare all variables we need.\r\n            embedding = tf.get_variable(name=\"word_embedding\", shape = [71291,FLAGS.embedding_size],\r\n                        initializer = tf.random_uniform_initializer(-0.5 / FLAGS.embedding_size, 0.5 / FLAGS.embedding_size))\r\n\r\n            weights =   tf.get_variable(name=\"word_weights\",shape = [71291, FLAGS.embedding_size],\r\n                        initializer = tf.random_normal_initializer() )\r\n\r\n\r\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n\r\n            # The training data\r\n            ( words,\r\n              counts,\r\n              words_per_epoch,\r\n              current_epoch,\r\n              total_words_processed,examples,\r\n              labels) = word2vec.skipgram_word2vec(filename=FLAGS.train_data,\r\n                                           batch_size=FLAGS.batch_size,\r\n                                           window_size=FLAGS.window_size,\r\n                                           min_count=FLAGS.min_count,\r\n                                           subsample=FLAGS.subsample)\r\n\r\n            # Linear learning rate decay.\r\n            words_to_train = tf.cast(words_per_epoch * FLAGS.epochs_to_train, tf.float32)\r\n            lr = FLAGS.learning_rate * tf.maximum(\r\n                        0.0001,\r\n                        1.0 - tf.cast(total_words_processed, tf.float32) / words_to_train)\r\n\r\n            train_op = word2vec.neg_train_word2vec(embedding,\r\n                                          weights,\r\n                                          examples,\r\n                                          labels,\r\n                                          lr,\r\n                                          #vocab_count=counts.tolist(),\r\n                                           num_negative_samples=FLAGS.num_neg_samples)"}