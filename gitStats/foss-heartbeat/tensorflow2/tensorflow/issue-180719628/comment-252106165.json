{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/252106165", "html_url": "https://github.com/tensorflow/tensorflow/issues/4735#issuecomment-252106165", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4735", "id": 252106165, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MjEwNjE2NQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-06T22:26:37Z", "updated_at": "2016-10-06T22:26:37Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Depending on the width of the model, TensorFlow can be somewhat non-deterministic in terms of scheduling. Right now, it schedule ops that are ready to go eagerly. That could cause some models to use more memory sometimes. The effect is often not huge.</p>\n<p>But this could make your training to fail of your model is dangerously close to the edge. A few things to try:</p>\n<ol>\n<li>Use a slightly smaller batch size.</li>\n<li>Set TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a smaller number. The default is 4GB.</li>\n<li>If your model might have large number of parallel ops, group them together and add a control dependency to serialize them sometimes help. This is not recommended in general.</li>\n</ol>\n<p>If it really takes a huge number of steps to fail, some groups tend to ignore it. Load from a previously good checkpoint and restart your training automatically.</p>", "body_text": "Depending on the width of the model, TensorFlow can be somewhat non-deterministic in terms of scheduling. Right now, it schedule ops that are ready to go eagerly. That could cause some models to use more memory sometimes. The effect is often not huge.\nBut this could make your training to fail of your model is dangerously close to the edge. A few things to try:\n\nUse a slightly smaller batch size.\nSet TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a smaller number. The default is 4GB.\nIf your model might have large number of parallel ops, group them together and add a control dependency to serialize them sometimes help. This is not recommended in general.\n\nIf it really takes a huge number of steps to fail, some groups tend to ignore it. Load from a previously good checkpoint and restart your training automatically.", "body": "Depending on the width of the model, TensorFlow can be somewhat non-deterministic in terms of scheduling. Right now, it schedule ops that are ready to go eagerly. That could cause some models to use more memory sometimes. The effect is often not huge. \n\nBut this could make your training to fail of your model is dangerously close to the edge. A few things to try: \n1. Use a slightly smaller batch size. \n2. Set TF_CUDNN_WORKSPACE_LIMIT_IN_MB to a smaller number. The default is 4GB. \n3. If your model might have large number of parallel ops, group them together and add a control dependency to serialize them sometimes help. This is not recommended in general. \n\nIf it really takes a huge number of steps to fail, some groups tend to ignore it. Load from a previously good checkpoint and restart your training automatically. \n"}