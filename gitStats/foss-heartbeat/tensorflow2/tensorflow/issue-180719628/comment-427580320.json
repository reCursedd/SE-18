{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/427580320", "html_url": "https://github.com/tensorflow/tensorflow/issues/4735#issuecomment-427580320", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4735", "id": 427580320, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNzU4MDMyMA==", "user": {"login": "deepakmeena635", "id": 37202245, "node_id": "MDQ6VXNlcjM3MjAyMjQ1", "avatar_url": "https://avatars0.githubusercontent.com/u/37202245?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepakmeena635", "html_url": "https://github.com/deepakmeena635", "followers_url": "https://api.github.com/users/deepakmeena635/followers", "following_url": "https://api.github.com/users/deepakmeena635/following{/other_user}", "gists_url": "https://api.github.com/users/deepakmeena635/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepakmeena635/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepakmeena635/subscriptions", "organizations_url": "https://api.github.com/users/deepakmeena635/orgs", "repos_url": "https://api.github.com/users/deepakmeena635/repos", "events_url": "https://api.github.com/users/deepakmeena635/events{/privacy}", "received_events_url": "https://api.github.com/users/deepakmeena635/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-06T14:59:33Z", "updated_at": "2018-10-06T16:28:40Z", "author_association": "NONE", "body_html": "<p>Hello,<br>\nIts my first time posting an issue, its more of a query itd be nice if anyone can help me out asap <g-emoji class=\"g-emoji\" alias=\"crossed_fingers\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f91e.png\">\ud83e\udd1e</g-emoji></p>\n<p>I'm using google colab with a GPU backend,</p>\n<p>I'm having a similar issue,</p>\n<p>------error :{================================</p>\n<p>ResourceExhaustedError (see above for traceback):<br>\nOOM when allocating tensor with shape[12708,50320]<br>\nand type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc<br>\n[[{{node rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniform<a href=\"rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/shape\">T=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"</a>]]<br>\nHint:... .....<br>\n========================================}<br>\n(let me know i you need the complete log)</p>\n<p>MODEL:------------------------------</p>\n<p>embedding_size = 128<br>\nbatch_size = 8 (failing at 4 too )<br>\ntar_vocab = 12580</p>\n<p>cell = mrc(  cells= [  lstm(num_units = embedding_size,  dtype=tf.float32 ),<br>\nlstm(embedding_size),<br>\nlstm (  tar_vocab) ])<br>\nstate0 = cell.zero_state(batch_size=batch_size, dtype=tf.float32)</p>\n<p>def runn( inp, seq_len, batch_size=8 ):<br>\nseqs, _  = tf.nn.dynamic_rnn(cell= cell,<br>\ninitial_state=state0,<br>\ninputs= inp,<br>\nsequence_length = seq_len ,<br>\ntime_major=False )<br>\nreturn seqs</p>\n<p>logits = runn(   )<br>\ncost = reduce_mean(softmax_cross_entropy( logits, Y ) )</p>\n<p>opt = tf.train.AdamOptimizer(learning_rate = 0.0007)<br>\nidk = opt.minimize( cost, aggregation_method = tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N )</p>\n<p>=============</p>\n<h2>using a generator for training batch data</h2>\n<p>WHAT I'VE tried:<br>\na. reducing batch size from 16 to4<br>\nb.reducing embedding size from 512 to 64<br>\nc. I read the following thread on stack overflow seems really convincing<br>\n<a href=\"https://stackoverflow.com/questions/36139889/rnn-model-running-out-of-memory-in-tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/36139889/rnn-model-running-out-of-memory-in-tensorflow</a><br>\nbut none of the aggregation method's working</p>\n<p>SUSPICIONS :<br>\na. gpu memory is full before ram<br>\nb. I'm not sure about tf internals but could it be coz I'm using a for loop inside a session  here's the training loop</p>\n<p>def train( batch_size):</p>\n<pre><code>test = gen( batch_size  )\nbatches = np.asarray(x_train).shape[0] // batch_size\nbatches=10 # breakPoint (NOT EVEN 10 BATCHES ARE GETTING PAST IT  of size 4 example  )\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    costs = []\n    for batch in batches :\n        sample = next(test)\n        sample_x = np.array(sample[0])\n        sample_y = np.array(sample[1])\n        \n        cost, _  = sess.run([ cost,idk],\n                            feed_dict= {\n                                plc_x: sample_x, \n                                plc_y: sample_y })\n        costs.append(cost)\nreturn costs\n</code></pre>", "body_text": "Hello,\nIts my first time posting an issue, its more of a query itd be nice if anyone can help me out asap \ud83e\udd1e\nI'm using google colab with a GPU backend,\nI'm having a similar issue,\n------error :{================================\nResourceExhaustedError (see above for traceback):\nOOM when allocating tensor with shape[12708,50320]\nand type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n[[{{node rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniformT=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"]]\nHint:... .....\n========================================}\n(let me know i you need the complete log)\nMODEL:------------------------------\nembedding_size = 128\nbatch_size = 8 (failing at 4 too )\ntar_vocab = 12580\ncell = mrc(  cells= [  lstm(num_units = embedding_size,  dtype=tf.float32 ),\nlstm(embedding_size),\nlstm (  tar_vocab) ])\nstate0 = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\ndef runn( inp, seq_len, batch_size=8 ):\nseqs, _  = tf.nn.dynamic_rnn(cell= cell,\ninitial_state=state0,\ninputs= inp,\nsequence_length = seq_len ,\ntime_major=False )\nreturn seqs\nlogits = runn(   )\ncost = reduce_mean(softmax_cross_entropy( logits, Y ) )\nopt = tf.train.AdamOptimizer(learning_rate = 0.0007)\nidk = opt.minimize( cost, aggregation_method = tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N )\n=============\nusing a generator for training batch data\nWHAT I'VE tried:\na. reducing batch size from 16 to4\nb.reducing embedding size from 512 to 64\nc. I read the following thread on stack overflow seems really convincing\nhttps://stackoverflow.com/questions/36139889/rnn-model-running-out-of-memory-in-tensorflow\nbut none of the aggregation method's working\nSUSPICIONS :\na. gpu memory is full before ram\nb. I'm not sure about tf internals but could it be coz I'm using a for loop inside a session  here's the training loop\ndef train( batch_size):\ntest = gen( batch_size  )\nbatches = np.asarray(x_train).shape[0] // batch_size\nbatches=10 # breakPoint (NOT EVEN 10 BATCHES ARE GETTING PAST IT  of size 4 example  )\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    costs = []\n    for batch in batches :\n        sample = next(test)\n        sample_x = np.array(sample[0])\n        sample_y = np.array(sample[1])\n        \n        cost, _  = sess.run([ cost,idk],\n                            feed_dict= {\n                                plc_x: sample_x, \n                                plc_y: sample_y })\n        costs.append(cost)\nreturn costs", "body": "Hello, \r\nIts my first time posting an issue, its more of a query itd be nice if anyone can help me out asap :crossed_fingers:  \r\n\r\nI'm using google colab with a GPU backend, \r\n\r\nI'm having a similar issue,\r\n\r\n------error :{================================\r\n\r\nResourceExhaustedError (see above for traceback): \r\nOOM when allocating tensor with shape[12708,50320] \r\nand type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[{{node rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/RandomUniform}} = RandomUniform[T=DT_INT32, _class=[\"loc:@rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Assign\"], dtype=DT_FLOAT, seed=0, seed2=0, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](rnn/multi_rnn_cell/cell_2/basic_lstm_cell/kernel/Initializer/random_uniform/shape)]]\r\nHint:... ..... \r\n========================================}\r\n(let me know i you need the complete log)\r\n\r\n\r\n\r\nMODEL:------------------------------\r\n\r\nembedding_size = 128\r\nbatch_size = 8 (failing at 4 too )\r\ntar_vocab = 12580\r\n\r\ncell = mrc(  cells= [  lstm(num_units = embedding_size,  dtype=tf.float32 ), \r\n                               lstm(embedding_size), \r\n                               lstm (  tar_vocab) ])\r\nstate0 = cell.zero_state(batch_size=batch_size, dtype=tf.float32)\r\n\r\n\r\ndef runn( inp, seq_len, batch_size=8 ):\r\n    seqs, _  = tf.nn.dynamic_rnn(cell= cell,\r\n                                     initial_state=state0,\r\n                                     inputs= inp,\r\n                                     sequence_length = seq_len ,\r\n                                     time_major=False )\r\n    return seqs \r\n\r\n\r\nlogits = runn(   )\r\ncost = reduce_mean(softmax_cross_entropy( logits, Y ) )\r\n \r\nopt = tf.train.AdamOptimizer(learning_rate = 0.0007)\r\nidk = opt.minimize( cost, aggregation_method = tf.AggregationMethod.EXPERIMENTAL_ACCUMULATE_N )\r\n\r\n\r\n=============\r\n\r\nusing a generator for training batch data \r\n--------------------------------------------------\r\n\r\nWHAT I'VE tried:\r\na. reducing batch size from 16 to4 \r\nb.reducing embedding size from 512 to 64  \r\nc. I read the following thread on stack overflow seems really convincing  \r\n  https://stackoverflow.com/questions/36139889/rnn-model-running-out-of-memory-in-tensorflow\r\n  but none of the aggregation method's working   \r\n\r\n\r\nSUSPICIONS :\r\na. gpu memory is full before ram \r\nb. I'm not sure about tf internals but could it be coz I'm using a for loop inside a session  here's the training loop \r\n\r\ndef train( batch_size):\r\n    \r\n    test = gen( batch_size  )\r\n    batches = np.asarray(x_train).shape[0] // batch_size\r\n    batches=10 # breakPoint (NOT EVEN 10 BATCHES ARE GETTING PAST IT  of size 4 example  )\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        costs = []\r\n        for batch in batches :\r\n            sample = next(test)\r\n            sample_x = np.array(sample[0])\r\n            sample_y = np.array(sample[1])\r\n            \r\n            cost, _  = sess.run([ cost,idk],\r\n                                feed_dict= {\r\n                                    plc_x: sample_x, \r\n                                    plc_y: sample_y })\r\n            costs.append(cost)\r\n    return costs\r\n"}