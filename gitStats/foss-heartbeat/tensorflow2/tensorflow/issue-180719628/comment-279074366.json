{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/279074366", "html_url": "https://github.com/tensorflow/tensorflow/issues/4735#issuecomment-279074366", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4735", "id": 279074366, "node_id": "MDEyOklzc3VlQ29tbWVudDI3OTA3NDM2Ng==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-10T21:43:53Z", "updated_at": "2017-02-10T21:43:53Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>so you mean i can set this value to 12000, that is 12GB, as I am using Tian X GPU. if reduce the chance &gt; of out of memory error, i can set a lower value, like 11000 or 10000. is that correct?</p>\n</blockquote>\n<p>No. TF_CUDNN_WORKSPACE_LIMIT_IN_MB is how much to use for scratch space. This is temporary scratch space that is used by individual cudnn ops. If you set it to 12000, you will have no space in your GPU  to store tensors persistently or for TensorFlow to use as scratch space.</p>\n<p>You are at the cusp of running out of memory. There is likely memory fragmentation going on which means that as your process goes on you are unable to allocate temporary buffers anymore. TF_CUDNN_WORKSPACE_LIMIT_IN_MB reduces the scratch space which reduces the chance you will see an out of memory. There are several things you</p>\n<ol>\n<li>\n<p>Use  a smaller batch size. You said that worked with 128 but is twice as slow. What about 192?  Try using a binary search to choose the largest batch size.</p>\n</li>\n<li>\n<p>Restart the process periodically.  Checkpoint every step 10000 steps and stop the training process and  restart. This will reset the memory fragmentation.</p>\n</li>\n<li>\n<p>Improve the GPU block allocator to be more efficient. That memory allocator tries to avoid fragmentation. Insert instrumentation into it, try to measure the fragmentation and verify that is indeed your problem. Then, try to devise a better allocator algorithm that reduces fragmentation but doesn't reduce performance.</p>\n</li>\n</ol>\n<p>Obviously 1 and 2 are easy solutions and 3 is a long solution. Computers simply do not work well when pushed to the edge of memory utilization. Our memory allocators could be conservative and add a buffer and try to stop you at training step 0 before you get anywhere, but that would prevent some safe models from working and would not be guaranteed anyways. Memory allocation patterns are not deterministic in a parallel environment, unfortunately.</p>", "body_text": "so you mean i can set this value to 12000, that is 12GB, as I am using Tian X GPU. if reduce the chance > of out of memory error, i can set a lower value, like 11000 or 10000. is that correct?\n\nNo. TF_CUDNN_WORKSPACE_LIMIT_IN_MB is how much to use for scratch space. This is temporary scratch space that is used by individual cudnn ops. If you set it to 12000, you will have no space in your GPU  to store tensors persistently or for TensorFlow to use as scratch space.\nYou are at the cusp of running out of memory. There is likely memory fragmentation going on which means that as your process goes on you are unable to allocate temporary buffers anymore. TF_CUDNN_WORKSPACE_LIMIT_IN_MB reduces the scratch space which reduces the chance you will see an out of memory. There are several things you\n\n\nUse  a smaller batch size. You said that worked with 128 but is twice as slow. What about 192?  Try using a binary search to choose the largest batch size.\n\n\nRestart the process periodically.  Checkpoint every step 10000 steps and stop the training process and  restart. This will reset the memory fragmentation.\n\n\nImprove the GPU block allocator to be more efficient. That memory allocator tries to avoid fragmentation. Insert instrumentation into it, try to measure the fragmentation and verify that is indeed your problem. Then, try to devise a better allocator algorithm that reduces fragmentation but doesn't reduce performance.\n\n\nObviously 1 and 2 are easy solutions and 3 is a long solution. Computers simply do not work well when pushed to the edge of memory utilization. Our memory allocators could be conservative and add a buffer and try to stop you at training step 0 before you get anywhere, but that would prevent some safe models from working and would not be guaranteed anyways. Memory allocation patterns are not deterministic in a parallel environment, unfortunately.", "body": "> so you mean i can set this value to 12000, that is 12GB, as I am using Tian X GPU. if reduce the chance > of out of memory error, i can set a lower value, like 11000 or 10000. is that correct?\r\n\r\nNo. TF_CUDNN_WORKSPACE_LIMIT_IN_MB is how much to use for scratch space. This is temporary scratch space that is used by individual cudnn ops. If you set it to 12000, you will have no space in your GPU  to store tensors persistently or for TensorFlow to use as scratch space.\r\n\r\nYou are at the cusp of running out of memory. There is likely memory fragmentation going on which means that as your process goes on you are unable to allocate temporary buffers anymore. TF_CUDNN_WORKSPACE_LIMIT_IN_MB reduces the scratch space which reduces the chance you will see an out of memory. There are several things you \r\n\r\n1. Use  a smaller batch size. You said that worked with 128 but is twice as slow. What about 192?  Try using a binary search to choose the largest batch size.\r\n\r\n2. Restart the process periodically.  Checkpoint every step 10000 steps and stop the training process and  restart. This will reset the memory fragmentation.\r\n\r\n3. Improve the GPU block allocator to be more efficient. That memory allocator tries to avoid fragmentation. Insert instrumentation into it, try to measure the fragmentation and verify that is indeed your problem. Then, try to devise a better allocator algorithm that reduces fragmentation but doesn't reduce performance.\r\n\r\nObviously 1 and 2 are easy solutions and 3 is a long solution. Computers simply do not work well when pushed to the edge of memory utilization. Our memory allocators could be conservative and add a buffer and try to stop you at training step 0 before you get anywhere, but that would prevent some safe models from working and would not be guaranteed anyways. Memory allocation patterns are not deterministic in a parallel environment, unfortunately.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}