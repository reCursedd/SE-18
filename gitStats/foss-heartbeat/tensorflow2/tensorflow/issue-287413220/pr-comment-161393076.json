{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/161393076", "pull_request_review_id": 88678579, "id": 161393076, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTM5MzA3Ng==", "diff_hunk": "@@ -1029,244 +894,318 @@ void RdmaMessageBuffer::SendNextItem() {\n   }\n }\n \n-Rendezvous::DoneCallback RdmaTensorBuffer::getRecvTensorCallback(\n-    const string& key_with_step_id, const string& key, int64 step_id,\n-    const Rendezvous::ParsedKey& parsed) {\n-  Rendezvous::DoneCallback cb = [this, key_with_step_id, key, step_id, parsed](\n-      const Status& status, const Rendezvous::Args& send_args,\n-      const Rendezvous::Args& recv_args, const Tensor& in, bool is_dead) {\n-    CHECK(status.ok()) << \"RecvLocalAsync was not ok, key\" << key_with_step_id\n-                       << \" error message: \" << status.error_message();\n-    size_t buffer_size = RdmaMessage::kMessageTotalBytes;\n-    size_t tensor_bytes = 0;\n-    // Figures out which device the tensor is hosted on.\n-    Device* src_dev = nullptr;\n-    Status s = channel_->adapter_->worker_env_->device_mgr->LookupDevice(\n-        parsed.src_device, &src_dev);\n-    CHECK(s.ok()) << \"src device not found\";\n-    // Does the device have the right incarnation number we expect?\n-    CHECK(src_dev->attributes().incarnation() == parsed.src_incarnation)\n-        << \"RecvTensor expects a different device incarnation: \"\n-        << parsed.src_incarnation << \" vs. \"\n-        << src_dev->attributes().incarnation()\n-        << \". Your worker job was probably restarted. Check your \"\n-        << \"worker job for the reason why it was restarted.\";\n-    Device* dst_dev = nullptr;\n-    // destination is on CPU.\n-    s = channel_->adapter_->worker_env_->device_mgr->LookupDevice(\"CPU:0\",\n-                                                                  &dst_dev);\n-    CHECK(s.ok()) << \"dst device not found\";\n-    AllocatorAttributes dst_alloc_attr;\n-    dst_alloc_attr.set_on_host(true);\n-\n-    bool can_memcpy = DataTypeCanUseMemcpy(in.dtype());\n-    // string tensor needs to be serialized\n-    Tensor copy;\n-    TensorProto proto;\n-    if (src_dev->tensorflow_gpu_device_info() &&\n-        (!send_args.alloc_attrs.on_host())) {\n+static void CountCopies(const std::string& key, void* src_addr, void* dst_addr,\n+                        size_t tensor_bytes, bool is_gpu_to_cpu) {\n+#ifdef RDMA_COUNT_COPIES\n+  static uint64_t numGPUToCPUCopies = 0;\n+  static uint64_t numGPUToCPUCopiedBytes = 0;\n+  static uint64_t numCPUToGPUCopies = 0;\n+  static uint64_t numCPUToGPUCopiedBytes = 0;\n+  static uint64_t numTotalCopies = 0;\n+\n+  if (is_gpu_to_cpu) {\n+    ++numGPUToCPUCopies;\n+    numGPUToCPUCopiedBytes += tensor_bytes;\n+  } else {\n+    ++numCPUToGPUCopies;\n+    numCPUToGPUCopiedBytes += tensor_bytes;\n+  }\n+  if ((++numTotalCopies % 0x400) == 0) {\n+    RDMA_LOG(0) << \"Tensor copies:\"\n+                << \" GPU to CPU: \" << numGPUToCPUCopies\n+                << \" (\" << numGPUToCPUCopiedBytes << \" Bytes)\"\n+                << \" CPU to GPU: \" << numCPUToGPUCopies\n+                << \" (\" << numCPUToGPUCopiedBytes << \" Bytes)\";\n+  }\n+  RDMA_LOG(2) << \"Copying tensor \" << key\n+              << \" From: \" << src_addr << \" To: \" << dst_addr;\n+#endif\n+}\n+\n+static uint64_t Checksum(Device* device, const DeviceContext* device_context,\n+                         const Tensor& in) {\n+  uint64 checksum = 0;\n+  if (DataTypeCanUseMemcpy(in.dtype())) {\n #if GOOGLE_CUDA\n-      CHECK(send_args.device_context) << \"send dev name: \" << src_dev->name()\n-                                      << \" gpu_info: \"\n-                                      << src_dev->tensorflow_gpu_device_info();\n-\n-      if (can_memcpy) {\n-        AllocatorAttributes host_alloc_attrs;\n-        host_alloc_attrs.set_gpu_compatible(true);\n-        host_alloc_attrs.set_on_host(true);\n-        Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n-        copy = Tensor(alloc, in.dtype(), in.shape());\n-        tensor_bytes = in.TotalBytes();\n-        buffer_size += tensor_bytes;\n-        GPUUtil::CopyGPUTensorToCPU(\n-            src_dev, send_args.device_context, &in, &copy,\n-            [this, copy, tensor_bytes, buffer_size, key, in, step_id,\n-             key_with_step_id, is_dead, send_args, recv_args](const Status& s) {\n-              CHECK(s.ok()) << \"copy tensor from gpu sync\";\n-              StringPiece copy_buf;\n-              copy_buf = copy.tensor_data();\n-              PostCopyOperations(true, buffer_size, tensor_bytes, key, in,\n-                                 step_id, is_dead, key_with_step_id, &copy,\n-                                 NULL, &copy_buf, send_args, recv_args);\n-            });\n-      } else {\n-        // \"val\" is on a GPU. No longer uses GPUUtil to fill the proto, use\n-        // aync instead\n-        GPUUtil::SetProtoFromGPU(\n-            in, src_dev, send_args.device_context, &proto, is_dead,\n-\t    [this, proto, buffer_size, key, in, step_id, key_with_step_id,\n-            is_dead, send_args, recv_args](const Status& s) mutable {\n-              CHECK(s.ok()) << \"copy proto from gpu sync\";\n-              auto tensor_bytes = proto.ByteSize();\n-              buffer_size += tensor_bytes;\n-              PostCopyOperations(false, buffer_size, tensor_bytes, key, in,\n-                                 step_id, is_dead, key_with_step_id, NULL,\n-                                 &proto, NULL, send_args, recv_args);\n-            });\n-      }\n-#endif  // GOOGLE_CUDA\n-    } else {\n-      // tensor is in CPU memory.\n-      StringPiece copy_buf;\n-      if (can_memcpy) {\n-        copy_buf = in.tensor_data();\n-        tensor_bytes = in.TotalBytes();\n-      } else {\n-        in.AsProtoTensorContent(&proto);\n-        tensor_bytes = proto.ByteSize();\n-      }\n-      buffer_size += tensor_bytes;\n-      PostCopyOperations(can_memcpy, buffer_size, tensor_bytes, key, in,\n-                         step_id, is_dead, key_with_step_id, &copy, &proto,\n-                         &copy_buf, send_args, recv_args);\n+    if (in.TotalBytes() == 0) {\n+      return 0;\n     }\n-  };\n-  return cb;\n+    checksum = (device_context != nullptr)\n+                   ? GPUUtil::Checksum(device, device_context, in)\n+                   : GPUUtil::Checksum(in);\n+#endif\n+  } else {\n+    string s = in.SummarizeValue(999999);\n+    checksum = Hash64(s.c_str(), s.size(), 0);\n+  }\n+  return checksum;\n }\n \n-// Send the next tensor from the buffer's job queue.\n-void RdmaTensorBuffer::SendNextItem() {\n-  // get the key\n-  string key_with_step_id = \"\";\n-  {\n-    mutex_lock lock{mu_};\n-    if (!queue_.empty()) {\n-      key_with_step_id = queue_.front();\n-      queue_.pop();\n+static void ValidateChecksum(uint64_t expected, uint64_t actual,\n+                             const Tensor& in, uint32_t request_index,\n+                             const std::string& key, const std::string& msg) {\n+  RDMA_LOG(2) << \"Request #\" << request_index << \": \" << key\n+              << \": Checksum: \" << std::hex << \" Expected = 0x\" << expected\n+              << \". Actual = 0x\" << actual << \".\";\n+\n+  if (expected != actual) {\n+    // Checksum failed. There is one case where this is allowed - if the\n+    // tensor is an AssignAdd of the global step. Since the data-validation\n+    // always postpones the Tensor response in order to send a checksum message,\n+    // it is possible that the global-step was updated while the response was\n+    // still in queue.\n+    if ((in.TotalBytes() == 8) && (in.dtype() == DT_INT64)) {\n+      int64_t prev_val = *(int64_t*)DMAHelper::base(&in) - 1;\n+      actual = Hash64((const char*)&prev_val, 8, 0);\n+    }\n+    if (expected != actual) {\n+      LOG(FATAL) << \"[\" << msg << \"]: Checksum validation failed for request #\"\n+                 << request_index << \": \" << key << std::hex << \" \"\n+                 << DataTypeString(in.dtype()) << \" \"\n+                 << in.shape().DebugString() << \" (0x\" << in.TotalBytes()\n+                 << \" bytes): \"\n+                 << \" Expected 0x\" << expected << \". Got 0x\" << actual << \".\";\n     }\n   }\n+}\n \n-  // send the tensor if a key is acquired.\n-  if (key_with_step_id != \"\") {\n-    VLOG(2) << \"try to send tensor: \" << key_with_step_id;\n-    string key;\n-    int64 step_id;\n-    VerbsUtil::GetKeyAndStepId(key_with_step_id, key, step_id);\n-    CHECK(key.compare(name_) == 0);\n-    Rendezvous::ParsedKey parsed;\n-    Rendezvous::ParseKey(key, &parsed);\n-    Rendezvous::DoneCallback cb =\n-        getRecvTensorCallback(key_with_step_id, key, step_id, parsed);\n-    channel_->adapter_->worker_env_->rendezvous_mgr->RecvLocalAsync(step_id,\n-                                                                    parsed, cb);\n-  }\n+RdmaTensorResponse* RdmaChannel::AddTensorResponse(const RdmaMessage& rm) {\n+  mutex_lock lock{mu_};\n+  auto it =\n+      responses_table_.emplace(rm.request_index_, RdmaTensorResponse(this, rm));\n+  CHECK(it.second) << \"Response with the ID \" << rm.request_index_\n+                   << \" already exists.\";\n+  return &it.first->second;\n }\n \n-void RdmaTensorBuffer::ReSendNextItem() {\n-  // get the key\n-  string key_with_step_id = \"\";\n-  {\n-    mutex_lock lock{mu_};\n-    if (!requeue.empty()) {\n-      key_with_step_id = requeue.front();\n-      requeue.pop();\n+RdmaTensorResponse* RdmaChannel::UpdateTensorResponse(const RdmaMessage& rm) {\n+  mutex_lock lock{mu_};\n+  auto it = responses_table_.find(rm.request_index_);\n+  CHECK(it != responses_table_.end()) << \"No response found.\";\n+  RdmaTensorResponse* response = &it->second;\n+  response->Update(rm);\n+  return response;\n+}\n+\n+void RdmaChannel::RemoveTensorResponse(uint32_t request_index) {\n+  mutex_lock lock{mu_};\n+  responses_table_.erase(request_index);\n+}\n+\n+void RdmaTensorResponse::Start() {\n+  Rendezvous::ParsedKey parsed;\n+  Rendezvous::ParseKey(rm_.name_, &parsed);\n+  channel_->adapter_->worker_env_->rendezvous_mgr->RecvLocalAsync(\n+      rm_.step_id_, parsed,\n+      [this, parsed](const Status& status, const Rendezvous::Args& send_args,\n+                     const Rendezvous::Args& recv_args, const Tensor& in,\n+                     bool is_dead) {\n+        CHECK(status.ok()) << \"RecvLocalAsync was not ok.\"\n+                           << \" error message: \" << status.error_message();\n+        RecvHandler(parsed, send_args, recv_args, in, is_dead);\n+      });\n+}\n+\n+void RdmaTensorResponse::Resume() { SendContent(*tensor_, *proto_, is_dead_); }\n+\n+void RdmaTensorResponse::RecvHandler(Rendezvous::ParsedKey parsed,\n+                                     const Rendezvous::Args& send_args,\n+                                     const Rendezvous::Args& recv_args,\n+                                     const Tensor& in, bool is_dead) {\n+  // Figures out which device the tensor is hosted on.\n+  Device* src_dev = nullptr;\n+  Status s = channel_->adapter_->worker_env_->device_mgr->LookupDevice(\n+      parsed.src_device, &src_dev);\n+  CHECK(s.ok()) << \"src device not found\";\n+  // Does the device have the right incarnation number we expect?\n+  CHECK(src_dev->attributes().incarnation() == parsed.src_incarnation)\n+      << \"RecvTensor expects a different device incarnation: \"\n+      << parsed.src_incarnation << \" vs. \"\n+      << src_dev->attributes().incarnation()\n+      << \". Your worker job was probably restarted. Check your \"\n+      << \"worker job for the reason why it was restarted.\";\n+\n+#ifdef RDMA_DATA_VALIDATION\n+  checksum_ = Checksum(src_dev, send_args.device_context, in);\n+#endif\n+  bool can_memcpy = DataTypeCanUseMemcpy(in.dtype());\n+  // string tensor needs to be serialized\n+  Tensor copy;\n+  TensorProto proto;\n+  if (src_dev->tensorflow_gpu_device_info() &&\n+      (!send_args.alloc_attrs.on_host())) {\n+#if GOOGLE_CUDA\n+    CHECK(send_args.device_context) << \"send dev name: \" << src_dev->name()\n+                                    << \" gpu_info: \"\n+                                    << src_dev->tensorflow_gpu_device_info();\n+\n+    if (can_memcpy) {\n+      AllocatorAttributes host_alloc_attrs;\n+      host_alloc_attrs.set_gpu_compatible(true);\n+      host_alloc_attrs.set_on_host(true);\n+      Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n+      copy = Tensor(alloc, in.dtype(), in.shape());\n+\n+      CountCopies(rm_.name_, (void*)DMAHelper::base(&in),\n+                  (void*)DMAHelper::base(&copy), in.TotalBytes(), true);\n+\n+      GPUUtil::CopyGPUTensorToCPU(\n+          src_dev, send_args.device_context, &in, &copy,\n+          [this, copy, proto, is_dead](const Status& s) {\n+            CHECK(s.ok()) << \"copy tensor from gpu sync\";\n+            Send(copy, proto, is_dead);\n+          });\n+    } else {\n+      // \"val\" is on a GPU. No longer uses GPUUtil to fill the proto, use\n+      // aync instead\n+      GPUUtil::SetProtoFromGPU(\n+          in, src_dev, send_args.device_context, &proto, is_dead,\n+          [this, in, proto, is_dead](const Status& s) mutable {\n+            CHECK(s.ok()) << \"copy proto from gpu sync\";\n+            Send(in, proto, is_dead);\n+          });\n+    }\n+#endif  // GOOGLE_CUDA\n+  } else {\n+    // tensor is in CPU memory.\n+    if (!can_memcpy) {\n+      in.AsProtoTensorContent(&proto);\n     }\n+    Send(in, proto, is_dead);\n   }\n+}\n \n-  // send the tensor if a key is acquired.\n-  if (key_with_step_id != \"\") {\n-    VLOG(2) << \"try to send tensor: \" << key_with_step_id;\n-    string key;\n-    int64 step_id;\n-    VerbsUtil::GetKeyAndStepId(key_with_step_id, key, step_id);\n-    CHECK(key.compare(name_) == 0);\n-    Rendezvous::ParsedKey parsed;\n-    Rendezvous::ParseKey(key, &parsed);\n-    Rendezvous::DoneCallback cb =\n-        getRecvTensorCallback(key_with_step_id, key, step_id, parsed);\n-    ReItem* item;\n-    {\n-      mutex_lock lock{mu_};\n-      Itable it = retable.find(key_with_step_id);\n-      CHECK(it != retable.end()) << \"Could not find dup-recv context\";\n-      item = it->second;\n-      retable.erase(it);\n-    }\n-    cb(Status::OK(), item->send_args, item->recv_args, item->in, item->is_dead);\n-    delete (item);\n+void RdmaTensorResponse::Send(const Tensor& in, const TensorProto& proto,\n+                              bool is_dead) {\n+  bool can_memcpy = DataTypeCanUseMemcpy(in.dtype());\n+  size_t tensor_bytes = (can_memcpy) ? in.TotalBytes() : proto.ByteSize();\n+  bool meta_data_changed = TensorMetaDataChanged(in, is_dead, tensor_bytes);\n+#ifdef RDMA_DATA_VALIDATION\n+  // Always send meta-data\n+  meta_data_changed = rm_.type_ == RDMA_MESSAGE_TENSOR_REQUEST;\n+#endif\n+  if (meta_data_changed) {\n+    Clone(in, proto, is_dead);\n+    SendMetaData(in, proto, is_dead);\n+  } else {\n+    SendContent(in, proto, is_dead);\n   }\n }\n \n-void RdmaTensorBuffer::PostCopyOperations(\n-    bool can_memcpy, size_t buffer_size, size_t tensor_bytes, const string& key,\n-    const Tensor& in, int64 step_id, bool is_dead,\n-    const string& key_with_step_id, const Tensor* copy,\n-    const TensorProto* proto, const StringPiece* copy_buf,\n-    const Rendezvous::Args& send_args, const Rendezvous::Args& recv_args) {\n-  // prepare message\n+bool RdmaTensorResponse::TensorMetaDataChanged(const Tensor& in, bool is_dead,\n+                                               size_t tensor_bytes) {\n+  return (rm_.data_type_ != in.dtype()) || (rm_.tensor_shape_ != in.shape()) ||\n+         (rm_.is_dead_ != is_dead) || (rm_.tensor_bytes_ != tensor_bytes);\n+}\n+\n+void RdmaTensorResponse::Clone(const Tensor& in, const TensorProto& proto,\n+                               bool is_dead) {\n+  // Clone the data to be sent later. For simplicity, we clone the tensor's\n+  // data even if it is already a copy. Performance is less of a concern here\n+  // since the meta-data hardly ever changes. The reason we create a copy, is", "path": "tensorflow/contrib/verbs/rdma.cc", "position": 938, "original_position": 870, "commit_id": "c9189ae40f9860f77fe6a3741d287b049184d5f2", "original_commit_id": "f15b6492deda4f15a7c1eb47895b4adbc87bd1e6", "user": {"login": "eladweiss", "id": 31474666, "node_id": "MDQ6VXNlcjMxNDc0NjY2", "avatar_url": "https://avatars2.githubusercontent.com/u/31474666?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eladweiss", "html_url": "https://github.com/eladweiss", "followers_url": "https://api.github.com/users/eladweiss/followers", "following_url": "https://api.github.com/users/eladweiss/following{/other_user}", "gists_url": "https://api.github.com/users/eladweiss/gists{/gist_id}", "starred_url": "https://api.github.com/users/eladweiss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eladweiss/subscriptions", "organizations_url": "https://api.github.com/users/eladweiss/orgs", "repos_url": "https://api.github.com/users/eladweiss/repos", "events_url": "https://api.github.com/users/eladweiss/events{/privacy}", "received_events_url": "https://api.github.com/users/eladweiss/received_events", "type": "User", "site_admin": false}, "body": "@poxvoculi @shamoya @yanivbl6 Yes, we were also surprised. I mentioned this in the comment - assuming I'm not wrong, what I saw is that some tensors share their buffer between step-ids, and that the tensor's content changes even though its ref-count never goes to 0. This makes some sense if the tensor's buffer is not deallocated, but kept alive and shared between step-ids. We noticed that issue by getting checksum differences between source and destination tensors.\r\n\r\nIn order to investigate the issue I conducted a simple experiment - while still inside the callback to **RecvLocalAsync()** I would: \r\n1. calculate checksum. \r\n2. usleep some. \r\n3. calculate checksum again and compare it. Got differences here as well.\r\n\r\nFor that experiment I would get the same errors for the previous verbs implementation and  GRPC as well. \r\n\r\nhttps://stackoverflow.com/questions/47961352/how-does-tensorflow-sync-tensors-which-share-a-buffer-between-different-step-ids\r\nhttps://stackoverflow.com/questions/47980283/distributed-tensorflow-tensor-content-changes-while-sending\r\n\r\nAgain I can be wrong, but I did do double and triple check on this. It is also possible that I'm completely missing something out. Our team's work plan for the next few weeks does include further debugging this issue as well a opening an issue request about it.\r\n\r\nIf it is in fact an issue, saving a shallow copy of the tensor while still exchanging messages would make it surface, which is something we don't want. Saving a deep copy instead would make our solution resemble GRPC in that aspect. So it's a safer approach.\r\n\r\n\r\n\r\n\r\n\r\n\r\n", "created_at": "2018-01-14T10:23:31Z", "updated_at": "2018-01-17T14:03:09Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16005#discussion_r161393076", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16005", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/161393076"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16005#discussion_r161393076"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16005"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22274255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shamoya\">@shamoya</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=24679884\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yanivbl6\">@yanivbl6</a> Yes, we were also surprised. I mentioned this in the comment - assuming I'm not wrong, what I saw is that some tensors share their buffer between step-ids, and that the tensor's content changes even though its ref-count never goes to 0. This makes some sense if the tensor's buffer is not deallocated, but kept alive and shared between step-ids. We noticed that issue by getting checksum differences between source and destination tensors.</p>\n<p>In order to investigate the issue I conducted a simple experiment - while still inside the callback to <strong>RecvLocalAsync()</strong> I would:</p>\n<ol>\n<li>calculate checksum.</li>\n<li>usleep some.</li>\n<li>calculate checksum again and compare it. Got differences here as well.</li>\n</ol>\n<p>For that experiment I would get the same errors for the previous verbs implementation and  GRPC as well.</p>\n<p><a href=\"https://stackoverflow.com/questions/47961352/how-does-tensorflow-sync-tensors-which-share-a-buffer-between-different-step-ids\" rel=\"nofollow\">https://stackoverflow.com/questions/47961352/how-does-tensorflow-sync-tensors-which-share-a-buffer-between-different-step-ids</a><br>\n<a href=\"https://stackoverflow.com/questions/47980283/distributed-tensorflow-tensor-content-changes-while-sending\" rel=\"nofollow\">https://stackoverflow.com/questions/47980283/distributed-tensorflow-tensor-content-changes-while-sending</a></p>\n<p>Again I can be wrong, but I did do double and triple check on this. It is also possible that I'm completely missing something out. Our team's work plan for the next few weeks does include further debugging this issue as well a opening an issue request about it.</p>\n<p>If it is in fact an issue, saving a shallow copy of the tensor while still exchanging messages would make it surface, which is something we don't want. Saving a deep copy instead would make our solution resemble GRPC in that aspect. So it's a safer approach.</p>", "body_text": "@poxvoculi @shamoya @yanivbl6 Yes, we were also surprised. I mentioned this in the comment - assuming I'm not wrong, what I saw is that some tensors share their buffer between step-ids, and that the tensor's content changes even though its ref-count never goes to 0. This makes some sense if the tensor's buffer is not deallocated, but kept alive and shared between step-ids. We noticed that issue by getting checksum differences between source and destination tensors.\nIn order to investigate the issue I conducted a simple experiment - while still inside the callback to RecvLocalAsync() I would:\n\ncalculate checksum.\nusleep some.\ncalculate checksum again and compare it. Got differences here as well.\n\nFor that experiment I would get the same errors for the previous verbs implementation and  GRPC as well.\nhttps://stackoverflow.com/questions/47961352/how-does-tensorflow-sync-tensors-which-share-a-buffer-between-different-step-ids\nhttps://stackoverflow.com/questions/47980283/distributed-tensorflow-tensor-content-changes-while-sending\nAgain I can be wrong, but I did do double and triple check on this. It is also possible that I'm completely missing something out. Our team's work plan for the next few weeks does include further debugging this issue as well a opening an issue request about it.\nIf it is in fact an issue, saving a shallow copy of the tensor while still exchanging messages would make it surface, which is something we don't want. Saving a deep copy instead would make our solution resemble GRPC in that aspect. So it's a safer approach.", "in_reply_to_id": 161313327}