{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/161310493", "pull_request_review_id": 88571426, "id": 161310493, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTMxMDQ5Mw==", "diff_hunk": "@@ -1029,244 +894,318 @@ void RdmaMessageBuffer::SendNextItem() {\n   }\n }\n \n-Rendezvous::DoneCallback RdmaTensorBuffer::getRecvTensorCallback(\n-    const string& key_with_step_id, const string& key, int64 step_id,\n-    const Rendezvous::ParsedKey& parsed) {\n-  Rendezvous::DoneCallback cb = [this, key_with_step_id, key, step_id, parsed](\n-      const Status& status, const Rendezvous::Args& send_args,\n-      const Rendezvous::Args& recv_args, const Tensor& in, bool is_dead) {\n-    CHECK(status.ok()) << \"RecvLocalAsync was not ok, key\" << key_with_step_id\n-                       << \" error message: \" << status.error_message();\n-    size_t buffer_size = RdmaMessage::kMessageTotalBytes;\n-    size_t tensor_bytes = 0;\n-    // Figures out which device the tensor is hosted on.\n-    Device* src_dev = nullptr;\n-    Status s = channel_->adapter_->worker_env_->device_mgr->LookupDevice(\n-        parsed.src_device, &src_dev);\n-    CHECK(s.ok()) << \"src device not found\";\n-    // Does the device have the right incarnation number we expect?\n-    CHECK(src_dev->attributes().incarnation() == parsed.src_incarnation)\n-        << \"RecvTensor expects a different device incarnation: \"\n-        << parsed.src_incarnation << \" vs. \"\n-        << src_dev->attributes().incarnation()\n-        << \". Your worker job was probably restarted. Check your \"\n-        << \"worker job for the reason why it was restarted.\";\n-    Device* dst_dev = nullptr;\n-    // destination is on CPU.\n-    s = channel_->adapter_->worker_env_->device_mgr->LookupDevice(\"CPU:0\",\n-                                                                  &dst_dev);\n-    CHECK(s.ok()) << \"dst device not found\";\n-    AllocatorAttributes dst_alloc_attr;\n-    dst_alloc_attr.set_on_host(true);\n-\n-    bool can_memcpy = DataTypeCanUseMemcpy(in.dtype());\n-    // string tensor needs to be serialized\n-    Tensor copy;\n-    TensorProto proto;\n-    if (src_dev->tensorflow_gpu_device_info() &&\n-        (!send_args.alloc_attrs.on_host())) {\n+static void CountCopies(const std::string& key, void* src_addr, void* dst_addr,\n+                        size_t tensor_bytes, bool is_gpu_to_cpu) {\n+#ifdef RDMA_COUNT_COPIES\n+  static uint64_t numGPUToCPUCopies = 0;\n+  static uint64_t numGPUToCPUCopiedBytes = 0;\n+  static uint64_t numCPUToGPUCopies = 0;\n+  static uint64_t numCPUToGPUCopiedBytes = 0;\n+  static uint64_t numTotalCopies = 0;\n+\n+  if (is_gpu_to_cpu) {\n+    ++numGPUToCPUCopies;\n+    numGPUToCPUCopiedBytes += tensor_bytes;\n+  } else {\n+    ++numCPUToGPUCopies;\n+    numCPUToGPUCopiedBytes += tensor_bytes;\n+  }\n+  if ((++numTotalCopies % 0x400) == 0) {\n+    RDMA_LOG(0) << \"Tensor copies:\"\n+                << \" GPU to CPU: \" << numGPUToCPUCopies\n+                << \" (\" << numGPUToCPUCopiedBytes << \" Bytes)\"\n+                << \" CPU to GPU: \" << numCPUToGPUCopies\n+                << \" (\" << numCPUToGPUCopiedBytes << \" Bytes)\";\n+  }\n+  RDMA_LOG(2) << \"Copying tensor \" << key\n+              << \" From: \" << src_addr << \" To: \" << dst_addr;\n+#endif\n+}\n+\n+static uint64_t Checksum(Device* device, const DeviceContext* device_context,\n+                         const Tensor& in) {\n+  uint64 checksum = 0;\n+  if (DataTypeCanUseMemcpy(in.dtype())) {\n #if GOOGLE_CUDA\n-      CHECK(send_args.device_context) << \"send dev name: \" << src_dev->name()\n-                                      << \" gpu_info: \"\n-                                      << src_dev->tensorflow_gpu_device_info();\n-\n-      if (can_memcpy) {\n-        AllocatorAttributes host_alloc_attrs;\n-        host_alloc_attrs.set_gpu_compatible(true);\n-        host_alloc_attrs.set_on_host(true);\n-        Allocator* alloc = ProcessState::singleton()->GetCUDAHostAllocator(0);\n-        copy = Tensor(alloc, in.dtype(), in.shape());\n-        tensor_bytes = in.TotalBytes();\n-        buffer_size += tensor_bytes;\n-        GPUUtil::CopyGPUTensorToCPU(\n-            src_dev, send_args.device_context, &in, &copy,\n-            [this, copy, tensor_bytes, buffer_size, key, in, step_id,\n-             key_with_step_id, is_dead, send_args, recv_args](const Status& s) {\n-              CHECK(s.ok()) << \"copy tensor from gpu sync\";\n-              StringPiece copy_buf;\n-              copy_buf = copy.tensor_data();\n-              PostCopyOperations(true, buffer_size, tensor_bytes, key, in,\n-                                 step_id, is_dead, key_with_step_id, &copy,\n-                                 NULL, &copy_buf, send_args, recv_args);\n-            });\n-      } else {\n-        // \"val\" is on a GPU. No longer uses GPUUtil to fill the proto, use\n-        // aync instead\n-        GPUUtil::SetProtoFromGPU(\n-            in, src_dev, send_args.device_context, &proto, is_dead,\n-\t    [this, proto, buffer_size, key, in, step_id, key_with_step_id,\n-            is_dead, send_args, recv_args](const Status& s) mutable {\n-              CHECK(s.ok()) << \"copy proto from gpu sync\";\n-              auto tensor_bytes = proto.ByteSize();\n-              buffer_size += tensor_bytes;\n-              PostCopyOperations(false, buffer_size, tensor_bytes, key, in,\n-                                 step_id, is_dead, key_with_step_id, NULL,\n-                                 &proto, NULL, send_args, recv_args);\n-            });\n-      }\n-#endif  // GOOGLE_CUDA\n-    } else {\n-      // tensor is in CPU memory.\n-      StringPiece copy_buf;\n-      if (can_memcpy) {\n-        copy_buf = in.tensor_data();\n-        tensor_bytes = in.TotalBytes();\n-      } else {\n-        in.AsProtoTensorContent(&proto);\n-        tensor_bytes = proto.ByteSize();\n-      }\n-      buffer_size += tensor_bytes;\n-      PostCopyOperations(can_memcpy, buffer_size, tensor_bytes, key, in,\n-                         step_id, is_dead, key_with_step_id, &copy, &proto,\n-                         &copy_buf, send_args, recv_args);\n+    if (in.TotalBytes() == 0) {\n+      return 0;\n     }\n-  };\n-  return cb;\n+    checksum = (device_context != nullptr)\n+                   ? GPUUtil::Checksum(device, device_context, in)\n+                   : GPUUtil::Checksum(in);\n+#endif\n+  } else {\n+    string s = in.SummarizeValue(999999);\n+    checksum = Hash64(s.c_str(), s.size(), 0);\n+  }\n+  return checksum;\n }\n \n-// Send the next tensor from the buffer's job queue.\n-void RdmaTensorBuffer::SendNextItem() {\n-  // get the key\n-  string key_with_step_id = \"\";\n-  {\n-    mutex_lock lock{mu_};\n-    if (!queue_.empty()) {\n-      key_with_step_id = queue_.front();\n-      queue_.pop();\n+static void ValidateChecksum(uint64_t expected, uint64_t actual,\n+                             const Tensor& in, uint32_t request_index,\n+                             const std::string& key, const std::string& msg) {\n+  RDMA_LOG(2) << \"Request #\" << request_index << \": \" << key\n+              << \": Checksum: \" << std::hex << \" Expected = 0x\" << expected\n+              << \". Actual = 0x\" << actual << \".\";\n+\n+  if (expected != actual) {\n+    // Checksum failed. There is one case where this is allowed - if the\n+    // tensor is an AssignAdd of the global step. Since the data-validation\n+    // always postpones the Tensor response in order to send a checksum message,\n+    // it is possible that the global-step was updated while the response was\n+    // still in queue.\n+    if ((in.TotalBytes() == 8) && (in.dtype() == DT_INT64)) {\n+      int64_t prev_val = *(int64_t*)DMAHelper::base(&in) - 1;\n+      actual = Hash64((const char*)&prev_val, 8, 0);\n+    }\n+    if (expected != actual) {\n+      LOG(FATAL) << \"[\" << msg << \"]: Checksum validation failed for request #\"\n+                 << request_index << \": \" << key << std::hex << \" \"\n+                 << DataTypeString(in.dtype()) << \" \"\n+                 << in.shape().DebugString() << \" (0x\" << in.TotalBytes()\n+                 << \" bytes): \"\n+                 << \" Expected 0x\" << expected << \". Got 0x\" << actual << \".\";\n     }\n   }\n+}\n \n-  // send the tensor if a key is acquired.\n-  if (key_with_step_id != \"\") {\n-    VLOG(2) << \"try to send tensor: \" << key_with_step_id;\n-    string key;\n-    int64 step_id;\n-    VerbsUtil::GetKeyAndStepId(key_with_step_id, key, step_id);\n-    CHECK(key.compare(name_) == 0);\n-    Rendezvous::ParsedKey parsed;\n-    Rendezvous::ParseKey(key, &parsed);\n-    Rendezvous::DoneCallback cb =\n-        getRecvTensorCallback(key_with_step_id, key, step_id, parsed);\n-    channel_->adapter_->worker_env_->rendezvous_mgr->RecvLocalAsync(step_id,\n-                                                                    parsed, cb);\n-  }\n+RdmaTensorResponse* RdmaChannel::AddTensorResponse(const RdmaMessage& rm) {\n+  mutex_lock lock{mu_};\n+  auto it =\n+      responses_table_.emplace(rm.request_index_, RdmaTensorResponse(this, rm));\n+  CHECK(it.second) << \"Response with the ID \" << rm.request_index_\n+                   << \" already exists.\";\n+  return &it.first->second;\n }\n \n-void RdmaTensorBuffer::ReSendNextItem() {\n-  // get the key\n-  string key_with_step_id = \"\";\n-  {\n-    mutex_lock lock{mu_};\n-    if (!requeue.empty()) {\n-      key_with_step_id = requeue.front();\n-      requeue.pop();\n+RdmaTensorResponse* RdmaChannel::UpdateTensorResponse(const RdmaMessage& rm) {\n+  mutex_lock lock{mu_};\n+  auto it = responses_table_.find(rm.request_index_);\n+  CHECK(it != responses_table_.end()) << \"No response found.\";\n+  RdmaTensorResponse* response = &it->second;\n+  response->Update(rm);\n+  return response;\n+}\n+\n+void RdmaChannel::RemoveTensorResponse(uint32_t request_index) {\n+  mutex_lock lock{mu_};\n+  responses_table_.erase(request_index);\n+}\n+\n+void RdmaTensorResponse::Start() {\n+  Rendezvous::ParsedKey parsed;\n+  Rendezvous::ParseKey(rm_.name_, &parsed);\n+  channel_->adapter_->worker_env_->rendezvous_mgr->RecvLocalAsync(\n+      rm_.step_id_, parsed,\n+      [this, parsed](const Status& status, const Rendezvous::Args& send_args,\n+                     const Rendezvous::Args& recv_args, const Tensor& in,\n+                     bool is_dead) {\n+        CHECK(status.ok()) << \"RecvLocalAsync was not ok.\"\n+                           << \" error message: \" << status.error_message();\n+        RecvHandler(parsed, send_args, recv_args, in, is_dead);\n+      });\n+}\n+\n+void RdmaTensorResponse::Resume() { SendContent(*tensor_, *proto_, is_dead_); }\n+\n+void RdmaTensorResponse::RecvHandler(Rendezvous::ParsedKey parsed,\n+                                     const Rendezvous::Args& send_args,\n+                                     const Rendezvous::Args& recv_args,\n+                                     const Tensor& in, bool is_dead) {\n+  // Figures out which device the tensor is hosted on.\n+  Device* src_dev = nullptr;\n+  Status s = channel_->adapter_->worker_env_->device_mgr->LookupDevice(\n+      parsed.src_device, &src_dev);\n+  CHECK(s.ok()) << \"src device not found\";\n+  // Does the device have the right incarnation number we expect?\n+  CHECK(src_dev->attributes().incarnation() == parsed.src_incarnation)\n+      << \"RecvTensor expects a different device incarnation: \"\n+      << parsed.src_incarnation << \" vs. \"\n+      << src_dev->attributes().incarnation()\n+      << \". Your worker job was probably restarted. Check your \"\n+      << \"worker job for the reason why it was restarted.\";\n+\n+#ifdef RDMA_DATA_VALIDATION\n+  checksum_ = Checksum(src_dev, send_args.device_context, in);\n+#endif\n+  bool can_memcpy = DataTypeCanUseMemcpy(in.dtype());\n+  // string tensor needs to be serialized\n+  Tensor copy;\n+  TensorProto proto;\n+  if (src_dev->tensorflow_gpu_device_info() &&\n+      (!send_args.alloc_attrs.on_host())) {\n+#if GOOGLE_CUDA\n+    CHECK(send_args.device_context) << \"send dev name: \" << src_dev->name()\n+                                    << \" gpu_info: \"\n+                                    << src_dev->tensorflow_gpu_device_info();\n+\n+    if (can_memcpy) {\n+      AllocatorAttributes host_alloc_attrs;", "path": "tensorflow/contrib/verbs/rdma.cc", "position": null, "original_position": 780, "commit_id": "c9189ae40f9860f77fe6a3741d287b049184d5f2", "original_commit_id": "f15b6492deda4f15a7c1eb47895b4adbc87bd1e6", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "Looks like host_alloc_attrs isn't used, so it should be deleted.  The conventional way to get the correct allocator for a tensor is to construct an AllocatorAttributes object then pass it to DeviceBase::GetAllocator() on the device where the tensor is to be used.   In this case it seems ok to call GetCUDAHostAllocator() directly.\r\n\r\nIn the Google internal environment instead of calling GetCUDAHostAllocator() directly in a circumstance like this we call an additional method GetAllocator(const AllocatorAttributes& aa) on an extended ProcessState interface.  The attribute bit gpu_compatible means that memory should come from a pool pre-registered for GPU DMA, and the attribute nic_compatible means that memory should come from a pool pre-registered for NIC DMA.   In practice, when using RDMA we register one pool for both purposes. It hasn't seemed feasible to open-source that part yet, but maybe if this library evolves in a compatible direction some common code can eventually go into open-source core.", "created_at": "2018-01-12T19:46:52Z", "updated_at": "2018-01-17T14:03:09Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16005#discussion_r161310493", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16005", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/161310493"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16005#discussion_r161310493"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16005"}}, "body_html": "<p>Looks like host_alloc_attrs isn't used, so it should be deleted.  The conventional way to get the correct allocator for a tensor is to construct an AllocatorAttributes object then pass it to DeviceBase::GetAllocator() on the device where the tensor is to be used.   In this case it seems ok to call GetCUDAHostAllocator() directly.</p>\n<p>In the Google internal environment instead of calling GetCUDAHostAllocator() directly in a circumstance like this we call an additional method GetAllocator(const AllocatorAttributes&amp; aa) on an extended ProcessState interface.  The attribute bit gpu_compatible means that memory should come from a pool pre-registered for GPU DMA, and the attribute nic_compatible means that memory should come from a pool pre-registered for NIC DMA.   In practice, when using RDMA we register one pool for both purposes. It hasn't seemed feasible to open-source that part yet, but maybe if this library evolves in a compatible direction some common code can eventually go into open-source core.</p>", "body_text": "Looks like host_alloc_attrs isn't used, so it should be deleted.  The conventional way to get the correct allocator for a tensor is to construct an AllocatorAttributes object then pass it to DeviceBase::GetAllocator() on the device where the tensor is to be used.   In this case it seems ok to call GetCUDAHostAllocator() directly.\nIn the Google internal environment instead of calling GetCUDAHostAllocator() directly in a circumstance like this we call an additional method GetAllocator(const AllocatorAttributes& aa) on an extended ProcessState interface.  The attribute bit gpu_compatible means that memory should come from a pool pre-registered for GPU DMA, and the attribute nic_compatible means that memory should come from a pool pre-registered for NIC DMA.   In practice, when using RDMA we register one pool for both purposes. It hasn't seemed feasible to open-source that part yet, but maybe if this library evolves in a compatible direction some common code can eventually go into open-source core."}