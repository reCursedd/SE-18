{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/161299373", "pull_request_review_id": 88571426, "id": 161299373, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MTI5OTM3Mw==", "diff_hunk": "@@ -468,97 +453,72 @@ void RdmaAdapter::Process_CQ() {\n         rc->Recv();\n         // imm_data is the index of RX buffer in the buffer table.\n         uint32_t imm_data = wc_[i].imm_data;\n-        RdmaBuffer* rb = rc->FindBuffer(imm_data);\n+        RdmaMessageBuffer* rb;\n         RdmaMessage rm;\n-        RdmaMessage::ParseMessage(rm, rb->buffer_);\n-        VLOG(2) << \"recv RDMA message: \" << MessageTypeToString(rm.type_);\n \n-        if (rm.type_ == RDMA_MESSAGE_ACK) {\n+        if (imm_data == RDMA_IMM_DATA_ACK) {\n           // receive an ack to a message\n           rb = rc->tx_message_buffer_;\n           rb->SetBufferStatus(remote, idle);\n           rb->SendNextItem();\n-        } else if (rm.type_ == RDMA_MESSAGE_TENSOR_REQUEST) {\n-          // received a request-for-tensor message\n-          // send ack to release remote tx message buffer\n-          RdmaBuffer* ab = rc->tx_ack_buffer_;\n-          ab->SendNextItem();\n-          // find or create buffer\n-          RdmaBuffer* tb = rc->FindOrCreateBuffer(rm.name_);\n-          string key_with_step_id =\n-              VerbsUtil::AppendStepidToKey(rm.name_, rm.step_id_);\n-          tb->EnqueueItem(key_with_step_id);\n-          // send the next tensor\n-          worker_env_->compute_pool->Schedule([tb]() { tb->SendNextItem(); });\n-        } else if (rm.type_ == RDMA_MESSAGE_BUFFER_IDLE) {\n-          // receive tensor-buffer-ready message\n-          // send ack to release remote tx message buffer\n-          RdmaBuffer* ab = rc->tx_ack_buffer_;\n-          ab->SendNextItem();\n-          // find buffer\n-          RdmaTensorBuffer* tb =\n-              reinterpret_cast<RdmaTensorBuffer*>(rc->FindBuffer(rm.name_));\n-          tb->SetBufferStatus(remote, idle);\n-          worker_env_->compute_pool->Schedule([tb]() { tb->ReSendNextItem(); });\n-        } else if (rm.type_ == RDMA_MESSAGE_BUFFER_REQUEST) {\n-          // remote host requests to create a tensor buffer;\n-          // send ack to release remote tx message buffer\n-          RdmaBuffer* ab = rc->tx_ack_buffer_;\n-          ab->SendNextItem();\n-          // find or create the buffer\n-          RdmaBuffer* tb = rc->FindOrCreateBuffer(rm.name_, TENSOR);\n-          RemoteMR rmr;\n-          rmr.remote_addr = rm.remote_addr_;\n-          rmr.rkey = rm.rkey_;\n-          tb->SetRemoteMR(rmr, true);\n-          tb->CreateCPUBuffer(rm.buffer_size_);\n-          // create RDMA_MESSAGE_BUFFER_RESPONSE message\n-          RdmaMessage br;\n-          br.type_ = RDMA_MESSAGE_BUFFER_RESPONSE;\n-          br.name_size_ = rm.name_.size();\n-          br.name_ = rm.name_;\n-          br.buffer_size_ = rm.buffer_size_;\n-          br.remote_addr_ = reinterpret_cast<uint64_t>(tb->buffer_);\n-          br.rkey_ = tb->self_->rkey;\n-          string message = RdmaMessage::CreateMessage(br);\n-          RdmaBuffer* mb = rc->tx_message_buffer_;\n-          mb->EnqueueItem(message);\n-          mb->SendNextItem();\n-        } else if (rm.type_ == RDMA_MESSAGE_BUFFER_RESPONSE) {\n-          // remote creates a buffer and responds\n-          // send ack to release remote tx message buffer\n-          RdmaBuffer* ab = rc->tx_ack_buffer_;\n-          ab->SendNextItem();\n-          // find buffer\n-          RdmaTensorBuffer* tb =\n-              reinterpret_cast<RdmaTensorBuffer*>(rc->FindBuffer(rm.name_));\n-          CHECK(rm.buffer_size_ == tb->size_)\n-              << \"rm.buffer_size = \" << rm.buffer_size_\n-              << \"tb->size_ = \" << tb->size_ << \"rm.name_ = \" << rm.name_;\n-          RemoteMR rmr;\n-          rmr.remote_addr = rm.remote_addr_;\n-          rmr.rkey = rm.rkey_;\n-          tb->SetRemoteMR(rmr, true);\n-          tb->SetBufferStatus(local, idle);\n-          tb->SetBufferStatus(remote, idle);\n-          worker_env_->compute_pool->Schedule([tb]() { tb->ReSendNextItem(); });\n-        } else if (rm.type_ == RDMA_MESSAGE_TENSOR_WRITE) {\n-          // tensor RDMA write completed\n-          worker_env_->compute_pool->Schedule([rm, rc]() {\n-            string key_with_step_id =\n-                VerbsUtil::AppendStepidToKey(rm.name_, rm.step_id_);\n-            rc->RunRecvCallback(key_with_step_id);\n+          continue;\n+        }\n+\n+        if (imm_data < RDMA_IMM_DATA_ACK) {\n+          // receive a tensor RDMA write\n+          worker_env_->compute_pool->Schedule([imm_data, rc]() {", "path": "tensorflow/contrib/verbs/rdma.cc", "position": null, "original_position": 139, "commit_id": "c9189ae40f9860f77fe6a3741d287b049184d5f2", "original_commit_id": "f15b6492deda4f15a7c1eb47895b4adbc87bd1e6", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "Looks like this issue got past me on the initial submit of this utility.  Although it's not well documented, WorkerEnv::compute_pool is for non-blocking functions executing as part of a locally parallel Op.  For potentially blocking functions use SchedClosure() instead.   The former is a small fixed size pool, the later is a dynamically expandable pool.", "created_at": "2018-01-12T18:54:29Z", "updated_at": "2018-01-17T14:03:09Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/16005#discussion_r161299373", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16005", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/161299373"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/16005#discussion_r161299373"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/16005"}}, "body_html": "<p>Looks like this issue got past me on the initial submit of this utility.  Although it's not well documented, WorkerEnv::compute_pool is for non-blocking functions executing as part of a locally parallel Op.  For potentially blocking functions use SchedClosure() instead.   The former is a small fixed size pool, the later is a dynamically expandable pool.</p>", "body_text": "Looks like this issue got past me on the initial submit of this utility.  Although it's not well documented, WorkerEnv::compute_pool is for non-blocking functions executing as part of a locally parallel Op.  For potentially blocking functions use SchedClosure() instead.   The former is a small fixed size pool, the later is a dynamically expandable pool."}