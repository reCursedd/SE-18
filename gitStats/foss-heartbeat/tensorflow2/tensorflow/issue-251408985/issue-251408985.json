{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12410", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12410/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12410/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12410/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12410", "id": 251408985, "node_id": "MDU6SXNzdWUyNTE0MDg5ODU=", "number": 12410, "title": "Distributed Tensorflow: data feeding in the beginning", "user": {"login": "kinsumliu", "id": 8632201, "node_id": "MDQ6VXNlcjg2MzIyMDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8632201?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kinsumliu", "html_url": "https://github.com/kinsumliu", "followers_url": "https://api.github.com/users/kinsumliu/followers", "following_url": "https://api.github.com/users/kinsumliu/following{/other_user}", "gists_url": "https://api.github.com/users/kinsumliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/kinsumliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kinsumliu/subscriptions", "organizations_url": "https://api.github.com/users/kinsumliu/orgs", "repos_url": "https://api.github.com/users/kinsumliu/repos", "events_url": "https://api.github.com/users/kinsumliu/events{/privacy}", "received_events_url": "https://api.github.com/users/kinsumliu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-08-19T07:57:34Z", "updated_at": "2017-12-20T16:13:33Z", "closed_at": "2017-12-20T16:13:24Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1.31</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am using distributed tensorflow. I have a huge parameter matrix partitioned across multiple parameter servers. In the beginning of training, only some workers will have the following exception. If the failed workers are restarted (possibly need multiple restarts), then the workers can go through and continue the training.</p>\n<p>I have checked the hdfs paths and files. They all look good. It seems the problem is that the failed workers cannot get the parameters from some ps (in the last line, see embedding_lookup/DynamicPartition_S751)</p>\n<h3>Source code / logs</h3>\n<p>Exception thrown:<br>\nCaused by op u'ReaderReadV2', defined at:<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 174, in _run_module_as_main<br>\n\"<strong>main</strong>\", fname, loader, pkg_name)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 72, in _run_code<br>\nexec code in run_globals<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 180, in <br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 157, in manager<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 61, in worker<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 174, in main<br>\nprocess()<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 169, in process<br>\nserializer.dunits processed is zero<br>\nump_stream(func(split_index, iterator), outfile)<br>\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func<br>\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func<br>\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func<br>\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 345, in func<br>\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 793, in func<br>\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 240, in <em>mapfn<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/<strong>pyfiles</strong>/my_distributed_learning.py\", line 105, in map_fun<br>\n((x, y</em>), units_completed, records_prodcued, sg_key) = read_csv_examples(skipgrams, None, batch_size, num_epochs, task_index, num_workers)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/<strong>pyfiles</strong>/my_distributed_learning.py\", line 56, in read_csv_examples<br>\nsg_key, sg_csv = sg_reader.read(sg_queue)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read<br>\nreturn gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 422, in _reader_read_v2<br>\nqueue_handle=queue_handle, name=name)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op<br>\nop_def=op_def)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2359, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1242, in <strong>init</strong><br>\nself._traceback = _extract_stack()</p>\n<p>OutOfRangeError (see above for traceback): FIFOQueue '_1_sg_queue' is closed and has insufficient elements (requested 1, current size 0)<br>\n[[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:worker/replica:0/task:1/cpu:0\"](sg_reader, sg_queue)]]<br>\n[[Node: embedding_lookup/DynamicPartition_S751 = _Recv <a href=\"\">client_terminated=false, recv_device=\"/job:ps/replica:0/task:6/cpu:0\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=6148508285306453080, tensor_name=\"edge_726_embedding_lookup/DynamicPartition\", tensor_type=DT_INT32, _device=\"/job:ps/replica:0/task:6/cpu:0\"</a>]]</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.1.31\nPython version: 2.7\nBazel version (if compiling from source):\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:\n\nDescribe the problem\nI am using distributed tensorflow. I have a huge parameter matrix partitioned across multiple parameter servers. In the beginning of training, only some workers will have the following exception. If the failed workers are restarted (possibly need multiple restarts), then the workers can go through and continue the training.\nI have checked the hdfs paths and files. They all look good. It seems the problem is that the failed workers cannot get the parameters from some ps (in the last line, see embedding_lookup/DynamicPartition_S751)\nSource code / logs\nException thrown:\nCaused by op u'ReaderReadV2', defined at:\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n\"main\", fname, loader, pkg_name)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 72, in _run_code\nexec code in run_globals\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 180, in \nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 157, in manager\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 61, in worker\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 174, in main\nprocess()\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 169, in process\nserializer.dunits processed is zero\nump_stream(func(split_index, iterator), outfile)\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 345, in func\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 793, in func\nFile \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 240, in mapfn\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyfiles/my_distributed_learning.py\", line 105, in map_fun\n((x, y), units_completed, records_prodcued, sg_key) = read_csv_examples(skipgrams, None, batch_size, num_epochs, task_index, num_workers)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyfiles/my_distributed_learning.py\", line 56, in read_csv_examples\nsg_key, sg_csv = sg_reader.read(sg_queue)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\nreturn gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 422, in _reader_read_v2\nqueue_handle=queue_handle, name=name)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\nop_def=op_def)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2359, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1242, in init\nself._traceback = _extract_stack()\nOutOfRangeError (see above for traceback): FIFOQueue '_1_sg_queue' is closed and has insufficient elements (requested 1, current size 0)\n[[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:worker/replica:0/task:1/cpu:0\"](sg_reader, sg_queue)]]\n[[Node: embedding_lookup/DynamicPartition_S751 = _Recv client_terminated=false, recv_device=\"/job:ps/replica:0/task:6/cpu:0\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=6148508285306453080, tensor_name=\"edge_726_embedding_lookup/DynamicPartition\", tensor_type=DT_INT32, _device=\"/job:ps/replica:0/task:6/cpu:0\"]]", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.1.31\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI am using distributed tensorflow. I have a huge parameter matrix partitioned across multiple parameter servers. In the beginning of training, only some workers will have the following exception. If the failed workers are restarted (possibly need multiple restarts), then the workers can go through and continue the training.\r\n\r\nI have checked the hdfs paths and files. They all look good. It seems the problem is that the failed workers cannot get the parameters from some ps (in the last line, see embedding_lookup/DynamicPartition_S751)\r\n\r\n### Source code / logs\r\n\r\nException thrown:\r\nCaused by op u'ReaderReadV2', defined at:\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\r\n    \"__main__\", fname, loader, pkg_name)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/runpy.py\", line 72, in _run_code\r\n    exec code in run_globals\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 180, in <module>\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 157, in manager\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/daemon.py\", line 61, in worker\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 174, in main\r\n    process()\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/pyspark.zip/pyspark/worker.py\", line 169, in process\r\n    serializer.dunits processed is zero\r\nump_stream(func(split_index, iterator), outfile)\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 2408, in pipeline_func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 345, in func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/pyspark.zip/pyspark/rdd.py\", line 793, in func\r\n  File \"/grid/4/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000002/tfspark.zip/tensorflowonspark/TFSparkNode.py\", line 240, in _mapfn\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/__pyfiles__/my_distributed_learning.py\", line 105, in map_fun\r\n    ((x, y_), units_completed, records_prodcued, sg_key) = read_csv_examples(skipgrams, None, batch_size, num_epochs, task_index, num_workers)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/__pyfiles__/my_distributed_learning.py\", line 56, in read_csv_examples\r\n    sg_key, sg_csv = sg_reader.read(sg_queue)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/io_ops.py\", line 193, in read\r\n    return gen_io_ops._reader_read_v2(self._reader_ref, queue_ref, name=name)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 422, in _reader_read_v2\r\n    queue_handle=queue_handle, name=name)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2359, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/grid/5/tmp/yarn-local/usercache/myusername/appcache/application_1501873511061_2823832/container_e06_1501873511061_2823832_01_000168/tf/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1242, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOutOfRangeError (see above for traceback): FIFOQueue '_1_sg_queue' is closed and has insufficient elements (requested 1, current size 0)\r\n\t [[Node: ReaderReadV2 = ReaderReadV2[_device=\"/job:worker/replica:0/task:1/cpu:0\"](sg_reader, sg_queue)]]\r\n\t [[Node: embedding_lookup/DynamicPartition_S751 = _Recv [client_terminated=false, recv_device=\"/job:ps/replica:0/task:6/cpu:0\", send_device=\"/job:worker/replica:0/task:1/cpu:0\", send_device_incarnation=6148508285306453080, tensor_name=\"edge_726_embedding_lookup/DynamicPartition\", tensor_type=DT_INT32, _device=\"/job:ps/replica:0/task:6/cpu:0\"]()]]\r\n"}