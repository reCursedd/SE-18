{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/325526915", "html_url": "https://github.com/tensorflow/tensorflow/issues/12410#issuecomment-325526915", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12410", "id": 325526915, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNTUyNjkxNQ==", "user": {"login": "kinsumliu", "id": 8632201, "node_id": "MDQ6VXNlcjg2MzIyMDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/8632201?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kinsumliu", "html_url": "https://github.com/kinsumliu", "followers_url": "https://api.github.com/users/kinsumliu/followers", "following_url": "https://api.github.com/users/kinsumliu/following{/other_user}", "gists_url": "https://api.github.com/users/kinsumliu/gists{/gist_id}", "starred_url": "https://api.github.com/users/kinsumliu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kinsumliu/subscriptions", "organizations_url": "https://api.github.com/users/kinsumliu/orgs", "repos_url": "https://api.github.com/users/kinsumliu/repos", "events_url": "https://api.github.com/users/kinsumliu/events{/privacy}", "received_events_url": "https://api.github.com/users/kinsumliu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-29T01:21:18Z", "updated_at": "2017-08-29T01:21:29Z", "author_association": "NONE", "body_html": "<p><strong>The quick update: if I increase <code>num_epochs</code> to 1000 from 1, it seems to solve the problem (I don't understand why). I can run it more to see if this solves it completely.</strong></p>\n<p>The input pipeline is also my initial suspect. I tried and checked the followings when the <code>num_epochs</code>=1:</p>\n<ol>\n<li>filenames to <code>string_input_producer</code> is not empty and they are valid hdfs paths for every worker (each worker will work on multiple files)</li>\n<li>sanity check on data</li>\n<li>I am using TensorFlowOnSpark framework so I can relaunch a worker if it fails. When a worker is relaunched due to empty queue, it can possibly read data and train without popping the same error. (There is no randomization in my data pipeline. So I assume the worker tries to read the same data in the same order after relaunch)</li>\n</ol>\n<p>My pipeline code looks like:</p>\n<pre><code>f_queue = tf.train.string_input_producer(filenames, shuffle=False, capacity=10, num_epochs=num_epochs)\nkey, csv = tf.TextLineReader(name=\"reader\").read(f_queue)\nbatch_csv = tf.train.batch(tensors=tf.split(tf.decode_csv(csv, defaults), 2), batch_size=batch_size, capacity=1000+4*batch_size, num_threads=args.readers, allow_smaller_final_batch=True)\n</code></pre>\n<p>I can also try your suggestion to run on a single machine. The error happens on some workers randomly. So not sure if I can reproduce the error not using distributed learning.</p>", "body_text": "The quick update: if I increase num_epochs to 1000 from 1, it seems to solve the problem (I don't understand why). I can run it more to see if this solves it completely.\nThe input pipeline is also my initial suspect. I tried and checked the followings when the num_epochs=1:\n\nfilenames to string_input_producer is not empty and they are valid hdfs paths for every worker (each worker will work on multiple files)\nsanity check on data\nI am using TensorFlowOnSpark framework so I can relaunch a worker if it fails. When a worker is relaunched due to empty queue, it can possibly read data and train without popping the same error. (There is no randomization in my data pipeline. So I assume the worker tries to read the same data in the same order after relaunch)\n\nMy pipeline code looks like:\nf_queue = tf.train.string_input_producer(filenames, shuffle=False, capacity=10, num_epochs=num_epochs)\nkey, csv = tf.TextLineReader(name=\"reader\").read(f_queue)\nbatch_csv = tf.train.batch(tensors=tf.split(tf.decode_csv(csv, defaults), 2), batch_size=batch_size, capacity=1000+4*batch_size, num_threads=args.readers, allow_smaller_final_batch=True)\n\nI can also try your suggestion to run on a single machine. The error happens on some workers randomly. So not sure if I can reproduce the error not using distributed learning.", "body": "**The quick update: if I increase `num_epochs` to 1000 from 1, it seems to solve the problem (I don't understand why). I can run it more to see if this solves it completely.**\r\n\r\nThe input pipeline is also my initial suspect. I tried and checked the followings when the `num_epochs`=1:\r\n\r\n1. filenames to `string_input_producer` is not empty and they are valid hdfs paths for every worker (each worker will work on multiple files)\r\n2. sanity check on data\r\n3. I am using TensorFlowOnSpark framework so I can relaunch a worker if it fails. When a worker is relaunched due to empty queue, it can possibly read data and train without popping the same error. (There is no randomization in my data pipeline. So I assume the worker tries to read the same data in the same order after relaunch)\r\n\r\nMy pipeline code looks like:\r\n```\r\nf_queue = tf.train.string_input_producer(filenames, shuffle=False, capacity=10, num_epochs=num_epochs)\r\nkey, csv = tf.TextLineReader(name=\"reader\").read(f_queue)\r\nbatch_csv = tf.train.batch(tensors=tf.split(tf.decode_csv(csv, defaults), 2), batch_size=batch_size, capacity=1000+4*batch_size, num_threads=args.readers, allow_smaller_final_batch=True)\r\n```\r\n\r\nI can also try your suggestion to run on a single machine. The error happens on some workers randomly. So not sure if I can reproduce the error not using distributed learning."}