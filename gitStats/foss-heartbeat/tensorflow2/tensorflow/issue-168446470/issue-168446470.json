{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3583", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3583/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3583/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3583/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3583", "id": 168446470, "node_id": "MDU6SXNzdWUxNjg0NDY0NzA=", "number": 3583, "title": "Dying Threads?", "user": {"login": "Anthony-Tatowicz", "id": 10676798, "node_id": "MDQ6VXNlcjEwNjc2Nzk4", "avatar_url": "https://avatars1.githubusercontent.com/u/10676798?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Anthony-Tatowicz", "html_url": "https://github.com/Anthony-Tatowicz", "followers_url": "https://api.github.com/users/Anthony-Tatowicz/followers", "following_url": "https://api.github.com/users/Anthony-Tatowicz/following{/other_user}", "gists_url": "https://api.github.com/users/Anthony-Tatowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/Anthony-Tatowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Anthony-Tatowicz/subscriptions", "organizations_url": "https://api.github.com/users/Anthony-Tatowicz/orgs", "repos_url": "https://api.github.com/users/Anthony-Tatowicz/repos", "events_url": "https://api.github.com/users/Anthony-Tatowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/Anthony-Tatowicz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2016-07-30T03:27:34Z", "updated_at": "2016-10-26T17:04:38Z", "closed_at": "2016-10-26T17:04:38Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu trusty</p>\n<p>Installed version of CUDA and cuDNN: Cuda compilation tools, release 7.5, V7.5.17</p>\n<ol>\n<li>Which <del>pip</del> package you installed.  Docker gpu-devel image</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<p>I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally<br>\n0.9.0</p>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>\n<p>Run inception model on training with inception_train.py</p>\n<p>bazel-bin/inception/(pick your data)_train <br>\n--train_dir=\"${TRAIN_DIR}\" <br>\n--data_dir=\"${DATA_DIR}\" <br>\n--pretrained_model_checkpoint_path=\"${PRETRAINED_DIR}\" <br>\n--fine_tune=True <br>\n--initial_learning_rate=0.01 <br>\n--input_queue_memory_factor=12 <br>\n--batch_size=64 <br>\n--max_steps=100000 <br>\n--num_epochs_per_decay=30 <br>\n--num_preprocess_threads=4 <br>\n--num_readers=4 <br>\n--log_device_placement=True</p>\n</li>\n</ol>\n<h3>What have you tried?</h3>\n<ol>\n<li>This is ironic, because I have been wrestling with the problem for a few days, and serendipitously came to a conclusion. I was training the model and testing it out, and always<br>\nafter 1000~ steps the training slows to a crawl utilizing my GPU only every 10s - 15s and the CPU seems to be only utilizing 2 threads to 100% at this point for preprocessing , when i specified 4 &amp; 4 readers, but all threads kick in right before/at the GPU utilization. I tried restarted and playing with the threads and readers and other things, no luck, it still slowed after 1000~ steps. I left it running for whatever reason and later started running a Tensorflow bazel build, which used significant CPU for some time and after/during the build my training magically picked up to the original speed at the beginning, and by picked up i mean went from 1-4 exp/s not steady to 15 exp/s steady, hmm thats strange, because it had been crawling for hours at 1-4 exp/s?</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>None</p>", "body_text": "Environment info\nOperating System: Ubuntu trusty\nInstalled version of CUDA and cuDNN: Cuda compilation tools, release 7.5, V7.5.17\n\nWhich pip package you installed.  Docker gpu-devel image\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\nSteps to reproduce\n\n\nRun inception model on training with inception_train.py\nbazel-bin/inception/(pick your data)_train \n--train_dir=\"${TRAIN_DIR}\" \n--data_dir=\"${DATA_DIR}\" \n--pretrained_model_checkpoint_path=\"${PRETRAINED_DIR}\" \n--fine_tune=True \n--initial_learning_rate=0.01 \n--input_queue_memory_factor=12 \n--batch_size=64 \n--max_steps=100000 \n--num_epochs_per_decay=30 \n--num_preprocess_threads=4 \n--num_readers=4 \n--log_device_placement=True\n\n\nWhat have you tried?\n\nThis is ironic, because I have been wrestling with the problem for a few days, and serendipitously came to a conclusion. I was training the model and testing it out, and always\nafter 1000~ steps the training slows to a crawl utilizing my GPU only every 10s - 15s and the CPU seems to be only utilizing 2 threads to 100% at this point for preprocessing , when i specified 4 & 4 readers, but all threads kick in right before/at the GPU utilization. I tried restarted and playing with the threads and readers and other things, no luck, it still slowed after 1000~ steps. I left it running for whatever reason and later started running a Tensorflow bazel build, which used significant CPU for some time and after/during the build my training magically picked up to the original speed at the beginning, and by picked up i mean went from 1-4 exp/s not steady to 15 exp/s steady, hmm thats strange, because it had been crawling for hours at 1-4 exp/s?\n\nLogs or other output that would be helpful\nNone", "body": "### Environment info\n\nOperating System: Ubuntu trusty\n\nInstalled version of CUDA and cuDNN: Cuda compilation tools, release 7.5, V7.5.17\n1. Which ~~pip~~ package you installed.  Docker gpu-devel image\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\n0.9.0\n### Steps to reproduce\n1. Run inception model on training with inception_train.py\n   \n   bazel-bin/inception/(pick your data)_train \\\n     --train_dir=\"${TRAIN_DIR}\" \\\n     --data_dir=\"${DATA_DIR}\" \\\n     --pretrained_model_checkpoint_path=\"${PRETRAINED_DIR}\" \\\n     --fine_tune=True \\\n     --initial_learning_rate=0.01 \\\n     --input_queue_memory_factor=12 \\\n     --batch_size=64 \\\n     --max_steps=100000 \\\n     --num_epochs_per_decay=30 \\\n     --num_preprocess_threads=4 \\\n     --num_readers=4 \\\n     --log_device_placement=True\n### What have you tried?\n1. This is ironic, because I have been wrestling with the problem for a few days, and serendipitously came to a conclusion. I was training the model and testing it out, and always\n   after 1000~ steps the training slows to a crawl utilizing my GPU only every 10s - 15s and the CPU seems to be only utilizing 2 threads to 100% at this point for preprocessing , when i specified 4 & 4 readers, but all threads kick in right before/at the GPU utilization. I tried restarted and playing with the threads and readers and other things, no luck, it still slowed after 1000~ steps. I left it running for whatever reason and later started running a Tensorflow bazel build, which used significant CPU for some time and after/during the build my training magically picked up to the original speed at the beginning, and by picked up i mean went from 1-4 exp/s not steady to 15 exp/s steady, hmm thats strange, because it had been crawling for hours at 1-4 exp/s? \n### Logs or other output that would be helpful\n\nNone \n"}