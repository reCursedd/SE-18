{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4056", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4056/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4056/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4056/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4056", "id": 173446099, "node_id": "MDU6SXNzdWUxNzM0NDYwOTk=", "number": 4056, "title": "Adding more layers decreases model accuracy. ", "user": {"login": "inderpreetsganda", "id": 10248819, "node_id": "MDQ6VXNlcjEwMjQ4ODE5", "avatar_url": "https://avatars0.githubusercontent.com/u/10248819?v=4", "gravatar_id": "", "url": "https://api.github.com/users/inderpreetsganda", "html_url": "https://github.com/inderpreetsganda", "followers_url": "https://api.github.com/users/inderpreetsganda/followers", "following_url": "https://api.github.com/users/inderpreetsganda/following{/other_user}", "gists_url": "https://api.github.com/users/inderpreetsganda/gists{/gist_id}", "starred_url": "https://api.github.com/users/inderpreetsganda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/inderpreetsganda/subscriptions", "organizations_url": "https://api.github.com/users/inderpreetsganda/orgs", "repos_url": "https://api.github.com/users/inderpreetsganda/repos", "events_url": "https://api.github.com/users/inderpreetsganda/events{/privacy}", "received_events_url": "https://api.github.com/users/inderpreetsganda/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-08-26T12:51:49Z", "updated_at": "2016-08-26T15:28:34Z", "closed_at": "2016-08-26T15:28:34Z", "author_association": "NONE", "body_html": "<p>Here are two different reproducible codes, one has two conv layers and other has 10 conv layers. Model with two conv layers reaches the result in few iterations whereas model with 10 conv layers reaches the result in more iterations, and moreover they both produce same result 62.5 % accuracy. Model with 10 conv layers should provide better accuracy(because it has more layers) but it gives same accuracy as 2 conv layer model and reaches the same result after more iterations, so adding more layers is degrading performance.</p>\n<p>Here is 2 conv layer code:</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 1\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nx = tf.squeeze(outputs_fed_lstm2, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 64])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n</code></pre>\n<p>Here is code for 10 layer conv model:</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 100\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nfilter_shape3 = [1, 1, 64, 64]\nconv_weights3 = tf.get_variable(\"conv_weights3\" , filter_shape3, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases3 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv3 = tf.nn.conv2d(outputs_fed_lstm2, conv_weights3, strides=[1,1,1,1], padding = \"VALID\")\nnormalize3 = tf.nn.elu(conv3 + conv_biases3)\ntf_normalize3 = tf.contrib.layers.batch_norm(inputs = normalize3,is_training = True)\noutputs_fed_lstm3 = tf_normalize3\n\nfilter_shape4 = [1, 1, 64, 128]\nconv_weights4 = tf.get_variable(\"conv_weights4\" , filter_shape4, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases4 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv4 = tf.nn.conv2d(outputs_fed_lstm3, conv_weights4, strides=[1,1,1,1], padding = \"VALID\")\nnormalize4 = tf.nn.elu(conv4 + conv_biases4)\ntf_normalize4 = tf.contrib.layers.batch_norm(inputs = normalize4,is_training = True)\noutputs_fed_lstm4 = tf_normalize4\n\nfilter_shape5 = [1, 1, 128, 128]\nconv_weights5 = tf.get_variable(\"conv_weights5\" , filter_shape5, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases5 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv5 = tf.nn.conv2d(outputs_fed_lstm4, conv_weights5, strides=[1,1,1,1], padding = \"VALID\")\nnormalize5 = tf.nn.elu(conv5 + conv_biases5)\ntf_normalize5 = tf.contrib.layers.batch_norm(inputs = normalize5,is_training = True)\noutputs_fed_lstm5 = tf_normalize5\n\nfilter_shape6 = [1, 1, 128, 128]\nconv_weights6 = tf.get_variable(\"conv_weights6\" , filter_shape6, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases6 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv6 = tf.nn.conv2d(outputs_fed_lstm5, conv_weights6, strides=[1,1,1,1], padding = \"VALID\")\nnormalize6 = tf.nn.elu(conv6 + conv_biases6)\ntf_normalize6 = tf.contrib.layers.batch_norm(inputs = normalize6,is_training = True)\noutputs_fed_lstm6 = tf_normalize6  \n\nfilter_shape7 = [1, 1, 128, 256]\nconv_weights7 = tf.get_variable(\"conv_weights7\" , filter_shape7, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases7 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv7 = tf.nn.conv2d(outputs_fed_lstm6, conv_weights7, strides=[1,1,1,1], padding = \"VALID\")\nnormalize7 = tf.nn.elu(conv7 + conv_biases7)\ntf_normalize7 = tf.contrib.layers.batch_norm(inputs = normalize7,is_training = True)\noutputs_fed_lstm7 = tf_normalize7 \n\nfilter_shape8 = [1, 1, 256, 256]\nconv_weights8 = tf.get_variable(\"conv_weights8\" , filter_shape8, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases8 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv8 = tf.nn.conv2d(outputs_fed_lstm7, conv_weights8, strides=[1,1,1,1], padding = \"VALID\")\nnormalize8 = tf.nn.elu(conv8 + conv_biases8)\ntf_normalize8 = tf.contrib.layers.batch_norm(inputs = normalize8,is_training = True)\noutputs_fed_lstm8 = tf_normalize8 \n\nfilter_shape9 = [1, 1, 256, 256]\nconv_weights9 = tf.get_variable(\"conv_weights9\" , filter_shape9, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases9 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv9 = tf.nn.conv2d(outputs_fed_lstm8, conv_weights9, strides=[1,1,1,1], padding = \"VALID\")\nnormalize9 = tf.nn.elu(conv9 + conv_biases9)\ntf_normalize9 = tf.contrib.layers.batch_norm(inputs = normalize9,is_training = True)\noutputs_fed_lstm9 = tf_normalize9 \n\nfilter_shape0 = [1, 1, 256, 512]\nconv_weights0 = tf.get_variable(\"conv_weights0\" , filter_shape0, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases0 = tf.Variable(tf.constant(0.1, shape=[512]))\nconv0 = tf.nn.conv2d(outputs_fed_lstm9, conv_weights0, strides=[1,1,1,1], padding = \"VALID\")\nnormalize0 = tf.nn.elu(conv0 + conv_biases0)\ntf_normalize0 = tf.contrib.layers.batch_norm(inputs = normalize0,is_training = True)\noutputs_fed_lstm0 = tf_normalize0 \n\nx = tf.squeeze(outputs_fed_lstm0, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 512])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n</code></pre>", "body_text": "Here are two different reproducible codes, one has two conv layers and other has 10 conv layers. Model with two conv layers reaches the result in few iterations whereas model with 10 conv layers reaches the result in more iterations, and moreover they both produce same result 62.5 % accuracy. Model with 10 conv layers should provide better accuracy(because it has more layers) but it gives same accuracy as 2 conv layer model and reaches the same result after more iterations, so adding more layers is degrading performance.\nHere is 2 conv layer code:\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 1\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nx = tf.squeeze(outputs_fed_lstm2, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 64])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n\nHere is code for 10 layer conv model:\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 100\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nfilter_shape3 = [1, 1, 64, 64]\nconv_weights3 = tf.get_variable(\"conv_weights3\" , filter_shape3, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases3 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv3 = tf.nn.conv2d(outputs_fed_lstm2, conv_weights3, strides=[1,1,1,1], padding = \"VALID\")\nnormalize3 = tf.nn.elu(conv3 + conv_biases3)\ntf_normalize3 = tf.contrib.layers.batch_norm(inputs = normalize3,is_training = True)\noutputs_fed_lstm3 = tf_normalize3\n\nfilter_shape4 = [1, 1, 64, 128]\nconv_weights4 = tf.get_variable(\"conv_weights4\" , filter_shape4, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases4 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv4 = tf.nn.conv2d(outputs_fed_lstm3, conv_weights4, strides=[1,1,1,1], padding = \"VALID\")\nnormalize4 = tf.nn.elu(conv4 + conv_biases4)\ntf_normalize4 = tf.contrib.layers.batch_norm(inputs = normalize4,is_training = True)\noutputs_fed_lstm4 = tf_normalize4\n\nfilter_shape5 = [1, 1, 128, 128]\nconv_weights5 = tf.get_variable(\"conv_weights5\" , filter_shape5, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases5 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv5 = tf.nn.conv2d(outputs_fed_lstm4, conv_weights5, strides=[1,1,1,1], padding = \"VALID\")\nnormalize5 = tf.nn.elu(conv5 + conv_biases5)\ntf_normalize5 = tf.contrib.layers.batch_norm(inputs = normalize5,is_training = True)\noutputs_fed_lstm5 = tf_normalize5\n\nfilter_shape6 = [1, 1, 128, 128]\nconv_weights6 = tf.get_variable(\"conv_weights6\" , filter_shape6, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases6 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv6 = tf.nn.conv2d(outputs_fed_lstm5, conv_weights6, strides=[1,1,1,1], padding = \"VALID\")\nnormalize6 = tf.nn.elu(conv6 + conv_biases6)\ntf_normalize6 = tf.contrib.layers.batch_norm(inputs = normalize6,is_training = True)\noutputs_fed_lstm6 = tf_normalize6  \n\nfilter_shape7 = [1, 1, 128, 256]\nconv_weights7 = tf.get_variable(\"conv_weights7\" , filter_shape7, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases7 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv7 = tf.nn.conv2d(outputs_fed_lstm6, conv_weights7, strides=[1,1,1,1], padding = \"VALID\")\nnormalize7 = tf.nn.elu(conv7 + conv_biases7)\ntf_normalize7 = tf.contrib.layers.batch_norm(inputs = normalize7,is_training = True)\noutputs_fed_lstm7 = tf_normalize7 \n\nfilter_shape8 = [1, 1, 256, 256]\nconv_weights8 = tf.get_variable(\"conv_weights8\" , filter_shape8, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases8 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv8 = tf.nn.conv2d(outputs_fed_lstm7, conv_weights8, strides=[1,1,1,1], padding = \"VALID\")\nnormalize8 = tf.nn.elu(conv8 + conv_biases8)\ntf_normalize8 = tf.contrib.layers.batch_norm(inputs = normalize8,is_training = True)\noutputs_fed_lstm8 = tf_normalize8 \n\nfilter_shape9 = [1, 1, 256, 256]\nconv_weights9 = tf.get_variable(\"conv_weights9\" , filter_shape9, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases9 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv9 = tf.nn.conv2d(outputs_fed_lstm8, conv_weights9, strides=[1,1,1,1], padding = \"VALID\")\nnormalize9 = tf.nn.elu(conv9 + conv_biases9)\ntf_normalize9 = tf.contrib.layers.batch_norm(inputs = normalize9,is_training = True)\noutputs_fed_lstm9 = tf_normalize9 \n\nfilter_shape0 = [1, 1, 256, 512]\nconv_weights0 = tf.get_variable(\"conv_weights0\" , filter_shape0, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases0 = tf.Variable(tf.constant(0.1, shape=[512]))\nconv0 = tf.nn.conv2d(outputs_fed_lstm9, conv_weights0, strides=[1,1,1,1], padding = \"VALID\")\nnormalize0 = tf.nn.elu(conv0 + conv_biases0)\ntf_normalize0 = tf.contrib.layers.batch_norm(inputs = normalize0,is_training = True)\noutputs_fed_lstm0 = tf_normalize0 \n\nx = tf.squeeze(outputs_fed_lstm0, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 512])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))", "body": "Here are two different reproducible codes, one has two conv layers and other has 10 conv layers. Model with two conv layers reaches the result in few iterations whereas model with 10 conv layers reaches the result in more iterations, and moreover they both produce same result 62.5 % accuracy. Model with 10 conv layers should provide better accuracy(because it has more layers) but it gives same accuracy as 2 conv layer model and reaches the same result after more iterations, so adding more layers is degrading performance.\n\n Here is 2 conv layer code:\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 1\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nx = tf.squeeze(outputs_fed_lstm2, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 64])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n\nHere is code for 10 layer conv model:\n\n```\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport math\nimport os\nimport nltk\nimport random\nbatch_size = 100\nstart = 0\nend = batch_size\nlearning_rate = 0.001\nnum_classes = 2\ntime_steps = 4\nembedding = 2\nstep = 1\n_units = 100\nnum_of_filters = 2\n\ntrain_set_x = [[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]],[[1,2],[3,4],[5,6],[7,8]]]\ntrain_set_y = [0,1,1,1,1,1,1,1,1,1,1,1,0,0,0,1,1,1,1,1,0,0,0,0,0,0,1,0,1,0,1,1,1,0,1,0,1,0,1,1]\n\nX = tf.placeholder(tf.float32, [None,time_steps,embedding])\nY = tf.placeholder(tf.int32, [None])\n\n\nx = tf.expand_dims(X,3)\n\nfilter_shape = [1, embedding, 1, 64]\nconv_weights = tf.get_variable(\"conv_weights1\" , filter_shape, tf.float32, tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases = tf.Variable(tf.constant(0.1, shape=[64]))\nconv = tf.nn.conv2d(x, conv_weights, strides=[1,1,1,1], padding = \"VALID\")\nnormalize = tf.nn.elu(conv + conv_biases)\ntf_normalize = tf.contrib.layers.batch_norm(inputs = normalize,is_training = True)\noutputs_fed_lstm = tf_normalize\n\nfilter_shape2 = [1, 1, 64, 64]\nconv_weights2 = tf.get_variable(\"conv_weights2\" , filter_shape2, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases2 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv2 = tf.nn.conv2d(outputs_fed_lstm, conv_weights2, strides=[1,1,1,1], padding = \"VALID\")\nnormalize2 = tf.nn.elu(conv2 + conv_biases2)\ntf_normalize2 = tf.contrib.layers.batch_norm(inputs = normalize2,is_training = True)\noutputs_fed_lstm2 = tf_normalize2\n\nfilter_shape3 = [1, 1, 64, 64]\nconv_weights3 = tf.get_variable(\"conv_weights3\" , filter_shape3, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases3 = tf.Variable(tf.constant(0.1, shape=[64]))\nconv3 = tf.nn.conv2d(outputs_fed_lstm2, conv_weights3, strides=[1,1,1,1], padding = \"VALID\")\nnormalize3 = tf.nn.elu(conv3 + conv_biases3)\ntf_normalize3 = tf.contrib.layers.batch_norm(inputs = normalize3,is_training = True)\noutputs_fed_lstm3 = tf_normalize3\n\nfilter_shape4 = [1, 1, 64, 128]\nconv_weights4 = tf.get_variable(\"conv_weights4\" , filter_shape4, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases4 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv4 = tf.nn.conv2d(outputs_fed_lstm3, conv_weights4, strides=[1,1,1,1], padding = \"VALID\")\nnormalize4 = tf.nn.elu(conv4 + conv_biases4)\ntf_normalize4 = tf.contrib.layers.batch_norm(inputs = normalize4,is_training = True)\noutputs_fed_lstm4 = tf_normalize4\n\nfilter_shape5 = [1, 1, 128, 128]\nconv_weights5 = tf.get_variable(\"conv_weights5\" , filter_shape5, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases5 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv5 = tf.nn.conv2d(outputs_fed_lstm4, conv_weights5, strides=[1,1,1,1], padding = \"VALID\")\nnormalize5 = tf.nn.elu(conv5 + conv_biases5)\ntf_normalize5 = tf.contrib.layers.batch_norm(inputs = normalize5,is_training = True)\noutputs_fed_lstm5 = tf_normalize5\n\nfilter_shape6 = [1, 1, 128, 128]\nconv_weights6 = tf.get_variable(\"conv_weights6\" , filter_shape6, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases6 = tf.Variable(tf.constant(0.1, shape=[128]))\nconv6 = tf.nn.conv2d(outputs_fed_lstm5, conv_weights6, strides=[1,1,1,1], padding = \"VALID\")\nnormalize6 = tf.nn.elu(conv6 + conv_biases6)\ntf_normalize6 = tf.contrib.layers.batch_norm(inputs = normalize6,is_training = True)\noutputs_fed_lstm6 = tf_normalize6  \n\nfilter_shape7 = [1, 1, 128, 256]\nconv_weights7 = tf.get_variable(\"conv_weights7\" , filter_shape7, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases7 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv7 = tf.nn.conv2d(outputs_fed_lstm6, conv_weights7, strides=[1,1,1,1], padding = \"VALID\")\nnormalize7 = tf.nn.elu(conv7 + conv_biases7)\ntf_normalize7 = tf.contrib.layers.batch_norm(inputs = normalize7,is_training = True)\noutputs_fed_lstm7 = tf_normalize7 \n\nfilter_shape8 = [1, 1, 256, 256]\nconv_weights8 = tf.get_variable(\"conv_weights8\" , filter_shape8, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases8 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv8 = tf.nn.conv2d(outputs_fed_lstm7, conv_weights8, strides=[1,1,1,1], padding = \"VALID\")\nnormalize8 = tf.nn.elu(conv8 + conv_biases8)\ntf_normalize8 = tf.contrib.layers.batch_norm(inputs = normalize8,is_training = True)\noutputs_fed_lstm8 = tf_normalize8 \n\nfilter_shape9 = [1, 1, 256, 256]\nconv_weights9 = tf.get_variable(\"conv_weights9\" , filter_shape9, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases9 = tf.Variable(tf.constant(0.1, shape=[256]))\nconv9 = tf.nn.conv2d(outputs_fed_lstm8, conv_weights9, strides=[1,1,1,1], padding = \"VALID\")\nnormalize9 = tf.nn.elu(conv9 + conv_biases9)\ntf_normalize9 = tf.contrib.layers.batch_norm(inputs = normalize9,is_training = True)\noutputs_fed_lstm9 = tf_normalize9 \n\nfilter_shape0 = [1, 1, 256, 512]\nconv_weights0 = tf.get_variable(\"conv_weights0\" , filter_shape0, tf.float32,tf.truncated_normal_initializer(mean=0.0, stddev=1.0))\nconv_biases0 = tf.Variable(tf.constant(0.1, shape=[512]))\nconv0 = tf.nn.conv2d(outputs_fed_lstm9, conv_weights0, strides=[1,1,1,1], padding = \"VALID\")\nnormalize0 = tf.nn.elu(conv0 + conv_biases0)\ntf_normalize0 = tf.contrib.layers.batch_norm(inputs = normalize0,is_training = True)\noutputs_fed_lstm0 = tf_normalize0 \n\nx = tf.squeeze(outputs_fed_lstm0, [2])     \nx = tf.transpose(x, [1, 0, 2])\nx = tf.reshape(x, [-1, 512])\nx = tf.split(0, time_steps, x)\n\nlstm = tf.nn.rnn_cell.LSTMCell(num_units = _units)\n\n# multi_lstm = tf.nn.rnn_cell.MultiRNNCell([lstm] * lstm_layers, state_is_tuple = True)\n\noutputs , state = tf.nn.rnn(lstm,x, dtype = tf.float32)     \n\nweights = tf.Variable(tf.random_normal([_units,num_classes]))\nbiases  = tf.Variable(tf.random_normal([num_classes]))\n\nlogits = tf.matmul(outputs[-1], weights) + biases\n\n\n\nc_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,Y)\nloss = tf.reduce_mean(c_loss)\n\n\nglobal_step = tf.Variable(0, name=\"global_step\", trainable=False)\n# decayed_learning_rate = tf.train.exponential_decay(learning_rate,0,10000,0.9)\noptimizer= tf.train.AdamOptimizer(learning_rate)\nminimize_loss = optimizer.minimize(loss, global_step=global_step)   \n#grads_and_vars = optimizer.compute_gradients(loss,[conv_weights2]) \ncorrect_predict = tf.nn.in_top_k(logits, Y, 1)\naccuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n\n\ninit = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n     sess.run(init)\n     for i in range(1000):\n         for j in range(1000):\n             x = train_set_x\n             y = train_set_y\n             sess.run(minimize_loss,feed_dict={X : x, Y : y})\n             step += 1  \n             #gr_print = sess.run([grad for grad, _ in grads_and_vars], feed_dict={X : x, Y : y})\n             #print (gr_print)\n             cost = sess.run(loss,feed_dict = {X: x,Y: y})\n             accu = sess.run(accuracy,feed_dict = {X: x, Y: y})\n             print (\"Loss after one Epoch(Training) = \" + \"{:.6f}\".format(cost) + \", Training Accuracy= \" + \"{:.5f}\".format(accu))\n```\n"}