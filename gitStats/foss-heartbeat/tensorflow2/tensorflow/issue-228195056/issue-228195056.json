{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9851", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9851/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9851/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9851/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9851", "id": 228195056, "node_id": "MDU6SXNzdWUyMjgxOTUwNTY=", "number": 9851, "title": "Issue with tf.nn.max_pool", "user": {"login": "wgmao", "id": 5178826, "node_id": "MDQ6VXNlcjUxNzg4MjY=", "avatar_url": "https://avatars3.githubusercontent.com/u/5178826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wgmao", "html_url": "https://github.com/wgmao", "followers_url": "https://api.github.com/users/wgmao/followers", "following_url": "https://api.github.com/users/wgmao/following{/other_user}", "gists_url": "https://api.github.com/users/wgmao/gists{/gist_id}", "starred_url": "https://api.github.com/users/wgmao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wgmao/subscriptions", "organizations_url": "https://api.github.com/users/wgmao/orgs", "repos_url": "https://api.github.com/users/wgmao/repos", "events_url": "https://api.github.com/users/wgmao/events{/privacy}", "received_events_url": "https://api.github.com/users/wgmao/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-12T05:56:47Z", "updated_at": "2017-05-12T22:02:00Z", "closed_at": "2017-05-12T15:52:42Z", "author_association": "NONE", "body_html": "<p>I am a little confused about the implementation of tf.nn.max_pool() function. Below it's a toy example.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef weight_variable(shape, name):\n\tinitial = tf.truncated_normal(shape, stddev = 0.1)\n\treturn tf.Variable(initial, name = name)\n\n\nxe_ = tf.placeholder(tf.float32, shape = (None, 4, 20, 1))\nW_conv1 = weight_variable([4, 10, 1, 10], 'W_conv1')\n\nlayer0 = tf.nn.conv2d(xe_, W_conv1, strides = [1,1,1,1], padding='VALID')\nlayer1 = tf.nn.relu(layer0)\nlayer2 = tf.nn.max_pool(layer1, ksize = (1,1,5,1), strides = (1,1,5,1), padding='SAME')\n\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nnp.random.seed(0)\nsample  = np.random.rand(1,4,20,1)\nmat1 = layer1.eval(feed_dict = {xe_: sample}, session = sess)\nmat2 = layer2.eval(feed_dict = {xe_: sample}, session = sess)\n\nnp.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]\n</code></pre>\n<p>Here I just want to perform a convolution operation with ten <code>4*10</code> filters on a single <code>4*20</code> image with only one channel. After the convolution ,there is a ReLU activation layer, followed with a max pooling layer. For the max pooling layer, the window size is just <code>1*5</code> and the stride comes with the same size. <code>sample</code> is just a random image with desired size. <code>mat1</code> and <code>mat2</code> are the output after <code>sample</code> going through these designed layers, <code>conv2d+ReLU</code> and <code>conv2d+ReLU+max_pool</code> respectively.</p>\n<p>Am I supposed to get an all-zero array from the last line of the code <code>np.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]</code>. Please point out if I understand <code>tf.nn.max_pool()</code> in a wrong way. Great thanks.</p>\n<h3>Environment info</h3>\n<ul>\n<li>Ubuntu  14.04.5 LTS</li>\n<li>Python 2.7.6</li>\n<li>cuda 8, V8.0.44</li>\n<li>cudnn 5.1.3</li>\n<li>TensorFlow 1.0.1</li>\n<li>GeForce GTX 1080</li>\n</ul>", "body_text": "I am a little confused about the implementation of tf.nn.max_pool() function. Below it's a toy example.\nimport tensorflow as tf\nimport numpy as np\n\ndef weight_variable(shape, name):\n\tinitial = tf.truncated_normal(shape, stddev = 0.1)\n\treturn tf.Variable(initial, name = name)\n\n\nxe_ = tf.placeholder(tf.float32, shape = (None, 4, 20, 1))\nW_conv1 = weight_variable([4, 10, 1, 10], 'W_conv1')\n\nlayer0 = tf.nn.conv2d(xe_, W_conv1, strides = [1,1,1,1], padding='VALID')\nlayer1 = tf.nn.relu(layer0)\nlayer2 = tf.nn.max_pool(layer1, ksize = (1,1,5,1), strides = (1,1,5,1), padding='SAME')\n\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\nnp.random.seed(0)\nsample  = np.random.rand(1,4,20,1)\nmat1 = layer1.eval(feed_dict = {xe_: sample}, session = sess)\nmat2 = layer2.eval(feed_dict = {xe_: sample}, session = sess)\n\nnp.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]\n\nHere I just want to perform a convolution operation with ten 4*10 filters on a single 4*20 image with only one channel. After the convolution ,there is a ReLU activation layer, followed with a max pooling layer. For the max pooling layer, the window size is just 1*5 and the stride comes with the same size. sample is just a random image with desired size. mat1 and mat2 are the output after sample going through these designed layers, conv2d+ReLU and conv2d+ReLU+max_pool respectively.\nAm I supposed to get an all-zero array from the last line of the code np.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]. Please point out if I understand tf.nn.max_pool() in a wrong way. Great thanks.\nEnvironment info\n\nUbuntu  14.04.5 LTS\nPython 2.7.6\ncuda 8, V8.0.44\ncudnn 5.1.3\nTensorFlow 1.0.1\nGeForce GTX 1080", "body": "I am a little confused about the implementation of tf.nn.max_pool() function. Below it's a toy example.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef weight_variable(shape, name):\r\n\tinitial = tf.truncated_normal(shape, stddev = 0.1)\r\n\treturn tf.Variable(initial, name = name)\r\n\r\n\r\nxe_ = tf.placeholder(tf.float32, shape = (None, 4, 20, 1))\r\nW_conv1 = weight_variable([4, 10, 1, 10], 'W_conv1')\r\n\r\nlayer0 = tf.nn.conv2d(xe_, W_conv1, strides = [1,1,1,1], padding='VALID')\r\nlayer1 = tf.nn.relu(layer0)\r\nlayer2 = tf.nn.max_pool(layer1, ksize = (1,1,5,1), strides = (1,1,5,1), padding='SAME')\r\n\r\n\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\nnp.random.seed(0)\r\nsample  = np.random.rand(1,4,20,1)\r\nmat1 = layer1.eval(feed_dict = {xe_: sample}, session = sess)\r\nmat2 = layer2.eval(feed_dict = {xe_: sample}, session = sess)\r\n\r\nnp.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]\r\n```\r\nHere I just want to perform a convolution operation with ten `4*10` filters on a single `4*20` image with only one channel. After the convolution ,there is a ReLU activation layer, followed with a max pooling layer. For the max pooling layer, the window size is just `1*5` and the stride comes with the same size. `sample` is just a random image with desired size. `mat1` and `mat2` are the output after `sample` going through these designed layers, `conv2d+ReLU` and `conv2d+ReLU+max_pool` respectively. \r\n\r\nAm I supposed to get an all-zero array from the last line of the code `np.max(mat1[0,0,0:5,:],axis=0)-mat2[0,0,0,:]`. Please point out if I understand `tf.nn.max_pool()` in a wrong way. Great thanks.\r\n\r\n\r\n### Environment info\r\n- Ubuntu  14.04.5 LTS\r\n- Python 2.7.6\r\n- cuda 8, V8.0.44\r\n- cudnn 5.1.3\r\n- TensorFlow 1.0.1\r\n- GeForce GTX 1080\r\n"}