{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9399", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9399/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9399/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9399/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9399", "id": 223686320, "node_id": "MDU6SXNzdWUyMjM2ODYzMjA=", "number": 9399, "title": "Distributed Tensorflow model is no faster than standalone", "user": {"login": "ashwhall", "id": 14365341, "node_id": "MDQ6VXNlcjE0MzY1MzQx", "avatar_url": "https://avatars3.githubusercontent.com/u/14365341?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashwhall", "html_url": "https://github.com/ashwhall", "followers_url": "https://api.github.com/users/ashwhall/followers", "following_url": "https://api.github.com/users/ashwhall/following{/other_user}", "gists_url": "https://api.github.com/users/ashwhall/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashwhall/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashwhall/subscriptions", "organizations_url": "https://api.github.com/users/ashwhall/orgs", "repos_url": "https://api.github.com/users/ashwhall/repos", "events_url": "https://api.github.com/users/ashwhall/events{/privacy}", "received_events_url": "https://api.github.com/users/ashwhall/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-04-24T01:56:50Z", "updated_at": "2018-03-27T09:20:18Z", "closed_at": "2017-04-24T16:50:25Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nI have taken the <a href=\"https://github.com/tensorflow/models/tree/master/resnet\">Resnet code</a> from the Tensorflow model zoo and distributed it as according to <a href=\"https://www.tensorflow.org/deploy/distributed\" rel=\"nofollow\">https://www.tensorflow.org/deploy/distributed</a></li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04.1 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nOfficial Tensorflow-GPU docker image</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n('v1.0.0-2378-g2259213', '1.1.0-rc0')</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nVersion 8</li>\n<li><strong>GPU model and memory</strong>:<br>\nTested on multiple setups within a cluster<br>\n4 machines with 2x Nvidia TitanX 12GB<br>\n1 machine with 2x GeForce GTX 1080 8GB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nThe commands differ slightly between machines depending on cluster configuration, but for 1 PS and 2 workers it looks like this:</li>\n</ul>\n<p><code>CUDA_VISIBLE_DEVICES=\"\" python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"ps\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"</code></p>\n<p><code>CUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"</code></p>\n<p><code>CUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=1 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"</code></p>\n<h3>Describe the problem</h3>\n<p>When running the Resnet model on CIFAR10 from the tf zoo in a distributed environment (asynchronous, between-graph replication), there is no performance gain.</p>\n<p>Running Resnet 110 on a single (non-distributed) machine, single GPU resulted in approximately 3 iterations per second. This is considered the baseline for any tests on the following configurations - all of which resulted in approximately the same number of iterations per second irrespective of configuration.<br>\n<em>Note: in all cases, each worker has their own dedicated GPU</em></p>\n<ol>\n<li>1 PS, 2 workers on one machine</li>\n<li>1 PS with dedicated GPU &amp; 1 worker on one machine, 1 worker on another machine</li>\n<li>1 PS &amp; 2 workers on one machine, 2 workers on another machine</li>\n<li>1 PS &amp; 2 workers on one machine, 3 more machines with 2 workers each</li>\n<li>4 machines with 2 workers each, two of these with 1 PS each</li>\n<li>4 machines, each with 1 PS &amp; 2 workers</li>\n</ol>\n<p>When starting up the workers, each would begin training as soon as it's ready, printing information to the terminal during/after each iteration. The difference is speed is visually noticeable - when there is only 1 worker running, it's as fast as I would expect. As more workers spin up, all of the existing workers slow down. In this way, I can see (with 8 terminals on my screen) the entire cluster slowing as more workers begin.</p>\n<p>I monitored system stats during training to rule things out as the bottleneck and found the following for each machine:</p>\n<ul>\n<li>CPU usage: CPU usage is not at 100% for any machine</li>\n<li>GPU usage: GPU usage repeatedly flickers between 0% and ~80% - once per iteration</li>\n<li>Network usage: Network bandwidth in/out for any given machine is never more than ~80% of its capacity. the network speed is limited to 1Gbps, and it doesn't reach this cap. We raised the limit to 2Gbps and saw no increased usage or performance.</li>\n<li>HDD usage: Batches are loaded from disk multi-threaded. I printed to the screen during file-access and it's practically instantaneous.</li>\n</ul>\n<p>For the input pipeline, I have also tried switching between <code>tf.RandomShuffleQueue</code> and <code>tf.train.shuffle_batch</code> and playing with the number of threads/min after deque etc for each of these batching methods to no avail.</p>\n<h3>Source code / logs</h3>\n<p>Source code files attached below - <code>cifar_input.py</code> has small modifications from the original input pipeline. The original cifar input code is the file <code>cifar_input_orig.py</code></p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/950254/resnet_distrib.zip\">resnet_distrib.zip</a></p>\n<p><strong>Thank you in advance for any light you may be able to shed on this!</strong></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI have taken the Resnet code from the Tensorflow model zoo and distributed it as according to https://www.tensorflow.org/deploy/distributed\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04.1 LTS\nTensorFlow installed from (source or binary):\nOfficial Tensorflow-GPU docker image\nTensorFlow version (use command below):\n('v1.0.0-2378-g2259213', '1.1.0-rc0')\nBazel version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nVersion 8\nGPU model and memory:\nTested on multiple setups within a cluster\n4 machines with 2x Nvidia TitanX 12GB\n1 machine with 2x GeForce GTX 1080 8GB\nExact command to reproduce:\nThe commands differ slightly between machines depending on cluster configuration, but for 1 PS and 2 workers it looks like this:\n\nCUDA_VISIBLE_DEVICES=\"\" python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"ps\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"\nCUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"\nCUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=1 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"\nDescribe the problem\nWhen running the Resnet model on CIFAR10 from the tf zoo in a distributed environment (asynchronous, between-graph replication), there is no performance gain.\nRunning Resnet 110 on a single (non-distributed) machine, single GPU resulted in approximately 3 iterations per second. This is considered the baseline for any tests on the following configurations - all of which resulted in approximately the same number of iterations per second irrespective of configuration.\nNote: in all cases, each worker has their own dedicated GPU\n\n1 PS, 2 workers on one machine\n1 PS with dedicated GPU & 1 worker on one machine, 1 worker on another machine\n1 PS & 2 workers on one machine, 2 workers on another machine\n1 PS & 2 workers on one machine, 3 more machines with 2 workers each\n4 machines with 2 workers each, two of these with 1 PS each\n4 machines, each with 1 PS & 2 workers\n\nWhen starting up the workers, each would begin training as soon as it's ready, printing information to the terminal during/after each iteration. The difference is speed is visually noticeable - when there is only 1 worker running, it's as fast as I would expect. As more workers spin up, all of the existing workers slow down. In this way, I can see (with 8 terminals on my screen) the entire cluster slowing as more workers begin.\nI monitored system stats during training to rule things out as the bottleneck and found the following for each machine:\n\nCPU usage: CPU usage is not at 100% for any machine\nGPU usage: GPU usage repeatedly flickers between 0% and ~80% - once per iteration\nNetwork usage: Network bandwidth in/out for any given machine is never more than ~80% of its capacity. the network speed is limited to 1Gbps, and it doesn't reach this cap. We raised the limit to 2Gbps and saw no increased usage or performance.\nHDD usage: Batches are loaded from disk multi-threaded. I printed to the screen during file-access and it's practically instantaneous.\n\nFor the input pipeline, I have also tried switching between tf.RandomShuffleQueue and tf.train.shuffle_batch and playing with the number of threads/min after deque etc for each of these batching methods to no avail.\nSource code / logs\nSource code files attached below - cifar_input.py has small modifications from the original input pipeline. The original cifar input code is the file cifar_input_orig.py\nresnet_distrib.zip\nThank you in advance for any light you may be able to shed on this!", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have taken the [Resnet code](https://github.com/tensorflow/models/tree/master/resnet) from the Tensorflow model zoo and distributed it as according to https://www.tensorflow.org/deploy/distributed\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04.1 LTS\r\n- **TensorFlow installed from (source or binary)**:\r\nOfficial Tensorflow-GPU docker image\r\n- **TensorFlow version (use command below)**:\r\n('v1.0.0-2378-g2259213', '1.1.0-rc0')\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nVersion 8\r\n- **GPU model and memory**:\r\nTested on multiple setups within a cluster\r\n4 machines with 2x Nvidia TitanX 12GB\r\n1 machine with 2x GeForce GTX 1080 8GB\r\n- **Exact command to reproduce**:\r\nThe commands differ slightly between machines depending on cluster configuration, but for 1 PS and 2 workers it looks like this:\r\n\r\n`CUDA_VISIBLE_DEVICES=\"\" python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"ps\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"`\r\n\r\n`CUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=0 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"`\r\n\r\n`CUDA_VISIBLE_DEVICES=0 python resnet_main.py --dataset=\"cifar10\" --train_data_path=\"/notebooks/cifar_data/data_batch*\" --train_dir=\"/notebooks/tmp/resnet_model/train\" --log_root=\"/notebooks/tmp/resnet_model\" --job_name=\"worker\" --task_index=1 --eval_data_path=\"/notebooks/cifar_data/test_batch.bin\" --eval_dir=\"/notebooks/tmp/resnet_model/test\"`\r\n\r\n\r\n### Describe the problem\r\nWhen running the Resnet model on CIFAR10 from the tf zoo in a distributed environment (asynchronous, between-graph replication), there is no performance gain. \r\n\r\nRunning Resnet 110 on a single (non-distributed) machine, single GPU resulted in approximately 3 iterations per second. This is considered the baseline for any tests on the following configurations - all of which resulted in approximately the same number of iterations per second irrespective of configuration.\r\n_Note: in all cases, each worker has their own dedicated GPU_\r\n1. 1 PS, 2 workers on one machine\r\n2. 1 PS with dedicated GPU & 1 worker on one machine, 1 worker on another machine\r\n3. 1 PS & 2 workers on one machine, 2 workers on another machine\r\n4. 1 PS & 2 workers on one machine, 3 more machines with 2 workers each\r\n5. 4 machines with 2 workers each, two of these with 1 PS each\r\n6. 4 machines, each with 1 PS & 2 workers\r\n\r\nWhen starting up the workers, each would begin training as soon as it's ready, printing information to the terminal during/after each iteration. The difference is speed is visually noticeable - when there is only 1 worker running, it's as fast as I would expect. As more workers spin up, all of the existing workers slow down. In this way, I can see (with 8 terminals on my screen) the entire cluster slowing as more workers begin.\r\n\r\nI monitored system stats during training to rule things out as the bottleneck and found the following for each machine:\r\n- CPU usage: CPU usage is not at 100% for any machine\r\n- GPU usage: GPU usage repeatedly flickers between 0% and ~80% - once per iteration\r\n- Network usage: Network bandwidth in/out for any given machine is never more than ~80% of its capacity. the network speed is limited to 1Gbps, and it doesn't reach this cap. We raised the limit to 2Gbps and saw no increased usage or performance.\r\n- HDD usage: Batches are loaded from disk multi-threaded. I printed to the screen during file-access and it's practically instantaneous.\r\n\r\nFor the input pipeline, I have also tried switching between `tf.RandomShuffleQueue` and `tf.train.shuffle_batch` and playing with the number of threads/min after deque etc for each of these batching methods to no avail.\r\n\r\n### Source code / logs\r\nSource code files attached below - `cifar_input.py` has small modifications from the original input pipeline. The original cifar input code is the file `cifar_input_orig.py`\r\n\r\n[resnet_distrib.zip](https://github.com/tensorflow/tensorflow/files/950254/resnet_distrib.zip)\r\n\r\n\r\n\r\n**Thank you in advance for any light you may be able to shed on this!**\r\n"}