{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/278941390", "html_url": "https://github.com/tensorflow/tensorflow/issues/4904#issuecomment-278941390", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4904", "id": 278941390, "node_id": "MDEyOklzc3VlQ29tbWVudDI3ODk0MTM5MA==", "user": {"login": "gouwsmeister", "id": 239602, "node_id": "MDQ6VXNlcjIzOTYwMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/239602?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gouwsmeister", "html_url": "https://github.com/gouwsmeister", "followers_url": "https://api.github.com/users/gouwsmeister/followers", "following_url": "https://api.github.com/users/gouwsmeister/following{/other_user}", "gists_url": "https://api.github.com/users/gouwsmeister/gists{/gist_id}", "starred_url": "https://api.github.com/users/gouwsmeister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gouwsmeister/subscriptions", "organizations_url": "https://api.github.com/users/gouwsmeister/orgs", "repos_url": "https://api.github.com/users/gouwsmeister/repos", "events_url": "https://api.github.com/users/gouwsmeister/events{/privacy}", "received_events_url": "https://api.github.com/users/gouwsmeister/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-10T13:26:34Z", "updated_at": "2017-02-10T13:33:20Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6018009\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lpxz\">@lpxz</a>,</p>\n<p>Regarding your first question: <code>labels</code> are used both in the softmax computation and in the sampled softmax computation. Here is another more intuitive explanation that might be useful:</p>\n<p>Let each batch of the training data consist of (inputs, labels) examples. Here, inputs may be a (vector) of integer word ids (the \"contexts\"), and labels is a vector of batch_size target-class integer ids (the \"target words\").</p>\n<p>We now assume your model has a network which maps the <code>input</code>s into fixed-length <code>dim</code>-dimensional vectors. This [batch_size, dim] matrix is the <code>inputs</code> argument to tf.nn.sampled_softmax_loss. For each of the num_classes classes, we furthermore have a (learned) <code>dim</code>-dimensional embedding vector. This [num_classes, dim] matrix is the <code>weights</code> argument (same with full softmax). Finally, we also pass in the vector of observed target-class ids described above, i.e. <code>labels</code>.</p>\n<p>So far it's the same for both the normal softmax and the sampled softmax. The only difference is then that the sampled softmax only samples <code>num_sampled</code> so-called \"negative\" (i.e. not observed) classes for each observed class in <code>labels</code>, and then computes the cross-entropy loss between the observed (labels) and not observed (sampled) classes, whereas the full softmax uses all the actual classes for the \"negative\" part.</p>\n<p>Regarding your second question: You are correct that <code>weights</code> is simply a lookup table (as defined above). The matmul computes the dot-product between each <code>dim</code>-dimensional row in <code>inputs</code>, and each <code>dim</code>-dimensional column (\"output embedding\") in <code>weights</code>. In each example, this dot product, together with the biases, represents the input \"logit\" to the softmax function (i.e. the z_j in the first equation in <a href=\"https://en.wikipedia.org/wiki/Softmax_function\" rel=\"nofollow\">https://en.wikipedia.org/wiki/Softmax_function</a>).</p>\n<p>So I think the explanations are correct as is and I'll mark this closed for now. Please let me know if you have any other questions.</p>\n<p>Stephan</p>", "body_text": "Hi @lpxz,\nRegarding your first question: labels are used both in the softmax computation and in the sampled softmax computation. Here is another more intuitive explanation that might be useful:\nLet each batch of the training data consist of (inputs, labels) examples. Here, inputs may be a (vector) of integer word ids (the \"contexts\"), and labels is a vector of batch_size target-class integer ids (the \"target words\").\nWe now assume your model has a network which maps the inputs into fixed-length dim-dimensional vectors. This [batch_size, dim] matrix is the inputs argument to tf.nn.sampled_softmax_loss. For each of the num_classes classes, we furthermore have a (learned) dim-dimensional embedding vector. This [num_classes, dim] matrix is the weights argument (same with full softmax). Finally, we also pass in the vector of observed target-class ids described above, i.e. labels.\nSo far it's the same for both the normal softmax and the sampled softmax. The only difference is then that the sampled softmax only samples num_sampled so-called \"negative\" (i.e. not observed) classes for each observed class in labels, and then computes the cross-entropy loss between the observed (labels) and not observed (sampled) classes, whereas the full softmax uses all the actual classes for the \"negative\" part.\nRegarding your second question: You are correct that weights is simply a lookup table (as defined above). The matmul computes the dot-product between each dim-dimensional row in inputs, and each dim-dimensional column (\"output embedding\") in weights. In each example, this dot product, together with the biases, represents the input \"logit\" to the softmax function (i.e. the z_j in the first equation in https://en.wikipedia.org/wiki/Softmax_function).\nSo I think the explanations are correct as is and I'll mark this closed for now. Please let me know if you have any other questions.\nStephan", "body": "Hi @lpxz,\r\n\r\nRegarding your first question: `labels` are used both in the softmax computation and in the sampled softmax computation. Here is another more intuitive explanation that might be useful: \r\n\r\nLet each batch of the training data consist of (inputs, labels) examples. Here, inputs may be a (vector) of integer word ids (the \"contexts\"), and labels is a vector of batch_size target-class integer ids (the \"target words\"). \r\n\r\nWe now assume your model has a network which maps the `input`s into fixed-length `dim`-dimensional vectors. This [batch_size, dim] matrix is the `inputs` argument to tf.nn.sampled_softmax_loss. For each of the num_classes classes, we furthermore have a (learned) `dim`-dimensional embedding vector. This [num_classes, dim] matrix is the `weights` argument (same with full softmax). Finally, we also pass in the vector of observed target-class ids described above, i.e. `labels`.\r\n\r\nSo far it's the same for both the normal softmax and the sampled softmax. The only difference is then that the sampled softmax only samples `num_sampled` so-called \"negative\" (i.e. not observed) classes for each observed class in `labels`, and then computes the cross-entropy loss between the observed (labels) and not observed (sampled) classes, whereas the full softmax uses all the actual classes for the \"negative\" part.\r\n\r\nRegarding your second question: You are correct that `weights` is simply a lookup table (as defined above). The matmul computes the dot-product between each `dim`-dimensional row in `inputs`, and each `dim`-dimensional column (\"output embedding\") in `weights`. In each example, this dot product, together with the biases, represents the input \"logit\" to the softmax function (i.e. the z_j in the first equation in https://en.wikipedia.org/wiki/Softmax_function).\r\n\r\nSo I think the explanations are correct as is and I'll mark this closed for now. Please let me know if you have any other questions.\r\n\r\nStephan\r\n\r\n"}