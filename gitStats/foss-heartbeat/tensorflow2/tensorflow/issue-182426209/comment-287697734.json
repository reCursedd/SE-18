{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287697734", "html_url": "https://github.com/tensorflow/tensorflow/issues/4904#issuecomment-287697734", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4904", "id": 287697734, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzY5NzczNA==", "user": {"login": "WalkerWalker", "id": 8481463, "node_id": "MDQ6VXNlcjg0ODE0NjM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8481463?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WalkerWalker", "html_url": "https://github.com/WalkerWalker", "followers_url": "https://api.github.com/users/WalkerWalker/followers", "following_url": "https://api.github.com/users/WalkerWalker/following{/other_user}", "gists_url": "https://api.github.com/users/WalkerWalker/gists{/gist_id}", "starred_url": "https://api.github.com/users/WalkerWalker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WalkerWalker/subscriptions", "organizations_url": "https://api.github.com/users/WalkerWalker/orgs", "repos_url": "https://api.github.com/users/WalkerWalker/repos", "events_url": "https://api.github.com/users/WalkerWalker/events{/privacy}", "received_events_url": "https://api.github.com/users/WalkerWalker/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-20T08:18:47Z", "updated_at": "2017-03-20T08:19:37Z", "author_association": "NONE", "body_html": "<p>hey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=239602\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gouwsmeister\">@gouwsmeister</a></p>\n<p>Thanks for your great explanation above.  I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!</p>\n<p><strong>Is tf.nn.sampled_softmax_loss() an implementation of negative sampling?</strong></p>\n<p>I always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.</p>\n<p>It seems like they are from different paper and <a href=\"https://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"nofollow\">What is candidate sampling</a> describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?</p>", "body_text": "hey @gouwsmeister\nThanks for your great explanation above.  I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!\nIs tf.nn.sampled_softmax_loss() an implementation of negative sampling?\nI always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.\nIt seems like they are from different paper and What is candidate sampling describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?", "body": "hey @gouwsmeister \r\n\r\nThanks for your great explanation above.  I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!\r\n\r\n**Is tf.nn.sampled_softmax_loss() an implementation of negative sampling?**\r\n\r\nI always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.\r\n\r\nIt seems like they are from different paper and [What is candidate sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf) describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?"}