{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428392879", "html_url": "https://github.com/tensorflow/tensorflow/issues/4904#issuecomment-428392879", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4904", "id": 428392879, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODM5Mjg3OQ==", "user": {"login": "Santosh-Gupta", "id": 5524261, "node_id": "MDQ6VXNlcjU1MjQyNjE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5524261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Santosh-Gupta", "html_url": "https://github.com/Santosh-Gupta", "followers_url": "https://api.github.com/users/Santosh-Gupta/followers", "following_url": "https://api.github.com/users/Santosh-Gupta/following{/other_user}", "gists_url": "https://api.github.com/users/Santosh-Gupta/gists{/gist_id}", "starred_url": "https://api.github.com/users/Santosh-Gupta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Santosh-Gupta/subscriptions", "organizations_url": "https://api.github.com/users/Santosh-Gupta/orgs", "repos_url": "https://api.github.com/users/Santosh-Gupta/repos", "events_url": "https://api.github.com/users/Santosh-Gupta/events{/privacy}", "received_events_url": "https://api.github.com/users/Santosh-Gupta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T00:00:15Z", "updated_at": "2018-10-10T00:00:15Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>hey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=239602\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gouwsmeister\">@gouwsmeister</a></p>\n<p>Thanks for your great explanation above. I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!</p>\n<p><strong>Is tf.nn.sampled_softmax_loss() an implementation of negative sampling?</strong></p>\n<p>I always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.</p>\n<p>It seems like they are from different paper and <a href=\"https://www.tensorflow.org/extras/candidate_sampling.pdf\" rel=\"nofollow\">What is candidate sampling</a> describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?</p>\n</blockquote>\n<p>Would love to hear more details as well</p>", "body_text": "hey @gouwsmeister\nThanks for your great explanation above. I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!\nIs tf.nn.sampled_softmax_loss() an implementation of negative sampling?\nI always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.\nIt seems like they are from different paper and What is candidate sampling describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?\n\nWould love to hear more details as well", "body": "> hey @gouwsmeister\r\n> \r\n> Thanks for your great explanation above. I have a further question. I'm not sure if it's a good place to ask this. If not, please let me know and I will just delete it. thanks!\r\n> \r\n> **Is tf.nn.sampled_softmax_loss() an implementation of negative sampling?**\r\n> \r\n> I always think they are different. Sampled softmax loss is a fast way to compute softmax (because the denominator of the softmax is huge) and negative sampling is ... emm ... conceptually \"randomly select just a small number of \u201cnegative\u201d words (let\u2019s say 5) to update the weights for\", i don't really know how to implement it. It's like a big black box to me. And just now some friend mentioned that we can use sampled softmax loss to implement negative sampling. I was shocked and did a little research about it.\r\n> \r\n> It seems like they are from different paper and [What is candidate sampling](https://www.tensorflow.org/extras/candidate_sampling.pdf) describes that NCE is a generalized version of subsampled softmax and NEG is simplifying case of the NCE... this is just quite confusing. So I guess this is not the same but they are closely related and comparable?\r\n\r\nWould love to hear more details as well"}