{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4643", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4643/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4643/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4643/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4643", "id": 179927542, "node_id": "MDU6SXNzdWUxNzk5Mjc1NDI=", "number": 4643, "title": "Inf/Nan in MSE/CE with very simple demo", "user": {"login": "cauchyturing", "id": 3781543, "node_id": "MDQ6VXNlcjM3ODE1NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/3781543?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cauchyturing", "html_url": "https://github.com/cauchyturing", "followers_url": "https://api.github.com/users/cauchyturing/followers", "following_url": "https://api.github.com/users/cauchyturing/following{/other_user}", "gists_url": "https://api.github.com/users/cauchyturing/gists{/gist_id}", "starred_url": "https://api.github.com/users/cauchyturing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cauchyturing/subscriptions", "organizations_url": "https://api.github.com/users/cauchyturing/orgs", "repos_url": "https://api.github.com/users/cauchyturing/repos", "events_url": "https://api.github.com/users/cauchyturing/events{/privacy}", "received_events_url": "https://api.github.com/users/cauchyturing/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-09-29T00:55:59Z", "updated_at": "2016-09-29T01:58:25Z", "closed_at": "2016-09-29T01:58:24Z", "author_association": "NONE", "body_html": "<p>Hi there,</p>\n<p>I would use a very simple LSTM demo with only forward without any BP tuning to clear the question. I pass one random batch of size (2,2,3) to a rnn_cell.LSTMcell, using 100 hidden layer, and output the mse/ce between the output and the last time step (:, -1, :). What strike me is the both the SE/CE are very strange (too large or even inf).</p>\n<p>Below is my code.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\n\nn_steps = 2\nn_visible = 3\nn_hidden = 100\n\nx = tf.placeholder(tf.float64, [None, n_steps, n_visible])\nx_ = tf.placeholder(tf.float64, [None, n_steps, n_visible])\nbatch_size = tf.placeholder(tf.int32)\n\nlow = - 4 * np.sqrt(6. / (n_hidden + n_visible))\nhigh = 4 * np.sqrt(6. / (n_hidden + n_visible))\n\n\nW2 = tf.Variable(tf.random_uniform([n_hidden ,n_visible], minval=low, maxval=high, dtype=tf.float64))\n\nb2 = tf.Variable(tf.zeros(n_visible, dtype = tf.float64))\n\n\nlstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple = True, forget_bias=1.)\ninitial_state = lstm_cell.zero_state(batch_size, tf.float64)\nstate = initial_state\n\ninputs = [tf.squeeze(input_, [1])  for input_ in tf.split(1, n_steps, x)]\nencoder = tf.nn.rnn(lstm_cell, inputs, initial_state=initial_state)\ndecoder = tf.nn.sigmoid(tf.matmul(encoder[-1][0], W2) + b2)\n\nce = -x[:,-1,:] * tf.log(decoder)+(1-x[:,-1,:])*tf.log(1 - decoder)\nse = (x[:,-1,:] - decoder)**2\nmse = tf.reduce_mean(se)\nrmse = tf.sqrt(mse)\n\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nx_train_batch = np.random.rand(2,2,3)\nfeed_dict = {x: x_train_batch, x_:x_train_batch, batch_size: 2}\nencoder_, decoder_, se_, mse_, rmse_, ce_=  sess.run([encoder, decoder, se, mse, rmse, ce], feed_dict=feed_dict)\n\n</code></pre>\n<p>The outputs are:</p>\n<pre><code>ce_\nOut[2]: \narray([[ -9.30480083e-001,  -7.45718134e-001,   1.65488117e+243],\n       [ -8.92276217e-001,  -7.66764460e-001,   5.43823895e+064]])\n\nse_\nOut[3]: \narray([[  3.66794556e-001,   2.76262193e-001,               inf],\n       [  3.48428030e-001,   2.86745693e-001,   1.23185516e+129]])\n\ndecoder_\nOut[4]: \narray([[ 0.60563566,  0.5256065 ,  0.62439037],\n       [ 0.59027793,  0.53548641,  0.69399984]])\n\nx_train_batch[:,-1,:]\nOut[5]: \narray([[ 0.93217308,  0.28895401,  0.31784797],\n       [ 0.57656244,  0.73494513,  0.57125778]])\n\n</code></pre>\n<p>The value of decoder and x seems well, but why ce_ and se_ has inf and some weird value like 1.23E+129?</p>\n<p>It is running on a Mac OS EI Captain 64 bit without GPU support.</p>\n<pre><code>tf.__version__\nOut[6]: '0.10.0'\n</code></pre>", "body_text": "Hi there,\nI would use a very simple LSTM demo with only forward without any BP tuning to clear the question. I pass one random batch of size (2,2,3) to a rnn_cell.LSTMcell, using 100 hidden layer, and output the mse/ce between the output and the last time step (:, -1, :). What strike me is the both the SE/CE are very strange (too large or even inf).\nBelow is my code.\nimport tensorflow as tf\nimport numpy as np\n\n\nn_steps = 2\nn_visible = 3\nn_hidden = 100\n\nx = tf.placeholder(tf.float64, [None, n_steps, n_visible])\nx_ = tf.placeholder(tf.float64, [None, n_steps, n_visible])\nbatch_size = tf.placeholder(tf.int32)\n\nlow = - 4 * np.sqrt(6. / (n_hidden + n_visible))\nhigh = 4 * np.sqrt(6. / (n_hidden + n_visible))\n\n\nW2 = tf.Variable(tf.random_uniform([n_hidden ,n_visible], minval=low, maxval=high, dtype=tf.float64))\n\nb2 = tf.Variable(tf.zeros(n_visible, dtype = tf.float64))\n\n\nlstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple = True, forget_bias=1.)\ninitial_state = lstm_cell.zero_state(batch_size, tf.float64)\nstate = initial_state\n\ninputs = [tf.squeeze(input_, [1])  for input_ in tf.split(1, n_steps, x)]\nencoder = tf.nn.rnn(lstm_cell, inputs, initial_state=initial_state)\ndecoder = tf.nn.sigmoid(tf.matmul(encoder[-1][0], W2) + b2)\n\nce = -x[:,-1,:] * tf.log(decoder)+(1-x[:,-1,:])*tf.log(1 - decoder)\nse = (x[:,-1,:] - decoder)**2\nmse = tf.reduce_mean(se)\nrmse = tf.sqrt(mse)\n\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nx_train_batch = np.random.rand(2,2,3)\nfeed_dict = {x: x_train_batch, x_:x_train_batch, batch_size: 2}\nencoder_, decoder_, se_, mse_, rmse_, ce_=  sess.run([encoder, decoder, se, mse, rmse, ce], feed_dict=feed_dict)\n\n\nThe outputs are:\nce_\nOut[2]: \narray([[ -9.30480083e-001,  -7.45718134e-001,   1.65488117e+243],\n       [ -8.92276217e-001,  -7.66764460e-001,   5.43823895e+064]])\n\nse_\nOut[3]: \narray([[  3.66794556e-001,   2.76262193e-001,               inf],\n       [  3.48428030e-001,   2.86745693e-001,   1.23185516e+129]])\n\ndecoder_\nOut[4]: \narray([[ 0.60563566,  0.5256065 ,  0.62439037],\n       [ 0.59027793,  0.53548641,  0.69399984]])\n\nx_train_batch[:,-1,:]\nOut[5]: \narray([[ 0.93217308,  0.28895401,  0.31784797],\n       [ 0.57656244,  0.73494513,  0.57125778]])\n\n\nThe value of decoder and x seems well, but why ce_ and se_ has inf and some weird value like 1.23E+129?\nIt is running on a Mac OS EI Captain 64 bit without GPU support.\ntf.__version__\nOut[6]: '0.10.0'", "body": "Hi there,\n\nI would use a very simple LSTM demo with only forward without any BP tuning to clear the question. I pass one random batch of size (2,2,3) to a rnn_cell.LSTMcell, using 100 hidden layer, and output the mse/ce between the output and the last time step (:, -1, :). What strike me is the both the SE/CE are very strange (too large or even inf).\n\nBelow is my code.\n\n```\nimport tensorflow as tf\nimport numpy as np\n\n\nn_steps = 2\nn_visible = 3\nn_hidden = 100\n\nx = tf.placeholder(tf.float64, [None, n_steps, n_visible])\nx_ = tf.placeholder(tf.float64, [None, n_steps, n_visible])\nbatch_size = tf.placeholder(tf.int32)\n\nlow = - 4 * np.sqrt(6. / (n_hidden + n_visible))\nhigh = 4 * np.sqrt(6. / (n_hidden + n_visible))\n\n\nW2 = tf.Variable(tf.random_uniform([n_hidden ,n_visible], minval=low, maxval=high, dtype=tf.float64))\n\nb2 = tf.Variable(tf.zeros(n_visible, dtype = tf.float64))\n\n\nlstm_cell = tf.nn.rnn_cell.LSTMCell(n_hidden, state_is_tuple = True, forget_bias=1.)\ninitial_state = lstm_cell.zero_state(batch_size, tf.float64)\nstate = initial_state\n\ninputs = [tf.squeeze(input_, [1])  for input_ in tf.split(1, n_steps, x)]\nencoder = tf.nn.rnn(lstm_cell, inputs, initial_state=initial_state)\ndecoder = tf.nn.sigmoid(tf.matmul(encoder[-1][0], W2) + b2)\n\nce = -x[:,-1,:] * tf.log(decoder)+(1-x[:,-1,:])*tf.log(1 - decoder)\nse = (x[:,-1,:] - decoder)**2\nmse = tf.reduce_mean(se)\nrmse = tf.sqrt(mse)\n\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\nx_train_batch = np.random.rand(2,2,3)\nfeed_dict = {x: x_train_batch, x_:x_train_batch, batch_size: 2}\nencoder_, decoder_, se_, mse_, rmse_, ce_=  sess.run([encoder, decoder, se, mse, rmse, ce], feed_dict=feed_dict)\n\n```\n\nThe outputs are:\n\n```\nce_\nOut[2]: \narray([[ -9.30480083e-001,  -7.45718134e-001,   1.65488117e+243],\n       [ -8.92276217e-001,  -7.66764460e-001,   5.43823895e+064]])\n\nse_\nOut[3]: \narray([[  3.66794556e-001,   2.76262193e-001,               inf],\n       [  3.48428030e-001,   2.86745693e-001,   1.23185516e+129]])\n\ndecoder_\nOut[4]: \narray([[ 0.60563566,  0.5256065 ,  0.62439037],\n       [ 0.59027793,  0.53548641,  0.69399984]])\n\nx_train_batch[:,-1,:]\nOut[5]: \narray([[ 0.93217308,  0.28895401,  0.31784797],\n       [ 0.57656244,  0.73494513,  0.57125778]])\n\n```\n\nThe value of decoder and x seems well, but why ce_ and se_ has inf and some weird value like 1.23E+129?\n\nIt is running on a Mac OS EI Captain 64 bit without GPU support.\n\n```\ntf.__version__\nOut[6]: '0.10.0'\n```\n"}