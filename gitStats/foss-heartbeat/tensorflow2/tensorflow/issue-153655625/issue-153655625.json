{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2271", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2271/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2271/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2271/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2271", "id": 153655625, "node_id": "MDU6SXNzdWUxNTM2NTU2MjU=", "number": 2271, "title": "Sudden slowdowns due to the TLB shootdowns", "user": {"login": "ivankreso", "id": 2056432, "node_id": "MDQ6VXNlcjIwNTY0MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2056432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivankreso", "html_url": "https://github.com/ivankreso", "followers_url": "https://api.github.com/users/ivankreso/followers", "following_url": "https://api.github.com/users/ivankreso/following{/other_user}", "gists_url": "https://api.github.com/users/ivankreso/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivankreso/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivankreso/subscriptions", "organizations_url": "https://api.github.com/users/ivankreso/orgs", "repos_url": "https://api.github.com/users/ivankreso/repos", "events_url": "https://api.github.com/users/ivankreso/events{/privacy}", "received_events_url": "https://api.github.com/users/ivankreso/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-05-08T13:22:13Z", "updated_at": "2016-07-25T09:27:58Z", "closed_at": "2016-07-25T09:27:58Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Arch Linux</p>\n<p>Installed version of CUDA and cuDNN: CUDA 7.5, CUDNN v4<br>\nIf installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed: <a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl</a></li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\": 0.8.0</li>\n</ol>\n<h3>Steps to reproduce</h3>\n<p>I am experiencing slowdowns due to some problem with memory caching. My system has 32GB of RAM. I am training a CNN on one large image dataset. During training at some point almost 100% of memory is cached which is expected when working with a large dataset. But then soon after TF starts to experience slowdowns (3-5x slower execution per batch). I used netdata and observed that the slowdowns are accompanied by a sudden huge spikes in TLB shootdowns. I found a way how to fix a problem from outside, the slowdowns disappear for quite some time if I just clear all the cached memory like this:</p>\n<pre><code>$ free &amp;&amp; sync &amp;&amp; echo 3 &gt; /proc/sys/vm/drop_caches &amp;&amp; free\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3454324      209124      638072    29162044    28463116\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3455692    28603760      637636      766040    28476376\n\n</code></pre>\n<p>Then later when memory is fully cached again they reappear. First row is the state at which the slowdown is happening. You can see that most memory is cached and I am actually using only around 10% of RAM.<br>\nDoes anyone have an idea what could cause these TLB shootdowns or what can I do to find out more about what is going on?</p>", "body_text": "Environment info\nOperating System: Arch Linux\nInstalled version of CUDA and cuDNN: CUDA 7.5, CUDNN v4\nIf installed from binary pip package, provide:\n\nWhich pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.version)\": 0.8.0\n\nSteps to reproduce\nI am experiencing slowdowns due to some problem with memory caching. My system has 32GB of RAM. I am training a CNN on one large image dataset. During training at some point almost 100% of memory is cached which is expected when working with a large dataset. But then soon after TF starts to experience slowdowns (3-5x slower execution per batch). I used netdata and observed that the slowdowns are accompanied by a sudden huge spikes in TLB shootdowns. I found a way how to fix a problem from outside, the slowdowns disappear for quite some time if I just clear all the cached memory like this:\n$ free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3454324      209124      638072    29162044    28463116\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3455692    28603760      637636      766040    28476376\n\n\nThen later when memory is fully cached again they reappear. First row is the state at which the slowdown is happening. You can see that most memory is cached and I am actually using only around 10% of RAM.\nDoes anyone have an idea what could cause these TLB shootdowns or what can I do to find out more about what is going on?", "body": "### Environment info\n\nOperating System: Arch Linux\n\nInstalled version of CUDA and cuDNN: CUDA 7.5, CUDNN v4\nIf installed from binary pip package, provide:\n1. Which pip package you installed: https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp34-cp34m-linux_x86_64.whl\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\": 0.8.0\n### Steps to reproduce\n\nI am experiencing slowdowns due to some problem with memory caching. My system has 32GB of RAM. I am training a CNN on one large image dataset. During training at some point almost 100% of memory is cached which is expected when working with a large dataset. But then soon after TF starts to experience slowdowns (3-5x slower execution per batch). I used netdata and observed that the slowdowns are accompanied by a sudden huge spikes in TLB shootdowns. I found a way how to fix a problem from outside, the slowdowns disappear for quite some time if I just clear all the cached memory like this:\n\n```\n$ free && sync && echo 3 > /proc/sys/vm/drop_caches && free\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3454324      209124      638072    29162044    28463116\n              total        used        free      shared  buff/cache   available\nMem:       32825492     3455692    28603760      637636      766040    28476376\n\n```\n\nThen later when memory is fully cached again they reappear. First row is the state at which the slowdown is happening. You can see that most memory is cached and I am actually using only around 10% of RAM.\nDoes anyone have an idea what could cause these TLB shootdowns or what can I do to find out more about what is going on?\n"}