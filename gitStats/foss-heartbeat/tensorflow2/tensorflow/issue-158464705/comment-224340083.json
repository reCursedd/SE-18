{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224340083", "html_url": "https://github.com/tensorflow/tensorflow/issues/2648#issuecomment-224340083", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2648", "id": 224340083, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDM0MDA4Mw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-07T16:41:17Z", "updated_at": "2016-06-07T16:41:17Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for persisting with this. I've been trying to reproduce this, but haven't managed to get the exact same failure.</p>\n<p>One potential issue is that protobuf doesn't handle messages larger than 2GB. We haven't optimized this because we haven't seen any realistic models where it is necessary to transfer this much in a single step, but it obviously shouldn't crash like this. If I set up two servers, and try to transfer a tensor larger than 2GB from one to the other, it fails with <code>SIGABRT</code> on the <em>sending</em> side. However, I haven't managed to get it to \"successfully\" send a tensor that the other side fails to retrieve.</p>\n<p>Are there any very large (&gt; ~2GB) variables in your model that could be brushing up against the protobuf limit?</p>", "body_text": "Thanks for persisting with this. I've been trying to reproduce this, but haven't managed to get the exact same failure.\nOne potential issue is that protobuf doesn't handle messages larger than 2GB. We haven't optimized this because we haven't seen any realistic models where it is necessary to transfer this much in a single step, but it obviously shouldn't crash like this. If I set up two servers, and try to transfer a tensor larger than 2GB from one to the other, it fails with SIGABRT on the sending side. However, I haven't managed to get it to \"successfully\" send a tensor that the other side fails to retrieve.\nAre there any very large (> ~2GB) variables in your model that could be brushing up against the protobuf limit?", "body": "Thanks for persisting with this. I've been trying to reproduce this, but haven't managed to get the exact same failure.\n\nOne potential issue is that protobuf doesn't handle messages larger than 2GB. We haven't optimized this because we haven't seen any realistic models where it is necessary to transfer this much in a single step, but it obviously shouldn't crash like this. If I set up two servers, and try to transfer a tensor larger than 2GB from one to the other, it fails with `SIGABRT` on the _sending_ side. However, I haven't managed to get it to \"successfully\" send a tensor that the other side fails to retrieve.\n\nAre there any very large (> ~2GB) variables in your model that could be brushing up against the protobuf limit?\n"}