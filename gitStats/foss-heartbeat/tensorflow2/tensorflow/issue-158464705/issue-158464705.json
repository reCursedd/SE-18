{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2648", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2648/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2648/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2648/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2648", "id": 158464705, "node_id": "MDU6SXNzdWUxNTg0NjQ3MDU=", "number": 2648, "title": "Saver.restore() causes segfault in distributed mode", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 28, "created_at": "2016-06-03T22:15:02Z", "updated_at": "2017-02-09T22:02:14Z", "closed_at": "2016-07-21T17:39:59Z", "author_association": "NONE", "body_html": "<p>I use a custom saver object in distributed mode that operates over a subset of the parameters in my model so that I can perform transfer learning between my models. I'm running into a situation where loading weights for one of my models causes a segfault, and I can't seem to figure out why. I'm running this code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">...</span>\nsupervisor <span class=\"pl-k\">=</span> tf.train.Supervisor(<span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>(task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>),\n                                 <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span>log_dir,\n                                 <span class=\"pl-v\">init_op</span><span class=\"pl-k\">=</span>init_op,\n                                 <span class=\"pl-v\">saver</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                 <span class=\"pl-v\">summary_op</span><span class=\"pl-k\">=</span>training_summary_op,\n                                 <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n                                 <span class=\"pl-v\">summary_writer</span><span class=\"pl-k\">=</span>summary_writer)\n\n<span class=\"pl-k\">if</span> task_index <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n    logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>waiting for session.<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">else</span>:\n    logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>starting session.<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">with</span> supervisor.prepare_or_wait_for_session(\n        server.target, <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)) <span class=\"pl-k\">as</span> sesh:\n    <span class=\"pl-k\">if</span> task_index <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            <span class=\"pl-k\">if</span> supervisor.should_stop():\n                logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>training completed<span class=\"pl-pds\">'</span></span>)\n                <span class=\"pl-k\">return</span>\n            <span class=\"pl-k\">if</span> ready.eval(sesh):\n                <span class=\"pl-k\">break</span>\n            sleep(<span class=\"pl-c1\">0.01</span>)\n    logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>session started.<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">if</span> task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-k\">if</span> isfile(save_file):\n            logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loading from save file: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> save_file.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n            saver.restore(sesh, save_file)\n        <span class=\"pl-k\">elif</span> transfer_file <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> isfile(transfer_file):\n            logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>transfering weights from file: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> transfer_file.split(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>)[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n            loader.restore(sesh, transfer_file)\n            logger.info(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>executing post transfer ops<span class=\"pl-pds\">'</span></span>)\n            sesh.run(post_transfer_ops)\n        sesh.run(is_ready)\n<span class=\"pl-c1\">...</span></pre></div>\n<p>The transfer file exists on both the parameter servers and the chief worker and running the code results in this output:</p>\n<pre><code>INFO: starting session.\nINFO: session started.\nINFO: transfering weights from file: test_train5.0.ckpt\nSegmentation fault (core dumped)\n</code></pre>\n<p>The graph I'm using is identical to one that I can deploy on a single machine without error, I can even load the weights into the local version, but even if an issue with parameter names were the issue I would still hope that it would give me an error instead of a segfault. Note that this issue does not appear for every model, though it is deterministic.</p>", "body_text": "I use a custom saver object in distributed mode that operates over a subset of the parameters in my model so that I can perform transfer learning between my models. I'm running into a situation where loading weights for one of my models causes a segfault, and I can't seem to figure out why. I'm running this code:\n...\nsupervisor = tf.train.Supervisor(is_chief=(task_index == 0),\n                                 logdir=log_dir,\n                                 init_op=init_op,\n                                 saver=None,\n                                 summary_op=training_summary_op,\n                                 global_step=global_step,\n                                 summary_writer=summary_writer)\n\nif task_index != 0:\n    logger.info('waiting for session.')\nelse:\n    logger.info('starting session.')\nwith supervisor.prepare_or_wait_for_session(\n        server.target, config=tf.ConfigProto(allow_soft_placement=True)) as sesh:\n    if task_index != 0:\n        while True:\n            if supervisor.should_stop():\n                logger.info('training completed')\n                return\n            if ready.eval(sesh):\n                break\n            sleep(0.01)\n    logger.info('session started.')\n\n    if task_index == 0:\n        if isfile(save_file):\n            logger.info('loading from save file: %s' % save_file.split('/')[-1])\n            saver.restore(sesh, save_file)\n        elif transfer_file is not None and isfile(transfer_file):\n            logger.info('transfering weights from file: %s' % transfer_file.split('/')[-1])\n            loader.restore(sesh, transfer_file)\n            logger.info('executing post transfer ops')\n            sesh.run(post_transfer_ops)\n        sesh.run(is_ready)\n...\nThe transfer file exists on both the parameter servers and the chief worker and running the code results in this output:\nINFO: starting session.\nINFO: session started.\nINFO: transfering weights from file: test_train5.0.ckpt\nSegmentation fault (core dumped)\n\nThe graph I'm using is identical to one that I can deploy on a single machine without error, I can even load the weights into the local version, but even if an issue with parameter names were the issue I would still hope that it would give me an error instead of a segfault. Note that this issue does not appear for every model, though it is deterministic.", "body": "I use a custom saver object in distributed mode that operates over a subset of the parameters in my model so that I can perform transfer learning between my models. I'm running into a situation where loading weights for one of my models causes a segfault, and I can't seem to figure out why. I'm running this code:\n\n``` python\n...\nsupervisor = tf.train.Supervisor(is_chief=(task_index == 0),\n                                 logdir=log_dir,\n                                 init_op=init_op,\n                                 saver=None,\n                                 summary_op=training_summary_op,\n                                 global_step=global_step,\n                                 summary_writer=summary_writer)\n\nif task_index != 0:\n    logger.info('waiting for session.')\nelse:\n    logger.info('starting session.')\nwith supervisor.prepare_or_wait_for_session(\n        server.target, config=tf.ConfigProto(allow_soft_placement=True)) as sesh:\n    if task_index != 0:\n        while True:\n            if supervisor.should_stop():\n                logger.info('training completed')\n                return\n            if ready.eval(sesh):\n                break\n            sleep(0.01)\n    logger.info('session started.')\n\n    if task_index == 0:\n        if isfile(save_file):\n            logger.info('loading from save file: %s' % save_file.split('/')[-1])\n            saver.restore(sesh, save_file)\n        elif transfer_file is not None and isfile(transfer_file):\n            logger.info('transfering weights from file: %s' % transfer_file.split('/')[-1])\n            loader.restore(sesh, transfer_file)\n            logger.info('executing post transfer ops')\n            sesh.run(post_transfer_ops)\n        sesh.run(is_ready)\n...\n```\n\nThe transfer file exists on both the parameter servers and the chief worker and running the code results in this output:\n\n```\nINFO: starting session.\nINFO: session started.\nINFO: transfering weights from file: test_train5.0.ckpt\nSegmentation fault (core dumped)\n```\n\nThe graph I'm using is identical to one that I can deploy on a single machine without error, I can even load the weights into the local version, but even if an issue with parameter names were the issue I would still hope that it would give me an error instead of a segfault. Note that this issue does not appear for every model, though it is deterministic.\n"}