{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14356", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14356/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14356/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14356/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14356", "id": 272133704, "node_id": "MDU6SXNzdWUyNzIxMzM3MDQ=", "number": 14356, "title": "Keras application - Tensor is not an element of this graph on eval after train", "user": {"login": "damienpontifex", "id": 1321276, "node_id": "MDQ6VXNlcjEzMjEyNzY=", "avatar_url": "https://avatars1.githubusercontent.com/u/1321276?v=4", "gravatar_id": "", "url": "https://api.github.com/users/damienpontifex", "html_url": "https://github.com/damienpontifex", "followers_url": "https://api.github.com/users/damienpontifex/followers", "following_url": "https://api.github.com/users/damienpontifex/following{/other_user}", "gists_url": "https://api.github.com/users/damienpontifex/gists{/gist_id}", "starred_url": "https://api.github.com/users/damienpontifex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/damienpontifex/subscriptions", "organizations_url": "https://api.github.com/users/damienpontifex/orgs", "repos_url": "https://api.github.com/users/damienpontifex/repos", "events_url": "https://api.github.com/users/damienpontifex/events{/privacy}", "received_events_url": "https://api.github.com/users/damienpontifex/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 18, "created_at": "2017-11-08T09:42:53Z", "updated_at": "2018-11-05T13:23:31Z", "closed_at": "2017-11-08T22:12:49Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS 10.13.1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.4.0-rc1-11-g130a514 1.4.0</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A CPU only</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using the estimator API and using <code>tf.keras.applications.VGG16</code> and it's output for transfer learning, I get an exception raised of <code>TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.</code> when the model is run a second time.</p>\n<p>This is raised when it runs the eval step after train from <code>tf.estimator.train_and_evaluate</code>. See source code for model and estimator output. This also occurs if I re-run the train_and_evaluate a second time. I am running in a Jupyter notebook and my assumption about memory is that if I do a Kernel \u279d Restart it will run a training run again without the error, but cannot be run in two executions without this.</p>\n<p>See <a href=\"https://github.com/damienpontifex/fastai-course/blob/master/deeplearning1/lesson1%2B3/DogsVsCats.ipynb\">https://github.com/damienpontifex/fastai-course/blob/master/deeplearning1/lesson1%2B3/DogsVsCats.ipynb</a> for full notebook, but main parts for estimator model and output are below:</p>\n<h3>Source code / logs</h3>\n<h4>Estimator Model</h4>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">vgg16_model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">params</span>):\n    \n    is_training <span class=\"pl-k\">=</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>\n    \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>vgg_base<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Use a pre-trained VGG16 model and drop off the top layers as we will retrain </span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> with our own dense output for our custom classes</span>\n        vgg16_base <span class=\"pl-k\">=</span> tf.keras.applications.VGG16(\n            <span class=\"pl-v\">include_top</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n            <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">3</span>),\n            <span class=\"pl-v\">input_tensor</span><span class=\"pl-k\">=</span>features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image<span class=\"pl-pds\">'</span></span>],\n            <span class=\"pl-v\">pooling</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>avg<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Disable training for all layers to increase speed for transfer learning</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> If new classes significantely different from ImageNet, this may be worth leaving as trainable = True</span>\n        <span class=\"pl-k\">for</span> layer <span class=\"pl-k\">in</span> vgg16_base.layers:\n            layer.trainable <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\n        x <span class=\"pl-k\">=</span> vgg16_base.output\n    \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fc<span class=\"pl-pds\">\"</span></span>):\n        x <span class=\"pl-k\">=</span> tf.layers.flatten(x)\n        x <span class=\"pl-k\">=</span> tf.layers.dense(x, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4096</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc1<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.dense(x, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4096</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.relu, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fc2<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.layers.dropout(x, <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span>is_training)\n        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Finally add a 2 dense layer for class predictions</span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Prediction<span class=\"pl-pds\">\"</span></span>):\n        x <span class=\"pl-k\">=</span> tf.layers.dense(x, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">NUM_CLASSES</span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span>is_training)\n        <span class=\"pl-k\">return</span> x</pre></div>\n<h4>Estimator setup</h4>\n<div class=\"highlight highlight-source-python\"><pre>dog_cat_estimator <span class=\"pl-k\">=</span> tf.estimator.Estimator(\n    <span class=\"pl-v\">model_fn</span><span class=\"pl-k\">=</span>model_fn,\n    <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>run_config,\n    <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>params\n)\ntrain_spec <span class=\"pl-k\">=</span> tf.estimator.TrainSpec(\n    <span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>data_input_fn(train_record_filenames, <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), \n    <span class=\"pl-v\">max_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\neval_spec <span class=\"pl-k\">=</span> tf.estimator.EvalSpec(\n    <span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span>data_input_fn(validation_record_filenames)\n)\ntf.estimator.train_and_evaluate(dog_cat_estimator, train_spec, eval_spec)</pre></div>\n<h4>train_and_evaluate output</h4>\n<pre><code>INFO:tensorflow:Running training and evaluation locally (non-distributed).\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Restoring parameters from /tmp/DogsVsCats/model.ckpt-1\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/DogsVsCats/model.ckpt.\nINFO:tensorflow:loss = 0.0, step = 2\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/DogsVsCats/model.ckpt.\nINFO:tensorflow:Loss for final step: 0.0.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1063             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n-&gt; 1064                                                     allow_operation=False)\n   1065           except Exception as e:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\n   3034     with self._lock:\n-&gt; 3035       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n   3036 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\n   3113       if obj.graph is not self:\n-&gt; 3114         raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\n   3115       return obj\n\nValueError: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\n&lt;ipython-input-12-67c818ea66c5&gt; in &lt;module&gt;()\n----&gt; 1 tf.estimator.train_and_evaluate(dog_cat_estimator, train_spec, eval_spec)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\n    428       config.task_type != run_config_lib.TaskType.EVALUATOR):\n    429     logging.info('Running training and evaluation locally (non-distributed).')\n--&gt; 430     executor.run_local()\n    431     return\n    432 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run_local(self)\n    614       # condition is satisfied (both checks use the same global_step value,\n    615       # i.e., no race condition)\n--&gt; 616       metrics = evaluator.evaluate_and_export()\n    617 \n    618       if not metrics:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in evaluate_and_export(self)\n    749           name=self._eval_spec.name,\n    750           checkpoint_path=latest_ckpt_path,\n--&gt; 751           hooks=self._eval_spec.hooks)\n    752 \n    753       if not eval_result:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in evaluate(self, input_fn, steps, hooks, checkpoint_path, name)\n    353         hooks=hooks,\n    354         checkpoint_path=checkpoint_path,\n--&gt; 355         name=name)\n    356 \n    357   def _convert_eval_steps_to_hooks(self, steps):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _evaluate_model(self, input_fn, hooks, checkpoint_path, name)\n    808           input_fn, model_fn_lib.ModeKeys.EVAL)\n    809       estimator_spec = self._call_model_fn(\n--&gt; 810           features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\n    811 \n    812       if model_fn_lib.LOSS_METRIC_KEY in estimator_spec.eval_metric_ops:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\n    692     if 'config' in model_fn_args:\n    693       kwargs['config'] = config\n--&gt; 694     model_fn_results = self._model_fn(features=features, **kwargs)\n    695 \n    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\n\n&lt;ipython-input-8-e251e8b8fccf&gt; in model_fn(features, labels, mode, params)\n      3     tf.summary.image('images', features['image'], max_outputs=6)\n      4 \n----&gt; 5     logits = vgg16_model_fn(features, mode, params)\n      6 \n      7     # Dictionary with label as outcome with greatest probability\n\n&lt;ipython-input-7-93330b8a5aa6&gt; in vgg16_model_fn(features, mode, params)\n     10             input_shape=(224, 224, 3),\n     11             input_tensor=features['image'],\n---&gt; 12             pooling='avg')\n     13 \n     14         # Disable training for all layers to increase speed for transfer learning\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/applications/vgg16.py in VGG16(include_top, weights, input_tensor, input_shape, pooling, classes)\n    199           WEIGHTS_PATH_NO_TOP,\n    200           cache_subdir='models')\n--&gt; 201     model.load_weights(weights_path)\n    202     if K.backend() == 'theano':\n    203       layer_utils.convert_all_kernels_in_model(model)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)\n   1097       load_weights_from_hdf5_group_by_name(f, self.layers)\n   1098     else:\n-&gt; 1099       load_weights_from_hdf5_group(f, self.layers)\n   1100 \n   1101     if hasattr(f, 'close'):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights_from_hdf5_group(f, layers)\n   1484                        str(len(weight_values)) + ' elements.')\n   1485     weight_value_tuples += zip(symbolic_weights, weight_values)\n-&gt; 1486   K.batch_set_value(weight_value_tuples)\n   1487 \n   1488 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py in batch_set_value(tuples)\n   2404       assign_ops.append(assign_op)\n   2405       feed_dict[assign_placeholder] = value\n-&gt; 2406     get_session().run(assign_ops, feed_dict=feed_dict)\n   2407 \n   2408 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    887     try:\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\n--&gt; 889                          run_metadata_ptr)\n    890       if run_metadata:\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1065           except Exception as e:\n   1066             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n-&gt; 1067                             + e.args[0])\n   1068 \n   1069           if isinstance(subfeed_val, ops.Tensor):\n\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.1\nTensorFlow installed from (source or binary): pip\nTensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0\nPython version: 3.6.3\nCUDA/cuDNN version: N/A CPU only\nExact command to reproduce:\n\nDescribe the problem\nUsing the estimator API and using tf.keras.applications.VGG16 and it's output for transfer learning, I get an exception raised of TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph. when the model is run a second time.\nThis is raised when it runs the eval step after train from tf.estimator.train_and_evaluate. See source code for model and estimator output. This also occurs if I re-run the train_and_evaluate a second time. I am running in a Jupyter notebook and my assumption about memory is that if I do a Kernel \u279d Restart it will run a training run again without the error, but cannot be run in two executions without this.\nSee https://github.com/damienpontifex/fastai-course/blob/master/deeplearning1/lesson1%2B3/DogsVsCats.ipynb for full notebook, but main parts for estimator model and output are below:\nSource code / logs\nEstimator Model\ndef vgg16_model_fn(features, mode, params):\n    \n    is_training = mode == tf.estimator.ModeKeys.TRAIN\n    \n    with tf.variable_scope('vgg_base'):\n        # Use a pre-trained VGG16 model and drop off the top layers as we will retrain \n        # with our own dense output for our custom classes\n        vgg16_base = tf.keras.applications.VGG16(\n            include_top=False,\n            input_shape=(224, 224, 3),\n            input_tensor=features['image'],\n            pooling='avg')\n\n        # Disable training for all layers to increase speed for transfer learning\n        # If new classes significantely different from ImageNet, this may be worth leaving as trainable = True\n        for layer in vgg16_base.layers:\n            layer.trainable = False\n\n        x = vgg16_base.output\n    \n    with tf.variable_scope(\"fc\"):\n        x = tf.layers.flatten(x)\n        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu, trainable=is_training, name='fc1')\n        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu, trainable=is_training, name='fc2')\n        x = tf.layers.dropout(x, rate=0.5, training=is_training)\n        \n    # Finally add a 2 dense layer for class predictions\n    with tf.variable_scope(\"Prediction\"):\n        x = tf.layers.dense(x, units=NUM_CLASSES, trainable=is_training)\n        return x\nEstimator setup\ndog_cat_estimator = tf.estimator.Estimator(\n    model_fn=model_fn,\n    config=run_config,\n    params=params\n)\ntrain_spec = tf.estimator.TrainSpec(\n    input_fn=data_input_fn(train_record_filenames, num_epochs=None, batch_size=10, shuffle=True), \n    max_steps=10)\neval_spec = tf.estimator.EvalSpec(\n    input_fn=data_input_fn(validation_record_filenames)\n)\ntf.estimator.train_and_evaluate(dog_cat_estimator, train_spec, eval_spec)\ntrain_and_evaluate output\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Restoring parameters from /tmp/DogsVsCats/model.ckpt-1\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/DogsVsCats/model.ckpt.\nINFO:tensorflow:loss = 0.0, step = 2\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/DogsVsCats/model.ckpt.\nINFO:tensorflow:Loss for final step: 0.0.\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1063             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\n-> 1064                                                     allow_operation=False)\n   1065           except Exception as e:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\n   3034     with self._lock:\n-> 3035       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\n   3036 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\n   3113       if obj.graph is not self:\n-> 3114         raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\n   3115       return obj\n\nValueError: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\n\nDuring handling of the above exception, another exception occurred:\n\nTypeError                                 Traceback (most recent call last)\n<ipython-input-12-67c818ea66c5> in <module>()\n----> 1 tf.estimator.train_and_evaluate(dog_cat_estimator, train_spec, eval_spec)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\n    428       config.task_type != run_config_lib.TaskType.EVALUATOR):\n    429     logging.info('Running training and evaluation locally (non-distributed).')\n--> 430     executor.run_local()\n    431     return\n    432 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run_local(self)\n    614       # condition is satisfied (both checks use the same global_step value,\n    615       # i.e., no race condition)\n--> 616       metrics = evaluator.evaluate_and_export()\n    617 \n    618       if not metrics:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in evaluate_and_export(self)\n    749           name=self._eval_spec.name,\n    750           checkpoint_path=latest_ckpt_path,\n--> 751           hooks=self._eval_spec.hooks)\n    752 \n    753       if not eval_result:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in evaluate(self, input_fn, steps, hooks, checkpoint_path, name)\n    353         hooks=hooks,\n    354         checkpoint_path=checkpoint_path,\n--> 355         name=name)\n    356 \n    357   def _convert_eval_steps_to_hooks(self, steps):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _evaluate_model(self, input_fn, hooks, checkpoint_path, name)\n    808           input_fn, model_fn_lib.ModeKeys.EVAL)\n    809       estimator_spec = self._call_model_fn(\n--> 810           features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\n    811 \n    812       if model_fn_lib.LOSS_METRIC_KEY in estimator_spec.eval_metric_ops:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\n    692     if 'config' in model_fn_args:\n    693       kwargs['config'] = config\n--> 694     model_fn_results = self._model_fn(features=features, **kwargs)\n    695 \n    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\n\n<ipython-input-8-e251e8b8fccf> in model_fn(features, labels, mode, params)\n      3     tf.summary.image('images', features['image'], max_outputs=6)\n      4 \n----> 5     logits = vgg16_model_fn(features, mode, params)\n      6 \n      7     # Dictionary with label as outcome with greatest probability\n\n<ipython-input-7-93330b8a5aa6> in vgg16_model_fn(features, mode, params)\n     10             input_shape=(224, 224, 3),\n     11             input_tensor=features['image'],\n---> 12             pooling='avg')\n     13 \n     14         # Disable training for all layers to increase speed for transfer learning\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/applications/vgg16.py in VGG16(include_top, weights, input_tensor, input_shape, pooling, classes)\n    199           WEIGHTS_PATH_NO_TOP,\n    200           cache_subdir='models')\n--> 201     model.load_weights(weights_path)\n    202     if K.backend() == 'theano':\n    203       layer_utils.convert_all_kernels_in_model(model)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)\n   1097       load_weights_from_hdf5_group_by_name(f, self.layers)\n   1098     else:\n-> 1099       load_weights_from_hdf5_group(f, self.layers)\n   1100 \n   1101     if hasattr(f, 'close'):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights_from_hdf5_group(f, layers)\n   1484                        str(len(weight_values)) + ' elements.')\n   1485     weight_value_tuples += zip(symbolic_weights, weight_values)\n-> 1486   K.batch_set_value(weight_value_tuples)\n   1487 \n   1488 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py in batch_set_value(tuples)\n   2404       assign_ops.append(assign_op)\n   2405       feed_dict[assign_placeholder] = value\n-> 2406     get_session().run(assign_ops, feed_dict=feed_dict)\n   2407 \n   2408 \n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    887     try:\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 889                          run_metadata_ptr)\n    890       if run_metadata:\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n   1065           except Exception as e:\n   1066             raise TypeError('Cannot interpret feed_dict key as Tensor: '\n-> 1067                             + e.args[0])\n   1068 \n   1069           if isinstance(subfeed_val, ops.Tensor):\n\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.1\r\n- **TensorFlow installed from (source or binary)**: pip\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: 3.6.3\r\n- **CUDA/cuDNN version**: N/A CPU only\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nUsing the estimator API and using `tf.keras.applications.VGG16` and it's output for transfer learning, I get an exception raised of `TypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.` when the model is run a second time. \r\n\r\nThis is raised when it runs the eval step after train from `tf.estimator.train_and_evaluate`. See source code for model and estimator output. This also occurs if I re-run the train_and_evaluate a second time. I am running in a Jupyter notebook and my assumption about memory is that if I do a Kernel \u279d Restart it will run a training run again without the error, but cannot be run in two executions without this.\r\n\r\nSee https://github.com/damienpontifex/fastai-course/blob/master/deeplearning1/lesson1%2B3/DogsVsCats.ipynb for full notebook, but main parts for estimator model and output are below:\r\n\r\n### Source code / logs\r\n#### Estimator Model\r\n```python\r\ndef vgg16_model_fn(features, mode, params):\r\n    \r\n    is_training = mode == tf.estimator.ModeKeys.TRAIN\r\n    \r\n    with tf.variable_scope('vgg_base'):\r\n        # Use a pre-trained VGG16 model and drop off the top layers as we will retrain \r\n        # with our own dense output for our custom classes\r\n        vgg16_base = tf.keras.applications.VGG16(\r\n            include_top=False,\r\n            input_shape=(224, 224, 3),\r\n            input_tensor=features['image'],\r\n            pooling='avg')\r\n\r\n        # Disable training for all layers to increase speed for transfer learning\r\n        # If new classes significantely different from ImageNet, this may be worth leaving as trainable = True\r\n        for layer in vgg16_base.layers:\r\n            layer.trainable = False\r\n\r\n        x = vgg16_base.output\r\n    \r\n    with tf.variable_scope(\"fc\"):\r\n        x = tf.layers.flatten(x)\r\n        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu, trainable=is_training, name='fc1')\r\n        x = tf.layers.dense(x, units=4096, activation=tf.nn.relu, trainable=is_training, name='fc2')\r\n        x = tf.layers.dropout(x, rate=0.5, training=is_training)\r\n        \r\n    # Finally add a 2 dense layer for class predictions\r\n    with tf.variable_scope(\"Prediction\"):\r\n        x = tf.layers.dense(x, units=NUM_CLASSES, trainable=is_training)\r\n        return x\r\n```\r\n#### Estimator setup\r\n```python\r\ndog_cat_estimator = tf.estimator.Estimator(\r\n    model_fn=model_fn,\r\n    config=run_config,\r\n    params=params\r\n)\r\ntrain_spec = tf.estimator.TrainSpec(\r\n    input_fn=data_input_fn(train_record_filenames, num_epochs=None, batch_size=10, shuffle=True), \r\n    max_steps=10)\r\neval_spec = tf.estimator.EvalSpec(\r\n    input_fn=data_input_fn(validation_record_filenames)\r\n)\r\ntf.estimator.train_and_evaluate(dog_cat_estimator, train_spec, eval_spec)\r\n```\r\n#### train_and_evaluate output\r\n```\r\nINFO:tensorflow:Running training and evaluation locally (non-distributed).\r\nINFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Restoring parameters from /tmp/DogsVsCats/model.ckpt-1\r\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/DogsVsCats/model.ckpt.\r\nINFO:tensorflow:loss = 0.0, step = 2\r\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/DogsVsCats/model.ckpt.\r\nINFO:tensorflow:Loss for final step: 0.0.\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1063             subfeed_t = self.graph.as_graph_element(subfeed, allow_tensor=True,\r\n-> 1064                                                     allow_operation=False)\r\n   1065           except Exception as e:\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in as_graph_element(self, obj, allow_tensor, allow_operation)\r\n   3034     with self._lock:\r\n-> 3035       return self._as_graph_element_locked(obj, allow_tensor, allow_operation)\r\n   3036 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in _as_graph_element_locked(self, obj, allow_tensor, allow_operation)\r\n   3113       if obj.graph is not self:\r\n-> 3114         raise ValueError(\"Tensor %s is not an element of this graph.\" % obj)\r\n   3115       return obj\r\n\r\nValueError: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-12-67c818ea66c5> in <module>()\r\n----> 1 tf.estimator.train_and_evaluate(dog_cat_estimator, train_spec, eval_spec)\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in train_and_evaluate(estimator, train_spec, eval_spec)\r\n    428       config.task_type != run_config_lib.TaskType.EVALUATOR):\r\n    429     logging.info('Running training and evaluation locally (non-distributed).')\r\n--> 430     executor.run_local()\r\n    431     return\r\n    432 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in run_local(self)\r\n    614       # condition is satisfied (both checks use the same global_step value,\r\n    615       # i.e., no race condition)\r\n--> 616       metrics = evaluator.evaluate_and_export()\r\n    617 \r\n    618       if not metrics:\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/training.py in evaluate_and_export(self)\r\n    749           name=self._eval_spec.name,\r\n    750           checkpoint_path=latest_ckpt_path,\r\n--> 751           hooks=self._eval_spec.hooks)\r\n    752 \r\n    753       if not eval_result:\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in evaluate(self, input_fn, steps, hooks, checkpoint_path, name)\r\n    353         hooks=hooks,\r\n    354         checkpoint_path=checkpoint_path,\r\n--> 355         name=name)\r\n    356 \r\n    357   def _convert_eval_steps_to_hooks(self, steps):\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _evaluate_model(self, input_fn, hooks, checkpoint_path, name)\r\n    808           input_fn, model_fn_lib.ModeKeys.EVAL)\r\n    809       estimator_spec = self._call_model_fn(\r\n--> 810           features, labels, model_fn_lib.ModeKeys.EVAL, self.config)\r\n    811 \r\n    812       if model_fn_lib.LOSS_METRIC_KEY in estimator_spec.eval_metric_ops:\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/estimator/estimator.py in _call_model_fn(self, features, labels, mode, config)\r\n    692     if 'config' in model_fn_args:\r\n    693       kwargs['config'] = config\r\n--> 694     model_fn_results = self._model_fn(features=features, **kwargs)\r\n    695 \r\n    696     if not isinstance(model_fn_results, model_fn_lib.EstimatorSpec):\r\n\r\n<ipython-input-8-e251e8b8fccf> in model_fn(features, labels, mode, params)\r\n      3     tf.summary.image('images', features['image'], max_outputs=6)\r\n      4 \r\n----> 5     logits = vgg16_model_fn(features, mode, params)\r\n      6 \r\n      7     # Dictionary with label as outcome with greatest probability\r\n\r\n<ipython-input-7-93330b8a5aa6> in vgg16_model_fn(features, mode, params)\r\n     10             input_shape=(224, 224, 3),\r\n     11             input_tensor=features['image'],\r\n---> 12             pooling='avg')\r\n     13 \r\n     14         # Disable training for all layers to increase speed for transfer learning\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/applications/vgg16.py in VGG16(include_top, weights, input_tensor, input_shape, pooling, classes)\r\n    199           WEIGHTS_PATH_NO_TOP,\r\n    200           cache_subdir='models')\r\n--> 201     model.load_weights(weights_path)\r\n    202     if K.backend() == 'theano':\r\n    203       layer_utils.convert_all_kernels_in_model(model)\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights(self, filepath, by_name)\r\n   1097       load_weights_from_hdf5_group_by_name(f, self.layers)\r\n   1098     else:\r\n-> 1099       load_weights_from_hdf5_group(f, self.layers)\r\n   1100 \r\n   1101     if hasattr(f, 'close'):\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/engine/topology.py in load_weights_from_hdf5_group(f, layers)\r\n   1484                        str(len(weight_values)) + ' elements.')\r\n   1485     weight_value_tuples += zip(symbolic_weights, weight_values)\r\n-> 1486   K.batch_set_value(weight_value_tuples)\r\n   1487 \r\n   1488 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/_impl/keras/backend.py in batch_set_value(tuples)\r\n   2404       assign_ops.append(assign_op)\r\n   2405       feed_dict[assign_placeholder] = value\r\n-> 2406     get_session().run(assign_ops, feed_dict=feed_dict)\r\n   2407 \r\n   2408 \r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\r\n    887     try:\r\n    888       result = self._run(None, fetches, feed_dict, options_ptr,\r\n--> 889                          run_metadata_ptr)\r\n    890       if run_metadata:\r\n    891         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\r\n   1065           except Exception as e:\r\n   1066             raise TypeError('Cannot interpret feed_dict key as Tensor: '\r\n-> 1067                             + e.args[0])\r\n   1068 \r\n   1069           if isinstance(subfeed_val, ops.Tensor):\r\n\r\nTypeError: Cannot interpret feed_dict key as Tensor: Tensor Tensor(\"vgg_base/Placeholder:0\", shape=(3, 3, 3, 64), dtype=float32) is not an element of this graph.\r\n```\r\n"}