{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12294", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12294/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12294/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12294/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12294", "id": 250303745, "node_id": "MDU6SXNzdWUyNTAzMDM3NDU=", "number": 12294, "title": "Computation of Mean IoU", "user": {"login": "Sylvus", "id": 3027286, "node_id": "MDQ6VXNlcjMwMjcyODY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3027286?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Sylvus", "html_url": "https://github.com/Sylvus", "followers_url": "https://api.github.com/users/Sylvus/followers", "following_url": "https://api.github.com/users/Sylvus/following{/other_user}", "gists_url": "https://api.github.com/users/Sylvus/gists{/gist_id}", "starred_url": "https://api.github.com/users/Sylvus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Sylvus/subscriptions", "organizations_url": "https://api.github.com/users/Sylvus/orgs", "repos_url": "https://api.github.com/users/Sylvus/repos", "events_url": "https://api.github.com/users/Sylvus/events{/privacy}", "received_events_url": "https://api.github.com/users/Sylvus/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-08-15T12:43:05Z", "updated_at": "2017-11-26T16:38:53Z", "closed_at": "2017-11-26T16:38:53Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nWindows 7</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nSource.</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nr1.2</li>\n<li><strong>Python version</strong>:<br>\n3.5</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I believe that the calculation of the IoU metric in tf.metrics.mean_iou is incorrect. Consider an image which can have 3 classes (sky, road, building). In one image there is no building and everything has been perfectly classified as either sky or road so the tp + fn + fp for buildings is zero. To avoid divisions by zero in the IoU calculation (tp / (tp + fn + fp)) the current implementation sets the denominator to 1. This causes a result of 0 because tp=0 and we are computing 0=0/1.</p>\n<p>Both the sky and road class get a score of 1 so the overall image gets a mean_iou score of 2/3 even though every single pixel was correctly classified. I propose to change the metric so that when the denominator is 0 the result is 1. I can send a pull request if this is needed.</p>\n<h3>Source code / logs</h3>\n<p>Complete example as a unit test:</p>\n<pre><code>class TestMetrics(tf.test.TestCase):\n\n    def get_data(self):\n        label_int64 = tf.constant([\n            [[0, 1, 2, 2, 1, 0],\n             [2, 1, 0, 0, 1, 2]],\n            [[2, 2, 2, 2, 2, 2],\n             [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)\n\n        predicted_perfect_int64 = tf.constant([\n            [[1, 1, 2, 2, 1, 2],\n             [2, 1, 9, 3, 1, 2]],\n            [[2, 2, 2, 2, 2, 2],\n             [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)\n        return label_int64, predicted_perfect_int64\n\n\n    def test_iou_metric(self):\n        with self.test_session():\n            l, p = self.get_data()\n            num_classes = 9\n            weights = tf.cast(tf.not_equal(l, 0), tf.int32)\n\n            iou, update_op = tf.metrics.mean_iou(\n                l - 1, p - 1, num_classes, weights=weights)\n            tf.global_variables_initializer().run()\n            tf.local_variables_initializer().run()\n            update_op.eval()\n            self.assertAlmostEqual(iou.eval(), 1)\n</code></pre>\n<p>Currently this test fails with: 0.222 is not equal to 1.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nWindows 7\nTensorFlow installed from (source or binary):\nSource.\nTensorFlow version (use command below):\nr1.2\nPython version:\n3.5\n\nDescribe the problem\nI believe that the calculation of the IoU metric in tf.metrics.mean_iou is incorrect. Consider an image which can have 3 classes (sky, road, building). In one image there is no building and everything has been perfectly classified as either sky or road so the tp + fn + fp for buildings is zero. To avoid divisions by zero in the IoU calculation (tp / (tp + fn + fp)) the current implementation sets the denominator to 1. This causes a result of 0 because tp=0 and we are computing 0=0/1.\nBoth the sky and road class get a score of 1 so the overall image gets a mean_iou score of 2/3 even though every single pixel was correctly classified. I propose to change the metric so that when the denominator is 0 the result is 1. I can send a pull request if this is needed.\nSource code / logs\nComplete example as a unit test:\nclass TestMetrics(tf.test.TestCase):\n\n    def get_data(self):\n        label_int64 = tf.constant([\n            [[0, 1, 2, 2, 1, 0],\n             [2, 1, 0, 0, 1, 2]],\n            [[2, 2, 2, 2, 2, 2],\n             [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)\n\n        predicted_perfect_int64 = tf.constant([\n            [[1, 1, 2, 2, 1, 2],\n             [2, 1, 9, 3, 1, 2]],\n            [[2, 2, 2, 2, 2, 2],\n             [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)\n        return label_int64, predicted_perfect_int64\n\n\n    def test_iou_metric(self):\n        with self.test_session():\n            l, p = self.get_data()\n            num_classes = 9\n            weights = tf.cast(tf.not_equal(l, 0), tf.int32)\n\n            iou, update_op = tf.metrics.mean_iou(\n                l - 1, p - 1, num_classes, weights=weights)\n            tf.global_variables_initializer().run()\n            tf.local_variables_initializer().run()\n            update_op.eval()\n            self.assertAlmostEqual(iou.eval(), 1)\n\nCurrently this test fails with: 0.222 is not equal to 1.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNo.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 7\r\n- **TensorFlow installed from (source or binary)**:\r\nSource.\r\n- **TensorFlow version (use command below)**:\r\nr1.2\r\n- **Python version**: \r\n3.5\r\n\r\n### Describe the problem\r\n\r\nI believe that the calculation of the IoU metric in tf.metrics.mean_iou is incorrect. Consider an image which can have 3 classes (sky, road, building). In one image there is no building and everything has been perfectly classified as either sky or road so the tp + fn + fp for buildings is zero. To avoid divisions by zero in the IoU calculation (tp / (tp + fn + fp)) the current implementation sets the denominator to 1. This causes a result of 0 because tp=0 and we are computing 0=0/1. \r\n\r\nBoth the sky and road class get a score of 1 so the overall image gets a mean_iou score of 2/3 even though every single pixel was correctly classified. I propose to change the metric so that when the denominator is 0 the result is 1. I can send a pull request if this is needed. \r\n\r\n### Source code / logs\r\n\r\nComplete example as a unit test:\r\n\r\n    class TestMetrics(tf.test.TestCase):\r\n\r\n        def get_data(self):\r\n            label_int64 = tf.constant([\r\n                [[0, 1, 2, 2, 1, 0],\r\n                 [2, 1, 0, 0, 1, 2]],\r\n                [[2, 2, 2, 2, 2, 2],\r\n                 [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)\r\n\r\n            predicted_perfect_int64 = tf.constant([\r\n                [[1, 1, 2, 2, 1, 2],\r\n                 [2, 1, 9, 3, 1, 2]],\r\n                [[2, 2, 2, 2, 2, 2],\r\n                 [1, 1, 1, 1, 1, 1]]], dtype=tf.int64)\r\n            return label_int64, predicted_perfect_int64\r\n\r\n\r\n        def test_iou_metric(self):\r\n            with self.test_session():\r\n                l, p = self.get_data()\r\n                num_classes = 9\r\n                weights = tf.cast(tf.not_equal(l, 0), tf.int32)\r\n\r\n                iou, update_op = tf.metrics.mean_iou(\r\n                    l - 1, p - 1, num_classes, weights=weights)\r\n                tf.global_variables_initializer().run()\r\n                tf.local_variables_initializer().run()\r\n                update_op.eval()\r\n                self.assertAlmostEqual(iou.eval(), 1)\r\n\r\nCurrently this test fails with: 0.222 is not equal to 1.\r\n"}