{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18642", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18642/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18642/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18642/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18642", "id": 315405202, "node_id": "MDU6SXNzdWUzMTU0MDUyMDI=", "number": 18642, "title": "Keras models fit() method not converging with eager execution", "user": {"login": "fantauzzi", "id": 2722433, "node_id": "MDQ6VXNlcjI3MjI0MzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/2722433?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fantauzzi", "html_url": "https://github.com/fantauzzi", "followers_url": "https://api.github.com/users/fantauzzi/followers", "following_url": "https://api.github.com/users/fantauzzi/following{/other_user}", "gists_url": "https://api.github.com/users/fantauzzi/gists{/gist_id}", "starred_url": "https://api.github.com/users/fantauzzi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fantauzzi/subscriptions", "organizations_url": "https://api.github.com/users/fantauzzi/orgs", "repos_url": "https://api.github.com/users/fantauzzi/repos", "events_url": "https://api.github.com/users/fantauzzi/events{/privacy}", "received_events_url": "https://api.github.com/users/fantauzzi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-04-18T09:40:46Z", "updated_at": "2018-04-19T16:28:09Z", "closed_at": "2018-04-19T16:28:08Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes, see below.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary, with pip3</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nv1.7.0-3-g024aecf414 1.7.0</li>\n<li><strong>Python version</strong>:<br>\n3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.0.176/7.0.5</li>\n<li><strong>GPU model and memory</strong>:<br>\nGTX 1080 (Armor MSI) with 8 GB VRAM</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nRun program below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>After enabling eager execution, method <code>tf.keras.models.Sequential.fit()</code> doesn't seem to converge, as the computed loss doesn't go down (stays around 9 to 11); also, method <code>fit()</code> doesn't report the requested metric \"accuracy\": it doesn't print the metric to console, and does not return it in the History object.</p>\n<p>After disabling eager execution, the same optimization converges, as loss goes down (to around 1); also, method <code>fit()</code> correctly reports the requested metric \"accuracy\", both to console and in the returned History object.</p>\n<h3>Source code / logs</h3>\n<pre><code>import os\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\n\"\"\"\nCheck the beginning of main() for parameters\n\"\"\"\n\n\ndef load_data(folder):\n    file_name = os.path.join(folder, 'winequality-data.csv')\n    if os.path.isfile(file_name):\n        data = pd.read_csv(file_name)\n    else:\n        print('File {} not found.'.format(file_name, folder))\n        print('Dataset can be downloaded from https://www.kaggle.com/c/uci-wine-quality-dataset/data')\n        exit(1)\n    # solutions = pd.read_csv(os.path.join(folder, 'winequality-solution-input.csv'))\n    return data\n\n\ndef train_input_fn(features, labels, batch_size):\n    features_tensor = tf.constant(features)\n    labels_tensor = tf.constant(labels)\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((features_tensor, labels_tensor))\n\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(len(labels)).repeat(count=1).batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\n\ndef loss(model, X, y):\n    logits = model(X)\n    the_loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n    return the_loss\n\n\ndef grad(model, inputs, targets):\n    with tfe.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets)\n    return tape.gradient(loss_value, model.variables)\n\n\ndef train(model, X, y, batch_size, epochs):\n    train_ds = train_input_fn(X, y, batch_size=batch_size)\n    optimizer = tf.train.AdamOptimizer()\n\n    loss_by_epoch = []\n    accuracy_by_epoch = []\n\n    for epoch in range(epochs):\n        epoch_loss_avg = tfe.metrics.Mean()\n        epoch_error = tfe.metrics.Mean()\n        for batch, (batch_X, batch_y) in enumerate(tfe.Iterator(train_ds)):\n            grads = grad(model, batch_X, batch_y)\n            optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n            epoch_loss_avg(loss(model, batch_X, batch_y))\n            correct_prediction = tf.equal(tf.argmax(model(batch_X), axis=1, output_type=tf.int32), batch_y)\n            epoch_error(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n        print('Epoch {}:  loss={}  accuracy={}'.format(epoch, epoch_loss_avg.result(), epoch_error.result()))\n        loss_by_epoch.append(epoch_loss_avg.result())\n        accuracy_by_epoch.append(epoch_error.result())\n\n    return loss_by_epoch, accuracy_by_epoch\n\n\ndef main():\n    # Just comment the next line out to disable eager execution\n    tf.enable_eager_execution()\n\n    \"\"\"\n    Set use_fit to True to optimize by calling tf.keras.models.Sequential.fit(),\n    set to False to use tfe.GradientTape() instead. Note that in order to use tfe.Gradient.tape(),\n    eager execution must be enabled\n    \"\"\"\n    use_fit = True\n\n    epochs = 200\n    batch_size = 64\n    dataset_folder = '.'\n\n    # Load dataset and convert it to numpy arrays\n    data = load_data(dataset_folder)\n    train_X = data.iloc[:, 0:11].values.astype(np.float32)\n    train_y = data.iloc[:, 11].values.astype(np.int32)\n\n    if use_fit:  # train_y needs to be 1-hot encoded for usage with model.fit()\n        train_y = tf.keras.utils.to_categorical(train_y, num_classes=11)\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(11,)),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(11, activation='softmax') if use_fit else tf.keras.layers.Dense(11)\n    ])\n\n    start = time.time()\n\n    if use_fit:\n        optimizer = tf.train.AdamOptimizer()\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n        history = model.fit(x=train_X, y=train_y, epochs=epochs, batch_size=batch_size, verbose=2)\n        loss_by_epoch = history.history['loss']\n        accuracy_by_epoch = history.history['acc'] if 'acc' in history.history else []\n\n    else:\n        loss_by_epoch, accuracy_by_epoch = train(model=model, X=train_X, y=train_y, batch_size=batch_size,\n                                                 epochs=epochs)\n\n    end = time.time()\n    print('It took {} seconds'.format(end - start))\n\n    # Chart loss and error\n    fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n    fig.suptitle('Training Metrics')\n\n    axes[0].set_ylabel(\"Loss\", fontsize=14)\n    axes[0].plot(loss_by_epoch)\n\n    axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n    axes[1].set_xlabel(\"Epoch\", fontsize=14)\n    axes[1].plot(accuracy_by_epoch)\n\n    plt.show()\n\n\nif __name__ == '__main__':\n    main()\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes, see below.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\nTensorFlow installed from (source or binary):\nBinary, with pip3\nTensorFlow version (use command below):\nv1.7.0-3-g024aecf414 1.7.0\nPython version:\n3.5.2\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\n9.0.176/7.0.5\nGPU model and memory:\nGTX 1080 (Armor MSI) with 8 GB VRAM\nExact command to reproduce:\nRun program below\n\nDescribe the problem\nAfter enabling eager execution, method tf.keras.models.Sequential.fit() doesn't seem to converge, as the computed loss doesn't go down (stays around 9 to 11); also, method fit() doesn't report the requested metric \"accuracy\": it doesn't print the metric to console, and does not return it in the History object.\nAfter disabling eager execution, the same optimization converges, as loss goes down (to around 1); also, method fit() correctly reports the requested metric \"accuracy\", both to console and in the returned History object.\nSource code / logs\nimport os\nimport pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\n\"\"\"\nCheck the beginning of main() for parameters\n\"\"\"\n\n\ndef load_data(folder):\n    file_name = os.path.join(folder, 'winequality-data.csv')\n    if os.path.isfile(file_name):\n        data = pd.read_csv(file_name)\n    else:\n        print('File {} not found.'.format(file_name, folder))\n        print('Dataset can be downloaded from https://www.kaggle.com/c/uci-wine-quality-dataset/data')\n        exit(1)\n    # solutions = pd.read_csv(os.path.join(folder, 'winequality-solution-input.csv'))\n    return data\n\n\ndef train_input_fn(features, labels, batch_size):\n    features_tensor = tf.constant(features)\n    labels_tensor = tf.constant(labels)\n    # Convert the inputs to a Dataset.\n    dataset = tf.data.Dataset.from_tensor_slices((features_tensor, labels_tensor))\n\n    # Shuffle, repeat, and batch the examples.\n    dataset = dataset.shuffle(len(labels)).repeat(count=1).batch(batch_size)\n\n    # Return the dataset.\n    return dataset\n\n\ndef loss(model, X, y):\n    logits = model(X)\n    the_loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n    return the_loss\n\n\ndef grad(model, inputs, targets):\n    with tfe.GradientTape() as tape:\n        loss_value = loss(model, inputs, targets)\n    return tape.gradient(loss_value, model.variables)\n\n\ndef train(model, X, y, batch_size, epochs):\n    train_ds = train_input_fn(X, y, batch_size=batch_size)\n    optimizer = tf.train.AdamOptimizer()\n\n    loss_by_epoch = []\n    accuracy_by_epoch = []\n\n    for epoch in range(epochs):\n        epoch_loss_avg = tfe.metrics.Mean()\n        epoch_error = tfe.metrics.Mean()\n        for batch, (batch_X, batch_y) in enumerate(tfe.Iterator(train_ds)):\n            grads = grad(model, batch_X, batch_y)\n            optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\n            epoch_loss_avg(loss(model, batch_X, batch_y))\n            correct_prediction = tf.equal(tf.argmax(model(batch_X), axis=1, output_type=tf.int32), batch_y)\n            epoch_error(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\n        print('Epoch {}:  loss={}  accuracy={}'.format(epoch, epoch_loss_avg.result(), epoch_error.result()))\n        loss_by_epoch.append(epoch_loss_avg.result())\n        accuracy_by_epoch.append(epoch_error.result())\n\n    return loss_by_epoch, accuracy_by_epoch\n\n\ndef main():\n    # Just comment the next line out to disable eager execution\n    tf.enable_eager_execution()\n\n    \"\"\"\n    Set use_fit to True to optimize by calling tf.keras.models.Sequential.fit(),\n    set to False to use tfe.GradientTape() instead. Note that in order to use tfe.Gradient.tape(),\n    eager execution must be enabled\n    \"\"\"\n    use_fit = True\n\n    epochs = 200\n    batch_size = 64\n    dataset_folder = '.'\n\n    # Load dataset and convert it to numpy arrays\n    data = load_data(dataset_folder)\n    train_X = data.iloc[:, 0:11].values.astype(np.float32)\n    train_y = data.iloc[:, 11].values.astype(np.int32)\n\n    if use_fit:  # train_y needs to be 1-hot encoded for usage with model.fit()\n        train_y = tf.keras.utils.to_categorical(train_y, num_classes=11)\n\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.InputLayer(input_shape=(11,)),\n        tf.keras.layers.Dense(64, activation='relu'),\n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dense(11, activation='softmax') if use_fit else tf.keras.layers.Dense(11)\n    ])\n\n    start = time.time()\n\n    if use_fit:\n        optimizer = tf.train.AdamOptimizer()\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n        history = model.fit(x=train_X, y=train_y, epochs=epochs, batch_size=batch_size, verbose=2)\n        loss_by_epoch = history.history['loss']\n        accuracy_by_epoch = history.history['acc'] if 'acc' in history.history else []\n\n    else:\n        loss_by_epoch, accuracy_by_epoch = train(model=model, X=train_X, y=train_y, batch_size=batch_size,\n                                                 epochs=epochs)\n\n    end = time.time()\n    print('It took {} seconds'.format(end - start))\n\n    # Chart loss and error\n    fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\n    fig.suptitle('Training Metrics')\n\n    axes[0].set_ylabel(\"Loss\", fontsize=14)\n    axes[0].plot(loss_by_epoch)\n\n    axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n    axes[1].set_xlabel(\"Epoch\", fontsize=14)\n    axes[1].plot(accuracy_by_epoch)\n\n    plt.show()\n\n\nif __name__ == '__main__':\n    main()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, see below.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary, with pip3\r\n- **TensorFlow version (use command below)**:\r\nv1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: \r\n3.5.2\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0.176/7.0.5\r\n- **GPU model and memory**:\r\nGTX 1080 (Armor MSI) with 8 GB VRAM\r\n- **Exact command to reproduce**:\r\nRun program below\r\n\r\n### Describe the problem\r\n\r\nAfter enabling eager execution, method `tf.keras.models.Sequential.fit()` doesn't seem to converge, as the computed loss doesn't go down (stays around 9 to 11); also, method `fit()` doesn't report the requested metric \"accuracy\": it doesn't print the metric to console, and does not return it in the History object.\r\n\r\nAfter disabling eager execution, the same optimization converges, as loss goes down (to around 1); also, method `fit()` correctly reports the requested metric \"accuracy\", both to console and in the returned History object.\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport os\r\nimport pandas as pd\r\nimport numpy as np\r\nimport time\r\nimport matplotlib.pyplot as plt\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\n\"\"\"\r\nCheck the beginning of main() for parameters\r\n\"\"\"\r\n\r\n\r\ndef load_data(folder):\r\n    file_name = os.path.join(folder, 'winequality-data.csv')\r\n    if os.path.isfile(file_name):\r\n        data = pd.read_csv(file_name)\r\n    else:\r\n        print('File {} not found.'.format(file_name, folder))\r\n        print('Dataset can be downloaded from https://www.kaggle.com/c/uci-wine-quality-dataset/data')\r\n        exit(1)\r\n    # solutions = pd.read_csv(os.path.join(folder, 'winequality-solution-input.csv'))\r\n    return data\r\n\r\n\r\ndef train_input_fn(features, labels, batch_size):\r\n    features_tensor = tf.constant(features)\r\n    labels_tensor = tf.constant(labels)\r\n    # Convert the inputs to a Dataset.\r\n    dataset = tf.data.Dataset.from_tensor_slices((features_tensor, labels_tensor))\r\n\r\n    # Shuffle, repeat, and batch the examples.\r\n    dataset = dataset.shuffle(len(labels)).repeat(count=1).batch(batch_size)\r\n\r\n    # Return the dataset.\r\n    return dataset\r\n\r\n\r\ndef loss(model, X, y):\r\n    logits = model(X)\r\n    the_loss = tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\r\n    return the_loss\r\n\r\n\r\ndef grad(model, inputs, targets):\r\n    with tfe.GradientTape() as tape:\r\n        loss_value = loss(model, inputs, targets)\r\n    return tape.gradient(loss_value, model.variables)\r\n\r\n\r\ndef train(model, X, y, batch_size, epochs):\r\n    train_ds = train_input_fn(X, y, batch_size=batch_size)\r\n    optimizer = tf.train.AdamOptimizer()\r\n\r\n    loss_by_epoch = []\r\n    accuracy_by_epoch = []\r\n\r\n    for epoch in range(epochs):\r\n        epoch_loss_avg = tfe.metrics.Mean()\r\n        epoch_error = tfe.metrics.Mean()\r\n        for batch, (batch_X, batch_y) in enumerate(tfe.Iterator(train_ds)):\r\n            grads = grad(model, batch_X, batch_y)\r\n            optimizer.apply_gradients(zip(grads, model.variables), global_step=tf.train.get_or_create_global_step())\r\n            epoch_loss_avg(loss(model, batch_X, batch_y))\r\n            correct_prediction = tf.equal(tf.argmax(model(batch_X), axis=1, output_type=tf.int32), batch_y)\r\n            epoch_error(tf.reduce_mean(tf.cast(correct_prediction, tf.float32)))\r\n        print('Epoch {}:  loss={}  accuracy={}'.format(epoch, epoch_loss_avg.result(), epoch_error.result()))\r\n        loss_by_epoch.append(epoch_loss_avg.result())\r\n        accuracy_by_epoch.append(epoch_error.result())\r\n\r\n    return loss_by_epoch, accuracy_by_epoch\r\n\r\n\r\ndef main():\r\n    # Just comment the next line out to disable eager execution\r\n    tf.enable_eager_execution()\r\n\r\n    \"\"\"\r\n    Set use_fit to True to optimize by calling tf.keras.models.Sequential.fit(),\r\n    set to False to use tfe.GradientTape() instead. Note that in order to use tfe.Gradient.tape(),\r\n    eager execution must be enabled\r\n    \"\"\"\r\n    use_fit = True\r\n\r\n    epochs = 200\r\n    batch_size = 64\r\n    dataset_folder = '.'\r\n\r\n    # Load dataset and convert it to numpy arrays\r\n    data = load_data(dataset_folder)\r\n    train_X = data.iloc[:, 0:11].values.astype(np.float32)\r\n    train_y = data.iloc[:, 11].values.astype(np.int32)\r\n\r\n    if use_fit:  # train_y needs to be 1-hot encoded for usage with model.fit()\r\n        train_y = tf.keras.utils.to_categorical(train_y, num_classes=11)\r\n\r\n    model = tf.keras.models.Sequential([\r\n        tf.keras.layers.InputLayer(input_shape=(11,)),\r\n        tf.keras.layers.Dense(64, activation='relu'),\r\n        tf.keras.layers.Dense(32, activation='relu'),\r\n        tf.keras.layers.Dense(11, activation='softmax') if use_fit else tf.keras.layers.Dense(11)\r\n    ])\r\n\r\n    start = time.time()\r\n\r\n    if use_fit:\r\n        optimizer = tf.train.AdamOptimizer()\r\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\r\n        history = model.fit(x=train_X, y=train_y, epochs=epochs, batch_size=batch_size, verbose=2)\r\n        loss_by_epoch = history.history['loss']\r\n        accuracy_by_epoch = history.history['acc'] if 'acc' in history.history else []\r\n\r\n    else:\r\n        loss_by_epoch, accuracy_by_epoch = train(model=model, X=train_X, y=train_y, batch_size=batch_size,\r\n                                                 epochs=epochs)\r\n\r\n    end = time.time()\r\n    print('It took {} seconds'.format(end - start))\r\n\r\n    # Chart loss and error\r\n    fig, axes = plt.subplots(2, sharex=True, figsize=(12, 8))\r\n    fig.suptitle('Training Metrics')\r\n\r\n    axes[0].set_ylabel(\"Loss\", fontsize=14)\r\n    axes[0].plot(loss_by_epoch)\r\n\r\n    axes[1].set_ylabel(\"Accuracy\", fontsize=14)\r\n    axes[1].set_xlabel(\"Epoch\", fontsize=14)\r\n    axes[1].plot(accuracy_by_epoch)\r\n\r\n    plt.show()\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n\r\n```"}