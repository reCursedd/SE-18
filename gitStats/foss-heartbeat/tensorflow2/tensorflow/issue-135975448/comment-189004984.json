{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/189004984", "html_url": "https://github.com/tensorflow/tensorflow/issues/1270#issuecomment-189004984", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1270", "id": 189004984, "node_id": "MDEyOklzc3VlQ29tbWVudDE4OTAwNDk4NA==", "user": {"login": "kbrems", "id": 456665, "node_id": "MDQ6VXNlcjQ1NjY2NQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/456665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kbrems", "html_url": "https://github.com/kbrems", "followers_url": "https://api.github.com/users/kbrems/followers", "following_url": "https://api.github.com/users/kbrems/following{/other_user}", "gists_url": "https://api.github.com/users/kbrems/gists{/gist_id}", "starred_url": "https://api.github.com/users/kbrems/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kbrems/subscriptions", "organizations_url": "https://api.github.com/users/kbrems/orgs", "repos_url": "https://api.github.com/users/kbrems/repos", "events_url": "https://api.github.com/users/kbrems/events{/privacy}", "received_events_url": "https://api.github.com/users/kbrems/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-25T22:00:30Z", "updated_at": "2016-02-25T22:00:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The documentation is actually not correct. You have to add the line<br>\n#include \"tensorflow/core/framework/op_kernel.h\"<br>\nto your zero_op.cc in order for it to build (this line was in the original 0.6 release user operator tutorial).<br>\nHowever including this line will not work as stated if you try to use the binary installation, as this file includes \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\" which is not in the /usr/local/lib/python2.7/dist-packages/tensorflow/include directory.</p>\n<p>g++ -std=c++11 -shared zero_out.cc -o zero_out.so <br>\n-I $TF_INC -l tensorflow_framework -L $TF_LIB <br>\n-fPIC -Wl,-rpath $TF_LIB<br>\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/type_traits.h:22:0,<br>\nfrom /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:25,<br>\nfrom /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:22,<br>\nfrom zero_out.cc:10:<br>\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:23:61: fatal error: third_party/eigen3/unsupported/Eigen/CXX11/Tensor: No such file or directory<br>\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"</p>\n<p>It is not enough to just install eigen externally because the tensorflow include files hard-code the \"third_party\" path. Those files then include the \"real\" eigen files. I had to first download eigen-eigen-ed4c9730b545 and then download the full tensorflow source and copy the third_party directory and then put both eigen-eigen-ed4c9730b545 path and the path to my third_party directory in my build command as -I flags. This kind of defeats the goal of being able to create user operators with only the binary installation.</p>\n<p>With all the include paths set up, I was able to run the zero_out op from python, but it generates a segfault after the op is run. Exit code is 139, which is an invalid memory access. Note, I am running the gpu version of the pip package, but the zero_out op is running on my cpu as it has no gpu operator registered.</p>\n<p>$ cat zero_out_test.py<br>\nimport tensorflow as tf<br>\nzero_out_module = tf.load_op_library('/home/kbrems/zeroout/libzero_out.so')<br>\nwith tf.Session(''):<br>\nresult = zero_out_module.zero_out([[1, 2], [3, 4]]).eval()<br>\nprint result</p>\n<p>$ python zero_out_test.py<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\nname: GeForce GTX TITAN<br>\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928<br>\npciBusID 0000:05:00.0<br>\nTotal memory: 6.00GiB<br>\nFree memory: 5.51GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)<br>\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB<br>\n...<br>\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB<br>\n[[1 0]<br>\n[0 0]]<br>\nSegmentation fault (core dumped)</p>\n<p>Being able to create new operators without having to fork tensorflow and bury them in the user_ops directory and build all of tensorflow is a very useful feature. It seems though that it is not yet completely implemented. If I put this same zero_out.cc file into the tensorflow/core/user_ops directory and build all of tensorflow and run the test using the bazel test framework, putting the test in tensorflow/python/kernel_tests/zero_out_op_test.py, it does work.</p>", "body_text": "The documentation is actually not correct. You have to add the line\n#include \"tensorflow/core/framework/op_kernel.h\"\nto your zero_op.cc in order for it to build (this line was in the original 0.6 release user operator tutorial).\nHowever including this line will not work as stated if you try to use the binary installation, as this file includes \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\" which is not in the /usr/local/lib/python2.7/dist-packages/tensorflow/include directory.\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so \n-I $TF_INC -l tensorflow_framework -L $TF_LIB \n-fPIC -Wl,-rpath $TF_LIB\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/type_traits.h:22:0,\nfrom /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:25,\nfrom /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:22,\nfrom zero_out.cc:10:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:23:61: fatal error: third_party/eigen3/unsupported/Eigen/CXX11/Tensor: No such file or directory\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\nIt is not enough to just install eigen externally because the tensorflow include files hard-code the \"third_party\" path. Those files then include the \"real\" eigen files. I had to first download eigen-eigen-ed4c9730b545 and then download the full tensorflow source and copy the third_party directory and then put both eigen-eigen-ed4c9730b545 path and the path to my third_party directory in my build command as -I flags. This kind of defeats the goal of being able to create user operators with only the binary installation.\nWith all the include paths set up, I was able to run the zero_out op from python, but it generates a segfault after the op is run. Exit code is 139, which is an invalid memory access. Note, I am running the gpu version of the pip package, but the zero_out op is running on my cpu as it has no gpu operator registered.\n$ cat zero_out_test.py\nimport tensorflow as tf\nzero_out_module = tf.load_op_library('/home/kbrems/zeroout/libzero_out.so')\nwith tf.Session(''):\nresult = zero_out_module.zero_out([[1, 2], [3, 4]]).eval()\nprint result\n$ python zero_out_test.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.51GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\n...\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\n[[1 0]\n[0 0]]\nSegmentation fault (core dumped)\nBeing able to create new operators without having to fork tensorflow and bury them in the user_ops directory and build all of tensorflow is a very useful feature. It seems though that it is not yet completely implemented. If I put this same zero_out.cc file into the tensorflow/core/user_ops directory and build all of tensorflow and run the test using the bazel test framework, putting the test in tensorflow/python/kernel_tests/zero_out_op_test.py, it does work.", "body": "The documentation is actually not correct. You have to add the line\n#include \"tensorflow/core/framework/op_kernel.h\"\nto your zero_op.cc in order for it to build (this line was in the original 0.6 release user operator tutorial). \nHowever including this line will not work as stated if you try to use the binary installation, as this file includes \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\" which is not in the /usr/local/lib/python2.7/dist-packages/tensorflow/include directory. \n\ng++ -std=c++11 -shared zero_out.cc -o zero_out.so \\\n -I $TF_INC -l tensorflow_framework -L $TF_LIB \\\n -fPIC -Wl,-rpath $TF_LIB\nIn file included from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/type_traits.h:22:0,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/allocator.h:25,\n                 from /usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/op_kernel.h:22,\n                 from zero_out.cc:10:\n/usr/local/lib/python2.7/dist-packages/tensorflow/include/tensorflow/core/framework/types.h:23:61: fatal error: third_party/eigen3/unsupported/Eigen/CXX11/Tensor: No such file or directory\n #include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n\nIt is not enough to just install eigen externally because the tensorflow include files hard-code the \"third_party\" path. Those files then include the \"real\" eigen files. I had to first download eigen-eigen-ed4c9730b545 and then download the full tensorflow source and copy the third_party directory and then put both eigen-eigen-ed4c9730b545 path and the path to my third_party directory in my build command as -I flags. This kind of defeats the goal of being able to create user operators with only the binary installation. \n\nWith all the include paths set up, I was able to run the zero_out op from python, but it generates a segfault after the op is run. Exit code is 139, which is an invalid memory access. Note, I am running the gpu version of the pip package, but the zero_out op is running on my cpu as it has no gpu operator registered.\n\n$ cat zero_out_test.py \nimport tensorflow as tf\nzero_out_module = tf.load_op_library('/home/kbrems/zeroout/libzero_out.so')\nwith tf.Session(''):\n  result = zero_out_module.zero_out([[1, 2], [3, 4]]).eval()\n  print result\n\n$ python zero_out_test.py \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.51GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:717] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 1.0KiB\n...\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:51] Creating bin of max chunk size 8.00GiB\n[[1 0]\n [0 0]]\nSegmentation fault (core dumped)\n\nBeing able to create new operators without having to fork tensorflow and bury them in the user_ops directory and build all of tensorflow is a very useful feature. It seems though that it is not yet completely implemented. If I put this same zero_out.cc file into the tensorflow/core/user_ops directory and build all of tensorflow and run the test using the bazel test framework, putting the test in tensorflow/python/kernel_tests/zero_out_op_test.py, it does work. \n"}