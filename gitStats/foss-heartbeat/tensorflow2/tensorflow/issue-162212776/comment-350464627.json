{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/350464627", "html_url": "https://github.com/tensorflow/tensorflow/issues/3032#issuecomment-350464627", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3032", "id": 350464627, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MDQ2NDYyNw==", "user": {"login": "Joshuaalbert", "id": 14807032, "node_id": "MDQ6VXNlcjE0ODA3MDMy", "avatar_url": "https://avatars2.githubusercontent.com/u/14807032?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Joshuaalbert", "html_url": "https://github.com/Joshuaalbert", "followers_url": "https://api.github.com/users/Joshuaalbert/followers", "following_url": "https://api.github.com/users/Joshuaalbert/following{/other_user}", "gists_url": "https://api.github.com/users/Joshuaalbert/gists{/gist_id}", "starred_url": "https://api.github.com/users/Joshuaalbert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Joshuaalbert/subscriptions", "organizations_url": "https://api.github.com/users/Joshuaalbert/orgs", "repos_url": "https://api.github.com/users/Joshuaalbert/repos", "events_url": "https://api.github.com/users/Joshuaalbert/events{/privacy}", "received_events_url": "https://api.github.com/users/Joshuaalbert/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-09T13:13:29Z", "updated_at": "2017-12-09T13:18:26Z", "author_association": "NONE", "body_html": "<p>The paper you mention uses projection kernels of the form <code>W = W_r + i W_i</code> and input tensors of the form <code>x= x_r + i x_i</code>. The standard perceptron complex output is then</p>\n<pre><code>y_r + i y_i = f(Real(W x)) + i f(Imag(W x)) \n= f( W_r x_r - W_i x_i) + i f(W_r x_i + W_i x_r)\n= f(   (W_r, W_i)^T . (x_r, -x_i)^T ) + i f(   (W_r, W_i)^T . (x_i, x_r)^T ) \n</code></pre>\n<p>In the last line you see that you can represent the complex weight kernel as <code>W_r</code> and <code>W_i</code> stacked on top of each other.<br>\nTherefore you can represent this complex forward projection as the following set of operations:</p>\n<pre><code>Assume N = output size of each perceptron\n  x_r = tf.real(x)\n  x_i = tf.imag(x)\n  x_1 = tf.concat([x_r, -x_i], axis=0)\n  x_2 = tf.concat([x_i, x_r], axis=0)\n  layer_1 = tf.keras.layers.Dense(N)\n  # use the same layer twice and the kernel will represent (W_r, W_i)^T\n  y_r = layer_1(x_1)\n  y_i = layer_1(x_2)\n  # to add more layers again for x_1 and x_2 from y_r and y_i\n  x_1 = tf.concat([y_r, -y_i], axis=0)\n  x_2 = tf.concat([y_i, y_r], axis=0)\n  layer_2 = tf.keras.layers.Dense(N)\n  z_r = layer_2(z_1)\n  z_i = layer_2(z_2)\n  ...\nTo combine outputs into complex you can always do tf.complex(real_part, imag_part)\nThe loss function must be real and can be loss(real_part) + loss(imag_part)\n</code></pre>\n<p>My observations are that since complex numbers are merely a particular container of two real numbers you are actually restricting yourself by choosing to use complex numbers. In general if you wanted to allow more complex patterns you should form an input <code>input = tf.concat([tf.real(x), tf.imag(x)], axis=-1)</code>.</p>", "body_text": "The paper you mention uses projection kernels of the form W = W_r + i W_i and input tensors of the form x= x_r + i x_i. The standard perceptron complex output is then\ny_r + i y_i = f(Real(W x)) + i f(Imag(W x)) \n= f( W_r x_r - W_i x_i) + i f(W_r x_i + W_i x_r)\n= f(   (W_r, W_i)^T . (x_r, -x_i)^T ) + i f(   (W_r, W_i)^T . (x_i, x_r)^T ) \n\nIn the last line you see that you can represent the complex weight kernel as W_r and W_i stacked on top of each other.\nTherefore you can represent this complex forward projection as the following set of operations:\nAssume N = output size of each perceptron\n  x_r = tf.real(x)\n  x_i = tf.imag(x)\n  x_1 = tf.concat([x_r, -x_i], axis=0)\n  x_2 = tf.concat([x_i, x_r], axis=0)\n  layer_1 = tf.keras.layers.Dense(N)\n  # use the same layer twice and the kernel will represent (W_r, W_i)^T\n  y_r = layer_1(x_1)\n  y_i = layer_1(x_2)\n  # to add more layers again for x_1 and x_2 from y_r and y_i\n  x_1 = tf.concat([y_r, -y_i], axis=0)\n  x_2 = tf.concat([y_i, y_r], axis=0)\n  layer_2 = tf.keras.layers.Dense(N)\n  z_r = layer_2(z_1)\n  z_i = layer_2(z_2)\n  ...\nTo combine outputs into complex you can always do tf.complex(real_part, imag_part)\nThe loss function must be real and can be loss(real_part) + loss(imag_part)\n\nMy observations are that since complex numbers are merely a particular container of two real numbers you are actually restricting yourself by choosing to use complex numbers. In general if you wanted to allow more complex patterns you should form an input input = tf.concat([tf.real(x), tf.imag(x)], axis=-1).", "body": "The paper you mention uses projection kernels of the form `W = W_r + i W_i` and input tensors of the form `x= x_r + i x_i`. The standard perceptron complex output is then \r\n```\r\ny_r + i y_i = f(Real(W x)) + i f(Imag(W x)) \r\n= f( W_r x_r - W_i x_i) + i f(W_r x_i + W_i x_r)\r\n= f(   (W_r, W_i)^T . (x_r, -x_i)^T ) + i f(   (W_r, W_i)^T . (x_i, x_r)^T ) \r\n```\r\nIn the last line you see that you can represent the complex weight kernel as `W_r` and `W_i` stacked on top of each other.\r\nTherefore you can represent this complex forward projection as the following set of operations:\r\n```\r\nAssume N = output size of each perceptron\r\n  x_r = tf.real(x)\r\n  x_i = tf.imag(x)\r\n  x_1 = tf.concat([x_r, -x_i], axis=0)\r\n  x_2 = tf.concat([x_i, x_r], axis=0)\r\n  layer_1 = tf.keras.layers.Dense(N)\r\n  # use the same layer twice and the kernel will represent (W_r, W_i)^T\r\n  y_r = layer_1(x_1)\r\n  y_i = layer_1(x_2)\r\n  # to add more layers again for x_1 and x_2 from y_r and y_i\r\n  x_1 = tf.concat([y_r, -y_i], axis=0)\r\n  x_2 = tf.concat([y_i, y_r], axis=0)\r\n  layer_2 = tf.keras.layers.Dense(N)\r\n  z_r = layer_2(z_1)\r\n  z_i = layer_2(z_2)\r\n  ...\r\nTo combine outputs into complex you can always do tf.complex(real_part, imag_part)\r\nThe loss function must be real and can be loss(real_part) + loss(imag_part)\r\n```\r\n\r\nMy observations are that since complex numbers are merely a particular container of two real numbers you are actually restricting yourself by choosing to use complex numbers. In general if you wanted to allow more complex patterns you should form an input `input = tf.concat([tf.real(x), tf.imag(x)], axis=-1)`."}