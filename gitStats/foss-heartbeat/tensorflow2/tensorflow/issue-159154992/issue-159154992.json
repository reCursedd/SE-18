{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2728", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2728/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2728/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2728/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2728", "id": 159154992, "node_id": "MDU6SXNzdWUxNTkxNTQ5OTI=", "number": 2728, "title": "Memory leak (on cpu) in 0.9rc (vs. 0.7.1rc)", "user": {"login": "osdf", "id": 193341, "node_id": "MDQ6VXNlcjE5MzM0MQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/193341?v=4", "gravatar_id": "", "url": "https://api.github.com/users/osdf", "html_url": "https://github.com/osdf", "followers_url": "https://api.github.com/users/osdf/followers", "following_url": "https://api.github.com/users/osdf/following{/other_user}", "gists_url": "https://api.github.com/users/osdf/gists{/gist_id}", "starred_url": "https://api.github.com/users/osdf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/osdf/subscriptions", "organizations_url": "https://api.github.com/users/osdf/orgs", "repos_url": "https://api.github.com/users/osdf/repos", "events_url": "https://api.github.com/users/osdf/events{/privacy}", "received_events_url": "https://api.github.com/users/osdf/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2016-06-08T12:47:43Z", "updated_at": "2016-06-24T20:13:59Z", "closed_at": "2016-06-24T18:12:53Z", "author_association": "NONE", "body_html": "<p>Hi,<br>\nusing the provided wheel vor 0.9rc (also reproduced with a fresh install from source) I'm seeing a lot of memory consumption (on the cpu) during training a model (on a gpu (TITAN X) which gets its data from a <code>tf.FIFOQueue</code> pinned with <code>tf.device()</code> to a cpu). I tested two different dataset sizes and these lead to 600MB and 1.2GB leaked memory respectively accumulated during the training loop (GPU memory consumption stays constant during training time).</p>\n<p>This memory leak does not happen at all when running the same code using 0.7.1rc. Additionally,<br>\nwith 0.9rc setting up the whole graph takes about 600MB vs 400MB with 0.7.1rc.</p>\n<p>The code is rather large so it will take a bit to get a MWE to (hopefully) reproduce this issue. I suspect it is tied to using a queue (queues, actually, because the validation set is also feed through a different queue, relying on <code>tf.template()</code> to share model parameters). Has something like that been observed beforehand? Any hints how I could try figure out more of the problem myself?</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nUbuntu 15.10</p>\n<p>Installed version of CUDA and cuDNN:<br>\nCUDA7.5/cuDNN4.0.4</p>\n<p>(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<pre><code>-rw-r--r-- 1 root root 189170 Jan  1 23:25 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan  1 23:25 /usr/local/cuda/lib/libcudart_static.a\n</code></pre>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.<br>\n<a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl</a></li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally 0.9.0rc0\n</code></pre>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Working on MWE...</li>\n</ol>\n<h3>What have you tried?</h3>\n<p>The described behavior does not happen with 0.7.1rc (UPDATE: also not observed with 0.8rc).</p>", "body_text": "Hi,\nusing the provided wheel vor 0.9rc (also reproduced with a fresh install from source) I'm seeing a lot of memory consumption (on the cpu) during training a model (on a gpu (TITAN X) which gets its data from a tf.FIFOQueue pinned with tf.device() to a cpu). I tested two different dataset sizes and these lead to 600MB and 1.2GB leaked memory respectively accumulated during the training loop (GPU memory consumption stays constant during training time).\nThis memory leak does not happen at all when running the same code using 0.7.1rc. Additionally,\nwith 0.9rc setting up the whole graph takes about 600MB vs 400MB with 0.7.1rc.\nThe code is rather large so it will take a bit to get a MWE to (hopefully) reproduce this issue. I suspect it is tied to using a queue (queues, actually, because the validation set is also feed through a different queue, relying on tf.template() to share model parameters). Has something like that been observed beforehand? Any hints how I could try figure out more of the problem myself?\nEnvironment info\nOperating System:\nUbuntu 15.10\nInstalled version of CUDA and cuDNN:\nCUDA7.5/cuDNN4.0.4\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root 189170 Jan  1 23:25 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan  1 23:25 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally 0.9.0rc0\n\nSteps to reproduce\n\nWorking on MWE...\n\nWhat have you tried?\nThe described behavior does not happen with 0.7.1rc (UPDATE: also not observed with 0.8rc).", "body": "Hi,\nusing the provided wheel vor 0.9rc (also reproduced with a fresh install from source) I'm seeing a lot of memory consumption (on the cpu) during training a model (on a gpu (TITAN X) which gets its data from a `tf.FIFOQueue` pinned with `tf.device()` to a cpu). I tested two different dataset sizes and these lead to 600MB and 1.2GB leaked memory respectively accumulated during the training loop (GPU memory consumption stays constant during training time).\n\nThis memory leak does not happen at all when running the same code using 0.7.1rc. Additionally,\nwith 0.9rc setting up the whole graph takes about 600MB vs 400MB with 0.7.1rc.\n\nThe code is rather large so it will take a bit to get a MWE to (hopefully) reproduce this issue. I suspect it is tied to using a queue (queues, actually, because the validation set is also feed through a different queue, relying on `tf.template()` to share model parameters). Has something like that been observed beforehand? Any hints how I could try figure out more of the problem myself?\n### Environment info\n\nOperating System:\nUbuntu 15.10\n\nInstalled version of CUDA and cuDNN: \nCUDA7.5/cuDNN4.0.4\n\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\n```\n-rw-r--r-- 1 root root 189170 Jan  1 23:25 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Jan  1 23:25 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Jan  1 23:25 /usr/local/cuda/lib/libcudart_static.a\n```\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.9.0rc0-cp27-none-linux_x86_64.whl\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally 0.9.0rc0\n```\n### Steps to reproduce\n1. Working on MWE...\n### What have you tried?\n\nThe described behavior does not happen with 0.7.1rc (UPDATE: also not observed with 0.8rc).\n"}