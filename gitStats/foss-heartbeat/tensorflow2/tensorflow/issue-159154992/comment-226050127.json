{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226050127", "html_url": "https://github.com/tensorflow/tensorflow/issues/2728#issuecomment-226050127", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2728", "id": 226050127, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjA1MDEyNw==", "user": {"login": "abezuglov", "id": 6935493, "node_id": "MDQ6VXNlcjY5MzU0OTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6935493?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abezuglov", "html_url": "https://github.com/abezuglov", "followers_url": "https://api.github.com/users/abezuglov/followers", "following_url": "https://api.github.com/users/abezuglov/following{/other_user}", "gists_url": "https://api.github.com/users/abezuglov/gists{/gist_id}", "starred_url": "https://api.github.com/users/abezuglov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abezuglov/subscriptions", "organizations_url": "https://api.github.com/users/abezuglov/orgs", "repos_url": "https://api.github.com/users/abezuglov/repos", "events_url": "https://api.github.com/users/abezuglov/events{/privacy}", "received_events_url": "https://api.github.com/users/abezuglov/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T23:48:06Z", "updated_at": "2016-06-14T23:48:06Z", "author_association": "NONE", "body_html": "<p>If I remember correctly, it did happen with both single and multi GPU's.</p>\n<p>Please, find the main training loop below. The complete code is here: [https://github.com/abezuglov/ANN/tree/master/code], where ilt_one_layer.py, ilt_two_layers.py, etc. are the models; ilt_default_feed.py -- training on a single device, ilt_multi_gpu_feed.py -- training on multiple devices. Let me know if I can be of further help.</p>\n<p>def train():<br>\n# Read datasets<br>\ntrain_dataset, valid_dataset, test_dataset = ld.read_data_sets()</p>\n<pre><code>with tf.Graph().as_default(), tf.device('/cpu:0'):\n    x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, FLAGS.output_vars], name = 'y-input')\n\n    # Prepare global step and learning rate for optimization...\n\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    outputs = ilt.inference(x)\n    loss = ilt.loss(outputs, y_)\n\n    # Calculate gradients and apply them\n    grads = optimizer.compute_gradients(loss)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step = global_step)\n\n    # Smoothen variables after gradient applications\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_avg_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    merged = tf.merge_all_summaries()\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session(config = tf.ConfigProto(\n        allow_soft_placement = False, # allows to utilize GPU's &amp; CPU's\n        log_device_placement = False)) # shows GPU/CPU allocation\n\n    # Finish graph creation. Below is the code for running graph\n    sess.run(init)\n    tf.train.start_queue_runners(sess=sess)\n\n    # Main training loop\n    for step in xrange(FLAGS.max_steps):\n        start_time = time.time()\n\n        _, train_loss, summary, lr = sess.run(\n            [train_op, loss, merged, learning_rate], feed_dict=fill_feed_dict(train_dataset, x, y_, train = True))\n\n        duration = time.time()-start_time\n        if step%(FLAGS.max_steps//20) == 0:\n            feed_dict = fill_feed_dict(valid_dataset, x, y_, train = False)\n            valid_loss, summary = sess.run([loss, merged], feed_dict = feed_dict)\n            print('Step %d (%.2f op/sec): Training MSE: %.5f, Validation MSE: %.5f' % (\n                step, 1.0/duration, np.float32(train_loss).item(), np.float32(valid_loss).item()))\n\n    feed_dict = fill_feed_dict(test_dataset, x, y_, train = False)\n    test_loss = sess.run([loss], feed_dict = feed_dict)\n    print('Test MSE: %.5f' % (np.float32(test_loss).item()))\n    sess.close()\n</code></pre>", "body_text": "If I remember correctly, it did happen with both single and multi GPU's.\nPlease, find the main training loop below. The complete code is here: [https://github.com/abezuglov/ANN/tree/master/code], where ilt_one_layer.py, ilt_two_layers.py, etc. are the models; ilt_default_feed.py -- training on a single device, ilt_multi_gpu_feed.py -- training on multiple devices. Let me know if I can be of further help.\ndef train():\n# Read datasets\ntrain_dataset, valid_dataset, test_dataset = ld.read_data_sets()\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, FLAGS.output_vars], name = 'y-input')\n\n    # Prepare global step and learning rate for optimization...\n\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    outputs = ilt.inference(x)\n    loss = ilt.loss(outputs, y_)\n\n    # Calculate gradients and apply them\n    grads = optimizer.compute_gradients(loss)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step = global_step)\n\n    # Smoothen variables after gradient applications\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_avg_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    merged = tf.merge_all_summaries()\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session(config = tf.ConfigProto(\n        allow_soft_placement = False, # allows to utilize GPU's & CPU's\n        log_device_placement = False)) # shows GPU/CPU allocation\n\n    # Finish graph creation. Below is the code for running graph\n    sess.run(init)\n    tf.train.start_queue_runners(sess=sess)\n\n    # Main training loop\n    for step in xrange(FLAGS.max_steps):\n        start_time = time.time()\n\n        _, train_loss, summary, lr = sess.run(\n            [train_op, loss, merged, learning_rate], feed_dict=fill_feed_dict(train_dataset, x, y_, train = True))\n\n        duration = time.time()-start_time\n        if step%(FLAGS.max_steps//20) == 0:\n            feed_dict = fill_feed_dict(valid_dataset, x, y_, train = False)\n            valid_loss, summary = sess.run([loss, merged], feed_dict = feed_dict)\n            print('Step %d (%.2f op/sec): Training MSE: %.5f, Validation MSE: %.5f' % (\n                step, 1.0/duration, np.float32(train_loss).item(), np.float32(valid_loss).item()))\n\n    feed_dict = fill_feed_dict(test_dataset, x, y_, train = False)\n    test_loss = sess.run([loss], feed_dict = feed_dict)\n    print('Test MSE: %.5f' % (np.float32(test_loss).item()))\n    sess.close()", "body": "If I remember correctly, it did happen with both single and multi GPU's.\n\nPlease, find the main training loop below. The complete code is here: [https://github.com/abezuglov/ANN/tree/master/code], where ilt_one_layer.py, ilt_two_layers.py, etc. are the models; ilt_default_feed.py -- training on a single device, ilt_multi_gpu_feed.py -- training on multiple devices. Let me know if I can be of further help.\n\ndef train():\n    # Read datasets \n    train_dataset, valid_dataset, test_dataset = ld.read_data_sets()\n\n```\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    x = tf.placeholder(tf.float32, [None, FLAGS.input_vars], name='x-input')\n    y_ = tf.placeholder(tf.float32, [None, FLAGS.output_vars], name = 'y-input')\n\n    # Prepare global step and learning rate for optimization...\n\n    optimizer = tf.train.AdamOptimizer(learning_rate)\n    outputs = ilt.inference(x)\n    loss = ilt.loss(outputs, y_)\n\n    # Calculate gradients and apply them\n    grads = optimizer.compute_gradients(loss)\n    apply_gradient_op = optimizer.apply_gradients(grads, global_step = global_step)\n\n    # Smoothen variables after gradient applications\n    variable_averages = tf.train.ExponentialMovingAverage(\n        FLAGS.moving_avg_decay, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n    train_op = tf.group(apply_gradient_op, variables_averages_op)\n\n    merged = tf.merge_all_summaries()\n\n    init = tf.initialize_all_variables()\n    sess = tf.Session(config = tf.ConfigProto(\n        allow_soft_placement = False, # allows to utilize GPU's & CPU's\n        log_device_placement = False)) # shows GPU/CPU allocation\n\n    # Finish graph creation. Below is the code for running graph\n    sess.run(init)\n    tf.train.start_queue_runners(sess=sess)\n\n    # Main training loop\n    for step in xrange(FLAGS.max_steps):\n        start_time = time.time()\n\n        _, train_loss, summary, lr = sess.run(\n            [train_op, loss, merged, learning_rate], feed_dict=fill_feed_dict(train_dataset, x, y_, train = True))\n\n        duration = time.time()-start_time\n        if step%(FLAGS.max_steps//20) == 0:\n            feed_dict = fill_feed_dict(valid_dataset, x, y_, train = False)\n            valid_loss, summary = sess.run([loss, merged], feed_dict = feed_dict)\n            print('Step %d (%.2f op/sec): Training MSE: %.5f, Validation MSE: %.5f' % (\n                step, 1.0/duration, np.float32(train_loss).item(), np.float32(valid_loss).item()))\n\n    feed_dict = fill_feed_dict(test_dataset, x, y_, train = False)\n    test_loss = sess.run([loss], feed_dict = feed_dict)\n    print('Test MSE: %.5f' % (np.float32(test_loss).item()))\n    sess.close()\n```\n"}