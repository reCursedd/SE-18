{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225378564", "html_url": "https://github.com/tensorflow/tensorflow/issues/2728#issuecomment-225378564", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2728", "id": 225378564, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTM3ODU2NA==", "user": {"login": "nsuke", "id": 3064114, "node_id": "MDQ6VXNlcjMwNjQxMTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/3064114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nsuke", "html_url": "https://github.com/nsuke", "followers_url": "https://api.github.com/users/nsuke/followers", "following_url": "https://api.github.com/users/nsuke/following{/other_user}", "gists_url": "https://api.github.com/users/nsuke/gists{/gist_id}", "starred_url": "https://api.github.com/users/nsuke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nsuke/subscriptions", "organizations_url": "https://api.github.com/users/nsuke/orgs", "repos_url": "https://api.github.com/users/nsuke/repos", "events_url": "https://api.github.com/users/nsuke/events{/privacy}", "received_events_url": "https://api.github.com/users/nsuke/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-11T17:34:15Z", "updated_at": "2016-06-11T17:38:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm experiencing a similar problem.<br>\nWhen launching a graph, around 16 GB memory is consumed, but only ~3 GB is release upon exiting the process.</p>\n<p>Using <code>top</code> command, lost 13 GB is only in total used memory, not on any of listed processes.<br>\nI.e., summing up every process's memory usage results in a fewer number by ~13 GB.</p>\n<p>Memory consumption is constant during training, and re-runing the same graph within a process does not increase the consumption either.<br>\nOn the other hand, after re-launching another process, the first path consumes another 16 GB.<br>\nThe memory seems to never go to swap, and when it exhausts the available main memory, Linux hangs.</p>\n<p>I'm running the graph on a Titan X on a machine with two Titan X and 64 GB main memory.<br>\nI've experienced this for at least master branch at 6/8 and 5/24.<br>\nThe OS is Ubuntu 16.04.<br>\nWhen I used 0.9rc without GPU support, the leak didn't happen.</p>\n<p>Interestingly, when I run very similar graph at home with single GTX 960 machine and 16 GB main memory, this kind of leak never happens.<br>\nThe batch size fed is much smaller due to GPU memory size, though.<br>\nI've tested this on master branch at 6/4 and 6/12.<br>\nThe OS is ArchLinux.</p>\n<p>CUDA7.5/cuDNN5.0.5 for both of the above.<br>\nAlso I should note that I compiled tensorflow with GPU support using GCC 5.</p>", "body_text": "I'm experiencing a similar problem.\nWhen launching a graph, around 16 GB memory is consumed, but only ~3 GB is release upon exiting the process.\nUsing top command, lost 13 GB is only in total used memory, not on any of listed processes.\nI.e., summing up every process's memory usage results in a fewer number by ~13 GB.\nMemory consumption is constant during training, and re-runing the same graph within a process does not increase the consumption either.\nOn the other hand, after re-launching another process, the first path consumes another 16 GB.\nThe memory seems to never go to swap, and when it exhausts the available main memory, Linux hangs.\nI'm running the graph on a Titan X on a machine with two Titan X and 64 GB main memory.\nI've experienced this for at least master branch at 6/8 and 5/24.\nThe OS is Ubuntu 16.04.\nWhen I used 0.9rc without GPU support, the leak didn't happen.\nInterestingly, when I run very similar graph at home with single GTX 960 machine and 16 GB main memory, this kind of leak never happens.\nThe batch size fed is much smaller due to GPU memory size, though.\nI've tested this on master branch at 6/4 and 6/12.\nThe OS is ArchLinux.\nCUDA7.5/cuDNN5.0.5 for both of the above.\nAlso I should note that I compiled tensorflow with GPU support using GCC 5.", "body": "I'm experiencing a similar problem.\nWhen launching a graph, around 16 GB memory is consumed, but only ~3 GB is release upon exiting the process.\n\nUsing `top` command, lost 13 GB is only in total used memory, not on any of listed processes.\nI.e., summing up every process's memory usage results in a fewer number by ~13 GB.\n\nMemory consumption is constant during training, and re-runing the same graph within a process does not increase the consumption either.\nOn the other hand, after re-launching another process, the first path consumes another 16 GB.\nThe memory seems to never go to swap, and when it exhausts the available main memory, Linux hangs.\n\nI'm running the graph on a Titan X on a machine with two Titan X and 64 GB main memory.\nI've experienced this for at least master branch at 6/8 and 5/24.\nThe OS is Ubuntu 16.04.\nWhen I used 0.9rc without GPU support, the leak didn't happen.\n\nInterestingly, when I run very similar graph at home with single GTX 960 machine and 16 GB main memory, this kind of leak never happens.\nThe batch size fed is much smaller due to GPU memory size, though.\nI've tested this on master branch at 6/4 and 6/12.\nThe OS is ArchLinux.\n\nCUDA7.5/cuDNN5.0.5 for both of the above.\nAlso I should note that I compiled tensorflow with GPU support using GCC 5.\n"}