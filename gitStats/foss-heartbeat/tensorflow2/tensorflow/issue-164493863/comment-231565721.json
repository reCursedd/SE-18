{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/231565721", "html_url": "https://github.com/tensorflow/tensorflow/pull/3235#issuecomment-231565721", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3235", "id": 231565721, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMTU2NTcyMQ==", "user": {"login": "keveman", "id": 229914, "node_id": "MDQ6VXNlcjIyOTkxNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/229914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keveman", "html_url": "https://github.com/keveman", "followers_url": "https://api.github.com/users/keveman/followers", "following_url": "https://api.github.com/users/keveman/following{/other_user}", "gists_url": "https://api.github.com/users/keveman/gists{/gist_id}", "starred_url": "https://api.github.com/users/keveman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keveman/subscriptions", "organizations_url": "https://api.github.com/users/keveman/orgs", "repos_url": "https://api.github.com/users/keveman/repos", "events_url": "https://api.github.com/users/keveman/events{/privacy}", "received_events_url": "https://api.github.com/users/keveman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-10T01:40:31Z", "updated_at": "2016-07-10T01:40:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12388137\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dauba-dauba\">@dauba-dauba</a> Reading the documentation <a href=\"http://pubs.opengroup.org/onlinepubs/009695399/functions/dlclose.html\" rel=\"nofollow\">here</a>, it doesn't seem like there are any guarantees about the shared library being unmapped from the memory space. Also, with a multithreaded application like TensorFlow, I don't know how we can reason about <code>dlclose</code> when there is another thread executing a kernel loaded from the shared library. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=57440\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/IvanUkhov\">@IvanUkhov</a>, while I share your sentiment about avoiding memory leaks, I can't imagine people loading hundreds of shared libraries via <code>TF_LoadLibrary</code> and leaking the handles. If we get into a situation like that, we can definitely revisit this, but until I see some compelling reason, I would rather have libraries loaded and remain in memory.</p>", "body_text": "@dauba-dauba Reading the documentation here, it doesn't seem like there are any guarantees about the shared library being unmapped from the memory space. Also, with a multithreaded application like TensorFlow, I don't know how we can reason about dlclose when there is another thread executing a kernel loaded from the shared library. @IvanUkhov, while I share your sentiment about avoiding memory leaks, I can't imagine people loading hundreds of shared libraries via TF_LoadLibrary and leaking the handles. If we get into a situation like that, we can definitely revisit this, but until I see some compelling reason, I would rather have libraries loaded and remain in memory.", "body": "@dauba-dauba Reading the documentation [here](http://pubs.opengroup.org/onlinepubs/009695399/functions/dlclose.html), it doesn't seem like there are any guarantees about the shared library being unmapped from the memory space. Also, with a multithreaded application like TensorFlow, I don't know how we can reason about `dlclose` when there is another thread executing a kernel loaded from the shared library. @IvanUkhov, while I share your sentiment about avoiding memory leaks, I can't imagine people loading hundreds of shared libraries via `TF_LoadLibrary` and leaking the handles. If we get into a situation like that, we can definitely revisit this, but until I see some compelling reason, I would rather have libraries loaded and remain in memory.\n"}