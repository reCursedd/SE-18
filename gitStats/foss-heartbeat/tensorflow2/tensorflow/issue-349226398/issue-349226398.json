{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21522", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21522/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21522/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21522/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21522", "id": 349226398, "node_id": "MDU6SXNzdWUzNDkyMjYzOTg=", "number": 21522, "title": "Model Parallelism Memory usage issue", "user": {"login": "reger-men", "id": 8779942, "node_id": "MDQ6VXNlcjg3Nzk5NDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8779942?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reger-men", "html_url": "https://github.com/reger-men", "followers_url": "https://api.github.com/users/reger-men/followers", "following_url": "https://api.github.com/users/reger-men/following{/other_user}", "gists_url": "https://api.github.com/users/reger-men/gists{/gist_id}", "starred_url": "https://api.github.com/users/reger-men/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reger-men/subscriptions", "organizations_url": "https://api.github.com/users/reger-men/orgs", "repos_url": "https://api.github.com/users/reger-men/repos", "events_url": "https://api.github.com/users/reger-men/events{/privacy}", "received_events_url": "https://api.github.com/users/reger-men/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-08-09T17:46:46Z", "updated_at": "2018-11-20T07:43:41Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from</strong>: From binary</li>\n<li><strong>TensorFlow version</strong>: 1.9</li>\n<li><strong>Python version</strong>: 3.6.1</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0.176</li>\n<li><strong>GPU model and memory</strong>: Tested on 4X GeForce GTX 1080 with 10GB</li>\n<li><strong>Exact command to reproduce</strong>: <code>CUDA_VISIBLE_DEVICES=0,1,2,3 python3 train.py --data-dir=Datasets_Path/cityscapes/ --input-size=\"1000,1000\" --batch-size=1</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I run the above Model training on single GPU and on multiple GPUs and it looks like the memory is being allocated several times. The model size is about 10,7 GB. By distributing over several GPUs the total model size should not be increased. But as you can see it almost triples. Thereby I can not profit from model parallelism. Even a input size of 1200 x 1200 return an OOM error.</p>\n<p>The following screenshots show the memory usage on different GPUs:</p>\n<h3>On GPU:0</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8779942/43915687-c2372af4-9c0b-11e8-8eac-c7d3d28b7844.png\"><img src=\"https://user-images.githubusercontent.com/8779942/43915687-c2372af4-9c0b-11e8-8eac-c7d3d28b7844.png\" alt=\"gpu0\" style=\"max-width:100%;\"></a></p>\n<h3>On GPU:0 and 1</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8779942/43915680-bda22de0-9c0b-11e8-957e-79321a8a834b.png\"><img src=\"https://user-images.githubusercontent.com/8779942/43915680-bda22de0-9c0b-11e8-957e-79321a8a834b.png\" alt=\"gpu01\" style=\"max-width:100%;\"></a></p>\n<h3>On GPU:0, 1 and 3</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8779942/43915644-a35d67ec-9c0b-11e8-8cd6-e79e948699c4.png\"><img src=\"https://user-images.githubusercontent.com/8779942/43915644-a35d67ec-9c0b-11e8-8cd6-e79e948699c4.png\" alt=\"gpu012\" style=\"max-width:100%;\"></a></p>\n<h3>On GPU:0, 1,2 and 3</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/8779942/43915637-9b08e7a6-9c0b-11e8-9621-5e679af4bae6.png\"><img src=\"https://user-images.githubusercontent.com/8779942/43915637-9b08e7a6-9c0b-11e8-9621-5e679af4bae6.png\" alt=\"gpu0123\" style=\"max-width:100%;\"></a></p>\n<h3>Source code / logs</h3>\n<p><a href=\"https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism\">Source code</a></p>", "body_text": "System information\n\nHave I written custom code: Yes\nOS Platform and Distribution: Linux Ubuntu 16.04\nMobile device: N/A\nTensorFlow installed from: From binary\nTensorFlow version: 1.9\nPython version: 3.6.1\nBazel version: N/A\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.0.176\nGPU model and memory: Tested on 4X GeForce GTX 1080 with 10GB\nExact command to reproduce: CUDA_VISIBLE_DEVICES=0,1,2,3 python3 train.py --data-dir=Datasets_Path/cityscapes/ --input-size=\"1000,1000\" --batch-size=1\n\nDescribe the problem\nI run the above Model training on single GPU and on multiple GPUs and it looks like the memory is being allocated several times. The model size is about 10,7 GB. By distributing over several GPUs the total model size should not be increased. But as you can see it almost triples. Thereby I can not profit from model parallelism. Even a input size of 1200 x 1200 return an OOM error.\nThe following screenshots show the memory usage on different GPUs:\nOn GPU:0\n\nOn GPU:0 and 1\n\nOn GPU:0, 1 and 3\n\nOn GPU:0, 1,2 and 3\n\nSource code / logs\nSource code", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **Mobile device**: N/A\r\n- **TensorFlow installed from**: From binary\r\n- **TensorFlow version**: 1.9\r\n- **Python version**: 3.6.1\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0.176\r\n- **GPU model and memory**: Tested on 4X GeForce GTX 1080 with 10GB\r\n- **Exact command to reproduce**: `CUDA_VISIBLE_DEVICES=0,1,2,3 python3 train.py --data-dir=Datasets_Path/cityscapes/ --input-size=\"1000,1000\" --batch-size=1`\r\n\r\n### Describe the problem\r\nI run the above Model training on single GPU and on multiple GPUs and it looks like the memory is being allocated several times. The model size is about 10,7 GB. By distributing over several GPUs the total model size should not be increased. But as you can see it almost triples. Thereby I can not profit from model parallelism. Even a input size of 1200 x 1200 return an OOM error.\r\n\r\nThe following screenshots show the memory usage on different GPUs:\r\n\r\n### On GPU:0\r\n![gpu0](https://user-images.githubusercontent.com/8779942/43915687-c2372af4-9c0b-11e8-8eac-c7d3d28b7844.png)\r\n\r\n\r\n### On GPU:0 and 1\r\n![gpu01](https://user-images.githubusercontent.com/8779942/43915680-bda22de0-9c0b-11e8-957e-79321a8a834b.png)\r\n\r\n\r\n### On GPU:0, 1 and 3\r\n![gpu012](https://user-images.githubusercontent.com/8779942/43915644-a35d67ec-9c0b-11e8-8cd6-e79e948699c4.png)\r\n\r\n\r\n### On GPU:0, 1,2 and 3\r\n![gpu0123](https://user-images.githubusercontent.com/8779942/43915637-9b08e7a6-9c0b-11e8-9621-5e679af4bae6.png)\r\n\r\n\r\n\r\n\r\n### Source code / logs\r\n[Source code](https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism)\r\n"}