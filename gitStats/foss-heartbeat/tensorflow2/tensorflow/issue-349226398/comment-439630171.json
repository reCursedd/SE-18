{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439630171", "html_url": "https://github.com/tensorflow/tensorflow/issues/21522#issuecomment-439630171", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21522", "id": 439630171, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTYzMDE3MQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-17T16:42:00Z", "updated_at": "2018-11-17T16:42:00Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8779942\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reger-men\">@reger-men</a> : The memory consumed by a model depends on the computation in the model, not just variable placement. For example, consider this trivial computation:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> A ~4GB tensor</span>\ny <span class=\"pl-k\">=</span> tf.add(x, <span class=\"pl-c1\">1</span>.)\nz <span class=\"pl-k\">=</span> tf.add(y, <span class=\"pl-c1\">1</span>.)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  sess.run(z)</pre></div>\n<p>This computation will consume 4GB. If I split it across 2 GPUs with something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  x <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  y <span class=\"pl-k\">=</span> tf.add(x, <span class=\"pl-c1\">1</span>.)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:1<span class=\"pl-pds\">\"</span></span>):\n  z <span class=\"pl-k\">=</span> tf.add(y, <span class=\"pl-c1\">1</span>.)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  sess.run(z)</pre></div>\n<p>it will consume 4GB on each GPU since a 4GB tensor needs to be materialized in the memory of each device.</p>\n<p>When distributing the computation across devices the memory usage characteristics would depend on how the computation (and thus intermediate tensor values) is distributed. Consider another trivial program:</p>\n<div class=\"highlight highlight-source-python\"><pre>a <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~4GB</span>\nb <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~4GB</span>\nc <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> ~4GB</span>\nd <span class=\"pl-k\">=</span> tf.add_n([a, b, c])\ne <span class=\"pl-k\">=</span> tf.reduce_sum(d)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  sess.run(e)</pre></div>\n<p>will consume ~12GB on one GPU (as 3 4GB tensors are materialized), but if you split it across 3 GPUs like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  a <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:1<span class=\"pl-pds\">\"</span></span>):\n  b <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:2<span class=\"pl-pds\">\"</span></span>):\n  c <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  d <span class=\"pl-k\">=</span> tf.add_n([a, b, c])\n  e <span class=\"pl-k\">=</span> tf.reduce_sum(d)\n\n<span class=\"pl-k\">with</span> tf.Session()\n  sess.run(e)</pre></div>\n<p>then it will consume ~4GB on GPU 0, ~4GB on GPU 1 and ~12GB on GPU 2 (since a 4GB tensor is materialized on each GPU, but additionally a 4GB tensor is shipped from GPU 0 to GPU 2 and another 4GB tensor is shipped from GPU 1 to GPU 2).</p>\n<p>But if you split it like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  a <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:1<span class=\"pl-pds\">\"</span></span>):\n  b <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  b_1 <span class=\"pl-k\">=</span> tf.add(a, b)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:2<span class=\"pl-pds\">\"</span></span>):\n  c <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  d <span class=\"pl-k\">=</span> tf.add(b_1, c)\n  e <span class=\"pl-k\">=</span> tf.reduce_sum(d)\n\n<span class=\"pl-k\">with</span> tf.Session()\n  sess.run(e)</pre></div>\n<p>it will consume ~4GB on GPU 1, ~8GB on GPU 1 and ~8GB on GPU 2, and if you split it like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  a <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  a_1 <span class=\"pl-k\">=</span> tf.reduce_sum(a)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:1<span class=\"pl-pds\">\"</span></span>):\n  b <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  b_1 <span class=\"pl-k\">=</span> tf.reduce_sum(b)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:2<span class=\"pl-pds\">\"</span></span>):\n  c <span class=\"pl-k\">=</span> tf.random_uniform([<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\n  c_1 <span class=\"pl-k\">=</span> tf.reduce_sum(c)\n  e <span class=\"pl-k\">=</span> tf.add_n([a_1, b_1, c_1])\n\n<span class=\"pl-k\">with</span> tf.Session()\n  sess.run(e)</pre></div>\n<p>it will consume ~4GB on each device.</p>\n<p>Does that make sense?</p>\n<p>Note that the TensorFlow runtime is free to optimize the graph execution and in theory could do such memory saving transformations itself. However, as of <em>today</em>, the examples I've described above should have the characteristics described.</p>\n<p>I haven't had the time to dig into your sample code to understand the custom framework being used to build the network and even so running it seems to require setting up a dataset etc. first. You might get some hint of where the memory is coming from by looking at the shapes of tensors being shipped across devices.  From a cursory look at <a href=\"https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism/blob/279285d4e9ea9741d7208a0e4c37d3ecf42d55c7/model.py#L75\">https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism/blob/279285d4e9ea9741d7208a0e4c37d3ecf42d55c7/model.py#L75</a> it seems that the tensors <code>conv3_1_1x1_proj_bn1</code> and <code>conv3_1_1x1_increase_bn</code> are being shipped from GPU 0 to 1, so I'd look at the shapes of those tensors, and any others going across devices. Besides the tensors being shipped, there is also the question of what the shapes and sizes of intermediate tensors being computed on the device is.</p>\n<p>Again, the code you have is a lot to go through to figure this out. If you can reduce it to a <a href=\"https://stackoverflow.com/help/mcve\" rel=\"nofollow\">minimal, complete, verifiable example</a> that will make it easier to figure out if there is a real problem or if the behavior you're seeing is justified given how the model is being distributed across devices.</p>\n<p>Hope that helps.</p>", "body_text": "@reger-men : The memory consumed by a model depends on the computation in the model, not just variable placement. For example, consider this trivial computation:\nx = tf.random_uniform([1000, 1000, 1000]) # A ~4GB tensor\ny = tf.add(x, 1.)\nz = tf.add(y, 1.)\n\nwith tf.Session() as sess:\n  sess.run(z)\nThis computation will consume 4GB. If I split it across 2 GPUs with something like:\nwith tf.device(\"/gpu:0\"):\n  x = tf.random_uniform([1000, 1000, 1000])\n  y = tf.add(x, 1.)\nwith tf.device(\"/gpu:1\"):\n  z = tf.add(y, 1.)\n\nwith tf.Session() as sess:\n  sess.run(z)\nit will consume 4GB on each GPU since a 4GB tensor needs to be materialized in the memory of each device.\nWhen distributing the computation across devices the memory usage characteristics would depend on how the computation (and thus intermediate tensor values) is distributed. Consider another trivial program:\na = tf.random_uniform([1000, 1000, 1000]) # ~4GB\nb = tf.random_uniform([1000, 1000, 1000]) # ~4GB\nc = tf.random_uniform([1000, 1000, 1000]) # ~4GB\nd = tf.add_n([a, b, c])\ne = tf.reduce_sum(d)\n\nwith tf.Session() as sess:\n  sess.run(e)\nwill consume ~12GB on one GPU (as 3 4GB tensors are materialized), but if you split it across 3 GPUs like:\nwith tf.device(\"/gpu:0\"):\n  a = tf.random_uniform([1000, 1000, 1000])\nwith tf.device(\"/gpu:1\"):\n  b = tf.random_uniform([1000, 1000, 1000])\nwith tf.device(\"/gpu:2\"):\n  c = tf.random_uniform([1000, 1000, 1000])\n  d = tf.add_n([a, b, c])\n  e = tf.reduce_sum(d)\n\nwith tf.Session()\n  sess.run(e)\nthen it will consume ~4GB on GPU 0, ~4GB on GPU 1 and ~12GB on GPU 2 (since a 4GB tensor is materialized on each GPU, but additionally a 4GB tensor is shipped from GPU 0 to GPU 2 and another 4GB tensor is shipped from GPU 1 to GPU 2).\nBut if you split it like:\nwith tf.device(\"/gpu:0\"):\n  a = tf.random_uniform([1000, 1000, 1000])\nwith tf.device(\"/gpu:1\"):\n  b = tf.random_uniform([1000, 1000, 1000])\n  b_1 = tf.add(a, b)\nwith tf.device(\"/gpu:2\"):\n  c = tf.random_uniform([1000, 1000, 1000])\n  d = tf.add(b_1, c)\n  e = tf.reduce_sum(d)\n\nwith tf.Session()\n  sess.run(e)\nit will consume ~4GB on GPU 1, ~8GB on GPU 1 and ~8GB on GPU 2, and if you split it like:\nwith tf.device(\"/gpu:0\"):\n  a = tf.random_uniform([1000, 1000, 1000])\n  a_1 = tf.reduce_sum(a)\nwith tf.device(\"/gpu:1\"):\n  b = tf.random_uniform([1000, 1000, 1000])\n  b_1 = tf.reduce_sum(b)\nwith tf.device(\"/gpu:2\"):\n  c = tf.random_uniform([1000, 1000, 1000])\n  c_1 = tf.reduce_sum(c)\n  e = tf.add_n([a_1, b_1, c_1])\n\nwith tf.Session()\n  sess.run(e)\nit will consume ~4GB on each device.\nDoes that make sense?\nNote that the TensorFlow runtime is free to optimize the graph execution and in theory could do such memory saving transformations itself. However, as of today, the examples I've described above should have the characteristics described.\nI haven't had the time to dig into your sample code to understand the custom framework being used to build the network and even so running it seems to require setting up a dataset etc. first. You might get some hint of where the memory is coming from by looking at the shapes of tensors being shipped across devices.  From a cursory look at https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism/blob/279285d4e9ea9741d7208a0e4c37d3ecf42d55c7/model.py#L75 it seems that the tensors conv3_1_1x1_proj_bn1 and conv3_1_1x1_increase_bn are being shipped from GPU 0 to 1, so I'd look at the shapes of those tensors, and any others going across devices. Besides the tensors being shipped, there is also the question of what the shapes and sizes of intermediate tensors being computed on the device is.\nAgain, the code you have is a lot to go through to figure this out. If you can reduce it to a minimal, complete, verifiable example that will make it easier to figure out if there is a real problem or if the behavior you're seeing is justified given how the model is being distributed across devices.\nHope that helps.", "body": "@reger-men : The memory consumed by a model depends on the computation in the model, not just variable placement. For example, consider this trivial computation:\r\n\r\n```python\r\nx = tf.random_uniform([1000, 1000, 1000]) # A ~4GB tensor\r\ny = tf.add(x, 1.)\r\nz = tf.add(y, 1.)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(z)\r\n```\r\n\r\nThis computation will consume 4GB. If I split it across 2 GPUs with something like:\r\n\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  x = tf.random_uniform([1000, 1000, 1000])\r\n  y = tf.add(x, 1.)\r\nwith tf.device(\"/gpu:1\"):\r\n  z = tf.add(y, 1.)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(z)\r\n```\r\n\r\nit will consume 4GB on each GPU since a 4GB tensor needs to be materialized in the memory of each device.\r\n\r\nWhen distributing the computation across devices the memory usage characteristics would depend on how the computation (and thus intermediate tensor values) is distributed. Consider another trivial program:\r\n\r\n```python\r\na = tf.random_uniform([1000, 1000, 1000]) # ~4GB\r\nb = tf.random_uniform([1000, 1000, 1000]) # ~4GB\r\nc = tf.random_uniform([1000, 1000, 1000]) # ~4GB\r\nd = tf.add_n([a, b, c])\r\ne = tf.reduce_sum(d)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(e)\r\n```\r\n\r\nwill consume ~12GB on one GPU (as 3 4GB tensors are materialized), but if you split it across 3 GPUs like:\r\n\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  a = tf.random_uniform([1000, 1000, 1000])\r\nwith tf.device(\"/gpu:1\"):\r\n  b = tf.random_uniform([1000, 1000, 1000])\r\nwith tf.device(\"/gpu:2\"):\r\n  c = tf.random_uniform([1000, 1000, 1000])\r\n  d = tf.add_n([a, b, c])\r\n  e = tf.reduce_sum(d)\r\n\r\nwith tf.Session()\r\n  sess.run(e)\r\n```\r\n\r\nthen it will consume ~4GB on GPU 0, ~4GB on GPU 1 and ~12GB on GPU 2 (since a 4GB tensor is materialized on each GPU, but additionally a 4GB tensor is shipped from GPU 0 to GPU 2 and another 4GB tensor is shipped from GPU 1 to GPU 2).\r\n\r\nBut if you split it like:\r\n\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  a = tf.random_uniform([1000, 1000, 1000])\r\nwith tf.device(\"/gpu:1\"):\r\n  b = tf.random_uniform([1000, 1000, 1000])\r\n  b_1 = tf.add(a, b)\r\nwith tf.device(\"/gpu:2\"):\r\n  c = tf.random_uniform([1000, 1000, 1000])\r\n  d = tf.add(b_1, c)\r\n  e = tf.reduce_sum(d)\r\n\r\nwith tf.Session()\r\n  sess.run(e)\r\n```\r\n\r\nit will consume ~4GB on GPU 1, ~8GB on GPU 1 and ~8GB on GPU 2, and if you split it like:\r\n\r\n```python\r\nwith tf.device(\"/gpu:0\"):\r\n  a = tf.random_uniform([1000, 1000, 1000])\r\n  a_1 = tf.reduce_sum(a)\r\nwith tf.device(\"/gpu:1\"):\r\n  b = tf.random_uniform([1000, 1000, 1000])\r\n  b_1 = tf.reduce_sum(b)\r\nwith tf.device(\"/gpu:2\"):\r\n  c = tf.random_uniform([1000, 1000, 1000])\r\n  c_1 = tf.reduce_sum(c)\r\n  e = tf.add_n([a_1, b_1, c_1])\r\n\r\nwith tf.Session()\r\n  sess.run(e)\r\n```\r\n\r\nit will consume ~4GB on each device.\r\n\r\nDoes that make sense?\r\n\r\nNote that the TensorFlow runtime is free to optimize the graph execution and in theory could do such memory saving transformations itself. However, as of _today_, the examples I've described above should have the characteristics described.\r\n\r\nI haven't had the time to dig into your sample code to understand the custom framework being used to build the network and even so running it seems to require setting up a dataset etc. first. You might get some hint of where the memory is coming from by looking at the shapes of tensors being shipped across devices.  From a cursory look at https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism/blob/279285d4e9ea9741d7208a0e4c37d3ecf42d55c7/model.py#L75 it seems that the tensors `conv3_1_1x1_proj_bn1` and `conv3_1_1x1_increase_bn` are being shipped from GPU 0 to 1, so I'd look at the shapes of those tensors, and any others going across devices. Besides the tensors being shipped, there is also the question of what the shapes and sizes of intermediate tensors being computed on the device is. \r\n\r\nAgain, the code you have is a lot to go through to figure this out. If you can reduce it to a [minimal, complete, verifiable example](https://stackoverflow.com/help/mcve) that will make it easier to figure out if there is a real problem or if the behavior you're seeing is justified given how the model is being distributed across devices.\r\n\r\nHope that helps."}