{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439601786", "html_url": "https://github.com/tensorflow/tensorflow/issues/21522#issuecomment-439601786", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21522", "id": 439601786, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTYwMTc4Ng==", "user": {"login": "reger-men", "id": 8779942, "node_id": "MDQ6VXNlcjg3Nzk5NDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/8779942?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reger-men", "html_url": "https://github.com/reger-men", "followers_url": "https://api.github.com/users/reger-men/followers", "following_url": "https://api.github.com/users/reger-men/following{/other_user}", "gists_url": "https://api.github.com/users/reger-men/gists{/gist_id}", "starred_url": "https://api.github.com/users/reger-men/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reger-men/subscriptions", "organizations_url": "https://api.github.com/users/reger-men/orgs", "repos_url": "https://api.github.com/users/reger-men/repos", "events_url": "https://api.github.com/users/reger-men/events{/privacy}", "received_events_url": "https://api.github.com/users/reger-men/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-17T08:58:28Z", "updated_at": "2018-11-17T09:02:09Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> as you can see in the screeshots, when I run the training on only 1 GPU the allocated memory is about 10GB. Run the same model with the same parameters (batch_size, input_size, etc.) on multiple GPUs required more than 24GB. How to explain that?</p>\n<p>My understanding the using of model parallelism, if my model is very large and does not fit into a single GPU. Therefore I distribute the model on different GPUs to make training possible. Of course, the data here has to be exchanged between the GPUs for both forward and backward propagation and the only way is through the host. But this does not explain why too much memory is allocated on the GPUs.</p>\n<p>I already inserted a new branch in the repo <a href=\"https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism.git\">\"cifar10\"</a>. You can also observe the same problem there.</p>", "body_text": "@asimshankar as you can see in the screeshots, when I run the training on only 1 GPU the allocated memory is about 10GB. Run the same model with the same parameters (batch_size, input_size, etc.) on multiple GPUs required more than 24GB. How to explain that?\nMy understanding the using of model parallelism, if my model is very large and does not fit into a single GPU. Therefore I distribute the model on different GPUs to make training possible. Of course, the data here has to be exchanged between the GPUs for both forward and backward propagation and the only way is through the host. But this does not explain why too much memory is allocated on the GPUs.\nI already inserted a new branch in the repo \"cifar10\". You can also observe the same problem there.", "body": "@asimshankar as you can see in the screeshots, when I run the training on only 1 GPU the allocated memory is about 10GB. Run the same model with the same parameters (batch_size, input_size, etc.) on multiple GPUs required more than 24GB. How to explain that?\r\n\r\nMy understanding the using of model parallelism, if my model is very large and does not fit into a single GPU. Therefore I distribute the model on different GPUs to make training possible. Of course, the data here has to be exchanged between the GPUs for both forward and backward propagation and the only way is through the host. But this does not explain why too much memory is allocated on the GPUs.\r\n\r\nI already inserted a new branch in the repo [\"cifar10\"](https://github.com/reger-men/PSPNet-Tensorflow-ModelParallelism.git). You can also observe the same problem there."}