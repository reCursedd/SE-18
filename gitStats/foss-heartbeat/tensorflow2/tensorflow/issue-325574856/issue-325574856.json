{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19489", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19489/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19489/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19489/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19489", "id": 325574856, "node_id": "MDU6SXNzdWUzMjU1NzQ4NTY=", "number": 19489, "title": "TF eager backdrop does not co-locate gradient computation correctly.  Results in Placement warning.  ", "user": {"login": "Noahyt", "id": 23128135, "node_id": "MDQ6VXNlcjIzMTI4MTM1", "avatar_url": "https://avatars1.githubusercontent.com/u/23128135?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Noahyt", "html_url": "https://github.com/Noahyt", "followers_url": "https://api.github.com/users/Noahyt/followers", "following_url": "https://api.github.com/users/Noahyt/following{/other_user}", "gists_url": "https://api.github.com/users/Noahyt/gists{/gist_id}", "starred_url": "https://api.github.com/users/Noahyt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Noahyt/subscriptions", "organizations_url": "https://api.github.com/users/Noahyt/orgs", "repos_url": "https://api.github.com/users/Noahyt/repos", "events_url": "https://api.github.com/users/Noahyt/events{/privacy}", "received_events_url": "https://api.github.com/users/Noahyt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-23T07:07:31Z", "updated_at": "2018-07-10T18:53:22Z", "closed_at": "2018-07-10T18:53:22Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.08</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:  Tesla V100-SXM2-16GB</li>\n<li><strong>Exact command to reproduce</strong>: -</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>While moving a model to eager execution, I encountered an error using gradient_tape for back propagation.  While as far as I can tell all operations are taking place on the GPU, during back prop I get the following error:</p>\n<pre><code>&gt;   File \"tf_registration_continuous.py\", line 128, in single_registration_step\n&gt;     elastic_grads = tape.gradient(loss_value, elastic_variable_list)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 767, in gradient\n&gt;     output_gradients=output_gradients)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 63, in imperative_grad\n&gt;     tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 147, in grad_fn\n&gt;     op_inputs, op_outputs, orig_outputs)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 115, in _magic_gradient_function\n&gt;     return grad_fn(mock_op, *out_grads)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 427, in _GatherV2Grad\n&gt;     params_shape = math_ops.to_int32(params_shape)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 875, in to_int32\n&gt;     return cast(x, dtypes.int32, name=name)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 787, in cast\n&gt;     x = gen_math_ops.cast(x, base_type, name=name)\n&gt;   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1548, in cast\n&gt;     _six.raise_from(_core._status_to_exception(e.code, message), None)\n&gt;   File \"/share/software/user/open/py-scipystack/1.0_py27/lib/python2.7/site-packages/six.py\", line 718, in raise_from\n&gt;     raise value\n&gt; tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu() methods, or transparently copied by using tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: ToInt32/\n</code></pre>\n<p>I might also put in a feature request to make backdrop error messages more verbose -- since I can not tell what operation in the model is actually causing this error.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.08\nPython version: 2.7\nCUDA/cuDNN version:\nGPU model and memory:  Tesla V100-SXM2-16GB\nExact command to reproduce: -\n\nDescribe the problem\nWhile moving a model to eager execution, I encountered an error using gradient_tape for back propagation.  While as far as I can tell all operations are taking place on the GPU, during back prop I get the following error:\n>   File \"tf_registration_continuous.py\", line 128, in single_registration_step\n>     elastic_grads = tape.gradient(loss_value, elastic_variable_list)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 767, in gradient\n>     output_gradients=output_gradients)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 63, in imperative_grad\n>     tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 147, in grad_fn\n>     op_inputs, op_outputs, orig_outputs)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 115, in _magic_gradient_function\n>     return grad_fn(mock_op, *out_grads)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 427, in _GatherV2Grad\n>     params_shape = math_ops.to_int32(params_shape)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 875, in to_int32\n>     return cast(x, dtypes.int32, name=name)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 787, in cast\n>     x = gen_math_ops.cast(x, base_type, name=name)\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1548, in cast\n>     _six.raise_from(_core._status_to_exception(e.code, message), None)\n>   File \"/share/software/user/open/py-scipystack/1.0_py27/lib/python2.7/site-packages/six.py\", line 718, in raise_from\n>     raise value\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu() methods, or transparently copied by using tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: ToInt32/\n\nI might also put in a feature request to make backdrop error messages more verbose -- since I can not tell what operation in the model is actually causing this error.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux \r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.08\r\n- **Python version**: 2.7\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:  Tesla V100-SXM2-16GB\r\n- **Exact command to reproduce**: - \r\n\r\n### Describe the problem\r\nWhile moving a model to eager execution, I encountered an error using gradient_tape for back propagation.  While as far as I can tell all operations are taking place on the GPU, during back prop I get the following error:\r\n\r\n```\r\n>   File \"tf_registration_continuous.py\", line 128, in single_registration_step\r\n>     elastic_grads = tape.gradient(loss_value, elastic_variable_list)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 767, in gradient\r\n>     output_gradients=output_gradients)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/imperative_grad.py\", line 63, in imperative_grad\r\n>     tape._tape, vspace, target, sources, output_gradients)  # pylint: disable=protected-access\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 147, in grad_fn\r\n>     op_inputs, op_outputs, orig_outputs)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/eager/backprop.py\", line 115, in _magic_gradient_function\r\n>     return grad_fn(mock_op, *out_grads)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/array_grad.py\", line 427, in _GatherV2Grad\r\n>     params_shape = math_ops.to_int32(params_shape)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 875, in to_int32\r\n>     return cast(x, dtypes.int32, name=name)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/math_ops.py\", line 787, in cast\r\n>     x = gen_math_ops.cast(x, base_type, name=name)\r\n>   File \"/share/software/user/open/py-tensorflow/1.8.0_py27/lib/python2.7/site-packages/tensorflow/python/ops/gen_math_ops.py\", line 1548, in cast\r\n>     _six.raise_from(_core._status_to_exception(e.code, message), None)\r\n>   File \"/share/software/user/open/py-scipystack/1.0_py27/lib/python2.7/site-packages/six.py\", line 718, in raise_from\r\n>     raise value\r\n> tensorflow.python.framework.errors_impl.InvalidArgumentError: Tensors on conflicting devices: cannot compute Cast as input #0 was expected to be on /job:localhost/replica:0/task:0/device:GPU:0 but is actually on /job:localhost/replica:0/task:0/device:CPU:0 (operation running on /job:localhost/replica:0/task:0/device:GPU:0) Tensors can be copied explicitly using .gpu() or .cpu() methods, or transparently copied by using tf.enable_eager_execution(device_policy=tfe.DEVICE_PLACEMENT_SILENT). Copying tensors between devices may slow down your model [Op:Cast] name: ToInt32/\r\n```\r\n\r\nI might also put in a feature request to make backdrop error messages more verbose -- since I can not tell what operation in the model is actually causing this error.  "}