{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2397", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2397/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2397/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2397/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2397", "id": 155184824, "node_id": "MDU6SXNzdWUxNTUxODQ4MjQ=", "number": 2397, "title": "Performance of Tensorflow distributed training is much slower than caffe multi-GPU training", "user": {"login": "ZhuFengdaaa", "id": 9649227, "node_id": "MDQ6VXNlcjk2NDkyMjc=", "avatar_url": "https://avatars0.githubusercontent.com/u/9649227?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhuFengdaaa", "html_url": "https://github.com/ZhuFengdaaa", "followers_url": "https://api.github.com/users/ZhuFengdaaa/followers", "following_url": "https://api.github.com/users/ZhuFengdaaa/following{/other_user}", "gists_url": "https://api.github.com/users/ZhuFengdaaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhuFengdaaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhuFengdaaa/subscriptions", "organizations_url": "https://api.github.com/users/ZhuFengdaaa/orgs", "repos_url": "https://api.github.com/users/ZhuFengdaaa/repos", "events_url": "https://api.github.com/users/ZhuFengdaaa/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhuFengdaaa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 23, "created_at": "2016-05-17T06:19:57Z", "updated_at": "2018-11-03T09:49:34Z", "closed_at": "2016-11-14T19:05:21Z", "author_association": "NONE", "body_html": "<p>I have trained inceptionv3 using tensorflow both on multi-GPU version and distributed version (two machine, four GPU each). Each GPU processes 32 images per iteration under both settings. However, the distributed training speed is twice as slow as the <code>caffe</code> multi-GPU version. I'm wondering how to improve the performance of distributed training.</p>\n<p><strong>Configuration:</strong></p>\n<p>Two machine, both of them have totally same environment: CentOS 7, 4 GTX TITAN X GPUs, 32 Intel Xeon CPU E5-2630 v3 2.40GHz processors, and 131GB Memory. Network IO between machines is 125MB/s, <code>ping</code> delay is 0.1ms and local read speed is 1GB/s (RAID5). The distributed training code is the newest master branch <a href=\"https://github.com/tensorflow/models/tree/master/inception\">here</a>. In distributed version, there are 4 <code>workers</code> on each machine, each work are assigned to 1 GPU and there is only one <code>ps</code> server started. I read data file from local disk.</p>\n<p>the start script for each worker (8 workers in total) is</p>\n<pre><code>~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--ps_hosts='10.10.102.28:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.28:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &amp;\n</code></pre>\n<p><strong>Runtime logging:</strong></p>\n<p>The training speed is</p>\n<pre><code>INFO:tensorflow:Worker 6: 2016-05-16 21:07:22.101672: step 390, loss = 8.15(24.0 examples/sec; 1.334  sec/batch)\nINFO:tensorflow:Worker 5: 2016-05-16 21:07:22.101666: step 390, loss = 8.10(24.0 examples/sec; 1.335  sec/batch)\nINFO:tensorflow:Worker 4: 2016-05-16 21:07:22.101768: step 390, loss = 8.11(24.0 examples/sec; 1.333  sec/batch)\nINFO:tensorflow:Worker 7: 2016-05-16 21:07:22.102245: step 390, loss = 8.03(24.1 examples/sec; 1.330  sec/batch)\n</code></pre>\n<p>This speed is twice as slow as <code>caffe</code> multi-GPU training on one machine(ensure that experiments are under same settings, each GPU process 32 images per iteration). I used <code>nvidia-smi -l 1</code> to watch the GPU usage, and find that GPUs only busy in 25% running time. The <code>top</code> command print like this:</p>\n<pre><code> %Cpu(s):  2.4 us, 35.2 sy,  0.0 ni, 62.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem : 13174038+total, 21064972 free, 18241616 used, 92433792 buff/cache\nKiB Swap: 16777212 total, 16724332 free,    52880 used. 88948128 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \n  685 peter+  20   0  0.191t 2.336g 453624 S 878.1  1.9  83:03.03 python      \n15102 peter+  20   0 13.715g 5.522g  41820 S 293.7  4.4 348:39.74 python      \n  682 peter+  20   0  0.192t 2.476g 453608 S  13.0  2.0  69:18.00 python      \n  683 peter+  20   0  0.193t 4.093g 978228 S  11.6  3.3  84:40.56 python      \n  684 peter+  20   0  0.191t 2.410g 453612 S  10.6  1.9  88:50.77 python \n</code></pre>\n<p><strong>What I have tried</strong></p>\n<ul>\n<li>By modifying the <code>num_preprocess_threads</code> and the <code>num_readers</code>, I got the best performance <code>1.333  sec/batch</code>, when I set both variable to 1.</li>\n<li>Modify the queue capacity is not helpful.</li>\n<li>I believe the <code>bath_inputs()</code> is executed on <code>CPU:0</code>, because in <code>image_processing.py</code> line 132: <code>with tf.device('/cpu:0'):</code></li>\n<li>I refered to this <a href=\"https://github.com/tensorflow/models/issues/47\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/47/hovercard\">issue</a> and wondering if the bottleneck is CPU/IO.</li>\n</ul>\n<p>How to get a better performance ?</p>", "body_text": "I have trained inceptionv3 using tensorflow both on multi-GPU version and distributed version (two machine, four GPU each). Each GPU processes 32 images per iteration under both settings. However, the distributed training speed is twice as slow as the caffe multi-GPU version. I'm wondering how to improve the performance of distributed training.\nConfiguration:\nTwo machine, both of them have totally same environment: CentOS 7, 4 GTX TITAN X GPUs, 32 Intel Xeon CPU E5-2630 v3 2.40GHz processors, and 131GB Memory. Network IO between machines is 125MB/s, ping delay is 0.1ms and local read speed is 1GB/s (RAID5). The distributed training code is the newest master branch here. In distributed version, there are 4 workers on each machine, each work are assigned to 1 GPU and there is only one ps server started. I read data file from local disk.\nthe start script for each worker (8 workers in total) is\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--ps_hosts='10.10.102.28:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.28:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n\nRuntime logging:\nThe training speed is\nINFO:tensorflow:Worker 6: 2016-05-16 21:07:22.101672: step 390, loss = 8.15(24.0 examples/sec; 1.334  sec/batch)\nINFO:tensorflow:Worker 5: 2016-05-16 21:07:22.101666: step 390, loss = 8.10(24.0 examples/sec; 1.335  sec/batch)\nINFO:tensorflow:Worker 4: 2016-05-16 21:07:22.101768: step 390, loss = 8.11(24.0 examples/sec; 1.333  sec/batch)\nINFO:tensorflow:Worker 7: 2016-05-16 21:07:22.102245: step 390, loss = 8.03(24.1 examples/sec; 1.330  sec/batch)\n\nThis speed is twice as slow as caffe multi-GPU training on one machine(ensure that experiments are under same settings, each GPU process 32 images per iteration). I used nvidia-smi -l 1 to watch the GPU usage, and find that GPUs only busy in 25% running time. The top command print like this:\n %Cpu(s):  2.4 us, 35.2 sy,  0.0 ni, 62.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem : 13174038+total, 21064972 free, 18241616 used, 92433792 buff/cache\nKiB Swap: 16777212 total, 16724332 free,    52880 used. 88948128 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \n  685 peter+  20   0  0.191t 2.336g 453624 S 878.1  1.9  83:03.03 python      \n15102 peter+  20   0 13.715g 5.522g  41820 S 293.7  4.4 348:39.74 python      \n  682 peter+  20   0  0.192t 2.476g 453608 S  13.0  2.0  69:18.00 python      \n  683 peter+  20   0  0.193t 4.093g 978228 S  11.6  3.3  84:40.56 python      \n  684 peter+  20   0  0.191t 2.410g 453612 S  10.6  1.9  88:50.77 python \n\nWhat I have tried\n\nBy modifying the num_preprocess_threads and the num_readers, I got the best performance 1.333  sec/batch, when I set both variable to 1.\nModify the queue capacity is not helpful.\nI believe the bath_inputs() is executed on CPU:0, because in image_processing.py line 132: with tf.device('/cpu:0'):\nI refered to this issue and wondering if the bottleneck is CPU/IO.\n\nHow to get a better performance ?", "body": "I have trained inceptionv3 using tensorflow both on multi-GPU version and distributed version (two machine, four GPU each). Each GPU processes 32 images per iteration under both settings. However, the distributed training speed is twice as slow as the `caffe` multi-GPU version. I'm wondering how to improve the performance of distributed training. \n\n**Configuration:** \n\nTwo machine, both of them have totally same environment: CentOS 7, 4 GTX TITAN X GPUs, 32 Intel Xeon CPU E5-2630 v3 2.40GHz processors, and 131GB Memory. Network IO between machines is 125MB/s, `ping` delay is 0.1ms and local read speed is 1GB/s (RAID5). The distributed training code is the newest master branch [here](https://github.com/tensorflow/models/tree/master/inception). In distributed version, there are 4 `workers` on each machine, each work are assigned to 1 GPU and there is only one `ps` server started. I read data file from local disk. \n\nthe start script for each worker (8 workers in total) is\n\n```\n~/models/inception/bazel-bin/inception/imagenet_distributed_train \\\n--batch_size=32 \\\n--data_dir=/data1/imagenet1k \\\n--job_name='worker' \\\n--task_id=2 \\\n--ps_hosts='10.10.102.28:2220' \\\n--worker_hosts='10.10.102.28:2221,10.10.102.28:2222,10.10.102.28:2223,10.10.102.28:2224,10.10.102.29:2221,10.10.102.29:2222,10.10.102.29:2223,10.10.102.29:2224' &\n```\n\n**Runtime logging:**\n\nThe training speed is \n\n```\nINFO:tensorflow:Worker 6: 2016-05-16 21:07:22.101672: step 390, loss = 8.15(24.0 examples/sec; 1.334  sec/batch)\nINFO:tensorflow:Worker 5: 2016-05-16 21:07:22.101666: step 390, loss = 8.10(24.0 examples/sec; 1.335  sec/batch)\nINFO:tensorflow:Worker 4: 2016-05-16 21:07:22.101768: step 390, loss = 8.11(24.0 examples/sec; 1.333  sec/batch)\nINFO:tensorflow:Worker 7: 2016-05-16 21:07:22.102245: step 390, loss = 8.03(24.1 examples/sec; 1.330  sec/batch)\n```\n\nThis speed is twice as slow as `caffe` multi-GPU training on one machine(ensure that experiments are under same settings, each GPU process 32 images per iteration). I used `nvidia-smi -l 1` to watch the GPU usage, and find that GPUs only busy in 25% running time. The `top` command print like this:\n\n```\n %Cpu(s):  2.4 us, 35.2 sy,  0.0 ni, 62.3 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem : 13174038+total, 21064972 free, 18241616 used, 92433792 buff/cache\nKiB Swap: 16777212 total, 16724332 free,    52880 used. 88948128 avail Mem \n\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     \n  685 peter+  20   0  0.191t 2.336g 453624 S 878.1  1.9  83:03.03 python      \n15102 peter+  20   0 13.715g 5.522g  41820 S 293.7  4.4 348:39.74 python      \n  682 peter+  20   0  0.192t 2.476g 453608 S  13.0  2.0  69:18.00 python      \n  683 peter+  20   0  0.193t 4.093g 978228 S  11.6  3.3  84:40.56 python      \n  684 peter+  20   0  0.191t 2.410g 453612 S  10.6  1.9  88:50.77 python \n```\n\n**What I have tried**\n- By modifying the `num_preprocess_threads` and the `num_readers`, I got the best performance `1.333  sec/batch`, when I set both variable to 1.\n- Modify the queue capacity is not helpful.\n- I believe the `bath_inputs()` is executed on `CPU:0`, because in `image_processing.py` line 132: `with tf.device('/cpu:0'):`\n- I refered to this [issue](https://github.com/tensorflow/models/issues/47) and wondering if the bottleneck is CPU/IO. \n\nHow to get a better performance ?\n"}