{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20846", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20846/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20846/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20846/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20846", "id": 341600175, "node_id": "MDU6SXNzdWUzNDE2MDAxNzU=", "number": 20846, "title": "Tensorflow inference on Android, can't find 'Iterator' op. No OpKernel was registered to support Op", "user": {"login": "gerardocervantes8", "id": 21228908, "node_id": "MDQ6VXNlcjIxMjI4OTA4", "avatar_url": "https://avatars3.githubusercontent.com/u/21228908?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gerardocervantes8", "html_url": "https://github.com/gerardocervantes8", "followers_url": "https://api.github.com/users/gerardocervantes8/followers", "following_url": "https://api.github.com/users/gerardocervantes8/following{/other_user}", "gists_url": "https://api.github.com/users/gerardocervantes8/gists{/gist_id}", "starred_url": "https://api.github.com/users/gerardocervantes8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gerardocervantes8/subscriptions", "organizations_url": "https://api.github.com/users/gerardocervantes8/orgs", "repos_url": "https://api.github.com/users/gerardocervantes8/repos", "events_url": "https://api.github.com/users/gerardocervantes8/events{/privacy}", "received_events_url": "https://api.github.com/users/gerardocervantes8/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "robieta", "id": 13089297, "node_id": "MDQ6VXNlcjEzMDg5Mjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/13089297?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robieta", "html_url": "https://github.com/robieta", "followers_url": "https://api.github.com/users/robieta/followers", "following_url": "https://api.github.com/users/robieta/following{/other_user}", "gists_url": "https://api.github.com/users/robieta/gists{/gist_id}", "starred_url": "https://api.github.com/users/robieta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robieta/subscriptions", "organizations_url": "https://api.github.com/users/robieta/orgs", "repos_url": "https://api.github.com/users/robieta/repos", "events_url": "https://api.github.com/users/robieta/events{/privacy}", "received_events_url": "https://api.github.com/users/robieta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-07-16T17:02:56Z", "updated_at": "2018-08-10T18:11:30Z", "closed_at": "2018-08-10T18:11:30Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0 (For training and for creating the .so and .jar files)</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.14.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc version 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: None, running using CPU</li>\n<li><strong>Android Studio</strong>: 3.1.3</li>\n<li><strong>Android NDK</strong>: 14b</li>\n<li><strong>Andorid SDK</strong>: v28</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to run a trained Tensorflow model on an Android device.  The model I am trying to run on the android mobile device uses an Iterator operation in the inference graph.</p>\n<p>I am compiling from source using:<br>\n<code>bazel build -c opt --copt=-D__ANDROID_TYPES_FULL__ //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a</code></p>\n<p>17.2 MB (.so file)<br>\nI can load the graph and use the feed method from TensorFlowInferenceInterface, but whenever I try to make a prediction by using a run method from the  TensorFlowInferenceInterface.  I get the \"java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.\" error.</p>\n<p>I asked on Stackoverflow here <a href=\"https://stackoverflow.com/questions/51331474/tensorflow-inferences-on-android-cant-find-iterator-op-no-opkernel-was-regi\" rel=\"nofollow\">https://stackoverflow.com/questions/51331474/tensorflow-inferences-on-android-cant-find-iterator-op-no-opkernel-was-regi</a> but I believe this might be a better place to ask because this appears to be a bug.</p>\n<p>I am compiling from source using the D__ANDROID_TYPES_FULL__ so I would expect it to be able to find the iterator operation, which is why I believe it's a bug.</p>\n<p>I am able to run this using a Python/Tensorflow script on my laptop, but am unable to make it run on an Android device.  I give the python script I use to run the model below.</p>\n<p>I tried attaching the frozen model that I run in the code, but I couldn't attach on Github because the file size was too big.  The model was made from using this code: <a href=\"https://github.com/tensorflow/nmt\">https://github.com/tensorflow/nmt</a> and saving the model when making an inference.</p>\n<h3>Source code / logs</h3>\n<p>This the error I get in Android Studio</p>\n<pre><code>I/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\n    E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\n    I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\n    I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\n    I/TensorFlowInferenceInterface: Model load took 764ms, TensorFlow version: 1.8.0\n    I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/laptop_frozen_graph_init_tables.pb'\n    E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[batch_size, src_data], outputs:[index_to_string_Lookup]\n    E/AndroidRuntime: FATAL EXCEPTION: Thread-5769\n                      Process: com.example.student.projecttest, PID: 6805\n                      java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.  Registered devices: [CPU], Registered kernels:\n                        &lt;no registered kernels&gt;\n    \n                      \t [[Node: Iterator = Iterator[container=\"infer\", output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], shared_name=\"\"]()]]\n                          at org.tensorflow.Session.run(Native Method)\n                          at org.tensorflow.Session.access$100(Session.java:48)\n                          at org.tensorflow.Session$Runner.runHelper(Session.java:298)\n                          at org.tensorflow.Session$Runner.run(Session.java:248)\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\n                          at com.example.student.projecttest.MainActivity.translateToFrench(MainActivity.java:79)\n                          at com.example.student.projecttest.MainActivity$1$1.run(MainActivity.java:42)\n</code></pre>\n<p>This is the code I use to run the model on my laptop</p>\n<pre><code>import tensorflow as tf\n\ndef load_graph(frozen_graph_filename):\n    # We load the protobuf file from the disk and parse it to retrieve the \n    # unserialized graph_def\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we import the graph_def into a new Graph and returns it \n    with tf.Graph().as_default() as graph:\n        # The name var will prefix every op/nodes in your graph\n        # Since we load everything in a new graph, this is not needed\n        tf.import_graph_def(graph_def, name=\"prefix\")\n    return graph\n\ntgt_eos = '&lt;/s&gt;'\nfrozen_model_path =  'laptop_frozen_graph_init_tables.pb'\ngraph = load_graph(frozen_model_path)\n\nwith tf.Session(graph = graph) as sess:\n\n  #Get input tensors\n  src_file_placeholder = sess.graph.get_tensor_by_name(\"prefix/src_data:0\")\n  batch_size_placeholder = sess.graph.get_tensor_by_name(\"prefix/batch_size:0\")\n  \n  \n  #Initialize tables\n  init_tables_operation = sess.graph.get_operation_by_name(\"prefix/init_all_tables\")\n  sess.run(init_tables_operation)\n  \n  #Get init iterator operation\n  make_iterator_operation = sess.graph.get_operation_by_name(\"prefix/MakeIterator\")\n  \n  \n  src_data = ['Bonjour', 'tout', 'le', 'monde', 'c', '\\'', 'est', 'vrai', tgt_eos]\n  \n  #Make_iterator has to be fed with src_data\n  sess.run(make_iterator_operation, feed_dict={\n            src_file_placeholder: src_data,\n            batch_size_placeholder: 32\n        })\n\n  #Get output tensor\n  output0 = graph.get_tensor_by_name(\"prefix/index_to_string_Lookup:0\")\n  \n  #Iterate through input until we run out of input\n  try:\n    while True:\n      txt_output = sess.run(output0)\n      print(txt_output)\n  except tf.errors.OutOfRangeError:\n      print('Done inferencing')\n</code></pre>\n<p>This is the code I use in Android Studio which produces a very error to the one mentioned above.</p>\n<pre><code>        String modelName = \"laptop_frozen_graph_init_tables.pb\";\n        String fullModelPath = \"file:///android_asset/\" + modelName;\n\n        AssetManager assetManager = getAssets();\n\n        TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface(assetManager, fullModelPath);\n        Graph graph = inferenceInterface.graph();\n\n        inferenceInterface.run(new String[]{\"init_all_tables\"});\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.8.0 (For training and for creating the .so and .jar files)\nPython version: Python 3.5.2\nBazel version (if compiling from source): 0.14.1\nGCC/Compiler version (if compiling from source): gcc version 5.4.0\nCUDA/cuDNN version: N/A\nGPU model and memory: None, running using CPU\nAndroid Studio: 3.1.3\nAndroid NDK: 14b\nAndorid SDK: v28\nExact command to reproduce:\n\nDescribe the problem\nI am trying to run a trained Tensorflow model on an Android device.  The model I am trying to run on the android mobile device uses an Iterator operation in the inference graph.\nI am compiling from source using:\nbazel build -c opt --copt=-D__ANDROID_TYPES_FULL__ //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a\n17.2 MB (.so file)\nI can load the graph and use the feed method from TensorFlowInferenceInterface, but whenever I try to make a prediction by using a run method from the  TensorFlowInferenceInterface.  I get the \"java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.\" error.\nI asked on Stackoverflow here https://stackoverflow.com/questions/51331474/tensorflow-inferences-on-android-cant-find-iterator-op-no-opkernel-was-regi but I believe this might be a better place to ask because this appears to be a bug.\nI am compiling from source using the D__ANDROID_TYPES_FULL__ so I would expect it to be able to find the iterator operation, which is why I believe it's a bug.\nI am able to run this using a Python/Tensorflow script on my laptop, but am unable to make it run on an Android device.  I give the python script I use to run the model below.\nI tried attaching the frozen model that I run in the code, but I couldn't attach on Github because the file size was too big.  The model was made from using this code: https://github.com/tensorflow/nmt and saving the model when making an inference.\nSource code / logs\nThis the error I get in Android Studio\nI/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\n    E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\n    I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\n    I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\n    I/TensorFlowInferenceInterface: Model load took 764ms, TensorFlow version: 1.8.0\n    I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/laptop_frozen_graph_init_tables.pb'\n    E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[batch_size, src_data], outputs:[index_to_string_Lookup]\n    E/AndroidRuntime: FATAL EXCEPTION: Thread-5769\n                      Process: com.example.student.projecttest, PID: 6805\n                      java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.  Registered devices: [CPU], Registered kernels:\n                        <no registered kernels>\n    \n                      \t [[Node: Iterator = Iterator[container=\"infer\", output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], shared_name=\"\"]()]]\n                          at org.tensorflow.Session.run(Native Method)\n                          at org.tensorflow.Session.access$100(Session.java:48)\n                          at org.tensorflow.Session$Runner.runHelper(Session.java:298)\n                          at org.tensorflow.Session$Runner.run(Session.java:248)\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\n                          at com.example.student.projecttest.MainActivity.translateToFrench(MainActivity.java:79)\n                          at com.example.student.projecttest.MainActivity$1$1.run(MainActivity.java:42)\n\nThis is the code I use to run the model on my laptop\nimport tensorflow as tf\n\ndef load_graph(frozen_graph_filename):\n    # We load the protobuf file from the disk and parse it to retrieve the \n    # unserialized graph_def\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we import the graph_def into a new Graph and returns it \n    with tf.Graph().as_default() as graph:\n        # The name var will prefix every op/nodes in your graph\n        # Since we load everything in a new graph, this is not needed\n        tf.import_graph_def(graph_def, name=\"prefix\")\n    return graph\n\ntgt_eos = '</s>'\nfrozen_model_path =  'laptop_frozen_graph_init_tables.pb'\ngraph = load_graph(frozen_model_path)\n\nwith tf.Session(graph = graph) as sess:\n\n  #Get input tensors\n  src_file_placeholder = sess.graph.get_tensor_by_name(\"prefix/src_data:0\")\n  batch_size_placeholder = sess.graph.get_tensor_by_name(\"prefix/batch_size:0\")\n  \n  \n  #Initialize tables\n  init_tables_operation = sess.graph.get_operation_by_name(\"prefix/init_all_tables\")\n  sess.run(init_tables_operation)\n  \n  #Get init iterator operation\n  make_iterator_operation = sess.graph.get_operation_by_name(\"prefix/MakeIterator\")\n  \n  \n  src_data = ['Bonjour', 'tout', 'le', 'monde', 'c', '\\'', 'est', 'vrai', tgt_eos]\n  \n  #Make_iterator has to be fed with src_data\n  sess.run(make_iterator_operation, feed_dict={\n            src_file_placeholder: src_data,\n            batch_size_placeholder: 32\n        })\n\n  #Get output tensor\n  output0 = graph.get_tensor_by_name(\"prefix/index_to_string_Lookup:0\")\n  \n  #Iterate through input until we run out of input\n  try:\n    while True:\n      txt_output = sess.run(output0)\n      print(txt_output)\n  except tf.errors.OutOfRangeError:\n      print('Done inferencing')\n\nThis is the code I use in Android Studio which produces a very error to the one mentioned above.\n        String modelName = \"laptop_frozen_graph_init_tables.pb\";\n        String fullModelPath = \"file:///android_asset/\" + modelName;\n\n        AssetManager assetManager = getAssets();\n\n        TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface(assetManager, fullModelPath);\n        Graph graph = inferenceInterface.graph();\n\n        inferenceInterface.run(new String[]{\"init_all_tables\"});", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.8.0 (For training and for creating the .so and .jar files)\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 5.4.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: None, running using CPU\r\n- **Android Studio**: 3.1.3\r\n- **Android NDK**: 14b\r\n- **Andorid SDK**: v28\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\n\r\nI am trying to run a trained Tensorflow model on an Android device.  The model I am trying to run on the android mobile device uses an Iterator operation in the inference graph.\r\n\r\nI am compiling from source using:\r\n`bazel build -c opt --copt=-D__ANDROID_TYPES_FULL__ //tensorflow/contrib/android:libtensorflow_inference.so --crosstool_top=//external:android/crosstool --host_crosstool_top=@bazel_tools//tools/cpp:toolchain --cpu=armeabi-v7a`\r\n\r\n17.2 MB (.so file)\r\nI can load the graph and use the feed method from TensorFlowInferenceInterface, but whenever I try to make a prediction by using a run method from the  TensorFlowInferenceInterface.  I get the \"java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.\" error.\r\n\r\nI asked on Stackoverflow here https://stackoverflow.com/questions/51331474/tensorflow-inferences-on-android-cant-find-iterator-op-no-opkernel-was-regi but I believe this might be a better place to ask because this appears to be a bug.\r\n\r\nI am compiling from source using the D__ANDROID_TYPES_FULL__ so I would expect it to be able to find the iterator operation, which is why I believe it's a bug.  \r\n\r\nI am able to run this using a Python/Tensorflow script on my laptop, but am unable to make it run on an Android device.  I give the python script I use to run the model below.\r\n\r\n I tried attaching the frozen model that I run in the code, but I couldn't attach on Github because the file size was too big.  The model was made from using this code: https://github.com/tensorflow/nmt and saving the model when making an inference.\r\n### Source code / logs\r\n\r\nThis the error I get in Android Studio\r\n```\r\nI/TensorFlowInferenceInterface: Checking to see if TensorFlow native methods are already loaded\r\n    E/art: No implementation found for long org.tensorflow.contrib.android.RunStats.allocate() (tried Java_org_tensorflow_contrib_android_RunStats_allocate and Java_org_tensorflow_contrib_android_RunStats_allocate__)\r\n    I/TensorFlowInferenceInterface: TensorFlow native methods not found, attempting to load via tensorflow_inference\r\n    I/TensorFlowInferenceInterface: Successfully loaded TensorFlow native methods (RunStats error may be ignored)\r\n    I/TensorFlowInferenceInterface: Model load took 764ms, TensorFlow version: 1.8.0\r\n    I/TensorFlowInferenceInterface: Successfully loaded model from 'file:///android_asset/laptop_frozen_graph_init_tables.pb'\r\n    E/TensorFlowInferenceInterface: Failed to run TensorFlow inference with inputs:[batch_size, src_data], outputs:[index_to_string_Lookup]\r\n    E/AndroidRuntime: FATAL EXCEPTION: Thread-5769\r\n                      Process: com.example.student.projecttest, PID: 6805\r\n                      java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Iterator' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n                        <no registered kernels>\r\n    \r\n                      \t [[Node: Iterator = Iterator[container=\"infer\", output_shapes=[[?,?], [?]], output_types=[DT_INT32, DT_INT32], shared_name=\"\"]()]]\r\n                          at org.tensorflow.Session.run(Native Method)\r\n                          at org.tensorflow.Session.access$100(Session.java:48)\r\n                          at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n                          at org.tensorflow.Session$Runner.run(Session.java:248)\r\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:228)\r\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n                          at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:187)\r\n                          at com.example.student.projecttest.MainActivity.translateToFrench(MainActivity.java:79)\r\n                          at com.example.student.projecttest.MainActivity$1$1.run(MainActivity.java:42)\r\n```\r\n\r\n\r\nThis is the code I use to run the model on my laptop\r\n```\r\nimport tensorflow as tf\r\n\r\ndef load_graph(frozen_graph_filename):\r\n    # We load the protobuf file from the disk and parse it to retrieve the \r\n    # unserialized graph_def\r\n    with tf.gfile.GFile(frozen_graph_filename, \"rb\") as f:\r\n        graph_def = tf.GraphDef()\r\n        graph_def.ParseFromString(f.read())\r\n\r\n    # Then, we import the graph_def into a new Graph and returns it \r\n    with tf.Graph().as_default() as graph:\r\n        # The name var will prefix every op/nodes in your graph\r\n        # Since we load everything in a new graph, this is not needed\r\n        tf.import_graph_def(graph_def, name=\"prefix\")\r\n    return graph\r\n\r\ntgt_eos = '</s>'\r\nfrozen_model_path =  'laptop_frozen_graph_init_tables.pb'\r\ngraph = load_graph(frozen_model_path)\r\n\r\nwith tf.Session(graph = graph) as sess:\r\n\r\n  #Get input tensors\r\n  src_file_placeholder = sess.graph.get_tensor_by_name(\"prefix/src_data:0\")\r\n  batch_size_placeholder = sess.graph.get_tensor_by_name(\"prefix/batch_size:0\")\r\n  \r\n  \r\n  #Initialize tables\r\n  init_tables_operation = sess.graph.get_operation_by_name(\"prefix/init_all_tables\")\r\n  sess.run(init_tables_operation)\r\n  \r\n  #Get init iterator operation\r\n  make_iterator_operation = sess.graph.get_operation_by_name(\"prefix/MakeIterator\")\r\n  \r\n  \r\n  src_data = ['Bonjour', 'tout', 'le', 'monde', 'c', '\\'', 'est', 'vrai', tgt_eos]\r\n  \r\n  #Make_iterator has to be fed with src_data\r\n  sess.run(make_iterator_operation, feed_dict={\r\n            src_file_placeholder: src_data,\r\n            batch_size_placeholder: 32\r\n        })\r\n\r\n  #Get output tensor\r\n  output0 = graph.get_tensor_by_name(\"prefix/index_to_string_Lookup:0\")\r\n  \r\n  #Iterate through input until we run out of input\r\n  try:\r\n    while True:\r\n      txt_output = sess.run(output0)\r\n      print(txt_output)\r\n  except tf.errors.OutOfRangeError:\r\n      print('Done inferencing')\r\n```\r\n\r\nThis is the code I use in Android Studio which produces a very error to the one mentioned above.\r\n```\r\n        String modelName = \"laptop_frozen_graph_init_tables.pb\";\r\n        String fullModelPath = \"file:///android_asset/\" + modelName;\r\n\r\n        AssetManager assetManager = getAssets();\r\n\r\n        TensorFlowInferenceInterface inferenceInterface = new TensorFlowInferenceInterface(assetManager, fullModelPath);\r\n        Graph graph = inferenceInterface.graph();\r\n\r\n        inferenceInterface.run(new String[]{\"init_all_tables\"});\r\n```\r\n\r\n"}