{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253895751", "html_url": "https://github.com/tensorflow/tensorflow/issues/4938#issuecomment-253895751", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4938", "id": 253895751, "node_id": "MDEyOklzc3VlQ29tbWVudDI1Mzg5NTc1MQ==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-14T19:15:40Z", "updated_at": "2016-10-14T19:15:40Z", "author_association": "MEMBER", "body_html": "<p>This is indeed a bug, it's exactly as you say and it shouldn't be like this. The problem is that there doesn't seem to be a backward-compatible way of correcting it. If we change the default, we'll break every model trained with the current function without output_projection (because the change of attention variables will change and old checkpoints won't load any more).</p>\n<p>That's why, unless you have something in mind to make it backwards-compatible, I'd rather avoid correcting this. The current seq2seq module will be deprecated anyway, because it does static graph construction (we have while_loop now) and we're also moving away from the list-based API to a single-tensor one (time being the first or second dimension). So it looks to me like correcting this is more troube than it's worth.</p>\n<p>By the way - the work on the new, dynamic seq2seq is happening in contrib.seq2seq and it's happening on github, lead by alrojo -- see, for example, issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"180329292\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4686\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/4686/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/4686\">#4686</a>. There is no attention decoder there yet, though ethancaballero has asked for it in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"180969014\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4761\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/4761/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/4761\">#4761</a>. So maybe you could sync and work with them to make a new, bug-free attention decoder for the new contrib.seq2seq?</p>\n<p>Let me know what you think, thanks for catching this!</p>", "body_text": "This is indeed a bug, it's exactly as you say and it shouldn't be like this. The problem is that there doesn't seem to be a backward-compatible way of correcting it. If we change the default, we'll break every model trained with the current function without output_projection (because the change of attention variables will change and old checkpoints won't load any more).\nThat's why, unless you have something in mind to make it backwards-compatible, I'd rather avoid correcting this. The current seq2seq module will be deprecated anyway, because it does static graph construction (we have while_loop now) and we're also moving away from the list-based API to a single-tensor one (time being the first or second dimension). So it looks to me like correcting this is more troube than it's worth.\nBy the way - the work on the new, dynamic seq2seq is happening in contrib.seq2seq and it's happening on github, lead by alrojo -- see, for example, issue #4686. There is no attention decoder there yet, though ethancaballero has asked for it in #4761. So maybe you could sync and work with them to make a new, bug-free attention decoder for the new contrib.seq2seq?\nLet me know what you think, thanks for catching this!", "body": "This is indeed a bug, it's exactly as you say and it shouldn't be like this. The problem is that there doesn't seem to be a backward-compatible way of correcting it. If we change the default, we'll break every model trained with the current function without output_projection (because the change of attention variables will change and old checkpoints won't load any more).\n\nThat's why, unless you have something in mind to make it backwards-compatible, I'd rather avoid correcting this. The current seq2seq module will be deprecated anyway, because it does static graph construction (we have while_loop now) and we're also moving away from the list-based API to a single-tensor one (time being the first or second dimension). So it looks to me like correcting this is more troube than it's worth.\n\nBy the way - the work on the new, dynamic seq2seq is happening in contrib.seq2seq and it's happening on github, lead by alrojo -- see, for example, issue #4686. There is no attention decoder there yet, though ethancaballero has asked for it in #4761. So maybe you could sync and work with them to make a new, bug-free attention decoder for the new contrib.seq2seq?\n\nLet me know what you think, thanks for catching this!\n"}