{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4938", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4938/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4938/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4938/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4938", "id": 182747625, "node_id": "MDU6SXNzdWUxODI3NDc2MjU=", "number": 4938, "title": "A bug on embedding_attention_seq2seq when output_projection is None?", "user": {"login": "jihunchoi", "id": 1898501, "node_id": "MDQ6VXNlcjE4OTg1MDE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1898501?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jihunchoi", "html_url": "https://github.com/jihunchoi", "followers_url": "https://api.github.com/users/jihunchoi/followers", "following_url": "https://api.github.com/users/jihunchoi/following{/other_user}", "gists_url": "https://api.github.com/users/jihunchoi/gists{/gist_id}", "starred_url": "https://api.github.com/users/jihunchoi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jihunchoi/subscriptions", "organizations_url": "https://api.github.com/users/jihunchoi/orgs", "repos_url": "https://api.github.com/users/jihunchoi/repos", "events_url": "https://api.github.com/users/jihunchoi/events{/privacy}", "received_events_url": "https://api.github.com/users/jihunchoi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-10-13T10:04:49Z", "updated_at": "2016-10-15T07:52:18Z", "closed_at": "2016-10-14T19:15:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>When I use embedding_attention_seq2seq without giving an <code>output_projection</code> argument, I experienced that the program crashes by a memory allocation error, even when the model ran fine in other libraries or my own implementation of attention seq2seq.<br>\nI didn't suffer this problem when I give an <code>output_projection</code> argument to the function explicitly.</p>\n<p>I suspect it occurs by the following fact: when <code>output_projection</code> is None, it wraps the <code>cell</code> with <code>OutputProjectionWrapper</code>) in <code>embedding_attention_seq2seq</code>. This wrapped <code>cell</code> emits output whose dimension matches the number of decoder symbols.<br>\nThen the wrapped <code>cell</code> variable is passed to <code>embedding_attention_decoder</code>, and to <code>attention_decoder</code>.<br>\nHere, in the <code>attention_decoder</code> function, the awkward memory allocation happens.</p>\n<ol>\n<li>Since the <code>output_size</code> argument is None, it is set to <code>cell.output_size</code>, which is identical to the number of decoder symbols. (line 576)</li>\n<li><code>cell_output</code> of line 650 has dimensions proportional to the number of decoder symbols.</li>\n<li>Thus, in line 660, it creates a very large matrix whose size is proportional to the square of the number of decoder symbols.</li>\n</ol>\n<p>According to the paper the implementation is referencing, the attention mechanism should not depend on the number of decoder symbols.<br>\nSo I think the implementation is somewhat wrong and should be corrected by passing the <code>cell</code> without wrapping an <code>OutputProjectionWrapper</code> and then performing projection afterwards.</p>\n<p>If it is truly a bug and no one is working now, I will submit a pull request since it can be easily fixed, IMO.</p>", "body_text": "When I use embedding_attention_seq2seq without giving an output_projection argument, I experienced that the program crashes by a memory allocation error, even when the model ran fine in other libraries or my own implementation of attention seq2seq.\nI didn't suffer this problem when I give an output_projection argument to the function explicitly.\nI suspect it occurs by the following fact: when output_projection is None, it wraps the cell with OutputProjectionWrapper) in embedding_attention_seq2seq. This wrapped cell emits output whose dimension matches the number of decoder symbols.\nThen the wrapped cell variable is passed to embedding_attention_decoder, and to attention_decoder.\nHere, in the attention_decoder function, the awkward memory allocation happens.\n\nSince the output_size argument is None, it is set to cell.output_size, which is identical to the number of decoder symbols. (line 576)\ncell_output of line 650 has dimensions proportional to the number of decoder symbols.\nThus, in line 660, it creates a very large matrix whose size is proportional to the square of the number of decoder symbols.\n\nAccording to the paper the implementation is referencing, the attention mechanism should not depend on the number of decoder symbols.\nSo I think the implementation is somewhat wrong and should be corrected by passing the cell without wrapping an OutputProjectionWrapper and then performing projection afterwards.\nIf it is truly a bug and no one is working now, I will submit a pull request since it can be easily fixed, IMO.", "body": "When I use embedding_attention_seq2seq without giving an `output_projection` argument, I experienced that the program crashes by a memory allocation error, even when the model ran fine in other libraries or my own implementation of attention seq2seq.\nI didn't suffer this problem when I give an `output_projection` argument to the function explicitly.\n\nI suspect it occurs by the following fact: when `output_projection` is None, it wraps the `cell` with `OutputProjectionWrapper`) in `embedding_attention_seq2seq`. This wrapped `cell` emits output whose dimension matches the number of decoder symbols.\nThen the wrapped `cell` variable is passed to `embedding_attention_decoder`, and to `attention_decoder`.\nHere, in the `attention_decoder` function, the awkward memory allocation happens.\n1) Since the `output_size` argument is None, it is set to `cell.output_size`, which is identical to the number of decoder symbols. (line 576)\n2) `cell_output` of line 650 has dimensions proportional to the number of decoder symbols.\n3) Thus, in line 660, it creates a very large matrix whose size is proportional to the square of the number of decoder symbols.\n\nAccording to the paper the implementation is referencing, the attention mechanism should not depend on the number of decoder symbols.\nSo I think the implementation is somewhat wrong and should be corrected by passing the `cell` without wrapping an `OutputProjectionWrapper` and then performing projection afterwards.\n\nIf it is truly a bug and no one is working now, I will submit a pull request since it can be easily fixed, IMO.\n"}