{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/304999354", "html_url": "https://github.com/tensorflow/tensorflow/issues/10142#issuecomment-304999354", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10142", "id": 304999354, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDk5OTM1NA==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-30T20:32:26Z", "updated_at": "2017-05-30T20:32:26Z", "author_association": "MEMBER", "body_html": "<p>I'm not so sure you can do the exact same thing. That method is taking into account that if you scale the computation based on the range of the existing values, you can exp and reduce (sum) in a space where there is more floating accuracy. You'd have to do that scaling and return the caling to the user. That would make for a complicated interface for users of tf.softmax.  As you say this is a known problem, which is why we have reduce_logsumexp (and softmax_cross_entropy_with_logit), but tf.nn.softmax may be useful in other situations.</p>", "body_text": "I'm not so sure you can do the exact same thing. That method is taking into account that if you scale the computation based on the range of the existing values, you can exp and reduce (sum) in a space where there is more floating accuracy. You'd have to do that scaling and return the caling to the user. That would make for a complicated interface for users of tf.softmax.  As you say this is a known problem, which is why we have reduce_logsumexp (and softmax_cross_entropy_with_logit), but tf.nn.softmax may be useful in other situations.", "body": "I'm not so sure you can do the exact same thing. That method is taking into account that if you scale the computation based on the range of the existing values, you can exp and reduce (sum) in a space where there is more floating accuracy. You'd have to do that scaling and return the caling to the user. That would make for a complicated interface for users of tf.softmax.  As you say this is a known problem, which is why we have reduce_logsumexp (and softmax_cross_entropy_with_logit), but tf.nn.softmax may be useful in other situations. "}