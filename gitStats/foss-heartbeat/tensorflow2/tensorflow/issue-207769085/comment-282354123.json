{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282354123", "html_url": "https://github.com/tensorflow/tensorflow/issues/7523#issuecomment-282354123", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7523", "id": 282354123, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjM1NDEyMw==", "user": {"login": "andrehentz", "id": 25754898, "node_id": "MDQ6VXNlcjI1NzU0ODk4", "avatar_url": "https://avatars3.githubusercontent.com/u/25754898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrehentz", "html_url": "https://github.com/andrehentz", "followers_url": "https://api.github.com/users/andrehentz/followers", "following_url": "https://api.github.com/users/andrehentz/following{/other_user}", "gists_url": "https://api.github.com/users/andrehentz/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrehentz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrehentz/subscriptions", "organizations_url": "https://api.github.com/users/andrehentz/orgs", "repos_url": "https://api.github.com/users/andrehentz/repos", "events_url": "https://api.github.com/users/andrehentz/events{/privacy}", "received_events_url": "https://api.github.com/users/andrehentz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-24T17:38:27Z", "updated_at": "2017-02-24T17:38:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm still looking into quantize_graph, but here's a quick update.</p>\n<p>In short, avoid using only 'quantize_weights' with transform_graph. Prefer the full transforms from 'eight-bit-calculations'.</p>\n<p>I've run the following two commands on the files <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25055499\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Bruczzz\">@Bruczzz</a> provided:</p>\n<pre><code>transform_graph --logtostderr --in_graph=./Retrained_graph.pb --out_graph=./Quantized_nodes_and_weights_graph.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes'\ntransform_graph --logtostderr --in_graph=./Retrained_graph.pb --out_graph=./Quantized_only_weights_graph.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='quantize_weights'\n</code></pre>\n<p>And the performance of the two quantized graphs is:</p>\n<pre><code>test_bird.jpeg\n-&gt; Retrained:  birds (3): 0.988302\n-&gt; Quantized (nodes and weights):  birds (3): 0.964491\n-&gt; Quantized (only weights):  animals (2): 0.56042\ntest_plant.jpg\n-&gt; Retrained:  plants (0): 0.999204\n-&gt; Quantized (nodes and weights):  plants (0): 0.999696\n-&gt; Quantized (only weights):  animals (2): 0.669282\ntest_buliding.jpg\n-&gt; Retrained:  buildings (1): 0.968189\n-&gt; Quantized (nodes and weights):  buildings (1): 0.943537\n-&gt; Quantized (only weights):  animals (2): 0.490827\n</code></pre>", "body_text": "I'm still looking into quantize_graph, but here's a quick update.\nIn short, avoid using only 'quantize_weights' with transform_graph. Prefer the full transforms from 'eight-bit-calculations'.\nI've run the following two commands on the files @Bruczzz provided:\ntransform_graph --logtostderr --in_graph=./Retrained_graph.pb --out_graph=./Quantized_nodes_and_weights_graph.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes'\ntransform_graph --logtostderr --in_graph=./Retrained_graph.pb --out_graph=./Quantized_only_weights_graph.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='quantize_weights'\n\nAnd the performance of the two quantized graphs is:\ntest_bird.jpeg\n-> Retrained:  birds (3): 0.988302\n-> Quantized (nodes and weights):  birds (3): 0.964491\n-> Quantized (only weights):  animals (2): 0.56042\ntest_plant.jpg\n-> Retrained:  plants (0): 0.999204\n-> Quantized (nodes and weights):  plants (0): 0.999696\n-> Quantized (only weights):  animals (2): 0.669282\ntest_buliding.jpg\n-> Retrained:  buildings (1): 0.968189\n-> Quantized (nodes and weights):  buildings (1): 0.943537\n-> Quantized (only weights):  animals (2): 0.490827", "body": "I'm still looking into quantize_graph, but here's a quick update.\r\n\r\nIn short, avoid using only 'quantize_weights' with transform_graph. Prefer the full transforms from 'eight-bit-calculations'.\r\n\r\nI've run the following two commands on the files @Bruczzz provided:\r\n\r\n```\r\ntransform_graph --logtostderr --in_graph=./Retrained_graph.pb --out_graph=./Quantized_nodes_and_weights_graph.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\") remove_nodes(op=Identity, op=CheckNumerics) fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes'\r\ntransform_graph --logtostderr --in_graph=./Retrained_graph.pb --out_graph=./Quantized_only_weights_graph.pb --inputs=Mul:0 --outputs=final_result:0 --transforms='quantize_weights'\r\n```\r\n\r\nAnd the performance of the two quantized graphs is:\r\n```\r\ntest_bird.jpeg\r\n-> Retrained:  birds (3): 0.988302\r\n-> Quantized (nodes and weights):  birds (3): 0.964491\r\n-> Quantized (only weights):  animals (2): 0.56042\r\ntest_plant.jpg\r\n-> Retrained:  plants (0): 0.999204\r\n-> Quantized (nodes and weights):  plants (0): 0.999696\r\n-> Quantized (only weights):  animals (2): 0.669282\r\ntest_buliding.jpg\r\n-> Retrained:  buildings (1): 0.968189\r\n-> Quantized (nodes and weights):  buildings (1): 0.943537\r\n-> Quantized (only weights):  animals (2): 0.490827\r\n```\r\n"}