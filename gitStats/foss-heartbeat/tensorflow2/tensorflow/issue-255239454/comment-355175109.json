{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/355175109", "html_url": "https://github.com/tensorflow/tensorflow/issues/12818#issuecomment-355175109", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12818", "id": 355175109, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTE3NTEwOQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T01:27:22Z", "updated_at": "2018-01-04T01:27:22Z", "author_association": "MEMBER", "body_html": "<p>First, I apologize for not noticing this issue earlier.  In case it's still of interest, here's some analysis.</p>\n<p>Short summary: It's complicated. I'll give a longer answer than strictly necessary, to address cases more general than you describe.</p>\n<p>Control edges introduce a dependency where if there is such an edge from node i to node j, node j will not be scheduled for execution until node i has 'finished' execution.  I put the scare quotes around finished because while it's true in the way you probably expect for CPU operations, it's not really true in the case of GPU operations.</p>\n<p>CPU operations are scheduled by immediately executing them or by queuing them on a threadpool, and their completion is detected synchronously or via callback.</p>\n<p>GPU operations are scheduled by making a request to the StreamExecutor (an abstract wrapper around the CUDA driver).  TF pre-establishes a small number of execution streams on each GPU, for dedicated purposes.  All compute kernels are executed on a single compute stream.  Since that is the case, if nodes i and j in the example are both to execute on the same GPU, we can schedule j immediately after scheduling i, without waiting for i to finish, because j won't actually begin execution until i completes.   (In general we don't know or care exactly when a stream operation completes, just that the proper stream event ordering dependencies are introduced to guarantee safe execution.)  H2D copy operations are scheduled on their own dedicated stream, D2H copy operations on another, and D2D copy operations on yet another.  So it's possible to have up to 4 concurrent operations involving one GPU: one compute and three copies.  At least from the TF point of view; at the NVIDIA device level there may be a more detailed description with a different character.</p>\n<p>When a TF graph is executed, it is first partitioned into a subgraph of only nodes assigned to a single device, and value copy operations between those graphs are introduced in the form of (internal only) Send and Recv nodes.  Unless there's an optional control input a Recv node is eligible for immediate execution, but the data transfer doesn't start until it meets the corresponding Send in the Rendezvous table associated with the process of the source device.</p>\n<p>With these considerations, let's examine your program.  After the first run C is fully initialized and the only operations that execute in each run are</p>\n<ol>\n<li>feeding A</li>\n<li>feeding B</li>\n<li>copying A to A_GPU</li>\n<li>copying B to B_GPU</li>\n<li>doing the matmul</li>\n<li>doing the add<br>\nOnly operations 1 &amp; 2 execute on the CPU, all the rest are GPU stream operations.  I don't really know how the feeding works; if it's multi-threaded I suppose it may be non-deterministic whether A or B is completely populated first.</li>\n</ol>\n<p>Your program adds control edges from the tf.identity that establishes A_GPU to the tf.identity that establishes B_GPU and also the matmul and the add.  Strictly speaking, the first tf.identity executes on the GPU so it's eligible to schedule as soon as the Recv of A is 'finished'.  The second tf.identity is also a GPU operation and is eligible to schedule as soon as the Recv of B is finished <em>and</em> the first tf.identity is 'finished'.  Note one tricky point however; the data copy is triggered by the corresponding Send and Recv meeting in the process Rendezvous table, and your control edge is between two tf.identity operations, both of which are actually downstream (in terms of the dataflow graph) from the Recv operations.  From the point-of-view of the GPU executor, both of those Recv nodes are eligible for immediate scheduling, and in your multi-threaded environment it appears to be non-deterministic which copy gets into the H2D stream first.  Once it's in there, it will run to completion before the other copy starts.</p>\n<p>The semantics of tf.identity are potentially confusing where the input tensor is non-local and the purpose of the operation is to introduce a copy.  In general, identity creates a new immutable tensor which shares type, shape and the same backing memory buffer with its input tensor.  Where the given input is non-local, the execution environment introduces an upstream Recv which actually allocates the memory buffer and populates it, and that Recv op is technically the input to the identity op.   If this is happening on a GPU then the Recv-triggered copy is scheduled on the H2D stream and the identity is scheduled on the compute stream. To ensure that the identity doesn't execute too soon, we could insert a dependency on the H2D stream in the compute stream immediately before enqueuing the identity op.  However, instead we treat Recv specially as the only async GPU operation and actually wait for it to terminate before considering it to be finished and enqueuing the identity op.</p>\n<p>In the case of your program, the main problem is that you've introduced the control dependencies downstream of the actual data copies, but it's tricky to realize your intended behavior because of the strange semantics of GPU operations.  I would try rewriting your program as follows:</p>\n<pre><code>  with tf.device('/gpu:0'):\n    A_GPU = tf.identity(A)\n  with tf.device('/cpu:0'):\n    with tf.control_dependencies([A_GPU]):\n      B_staged = tf.identity(B)\n  with tf.device('gpu:0'):\n    M = tf.matmul(A_GPU, C)\n    B_GPU = tf.identity(B_staged)\n    D = tf.add(M, B_GPU, name=\"D\")\n</code></pre>\n<p>I find that with this definition the A copy consistently happens before the B copy, and the matmul may overlap with the B copy, BUT: overall execution is slower.  That's because starting the B copy stalls on waiting for a stream event to be harvested out of the CUDA driver and handled by the CPU, and in this particular program the matmul is relatively fast compared to copy times.  Also, maybe the B populate finishes first a lot of the time, but now we stall waiting for the B populate to finish before doing anything.<br>\nBut, maybe you can now find a way to improve your real problem.</p>", "body_text": "First, I apologize for not noticing this issue earlier.  In case it's still of interest, here's some analysis.\nShort summary: It's complicated. I'll give a longer answer than strictly necessary, to address cases more general than you describe.\nControl edges introduce a dependency where if there is such an edge from node i to node j, node j will not be scheduled for execution until node i has 'finished' execution.  I put the scare quotes around finished because while it's true in the way you probably expect for CPU operations, it's not really true in the case of GPU operations.\nCPU operations are scheduled by immediately executing them or by queuing them on a threadpool, and their completion is detected synchronously or via callback.\nGPU operations are scheduled by making a request to the StreamExecutor (an abstract wrapper around the CUDA driver).  TF pre-establishes a small number of execution streams on each GPU, for dedicated purposes.  All compute kernels are executed on a single compute stream.  Since that is the case, if nodes i and j in the example are both to execute on the same GPU, we can schedule j immediately after scheduling i, without waiting for i to finish, because j won't actually begin execution until i completes.   (In general we don't know or care exactly when a stream operation completes, just that the proper stream event ordering dependencies are introduced to guarantee safe execution.)  H2D copy operations are scheduled on their own dedicated stream, D2H copy operations on another, and D2D copy operations on yet another.  So it's possible to have up to 4 concurrent operations involving one GPU: one compute and three copies.  At least from the TF point of view; at the NVIDIA device level there may be a more detailed description with a different character.\nWhen a TF graph is executed, it is first partitioned into a subgraph of only nodes assigned to a single device, and value copy operations between those graphs are introduced in the form of (internal only) Send and Recv nodes.  Unless there's an optional control input a Recv node is eligible for immediate execution, but the data transfer doesn't start until it meets the corresponding Send in the Rendezvous table associated with the process of the source device.\nWith these considerations, let's examine your program.  After the first run C is fully initialized and the only operations that execute in each run are\n\nfeeding A\nfeeding B\ncopying A to A_GPU\ncopying B to B_GPU\ndoing the matmul\ndoing the add\nOnly operations 1 & 2 execute on the CPU, all the rest are GPU stream operations.  I don't really know how the feeding works; if it's multi-threaded I suppose it may be non-deterministic whether A or B is completely populated first.\n\nYour program adds control edges from the tf.identity that establishes A_GPU to the tf.identity that establishes B_GPU and also the matmul and the add.  Strictly speaking, the first tf.identity executes on the GPU so it's eligible to schedule as soon as the Recv of A is 'finished'.  The second tf.identity is also a GPU operation and is eligible to schedule as soon as the Recv of B is finished and the first tf.identity is 'finished'.  Note one tricky point however; the data copy is triggered by the corresponding Send and Recv meeting in the process Rendezvous table, and your control edge is between two tf.identity operations, both of which are actually downstream (in terms of the dataflow graph) from the Recv operations.  From the point-of-view of the GPU executor, both of those Recv nodes are eligible for immediate scheduling, and in your multi-threaded environment it appears to be non-deterministic which copy gets into the H2D stream first.  Once it's in there, it will run to completion before the other copy starts.\nThe semantics of tf.identity are potentially confusing where the input tensor is non-local and the purpose of the operation is to introduce a copy.  In general, identity creates a new immutable tensor which shares type, shape and the same backing memory buffer with its input tensor.  Where the given input is non-local, the execution environment introduces an upstream Recv which actually allocates the memory buffer and populates it, and that Recv op is technically the input to the identity op.   If this is happening on a GPU then the Recv-triggered copy is scheduled on the H2D stream and the identity is scheduled on the compute stream. To ensure that the identity doesn't execute too soon, we could insert a dependency on the H2D stream in the compute stream immediately before enqueuing the identity op.  However, instead we treat Recv specially as the only async GPU operation and actually wait for it to terminate before considering it to be finished and enqueuing the identity op.\nIn the case of your program, the main problem is that you've introduced the control dependencies downstream of the actual data copies, but it's tricky to realize your intended behavior because of the strange semantics of GPU operations.  I would try rewriting your program as follows:\n  with tf.device('/gpu:0'):\n    A_GPU = tf.identity(A)\n  with tf.device('/cpu:0'):\n    with tf.control_dependencies([A_GPU]):\n      B_staged = tf.identity(B)\n  with tf.device('gpu:0'):\n    M = tf.matmul(A_GPU, C)\n    B_GPU = tf.identity(B_staged)\n    D = tf.add(M, B_GPU, name=\"D\")\n\nI find that with this definition the A copy consistently happens before the B copy, and the matmul may overlap with the B copy, BUT: overall execution is slower.  That's because starting the B copy stalls on waiting for a stream event to be harvested out of the CUDA driver and handled by the CPU, and in this particular program the matmul is relatively fast compared to copy times.  Also, maybe the B populate finishes first a lot of the time, but now we stall waiting for the B populate to finish before doing anything.\nBut, maybe you can now find a way to improve your real problem.", "body": "First, I apologize for not noticing this issue earlier.  In case it's still of interest, here's some analysis.\r\n\r\nShort summary: It's complicated. I'll give a longer answer than strictly necessary, to address cases more general than you describe.\r\n\r\nControl edges introduce a dependency where if there is such an edge from node i to node j, node j will not be scheduled for execution until node i has 'finished' execution.  I put the scare quotes around finished because while it's true in the way you probably expect for CPU operations, it's not really true in the case of GPU operations.  \r\n\r\nCPU operations are scheduled by immediately executing them or by queuing them on a threadpool, and their completion is detected synchronously or via callback.  \r\n\r\nGPU operations are scheduled by making a request to the StreamExecutor (an abstract wrapper around the CUDA driver).  TF pre-establishes a small number of execution streams on each GPU, for dedicated purposes.  All compute kernels are executed on a single compute stream.  Since that is the case, if nodes i and j in the example are both to execute on the same GPU, we can schedule j immediately after scheduling i, without waiting for i to finish, because j won't actually begin execution until i completes.   (In general we don't know or care exactly when a stream operation completes, just that the proper stream event ordering dependencies are introduced to guarantee safe execution.)  H2D copy operations are scheduled on their own dedicated stream, D2H copy operations on another, and D2D copy operations on yet another.  So it's possible to have up to 4 concurrent operations involving one GPU: one compute and three copies.  At least from the TF point of view; at the NVIDIA device level there may be a more detailed description with a different character.\r\n\r\nWhen a TF graph is executed, it is first partitioned into a subgraph of only nodes assigned to a single device, and value copy operations between those graphs are introduced in the form of (internal only) Send and Recv nodes.  Unless there's an optional control input a Recv node is eligible for immediate execution, but the data transfer doesn't start until it meets the corresponding Send in the Rendezvous table associated with the process of the source device.\r\n\r\nWith these considerations, let's examine your program.  After the first run C is fully initialized and the only operations that execute in each run are \r\n   1. feeding A  \r\n   2. feeding B\r\n   3. copying A to A_GPU\r\n   4. copying B to B_GPU\r\n   5. doing the matmul\r\n   6. doing the add\r\nOnly operations 1 & 2 execute on the CPU, all the rest are GPU stream operations.  I don't really know how the feeding works; if it's multi-threaded I suppose it may be non-deterministic whether A or B is completely populated first.\r\n\r\nYour program adds control edges from the tf.identity that establishes A_GPU to the tf.identity that establishes B_GPU and also the matmul and the add.  Strictly speaking, the first tf.identity executes on the GPU so it's eligible to schedule as soon as the Recv of A is 'finished'.  The second tf.identity is also a GPU operation and is eligible to schedule as soon as the Recv of B is finished *and* the first tf.identity is 'finished'.  Note one tricky point however; the data copy is triggered by the corresponding Send and Recv meeting in the process Rendezvous table, and your control edge is between two tf.identity operations, both of which are actually downstream (in terms of the dataflow graph) from the Recv operations.  From the point-of-view of the GPU executor, both of those Recv nodes are eligible for immediate scheduling, and in your multi-threaded environment it appears to be non-deterministic which copy gets into the H2D stream first.  Once it's in there, it will run to completion before the other copy starts.\r\n\r\nThe semantics of tf.identity are potentially confusing where the input tensor is non-local and the purpose of the operation is to introduce a copy.  In general, identity creates a new immutable tensor which shares type, shape and the same backing memory buffer with its input tensor.  Where the given input is non-local, the execution environment introduces an upstream Recv which actually allocates the memory buffer and populates it, and that Recv op is technically the input to the identity op.   If this is happening on a GPU then the Recv-triggered copy is scheduled on the H2D stream and the identity is scheduled on the compute stream. To ensure that the identity doesn't execute too soon, we could insert a dependency on the H2D stream in the compute stream immediately before enqueuing the identity op.  However, instead we treat Recv specially as the only async GPU operation and actually wait for it to terminate before considering it to be finished and enqueuing the identity op.\r\n\r\nIn the case of your program, the main problem is that you've introduced the control dependencies downstream of the actual data copies, but it's tricky to realize your intended behavior because of the strange semantics of GPU operations.  I would try rewriting your program as follows:\r\n\r\n      with tf.device('/gpu:0'):\r\n        A_GPU = tf.identity(A)\r\n      with tf.device('/cpu:0'):\r\n        with tf.control_dependencies([A_GPU]):\r\n          B_staged = tf.identity(B)\r\n      with tf.device('gpu:0'):\r\n        M = tf.matmul(A_GPU, C)\r\n        B_GPU = tf.identity(B_staged)\r\n        D = tf.add(M, B_GPU, name=\"D\")\r\n\r\nI find that with this definition the A copy consistently happens before the B copy, and the matmul may overlap with the B copy, BUT: overall execution is slower.  That's because starting the B copy stalls on waiting for a stream event to be harvested out of the CUDA driver and handled by the CPU, and in this particular program the matmul is relatively fast compared to copy times.  Also, maybe the B populate finishes first a lot of the time, but now we stall waiting for the B populate to finish before doing anything. \r\n But, maybe you can now find a way to improve your real problem."}