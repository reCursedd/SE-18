{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22957", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22957/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22957/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22957/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22957", "id": 369766067, "node_id": "MDU6SXNzdWUzNjk3NjYwNjc=", "number": 22957, "title": "Freezing network with batch norm does not work with TRT", "user": {"login": "fferroni", "id": 16327442, "node_id": "MDQ6VXNlcjE2MzI3NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/16327442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fferroni", "html_url": "https://github.com/fferroni", "followers_url": "https://api.github.com/users/fferroni/followers", "following_url": "https://api.github.com/users/fferroni/following{/other_user}", "gists_url": "https://api.github.com/users/fferroni/gists{/gist_id}", "starred_url": "https://api.github.com/users/fferroni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fferroni/subscriptions", "organizations_url": "https://api.github.com/users/fferroni/orgs", "repos_url": "https://api.github.com/users/fferroni/repos", "events_url": "https://api.github.com/users/fferroni/events{/privacy}", "received_events_url": "https://api.github.com/users/fferroni/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-10-13T04:31:34Z", "updated_at": "2018-11-09T09:26:01Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\n16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nSource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.10.1</li>\n<li><strong>Python version</strong>:<br>\n3.5</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.0</li>\n<li><strong>Bazel version</strong>:<br>\nN/A</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/A</li>\n<li><strong>Mobile device</strong>:<br>\nN/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport numpy as np\nimport tensorflow.contrib.tensorrt as trt\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.keras import backend as K\n\npath         = \"/tmp\"\noutput_trt_pb    = os.path.join(path, \"output_trt.pb\")\n\nnp.random.seed(0)\n\nX, Y = np.random.rand(1000, 100, 100, 3), np.random.rand(1000, 100, 100, 16)\n\nwith K.get_session() as sess:\n    \n    inp = tf.keras.layers.Input(shape=(100,100,3), name=\"input\")\n    x = tf.keras.layers.Conv2D(16, (3,3), padding=\"same\", kernel_initializer=\"ones\", name=\"conv2d\", use_bias=False)(inp)\n    x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    model = tf.keras.models.Model(inp, x)\n    model.compile(\"adam\", \"mse\")\n    model.fit(X, Y, epochs=3, verbose=True)\n    \n    # fix nodes (from https://github.com/tensorflow/tensorflow/issues/3628) here doesn't help\n    graph_def = sess.graph_def\n    \n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,                                     # The session is used to retrieve the weights\n        graph_def,                                # The graph_def is used to retrieve the nodes\n        [i.name[:-2] for i in model.outputs]      # The output node names are used to select the useful nodes\n    )\n    \n    trt_graph = trt.create_inference_graph(output_graph_def, \n                                           [i.name[:-2] for i in model.outputs],\n                                           max_batch_size=1,\n                                           max_workspace_size_bytes= 1256 &lt;&lt; 20,\n                                           precision_mode=\"FP32\") \n</code></pre>\n<h3>Describe the problem</h3>\n<p>When I freeze a protobuf that contains batch normalization and then try to use it with TRT, it fails with the error</p>\n<p><code>InvalidArgumentError: Input 0 of node bn/cond/ReadVariableOp/Switch_1 was passed float from bn/gamma_1:0 incompatible with expected resource.</code></p>\n<p>It seems like this is an issue in other threads, like <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"169195660\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3628\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3628/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3628\">#3628</a> and in my code I tried to include the suggested fixes, but this does not help.<br>\nI tried using both fused=True and fused=False, and also tried trainable=False/True in BatchNormalization.<br>\nIf you comment out<br>\n<code>x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)</code><br>\nthen everything works fine.</p>\n<p>Any comment would be appreciated.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\n16.04\nTensorFlow installed from (source or binary):\nSource\nTensorFlow version (use command below):\n1.10.1\nPython version:\n3.5\nCUDA/cuDNN version:\n9.0\nBazel version:\nN/A\nGPU model and memory:\nN/A\nMobile device:\nN/A\nExact command to reproduce:\n\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\nimport numpy as np\nimport tensorflow.contrib.tensorrt as trt\nimport tensorflow as tf\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.keras import backend as K\n\npath         = \"/tmp\"\noutput_trt_pb    = os.path.join(path, \"output_trt.pb\")\n\nnp.random.seed(0)\n\nX, Y = np.random.rand(1000, 100, 100, 3), np.random.rand(1000, 100, 100, 16)\n\nwith K.get_session() as sess:\n    \n    inp = tf.keras.layers.Input(shape=(100,100,3), name=\"input\")\n    x = tf.keras.layers.Conv2D(16, (3,3), padding=\"same\", kernel_initializer=\"ones\", name=\"conv2d\", use_bias=False)(inp)\n    x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)\n    x = tf.keras.layers.Activation(\"relu\")(x)\n    model = tf.keras.models.Model(inp, x)\n    model.compile(\"adam\", \"mse\")\n    model.fit(X, Y, epochs=3, verbose=True)\n    \n    # fix nodes (from https://github.com/tensorflow/tensorflow/issues/3628) here doesn't help\n    graph_def = sess.graph_def\n    \n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess,                                     # The session is used to retrieve the weights\n        graph_def,                                # The graph_def is used to retrieve the nodes\n        [i.name[:-2] for i in model.outputs]      # The output node names are used to select the useful nodes\n    )\n    \n    trt_graph = trt.create_inference_graph(output_graph_def, \n                                           [i.name[:-2] for i in model.outputs],\n                                           max_batch_size=1,\n                                           max_workspace_size_bytes= 1256 << 20,\n                                           precision_mode=\"FP32\") \n\nDescribe the problem\nWhen I freeze a protobuf that contains batch normalization and then try to use it with TRT, it fails with the error\nInvalidArgumentError: Input 0 of node bn/cond/ReadVariableOp/Switch_1 was passed float from bn/gamma_1:0 incompatible with expected resource.\nIt seems like this is an issue in other threads, like #3628 and in my code I tried to include the suggested fixes, but this does not help.\nI tried using both fused=True and fused=False, and also tried trainable=False/True in BatchNormalization.\nIf you comment out\nx = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)\nthen everything works fine.\nAny comment would be appreciated.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\n1.10.1\r\n- **Python version**:\r\n3.5\r\n- **CUDA/cuDNN version**:\r\n9.0\r\n- **Bazel version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n- **Mobile device**:\r\nN/A\r\n- **Exact command to reproduce**:\r\n```\r\nimport os\r\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\r\nimport numpy as np\r\nimport tensorflow.contrib.tensorrt as trt\r\nimport tensorflow as tf\r\nfrom tensorflow.python.platform import gfile\r\nfrom tensorflow.keras import backend as K\r\n\r\npath         = \"/tmp\"\r\noutput_trt_pb    = os.path.join(path, \"output_trt.pb\")\r\n\r\nnp.random.seed(0)\r\n\r\nX, Y = np.random.rand(1000, 100, 100, 3), np.random.rand(1000, 100, 100, 16)\r\n\r\nwith K.get_session() as sess:\r\n    \r\n    inp = tf.keras.layers.Input(shape=(100,100,3), name=\"input\")\r\n    x = tf.keras.layers.Conv2D(16, (3,3), padding=\"same\", kernel_initializer=\"ones\", name=\"conv2d\", use_bias=False)(inp)\r\n    x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)\r\n    x = tf.keras.layers.Activation(\"relu\")(x)\r\n    model = tf.keras.models.Model(inp, x)\r\n    model.compile(\"adam\", \"mse\")\r\n    model.fit(X, Y, epochs=3, verbose=True)\r\n    \r\n    # fix nodes (from https://github.com/tensorflow/tensorflow/issues/3628) here doesn't help\r\n    graph_def = sess.graph_def\r\n    \r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        sess,                                     # The session is used to retrieve the weights\r\n        graph_def,                                # The graph_def is used to retrieve the nodes\r\n        [i.name[:-2] for i in model.outputs]      # The output node names are used to select the useful nodes\r\n    )\r\n    \r\n    trt_graph = trt.create_inference_graph(output_graph_def, \r\n                                           [i.name[:-2] for i in model.outputs],\r\n                                           max_batch_size=1,\r\n                                           max_workspace_size_bytes= 1256 << 20,\r\n                                           precision_mode=\"FP32\") \r\n```\r\n\r\n### Describe the problem\r\n\r\nWhen I freeze a protobuf that contains batch normalization and then try to use it with TRT, it fails with the error \r\n\r\n`InvalidArgumentError: Input 0 of node bn/cond/ReadVariableOp/Switch_1 was passed float from bn/gamma_1:0 incompatible with expected resource.`\r\n\r\nIt seems like this is an issue in other threads, like https://github.com/tensorflow/tensorflow/issues/3628 and in my code I tried to include the suggested fixes, but this does not help.\r\nI tried using both fused=True and fused=False, and also tried trainable=False/True in BatchNormalization.\r\nIf you comment out\r\n`x = tf.keras.layers.BatchNormalization(name=\"bn\", fused=True)(x)`\r\nthen everything works fine.\r\n\r\nAny comment would be appreciated.\r\n"}