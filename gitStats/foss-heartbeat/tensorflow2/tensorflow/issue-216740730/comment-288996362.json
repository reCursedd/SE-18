{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/288996362", "html_url": "https://github.com/tensorflow/tensorflow/issues/8688#issuecomment-288996362", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8688", "id": 288996362, "node_id": "MDEyOklzc3VlQ29tbWVudDI4ODk5NjM2Mg==", "user": {"login": "wuguangbin1230", "id": 16016736, "node_id": "MDQ6VXNlcjE2MDE2NzM2", "avatar_url": "https://avatars0.githubusercontent.com/u/16016736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wuguangbin1230", "html_url": "https://github.com/wuguangbin1230", "followers_url": "https://api.github.com/users/wuguangbin1230/followers", "following_url": "https://api.github.com/users/wuguangbin1230/following{/other_user}", "gists_url": "https://api.github.com/users/wuguangbin1230/gists{/gist_id}", "starred_url": "https://api.github.com/users/wuguangbin1230/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wuguangbin1230/subscriptions", "organizations_url": "https://api.github.com/users/wuguangbin1230/orgs", "repos_url": "https://api.github.com/users/wuguangbin1230/repos", "events_url": "https://api.github.com/users/wuguangbin1230/events{/privacy}", "received_events_url": "https://api.github.com/users/wuguangbin1230/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-24T11:16:35Z", "updated_at": "2017-03-24T11:16:35Z", "author_association": "NONE", "body_html": "<ul>\n<li>#!/usr/bin/env python3</li>\n</ul>\n<h1>-<em>- coding: utf-8 -</em>-</h1>\n<p>\"\"\"<br>\nCreated on Thu Mar 23 16:19:39 2017</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"organization\" data-hovercard-url=\"/orgs/author/hovercard\" href=\"https://github.com/author\">@author</a>: wgb<br>\n\"\"\"<br>\nimport os<br>\nimport sys<br>\nimport numpy as np<br>\nimport types<br>\nfrom scipy.misc import imread, imresize<br>\nimport matplotlib.pyplot as plt</p>\n<p>import tensorflow as tf<br>\nfrom tensorflow.python.framework import ops<br>\n#import build_txt<br>\n#import build_txt</p>\n<h1></h1>\n<p>#k = build_txt</p>\n<p>file_name = os.getcwd()</p>\n<p>training_file  = file_name + '/training_webcam.txt'<br>\ntest_file = file_name + '/test_dslr.txt'</p>\n<p>image_size = 224<br>\ntraining_batch_size = 16</p>\n<p>sess = tf.InteractiveSession()</p>\n<p>\"\"\"read training and test data    ################################################################################################\"\"\"</p>\n<p>def read_txt(file_name):</p>\n<pre><code>f  = open(file_name, 'r')\nfile_lines = f.readlines()\n\nimage_path_total = []\nlabels_total = []\nimage_id_total = []    \nfor i in file_lines:\n    divid_data = i.split(' ')\n    image_path = divid_data[0]  \n    label = divid_data[1]\n    one_hot = [0] * 10\n    one_hot[int(label)-1] = 1\n    labels = one_hot\n    image_id = divid_data[2]\n    \n    image_path = ops.convert_to_tensor(image_path, tf.string)\n    labels = ops.convert_to_tensor(labels, tf.float32)\n    image_id = ops.convert_to_tensor(int(image_id), tf.float32)\n        \n    image_path_total.append(image_path)\n    labels_total.append(labels)\n    image_id_total.append(image_id)\n    \nreturn image_path_total,labels_total, image_id_total\n</code></pre>\n<p>def read_image(single_image_queue,size):<br>\n\"\"\"the image_path is the path of single image\"\"\"<br>\nfile_contents = tf.read_file(single_image_queue[0])<br>\nimage_data = tf.image.decode_jpeg(file_contents, channels=3)<br>\nimage_data = tf.cast(image_data, tf.float32)<br>\nimage_data = tf.image.resize_images(image_data, [size, size])<br>\nmean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 3])<br>\nimage_data = image_data - mean</p>\n<pre><code>image_path = single_image_queue[0]\nlabel = single_image_queue[1]\nimage_id_wgb = single_image_queue[2]\n\nreturn image_data, label, image_id_wgb,image_path\n</code></pre>\n<p>training_image_total= read_txt(file_name = training_file)<br>\ntraining_input_queue = tf.train.slice_input_producer(training_image_total, shuffle=True)<br>\ntraining_image_data = read_image(training_input_queue,image_size)<br>\ntraining_image_batch = tf.train.shuffle_batch(training_image_data,training_batch_size,num_threads = 2, min_after_dequeue =101,capacity = 1000)</p>\n<p>test_image_total= read_txt(file_name = test_file)<br>\ntest_input_queue = tf.train.slice_input_producer(test_image_total, shuffle=True)<br>\ntest_image_data = read_image(test_input_queue,image_size)<br>\ntest_image_batch = tf.train.shuffle_batch(test_image_data,batch_size=64,num_threads = 2, min_after_dequeue =101,capacity = 1000)</p>\n<p>\"\"\"define networks    ################################################################################################\"\"\"</p>\n<p>x = tf.placeholder(tf.float32, [None, 224,224,3])<br>\nx = tf.reshape(x,[-1,224,224,3])</p>\n<p>parameters = []</p>\n<p>with tf.device('/cpu:0'):<br>\n# conv1_1<br>\nwith tf.name_scope('conv1_1') as scope:<br>\nkernel1 = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,<br>\nstddev=1e-1), name='weights', trainable = False)<br>\nconv = tf.nn.conv2d(x, kernel1, [1, 1, 1, 1], padding='SAME')<br>\nbiases1 = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),<br>\ntrainable=True, name='biases')<br>\nout = tf.nn.bias_add(conv, biases1)<br>\nconv1_1 = tf.nn.relu(out, name=scope)<br>\nparameters += [kernel1, biases1]</p>\n<pre><code># conv1_2\nwith tf.name_scope('conv1_2') as scope:\n    kernel2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv1_1, kernel2, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv1_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel2, biases]\n\n# pool1\npool1 = tf.nn.max_pool(conv1_2,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool1')\n\n# conv2_1\nwith tf.name_scope('conv2_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv2_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv2_2\nwith tf.name_scope('conv2_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv2_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool2\npool2 = tf.nn.max_pool(conv2_2,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool2')\n\n# conv3_1\nwith tf.name_scope('conv3_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv3_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv3_2\nwith tf.name_scope('conv3_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv3_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv3_3\nwith tf.name_scope('conv3_3') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv3_3 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool3\npool3 = tf.nn.max_pool(conv3_3,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool3')\n\n# conv4_1\nwith tf.name_scope('conv4_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv4_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv4_2\nwith tf.name_scope('conv4_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv4_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv4_3\nwith tf.name_scope('conv4_3') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv4_3 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool4\npool4 = tf.nn.max_pool(conv4_3,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool4')\n\n# conv5_1\nwith tf.name_scope('conv5_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv5_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv5_2\nwith tf.name_scope('conv5_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv5_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv5_3\nwith tf.name_scope('conv5_3') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv5_3 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool5\npool5 = tf.nn.max_pool(conv5_3,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool4')\n\n#fc1\nwith tf.name_scope('fc1') as scope:\n    shape = int(np.prod(pool5.get_shape()[1:]))\n    fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='weights')\n    fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                         trainable=True, name='biases')\n    pool5_flat = tf.reshape(pool5, [-1, shape])\n    fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n    fc1 = tf.nn.relu(fc1l)\n    parameters += [fc1w, fc1b]\n\n# fc2\nwith tf.name_scope('fc2') as scope:\n    fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='weights')\n    fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                         trainable=True, name='biases')\n    fc2l = tf.nn.bias_add(tf.matmul(fc1, fc2w), fc2b)\n    fc2 = tf.nn.relu(fc2l)\n    parameters += [fc2w, fc2b]\n\n# fc3\nwith tf.name_scope('fc3') as scope:\n    fc3w = tf.Variable(tf.truncated_normal([4096, 10],  ##wgb\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='weights')\n    fc3b = tf.Variable(tf.constant(1.0, shape=[10], dtype=tf.float32),  ###wgb\n                         trainable=True, name='biases')\n    fc3l = tf.nn.bias_add(tf.matmul(fc2, fc3w), fc3b)\n    parameters += [fc3w, fc3b]\n\n#probs = tf.nn.softmax(self.fc3l)\n\n\nprobs = tf.nn.softmax(fc3l)\n\ny_ = tf.placeholder(tf.float32, [None,10])\n                          \ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(probs),reduction_indices = [1]))                             \ncorrect_prediction = tf.equal(tf.argmax(probs,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)   \n</code></pre>\n<p>sess = tf.Session()</p>\n<p>tf.global_variables_initializer().run()</p>\n<p>weight_file = 'vgg16_weights.npz'<br>\nweights = np.load(weight_file)<br>\nkeys = sorted(weights.keys())<br>\nfor i, k in enumerate(keys):<br>\nif i  &lt; 30:<br>\nprint (i, k, np.shape(weights[k]))<br>\nsess.run(parameters[i].assign(weights[k]))</p>\n<p>coord = tf.train.Coordinator()<br>\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)</p>\n<p>training_batch = sess.run(training_image_batch)</p>\n<p>kernel_wgb11 = []<br>\nkernel_wgb22 = []<br>\nfor i in range(10000):<br>\ntraining_batch = sess.run(training_image_batch)<br>\nprint(i)<br>\nkernel_wgb1 = sess.run(biases1,feed_dict = {x: training_batch[0], y_: training_batch[1]})</p>\n<pre><code>if i ==0:\n    train_accuracy = accuracy.eval(feed_dict = {x: training_batch[0], y_: training_batch[1]})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n\n    kernel_wgb11 = kernel_wgb1\n\n\nprint('i  : \\n', kernel_wgb11-kernel_wgb1)\n    \n    \n    \n    test_batch = sess.run(test_image_batch)\n    t_a = accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]})\n    print(\"test accuracy %g\"%t_a)\n</code></pre>\n<h1>print(\"test accuracy333 %g\"%accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]}))</h1>\n<pre><code>    test_accuracy.append(t_a)\n    num_i.append(i)\n    plt.plot(num_i,test_accuracy)\ntrain_step.run(feed_dict = {x: training_batch[0], y_: training_batch[1]})\n</code></pre>\n<p>preds = (np.argsort(prob)[::-1])[0:5]<br>\nfor p in preds:<br>\nprint (class_names[p], prob[p])</p>\n<p>coord.request_stop()<br>\ncoord.join(threads)</p>\n<p>plt.plot(num_i,test_accuracy)</p>", "body_text": "#!/usr/bin/env python3\n\n-- coding: utf-8 --\n\"\"\"\nCreated on Thu Mar 23 16:19:39 2017\n@author: wgb\n\"\"\"\nimport os\nimport sys\nimport numpy as np\nimport types\nfrom scipy.misc import imread, imresize\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\n#import build_txt\n#import build_txt\n\n#k = build_txt\nfile_name = os.getcwd()\ntraining_file  = file_name + '/training_webcam.txt'\ntest_file = file_name + '/test_dslr.txt'\nimage_size = 224\ntraining_batch_size = 16\nsess = tf.InteractiveSession()\n\"\"\"read training and test data    ################################################################################################\"\"\"\ndef read_txt(file_name):\nf  = open(file_name, 'r')\nfile_lines = f.readlines()\n\nimage_path_total = []\nlabels_total = []\nimage_id_total = []    \nfor i in file_lines:\n    divid_data = i.split(' ')\n    image_path = divid_data[0]  \n    label = divid_data[1]\n    one_hot = [0] * 10\n    one_hot[int(label)-1] = 1\n    labels = one_hot\n    image_id = divid_data[2]\n    \n    image_path = ops.convert_to_tensor(image_path, tf.string)\n    labels = ops.convert_to_tensor(labels, tf.float32)\n    image_id = ops.convert_to_tensor(int(image_id), tf.float32)\n        \n    image_path_total.append(image_path)\n    labels_total.append(labels)\n    image_id_total.append(image_id)\n    \nreturn image_path_total,labels_total, image_id_total\n\ndef read_image(single_image_queue,size):\n\"\"\"the image_path is the path of single image\"\"\"\nfile_contents = tf.read_file(single_image_queue[0])\nimage_data = tf.image.decode_jpeg(file_contents, channels=3)\nimage_data = tf.cast(image_data, tf.float32)\nimage_data = tf.image.resize_images(image_data, [size, size])\nmean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 3])\nimage_data = image_data - mean\nimage_path = single_image_queue[0]\nlabel = single_image_queue[1]\nimage_id_wgb = single_image_queue[2]\n\nreturn image_data, label, image_id_wgb,image_path\n\ntraining_image_total= read_txt(file_name = training_file)\ntraining_input_queue = tf.train.slice_input_producer(training_image_total, shuffle=True)\ntraining_image_data = read_image(training_input_queue,image_size)\ntraining_image_batch = tf.train.shuffle_batch(training_image_data,training_batch_size,num_threads = 2, min_after_dequeue =101,capacity = 1000)\ntest_image_total= read_txt(file_name = test_file)\ntest_input_queue = tf.train.slice_input_producer(test_image_total, shuffle=True)\ntest_image_data = read_image(test_input_queue,image_size)\ntest_image_batch = tf.train.shuffle_batch(test_image_data,batch_size=64,num_threads = 2, min_after_dequeue =101,capacity = 1000)\n\"\"\"define networks    ################################################################################################\"\"\"\nx = tf.placeholder(tf.float32, [None, 224,224,3])\nx = tf.reshape(x,[-1,224,224,3])\nparameters = []\nwith tf.device('/cpu:0'):\n# conv1_1\nwith tf.name_scope('conv1_1') as scope:\nkernel1 = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\nstddev=1e-1), name='weights', trainable = False)\nconv = tf.nn.conv2d(x, kernel1, [1, 1, 1, 1], padding='SAME')\nbiases1 = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\ntrainable=True, name='biases')\nout = tf.nn.bias_add(conv, biases1)\nconv1_1 = tf.nn.relu(out, name=scope)\nparameters += [kernel1, biases1]\n# conv1_2\nwith tf.name_scope('conv1_2') as scope:\n    kernel2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv1_1, kernel2, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv1_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel2, biases]\n\n# pool1\npool1 = tf.nn.max_pool(conv1_2,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool1')\n\n# conv2_1\nwith tf.name_scope('conv2_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv2_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv2_2\nwith tf.name_scope('conv2_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv2_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool2\npool2 = tf.nn.max_pool(conv2_2,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool2')\n\n# conv3_1\nwith tf.name_scope('conv3_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv3_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv3_2\nwith tf.name_scope('conv3_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv3_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv3_3\nwith tf.name_scope('conv3_3') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv3_3 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool3\npool3 = tf.nn.max_pool(conv3_3,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool3')\n\n# conv4_1\nwith tf.name_scope('conv4_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv4_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv4_2\nwith tf.name_scope('conv4_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv4_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv4_3\nwith tf.name_scope('conv4_3') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv4_3 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool4\npool4 = tf.nn.max_pool(conv4_3,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool4')\n\n# conv5_1\nwith tf.name_scope('conv5_1') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv5_1 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv5_2\nwith tf.name_scope('conv5_2') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv5_2 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# conv5_3\nwith tf.name_scope('conv5_3') as scope:\n    kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\n                                             stddev=1e-1), name='weights')\n    conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\n    biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\n                         trainable=True, name='biases')\n    out = tf.nn.bias_add(conv, biases)\n    conv5_3 = tf.nn.relu(out, name=scope)\n    parameters += [kernel, biases]\n\n# pool5\npool5 = tf.nn.max_pool(conv5_3,\n                       ksize=[1, 2, 2, 1],\n                       strides=[1, 2, 2, 1],\n                       padding='SAME',\n                       name='pool4')\n\n#fc1\nwith tf.name_scope('fc1') as scope:\n    shape = int(np.prod(pool5.get_shape()[1:]))\n    fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='weights')\n    fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                         trainable=True, name='biases')\n    pool5_flat = tf.reshape(pool5, [-1, shape])\n    fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\n    fc1 = tf.nn.relu(fc1l)\n    parameters += [fc1w, fc1b]\n\n# fc2\nwith tf.name_scope('fc2') as scope:\n    fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='weights')\n    fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\n                         trainable=True, name='biases')\n    fc2l = tf.nn.bias_add(tf.matmul(fc1, fc2w), fc2b)\n    fc2 = tf.nn.relu(fc2l)\n    parameters += [fc2w, fc2b]\n\n# fc3\nwith tf.name_scope('fc3') as scope:\n    fc3w = tf.Variable(tf.truncated_normal([4096, 10],  ##wgb\n                                                 dtype=tf.float32,\n                                                 stddev=1e-1), name='weights')\n    fc3b = tf.Variable(tf.constant(1.0, shape=[10], dtype=tf.float32),  ###wgb\n                         trainable=True, name='biases')\n    fc3l = tf.nn.bias_add(tf.matmul(fc2, fc3w), fc3b)\n    parameters += [fc3w, fc3b]\n\n#probs = tf.nn.softmax(self.fc3l)\n\n\nprobs = tf.nn.softmax(fc3l)\n\ny_ = tf.placeholder(tf.float32, [None,10])\n                          \ncross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(probs),reduction_indices = [1]))                             \ncorrect_prediction = tf.equal(tf.argmax(probs,1), tf.argmax(y_,1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\ntrain_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)   \n\nsess = tf.Session()\ntf.global_variables_initializer().run()\nweight_file = 'vgg16_weights.npz'\nweights = np.load(weight_file)\nkeys = sorted(weights.keys())\nfor i, k in enumerate(keys):\nif i  < 30:\nprint (i, k, np.shape(weights[k]))\nsess.run(parameters[i].assign(weights[k]))\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\ntraining_batch = sess.run(training_image_batch)\nkernel_wgb11 = []\nkernel_wgb22 = []\nfor i in range(10000):\ntraining_batch = sess.run(training_image_batch)\nprint(i)\nkernel_wgb1 = sess.run(biases1,feed_dict = {x: training_batch[0], y_: training_batch[1]})\nif i ==0:\n    train_accuracy = accuracy.eval(feed_dict = {x: training_batch[0], y_: training_batch[1]})\n    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n\n    kernel_wgb11 = kernel_wgb1\n\n\nprint('i  : \\n', kernel_wgb11-kernel_wgb1)\n    \n    \n    \n    test_batch = sess.run(test_image_batch)\n    t_a = accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]})\n    print(\"test accuracy %g\"%t_a)\n\nprint(\"test accuracy333 %g\"%accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]}))\n    test_accuracy.append(t_a)\n    num_i.append(i)\n    plt.plot(num_i,test_accuracy)\ntrain_step.run(feed_dict = {x: training_batch[0], y_: training_batch[1]})\n\npreds = (np.argsort(prob)[::-1])[0:5]\nfor p in preds:\nprint (class_names[p], prob[p])\ncoord.request_stop()\ncoord.join(threads)\nplt.plot(num_i,test_accuracy)", "body": "- #!/usr/bin/env python3\r\n# -*- coding: utf-8 -*-\r\n\"\"\"\r\nCreated on Thu Mar 23 16:19:39 2017\r\n\r\n@author: wgb\r\n\"\"\"\r\nimport os\r\nimport sys\r\nimport numpy as np \r\nimport types\r\nfrom scipy.misc import imread, imresize\r\nimport matplotlib.pyplot as plt\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import ops\r\n#import build_txt\r\n#import build_txt\r\n#\r\n#k = build_txt\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nfile_name = os.getcwd()\r\n\r\ntraining_file  = file_name + '/training_webcam.txt'\r\ntest_file = file_name + '/test_dslr.txt'    \r\n\r\n\r\n\r\nimage_size = 224\r\ntraining_batch_size = 16\r\n\r\nsess = tf.InteractiveSession()\r\n\r\n\"\"\"read training and test data    ################################################################################################\"\"\"\r\n\r\ndef read_txt(file_name):\r\n    \r\n    f  = open(file_name, 'r')\r\n    file_lines = f.readlines()\r\n    \r\n    image_path_total = []\r\n    labels_total = []\r\n    image_id_total = []    \r\n    for i in file_lines:\r\n        divid_data = i.split(' ')\r\n        image_path = divid_data[0]  \r\n        label = divid_data[1]\r\n        one_hot = [0] * 10\r\n        one_hot[int(label)-1] = 1\r\n        labels = one_hot\r\n        image_id = divid_data[2]\r\n        \r\n        image_path = ops.convert_to_tensor(image_path, tf.string)\r\n        labels = ops.convert_to_tensor(labels, tf.float32)\r\n        image_id = ops.convert_to_tensor(int(image_id), tf.float32)\r\n            \r\n        image_path_total.append(image_path)\r\n        labels_total.append(labels)\r\n        image_id_total.append(image_id)\r\n        \r\n    return image_path_total,labels_total, image_id_total\r\n\r\n\r\ndef read_image(single_image_queue,size):\r\n    \"\"\"the image_path is the path of single image\"\"\"\r\n    file_contents = tf.read_file(single_image_queue[0])\r\n    image_data = tf.image.decode_jpeg(file_contents, channels=3)\r\n    image_data = tf.cast(image_data, tf.float32)\r\n    image_data = tf.image.resize_images(image_data, [size, size])\r\n    mean = tf.constant([123.68, 116.779, 103.939], dtype=tf.float32, shape=[1, 1, 3])\r\n    image_data = image_data - mean\r\n    \r\n    image_path = single_image_queue[0]\r\n    label = single_image_queue[1]\r\n    image_id_wgb = single_image_queue[2]\r\n    \r\n    return image_data, label, image_id_wgb,image_path\r\n\r\ntraining_image_total= read_txt(file_name = training_file)\r\ntraining_input_queue = tf.train.slice_input_producer(training_image_total, shuffle=True)\r\ntraining_image_data = read_image(training_input_queue,image_size)\r\ntraining_image_batch = tf.train.shuffle_batch(training_image_data,training_batch_size,num_threads = 2, min_after_dequeue =101,capacity = 1000)\r\n\r\ntest_image_total= read_txt(file_name = test_file)\r\ntest_input_queue = tf.train.slice_input_producer(test_image_total, shuffle=True)\r\ntest_image_data = read_image(test_input_queue,image_size)\r\ntest_image_batch = tf.train.shuffle_batch(test_image_data,batch_size=64,num_threads = 2, min_after_dequeue =101,capacity = 1000)\r\n\r\n\"\"\"define networks    ################################################################################################\"\"\"\r\n\r\n\r\n\r\nx = tf.placeholder(tf.float32, [None, 224,224,3])\r\nx = tf.reshape(x,[-1,224,224,3])\r\n\r\nparameters = []\r\n    \r\nwith tf.device('/cpu:0'):    \r\n    # conv1_1\r\n    with tf.name_scope('conv1_1') as scope:\r\n        kernel1 = tf.Variable(tf.truncated_normal([3, 3, 3, 64], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights', trainable = False)\r\n        conv = tf.nn.conv2d(x, kernel1, [1, 1, 1, 1], padding='SAME')\r\n        biases1 = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases1)\r\n        conv1_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel1, biases1]\r\n    \r\n    # conv1_2\r\n    with tf.name_scope('conv1_2') as scope:\r\n        kernel2 = tf.Variable(tf.truncated_normal([3, 3, 64, 64], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv1_1, kernel2, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[64], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv1_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel2, biases]\r\n    \r\n    # pool1\r\n    pool1 = tf.nn.max_pool(conv1_2,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool1')\r\n    \r\n    # conv2_1\r\n    with tf.name_scope('conv2_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv2_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv2_2\r\n    with tf.name_scope('conv2_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 128], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv2_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv2_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool2\r\n    pool2 = tf.nn.max_pool(conv2_2,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool2')\r\n    \r\n    # conv3_1\r\n    with tf.name_scope('conv3_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 128, 256], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv3_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv3_2\r\n    with tf.name_scope('conv3_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv3_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv3_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv3_3\r\n    with tf.name_scope('conv3_3') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 256], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv3_2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[256], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv3_3 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool3\r\n    pool3 = tf.nn.max_pool(conv3_3,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool3')\r\n    \r\n    # conv4_1\r\n    with tf.name_scope('conv4_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 256, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool3, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv4_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv4_2\r\n    with tf.name_scope('conv4_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv4_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv4_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv4_3\r\n    with tf.name_scope('conv4_3') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv4_2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv4_3 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool4\r\n    pool4 = tf.nn.max_pool(conv4_3,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool4')\r\n    \r\n    # conv5_1\r\n    with tf.name_scope('conv5_1') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(pool4, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv5_1 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv5_2\r\n    with tf.name_scope('conv5_2') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv5_1, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv5_2 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # conv5_3\r\n    with tf.name_scope('conv5_3') as scope:\r\n        kernel = tf.Variable(tf.truncated_normal([3, 3, 512, 512], dtype=tf.float32,\r\n                                                 stddev=1e-1), name='weights')\r\n        conv = tf.nn.conv2d(conv5_2, kernel, [1, 1, 1, 1], padding='SAME')\r\n        biases = tf.Variable(tf.constant(0.0, shape=[512], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        out = tf.nn.bias_add(conv, biases)\r\n        conv5_3 = tf.nn.relu(out, name=scope)\r\n        parameters += [kernel, biases]\r\n    \r\n    # pool5\r\n    pool5 = tf.nn.max_pool(conv5_3,\r\n                           ksize=[1, 2, 2, 1],\r\n                           strides=[1, 2, 2, 1],\r\n                           padding='SAME',\r\n                           name='pool4')\r\n    \r\n    #fc1\r\n    with tf.name_scope('fc1') as scope:\r\n        shape = int(np.prod(pool5.get_shape()[1:]))\r\n        fc1w = tf.Variable(tf.truncated_normal([shape, 4096],\r\n                                                     dtype=tf.float32,\r\n                                                     stddev=1e-1), name='weights')\r\n        fc1b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        pool5_flat = tf.reshape(pool5, [-1, shape])\r\n        fc1l = tf.nn.bias_add(tf.matmul(pool5_flat, fc1w), fc1b)\r\n        fc1 = tf.nn.relu(fc1l)\r\n        parameters += [fc1w, fc1b]\r\n    \r\n    # fc2\r\n    with tf.name_scope('fc2') as scope:\r\n        fc2w = tf.Variable(tf.truncated_normal([4096, 4096],\r\n                                                     dtype=tf.float32,\r\n                                                     stddev=1e-1), name='weights')\r\n        fc2b = tf.Variable(tf.constant(1.0, shape=[4096], dtype=tf.float32),\r\n                             trainable=True, name='biases')\r\n        fc2l = tf.nn.bias_add(tf.matmul(fc1, fc2w), fc2b)\r\n        fc2 = tf.nn.relu(fc2l)\r\n        parameters += [fc2w, fc2b]\r\n    \r\n    # fc3\r\n    with tf.name_scope('fc3') as scope:\r\n        fc3w = tf.Variable(tf.truncated_normal([4096, 10],  ##wgb\r\n                                                     dtype=tf.float32,\r\n                                                     stddev=1e-1), name='weights')\r\n        fc3b = tf.Variable(tf.constant(1.0, shape=[10], dtype=tf.float32),  ###wgb\r\n                             trainable=True, name='biases')\r\n        fc3l = tf.nn.bias_add(tf.matmul(fc2, fc3w), fc3b)\r\n        parameters += [fc3w, fc3b]\r\n    \r\n    #probs = tf.nn.softmax(self.fc3l)\r\n    \r\n    \r\n    probs = tf.nn.softmax(fc3l)\r\n    \r\n    y_ = tf.placeholder(tf.float32, [None,10])\r\n                              \r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(probs),reduction_indices = [1]))                             \r\n    correct_prediction = tf.equal(tf.argmax(probs,1), tf.argmax(y_,1))\r\n    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\r\n    train_step = tf.train.AdamOptimizer(1e-2).minimize(cross_entropy)   \r\n                \r\n                \r\nsess = tf.Session()\r\n\r\ntf.global_variables_initializer().run()\r\n\r\nweight_file = 'vgg16_weights.npz'\r\nweights = np.load(weight_file)\r\nkeys = sorted(weights.keys())\r\nfor i, k in enumerate(keys):\r\n    if i  < 30:\r\n        print (i, k, np.shape(weights[k]))\r\n        sess.run(parameters[i].assign(weights[k]))\r\n\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\ntraining_batch = sess.run(training_image_batch)\r\n\r\nkernel_wgb11 = []\r\nkernel_wgb22 = []\r\nfor i in range(10000):\r\n    training_batch = sess.run(training_image_batch)\r\n    print(i)\r\n    kernel_wgb1 = sess.run(biases1,feed_dict = {x: training_batch[0], y_: training_batch[1]})\r\n    \r\n    if i ==0:\r\n        train_accuracy = accuracy.eval(feed_dict = {x: training_batch[0], y_: training_batch[1]})\r\n        print(\"step %d, training accuracy %g\"%(i, train_accuracy))\r\n\r\n        kernel_wgb11 = kernel_wgb1\r\n    \r\n\r\n    print('i  : \\n', kernel_wgb11-kernel_wgb1)\r\n        \r\n        \r\n        \r\n        test_batch = sess.run(test_image_batch)\r\n        t_a = accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]})\r\n        print(\"test accuracy %g\"%t_a)\r\n#        print(\"test accuracy333 %g\"%accuracy.eval(feed_dict = {x: test_batch[0], y_: test_batch[1]}))\r\n        test_accuracy.append(t_a)\r\n        num_i.append(i)\r\n        plt.plot(num_i,test_accuracy)\r\n    train_step.run(feed_dict = {x: training_batch[0], y_: training_batch[1]})\r\n    \r\n                                      \r\n                                      \r\npreds = (np.argsort(prob)[::-1])[0:5]\r\nfor p in preds:\r\n    print (class_names[p], prob[p])\r\n\r\n\r\ncoord.request_stop()\r\ncoord.join(threads)\r\n\r\nplt.plot(num_i,test_accuracy)"}