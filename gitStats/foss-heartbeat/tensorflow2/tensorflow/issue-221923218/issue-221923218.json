{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9232", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9232/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9232/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9232/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9232", "id": 221923218, "node_id": "MDU6SXNzdWUyMjE5MjMyMTg=", "number": 9232, "title": "One set of GPUs on same machine and same model work well, another gets OOM error", "user": {"login": "parshakova", "id": 22541119, "node_id": "MDQ6VXNlcjIyNTQxMTE5", "avatar_url": "https://avatars0.githubusercontent.com/u/22541119?v=4", "gravatar_id": "", "url": "https://api.github.com/users/parshakova", "html_url": "https://github.com/parshakova", "followers_url": "https://api.github.com/users/parshakova/followers", "following_url": "https://api.github.com/users/parshakova/following{/other_user}", "gists_url": "https://api.github.com/users/parshakova/gists{/gist_id}", "starred_url": "https://api.github.com/users/parshakova/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/parshakova/subscriptions", "organizations_url": "https://api.github.com/users/parshakova/orgs", "repos_url": "https://api.github.com/users/parshakova/repos", "events_url": "https://api.github.com/users/parshakova/events{/privacy}", "received_events_url": "https://api.github.com/users/parshakova/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-15T03:17:31Z", "updated_at": "2017-04-15T06:58:25Z", "closed_at": "2017-04-15T06:58:18Z", "author_association": "NONE", "body_html": "<p>I am using multiple GPUs (num_gpus = 4) for training one model with multiple towers. The model is training well on one set of GPUs: <code>CUDA_VISIBLE_DEVICES = 0,1,2,3</code> while it gets OOM problem during the first graph evaluation with <code>CUDA_VISIBLE_DEVICES = 0,1,4,5</code></p>\n<p>Following options are used for creating a session</p>\n<div class=\"highlight highlight-source-python\"><pre>session_config<span class=\"pl-k\">=</span>tf.ConfigProto(\n      <span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n      <span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nsession_config.gpu_options.per_process_gpu_memory_fraction <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.94</span>\nsession_config.gpu_options.allow_growth<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span></pre></div>\n<p>Batch size, is already super small, = 3</p>\n<h3>System information</h3>\n<p>Tensorflow 1.0<br>\nCuda 8.0<br>\nUbuntu 14.04.5 LTS<br>\nAll GPUs : GeForce GTX 1080</p>\n<h3>Source code / logs</h3>\n<pre><code>name: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:07:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xcc4593a0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:08:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd2404670\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:18:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd25591b0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:1c:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:07:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 1080, pci bus id: 0000:08:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: GeForce GTX 1080, pci bus id: 0000:18:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: GeForce GTX 1080, pci bus id: 0000:1c:00.0)\n\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 47441 get requests, put_count=8461 evicted_count=1000 eviction_rate=0.118189 and unsatisfied allocation rate=0.844839\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.68GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.86GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2698 get requests, put_count=8709 evicted_c \n</code></pre>", "body_text": "I am using multiple GPUs (num_gpus = 4) for training one model with multiple towers. The model is training well on one set of GPUs: CUDA_VISIBLE_DEVICES = 0,1,2,3 while it gets OOM problem during the first graph evaluation with CUDA_VISIBLE_DEVICES = 0,1,4,5\nFollowing options are used for creating a session\nsession_config=tf.ConfigProto(\n      allow_soft_placement=True,\n      log_device_placement=False)\nsession_config.gpu_options.per_process_gpu_memory_fraction = 0.94\nsession_config.gpu_options.allow_growth=False\nBatch size, is already super small, = 3\nSystem information\nTensorflow 1.0\nCuda 8.0\nUbuntu 14.04.5 LTS\nAll GPUs : GeForce GTX 1080\nSource code / logs\nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:07:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xcc4593a0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:08:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd2404670\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:18:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd25591b0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\npciBusID 0000:1c:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   Y Y Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:07:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:08:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080, pci bus id: 0000:18:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080, pci bus id: 0000:1c:00.0)\n\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 47441 get requests, put_count=8461 evicted_count=1000 eviction_rate=0.118189 and unsatisfied allocation rate=0.844839\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.68GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.86GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2698 get requests, put_count=8709 evicted_c", "body": "I am using multiple GPUs (num_gpus = 4) for training one model with multiple towers. The model is training well on one set of GPUs: `CUDA_VISIBLE_DEVICES = 0,1,2,3` while it gets OOM problem during the first graph evaluation with `CUDA_VISIBLE_DEVICES = 0,1,4,5`\r\n\r\n\r\nFollowing options are used for creating a session\r\n```python\r\nsession_config=tf.ConfigProto(\r\n      allow_soft_placement=True,\r\n      log_device_placement=False)\r\nsession_config.gpu_options.per_process_gpu_memory_fraction = 0.94\r\nsession_config.gpu_options.allow_growth=False\r\n```\r\n\r\nBatch size, is already super small, = 3\r\n\r\n\r\n\r\n   \r\n### System information\r\nTensorflow 1.0\r\nCuda 8.0\r\nUbuntu 14.04.5 LTS\r\nAll GPUs : GeForce GTX 1080 \r\n\r\n### Source code / logs\r\n```\r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:07:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xcc4593a0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:08:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd2404670\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 2 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:18:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0xd25591b0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 3 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.7335\r\npciBusID 0000:1c:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 2 3 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 2:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 3:   Y Y Y Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:07:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:08:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:2) -> (device: 2, name: GeForce GTX 1080, pci bus id: 0000:18:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:3) -> (device: 3, name: GeForce GTX 1080, pci bus id: 0000:1c:00.0)\r\n\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 47441 get requests, put_count=8461 evicted_count=1000 eviction_rate=0.118189 and unsatisfied allocation rate=0.844839\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.33GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.08GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.98GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.54GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.17GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 2.68GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:217] Ran out of memory trying to allocate 3.86GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2698 get requests, put_count=8709 evicted_c \r\n```"}