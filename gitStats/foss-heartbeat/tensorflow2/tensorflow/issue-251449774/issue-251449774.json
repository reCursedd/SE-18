{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12419", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12419/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12419/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12419/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12419", "id": 251449774, "node_id": "MDU6SXNzdWUyNTE0NDk3NzQ=", "number": 12419, "title": "Non-fused batch norm with NCHW is mush slower than with NHWC", "user": {"login": "shixiazuike", "id": 7998316, "node_id": "MDQ6VXNlcjc5OTgzMTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7998316?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shixiazuike", "html_url": "https://github.com/shixiazuike", "followers_url": "https://api.github.com/users/shixiazuike/followers", "following_url": "https://api.github.com/users/shixiazuike/following{/other_user}", "gists_url": "https://api.github.com/users/shixiazuike/gists{/gist_id}", "starred_url": "https://api.github.com/users/shixiazuike/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shixiazuike/subscriptions", "organizations_url": "https://api.github.com/users/shixiazuike/orgs", "repos_url": "https://api.github.com/users/shixiazuike/repos", "events_url": "https://api.github.com/users/shixiazuike/events{/privacy}", "received_events_url": "https://api.github.com/users/shixiazuike/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-08-19T21:06:27Z", "updated_at": "2017-08-24T22:10:13Z", "closed_at": "2017-08-24T22:04:57Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>I noticed that in my environment, non-fused batch norm with \"NCHW\" format  run significantly slower</p>\n<h3>Environment info</h3>\n<div class=\"highlight highlight-source-shell\"><pre>Environment\n\nGPU: 8x NVIDIA\u00ae Tesla\u00ae M40/P40\nOS: Ubuntu 16.04 LTS with tests run via Docker\nCUDA / cuDNN: 8.0 / 5.1\nTensorflow Version: 1.2.1\nDocker Images: docker pull tensorflow/tensorflow:1.2.1-gpu\nDataSet: Synthetic images</pre></div>\n<h3>Source code / logs</h3>\n<p>Here is my source code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time \n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> pdb \n\ndata_format <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NCHW<span class=\"pl-pds\">\"</span></span>\nfused<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\nbatch_size<span class=\"pl-k\">=</span><span class=\"pl-c1\">512</span>\nnclass <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1001</span>\nnum_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span> \n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n    images <span class=\"pl-k\">=</span> tf.truncated_normal(\n            <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[batch_size, <span class=\"pl-c1\">227</span>, <span class=\"pl-c1\">227</span>, <span class=\"pl-c1\">3</span>], \n            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n            <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1e-1</span>,\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fake_images<span class=\"pl-pds\">\"</span></span>)\n    images <span class=\"pl-k\">=</span> tf.contrib.framework.local_variable(images, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>images<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>):\n    labels <span class=\"pl-k\">=</span> tf.random_uniform(\n            [batch_size],\n            <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32,\n            <span class=\"pl-v\">minval</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n            <span class=\"pl-v\">maxval</span><span class=\"pl-k\">=</span>nclass,\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fake_labels<span class=\"pl-pds\">\"</span></span>)\n    labels <span class=\"pl-k\">=</span> tf.contrib.framework.local_variable(labels, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>)\n    labels <span class=\"pl-k\">-=</span> <span class=\"pl-c1\">1</span>\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n    images <span class=\"pl-k\">*=</span>  <span class=\"pl-c1\">1</span>. <span class=\"pl-k\">/</span> <span class=\"pl-c1\">256</span>\n    images <span class=\"pl-k\">=</span> tf.subtract(images, <span class=\"pl-c1\">0.5</span>)\n    images <span class=\"pl-k\">=</span> tf.multiply(images, <span class=\"pl-c1\">2.0</span>)\n    <span class=\"pl-k\">if</span> data_format <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NCHW<span class=\"pl-pds\">\"</span></span>:\n        images <span class=\"pl-k\">=</span> tf.transpose(images, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.conv2d(images, <span class=\"pl-c1\">64</span>, [<span class=\"pl-c1\">11</span>, <span class=\"pl-c1\">11</span>], [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>],\n                                      <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>VALID<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">biases_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                      <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.max_pool2d(logits, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.batch_norm(logits, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span>fused, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.conv2d(logits, <span class=\"pl-c1\">192</span>, [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>],\n                                      <span class=\"pl-v\">biases_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                      <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.batch_norm(logits, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span>fused, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.max_pool2d(logits, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.conv2d(logits, <span class=\"pl-c1\">384</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>],\n                                      <span class=\"pl-v\">biases_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                      <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.batch_norm(logits, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span>fused, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.conv2d(logits, <span class=\"pl-c1\">384</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>],\n                                      <span class=\"pl-v\">biases_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                      <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.conv2d(logits, <span class=\"pl-c1\">256</span>, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>],\n                                      <span class=\"pl-v\">biases_initializer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n                                      <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.max_pool2d(logits, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>], <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span>data_format)\n\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.flatten(logits)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(logits, <span class=\"pl-c1\">4096</span>)\n    logits <span class=\"pl-k\">=</span> tf.nn.dropout(logits, <span class=\"pl-c1\">0.5</span>)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(logits, <span class=\"pl-c1\">4096</span>)\n    logits <span class=\"pl-k\">=</span> tf.nn.dropout(logits, <span class=\"pl-c1\">0.5</span>)\n    logits <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(logits, nclass, <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n\n    cross_entropy <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(\n            <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>xentropy<span class=\"pl-pds\">'</span></span>)\n    loss <span class=\"pl-k\">=</span> tf.reduce_mean(cross_entropy, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>xentropy_mean<span class=\"pl-pds\">'</span></span>)\n\n    opt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.005</span>)\n    training_op <span class=\"pl-k\">=</span> opt.minimize(loss)\n    update_ops <span class=\"pl-k\">=</span> tf.get_collection(tf.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>)\n    train_op <span class=\"pl-k\">=</span> tf.group(<span class=\"pl-k\">*</span>([training_op] <span class=\"pl-k\">+</span> update_ops))\n\n    step_train_times <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n        <span class=\"pl-k\">for</span> local_step <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(num_steps):\n            batch_start <span class=\"pl-k\">=</span> time.time()\n            sess.run(train_op)\n            step_train_times.append(time.time() <span class=\"pl-k\">-</span> batch_start)\n            images_per_sec <span class=\"pl-k\">=</span> batch_size <span class=\"pl-k\">/</span> step_train_times[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n            <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>local step <span class=\"pl-c1\">%d</span>, <span class=\"pl-c1\">%.2f</span> images/sec<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (\n                    local_step<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, images_per_sec)\n</pre></div>\n<p>And my tested results:</p>\n<table>\n<thead>\n<tr>\n<th>data format</th>\n<th>fused/non-fused batch norm</th>\n<th>images/sec</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NHWC</td>\n<td>fused</td>\n<td>1085.5</td>\n</tr>\n<tr>\n<td>NHWC</td>\n<td>non-fused</td>\n<td>969.1</td>\n</tr>\n<tr>\n<td>NCHW</td>\n<td>fused</td>\n<td>1315.6</td>\n</tr>\n<tr>\n<td>NCHW</td>\n<td>non-fused</td>\n<td><strong>301.1</strong></td>\n</tr>\n</tbody>\n</table>\n<p>I test benchmarks with model resnet50 on P40 and get similar results</p>\n<table>\n<thead>\n<tr>\n<th>data format</th>\n<th>fused/non-fused batch norm</th>\n<th>images/sec</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>NHWC</td>\n<td>fused</td>\n<td>65.7</td>\n</tr>\n<tr>\n<td>NHWC</td>\n<td>non-fused</td>\n<td>51.0</td>\n</tr>\n<tr>\n<td>NCHW</td>\n<td>fused</td>\n<td>84.4</td>\n</tr>\n<tr>\n<td>NCHW</td>\n<td>non-fused</td>\n<td><strong>7.5</strong></td>\n</tr>\n</tbody>\n</table>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I found issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"208010826\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7551\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7551/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/7551\">#7551</a> similar with my problem but  with opposite result</p>", "body_text": "Describe the problem\nI noticed that in my environment, non-fused batch norm with \"NCHW\" format  run significantly slower\nEnvironment info\nEnvironment\n\nGPU: 8x NVIDIA\u00ae Tesla\u00ae M40/P40\nOS: Ubuntu 16.04 LTS with tests run via Docker\nCUDA / cuDNN: 8.0 / 5.1\nTensorflow Version: 1.2.1\nDocker Images: docker pull tensorflow/tensorflow:1.2.1-gpu\nDataSet: Synthetic images\nSource code / logs\nHere is my source code\nimport time \nimport tensorflow as tf\nimport pdb \n\ndata_format = \"NCHW\"\nfused=False\nbatch_size=512\nnclass = 1001\nnum_steps = 100 \n\nwith tf.device(\"/gpu:0\"):\n    images = tf.truncated_normal(\n            shape=[batch_size, 227, 227, 3], \n            dtype=tf.float32,\n            stddev=1e-1,\n            name=\"fake_images\")\n    images = tf.contrib.framework.local_variable(images, name='images')\nwith tf.device(\"/cpu:0\"):\n    labels = tf.random_uniform(\n            [batch_size],\n            dtype=tf.int32,\n            minval=1,\n            maxval=nclass,\n            name=\"fake_labels\")\n    labels = tf.contrib.framework.local_variable(labels, name='labels')\n    labels -= 1\n\nwith tf.device(\"/gpu:0\"):\n    images *=  1. / 256\n    images = tf.subtract(images, 0.5)\n    images = tf.multiply(images, 2.0)\n    if data_format == \"NCHW\":\n        images = tf.transpose(images, [0, 3, 1, 2])\n    logits = tf.contrib.layers.conv2d(images, 64, [11, 11], [4, 4],\n                                      padding=\"VALID\", biases_initializer=None,\n                                      data_format=data_format)\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\n\n    logits = tf.contrib.layers.conv2d(logits, 192, [5, 5],\n                                      biases_initializer=None,\n                                      data_format=data_format)\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\n\n    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],\n                                      biases_initializer=None,\n                                      data_format=data_format)\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\n    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],\n                                      biases_initializer=None,\n                                      data_format=data_format)\n    logits = tf.contrib.layers.conv2d(logits, 256, [3, 3],\n                                      biases_initializer=None,\n                                      data_format=data_format)\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\n\n    logits = tf.contrib.layers.flatten(logits)\n    logits = tf.contrib.layers.fully_connected(logits, 4096)\n    logits = tf.nn.dropout(logits, 0.5)\n    logits = tf.contrib.layers.fully_connected(logits, 4096)\n    logits = tf.nn.dropout(logits, 0.5)\n    logits = tf.contrib.layers.fully_connected(logits, nclass, activation_fn=None)\n\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n            logits=logits, labels=labels, name='xentropy')\n    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n\n    opt = tf.train.GradientDescentOptimizer(0.005)\n    training_op = opt.minimize(loss)\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    train_op = tf.group(*([training_op] + update_ops))\n\n    step_train_times = []\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        sess.run(tf.local_variables_initializer())\n\n        for local_step in xrange(num_steps):\n            batch_start = time.time()\n            sess.run(train_op)\n            step_train_times.append(time.time() - batch_start)\n            images_per_sec = batch_size / step_train_times[-1]\n            print \"local step %d, %.2f images/sec\" % (\n                    local_step+1, images_per_sec)\n\nAnd my tested results:\n\n\n\ndata format\nfused/non-fused batch norm\nimages/sec\n\n\n\n\nNHWC\nfused\n1085.5\n\n\nNHWC\nnon-fused\n969.1\n\n\nNCHW\nfused\n1315.6\n\n\nNCHW\nnon-fused\n301.1\n\n\n\nI test benchmarks with model resnet50 on P40 and get similar results\n\n\n\ndata format\nfused/non-fused batch norm\nimages/sec\n\n\n\n\nNHWC\nfused\n65.7\n\n\nNHWC\nnon-fused\n51.0\n\n\nNCHW\nfused\n84.4\n\n\nNCHW\nnon-fused\n7.5\n\n\n\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI found issue #7551 similar with my problem but  with opposite result", "body": "### Describe the problem\r\nI noticed that in my environment, non-fused batch norm with \"NCHW\" format  run significantly slower \r\n### Environment info\r\n```bash\r\nEnvironment\r\n\r\nGPU: 8x NVIDIA\u00ae Tesla\u00ae M40/P40\r\nOS: Ubuntu 16.04 LTS with tests run via Docker\r\nCUDA / cuDNN: 8.0 / 5.1\r\nTensorflow Version: 1.2.1\r\nDocker Images: docker pull tensorflow/tensorflow:1.2.1-gpu\r\nDataSet: Synthetic images\r\n```\r\n### Source code / logs\r\nHere is my source code\r\n```python\r\nimport time \r\nimport tensorflow as tf\r\nimport pdb \r\n\r\ndata_format = \"NCHW\"\r\nfused=False\r\nbatch_size=512\r\nnclass = 1001\r\nnum_steps = 100 \r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    images = tf.truncated_normal(\r\n            shape=[batch_size, 227, 227, 3], \r\n            dtype=tf.float32,\r\n            stddev=1e-1,\r\n            name=\"fake_images\")\r\n    images = tf.contrib.framework.local_variable(images, name='images')\r\nwith tf.device(\"/cpu:0\"):\r\n    labels = tf.random_uniform(\r\n            [batch_size],\r\n            dtype=tf.int32,\r\n            minval=1,\r\n            maxval=nclass,\r\n            name=\"fake_labels\")\r\n    labels = tf.contrib.framework.local_variable(labels, name='labels')\r\n    labels -= 1\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    images *=  1. / 256\r\n    images = tf.subtract(images, 0.5)\r\n    images = tf.multiply(images, 2.0)\r\n    if data_format == \"NCHW\":\r\n        images = tf.transpose(images, [0, 3, 1, 2])\r\n    logits = tf.contrib.layers.conv2d(images, 64, [11, 11], [4, 4],\r\n                                      padding=\"VALID\", biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\r\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\r\n\r\n    logits = tf.contrib.layers.conv2d(logits, 192, [5, 5],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\r\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\r\n\r\n    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.batch_norm(logits, fused=fused, data_format=data_format)\r\n    logits = tf.contrib.layers.conv2d(logits, 384, [3, 3],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.conv2d(logits, 256, [3, 3],\r\n                                      biases_initializer=None,\r\n                                      data_format=data_format)\r\n    logits = tf.contrib.layers.max_pool2d(logits, [3, 3], 2, data_format=data_format)\r\n\r\n    logits = tf.contrib.layers.flatten(logits)\r\n    logits = tf.contrib.layers.fully_connected(logits, 4096)\r\n    logits = tf.nn.dropout(logits, 0.5)\r\n    logits = tf.contrib.layers.fully_connected(logits, 4096)\r\n    logits = tf.nn.dropout(logits, 0.5)\r\n    logits = tf.contrib.layers.fully_connected(logits, nclass, activation_fn=None)\r\n\r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\r\n            logits=logits, labels=labels, name='xentropy')\r\n    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\r\n\r\n    opt = tf.train.GradientDescentOptimizer(0.005)\r\n    training_op = opt.minimize(loss)\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    train_op = tf.group(*([training_op] + update_ops))\r\n\r\n    step_train_times = []\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(tf.local_variables_initializer())\r\n\r\n        for local_step in xrange(num_steps):\r\n            batch_start = time.time()\r\n            sess.run(train_op)\r\n            step_train_times.append(time.time() - batch_start)\r\n            images_per_sec = batch_size / step_train_times[-1]\r\n            print \"local step %d, %.2f images/sec\" % (\r\n                    local_step+1, images_per_sec)\r\n\r\n```\r\n\r\nAnd my tested results:\r\n\r\n data format |  fused/non-fused batch norm  |  images/sec\r\n------------ | ------------- | --------------\r\nNHWC | fused |  1085.5 \r\nNHWC | non-fused |  969.1\r\nNCHW | fused |  1315.6\r\nNCHW | non-fused | **301.1**\r\n\r\nI test benchmarks with model resnet50 on P40 and get similar results\r\n\r\n data format |  fused/non-fused batch norm  |  images/sec\r\n------------ | ------------- | --------------\r\nNHWC | fused |  65.7 \r\nNHWC | non-fused |  51.0\r\nNCHW | fused |   84.4\r\nNCHW | non-fused | **7.5**\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI found issue https://github.com/tensorflow/tensorflow/issues/7551 similar with my problem but  with opposite result"}