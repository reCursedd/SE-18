{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23349", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23349/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23349/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23349/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23349", "id": 375010242, "node_id": "MDU6SXNzdWUzNzUwMTAyNDI=", "number": 23349, "title": "my CNN models print NAN. what is reason??? i don't know what is it.", "user": {"login": "pervin0527", "id": 43873288, "node_id": "MDQ6VXNlcjQzODczMjg4", "avatar_url": "https://avatars1.githubusercontent.com/u/43873288?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pervin0527", "html_url": "https://github.com/pervin0527", "followers_url": "https://api.github.com/users/pervin0527/followers", "following_url": "https://api.github.com/users/pervin0527/following{/other_user}", "gists_url": "https://api.github.com/users/pervin0527/gists{/gist_id}", "starred_url": "https://api.github.com/users/pervin0527/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pervin0527/subscriptions", "organizations_url": "https://api.github.com/users/pervin0527/orgs", "repos_url": "https://api.github.com/users/pervin0527/repos", "events_url": "https://api.github.com/users/pervin0527/events{/privacy}", "received_events_url": "https://api.github.com/users/pervin0527/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-29T13:16:54Z", "updated_at": "2018-10-30T16:50:36Z", "closed_at": "2018-10-30T16:50:12Z", "author_association": "NONE", "body_html": "<p>I made a CNN model.<br>\nAll results are output as \"nan\".<br>\nWhat's wrong? Please help me.</p>\n<p>here is my repo<br>\n<a href=\"https://github.com/pervin0527/pervin0527\">https://github.com/pervin0527/pervin0527</a></p>\n<p>def get_filename_set(data_set):<br>\nlabels = []<br>\nfilename_set = []</p>\n<pre><code>with open(FLAGS.data_dir + '/labels.txt') as f:\n    for line in f:\n        inner_list = [elt.strip() for elt in line.split(',')]\n        labels += inner_list\n\nfor i, lable in enumerate(labels):\n    list = os.listdir(FLAGS.data_dir  + '/' + data_set + '/' + lable)\n    for filename in list:\n        filename_set.append([i, FLAGS.data_dir  + '/' + data_set + '/' + lable + '/' + filename])\n\nrandom.shuffle(filename_set)\nreturn filename_set\n</code></pre>\n<p>def read_jpeg(filename):<br>\nvalue = tf.read_file(filename)<br>\ndecoded_image = tf.image.decode_jpeg(value, channels=FLAGS.depth)<br>\nresized_image = tf.image.resize_images(decoded_image, FLAGS.raw_height, FLAGS.raw_width)<br>\nresized_image = tf.cast(resized_image, tf.uint8)</p>\n<pre><code>return resized_image\n</code></pre>\n<p>def convert_images(sess, data_set):<br>\nfilename_set = get_filename_set(data_set)</p>\n<pre><code>with open('./data/' + data_set + '_data.bin', 'wb') as f:\n    for i in range(0, len(filename_set)):\n        resized_image = read_jpeg(filename_set[i][1])\n\n        try:\n            image = sess.run(resized_image)\n        except Exception as e:\n            print e.message\n            continue\n\n        #plt.imshow(np.reshape(image.data, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))\n        #plt.show()\n\n        print i, filename_set[i][0], image.shape\n        f.write(chr(filename_set[i][0]))\n        f.write(image.data)\n</code></pre>\n<p>def read_raw_images(sess, data_set):<br>\nfilename = ['./data/' + data_set + '_data.bin']<br>\nfilename_queue = tf.train.string_input_producer(filename)</p>\n<pre><code>record_bytes = (FLAGS.raw_height) * (FLAGS.raw_width) * FLAGS.depth + 1\nreader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\nkey, value = reader.read(filename_queue)\nrecord_bytes = tf.decode_raw(value, tf.uint8)\n\ntf.train.start_queue_runners(sess=sess)\n\nfor i in range(0, 100):\n    result = sess.run(record_bytes)\n    print i, result[0]\n    image = result[1:len(result)]\n\n    #plt.imshow(np.reshape(image, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))\n    #plt.show()\n</code></pre>\n<p>FLAGS = tf.app.flags.FLAGS</p>\n<p>def weight_variable(shape, name):<br>\nwith tf.device('/cpu:0'):<br>\ninitial = tf.truncated_normal(shape, stddev=0.1)<br>\nvar = tf.Variable(initial, name=name)<br>\nreturn var</p>\n<p>def bias_variable(shape, name):<br>\nwith tf.device('/cpu:0'):<br>\ninitial = tf.constant(0.1, shape=shape)<br>\nvar = tf.Variable(initial, name=name)<br>\nreturn var</p>\n<p>def conv2d(x, W):<br>\nreturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')</p>\n<p>def max_pool(x, height, width):<br>\nreturn tf.nn.max_pool(x, ksize=[1, height, width, 1], strides=[1, height, width, 1], padding='SAME')</p>\n<p>def make_network(images, labels, keep_prob):<br>\nnum_conv = FLAGS.num_conv<br>\nkernel_size = FLAGS.kernel_size<br>\npool_size = FLAGS.pool_size<br>\nnum_map = FLAGS.num_map<br>\nnum_fc_layer = FLAGS.num_fc_layer<br>\nnum_fc_input = FLAGS.num_fc_input</p>\n<pre><code>height = FLAGS.height\nwidth = FLAGS.width\nprev_num_map = FLAGS.depth\nh_pool = images\n\nfor i in range(num_conv):\n    W_conv = weight_variable([kernel_size, kernel_size, prev_num_map, num_map], 'W_conv' + str(i+1))\n    b_conv = bias_variable([num_map], 'b_conv' + str(i+1))\n    h_conv = tf.nn.relu(conv2d(h_pool, W_conv) + b_conv)\n    h_pool = max_pool(h_conv, pool_size, pool_size)\n    prev_num_map = num_map\n    num_map *= 2\n    height /= 2\n    width /= 2\n\nnum_map /= 2\nh_fc_input = tf.reshape(h_pool, [-1, height * width * num_map])\nprev_num_fc_input = height * width * num_map\n\nfor i in range(num_fc_layer):\n    W_fc = weight_variable([prev_num_fc_input, num_fc_input], 'W_fc' + str(i+1))\n    b_fc = bias_variable([num_fc_input], 'b_fc' + str(i+1))\n    h_fc = tf.nn.relu(tf.matmul(h_fc_input, W_fc) + b_fc)\n    h_fc_input = tf.nn.dropout(h_fc, keep_prob)\n    prev_num_fc_input = num_fc_input\n    num_fc_input /= 2\n\nnum_fc_input *= 2\nW_fc = weight_variable([num_fc_input, FLAGS.num_class], 'W_fc' + str(i+2))\nb_fc = bias_variable([FLAGS.num_class], 'b_fc' + str(i+2))\n\nhypothesis = tf.matmul(h_fc_input, W_fc) + b_fc\ncross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hypothesis, labels))\ntrain_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n\nreturn hypothesis, cross_entropy, train_step\n</code></pre>", "body_text": "I made a CNN model.\nAll results are output as \"nan\".\nWhat's wrong? Please help me.\nhere is my repo\nhttps://github.com/pervin0527/pervin0527\ndef get_filename_set(data_set):\nlabels = []\nfilename_set = []\nwith open(FLAGS.data_dir + '/labels.txt') as f:\n    for line in f:\n        inner_list = [elt.strip() for elt in line.split(',')]\n        labels += inner_list\n\nfor i, lable in enumerate(labels):\n    list = os.listdir(FLAGS.data_dir  + '/' + data_set + '/' + lable)\n    for filename in list:\n        filename_set.append([i, FLAGS.data_dir  + '/' + data_set + '/' + lable + '/' + filename])\n\nrandom.shuffle(filename_set)\nreturn filename_set\n\ndef read_jpeg(filename):\nvalue = tf.read_file(filename)\ndecoded_image = tf.image.decode_jpeg(value, channels=FLAGS.depth)\nresized_image = tf.image.resize_images(decoded_image, FLAGS.raw_height, FLAGS.raw_width)\nresized_image = tf.cast(resized_image, tf.uint8)\nreturn resized_image\n\ndef convert_images(sess, data_set):\nfilename_set = get_filename_set(data_set)\nwith open('./data/' + data_set + '_data.bin', 'wb') as f:\n    for i in range(0, len(filename_set)):\n        resized_image = read_jpeg(filename_set[i][1])\n\n        try:\n            image = sess.run(resized_image)\n        except Exception as e:\n            print e.message\n            continue\n\n        #plt.imshow(np.reshape(image.data, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))\n        #plt.show()\n\n        print i, filename_set[i][0], image.shape\n        f.write(chr(filename_set[i][0]))\n        f.write(image.data)\n\ndef read_raw_images(sess, data_set):\nfilename = ['./data/' + data_set + '_data.bin']\nfilename_queue = tf.train.string_input_producer(filename)\nrecord_bytes = (FLAGS.raw_height) * (FLAGS.raw_width) * FLAGS.depth + 1\nreader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\nkey, value = reader.read(filename_queue)\nrecord_bytes = tf.decode_raw(value, tf.uint8)\n\ntf.train.start_queue_runners(sess=sess)\n\nfor i in range(0, 100):\n    result = sess.run(record_bytes)\n    print i, result[0]\n    image = result[1:len(result)]\n\n    #plt.imshow(np.reshape(image, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))\n    #plt.show()\n\nFLAGS = tf.app.flags.FLAGS\ndef weight_variable(shape, name):\nwith tf.device('/cpu:0'):\ninitial = tf.truncated_normal(shape, stddev=0.1)\nvar = tf.Variable(initial, name=name)\nreturn var\ndef bias_variable(shape, name):\nwith tf.device('/cpu:0'):\ninitial = tf.constant(0.1, shape=shape)\nvar = tf.Variable(initial, name=name)\nreturn var\ndef conv2d(x, W):\nreturn tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\ndef max_pool(x, height, width):\nreturn tf.nn.max_pool(x, ksize=[1, height, width, 1], strides=[1, height, width, 1], padding='SAME')\ndef make_network(images, labels, keep_prob):\nnum_conv = FLAGS.num_conv\nkernel_size = FLAGS.kernel_size\npool_size = FLAGS.pool_size\nnum_map = FLAGS.num_map\nnum_fc_layer = FLAGS.num_fc_layer\nnum_fc_input = FLAGS.num_fc_input\nheight = FLAGS.height\nwidth = FLAGS.width\nprev_num_map = FLAGS.depth\nh_pool = images\n\nfor i in range(num_conv):\n    W_conv = weight_variable([kernel_size, kernel_size, prev_num_map, num_map], 'W_conv' + str(i+1))\n    b_conv = bias_variable([num_map], 'b_conv' + str(i+1))\n    h_conv = tf.nn.relu(conv2d(h_pool, W_conv) + b_conv)\n    h_pool = max_pool(h_conv, pool_size, pool_size)\n    prev_num_map = num_map\n    num_map *= 2\n    height /= 2\n    width /= 2\n\nnum_map /= 2\nh_fc_input = tf.reshape(h_pool, [-1, height * width * num_map])\nprev_num_fc_input = height * width * num_map\n\nfor i in range(num_fc_layer):\n    W_fc = weight_variable([prev_num_fc_input, num_fc_input], 'W_fc' + str(i+1))\n    b_fc = bias_variable([num_fc_input], 'b_fc' + str(i+1))\n    h_fc = tf.nn.relu(tf.matmul(h_fc_input, W_fc) + b_fc)\n    h_fc_input = tf.nn.dropout(h_fc, keep_prob)\n    prev_num_fc_input = num_fc_input\n    num_fc_input /= 2\n\nnum_fc_input *= 2\nW_fc = weight_variable([num_fc_input, FLAGS.num_class], 'W_fc' + str(i+2))\nb_fc = bias_variable([FLAGS.num_class], 'b_fc' + str(i+2))\n\nhypothesis = tf.matmul(h_fc_input, W_fc) + b_fc\ncross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hypothesis, labels))\ntrain_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\n\nreturn hypothesis, cross_entropy, train_step", "body": "I made a CNN model.\r\nAll results are output as \"nan\".\r\nWhat's wrong? Please help me.\r\n\r\nhere is my repo\r\nhttps://github.com/pervin0527/pervin0527 \r\n\r\ndef get_filename_set(data_set):\r\n    labels = []\r\n    filename_set = []\r\n\r\n    with open(FLAGS.data_dir + '/labels.txt') as f:\r\n        for line in f:\r\n            inner_list = [elt.strip() for elt in line.split(',')]\r\n            labels += inner_list\r\n\r\n    for i, lable in enumerate(labels):\r\n        list = os.listdir(FLAGS.data_dir  + '/' + data_set + '/' + lable)\r\n        for filename in list:\r\n            filename_set.append([i, FLAGS.data_dir  + '/' + data_set + '/' + lable + '/' + filename])\r\n\r\n    random.shuffle(filename_set)\r\n    return filename_set\r\n\r\ndef read_jpeg(filename):\r\n    value = tf.read_file(filename)\r\n    decoded_image = tf.image.decode_jpeg(value, channels=FLAGS.depth)\r\n    resized_image = tf.image.resize_images(decoded_image, FLAGS.raw_height, FLAGS.raw_width)\r\n    resized_image = tf.cast(resized_image, tf.uint8)\r\n\r\n    return resized_image\r\n\r\ndef convert_images(sess, data_set):\r\n    filename_set = get_filename_set(data_set)\r\n\r\n    with open('./data/' + data_set + '_data.bin', 'wb') as f:\r\n        for i in range(0, len(filename_set)):\r\n            resized_image = read_jpeg(filename_set[i][1])\r\n\r\n            try:\r\n                image = sess.run(resized_image)\r\n            except Exception as e:\r\n                print e.message\r\n                continue\r\n\r\n            #plt.imshow(np.reshape(image.data, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))\r\n            #plt.show()\r\n\r\n            print i, filename_set[i][0], image.shape\r\n            f.write(chr(filename_set[i][0]))\r\n            f.write(image.data)\r\n\r\ndef read_raw_images(sess, data_set):\r\n    filename = ['./data/' + data_set + '_data.bin']\r\n    filename_queue = tf.train.string_input_producer(filename)\r\n\r\n    record_bytes = (FLAGS.raw_height) * (FLAGS.raw_width) * FLAGS.depth + 1\r\n    reader = tf.FixedLengthRecordReader(record_bytes=record_bytes)\r\n    key, value = reader.read(filename_queue)\r\n    record_bytes = tf.decode_raw(value, tf.uint8)\r\n\r\n    tf.train.start_queue_runners(sess=sess)\r\n\r\n    for i in range(0, 100):\r\n        result = sess.run(record_bytes)\r\n        print i, result[0]\r\n        image = result[1:len(result)]\r\n\r\n        #plt.imshow(np.reshape(image, [FLAGS.raw_height, FLAGS.raw_width, FLAGS.depth]))\r\n        #plt.show()\r\n\r\n\r\n\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\ndef weight_variable(shape, name):\r\n    with tf.device('/cpu:0'):\r\n        initial = tf.truncated_normal(shape, stddev=0.1)\r\n        var = tf.Variable(initial, name=name)\r\n    return var\r\n\r\ndef bias_variable(shape, name):\r\n    with tf.device('/cpu:0'):\r\n        initial = tf.constant(0.1, shape=shape)\r\n        var = tf.Variable(initial, name=name)\r\n    return var\r\n\r\ndef conv2d(x, W):\r\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\r\n\r\ndef max_pool(x, height, width):\r\n    return tf.nn.max_pool(x, ksize=[1, height, width, 1], strides=[1, height, width, 1], padding='SAME')\r\n\r\ndef make_network(images, labels, keep_prob):\r\n    num_conv = FLAGS.num_conv\r\n    kernel_size = FLAGS.kernel_size\r\n    pool_size = FLAGS.pool_size\r\n    num_map = FLAGS.num_map\r\n    num_fc_layer = FLAGS.num_fc_layer\r\n    num_fc_input = FLAGS.num_fc_input\r\n\r\n    height = FLAGS.height\r\n    width = FLAGS.width\r\n    prev_num_map = FLAGS.depth\r\n    h_pool = images\r\n\r\n    for i in range(num_conv):\r\n        W_conv = weight_variable([kernel_size, kernel_size, prev_num_map, num_map], 'W_conv' + str(i+1))\r\n        b_conv = bias_variable([num_map], 'b_conv' + str(i+1))\r\n        h_conv = tf.nn.relu(conv2d(h_pool, W_conv) + b_conv)\r\n        h_pool = max_pool(h_conv, pool_size, pool_size)\r\n        prev_num_map = num_map\r\n        num_map *= 2\r\n        height /= 2\r\n        width /= 2\r\n\r\n    num_map /= 2\r\n    h_fc_input = tf.reshape(h_pool, [-1, height * width * num_map])\r\n    prev_num_fc_input = height * width * num_map\r\n\r\n    for i in range(num_fc_layer):\r\n        W_fc = weight_variable([prev_num_fc_input, num_fc_input], 'W_fc' + str(i+1))\r\n        b_fc = bias_variable([num_fc_input], 'b_fc' + str(i+1))\r\n        h_fc = tf.nn.relu(tf.matmul(h_fc_input, W_fc) + b_fc)\r\n        h_fc_input = tf.nn.dropout(h_fc, keep_prob)\r\n        prev_num_fc_input = num_fc_input\r\n        num_fc_input /= 2\r\n\r\n    num_fc_input *= 2\r\n    W_fc = weight_variable([num_fc_input, FLAGS.num_class], 'W_fc' + str(i+2))\r\n    b_fc = bias_variable([FLAGS.num_class], 'b_fc' + str(i+2))\r\n\r\n    hypothesis = tf.matmul(h_fc_input, W_fc) + b_fc\r\n    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(hypothesis, labels))\r\n    train_step = tf.train.AdamOptimizer(FLAGS.learning_rate).minimize(cross_entropy)\r\n\r\n    return hypothesis, cross_entropy, train_step"}