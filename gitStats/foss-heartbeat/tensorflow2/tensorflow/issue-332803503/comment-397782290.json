{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/397782290", "html_url": "https://github.com/tensorflow/tensorflow/issues/20062#issuecomment-397782290", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20062", "id": 397782290, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Nzc4MjI5MA==", "user": {"login": "hmorimitsu", "id": 24420973, "node_id": "MDQ6VXNlcjI0NDIwOTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/24420973?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmorimitsu", "html_url": "https://github.com/hmorimitsu", "followers_url": "https://api.github.com/users/hmorimitsu/followers", "following_url": "https://api.github.com/users/hmorimitsu/following{/other_user}", "gists_url": "https://api.github.com/users/hmorimitsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmorimitsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmorimitsu/subscriptions", "organizations_url": "https://api.github.com/users/hmorimitsu/orgs", "repos_url": "https://api.github.com/users/hmorimitsu/repos", "events_url": "https://api.github.com/users/hmorimitsu/events{/privacy}", "received_events_url": "https://api.github.com/users/hmorimitsu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-16T03:09:02Z", "updated_at": "2018-06-16T04:11:37Z", "author_association": "NONE", "body_html": "<p>Thank you for the reply <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3731025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/allenlavoie\">@allenlavoie</a> . This workaround fixes the problem for the loss, but it seems to create a leak with <code>tf.contrib.summary.record_summaries_every_n_global_steps</code>. This code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\ntf.enable_eager_execution()\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000</span>):\n  tf.reset_default_graph() \n  <span class=\"pl-k\">with</span> tf.contrib.summary.record_summaries_every_n_global_steps(<span class=\"pl-c1\">100</span>):\n    <span class=\"pl-k\">pass</span></pre></div>\n<p>produces the following memory plot:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24420973/41495224-d8ae5a7a-70f8-11e8-88e0-0c22ef9c2c00.png\"><img src=\"https://user-images.githubusercontent.com/24420973/41495224-d8ae5a7a-70f8-11e8-88e0-0c22ef9c2c00.png\" alt=\"mprof_summary_leak\" style=\"max-width:100%;\"></a></p>\n<p>Strangely, this leak only happens if we reset the graph. If we remove <code>tf.reset_default_graph()</code> from the loop, there is no leak at all. Removing the summary from the loop also stops the leak, so it is not the reset operation which leaks.</p>\n<p>EDIT: replacing <code>tf.contrib.summary.record_summaries_every_n_global_steps</code> by <code>tf.contrib.summary.always_record_summaries</code>fixes the leak and the code also runs more than 10 times faster.</p>", "body_text": "Thank you for the reply @allenlavoie . This workaround fixes the problem for the loss, but it seems to create a leak with tf.contrib.summary.record_summaries_every_n_global_steps. This code:\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nfor i in range(100000):\n  tf.reset_default_graph() \n  with tf.contrib.summary.record_summaries_every_n_global_steps(100):\n    pass\nproduces the following memory plot:\n\nStrangely, this leak only happens if we reset the graph. If we remove tf.reset_default_graph() from the loop, there is no leak at all. Removing the summary from the loop also stops the leak, so it is not the reset operation which leaks.\nEDIT: replacing tf.contrib.summary.record_summaries_every_n_global_steps by tf.contrib.summary.always_record_summariesfixes the leak and the code also runs more than 10 times faster.", "body": "Thank you for the reply @allenlavoie . This workaround fixes the problem for the loss, but it seems to create a leak with `tf.contrib.summary.record_summaries_every_n_global_steps`. This code:\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nfor i in range(100000):\r\n  tf.reset_default_graph() \r\n  with tf.contrib.summary.record_summaries_every_n_global_steps(100):\r\n    pass\r\n```\r\nproduces the following memory plot:\r\n![mprof_summary_leak](https://user-images.githubusercontent.com/24420973/41495224-d8ae5a7a-70f8-11e8-88e0-0c22ef9c2c00.png)\r\n\r\nStrangely, this leak only happens if we reset the graph. If we remove `tf.reset_default_graph()` from the loop, there is no leak at all. Removing the summary from the loop also stops the leak, so it is not the reset operation which leaks. \r\n\r\nEDIT: replacing `tf.contrib.summary.record_summaries_every_n_global_steps` by `tf.contrib.summary.always_record_summaries`fixes the leak and the code also runs more than 10 times faster."}