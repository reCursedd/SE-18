{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20062", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20062/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20062/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20062/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20062", "id": 332803503, "node_id": "MDU6SXNzdWUzMzI4MDM1MDM=", "number": 20062, "title": "Memory leak using loss in Eager Execution", "user": {"login": "hmorimitsu", "id": 24420973, "node_id": "MDQ6VXNlcjI0NDIwOTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/24420973?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmorimitsu", "html_url": "https://github.com/hmorimitsu", "followers_url": "https://api.github.com/users/hmorimitsu/followers", "following_url": "https://api.github.com/users/hmorimitsu/following{/other_user}", "gists_url": "https://api.github.com/users/hmorimitsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmorimitsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmorimitsu/subscriptions", "organizations_url": "https://api.github.com/users/hmorimitsu/orgs", "repos_url": "https://api.github.com/users/hmorimitsu/repos", "events_url": "https://api.github.com/users/hmorimitsu/events{/privacy}", "received_events_url": "https://api.github.com/users/hmorimitsu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-15T14:45:02Z", "updated_at": "2018-06-21T18:55:25Z", "closed_at": "2018-06-21T18:55:25Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Xubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.8.0-3211-g1aea422 1.9.0-rc0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1 / 7.1</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 960M - 2GB</li>\n<li><strong>Exact command to reproduce</strong>: The following code:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\ntf.enable_eager_execution()\n\nlayer <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-c1\">5</span>)\ninputs <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">100</span>], tf.float32)\nlabels <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">32</span>], tf.int64)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000</span>):\n  <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n    logits <span class=\"pl-k\">=</span> layer(inputs)\n    loss <span class=\"pl-k\">=</span> tf.losses.sparse_softmax_cross_entropy(labels, logits)</pre></div>\n<h3>Describe the problem</h3>\n<p>I observe a memory leak when computing the loss using the code above. I also tried using other losses, such as <code>tf.losses.mean_squared_error</code> and <code>tf.losses.hinge_loss</code>, and I observed similar leaks. Using memory_profiler produces the following plot:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24420973/41473326-53606eb2-708f-11e8-9618-2f7696781828.png\"><img src=\"https://user-images.githubusercontent.com/24420973/41473326-53606eb2-708f-11e8-9618-2f7696781828.png\" alt=\"mplot_leak_entropy\" style=\"max-width:100%;\"></a></p>\n<p>I tried to use the workaround suggested by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3731025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/allenlavoie\">@allenlavoie</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"328179203\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/19671\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/19671/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/19671\">#19671</a>, but it does not seem to work in this case.</p>\n<h3>Source code / logs</h3>\n<p>I used pympler to track the memory usage, by adding it to the code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> pympler.tracker <span class=\"pl-k\">import</span> SummaryTracker\ntf.enable_eager_execution()\n\nlayer <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-c1\">5</span>)\ninputs <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">100</span>], tf.float32)\nlabels <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">32</span>], tf.int64)\n\ntracker <span class=\"pl-k\">=</span> SummaryTracker()\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100000</span>):\n  <span class=\"pl-k\">with</span> tf.GradientTape() <span class=\"pl-k\">as</span> tape:\n    logits <span class=\"pl-k\">=</span> layer(inputs)\n    loss <span class=\"pl-k\">=</span> tf.losses.sparse_softmax_cross_entropy(labels, logits)\n  <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">1000</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n    tracker.print_diff()</pre></div>\n<p>After a few rounds, it starts to produce the following output:</p>\n<div class=\"highlight highlight-source-shell\"><pre>                types <span class=\"pl-k\">|</span>   <span class=\"pl-c\"><span class=\"pl-c\">#</span> objects |   total size</span>\n===================== <span class=\"pl-k\">|</span> =========== <span class=\"pl-k\">|</span> ============\n  <span class=\"pl-k\">&lt;</span>class <span class=\"pl-s\"><span class=\"pl-pds\">'</span>EagerTensor |        1000 |    156.25 KB</span>\n<span class=\"pl-s\">         &lt;class <span class=\"pl-pds\">'</span></span>list <span class=\"pl-k\">|</span>           0 <span class=\"pl-k\">|</span>      7.14 KB</pre></div>\n<p>So it seems an EagerTensor is being created and not released at every iteration, which may be the cause of the leak.</p>\n<p>I thank <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3731025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/allenlavoie\">@allenlavoie</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=122911\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akshaym\">@akshaym</a> for solving my previous issues in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"324423287\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/19385\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/19385/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/19385\">#19385</a>. I think this problem is related to those.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Xubuntu 16.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): v1.8.0-3211-g1aea422 1.9.0-rc0\nPython version: 3.6.5\nBazel version (if compiling from source): 0.11.1\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.1 / 7.1\nGPU model and memory: GeForce GTX 960M - 2GB\nExact command to reproduce: The following code:\n\nimport tensorflow as tf\ntf.enable_eager_execution()\n\nlayer = tf.keras.layers.Dense(5)\ninputs = tf.zeros([32, 100], tf.float32)\nlabels = tf.zeros([32], tf.int64)\n\nfor i in range(100000):\n  with tf.GradientTape() as tape:\n    logits = layer(inputs)\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\nDescribe the problem\nI observe a memory leak when computing the loss using the code above. I also tried using other losses, such as tf.losses.mean_squared_error and tf.losses.hinge_loss, and I observed similar leaks. Using memory_profiler produces the following plot:\n\nI tried to use the workaround suggested by @allenlavoie in #19671, but it does not seem to work in this case.\nSource code / logs\nI used pympler to track the memory usage, by adding it to the code:\nimport tensorflow as tf\nfrom pympler.tracker import SummaryTracker\ntf.enable_eager_execution()\n\nlayer = tf.keras.layers.Dense(5)\ninputs = tf.zeros([32, 100], tf.float32)\nlabels = tf.zeros([32], tf.int64)\n\ntracker = SummaryTracker()\n\nfor i in range(100000):\n  with tf.GradientTape() as tape:\n    logits = layer(inputs)\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n  if i % 1000 == 0:\n    tracker.print_diff()\nAfter a few rounds, it starts to produce the following output:\n                types |   # objects |   total size\n===================== | =========== | ============\n  <class 'EagerTensor |        1000 |    156.25 KB\n         <class 'list |           0 |      7.14 KB\nSo it seems an EagerTensor is being created and not released at every iteration, which may be the cause of the leak.\nI thank @allenlavoie and @akshaym for solving my previous issues in #19385. I think this problem is related to those.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Xubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-3211-g1aea422 1.9.0-rc0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**: 0.11.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.1 / 7.1\r\n- **GPU model and memory**: GeForce GTX 960M - 2GB\r\n- **Exact command to reproduce**: The following code:\r\n\r\n```python\r\nimport tensorflow as tf\r\ntf.enable_eager_execution()\r\n\r\nlayer = tf.keras.layers.Dense(5)\r\ninputs = tf.zeros([32, 100], tf.float32)\r\nlabels = tf.zeros([32], tf.int64)\r\n\r\nfor i in range(100000):\r\n  with tf.GradientTape() as tape:\r\n    logits = layer(inputs)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\r\n```\r\n\r\n### Describe the problem\r\n\r\nI observe a memory leak when computing the loss using the code above. I also tried using other losses, such as `tf.losses.mean_squared_error` and `tf.losses.hinge_loss`, and I observed similar leaks. Using memory_profiler produces the following plot:\r\n![mplot_leak_entropy](https://user-images.githubusercontent.com/24420973/41473326-53606eb2-708f-11e8-9618-2f7696781828.png)\r\n\r\nI tried to use the workaround suggested by @allenlavoie in #19671, but it does not seem to work in this case.\r\n\r\n### Source code / logs\r\n\r\nI used pympler to track the memory usage, by adding it to the code:\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom pympler.tracker import SummaryTracker\r\ntf.enable_eager_execution()\r\n\r\nlayer = tf.keras.layers.Dense(5)\r\ninputs = tf.zeros([32, 100], tf.float32)\r\nlabels = tf.zeros([32], tf.int64)\r\n\r\ntracker = SummaryTracker()\r\n\r\nfor i in range(100000):\r\n  with tf.GradientTape() as tape:\r\n    logits = layer(inputs)\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\r\n  if i % 1000 == 0:\r\n    tracker.print_diff()\r\n```\r\n\r\nAfter a few rounds, it starts to produce the following output:\r\n```bash\r\n                types |   # objects |   total size\r\n===================== | =========== | ============\r\n  <class 'EagerTensor |        1000 |    156.25 KB\r\n         <class 'list |           0 |      7.14 KB\r\n```\r\n\r\nSo it seems an EagerTensor is being created and not released at every iteration, which may be the cause of the leak.\r\n\r\nI thank @allenlavoie and @akshaym for solving my previous issues in #19385. I think this problem is related to those."}