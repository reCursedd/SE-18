{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6326", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6326/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6326/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6326/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6326", "id": 195728676, "node_id": "MDU6SXNzdWUxOTU3Mjg2NzY=", "number": 6326, "title": "Bug when generate checkpoint in distributed training", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-12-15T06:38:38Z", "updated_at": "2016-12-17T06:44:37Z", "closed_at": "2016-12-17T06:44:37Z", "author_association": "NONE", "body_html": "<p>I am training model in distributed mode  with 1 ps and 2 worker all in localhost with different ports.<br>\nMy model is a 4 layer with full connected network.<br>\nI find that , when traing in 10,10,10,10(4 layer, each layer 10 hidden unit ), every thinks works well,<br>\nBut, when enlarge network to 20,20,20,20 , the traing process works well, but can not generate checkponit any more.  with adding log , I find it's stuck at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1372\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1372</a><br>\nit maybe not slow , I wait for hours, nothing but stuck at it.</p>\n<p>more:<br>\nwhen traing not in distributed with same size(20,20,20,20) network,it's works well with checkpoint.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>cannot find  any thing about this.</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\ncentos6.5</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n    # Read TFRecords files for training\n    filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.train),\n        num_epochs=epoch_number)\n    serialized_example = read_and_decode(filename_queue)\n    batch_serialized_example = tf.train.shuffle_batch(\n        [serialized_example],\n        batch_size=batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    features = tf.parse_example(\n        batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    batch_labels = features[\"label\"]\n    batch_ids = features[\"ids\"]\n    batch_values = features[\"values\"]\n\n    # Read TFRecords file for validatioin\n    validate_filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.eval),\n        num_epochs=epoch_number)\n    validate_serialized_example = read_and_decode(validate_filename_queue)\n    validate_batch_serialized_example = tf.train.shuffle_batch(\n        [validate_serialized_example],\n        batch_size=validate_batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    validate_features = tf.parse_example(\n        validate_batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    validate_batch_labels = features[\"label\"]\n    validate_batch_ids = features[\"ids\"]\n    validate_batch_values = features[\"values\"]\n    logits = inference(batch_ids, batch_values)\n    batch_labels = tf.to_int64(batch_labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                   batch_labels)\n    loss = tf.reduce_mean(cross_entropy, name='loss')\n\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\n\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n\n\n\n\n    # Initialize saver and summary\n    steps_to_validate = FLAGS.steps_to_validate\n    init_op = tf.initialize_all_variables()\n\n    saver = tf.train.Saver(max_to_keep = 2)\n    keys_placeholder = tf.placeholder(\"float\")\n    keys = tf.identity(keys_placeholder)\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\n                                                'softmax': inference_softmax.name,\n                                                'prediction': inference_op.name}))\n\n    summary_op = tf.merge_all_summaries()\n\n\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                         logdir=\"./supervisor/\",\n                         init_op=init_op,\n                         summary_op=summary_op,\n                         saver=saver,\n                         global_step=global_step,\n                         save_model_secs=60)\n\n# Create session to run graph\nwith sv.managed_session(server.target) as sess:\n\n    while not sv.should_stop():\n        # Get coordinator and run queues to read data\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        start_time = datetime.datetime.now()\n\n        try:\n            while not coord.should_stop():\n                _, loss_value, step = sess.run([train_op, loss, global_step])\n                if step % steps_to_validate == 0:\n                    accuracy_value, auc_value, summary_value = sess.run(\n                        [accuracy, auc_op, summary_op])\n                    end_time = datetime.datetime.now()\n                    print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\n                        end_time - start_time,\n                        FLAGS.task_index,\n                        step, loss_value, accuracy_value,\n                        auc_value))\n\n                    start_time = end_time\n        except tf.errors.OutOfRangeError:\n            print(\"Done training after reading all data\")\n        finally:\n            coord.request_stop()\n            print(\"coord stopped\")\n\n        # Wait for threads to exit\n        coord.join(threads)\n</code></pre>", "body_text": "I am training model in distributed mode  with 1 ps and 2 worker all in localhost with different ports.\nMy model is a 4 layer with full connected network.\nI find that , when traing in 10,10,10,10(4 layer, each layer 10 hidden unit ), every thinks works well,\nBut, when enlarge network to 20,20,20,20 , the traing process works well, but can not generate checkponit any more.  with adding log , I find it's stuck at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1372\nit maybe not slow , I wait for hours, nothing but stuck at it.\nmore:\nwhen traing not in distributed with same size(20,20,20,20) network,it's works well with checkpoint.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\ncannot find  any thing about this.\nEnvironment info\nOperating System:\ncentos6.5\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nwith tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n    # Read TFRecords files for training\n    filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.train),\n        num_epochs=epoch_number)\n    serialized_example = read_and_decode(filename_queue)\n    batch_serialized_example = tf.train.shuffle_batch(\n        [serialized_example],\n        batch_size=batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    features = tf.parse_example(\n        batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    batch_labels = features[\"label\"]\n    batch_ids = features[\"ids\"]\n    batch_values = features[\"values\"]\n\n    # Read TFRecords file for validatioin\n    validate_filename_queue = tf.train.string_input_producer(\n        tf.train.match_filenames_once(FLAGS.eval),\n        num_epochs=epoch_number)\n    validate_serialized_example = read_and_decode(validate_filename_queue)\n    validate_batch_serialized_example = tf.train.shuffle_batch(\n        [validate_serialized_example],\n        batch_size=validate_batch_size,\n        num_threads=thread_number,\n        capacity=capacity,\n        min_after_dequeue=min_after_dequeue)\n    validate_features = tf.parse_example(\n        validate_batch_serialized_example,\n        features={\n            \"label\": tf.FixedLenFeature([], tf.float32),\n            \"ids\": tf.VarLenFeature(tf.int64),\n            \"values\": tf.VarLenFeature(tf.float32),\n        })\n    validate_batch_labels = features[\"label\"]\n    validate_batch_ids = features[\"ids\"]\n    validate_batch_values = features[\"values\"]\n    logits = inference(batch_ids, batch_values)\n    batch_labels = tf.to_int64(batch_labels)\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                   batch_labels)\n    loss = tf.reduce_mean(cross_entropy, name='loss')\n\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\n\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\n\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op = optimizer.minimize(loss, global_step=global_step)\n\n\n\n\n    # Initialize saver and summary\n    steps_to_validate = FLAGS.steps_to_validate\n    init_op = tf.initialize_all_variables()\n\n    saver = tf.train.Saver(max_to_keep = 2)\n    keys_placeholder = tf.placeholder(\"float\")\n    keys = tf.identity(keys_placeholder)\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\n                                                'softmax': inference_softmax.name,\n                                                'prediction': inference_op.name}))\n\n    summary_op = tf.merge_all_summaries()\n\n\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                         logdir=\"./supervisor/\",\n                         init_op=init_op,\n                         summary_op=summary_op,\n                         saver=saver,\n                         global_step=global_step,\n                         save_model_secs=60)\n\n# Create session to run graph\nwith sv.managed_session(server.target) as sess:\n\n    while not sv.should_stop():\n        # Get coordinator and run queues to read data\n        coord = tf.train.Coordinator()\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n        start_time = datetime.datetime.now()\n\n        try:\n            while not coord.should_stop():\n                _, loss_value, step = sess.run([train_op, loss, global_step])\n                if step % steps_to_validate == 0:\n                    accuracy_value, auc_value, summary_value = sess.run(\n                        [accuracy, auc_op, summary_op])\n                    end_time = datetime.datetime.now()\n                    print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\n                        end_time - start_time,\n                        FLAGS.task_index,\n                        step, loss_value, accuracy_value,\n                        auc_value))\n\n                    start_time = end_time\n        except tf.errors.OutOfRangeError:\n            print(\"Done training after reading all data\")\n        finally:\n            coord.request_stop()\n            print(\"coord stopped\")\n\n        # Wait for threads to exit\n        coord.join(threads)", "body": "I am training model in distributed mode  with 1 ps and 2 worker all in localhost with different ports.\r\nMy model is a 4 layer with full connected network.\r\nI find that , when traing in 10,10,10,10(4 layer, each layer 10 hidden unit ), every thinks works well,\r\nBut, when enlarge network to 20,20,20,20 , the traing process works well, but can not generate checkponit any more.  with adding log , I find it's stuck at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/saver.py#L1372\r\nit maybe not slow , I wait for hours, nothing but stuck at it.\r\n\r\nmore:\r\nwhen traing not in distributed with same size(20,20,20,20) network,it's works well with checkpoint.\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\ncannot find  any thing about this.\r\n\r\n### Environment info\r\nOperating System:\r\ncentos6.5\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nwith tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n    # Read TFRecords files for training\r\n    filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.train),\r\n        num_epochs=epoch_number)\r\n    serialized_example = read_and_decode(filename_queue)\r\n    batch_serialized_example = tf.train.shuffle_batch(\r\n        [serialized_example],\r\n        batch_size=batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    features = tf.parse_example(\r\n        batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    batch_labels = features[\"label\"]\r\n    batch_ids = features[\"ids\"]\r\n    batch_values = features[\"values\"]\r\n\r\n    # Read TFRecords file for validatioin\r\n    validate_filename_queue = tf.train.string_input_producer(\r\n        tf.train.match_filenames_once(FLAGS.eval),\r\n        num_epochs=epoch_number)\r\n    validate_serialized_example = read_and_decode(validate_filename_queue)\r\n    validate_batch_serialized_example = tf.train.shuffle_batch(\r\n        [validate_serialized_example],\r\n        batch_size=validate_batch_size,\r\n        num_threads=thread_number,\r\n        capacity=capacity,\r\n        min_after_dequeue=min_after_dequeue)\r\n    validate_features = tf.parse_example(\r\n        validate_batch_serialized_example,\r\n        features={\r\n            \"label\": tf.FixedLenFeature([], tf.float32),\r\n            \"ids\": tf.VarLenFeature(tf.int64),\r\n            \"values\": tf.VarLenFeature(tf.float32),\r\n        })\r\n    validate_batch_labels = features[\"label\"]\r\n    validate_batch_ids = features[\"ids\"]\r\n    validate_batch_values = features[\"values\"]\r\n    logits = inference(batch_ids, batch_values)\r\n    batch_labels = tf.to_int64(batch_labels)\r\n    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\r\n                                                                   batch_labels)\r\n    loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\n    print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\r\n\r\n    optimizer = tf.train.FtrlOptimizer(learning_rate)\r\n\r\n    global_step = tf.Variable(0, name='global_step', trainable=False)\r\n    train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n\r\n\r\n\r\n    # Initialize saver and summary\r\n    steps_to_validate = FLAGS.steps_to_validate\r\n    init_op = tf.initialize_all_variables()\r\n\r\n    saver = tf.train.Saver(max_to_keep = 2)\r\n    keys_placeholder = tf.placeholder(\"float\")\r\n    keys = tf.identity(keys_placeholder)\r\n    tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\r\n    tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\r\n                                                'softmax': inference_softmax.name,\r\n                                                'prediction': inference_op.name}))\r\n\r\n    summary_op = tf.merge_all_summaries()\r\n\r\n\r\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                         logdir=\"./supervisor/\",\r\n                         init_op=init_op,\r\n                         summary_op=summary_op,\r\n                         saver=saver,\r\n                         global_step=global_step,\r\n                         save_model_secs=60)\r\n\r\n# Create session to run graph\r\nwith sv.managed_session(server.target) as sess:\r\n\r\n    while not sv.should_stop():\r\n        # Get coordinator and run queues to read data\r\n        coord = tf.train.Coordinator()\r\n        threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n        start_time = datetime.datetime.now()\r\n\r\n        try:\r\n            while not coord.should_stop():\r\n                _, loss_value, step = sess.run([train_op, loss, global_step])\r\n                if step % steps_to_validate == 0:\r\n                    accuracy_value, auc_value, summary_value = sess.run(\r\n                        [accuracy, auc_op, summary_op])\r\n                    end_time = datetime.datetime.now()\r\n                    print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\r\n                        end_time - start_time,\r\n                        FLAGS.task_index,\r\n                        step, loss_value, accuracy_value,\r\n                        auc_value))\r\n\r\n                    start_time = end_time\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done training after reading all data\")\r\n        finally:\r\n            coord.request_stop()\r\n            print(\"coord stopped\")\r\n\r\n        # Wait for threads to exit\r\n        coord.join(threads)\r\n```\r\n"}