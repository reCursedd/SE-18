{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/267624690", "html_url": "https://github.com/tensorflow/tensorflow/issues/6326#issuecomment-267624690", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6326", "id": 267624690, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NzYyNDY5MA==", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-16T15:52:55Z", "updated_at": "2016-12-16T16:00:10Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>  sorry to reopen this issue due to the checkpoint again.<br>\nafter I add sharded=True, the test seems fine with the previous 20,20,20,20 network in single machine with 2ps and 2 worker.</p>\n<p>But when training in multi-host , the master(taskid=0) report:</p>\n<pre><code>Traceback (most recent call last):\n  File \"distributed_deepcake.py\", line 441, in &lt;module&gt;\n    exit(1)\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\n    yield\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\n    self.run_loop()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\n    global_step=self._sv.global_step)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\n    {self.saver_def.filename_tensor_name: checkpoint_file})\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'save/SaveV2_4', defined at:\n  File \"distributed_deepcake.py\", line 293, in &lt;module&gt;\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\n    tensors)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\n    tensors=tensors, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n</code></pre>\n<p>and  the other tasks report</p>\n<pre><code>W tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00005-of-00006.data-00000-of-00001\n</code></pre>\n<p>I have two question:</p>\n<ul>\n<li>why the master task report NotFoundError error?</li>\n<li>why the non-master task need to access the checkpoint folder?</li>\n</ul>\n<p>my code is in the original post.</p>", "body_text": "@yaroslavvb  sorry to reopen this issue due to the checkpoint again.\nafter I add sharded=True, the test seems fine with the previous 20,20,20,20 network in single machine with 2ps and 2 worker.\nBut when training in multi-host , the master(taskid=0) report:\nTraceback (most recent call last):\n  File \"distributed_deepcake.py\", line 441, in <module>\n    exit(1)\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\n    yield\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\n    self.run_loop()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\n    global_step=self._sv.global_step)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\n    {self.saver_def.filename_tensor_name: checkpoint_file})\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'save/SaveV2_4', defined at:\n  File \"distributed_deepcake.py\", line 293, in <module>\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\n    save = self.save_op(filename_tensor, saveables)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\n    tensors)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\n    tensors=tensors, name=name)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\n\nand  the other tasks report\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00005-of-00006.data-00000-of-00001\n\nI have two question:\n\nwhy the master task report NotFoundError error?\nwhy the non-master task need to access the checkpoint folder?\n\nmy code is in the original post.", "body": "@yaroslavvb  sorry to reopen this issue due to the checkpoint again.\r\nafter I add sharded=True, the test seems fine with the previous 20,20,20,20 network in single machine with 2ps and 2 worker.\r\n\r\nBut when training in multi-host , the master(taskid=0) report:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"distributed_deepcake.py\", line 441, in <module>\r\n    exit(1)\r\n  File \"/home/serving/anaconda2/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 974, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 802, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 386, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 296, in stop_on_exception\r\n    yield\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 487, in run\r\n    self.run_loop()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 1069, in run_loop\r\n    global_step=self._sv.global_step)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1323, in save\r\n    {self.saver_def.filename_tensor_name: checkpoint_file})\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n\r\nCaused by op u'save/SaveV2_4', defined at:\r\n  File \"distributed_deepcake.py\", line 293, in <module>\r\n    saver = tf.train.Saver(max_to_keep = 2, sharded=True)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1030, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 618, in build\r\n    save_tensor = self._AddShardedSaveOps(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 314, in _AddShardedSaveOps\r\n    return self._AddShardedSaveOpsForV2(filename_tensor, per_device)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 288, in _AddShardedSaveOpsForV2\r\n    sharded_saves.append(self._AddSaveOps(sharded_filename, saveables))\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 229, in _AddSaveOps\r\n    save = self.save_op(filename_tensor, saveables)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 172, in save_op\r\n    tensors)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 552, in save_v2\r\n    tensors=tensors, name=name)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2240, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1128, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\n         [[Node: save/SaveV2_4 = SaveV2[dtypes=[DT_INT32], _device=\"/job:ps/replica:0/task:2/cpu:0\"](save/ShardedFilename_4, save/SaveV2_4/tensor_names, save/SaveV2_4/shape_and_slices, global_step)]]\r\n         [[Node: save/Identity_S197 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/cpu:0\", send_device=\"/job:ps/replica:0/task:2/cpu:0\", send_device_incarnation=-6527834651342755590, tensor_name=\"edge_107_save/Identity\", tensor_type=DT_STRING, _device=\"/job:worker/replica:0/task:0/cpu:0\"]()]]\r\n```\r\n\r\nand  the other tasks report\r\n```\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00004-of-00006.data-00000-of-00001\r\nW tensorflow/core/framework/op_kernel.cc:975] Not found: ./supervisor/distributed_img0_uid_extra-ftrl-lr0.01-fs30000000-b1000-u63.31.31.15/model.ckpt-0_temp_cde50a9b175b4ca596e757677ce5d86c/part-00005-of-00006.data-00000-of-00001\r\n```\r\n\r\n\r\nI have two question:\r\n* why the master task report NotFoundError error?\r\n* why the non-master task need to access the checkpoint folder?\r\n\r\nmy code is in the original post."}