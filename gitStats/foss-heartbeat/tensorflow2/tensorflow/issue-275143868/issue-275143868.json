{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14701", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14701/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14701/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14701/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14701", "id": 275143868, "node_id": "MDU6SXNzdWUyNzUxNDM4Njg=", "number": 14701, "title": "startup time (_make_train_function()) very slow on Tesla V100-SXM2-16GB GPU, compared to less powerful GPU", "user": {"login": "ophiry", "id": 5228696, "node_id": "MDQ6VXNlcjUyMjg2OTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/5228696?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ophiry", "html_url": "https://github.com/ophiry", "followers_url": "https://api.github.com/users/ophiry/followers", "following_url": "https://api.github.com/users/ophiry/following{/other_user}", "gists_url": "https://api.github.com/users/ophiry/gists{/gist_id}", "starred_url": "https://api.github.com/users/ophiry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ophiry/subscriptions", "organizations_url": "https://api.github.com/users/ophiry/orgs", "repos_url": "https://api.github.com/users/ophiry/repos", "events_url": "https://api.github.com/users/ophiry/events{/privacy}", "received_events_url": "https://api.github.com/users/ophiry/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-11-19T09:02:52Z", "updated_at": "2018-06-12T11:34:28Z", "closed_at": "2017-12-01T15:49:52Z", "author_association": "NONE", "body_html": "<p>cross posted on keras: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"275142632\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/keras-team/keras/issues/8537\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/8537/hovercard\" href=\"https://github.com/keras-team/keras/issues/8537\">keras-team/keras#8537</a></p>\n<p>Running mnist_cnn.py (slightly modified - mainly adding logging) from tensorflow 1.4<br>\nrunning was done using a prebuilt docker image: tensorflow/tensorflow:1.4.0-gpu-py3<br>\non a p2.xlarge aws machine (that has a Tesla K80 GPU) performance is good, the 1st batch (which is dominated by the call to _make_train_function) takes about 2 seconds: (see time stamp for begin batch and end batch)</p>\n<pre><code>2017-11-19 08:26:26,172 : INFO : fit\n\n2017-11-19 08:26:26,637 : INFO : begin batch\n2017-11-19 08:26:26.638409: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-19 08:26:26.760940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-19 08:26:26.761478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:00:1e.0\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\n2017-11-19 08:26:26.761506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n\n2017-11-19 08:26:28,135 : INFO : end batch\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 12s - loss: 0.3526 - acc: 0.8920 - val_loss: 0.0818 - val_acc: 0.9755\nTest loss: 0.081773182778\nTest accuracy: 0.9755\n</code></pre>\n<p>on a p3.2xlarge machine (with a Tesla V100-SXM2-16GB GPU) the same part takes about 10 minutes</p>\n<pre><code>2017-11-19 08:26:44,120 : INFO : fit\n\n2017-11-19 08:26:44,715 : INFO : begin batch\n2017-11-19 08:26:44.716680: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-19 08:26:46.108295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-19 08:26:46.108775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\npciBusID: 0000:00:1e.0\ntotalMemory: 15.77GiB freeMemory: 15.36GiB\n2017-11-19 08:26:46.108815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n\n2017-11-19 08:36:16,552 : INFO : end batch\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 576s - loss: 0.3418 - acc: 0.8949 - val_loss: 0.0769 - val_acc: 0.9772\nTest loss: 0.0769035610346\nTest accuracy: 0.9772\n</code></pre>\n<p>the code that was used:</p>\n<pre><code>#!/usr/bin/env python\n'''Trains a simple convnet on the MNIST dataset.\n\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport cProfile\nimport os\nfrom tensorflow.contrib import keras\nfrom tensorflow.contrib.keras import backend as K\nimport logging\n\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format='\\n%(asctime)s : %(levelname)s : %(message)s')\n\nclass callback(keras.callbacks.Callback):\n    def on_batch_begin(self, batch, logs=None):\n      if batch &lt;= 1:\n            logger.info('begin batch')\n\nclass callback(keras.callbacks.Callback):\n    def on_batch_end(self, batch, logs=None):\n        if batch &lt;= 1:\n            logger.info('end batch')\n\nbatch_size = 128\nnum_classes = 10\nepochs = 1\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(keras.layers.Dropout(0.25))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(128, activation='relu'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nprofiler = cProfile.Profile()\nprofiler.enable()\nlogger.info('fit')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test), callbacks=[callback()])\nprofiler.dump_stats(os.path.expanduser('~/profiler.pstats'))\nscore = model.evaluate(x_test, y_test, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])\n\n</code></pre>", "body_text": "cross posted on keras: keras-team/keras#8537\nRunning mnist_cnn.py (slightly modified - mainly adding logging) from tensorflow 1.4\nrunning was done using a prebuilt docker image: tensorflow/tensorflow:1.4.0-gpu-py3\non a p2.xlarge aws machine (that has a Tesla K80 GPU) performance is good, the 1st batch (which is dominated by the call to _make_train_function) takes about 2 seconds: (see time stamp for begin batch and end batch)\n2017-11-19 08:26:26,172 : INFO : fit\n\n2017-11-19 08:26:26,637 : INFO : begin batch\n2017-11-19 08:26:26.638409: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-19 08:26:26.760940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-19 08:26:26.761478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:00:1e.0\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\n2017-11-19 08:26:26.761506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\n\n2017-11-19 08:26:28,135 : INFO : end batch\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 12s - loss: 0.3526 - acc: 0.8920 - val_loss: 0.0818 - val_acc: 0.9755\nTest loss: 0.081773182778\nTest accuracy: 0.9755\n\non a p3.2xlarge machine (with a Tesla V100-SXM2-16GB GPU) the same part takes about 10 minutes\n2017-11-19 08:26:44,120 : INFO : fit\n\n2017-11-19 08:26:44,715 : INFO : begin batch\n2017-11-19 08:26:44.716680: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-19 08:26:46.108295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-11-19 08:26:46.108775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\npciBusID: 0000:00:1e.0\ntotalMemory: 15.77GiB freeMemory: 15.36GiB\n2017-11-19 08:26:46.108815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\n\n2017-11-19 08:36:16,552 : INFO : end batch\nx_train shape: (60000, 28, 28, 1)\n60000 train samples\n10000 test samples\nTrain on 60000 samples, validate on 10000 samples\nEpoch 1/1\n60000/60000 [==============================] - 576s - loss: 0.3418 - acc: 0.8949 - val_loss: 0.0769 - val_acc: 0.9772\nTest loss: 0.0769035610346\nTest accuracy: 0.9772\n\nthe code that was used:\n#!/usr/bin/env python\n'''Trains a simple convnet on the MNIST dataset.\n\nGets to 99.25% test accuracy after 12 epochs\n(there is still a lot of margin for parameter tuning).\n16 seconds per epoch on a GRID K520 GPU.\n'''\n\nfrom __future__ import print_function\nimport cProfile\nimport os\nfrom tensorflow.contrib import keras\nfrom tensorflow.contrib.keras import backend as K\nimport logging\n\n\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO, format='\\n%(asctime)s : %(levelname)s : %(message)s')\n\nclass callback(keras.callbacks.Callback):\n    def on_batch_begin(self, batch, logs=None):\n      if batch <= 1:\n            logger.info('begin batch')\n\nclass callback(keras.callbacks.Callback):\n    def on_batch_end(self, batch, logs=None):\n        if batch <= 1:\n            logger.info('end batch')\n\nbatch_size = 128\nnum_classes = 10\nepochs = 1\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, shuffled and split between train and test sets\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=input_shape))\nmodel.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\nmodel.add(keras.layers.Dropout(0.25))\nmodel.add(keras.layers.Flatten())\nmodel.add(keras.layers.Dense(128, activation='relu'))\nmodel.add(keras.layers.Dropout(0.5))\nmodel.add(keras.layers.Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\nprofiler = cProfile.Profile()\nprofiler.enable()\nlogger.info('fit')\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test), callbacks=[callback()])\nprofiler.dump_stats(os.path.expanduser('~/profiler.pstats'))\nscore = model.evaluate(x_test, y_test, verbose=0)\n\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])", "body": "cross posted on keras: https://github.com/fchollet/keras/issues/8537\r\n\r\nRunning mnist_cnn.py (slightly modified - mainly adding logging) from tensorflow 1.4\r\nrunning was done using a prebuilt docker image: tensorflow/tensorflow:1.4.0-gpu-py3\r\non a p2.xlarge aws machine (that has a Tesla K80 GPU) performance is good, the 1st batch (which is dominated by the call to _make_train_function) takes about 2 seconds: (see time stamp for begin batch and end batch)\r\n\r\n```\r\n2017-11-19 08:26:26,172 : INFO : fit\r\n\r\n2017-11-19 08:26:26,637 : INFO : begin batch\r\n2017-11-19 08:26:26.638409: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-19 08:26:26.760940: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-19 08:26:26.761478: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2017-11-19 08:26:26.761506: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability: 3.7)\r\n\r\n2017-11-19 08:26:28,135 : INFO : end batch\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/1\r\n60000/60000 [==============================] - 12s - loss: 0.3526 - acc: 0.8920 - val_loss: 0.0818 - val_acc: 0.9755\r\nTest loss: 0.081773182778\r\nTest accuracy: 0.9755\r\n```\r\n\r\non a p3.2xlarge machine (with a Tesla V100-SXM2-16GB GPU) the same part takes about 10 minutes\r\n\r\n```\r\n2017-11-19 08:26:44,120 : INFO : fit\r\n\r\n2017-11-19 08:26:44,715 : INFO : begin batch\r\n2017-11-19 08:26:44.716680: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-19 08:26:46.108295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-11-19 08:26:46.108775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 15.77GiB freeMemory: 15.36GiB\r\n2017-11-19 08:26:46.108815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\n\r\n2017-11-19 08:36:16,552 : INFO : end batch\r\nx_train shape: (60000, 28, 28, 1)\r\n60000 train samples\r\n10000 test samples\r\nTrain on 60000 samples, validate on 10000 samples\r\nEpoch 1/1\r\n60000/60000 [==============================] - 576s - loss: 0.3418 - acc: 0.8949 - val_loss: 0.0769 - val_acc: 0.9772\r\nTest loss: 0.0769035610346\r\nTest accuracy: 0.9772\r\n```\r\n\r\nthe code that was used:\r\n```\r\n#!/usr/bin/env python\r\n'''Trains a simple convnet on the MNIST dataset.\r\n\r\nGets to 99.25% test accuracy after 12 epochs\r\n(there is still a lot of margin for parameter tuning).\r\n16 seconds per epoch on a GRID K520 GPU.\r\n'''\r\n\r\nfrom __future__ import print_function\r\nimport cProfile\r\nimport os\r\nfrom tensorflow.contrib import keras\r\nfrom tensorflow.contrib.keras import backend as K\r\nimport logging\r\n\r\n\r\nlogger = logging.getLogger(__name__)\r\nlogging.basicConfig(level=logging.INFO, format='\\n%(asctime)s : %(levelname)s : %(message)s')\r\n\r\nclass callback(keras.callbacks.Callback):\r\n    def on_batch_begin(self, batch, logs=None):\r\n      if batch <= 1:\r\n            logger.info('begin batch')\r\n\r\nclass callback(keras.callbacks.Callback):\r\n    def on_batch_end(self, batch, logs=None):\r\n        if batch <= 1:\r\n            logger.info('end batch')\r\n\r\nbatch_size = 128\r\nnum_classes = 10\r\nepochs = 1\r\n\r\n# input image dimensions\r\nimg_rows, img_cols = 28, 28\r\n\r\n# the data, shuffled and split between train and test sets\r\n(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\r\n\r\nif K.image_data_format() == 'channels_first':\r\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n    input_shape = (1, img_rows, img_cols)\r\nelse:\r\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n    input_shape = (img_rows, img_cols, 1)\r\n\r\nx_train = x_train.astype('float32')\r\nx_test = x_test.astype('float32')\r\nx_train /= 255\r\nx_test /= 255\r\nprint('x_train shape:', x_train.shape)\r\nprint(x_train.shape[0], 'train samples')\r\nprint(x_test.shape[0], 'test samples')\r\n\r\n# convert class vectors to binary class matrices\r\ny_train = keras.utils.to_categorical(y_train, num_classes)\r\ny_test = keras.utils.to_categorical(y_test, num_classes)\r\n\r\nmodel = keras.models.Sequential()\r\nmodel.add(keras.layers.Conv2D(32, kernel_size=(3, 3),\r\n                 activation='relu',\r\n                 input_shape=input_shape))\r\nmodel.add(keras.layers.Conv2D(64, (3, 3), activation='relu'))\r\nmodel.add(keras.layers.MaxPooling2D(pool_size=(2, 2)))\r\nmodel.add(keras.layers.Dropout(0.25))\r\nmodel.add(keras.layers.Flatten())\r\nmodel.add(keras.layers.Dense(128, activation='relu'))\r\nmodel.add(keras.layers.Dropout(0.5))\r\nmodel.add(keras.layers.Dense(num_classes, activation='softmax'))\r\n\r\nmodel.compile(loss=keras.losses.categorical_crossentropy,\r\n              optimizer=keras.optimizers.Adadelta(),\r\n              metrics=['accuracy'])\r\nprofiler = cProfile.Profile()\r\nprofiler.enable()\r\nlogger.info('fit')\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          verbose=1,\r\n          validation_data=(x_test, y_test), callbacks=[callback()])\r\nprofiler.dump_stats(os.path.expanduser('~/profiler.pstats'))\r\nscore = model.evaluate(x_test, y_test, verbose=0)\r\n\r\nprint('Test loss:', score[0])\r\nprint('Test accuracy:', score[1])\r\n\r\n```\r\n"}