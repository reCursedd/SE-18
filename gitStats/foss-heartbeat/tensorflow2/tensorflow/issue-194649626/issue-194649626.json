{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6216", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6216/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6216/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6216/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6216", "id": 194649626, "node_id": "MDU6SXNzdWUxOTQ2NDk2MjY=", "number": 6216, "title": "task assignment in tensorflow distributed process", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-12-09T17:15:25Z", "updated_at": "2016-12-09T19:18:16Z", "closed_at": "2016-12-09T19:18:16Z", "author_association": "NONE", "body_html": "<p>I'm confused about the distributed training process in tensorflow.<br>\nI think the tensorflow feed a batch_size of data to a worker and then the worker update the ps server,is this right?<br>\nBut when training , I noticed that the step number in the log may strange.<br>\nIf I have only 2 workers , I thinks the right process should be some thing like<br>\n[worker1] step 0 xxxxxxx<br>\n[worker2] step 100 xxxxxxx<br>\n[worker1] step 200 xxxxxxx<br>\n[worker2] step 300 xxxxxxx<br>\n.....<br>\nevery worker should print different step to log.</p>\n<p>Actually , the log are as below:<br>\n[worker1] step 0 xxxxxxx<br>\n[worker2] step 100 xxxxxxx<br>\n[worker1] step 100 xxxxxxx<br>\n[worker2] step 200 xxxxxxx<br>\n[worker1] step 300 xxxxxxx<br>\n...<br>\nWhy the worker1 dosn't print step 200?</p>\n<p>I am confused about the job assign .... a step to a worker? or every worker run every step?</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>there are nothing about the tensorflow distributed process in these websites</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>def main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Variables of the hidden layer\n      hid_w = tf.Variable(\n          tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n                              stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\n\n      # Variables of the softmax layer\n      sm_w = tf.Variable(\n          tf.truncated_normal([FLAGS.hidden_units, 10],\n                              stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\n          name=\"sm_w\")\n      sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n\n      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n      y_ = tf.placeholder(tf.float32, [None, 10])\n\n      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n      hid = tf.nn.relu(hid_lin)\n\n      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n      loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"/tmp/train_logs\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # The supervisor takes care of session initialization, restoring from\n    # a checkpoint, and closing when done or an error occurs.\n    with sv.managed_session(server.target) as sess:\n      # Loop until the supervisor shuts down or 1000000 steps have completed.\n      step = 0\n      while not sv.should_stop() and step &lt; 1000000:\n        # Run a training step asynchronously.\n        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n        # perform *synchronous* training.\n\n        batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n        train_feed = {x: batch_xs, y_: batch_ys}\n\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n        if step % 100 == 0: \n            print \"Done step %d\" % step\n\n    # Ask for all the services to stop.\n    sv.stop()\n</code></pre>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>\n<pre><code>[0:00:17.115814] Task: 0, Step: 74600, loss: 0.303285002708, accuracy: 0.910000026226, auc: 0.946377456188\n[0:00:03.804889] Task: 1, Step: 74700, loss: 0.287385582924, accuracy: 0.879999995232, auc: 0.946395516396\n[0:00:03.778589] Task: 0, Step: 74800, loss: 0.247096762061, accuracy: 0.860000014305, auc: 0.946370542049\n[0:00:03.772320] Task: 1, Step: 74900, loss: 0.264987647533, accuracy: 0.899999976158, auc: 0.946406364441\n[0:00:03.795459] Task: 0, Step: 75000, loss: 0.228719010949, accuracy: 0.899999976158, auc: 0.946437120438\n[0:00:01.902293] Task: 1, Step: 75000, loss: 0.217391207814, accuracy: 0.910000026226, auc: 0.946473121643\n[0:00:01.942055] Task: 1, Step: 75100, loss: 0.284583866596, accuracy: 0.889999985695, auc: 0.946496844292\n[0:00:03.860608] Task: 0, Step: 75200, loss: 0.273199081421, accuracy: 0.850000023842, auc: 0.946503221989\n[0:00:03.800881] Task: 1, Step: 75300, loss: 0.189931258559, accuracy: 0.930000007153, auc: 0.946559965611\n</code></pre>", "body_text": "I'm confused about the distributed training process in tensorflow.\nI think the tensorflow feed a batch_size of data to a worker and then the worker update the ps server,is this right?\nBut when training , I noticed that the step number in the log may strange.\nIf I have only 2 workers , I thinks the right process should be some thing like\n[worker1] step 0 xxxxxxx\n[worker2] step 100 xxxxxxx\n[worker1] step 200 xxxxxxx\n[worker2] step 300 xxxxxxx\n.....\nevery worker should print different step to log.\nActually , the log are as below:\n[worker1] step 0 xxxxxxx\n[worker2] step 100 xxxxxxx\n[worker1] step 100 xxxxxxx\n[worker2] step 200 xxxxxxx\n[worker1] step 300 xxxxxxx\n...\nWhy the worker1 dosn't print step 200?\nI am confused about the job assign .... a step to a worker? or every worker run every step?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nthere are nothing about the tensorflow distributed process in these websites\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\ndef main(_):\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n  # Create a cluster from the parameter server and worker hosts.\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n  # Create and start a server for the local task.\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n\n      # Variables of the hidden layer\n      hid_w = tf.Variable(\n          tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\n                              stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\n      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\n\n      # Variables of the softmax layer\n      sm_w = tf.Variable(\n          tf.truncated_normal([FLAGS.hidden_units, 10],\n                              stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\n          name=\"sm_w\")\n      sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\n\n      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n      y_ = tf.placeholder(tf.float32, [None, 10])\n\n      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n      hid = tf.nn.relu(hid_lin)\n\n      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n      loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n      global_step = tf.Variable(0)\n\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\n          loss, global_step=global_step)\n\n      saver = tf.train.Saver()\n      summary_op = tf.merge_all_summaries()\n      init_op = tf.initialize_all_variables()\n\n    # Create a \"supervisor\", which oversees the training process.\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"/tmp/train_logs\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=600)\n\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\n\n    # The supervisor takes care of session initialization, restoring from\n    # a checkpoint, and closing when done or an error occurs.\n    with sv.managed_session(server.target) as sess:\n      # Loop until the supervisor shuts down or 1000000 steps have completed.\n      step = 0\n      while not sv.should_stop() and step < 1000000:\n        # Run a training step asynchronously.\n        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\n        # perform *synchronous* training.\n\n        batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\n        train_feed = {x: batch_xs, y_: batch_ys}\n\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\n        if step % 100 == 0: \n            print \"Done step %d\" % step\n\n    # Ask for all the services to stop.\n    sv.stop()\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\n[0:00:17.115814] Task: 0, Step: 74600, loss: 0.303285002708, accuracy: 0.910000026226, auc: 0.946377456188\n[0:00:03.804889] Task: 1, Step: 74700, loss: 0.287385582924, accuracy: 0.879999995232, auc: 0.946395516396\n[0:00:03.778589] Task: 0, Step: 74800, loss: 0.247096762061, accuracy: 0.860000014305, auc: 0.946370542049\n[0:00:03.772320] Task: 1, Step: 74900, loss: 0.264987647533, accuracy: 0.899999976158, auc: 0.946406364441\n[0:00:03.795459] Task: 0, Step: 75000, loss: 0.228719010949, accuracy: 0.899999976158, auc: 0.946437120438\n[0:00:01.902293] Task: 1, Step: 75000, loss: 0.217391207814, accuracy: 0.910000026226, auc: 0.946473121643\n[0:00:01.942055] Task: 1, Step: 75100, loss: 0.284583866596, accuracy: 0.889999985695, auc: 0.946496844292\n[0:00:03.860608] Task: 0, Step: 75200, loss: 0.273199081421, accuracy: 0.850000023842, auc: 0.946503221989\n[0:00:03.800881] Task: 1, Step: 75300, loss: 0.189931258559, accuracy: 0.930000007153, auc: 0.946559965611", "body": "I'm confused about the distributed training process in tensorflow.\r\nI think the tensorflow feed a batch_size of data to a worker and then the worker update the ps server,is this right? \r\nBut when training , I noticed that the step number in the log may strange.\r\nIf I have only 2 workers , I thinks the right process should be some thing like\r\n[worker1] step 0 xxxxxxx\r\n[worker2] step 100 xxxxxxx\r\n[worker1] step 200 xxxxxxx\r\n[worker2] step 300 xxxxxxx\r\n.....\r\nevery worker should print different step to log.\r\n\r\nActually , the log are as below:\r\n[worker1] step 0 xxxxxxx\r\n[worker2] step 100 xxxxxxx\r\n[worker1] step 100 xxxxxxx\r\n[worker2] step 200 xxxxxxx\r\n[worker1] step 300 xxxxxxx\r\n...\r\nWhy the worker1 dosn't print step 200? \r\n\r\nI am confused about the job assign .... a step to a worker? or every worker run every step?\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nthere are nothing about the tensorflow distributed process in these websites\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\ndef main(_):\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n  # Create a cluster from the parameter server and worker hosts.\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n  # Create and start a server for the local task.\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n\r\n    # Assigns ops to the local worker by default.\r\n    with tf.device(tf.train.replica_device_setter(\r\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n        cluster=cluster)):\r\n\r\n      # Variables of the hidden layer\r\n      hid_w = tf.Variable(\r\n          tf.truncated_normal([IMAGE_PIXELS * IMAGE_PIXELS, FLAGS.hidden_units],\r\n                              stddev=1.0 / IMAGE_PIXELS), name=\"hid_w\")\r\n      hid_b = tf.Variable(tf.zeros([FLAGS.hidden_units]), name=\"hid_b\")\r\n\r\n      # Variables of the softmax layer\r\n      sm_w = tf.Variable(\r\n          tf.truncated_normal([FLAGS.hidden_units, 10],\r\n                              stddev=1.0 / math.sqrt(FLAGS.hidden_units)),\r\n          name=\"sm_w\")\r\n      sm_b = tf.Variable(tf.zeros([10]), name=\"sm_b\")\r\n\r\n      x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\r\n      y_ = tf.placeholder(tf.float32, [None, 10])\r\n\r\n      hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\r\n      hid = tf.nn.relu(hid_lin)\r\n\r\n      y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\r\n      loss = -tf.reduce_sum(y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\r\n\r\n      global_step = tf.Variable(0)\r\n\r\n      train_op = tf.train.AdagradOptimizer(0.01).minimize(\r\n          loss, global_step=global_step)\r\n\r\n      saver = tf.train.Saver()\r\n      summary_op = tf.merge_all_summaries()\r\n      init_op = tf.initialize_all_variables()\r\n\r\n    # Create a \"supervisor\", which oversees the training process.\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                             logdir=\"/tmp/train_logs\",\r\n                             init_op=init_op,\r\n                             summary_op=summary_op,\r\n                             saver=saver,\r\n                             global_step=global_step,\r\n                             save_model_secs=600)\r\n\r\n    mnist = input_data.read_data_sets(FLAGS.data_dir, one_hot=True)\r\n\r\n    # The supervisor takes care of session initialization, restoring from\r\n    # a checkpoint, and closing when done or an error occurs.\r\n    with sv.managed_session(server.target) as sess:\r\n      # Loop until the supervisor shuts down or 1000000 steps have completed.\r\n      step = 0\r\n      while not sv.should_stop() and step < 1000000:\r\n        # Run a training step asynchronously.\r\n        # See `tf.train.SyncReplicasOptimizer` for additional details on how to\r\n        # perform *synchronous* training.\r\n\r\n        batch_xs, batch_ys = mnist.train.next_batch(FLAGS.batch_size)\r\n        train_feed = {x: batch_xs, y_: batch_ys}\r\n\r\n        _, step = sess.run([train_op, global_step], feed_dict=train_feed)\r\n        if step % 100 == 0: \r\n            print \"Done step %d\" % step\r\n\r\n    # Ask for all the services to stop.\r\n    sv.stop()\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n```\r\n[0:00:17.115814] Task: 0, Step: 74600, loss: 0.303285002708, accuracy: 0.910000026226, auc: 0.946377456188\r\n[0:00:03.804889] Task: 1, Step: 74700, loss: 0.287385582924, accuracy: 0.879999995232, auc: 0.946395516396\r\n[0:00:03.778589] Task: 0, Step: 74800, loss: 0.247096762061, accuracy: 0.860000014305, auc: 0.946370542049\r\n[0:00:03.772320] Task: 1, Step: 74900, loss: 0.264987647533, accuracy: 0.899999976158, auc: 0.946406364441\r\n[0:00:03.795459] Task: 0, Step: 75000, loss: 0.228719010949, accuracy: 0.899999976158, auc: 0.946437120438\r\n[0:00:01.902293] Task: 1, Step: 75000, loss: 0.217391207814, accuracy: 0.910000026226, auc: 0.946473121643\r\n[0:00:01.942055] Task: 1, Step: 75100, loss: 0.284583866596, accuracy: 0.889999985695, auc: 0.946496844292\r\n[0:00:03.860608] Task: 0, Step: 75200, loss: 0.273199081421, accuracy: 0.850000023842, auc: 0.946503221989\r\n[0:00:03.800881] Task: 1, Step: 75300, loss: 0.189931258559, accuracy: 0.930000007153, auc: 0.946559965611\r\n```"}