{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251286787", "html_url": "https://github.com/tensorflow/tensorflow/issues/4620#issuecomment-251286787", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4620", "id": 251286787, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTI4Njc4Nw==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-04T03:26:59Z", "updated_at": "2016-10-04T03:26:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>United memory is great for minimizing effort to port existing CPU application to GPU. This could change in the future, but it doesn't seem to offer much performance benefit for frameworks like TensorFlow that are optimized for GPU. Since all the buffers are in forms of explicitly managed tensors, we can already reap most of the sharing between CPU and GPU through pinned memory, if necessary.</p>\n<p>Native support for FP16 had been targeted extensively. More support for INT8 is going to be added as well.</p>\n<p>There are also a lot of interest for high-bandwidth inter-GPU communication primitives. They are likely be to be targeted as well.</p>", "body_text": "United memory is great for minimizing effort to port existing CPU application to GPU. This could change in the future, but it doesn't seem to offer much performance benefit for frameworks like TensorFlow that are optimized for GPU. Since all the buffers are in forms of explicitly managed tensors, we can already reap most of the sharing between CPU and GPU through pinned memory, if necessary.\nNative support for FP16 had been targeted extensively. More support for INT8 is going to be added as well.\nThere are also a lot of interest for high-bandwidth inter-GPU communication primitives. They are likely be to be targeted as well.", "body": "United memory is great for minimizing effort to port existing CPU application to GPU. This could change in the future, but it doesn't seem to offer much performance benefit for frameworks like TensorFlow that are optimized for GPU. Since all the buffers are in forms of explicitly managed tensors, we can already reap most of the sharing between CPU and GPU through pinned memory, if necessary.\n\nNative support for FP16 had been targeted extensively. More support for INT8 is going to be added as well. \n\nThere are also a lot of interest for high-bandwidth inter-GPU communication primitives. They are likely be to be targeted as well. \n"}