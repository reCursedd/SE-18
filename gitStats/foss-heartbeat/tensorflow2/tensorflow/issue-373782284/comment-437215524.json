{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437215524", "html_url": "https://github.com/tensorflow/tensorflow/pull/23244#issuecomment-437215524", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23244", "id": 437215524, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzIxNTUyNA==", "user": {"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-09T01:17:30Z", "updated_at": "2018-11-09T01:17:30Z", "author_association": "MEMBER", "body_html": "<p>Thanks for your response.</p>\n<ul>\n<li>In terms of the parity between 'zero grad' and 'no grad update' -- <code>tf.gradients()</code> is a util function and thus should faithfully return the gradient result, and make no assumptions about how client would use the result. How to use the gradient if NaN/Inf, should stay in client code.</li>\n<li>In terms of the ease of use for <code>tf.gradients()</code> vs <code>opt.compute_gradient()</code>: maybe, but I don't find it very  concerning. I'm afraid that's not strong enough to justify to zero-out gradients in <code>tf.gradient()</code>.</li>\n<li>For \"compatibility problem of SyncReplicasOptimzier\":\n<ul>\n<li>(Sorry, but) this optimizer is depreciated.</li>\n<li>NV suggests just incr the global step even if you get a NaN/Inf grad, just don't update the variables. We did a series of exp and we found that it working.</li>\n</ul>\n</li>\n<li>For \"colocation constraints\": I think it's not fundamental limit. The <code>LossScaleOptimizer' can just do the extra work </code>gradients_with_scaling()` did.</li>\n</ul>\n<p>In summary I think <code>tf.gradients()</code> should return gradients as they are, and leave proper handling to client code. We probably couldn't merge this decorator in official TF core, but you can have a library for your production environment, and override the tf implementation when you call it.</p>", "body_text": "Thanks for your response.\n\nIn terms of the parity between 'zero grad' and 'no grad update' -- tf.gradients() is a util function and thus should faithfully return the gradient result, and make no assumptions about how client would use the result. How to use the gradient if NaN/Inf, should stay in client code.\nIn terms of the ease of use for tf.gradients() vs opt.compute_gradient(): maybe, but I don't find it very  concerning. I'm afraid that's not strong enough to justify to zero-out gradients in tf.gradient().\nFor \"compatibility problem of SyncReplicasOptimzier\":\n\n(Sorry, but) this optimizer is depreciated.\nNV suggests just incr the global step even if you get a NaN/Inf grad, just don't update the variables. We did a series of exp and we found that it working.\n\n\nFor \"colocation constraints\": I think it's not fundamental limit. The LossScaleOptimizer' can just do the extra work gradients_with_scaling()` did.\n\nIn summary I think tf.gradients() should return gradients as they are, and leave proper handling to client code. We probably couldn't merge this decorator in official TF core, but you can have a library for your production environment, and override the tf implementation when you call it.", "body": "Thanks for your response.\r\n\r\n* In terms of the parity between 'zero grad' and 'no grad update' -- `tf.gradients()` is a util function and thus should faithfully return the gradient result, and make no assumptions about how client would use the result. How to use the gradient if NaN/Inf, should stay in client code. \r\n* In terms of the ease of use for `tf.gradients()` vs `opt.compute_gradient()`: maybe, but I don't find it very  concerning. I'm afraid that's not strong enough to justify to zero-out gradients in `tf.gradient()`.\r\n* For \"compatibility problem of SyncReplicasOptimzier\":\r\n  * (Sorry, but) this optimizer is depreciated.\r\n  * NV suggests just incr the global step even if you get a NaN/Inf grad, just don't update the variables. We did a series of exp and we found that it working.\r\n* For \"colocation constraints\": I think it's not fundamental limit. The `LossScaleOptimizer' can just do the extra work `gradients_with_scaling()` did.\r\n\r\nIn summary I think `tf.gradients()` should return gradients as they are, and leave proper handling to client code. We probably couldn't merge this decorator in official TF core, but you can have a library for your production environment, and override the tf implementation when you call it."}