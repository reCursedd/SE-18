{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/434555446", "html_url": "https://github.com/tensorflow/tensorflow/pull/23244#issuecomment-434555446", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23244", "id": 434555446, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDU1NTQ0Ng==", "user": {"login": "Dido0o0", "id": 16295526, "node_id": "MDQ6VXNlcjE2Mjk1NTI2", "avatar_url": "https://avatars2.githubusercontent.com/u/16295526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dido0o0", "html_url": "https://github.com/Dido0o0", "followers_url": "https://api.github.com/users/Dido0o0/followers", "following_url": "https://api.github.com/users/Dido0o0/following{/other_user}", "gists_url": "https://api.github.com/users/Dido0o0/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dido0o0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dido0o0/subscriptions", "organizations_url": "https://api.github.com/users/Dido0o0/orgs", "repos_url": "https://api.github.com/users/Dido0o0/repos", "events_url": "https://api.github.com/users/Dido0o0/events{/privacy}", "received_events_url": "https://api.github.com/users/Dido0o0/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T04:16:28Z", "updated_at": "2018-10-31T04:16:28Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5117188\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/protoget\">@protoget</a> Thank you for your comments.<br>\nAs for the issue  \"applying zero gradients\" versus \"not applying gradient update\": the case when there is overflow appears is actually in quite low-frequency during the training procedure. In a certain training case, the frequency is approximately 4/10k, which contributes minor impact to the training process. Additionally, we conducted experiments on certain models, e.g. on Faster RCNN with ADAM optimizer, the result is mAP=69.78 (applying zero gradients) versus mAP=69.68 (not applying gradients update). The result is a little bit better. Of course we can say that it is because of random experiments noise, however it can demonstrate that the difference is ignorable.</p>\n<p>As for the points you pointed out:</p>\n<ul>\n<li>\n<p>If the users are using <code>tf.gradients()</code> to compute the gradients rather than <code>optimizer.compute_gradients()</code>/<code>optimizer.minimize()</code>, the wrapper is no longer in user.<br>\nCould you give an example where <code>tf.gradients()</code> couldn't be replaced by <code>optimizer.compute_gradients()</code>?<br>\nThis point is mainly for ease of use. Although, the usage of <code>tf.gradients()</code> can be replaced by <code>optimzier.compute_gradients()</code>, the users need to change more code to take use of <code>LossScaleOptimzier</code>. It may introduce more overhead for the users.</p>\n</li>\n<li>\n<p>The LossScaleOptimizer wrapper may have compatibility problem with other optimizer wrappers, e.g. SyncReplicasOptimizer<br>\nWe have to check on that. Most likely we'd fix that single optimizer if that's the case. Do you have an example?<br>\nWe have encountered this issue in our in-house model, when using LossScaleOptimizer and SyncReplicasOptimizer. When there is overflow, the apply_gradients will apply no_op, as a result, global_step will not be updated. SyncReplicasOptimizer is synchronized using a token related to global_step. The correlation will trigger hang, when using these two optimizer. Maybe there will be more similar cases for LossScaleOptimizer to be compatible with other optimizers. This is another reason that we choose to apply zeros gradients for overflow case in <code>tf.gradients()</code>. The decorator will make <code>tf.gradients</code> seems to have totally no difference with/without mixed precision training.</p>\n</li>\n<li>\n<p>The current implementation in LossScaleOptimizer did not take the colocation parameter into consideration.<br>\nOne can use <code>optimizer.compute_gradients()</code> and <code>optimizer.apply_gradients()</code> separately. The first API allows colocation options the same way <code>tf.gradients()</code> does. LossScaleOptimizer allows split <code>minimize()</code> into <code>compute_gradients()</code> and <code>apply_gradients()</code>.<br>\nIn loss scale algorithm, we can separate the gradients computation into three parts: 1. scale the loss; 2. backpropagation computation using standard <code>tf.gradients()</code> or compute_gradients(); 3. unscale the grads. What you point out that \"<code>optimizer.compute_gradients()</code>  allows colocation options the same way <code>tf.gradients()</code> does\" is actually referring the second part. What we would like to point out is the first and third step. The current <code>LossScaleOptimizer</code> did not deal with the colocation issue for the 1st and 3rd step. The colocation issue is quite important in certain cases, such as in distribution training.  We have already take it into consideration in the decorator implementation.</p>\n</li>\n</ul>\n<p>Thank you again to review our work in detail with great effort, the above is our consideration to the points you pointed out.</p>", "body_text": "@protoget Thank you for your comments.\nAs for the issue  \"applying zero gradients\" versus \"not applying gradient update\": the case when there is overflow appears is actually in quite low-frequency during the training procedure. In a certain training case, the frequency is approximately 4/10k, which contributes minor impact to the training process. Additionally, we conducted experiments on certain models, e.g. on Faster RCNN with ADAM optimizer, the result is mAP=69.78 (applying zero gradients) versus mAP=69.68 (not applying gradients update). The result is a little bit better. Of course we can say that it is because of random experiments noise, however it can demonstrate that the difference is ignorable.\nAs for the points you pointed out:\n\n\nIf the users are using tf.gradients() to compute the gradients rather than optimizer.compute_gradients()/optimizer.minimize(), the wrapper is no longer in user.\nCould you give an example where tf.gradients() couldn't be replaced by optimizer.compute_gradients()?\nThis point is mainly for ease of use. Although, the usage of tf.gradients() can be replaced by optimzier.compute_gradients(), the users need to change more code to take use of LossScaleOptimzier. It may introduce more overhead for the users.\n\n\nThe LossScaleOptimizer wrapper may have compatibility problem with other optimizer wrappers, e.g. SyncReplicasOptimizer\nWe have to check on that. Most likely we'd fix that single optimizer if that's the case. Do you have an example?\nWe have encountered this issue in our in-house model, when using LossScaleOptimizer and SyncReplicasOptimizer. When there is overflow, the apply_gradients will apply no_op, as a result, global_step will not be updated. SyncReplicasOptimizer is synchronized using a token related to global_step. The correlation will trigger hang, when using these two optimizer. Maybe there will be more similar cases for LossScaleOptimizer to be compatible with other optimizers. This is another reason that we choose to apply zeros gradients for overflow case in tf.gradients(). The decorator will make tf.gradients seems to have totally no difference with/without mixed precision training.\n\n\nThe current implementation in LossScaleOptimizer did not take the colocation parameter into consideration.\nOne can use optimizer.compute_gradients() and optimizer.apply_gradients() separately. The first API allows colocation options the same way tf.gradients() does. LossScaleOptimizer allows split minimize() into compute_gradients() and apply_gradients().\nIn loss scale algorithm, we can separate the gradients computation into three parts: 1. scale the loss; 2. backpropagation computation using standard tf.gradients() or compute_gradients(); 3. unscale the grads. What you point out that \"optimizer.compute_gradients()  allows colocation options the same way tf.gradients() does\" is actually referring the second part. What we would like to point out is the first and third step. The current LossScaleOptimizer did not deal with the colocation issue for the 1st and 3rd step. The colocation issue is quite important in certain cases, such as in distribution training.  We have already take it into consideration in the decorator implementation.\n\n\nThank you again to review our work in detail with great effort, the above is our consideration to the points you pointed out.", "body": "@protoget Thank you for your comments.\r\nAs for the issue  \"applying zero gradients\" versus \"not applying gradient update\": the case when there is overflow appears is actually in quite low-frequency during the training procedure. In a certain training case, the frequency is approximately 4/10k, which contributes minor impact to the training process. Additionally, we conducted experiments on certain models, e.g. on Faster RCNN with ADAM optimizer, the result is mAP=69.78 (applying zero gradients) versus mAP=69.68 (not applying gradients update). The result is a little bit better. Of course we can say that it is because of random experiments noise, however it can demonstrate that the difference is ignorable.\r\n\r\nAs for the points you pointed out:\r\n+ If the users are using `tf.gradients()` to compute the gradients rather than `optimizer.compute_gradients()`/`optimizer.minimize()`, the wrapper is no longer in user.\r\nCould you give an example where `tf.gradients()` couldn't be replaced by `optimizer.compute_gradients()`?\r\nThis point is mainly for ease of use. Although, the usage of `tf.gradients()` can be replaced by `optimzier.compute_gradients()`, the users need to change more code to take use of `LossScaleOptimzier`. It may introduce more overhead for the users.\r\n\r\n+ The LossScaleOptimizer wrapper may have compatibility problem with other optimizer wrappers, e.g. SyncReplicasOptimizer\r\nWe have to check on that. Most likely we'd fix that single optimizer if that's the case. Do you have an example?\r\nWe have encountered this issue in our in-house model, when using LossScaleOptimizer and SyncReplicasOptimizer. When there is overflow, the apply_gradients will apply no_op, as a result, global_step will not be updated. SyncReplicasOptimizer is synchronized using a token related to global_step. The correlation will trigger hang, when using these two optimizer. Maybe there will be more similar cases for LossScaleOptimizer to be compatible with other optimizers. This is another reason that we choose to apply zeros gradients for overflow case in `tf.gradients()`. The decorator will make `tf.gradients` seems to have totally no difference with/without mixed precision training.\r\n\r\n+ The current implementation in LossScaleOptimizer did not take the colocation parameter into consideration.\r\nOne can use `optimizer.compute_gradients()` and `optimizer.apply_gradients()` separately. The first API allows colocation options the same way `tf.gradients()` does. LossScaleOptimizer allows split `minimize()` into `compute_gradients()` and `apply_gradients()`.\r\nIn loss scale algorithm, we can separate the gradients computation into three parts: 1. scale the loss; 2. backpropagation computation using standard `tf.gradients()` or compute_gradients(); 3. unscale the grads. What you point out that \"`optimizer.compute_gradients()`  allows colocation options the same way `tf.gradients()` does\" is actually referring the second part. What we would like to point out is the first and third step. The current `LossScaleOptimizer` did not deal with the colocation issue for the 1st and 3rd step. The colocation issue is quite important in certain cases, such as in distribution training.  We have already take it into consideration in the decorator implementation.\r\n\r\nThank you again to review our work in detail with great effort, the above is our consideration to the points you pointed out. "}