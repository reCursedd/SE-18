{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407272631", "html_url": "https://github.com/tensorflow/tensorflow/issues/20509#issuecomment-407272631", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20509", "id": 407272631, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzI3MjYzMQ==", "user": {"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-24T03:52:39Z", "updated_at": "2018-07-24T03:52:39Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1595907\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/carlthome\">@carlthome</a> thanks for the code and stack trace. I think what's happening is that as part of <code>train_and_evaluate</code>, evaluate gets called in a hook (_NewCheckpointListenerForEvaluate) as part of the training call. This means that the distribution strategy scope that we open in estimator.train (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1194\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1194</a>)  is still open when we execute this hook.<br>\nDue to this, the metrics that you create in evaluate are created inside the scope in a cross tower context - and this leads to the error above.<br>\nThis setup to use _NewCheckpointListenerForEvaluate is new and hence you're probably one of the first people to run into this.</p>\n<p>I see 2 options to move forward:</p>\n<ol>\n<li>Can you use Estimator.train and Estimator.evaluate directly instead of train_and_evaluate? We've definitely tested those to work as expected in presence of MirroredStrategy (even though evaluate itself is not distributed). The main benefit of using train_and_evaluate (from my understanding) is to run on multiple machines with Parameter Server distribution. Since you are not looking for that, directly using train and evaluate might be better and get around this error.</li>\n<li>As I mentioned above, Estimator.evaluate itself in general is not yet distributed under MirroredStrategy so currently it would just run on the first GPU (but should not throw errors). This support is actually in progress right now and should be available in 1-2 weeks. I suspect this might address the issue while using train_and_evaluate as well but I haven't tested that (will do).</li>\n</ol>\n<p>Hope this helps!</p>", "body_text": "@carlthome thanks for the code and stack trace. I think what's happening is that as part of train_and_evaluate, evaluate gets called in a hook (_NewCheckpointListenerForEvaluate) as part of the training call. This means that the distribution strategy scope that we open in estimator.train (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1194)  is still open when we execute this hook.\nDue to this, the metrics that you create in evaluate are created inside the scope in a cross tower context - and this leads to the error above.\nThis setup to use _NewCheckpointListenerForEvaluate is new and hence you're probably one of the first people to run into this.\nI see 2 options to move forward:\n\nCan you use Estimator.train and Estimator.evaluate directly instead of train_and_evaluate? We've definitely tested those to work as expected in presence of MirroredStrategy (even though evaluate itself is not distributed). The main benefit of using train_and_evaluate (from my understanding) is to run on multiple machines with Parameter Server distribution. Since you are not looking for that, directly using train and evaluate might be better and get around this error.\nAs I mentioned above, Estimator.evaluate itself in general is not yet distributed under MirroredStrategy so currently it would just run on the first GPU (but should not throw errors). This support is actually in progress right now and should be available in 1-2 weeks. I suspect this might address the issue while using train_and_evaluate as well but I haven't tested that (will do).\n\nHope this helps!", "body": "@carlthome thanks for the code and stack trace. I think what's happening is that as part of `train_and_evaluate`, evaluate gets called in a hook (_NewCheckpointListenerForEvaluate) as part of the training call. This means that the distribution strategy scope that we open in estimator.train (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L1194)  is still open when we execute this hook.  \r\nDue to this, the metrics that you create in evaluate are created inside the scope in a cross tower context - and this leads to the error above. \r\nThis setup to use _NewCheckpointListenerForEvaluate is new and hence you're probably one of the first people to run into this. \r\n\r\nI see 2 options to move forward:\r\n1. Can you use Estimator.train and Estimator.evaluate directly instead of train_and_evaluate? We've definitely tested those to work as expected in presence of MirroredStrategy (even though evaluate itself is not distributed). The main benefit of using train_and_evaluate (from my understanding) is to run on multiple machines with Parameter Server distribution. Since you are not looking for that, directly using train and evaluate might be better and get around this error. \r\n2. As I mentioned above, Estimator.evaluate itself in general is not yet distributed under MirroredStrategy so currently it would just run on the first GPU (but should not throw errors). This support is actually in progress right now and should be available in 1-2 weeks. I suspect this might address the issue while using train_and_evaluate as well but I haven't tested that (will do). \r\n\r\nHope this helps! \r\n\r\n\r\n\r\n"}