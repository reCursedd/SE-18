{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/340449793", "html_url": "https://github.com/tensorflow/tensorflow/issues/14093#issuecomment-340449793", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14093", "id": 340449793, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MDQ0OTc5Mw==", "user": {"login": "MounirHader", "id": 15324040, "node_id": "MDQ6VXNlcjE1MzI0MDQw", "avatar_url": "https://avatars0.githubusercontent.com/u/15324040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MounirHader", "html_url": "https://github.com/MounirHader", "followers_url": "https://api.github.com/users/MounirHader/followers", "following_url": "https://api.github.com/users/MounirHader/following{/other_user}", "gists_url": "https://api.github.com/users/MounirHader/gists{/gist_id}", "starred_url": "https://api.github.com/users/MounirHader/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MounirHader/subscriptions", "organizations_url": "https://api.github.com/users/MounirHader/orgs", "repos_url": "https://api.github.com/users/MounirHader/repos", "events_url": "https://api.github.com/users/MounirHader/events{/privacy}", "received_events_url": "https://api.github.com/users/MounirHader/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-30T13:51:36Z", "updated_at": "2017-10-30T13:51:36Z", "author_association": "NONE", "body_html": "<p>Also, it is not a memory error on the GPUs. When explicitly setting CUDA_VISIBLE_DEVICES to a non occupied GPU I get the same error. One time I seemed to get lucky as the following command worked:</p>\n<pre><code>(root) [~]$ CUDA_VISIBLE_DEVICES=13 python -c \"import tensorflow as tf;tf.InteractiveSession()\n\n\"\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:92:00.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:92:00.0)\n</code></pre>\n<p>But it keeps failing all other times (on GPUs that aren't occupied)</p>", "body_text": "Also, it is not a memory error on the GPUs. When explicitly setting CUDA_VISIBLE_DEVICES to a non occupied GPU I get the same error. One time I seemed to get lucky as the following command worked:\n(root) [~]$ CUDA_VISIBLE_DEVICES=13 python -c \"import tensorflow as tf;tf.InteractiveSession()\n\n\"\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \nname: Tesla K80\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\npciBusID 0000:92:00.0\nTotal memory: 11.17GiB\nFree memory: 11.11GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:92:00.0)\n\nBut it keeps failing all other times (on GPUs that aren't occupied)", "body": "Also, it is not a memory error on the GPUs. When explicitly setting CUDA_VISIBLE_DEVICES to a non occupied GPU I get the same error. One time I seemed to get lucky as the following command worked:\r\n\r\n```\r\n(root) [~]$ CUDA_VISIBLE_DEVICES=13 python -c \"import tensorflow as tf;tf.InteractiveSession()\r\n\r\n\"\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: Tesla K80\r\nmajor: 3 minor: 7 memoryClockRate (GHz) 0.8235\r\npciBusID 0000:92:00.0\r\nTotal memory: 11.17GiB\r\nFree memory: 11.11GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:92:00.0)\r\n```\r\n\r\nBut it keeps failing all other times (on GPUs that aren't occupied)"}