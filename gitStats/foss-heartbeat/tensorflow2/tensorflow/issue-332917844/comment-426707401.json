{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/426707401", "html_url": "https://github.com/tensorflow/tensorflow/issues/20068#issuecomment-426707401", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20068", "id": 426707401, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNjcwNzQwMQ==", "user": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-03T16:37:57Z", "updated_at": "2018-10-03T16:37:57Z", "author_association": "MEMBER", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=33295291\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hbi-tianyi\">@hbi-tianyi</a> !</p>\n<p>There are a number of things that can be done to optimize. (1) I highly recommend batching up your training samples into larger TFRecord files (e.g. 100-300 MB files). GCS has relatively high latency compared to local disk, but extremely high throughput. (2) Make sure you're using Parallel Interleave to read from multiple TFRecord files simultaneously, and also <code>.prefetch(tf.contrib.data.AUTOTUNE)</code> as the final step of your <code>tf.data.Dataset</code>. (3) If you still have performance problems, or don't want to batch up your data into large TFRecord files, you can consider using the new TensorFlow / Cloud Bigtable integration, which can achieve even higher performance than Cloud Storage (GCS).</p>\n<p>If you'd like an example for a high performance input pipeline, check out the <a href=\"https://github.com/tensorflow/tpu/tree/master/models/official/resnet\">Cloud TPU example for ResNet-50</a>. That input pipeline is able to drive a full Cloud TPU, and loads data from GCS at over 4 Gbps!</p>\n<p>If you're still running into issues, I recommend checking out <a href=\"https://www.youtube.com/watch?v=SxOsJPaxHME&amp;vl=en\" rel=\"nofollow\">Training Performance</a> which has an emphasis on input pipelines, and try using the <a href=\"https://cloud.google.com/tpu/docs/cloud-tpu-tools#profile_tab\" rel=\"nofollow\">Cloud TPU profiling tools</a> which have good instrumentation and visualization of input pipelines and on-device execution.</p>\n<p>Hope this helps!<br>\n-Brennan</p>", "body_text": "Hi @hbi-tianyi !\nThere are a number of things that can be done to optimize. (1) I highly recommend batching up your training samples into larger TFRecord files (e.g. 100-300 MB files). GCS has relatively high latency compared to local disk, but extremely high throughput. (2) Make sure you're using Parallel Interleave to read from multiple TFRecord files simultaneously, and also .prefetch(tf.contrib.data.AUTOTUNE) as the final step of your tf.data.Dataset. (3) If you still have performance problems, or don't want to batch up your data into large TFRecord files, you can consider using the new TensorFlow / Cloud Bigtable integration, which can achieve even higher performance than Cloud Storage (GCS).\nIf you'd like an example for a high performance input pipeline, check out the Cloud TPU example for ResNet-50. That input pipeline is able to drive a full Cloud TPU, and loads data from GCS at over 4 Gbps!\nIf you're still running into issues, I recommend checking out Training Performance which has an emphasis on input pipelines, and try using the Cloud TPU profiling tools which have good instrumentation and visualization of input pipelines and on-device execution.\nHope this helps!\n-Brennan", "body": "Hi @hbi-tianyi !\r\n\r\nThere are a number of things that can be done to optimize. (1) I highly recommend batching up your training samples into larger TFRecord files (e.g. 100-300 MB files). GCS has relatively high latency compared to local disk, but extremely high throughput. (2) Make sure you're using Parallel Interleave to read from multiple TFRecord files simultaneously, and also `.prefetch(tf.contrib.data.AUTOTUNE)` as the final step of your `tf.data.Dataset`. (3) If you still have performance problems, or don't want to batch up your data into large TFRecord files, you can consider using the new TensorFlow / Cloud Bigtable integration, which can achieve even higher performance than Cloud Storage (GCS).\r\n\r\nIf you'd like an example for a high performance input pipeline, check out the [Cloud TPU example for ResNet-50](https://github.com/tensorflow/tpu/tree/master/models/official/resnet). That input pipeline is able to drive a full Cloud TPU, and loads data from GCS at over 4 Gbps!\r\n\r\nIf you're still running into issues, I recommend checking out [Training Performance](https://www.youtube.com/watch?v=SxOsJPaxHME&vl=en) which has an emphasis on input pipelines, and try using the [Cloud TPU profiling tools](https://cloud.google.com/tpu/docs/cloud-tpu-tools#profile_tab) which have good instrumentation and visualization of input pipelines and on-device execution.\r\n\r\nHope this helps!\r\n-Brennan\r\n"}