{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1317", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1317/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1317/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1317/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1317", "id": 136943254, "node_id": "MDU6SXNzdWUxMzY5NDMyNTQ=", "number": 1317, "title": "tf.get_variable() behavior somewhat inconsistent across reuse=True, reuse=None", "user": {"login": "ericjang", "id": 433201, "node_id": "MDQ6VXNlcjQzMzIwMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/433201?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericjang", "html_url": "https://github.com/ericjang", "followers_url": "https://api.github.com/users/ericjang/followers", "following_url": "https://api.github.com/users/ericjang/following{/other_user}", "gists_url": "https://api.github.com/users/ericjang/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericjang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericjang/subscriptions", "organizations_url": "https://api.github.com/users/ericjang/orgs", "repos_url": "https://api.github.com/users/ericjang/repos", "events_url": "https://api.github.com/users/ericjang/events{/privacy}", "received_events_url": "https://api.github.com/users/ericjang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-02-27T17:20:28Z", "updated_at": "2016-03-07T03:52:36Z", "closed_at": "2016-02-29T22:21:21Z", "author_association": "NONE", "body_html": "<p>By default, calling <code>tf.get_variable()</code> for a variable that does not exist yet creates that variable in the current scope, if the scope has <code>reuse=Nne</code>. However, if the scope has reuse=True, then  a Under-sharing error is raised.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scope1<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  x<span class=\"pl-k\">=</span>tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> this works</span>\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rnnscope<span class=\"pl-pds\">\"</span></span>,<span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n  w<span class=\"pl-k\">=</span>tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>w<span class=\"pl-pds\">\"</span></span>,[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> this fails</span>\n<span class=\"pl-c1\">ValueError</span>: Under<span class=\"pl-k\">-</span>sharing: Variable rnnscope<span class=\"pl-k\">/</span>w does <span class=\"pl-k\">not</span> exist, disallowed. Did you mean to <span class=\"pl-c1\">set</span> reuse<span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span> <span class=\"pl-k\">in</span> VarScope<span class=\"pl-ii\">?</span></pre></div>\n<p>I think I understand the motivation behind this behavior - variables retrieved in a reuse=True scope should be in \"reuse-mode\", so an error should be raised if it hasn't been created yet.</p>\n<p>The way I get around this is a global variable DO_SHARE=None that is permanently set to True for all t &gt; 0.</p>\n<p>An example implementation of building an RNN this way is here: <a href=\"https://github.com/ericjang/draw/blob/master/draw.py#L163\">https://github.com/ericjang/draw/blob/master/draw.py#L163</a></p>\n<p>However, this seems a bit inelegant - wouldn't it be easier to just have <code>get_variable()</code> create nonexistent variables, regardless of the value of <code>reuse</code>? Perhaps there is a better way to build RNNs that I'm not aware of?</p>\n<h3>Environment info</h3>\n<p>Operating System: <strong>Debian</strong></p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.<br>\n<strong>GPU-enabled wheel for Linux</strong></li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\".<br>\n<strong>0.6.0</strong></li>\n</ol>\n<h3>Steps to reproduce</h3>\n<h3>What have you tried?</h3>\n<ol>\n<li></li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>", "body_text": "By default, calling tf.get_variable() for a variable that does not exist yet creates that variable in the current scope, if the scope has reuse=Nne. However, if the scope has reuse=True, then  a Under-sharing error is raised.\nimport tensorflow as tf\nwith tf.variable_scope(\"scope1\",reuse=None):\n  x=tf.get_variable(\"x\", [1,1]) # this works\nwith tf.variable_scope(\"rnnscope\",reuse=True):\n  w=tf.get_variable(\"w\",[1,1]) # this fails\nValueError: Under-sharing: Variable rnnscope/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?\nI think I understand the motivation behind this behavior - variables retrieved in a reuse=True scope should be in \"reuse-mode\", so an error should be raised if it hasn't been created yet.\nThe way I get around this is a global variable DO_SHARE=None that is permanently set to True for all t > 0.\nAn example implementation of building an RNN this way is here: https://github.com/ericjang/draw/blob/master/draw.py#L163\nHowever, this seems a bit inelegant - wouldn't it be easier to just have get_variable() create nonexistent variables, regardless of the value of reuse? Perhaps there is a better way to build RNNs that I'm not aware of?\nEnvironment info\nOperating System: Debian\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\nGPU-enabled wheel for Linux\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n0.6.0\n\nSteps to reproduce\nWhat have you tried?\n\n\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).", "body": "By default, calling `tf.get_variable()` for a variable that does not exist yet creates that variable in the current scope, if the scope has `reuse=Nne`. However, if the scope has reuse=True, then  a Under-sharing error is raised.\n\n``` python\nimport tensorflow as tf\nwith tf.variable_scope(\"scope1\",reuse=None):\n  x=tf.get_variable(\"x\", [1,1]) # this works\nwith tf.variable_scope(\"rnnscope\",reuse=True):\n  w=tf.get_variable(\"w\",[1,1]) # this fails\nValueError: Under-sharing: Variable rnnscope/w does not exist, disallowed. Did you mean to set reuse=None in VarScope?\n```\n\nI think I understand the motivation behind this behavior - variables retrieved in a reuse=True scope should be in \"reuse-mode\", so an error should be raised if it hasn't been created yet.\n\nThe way I get around this is a global variable DO_SHARE=None that is permanently set to True for all t > 0. \n\nAn example implementation of building an RNN this way is here: https://github.com/ericjang/draw/blob/master/draw.py#L163\n\nHowever, this seems a bit inelegant - wouldn't it be easier to just have `get_variable()` create nonexistent variables, regardless of the value of `reuse`? Perhaps there is a better way to build RNNs that I'm not aware of?\n### Environment info\n\nOperating System: **Debian**\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   **GPU-enabled wheel for Linux**\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   **0.6.0**\n### Steps to reproduce\n### What have you tried?\n\n1.\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n"}