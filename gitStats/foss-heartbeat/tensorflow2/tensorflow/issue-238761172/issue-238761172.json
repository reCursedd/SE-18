{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11077", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11077/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11077/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11077/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11077", "id": 238761172, "node_id": "MDU6SXNzdWUyMzg3NjExNzI=", "number": 11077, "title": "something wrong with attentionwrapper?", "user": {"login": "sunxiaoben", "id": 5608481, "node_id": "MDQ6VXNlcjU2MDg0ODE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5608481?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sunxiaoben", "html_url": "https://github.com/sunxiaoben", "followers_url": "https://api.github.com/users/sunxiaoben/followers", "following_url": "https://api.github.com/users/sunxiaoben/following{/other_user}", "gists_url": "https://api.github.com/users/sunxiaoben/gists{/gist_id}", "starred_url": "https://api.github.com/users/sunxiaoben/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sunxiaoben/subscriptions", "organizations_url": "https://api.github.com/users/sunxiaoben/orgs", "repos_url": "https://api.github.com/users/sunxiaoben/repos", "events_url": "https://api.github.com/users/sunxiaoben/events{/privacy}", "received_events_url": "https://api.github.com/users/sunxiaoben/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-06-27T07:33:11Z", "updated_at": "2017-07-18T16:05:58Z", "closed_at": "2017-06-28T08:26:19Z", "author_association": "NONE", "body_html": "<p>I am trying to write a seq2seq model with attention. But it gets the following error:</p>\n<pre><code>cell_inputs = self._cell_input_fn(inputs, state.attention)\n</code></pre>\n<p>AttributeError: 'tuple' object has no attribute 'attention'</p>\n<p>It seems the state which initialize by the encoder state that does't has the attribute 'attention'.<br>\nHow should I call AttentionWrapper?</p>\n<p>Here is my code:</p>\n<pre><code>    cell_list = []\n    for layer_i in xrange(hps.num_layers):\n        cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\n        with tf.variable_scope('encoder%d'%layer_i):\n            cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\n    enc_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n\n    encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn(enc_cell, enc_inp, dtype=dtype)\n\n    cell_list = []\n    for layer_i in xrange(hps.num_layers):\n        with tf.variable_scope('decoder%d'%layer_i):\n            cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\n    dec_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n\n\n    new_enc_out = tf.reshape(encoder_outputs, [hps.batch_size, 30, 256])\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n            num_units = 128, # depth of query mechanism\n            memory = new_enc_out, # hidden states to attend (output of RNN)\n            normalize=False, # normalize energy term\n            name='BahdanauAttention')\n\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n            cell = dec_cell,# Instance of RNNCell\n            attention_mechanism = attn_mech, # Instance of AttentionMechanism\n            name=\"attention_wrapper\")\n\n    if hps.mode==\"train\":\n        # TrainingHelper does no sampling, only uses inputs\n        helper = tf.contrib.seq2seq.TrainingHelper(\n                inputs = dec_inp[:decoder_size], # decoder inputs\n                sequence_length = [1]*decoder_size, # decoder input length\n                name = \"decoder_training_helper\")\n\n        decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell = attn_cell,\n                helper = helper, # A Helper instance\n                initial_state = encoder_state, # initial state of decoder\n                output_layer = None) # instance of tf.layers.Layer, like Dense\n\n    decoder_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder)\n</code></pre>", "body_text": "I am trying to write a seq2seq model with attention. But it gets the following error:\ncell_inputs = self._cell_input_fn(inputs, state.attention)\n\nAttributeError: 'tuple' object has no attribute 'attention'\nIt seems the state which initialize by the encoder state that does't has the attribute 'attention'.\nHow should I call AttentionWrapper?\nHere is my code:\n    cell_list = []\n    for layer_i in xrange(hps.num_layers):\n        cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\n        with tf.variable_scope('encoder%d'%layer_i):\n            cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\n    enc_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n\n    encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn(enc_cell, enc_inp, dtype=dtype)\n\n    cell_list = []\n    for layer_i in xrange(hps.num_layers):\n        with tf.variable_scope('decoder%d'%layer_i):\n            cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\n    dec_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\n\n\n    new_enc_out = tf.reshape(encoder_outputs, [hps.batch_size, 30, 256])\n    attn_mech = tf.contrib.seq2seq.BahdanauAttention(\n            num_units = 128, # depth of query mechanism\n            memory = new_enc_out, # hidden states to attend (output of RNN)\n            normalize=False, # normalize energy term\n            name='BahdanauAttention')\n\n    attn_cell = tf.contrib.seq2seq.AttentionWrapper(\n            cell = dec_cell,# Instance of RNNCell\n            attention_mechanism = attn_mech, # Instance of AttentionMechanism\n            name=\"attention_wrapper\")\n\n    if hps.mode==\"train\":\n        # TrainingHelper does no sampling, only uses inputs\n        helper = tf.contrib.seq2seq.TrainingHelper(\n                inputs = dec_inp[:decoder_size], # decoder inputs\n                sequence_length = [1]*decoder_size, # decoder input length\n                name = \"decoder_training_helper\")\n\n        decoder = tf.contrib.seq2seq.BasicDecoder(\n                cell = attn_cell,\n                helper = helper, # A Helper instance\n                initial_state = encoder_state, # initial state of decoder\n                output_layer = None) # instance of tf.layers.Layer, like Dense\n\n    decoder_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder)", "body": "I am trying to write a seq2seq model with attention. But it gets the following error:\r\n\r\n    cell_inputs = self._cell_input_fn(inputs, state.attention)\r\nAttributeError: 'tuple' object has no attribute 'attention'\r\n\r\nIt seems the state which initialize by the encoder state that does't has the attribute 'attention'.\r\nHow should I call AttentionWrapper?\r\n\r\nHere is my code:\r\n\r\n        cell_list = []\r\n        for layer_i in xrange(hps.num_layers):\r\n            cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\r\n            with tf.variable_scope('encoder%d'%layer_i):\r\n                cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\r\n        enc_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\r\n\r\n        encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn(enc_cell, enc_inp, dtype=dtype)\r\n\r\n        cell_list = []\r\n        for layer_i in xrange(hps.num_layers):\r\n            with tf.variable_scope('decoder%d'%layer_i):\r\n                cell_list.append( tf.contrib.rnn.LSTMCell(hps.num_hidden) )\r\n        dec_cell = tf.contrib.rnn.MultiRNNCell(cell_list)\r\n\r\n\r\n        new_enc_out = tf.reshape(encoder_outputs, [hps.batch_size, 30, 256])\r\n        attn_mech = tf.contrib.seq2seq.BahdanauAttention(\r\n                num_units = 128, # depth of query mechanism\r\n                memory = new_enc_out, # hidden states to attend (output of RNN)\r\n                normalize=False, # normalize energy term\r\n                name='BahdanauAttention')\r\n\r\n        attn_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n                cell = dec_cell,# Instance of RNNCell\r\n                attention_mechanism = attn_mech, # Instance of AttentionMechanism\r\n                name=\"attention_wrapper\")\r\n\r\n        if hps.mode==\"train\":\r\n            # TrainingHelper does no sampling, only uses inputs\r\n            helper = tf.contrib.seq2seq.TrainingHelper(\r\n                    inputs = dec_inp[:decoder_size], # decoder inputs\r\n                    sequence_length = [1]*decoder_size, # decoder input length\r\n                    name = \"decoder_training_helper\")\r\n\r\n            decoder = tf.contrib.seq2seq.BasicDecoder(\r\n                    cell = attn_cell,\r\n                    helper = helper, # A Helper instance\r\n                    initial_state = encoder_state, # initial state of decoder\r\n                    output_layer = None) # instance of tf.layers.Layer, like Dense\r\n\r\n        decoder_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(decoder)"}