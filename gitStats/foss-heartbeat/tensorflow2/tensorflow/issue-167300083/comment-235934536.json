{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235934536", "html_url": "https://github.com/tensorflow/tensorflow/issues/3489#issuecomment-235934536", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3489", "id": 235934536, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTkzNDUzNg==", "user": {"login": "akors", "id": 3023492, "node_id": "MDQ6VXNlcjMwMjM0OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3023492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akors", "html_url": "https://github.com/akors", "followers_url": "https://api.github.com/users/akors/followers", "following_url": "https://api.github.com/users/akors/following{/other_user}", "gists_url": "https://api.github.com/users/akors/gists{/gist_id}", "starred_url": "https://api.github.com/users/akors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akors/subscriptions", "organizations_url": "https://api.github.com/users/akors/orgs", "repos_url": "https://api.github.com/users/akors/repos", "events_url": "https://api.github.com/users/akors/events{/privacy}", "received_events_url": "https://api.github.com/users/akors/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-28T15:40:31Z", "updated_at": "2016-07-28T15:40:31Z", "author_association": "NONE", "body_html": "<p>I started out with the assumption that running the same operation on data of the same shape should take approximately the same amount of time. I have found this to not be true, if you are interested you can take a look at my <a href=\"http://superuser.com/questions/1086626/machine-performance-drops-over-time-until-reboot\" rel=\"nofollow\">StackExchange Thread</a>. There have also been other <a href=\"https://github.com/tensorflow/models/issues/170\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/170/hovercard\">reports</a> of slowdowns that might, or might not be related.</p>\n<p>My primary goal is to get rid of those slowdowns. If that happens by some magic commit and the problem goes away, that would be very dandy. I'm not very hopeful this will happen though, so I am ready and willing to dig down and debug the issue.</p>\n<p>I was hoping that the Timeline thing would be able to help me. I created this issue because I was not sure (and it's not documented anywhere) of what the purpose of the TimeLine object is, and whether it's supposed to be used in only one invocation of Session.run() or multiple ones. Judging from your comment, it's supposed to cover only one invocaton?</p>\n<blockquote>\n<p>If you literally just want a concatenation, one way of doing this is to record both traces and generate a new protobuf with both of their elements</p>\n</blockquote>\n<p>Is there a somewhat sane python snippet that can do that? If not, I'd prefer not to dig into Protobuf affairs.</p>\n<blockquote>\n<p>It sounds like you want something more intelligent, though</p>\n</blockquote>\n<p>Not really. The only thing that interests me is why some batches take significantly longer to compute than other batches, and I was hoping that a simple concatenation of the timeline could yield some insight.</p>", "body_text": "I started out with the assumption that running the same operation on data of the same shape should take approximately the same amount of time. I have found this to not be true, if you are interested you can take a look at my StackExchange Thread. There have also been other reports of slowdowns that might, or might not be related.\nMy primary goal is to get rid of those slowdowns. If that happens by some magic commit and the problem goes away, that would be very dandy. I'm not very hopeful this will happen though, so I am ready and willing to dig down and debug the issue.\nI was hoping that the Timeline thing would be able to help me. I created this issue because I was not sure (and it's not documented anywhere) of what the purpose of the TimeLine object is, and whether it's supposed to be used in only one invocation of Session.run() or multiple ones. Judging from your comment, it's supposed to cover only one invocaton?\n\nIf you literally just want a concatenation, one way of doing this is to record both traces and generate a new protobuf with both of their elements\n\nIs there a somewhat sane python snippet that can do that? If not, I'd prefer not to dig into Protobuf affairs.\n\nIt sounds like you want something more intelligent, though\n\nNot really. The only thing that interests me is why some batches take significantly longer to compute than other batches, and I was hoping that a simple concatenation of the timeline could yield some insight.", "body": "I started out with the assumption that running the same operation on data of the same shape should take approximately the same amount of time. I have found this to not be true, if you are interested you can take a look at my [StackExchange Thread](http://superuser.com/questions/1086626/machine-performance-drops-over-time-until-reboot). There have also been other [reports](https://github.com/tensorflow/models/issues/170) of slowdowns that might, or might not be related.\n\nMy primary goal is to get rid of those slowdowns. If that happens by some magic commit and the problem goes away, that would be very dandy. I'm not very hopeful this will happen though, so I am ready and willing to dig down and debug the issue.\n\nI was hoping that the Timeline thing would be able to help me. I created this issue because I was not sure (and it's not documented anywhere) of what the purpose of the TimeLine object is, and whether it's supposed to be used in only one invocation of Session.run() or multiple ones. Judging from your comment, it's supposed to cover only one invocaton?\n\n> If you literally just want a concatenation, one way of doing this is to record both traces and generate a new protobuf with both of their elements\n\nIs there a somewhat sane python snippet that can do that? If not, I'd prefer not to dig into Protobuf affairs.\n\n> It sounds like you want something more intelligent, though\n\nNot really. The only thing that interests me is why some batches take significantly longer to compute than other batches, and I was hoping that a simple concatenation of the timeline could yield some insight.\n"}