{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3489", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3489/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3489/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3489/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3489", "id": 167300083, "node_id": "MDU6SXNzdWUxNjczMDAwODM=", "number": 3489, "title": "Timeline only produced for the last invocation on Session.run", "user": {"login": "akors", "id": 3023492, "node_id": "MDQ6VXNlcjMwMjM0OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3023492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akors", "html_url": "https://github.com/akors", "followers_url": "https://api.github.com/users/akors/followers", "following_url": "https://api.github.com/users/akors/following{/other_user}", "gists_url": "https://api.github.com/users/akors/gists{/gist_id}", "starred_url": "https://api.github.com/users/akors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akors/subscriptions", "organizations_url": "https://api.github.com/users/akors/orgs", "repos_url": "https://api.github.com/users/akors/repos", "events_url": "https://api.github.com/users/akors/events{/privacy}", "received_events_url": "https://api.github.com/users/akors/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-07-25T07:29:22Z", "updated_at": "2016-08-29T16:12:02Z", "closed_at": "2016-08-29T16:12:02Z", "author_association": "NONE", "body_html": "<p>Hi!</p>\n<p>First of all, thanks for the great profiling/timeline feature! I think it can be really amazing to debug model performance.</p>\n<p>I do have a question though: is it possible to create a timeline for <em>multiple</em> invocations of <code>Session.run</code>?</p>\n<p>In all the examples that I saw, timeline creation looks something along the lines of this:</p>\n<pre><code>run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\n\nsess.run(result, options=run_options, run_metadata=run_metadata)\n\ntl = timeline.Timeline(run_metadata.step_stats)\nctf = tl.generate_chrome_trace_format()\n</code></pre>\n<p>This is all good and well, but this generates the trace only for <em>one</em> invocation of Session.run. If it is invoked multiple times (for example in batch training), the <code>step_stats</code> in the run_metadata are always overwritten by the last run.</p>\n<p>Is there a way to aggregate the timing data to create a timeline that spans over multiple invocations?</p>\n<p>I would need this to debug performance decay over long periods.</p>", "body_text": "Hi!\nFirst of all, thanks for the great profiling/timeline feature! I think it can be really amazing to debug model performance.\nI do have a question though: is it possible to create a timeline for multiple invocations of Session.run?\nIn all the examples that I saw, timeline creation looks something along the lines of this:\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\n\nsess.run(result, options=run_options, run_metadata=run_metadata)\n\ntl = timeline.Timeline(run_metadata.step_stats)\nctf = tl.generate_chrome_trace_format()\n\nThis is all good and well, but this generates the trace only for one invocation of Session.run. If it is invoked multiple times (for example in batch training), the step_stats in the run_metadata are always overwritten by the last run.\nIs there a way to aggregate the timing data to create a timeline that spans over multiple invocations?\nI would need this to debug performance decay over long periods.", "body": "Hi!\n\nFirst of all, thanks for the great profiling/timeline feature! I think it can be really amazing to debug model performance.\n\nI do have a question though: is it possible to create a timeline for _multiple_ invocations of `Session.run`?\n\nIn all the examples that I saw, timeline creation looks something along the lines of this:\n\n```\nrun_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\nrun_metadata = tf.RunMetadata()\n\nsess.run(result, options=run_options, run_metadata=run_metadata)\n\ntl = timeline.Timeline(run_metadata.step_stats)\nctf = tl.generate_chrome_trace_format()\n```\n\nThis is all good and well, but this generates the trace only for _one_ invocation of Session.run. If it is invoked multiple times (for example in batch training), the `step_stats` in the run_metadata are always overwritten by the last run.\n\nIs there a way to aggregate the timing data to create a timeline that spans over multiple invocations?\n\nI would need this to debug performance decay over long periods.\n"}