{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260684291", "html_url": "https://github.com/tensorflow/tensorflow/issues/5606#issuecomment-260684291", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5606", "id": 260684291, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDY4NDI5MQ==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-15T16:08:38Z", "updated_at": "2016-11-15T16:08:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Unfortunately, the current code for transpose does template instantiation for each dimension.  We're already running into code size issues, so we won't be able to accept 40 or 50 (a rather large chunk of the tensorflow binary would become transposes).</p>\n<p>However, we'd be happy to accept PRs that implement arbitrary rank transposes in a manner that doesn't require unbounded template instantiation.  There are a couple options for this:</p>\n<ol>\n<li>Find a clever new algorithm that I've searched for but failed to find that efficiently does arbitrary rank transpose.  I would be thrilled.</li>\n<li>Split high rank transposes into a sequence of reshapes and smaller rank transposes.  This requires a fair amount of index machinery, but is \"straightforward\".  It could be done either in C++ and Python, but is probably easier to do in C++ since then the index manipulation doesn't have to be TensorFlow ops.</li>\n</ol>\n<p>In either case, it would be good to add the optimization that flattens dimensions that transpose together so as to use lower rank code if possible.</p>\n<p>Note that <a href=\"https://www.tensorflow.org/versions/master/resources/xla_prerelease.html\" rel=\"nofollow\">XLA</a> will likely have no trouble with high rank transposes, but that may take a while to land and wouldn't necessary be very efficient (for the same reasons I haven't found a fast high rank transpose algorithm).  Thus, I don't think the above work would be wasted.</p>", "body_text": "Unfortunately, the current code for transpose does template instantiation for each dimension.  We're already running into code size issues, so we won't be able to accept 40 or 50 (a rather large chunk of the tensorflow binary would become transposes).\nHowever, we'd be happy to accept PRs that implement arbitrary rank transposes in a manner that doesn't require unbounded template instantiation.  There are a couple options for this:\n\nFind a clever new algorithm that I've searched for but failed to find that efficiently does arbitrary rank transpose.  I would be thrilled.\nSplit high rank transposes into a sequence of reshapes and smaller rank transposes.  This requires a fair amount of index machinery, but is \"straightforward\".  It could be done either in C++ and Python, but is probably easier to do in C++ since then the index manipulation doesn't have to be TensorFlow ops.\n\nIn either case, it would be good to add the optimization that flattens dimensions that transpose together so as to use lower rank code if possible.\nNote that XLA will likely have no trouble with high rank transposes, but that may take a while to land and wouldn't necessary be very efficient (for the same reasons I haven't found a fast high rank transpose algorithm).  Thus, I don't think the above work would be wasted.", "body": "Unfortunately, the current code for transpose does template instantiation for each dimension.  We're already running into code size issues, so we won't be able to accept 40 or 50 (a rather large chunk of the tensorflow binary would become transposes).\n\nHowever, we'd be happy to accept PRs that implement arbitrary rank transposes in a manner that doesn't require unbounded template instantiation.  There are a couple options for this:\n1. Find a clever new algorithm that I've searched for but failed to find that efficiently does arbitrary rank transpose.  I would be thrilled.\n2. Split high rank transposes into a sequence of reshapes and smaller rank transposes.  This requires a fair amount of index machinery, but is \"straightforward\".  It could be done either in C++ and Python, but is probably easier to do in C++ since then the index manipulation doesn't have to be TensorFlow ops.\n\nIn either case, it would be good to add the optimization that flattens dimensions that transpose together so as to use lower rank code if possible.\n\nNote that [XLA](https://www.tensorflow.org/versions/master/resources/xla_prerelease.html) will likely have no trouble with high rank transposes, but that may take a while to land and wouldn't necessary be very efficient (for the same reasons I haven't found a fast high rank transpose algorithm).  Thus, I don't think the above work would be wasted.\n"}