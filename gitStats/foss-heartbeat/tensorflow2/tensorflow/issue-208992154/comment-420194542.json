{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/420194542", "html_url": "https://github.com/tensorflow/tensorflow/issues/7712#issuecomment-420194542", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7712", "id": 420194542, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMDE5NDU0Mg==", "user": {"login": "kacper1095", "id": 16157910, "node_id": "MDQ6VXNlcjE2MTU3OTEw", "avatar_url": "https://avatars2.githubusercontent.com/u/16157910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kacper1095", "html_url": "https://github.com/kacper1095", "followers_url": "https://api.github.com/users/kacper1095/followers", "following_url": "https://api.github.com/users/kacper1095/following{/other_user}", "gists_url": "https://api.github.com/users/kacper1095/gists{/gist_id}", "starred_url": "https://api.github.com/users/kacper1095/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kacper1095/subscriptions", "organizations_url": "https://api.github.com/users/kacper1095/orgs", "repos_url": "https://api.github.com/users/kacper1095/repos", "events_url": "https://api.github.com/users/kacper1095/events{/privacy}", "received_events_url": "https://api.github.com/users/kacper1095/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-11T08:38:53Z", "updated_at": "2018-09-11T08:38:53Z", "author_association": "NONE", "body_html": "<p>Is it a good implementation of the PELU? In the paper there are lines like:</p>\n<pre><code>Unlike Maxout, our PELU adds only 2L parameters, where\nL is the number of layers, which makes our activation as\ncomputationally demanding as the original ELU function\n</code></pre>\n<p>and</p>\n<pre><code>It is interesting to note that PELU only adds 112 additional\nparameters, a negligible increase of 0.006% over the total\nnumber of parameters.\n</code></pre>\n<p>(regarding to ResNet-112). Still, your implementation introduces one parameter for each neuron, which is much more then the number mentioned in the paper. I suggest following change from:</p>\n<div class=\"highlight highlight-source-python\"><pre>    shape <span class=\"pl-k\">=</span> x.get_shape().as_list()[<span class=\"pl-c1\">1</span>:]\n    alpha <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>alpha<span class=\"pl-pds\">'</span></span>, shape)\n    beta <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>beta<span class=\"pl-pds\">'</span></span>, shape)</pre></div>\n<p>to</p>\n<div class=\"highlight highlight-source-python\"><pre>    alpha <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>alpha<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">1</span>)\n    beta <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>beta<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">1</span>)</pre></div>\n<p>Please, correct me if I didn't understand the paper correctly.</p>", "body_text": "Is it a good implementation of the PELU? In the paper there are lines like:\nUnlike Maxout, our PELU adds only 2L parameters, where\nL is the number of layers, which makes our activation as\ncomputationally demanding as the original ELU function\n\nand\nIt is interesting to note that PELU only adds 112 additional\nparameters, a negligible increase of 0.006% over the total\nnumber of parameters.\n\n(regarding to ResNet-112). Still, your implementation introduces one parameter for each neuron, which is much more then the number mentioned in the paper. I suggest following change from:\n    shape = x.get_shape().as_list()[1:]\n    alpha = tf.get_variable('alpha', shape)\n    beta = tf.get_variable('beta', shape)\nto\n    alpha = tf.get_variable('alpha', 1)\n    beta = tf.get_variable('beta', 1)\nPlease, correct me if I didn't understand the paper correctly.", "body": "Is it a good implementation of the PELU? In the paper there are lines like: \r\n```\r\nUnlike Maxout, our PELU adds only 2L parameters, where\r\nL is the number of layers, which makes our activation as\r\ncomputationally demanding as the original ELU function\r\n``` \r\nand\r\n ```\r\nIt is interesting to note that PELU only adds 112 additional\r\nparameters, a negligible increase of 0.006% over the total\r\nnumber of parameters.\r\n``` \r\n(regarding to ResNet-112). Still, your implementation introduces one parameter for each neuron, which is much more then the number mentioned in the paper. I suggest following change from:\r\n```python\r\n    shape = x.get_shape().as_list()[1:]\r\n    alpha = tf.get_variable('alpha', shape)\r\n    beta = tf.get_variable('beta', shape)\r\n```\r\nto\r\n```python\r\n    alpha = tf.get_variable('alpha', 1)\r\n    beta = tf.get_variable('beta', 1)\r\n```\r\nPlease, correct me if I didn't understand the paper correctly."}