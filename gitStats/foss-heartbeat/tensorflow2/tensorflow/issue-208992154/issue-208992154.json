{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7712", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7712/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7712/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7712/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7712", "id": 208992154, "node_id": "MDU6SXNzdWUyMDg5OTIxNTQ=", "number": 7712, "title": "Feature request: add parametric ELU (PELU) activation function", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-02-20T22:05:41Z", "updated_at": "2018-09-11T19:25:33Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h1>Proposal</h1>\n<p>The exponential linear unit (ELU) is already in TensorFlow as <code>tf.nn.elu</code> which is great. The new parametric version (called PELU) shows very promising experimental results so I wonder if it could be added in to TensorFlow too in order to encourage more widespread experimentation with it by the deep learning community. One problem with it though is that it's stateful (e.g. <code>tf.Variable</code>), meaning it's not clear to me where in TensorFlow it fits in.</p>\n<h1>Implementation</h1>\n<p>Here's an implementation of the PELU that I've been using lately (I'm assuming batch_size is the first dimension in <code>x</code>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">pelu</span>(<span class=\"pl-smi\">x</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Parametric Exponential Linear Unit (https://arxiv.org/abs/1605.09332v1).<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">with</span> tf.variable_scope(x.op.name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_activation<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">1.0</span>)):\n    shape <span class=\"pl-k\">=</span> x.get_shape().as_list()[<span class=\"pl-c1\">1</span>:]\n    alpha <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>alpha<span class=\"pl-pds\">'</span></span>, shape)\n    beta <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>beta<span class=\"pl-pds\">'</span></span>, shape)\n    positive <span class=\"pl-k\">=</span> tf.nn.relu(x) <span class=\"pl-k\">*</span> alpha <span class=\"pl-k\">/</span> (beta <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-9</span>)\n    negative <span class=\"pl-k\">=</span> alpha <span class=\"pl-k\">*</span> (tf.exp((<span class=\"pl-k\">-</span>tf.nn.relu(<span class=\"pl-k\">-</span>x)) <span class=\"pl-k\">/</span> (beta <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-9</span>)) <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> negative <span class=\"pl-k\">+</span> positive</pre></div>\n<h1>Reference</h1>\n<p><a href=\"https://arxiv.org/abs/1605.09332v1\" rel=\"nofollow\">https://arxiv.org/abs/1605.09332v1</a></p>", "body_text": "Proposal\nThe exponential linear unit (ELU) is already in TensorFlow as tf.nn.elu which is great. The new parametric version (called PELU) shows very promising experimental results so I wonder if it could be added in to TensorFlow too in order to encourage more widespread experimentation with it by the deep learning community. One problem with it though is that it's stateful (e.g. tf.Variable), meaning it's not clear to me where in TensorFlow it fits in.\nImplementation\nHere's an implementation of the PELU that I've been using lately (I'm assuming batch_size is the first dimension in x):\ndef pelu(x):\n  \"\"\"Parametric Exponential Linear Unit (https://arxiv.org/abs/1605.09332v1).\"\"\"\n  with tf.variable_scope(x.op.name + '_activation', initializer=tf.constant_initializer(1.0)):\n    shape = x.get_shape().as_list()[1:]\n    alpha = tf.get_variable('alpha', shape)\n    beta = tf.get_variable('beta', shape)\n    positive = tf.nn.relu(x) * alpha / (beta + 1e-9)\n    negative = alpha * (tf.exp((-tf.nn.relu(-x)) / (beta + 1e-9)) - 1)\n    return negative + positive\nReference\nhttps://arxiv.org/abs/1605.09332v1", "body": "# Proposal\r\nThe exponential linear unit (ELU) is already in TensorFlow as `tf.nn.elu` which is great. The new parametric version (called PELU) shows very promising experimental results so I wonder if it could be added in to TensorFlow too in order to encourage more widespread experimentation with it by the deep learning community. One problem with it though is that it's stateful (e.g. `tf.Variable`), meaning it's not clear to me where in TensorFlow it fits in.\r\n\r\n# Implementation\r\nHere's an implementation of the PELU that I've been using lately (I'm assuming batch_size is the first dimension in `x`):\r\n```python\r\ndef pelu(x):\r\n  \"\"\"Parametric Exponential Linear Unit (https://arxiv.org/abs/1605.09332v1).\"\"\"\r\n  with tf.variable_scope(x.op.name + '_activation', initializer=tf.constant_initializer(1.0)):\r\n    shape = x.get_shape().as_list()[1:]\r\n    alpha = tf.get_variable('alpha', shape)\r\n    beta = tf.get_variable('beta', shape)\r\n    positive = tf.nn.relu(x) * alpha / (beta + 1e-9)\r\n    negative = alpha * (tf.exp((-tf.nn.relu(-x)) / (beta + 1e-9)) - 1)\r\n    return negative + positive\r\n```\r\n\r\n# Reference\r\nhttps://arxiv.org/abs/1605.09332v1"}