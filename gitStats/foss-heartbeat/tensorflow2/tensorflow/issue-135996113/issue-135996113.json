{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1271", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1271/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1271/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1271/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1271", "id": 135996113, "node_id": "MDU6SXNzdWUxMzU5OTYxMTM=", "number": 1271, "title": "Implementing Striving for Simplicity: The All Convolutional Net in Tensorflow: results on the test set lower than expected", "user": {"login": "jeandut", "id": 11030901, "node_id": "MDQ6VXNlcjExMDMwOTAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11030901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeandut", "html_url": "https://github.com/jeandut", "followers_url": "https://api.github.com/users/jeandut/followers", "following_url": "https://api.github.com/users/jeandut/following{/other_user}", "gists_url": "https://api.github.com/users/jeandut/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeandut/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeandut/subscriptions", "organizations_url": "https://api.github.com/users/jeandut/orgs", "repos_url": "https://api.github.com/users/jeandut/repos", "events_url": "https://api.github.com/users/jeandut/events{/privacy}", "received_events_url": "https://api.github.com/users/jeandut/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-02-24T08:53:25Z", "updated_at": "2016-02-25T07:07:35Z", "closed_at": "2016-02-25T07:07:35Z", "author_association": "NONE", "body_html": "<p>I posted <a href=\"http://stackoverflow.com/questions/35339636/erratic-training-for-all-cnn-on-cifar-10\" rel=\"nofollow\">this issue with my code</a> on stackoverflow a while ago but it has not received any answer or comment.<br>\nThe preprocessing I used is in the dataset class (that I adapted from the Deep MNIST for experts tutorial) and is exactly the same as in the neon/pylearn2 implementation: calculating the mean and Wzca matrix on training set and use it to whiten the data on the training and test set followed by a global contrast normalization step with Goodfellow scale factor of 55.<br>\nThe only difference of my implementation according to me (I would like to be mistaken) with the paper is the use of <code>tf.reduce_mean</code> instead of <code>tf.reduce_sum</code> (which led me to exploding ReLU grad).<br>\nSo I divide the weight decay by a factor batch_size to keep the change.<br>\nWith this I got up to 85.something% on the test set instead of the 91% claimed by the authors.<br>\nDo you see something wrong immediately ? For instance I feel the weight decay part of the cross entropy is ugly but I could not find any better way of doing that. Do you have some ideas ?<br>\nIf you think that this is not an appropriate place for my question tell me I will remove it but I do not know where to look.</p>", "body_text": "I posted this issue with my code on stackoverflow a while ago but it has not received any answer or comment.\nThe preprocessing I used is in the dataset class (that I adapted from the Deep MNIST for experts tutorial) and is exactly the same as in the neon/pylearn2 implementation: calculating the mean and Wzca matrix on training set and use it to whiten the data on the training and test set followed by a global contrast normalization step with Goodfellow scale factor of 55.\nThe only difference of my implementation according to me (I would like to be mistaken) with the paper is the use of tf.reduce_mean instead of tf.reduce_sum (which led me to exploding ReLU grad).\nSo I divide the weight decay by a factor batch_size to keep the change.\nWith this I got up to 85.something% on the test set instead of the 91% claimed by the authors.\nDo you see something wrong immediately ? For instance I feel the weight decay part of the cross entropy is ugly but I could not find any better way of doing that. Do you have some ideas ?\nIf you think that this is not an appropriate place for my question tell me I will remove it but I do not know where to look.", "body": "I posted [this issue with my code](http://stackoverflow.com/questions/35339636/erratic-training-for-all-cnn-on-cifar-10) on stackoverflow a while ago but it has not received any answer or comment.\nThe preprocessing I used is in the dataset class (that I adapted from the Deep MNIST for experts tutorial) and is exactly the same as in the neon/pylearn2 implementation: calculating the mean and Wzca matrix on training set and use it to whiten the data on the training and test set followed by a global contrast normalization step with Goodfellow scale factor of 55.  \nThe only difference of my implementation according to me (I would like to be mistaken) with the paper is the use of `tf.reduce_mean` instead of `tf.reduce_sum` (which led me to exploding ReLU grad).\nSo I divide the weight decay by a factor batch_size to keep the change.  \nWith this I got up to 85.something% on the test set instead of the 91% claimed by the authors.  \nDo you see something wrong immediately ? For instance I feel the weight decay part of the cross entropy is ugly but I could not find any better way of doing that. Do you have some ideas ?  \nIf you think that this is not an appropriate place for my question tell me I will remove it but I do not know where to look.\n"}