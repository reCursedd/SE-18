{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/245836676", "html_url": "https://github.com/tensorflow/tensorflow/issues/4128#issuecomment-245836676", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4128", "id": 245836676, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NTgzNjY3Ng==", "user": {"login": "Hvass-Labs", "id": 13588114, "node_id": "MDQ6VXNlcjEzNTg4MTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/13588114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hvass-Labs", "html_url": "https://github.com/Hvass-Labs", "followers_url": "https://api.github.com/users/Hvass-Labs/followers", "following_url": "https://api.github.com/users/Hvass-Labs/following{/other_user}", "gists_url": "https://api.github.com/users/Hvass-Labs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hvass-Labs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hvass-Labs/subscriptions", "organizations_url": "https://api.github.com/users/Hvass-Labs/orgs", "repos_url": "https://api.github.com/users/Hvass-Labs/repos", "events_url": "https://api.github.com/users/Hvass-Labs/events{/privacy}", "received_events_url": "https://api.github.com/users/Hvass-Labs/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-09T07:07:32Z", "updated_at": "2016-09-09T07:07:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the quick reply.</p>\n<p>(A) As I understand, ImageNet is a data-set used in academic competitions that has been around for several years now. I understand completely that it's a huge achievement to get so good scores on ImageNet. But fact is that the Inception v3 model you include with TensorFlow gives bizarre results when using it on some real-world images that it ought to be able to recognize. I assume the Inception models released last week give equally bizarre results because they were apparently trained on the same ImageNet data-set.</p>\n<p>In my own tutorial that I linked to above, the Inception v3 model classified an image of Elon Musk (299x299 pixels) as maybe being a sweatshirt (20% score) or an abaya (17% score). If I resized the image to 100x100 pixels then Inception said it was maybe a sweatshirt (18%) or a cowboy-boot (16%). Inception said that Gene Wilder portraying Willy Wonka was a bow-tie (97%), and that Johnny Depp portraying Willy Wonka was maybe sunglasses (31%) or sunglass (19%), that is, two very similar class-names.</p>\n<p>I don't know what the problem is. Is it because the ImageNet-classes really belong to a hierarchy, so that sweatshirt is really a sub-class of person, and the same with bow-tie and sunglasses? But that's not something that can be deduced from the data-files with class-names that you have included with the Inception model.</p>\n<p>I would have invited the main author of the Inception model Christian Szegedy to comment on this, but I can't find his GitHub-handle.</p>\n<p>(B) I would love to make a pull-request for this - it would be an honour to make such an important contribution to TensorFlow which is quickly becoming the leading AI library! But I'm a beginner in all this and I still only understand a fraction of TensorFlow, so I think it's better for me to just issue a 'feature-request' instead :-) So please don't get annoyed by this, in the end my questions and suggestions might help improve TensorFlow.</p>\n<p>At the moment, I try to learn the key points about TensorFlow and make tutorials aimed at beginners in both Deep Learning and TensorFlow - there's apparently a lot of us that found the Udacity course rather confusing.</p>", "body_text": "Thanks for the quick reply.\n(A) As I understand, ImageNet is a data-set used in academic competitions that has been around for several years now. I understand completely that it's a huge achievement to get so good scores on ImageNet. But fact is that the Inception v3 model you include with TensorFlow gives bizarre results when using it on some real-world images that it ought to be able to recognize. I assume the Inception models released last week give equally bizarre results because they were apparently trained on the same ImageNet data-set.\nIn my own tutorial that I linked to above, the Inception v3 model classified an image of Elon Musk (299x299 pixels) as maybe being a sweatshirt (20% score) or an abaya (17% score). If I resized the image to 100x100 pixels then Inception said it was maybe a sweatshirt (18%) or a cowboy-boot (16%). Inception said that Gene Wilder portraying Willy Wonka was a bow-tie (97%), and that Johnny Depp portraying Willy Wonka was maybe sunglasses (31%) or sunglass (19%), that is, two very similar class-names.\nI don't know what the problem is. Is it because the ImageNet-classes really belong to a hierarchy, so that sweatshirt is really a sub-class of person, and the same with bow-tie and sunglasses? But that's not something that can be deduced from the data-files with class-names that you have included with the Inception model.\nI would have invited the main author of the Inception model Christian Szegedy to comment on this, but I can't find his GitHub-handle.\n(B) I would love to make a pull-request for this - it would be an honour to make such an important contribution to TensorFlow which is quickly becoming the leading AI library! But I'm a beginner in all this and I still only understand a fraction of TensorFlow, so I think it's better for me to just issue a 'feature-request' instead :-) So please don't get annoyed by this, in the end my questions and suggestions might help improve TensorFlow.\nAt the moment, I try to learn the key points about TensorFlow and make tutorials aimed at beginners in both Deep Learning and TensorFlow - there's apparently a lot of us that found the Udacity course rather confusing.", "body": "Thanks for the quick reply.\n\n(A) As I understand, ImageNet is a data-set used in academic competitions that has been around for several years now. I understand completely that it's a huge achievement to get so good scores on ImageNet. But fact is that the Inception v3 model you include with TensorFlow gives bizarre results when using it on some real-world images that it ought to be able to recognize. I assume the Inception models released last week give equally bizarre results because they were apparently trained on the same ImageNet data-set.\n\nIn my own tutorial that I linked to above, the Inception v3 model classified an image of Elon Musk (299x299 pixels) as maybe being a sweatshirt (20% score) or an abaya (17% score). If I resized the image to 100x100 pixels then Inception said it was maybe a sweatshirt (18%) or a cowboy-boot (16%). Inception said that Gene Wilder portraying Willy Wonka was a bow-tie (97%), and that Johnny Depp portraying Willy Wonka was maybe sunglasses (31%) or sunglass (19%), that is, two very similar class-names.\n\nI don't know what the problem is. Is it because the ImageNet-classes really belong to a hierarchy, so that sweatshirt is really a sub-class of person, and the same with bow-tie and sunglasses? But that's not something that can be deduced from the data-files with class-names that you have included with the Inception model.\n\nI would have invited the main author of the Inception model Christian Szegedy to comment on this, but I can't find his GitHub-handle.\n\n(B) I would love to make a pull-request for this - it would be an honour to make such an important contribution to TensorFlow which is quickly becoming the leading AI library! But I'm a beginner in all this and I still only understand a fraction of TensorFlow, so I think it's better for me to just issue a 'feature-request' instead :-) So please don't get annoyed by this, in the end my questions and suggestions might help improve TensorFlow.\n\nAt the moment, I try to learn the key points about TensorFlow and make tutorials aimed at beginners in both Deep Learning and TensorFlow - there's apparently a lot of us that found the Udacity course rather confusing.\n"}