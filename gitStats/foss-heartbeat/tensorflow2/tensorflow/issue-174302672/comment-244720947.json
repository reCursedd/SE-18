{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244720947", "html_url": "https://github.com/tensorflow/tensorflow/issues/4128#issuecomment-244720947", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4128", "id": 244720947, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDcyMDk0Nw==", "user": {"login": "Hvass-Labs", "id": 13588114, "node_id": "MDQ6VXNlcjEzNTg4MTE0", "avatar_url": "https://avatars2.githubusercontent.com/u/13588114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hvass-Labs", "html_url": "https://github.com/Hvass-Labs", "followers_url": "https://api.github.com/users/Hvass-Labs/followers", "following_url": "https://api.github.com/users/Hvass-Labs/following{/other_user}", "gists_url": "https://api.github.com/users/Hvass-Labs/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hvass-Labs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hvass-Labs/subscriptions", "organizations_url": "https://api.github.com/users/Hvass-Labs/orgs", "repos_url": "https://api.github.com/users/Hvass-Labs/repos", "events_url": "https://api.github.com/users/Hvass-Labs/events{/privacy}", "received_events_url": "https://api.github.com/users/Hvass-Labs/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-05T11:05:53Z", "updated_at": "2016-09-05T11:05:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks again for the quick answer!</p>\n<h2>(1)</h2>\n<p>I suppose that when the Inception model outputs a class-number of zero it really means \"other\" and class-numbers between 1001-1007 are for back-compatibility. But I think the code breaks if the top-5 includes one of these special classes, because there is no mapping from these special class-numbers to ImageNet uid's in the file <code>imagenet_2012_challenge_label_map_proto.pbtxt</code> I think a comment in the code would be good.</p>\n<h2>(2)</h2>\n<p>Actually it is much more confusing to omit the information on image rescaling; the file <code>classify_image.py</code> doesn't even mention that images are rescaled to 299x299 pixels. I looked in the C++ file you linked to and it appears to be using <code>ResizeBilinear()</code>, which I can't really find in the source-code, but I guess it's bilinear interpolation. But it's still not clear whether the images are squeezed, cropped or padded if they're not exactly square. I tried doing the resizing manually in an image editor and using these images as input to the Inception model. With this image the Inception model does pretty well in most variants, but you can easily imagine that if e.g. a very wide or very high image is squeezed then the Inception model will no longer perform as well. Similarly with cropping where e.g. an essential part of the image is removed. Perhaps the best solution would be to pad the image so it's square and then resize?</p>\n<h3>Original image</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/13588114/18245844/e5765950-7367-11e6-83cc-2f9b378125b4.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/13588114/18245844/e5765950-7367-11e6-83cc-2f9b378125b4.jpg\" alt=\"parrot\" style=\"max-width:100%;\"></a></p>\n<h3>Squeezed image, Inception result: Macaw 97.0%</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/13588114/18245414/5f6c6252-7365-11e6-9cf5-aa20565379bf.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/13588114/18245414/5f6c6252-7365-11e6-9cf5-aa20565379bf.jpg\" alt=\"parrot_squeezed\" style=\"max-width:100%;\"></a></p>\n<h3>Cropped image version 1, Inception result: Macaw 97.4%</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/13588114/18245415/5f7460e2-7365-11e6-9f33-35e8143ba7e2.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/13588114/18245415/5f7460e2-7365-11e6-9f33-35e8143ba7e2.jpg\" alt=\"parrot_cropped1\" style=\"max-width:100%;\"></a></p>\n<h3>Cropped image version 2, Inception result: Macaw 93.9%</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/13588114/18245416/5f76646e-7365-11e6-9104-019fa5c7ca74.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/13588114/18245416/5f76646e-7365-11e6-9104-019fa5c7ca74.jpg\" alt=\"parrot_cropped2\" style=\"max-width:100%;\"></a></p>\n<h3>Cropped image version 3, Inception result: Jacamar (another bird) 26.1%, grasshopper 10.6%</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/13588114/18245418/5f799472-7365-11e6-8f26-39a0d07511d8.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/13588114/18245418/5f799472-7365-11e6-8f26-39a0d07511d8.jpg\" alt=\"parrot_cropped3\" style=\"max-width:100%;\"></a></p>\n<h3>Padded image, Inception result: Macaw 96.8%</h3>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/13588114/18245417/5f7821fa-7365-11e6-8245-35833b23851d.jpg\"><img src=\"https://cloud.githubusercontent.com/assets/13588114/18245417/5f7821fa-7365-11e6-8245-35833b23851d.jpg\" alt=\"parrot_padded\" style=\"max-width:100%;\"></a></p>\n<h2>(3 + more)</h2>\n<p>Thanks for the tip on the new Inception models. However, they're not reloaded in the same way as the Inception tutorial. It appears that you have to import the .py files, create a graph using those functions, and then reload the checkpoints in the tar-files. The Inception tutorial uses a frozen graph instead.</p>\n<p>I think it would be great if you had a standardized way of creating neural networks from a model-zoo. This ought to be a part of the TensorFlow API. The API could look something like this:</p>\n<pre><code>model3 = tf.models.Inception3(auto_download=True)\n\nmodel4 = tf.models.Inception4(auto_download=True)\n</code></pre>\n<p>These functions would automatically download a graph and checkpoint-file for each Inception model that was working with the user's TensorFlow version (or perhaps just the latest TF version), and then return a model-object which inherits from <code>class ModelZoo</code> and provides the following functions:</p>\n<pre><code>model3.get_graph()  # Returns the graph for the neural network.\nmodel3.get_placeholder_name()  # Name of placeholder-variable for inputting image.\nmodel3.get_softmax()  # Return tensor for softmax-classifier.\nmodel3.get_last_layer()  # Return tensor for last layer for re-training.\nmodel3.get_class_names(class_numbers)  # Return names of the given classes.\nmodel3.inference(session=None, images)  # Perform inference on the list of images.\n                                        # Create new session if None is supplied.\nmodel3.print_scores(predictions)  # Print scores and class-names for predicted classes.\n</code></pre>\n<p>This would allow users to do inference with only a few lines of code:</p>\n<pre><code>model = tf.models.Inception3(auto_download=True)\npredicted_labels = model.inference(images=my_images)\n</code></pre>\n<p>Perhaps this is what you want to do with the Slim library? I've looked at the following Notebook which is mostly well-documented (yay, finally some TensorFlow code with lots of comments! The author is apparently <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=21178140\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nathansilberman\">@nathansilberman</a>):</p>\n<p><a href=\"https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\">https://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb</a></p>\n<p>However, it is still quite complicated and I think a model zoo can be made even simpler with an API such as the above.</p>", "body_text": "Thanks again for the quick answer!\n(1)\nI suppose that when the Inception model outputs a class-number of zero it really means \"other\" and class-numbers between 1001-1007 are for back-compatibility. But I think the code breaks if the top-5 includes one of these special classes, because there is no mapping from these special class-numbers to ImageNet uid's in the file imagenet_2012_challenge_label_map_proto.pbtxt I think a comment in the code would be good.\n(2)\nActually it is much more confusing to omit the information on image rescaling; the file classify_image.py doesn't even mention that images are rescaled to 299x299 pixels. I looked in the C++ file you linked to and it appears to be using ResizeBilinear(), which I can't really find in the source-code, but I guess it's bilinear interpolation. But it's still not clear whether the images are squeezed, cropped or padded if they're not exactly square. I tried doing the resizing manually in an image editor and using these images as input to the Inception model. With this image the Inception model does pretty well in most variants, but you can easily imagine that if e.g. a very wide or very high image is squeezed then the Inception model will no longer perform as well. Similarly with cropping where e.g. an essential part of the image is removed. Perhaps the best solution would be to pad the image so it's square and then resize?\nOriginal image\n\nSqueezed image, Inception result: Macaw 97.0%\n\nCropped image version 1, Inception result: Macaw 97.4%\n\nCropped image version 2, Inception result: Macaw 93.9%\n\nCropped image version 3, Inception result: Jacamar (another bird) 26.1%, grasshopper 10.6%\n\nPadded image, Inception result: Macaw 96.8%\n\n(3 + more)\nThanks for the tip on the new Inception models. However, they're not reloaded in the same way as the Inception tutorial. It appears that you have to import the .py files, create a graph using those functions, and then reload the checkpoints in the tar-files. The Inception tutorial uses a frozen graph instead.\nI think it would be great if you had a standardized way of creating neural networks from a model-zoo. This ought to be a part of the TensorFlow API. The API could look something like this:\nmodel3 = tf.models.Inception3(auto_download=True)\n\nmodel4 = tf.models.Inception4(auto_download=True)\n\nThese functions would automatically download a graph and checkpoint-file for each Inception model that was working with the user's TensorFlow version (or perhaps just the latest TF version), and then return a model-object which inherits from class ModelZoo and provides the following functions:\nmodel3.get_graph()  # Returns the graph for the neural network.\nmodel3.get_placeholder_name()  # Name of placeholder-variable for inputting image.\nmodel3.get_softmax()  # Return tensor for softmax-classifier.\nmodel3.get_last_layer()  # Return tensor for last layer for re-training.\nmodel3.get_class_names(class_numbers)  # Return names of the given classes.\nmodel3.inference(session=None, images)  # Perform inference on the list of images.\n                                        # Create new session if None is supplied.\nmodel3.print_scores(predictions)  # Print scores and class-names for predicted classes.\n\nThis would allow users to do inference with only a few lines of code:\nmodel = tf.models.Inception3(auto_download=True)\npredicted_labels = model.inference(images=my_images)\n\nPerhaps this is what you want to do with the Slim library? I've looked at the following Notebook which is mostly well-documented (yay, finally some TensorFlow code with lots of comments! The author is apparently @nathansilberman):\nhttps://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\nHowever, it is still quite complicated and I think a model zoo can be made even simpler with an API such as the above.", "body": "Thanks again for the quick answer!\n\n## (1)\n\nI suppose that when the Inception model outputs a class-number of zero it really means \"other\" and class-numbers between 1001-1007 are for back-compatibility. But I think the code breaks if the top-5 includes one of these special classes, because there is no mapping from these special class-numbers to ImageNet uid's in the file `imagenet_2012_challenge_label_map_proto.pbtxt` I think a comment in the code would be good.\n\n## (2)\n\nActually it is much more confusing to omit the information on image rescaling; the file `classify_image.py` doesn't even mention that images are rescaled to 299x299 pixels. I looked in the C++ file you linked to and it appears to be using `ResizeBilinear()`, which I can't really find in the source-code, but I guess it's bilinear interpolation. But it's still not clear whether the images are squeezed, cropped or padded if they're not exactly square. I tried doing the resizing manually in an image editor and using these images as input to the Inception model. With this image the Inception model does pretty well in most variants, but you can easily imagine that if e.g. a very wide or very high image is squeezed then the Inception model will no longer perform as well. Similarly with cropping where e.g. an essential part of the image is removed. Perhaps the best solution would be to pad the image so it's square and then resize?\n\n### Original image\n\n![parrot](https://cloud.githubusercontent.com/assets/13588114/18245844/e5765950-7367-11e6-83cc-2f9b378125b4.jpg)\n\n### Squeezed image, Inception result: Macaw 97.0%\n\n![parrot_squeezed](https://cloud.githubusercontent.com/assets/13588114/18245414/5f6c6252-7365-11e6-9cf5-aa20565379bf.jpg)\n\n### Cropped image version 1, Inception result: Macaw 97.4%\n\n![parrot_cropped1](https://cloud.githubusercontent.com/assets/13588114/18245415/5f7460e2-7365-11e6-9f33-35e8143ba7e2.jpg)\n\n### Cropped image version 2, Inception result: Macaw 93.9%\n\n![parrot_cropped2](https://cloud.githubusercontent.com/assets/13588114/18245416/5f76646e-7365-11e6-9104-019fa5c7ca74.jpg)\n\n### Cropped image version 3, Inception result: Jacamar (another bird) 26.1%, grasshopper 10.6%\n\n![parrot_cropped3](https://cloud.githubusercontent.com/assets/13588114/18245418/5f799472-7365-11e6-8f26-39a0d07511d8.jpg)\n\n### Padded image, Inception result: Macaw 96.8%\n\n![parrot_padded](https://cloud.githubusercontent.com/assets/13588114/18245417/5f7821fa-7365-11e6-8245-35833b23851d.jpg)\n\n## (3 + more)\n\nThanks for the tip on the new Inception models. However, they're not reloaded in the same way as the Inception tutorial. It appears that you have to import the .py files, create a graph using those functions, and then reload the checkpoints in the tar-files. The Inception tutorial uses a frozen graph instead.\n\nI think it would be great if you had a standardized way of creating neural networks from a model-zoo. This ought to be a part of the TensorFlow API. The API could look something like this:\n\n```\nmodel3 = tf.models.Inception3(auto_download=True)\n\nmodel4 = tf.models.Inception4(auto_download=True)\n```\n\nThese functions would automatically download a graph and checkpoint-file for each Inception model that was working with the user's TensorFlow version (or perhaps just the latest TF version), and then return a model-object which inherits from `class ModelZoo` and provides the following functions:\n\n```\nmodel3.get_graph()  # Returns the graph for the neural network.\nmodel3.get_placeholder_name()  # Name of placeholder-variable for inputting image.\nmodel3.get_softmax()  # Return tensor for softmax-classifier.\nmodel3.get_last_layer()  # Return tensor for last layer for re-training.\nmodel3.get_class_names(class_numbers)  # Return names of the given classes.\nmodel3.inference(session=None, images)  # Perform inference on the list of images.\n                                        # Create new session if None is supplied.\nmodel3.print_scores(predictions)  # Print scores and class-names for predicted classes.\n```\n\nThis would allow users to do inference with only a few lines of code:\n\n```\nmodel = tf.models.Inception3(auto_download=True)\npredicted_labels = model.inference(images=my_images)\n```\n\nPerhaps this is what you want to do with the Slim library? I've looked at the following Notebook which is mostly well-documented (yay, finally some TensorFlow code with lots of comments! The author is apparently @nathansilberman):\n\nhttps://github.com/tensorflow/models/blob/master/slim/slim_walkthough.ipynb\n\nHowever, it is still quite complicated and I think a model zoo can be made even simpler with an API such as the above.\n"}