{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17994", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17994/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17994/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17994/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17994", "id": 308403948, "node_id": "MDU6SXNzdWUzMDg0MDM5NDg=", "number": 17994, "title": "tensorflow-gpu OOM with Geforce GTX980", "user": {"login": "Hunterwolf88", "id": 8464845, "node_id": "MDQ6VXNlcjg0NjQ4NDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/8464845?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hunterwolf88", "html_url": "https://github.com/Hunterwolf88", "followers_url": "https://api.github.com/users/Hunterwolf88/followers", "following_url": "https://api.github.com/users/Hunterwolf88/following{/other_user}", "gists_url": "https://api.github.com/users/Hunterwolf88/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hunterwolf88/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hunterwolf88/subscriptions", "organizations_url": "https://api.github.com/users/Hunterwolf88/orgs", "repos_url": "https://api.github.com/users/Hunterwolf88/repos", "events_url": "https://api.github.com/users/Hunterwolf88/events{/privacy}", "received_events_url": "https://api.github.com/users/Hunterwolf88/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-03-26T00:59:45Z", "updated_at": "2018-05-09T16:40:11Z", "closed_at": "2018-04-10T18:44:17Z", "author_association": "NONE", "body_html": "<p>Hello there!<br>\nSorry for the noob question, I'm new in the world of deep learning, especially with GPU processing.</p>\n<p>I'm trying to run Tensorflow-GPU but probably I'm doing something wrong.<br>\nI tried many scripts with always the same result without any solution.<br>\nI think I installed Cuda and cuDNN properly, as well tensorflow.</p>\n<p>This is my system:<br>\nUbuntu 16.04<br>\nGPU: NVIDIA Geforce GTX 980<br>\nRAM: 16 GB<br>\nCuda 9.0, cuDNN 7.0.</p>\n<p>conda environment in the example reported below:<br>\npython==3.6<br>\npathlib==1.0.1<br>\nscandir==1.6<br>\nh5py==2.7.1<br>\nKeras==2.1.2<br>\nopencv-python==3.3.0.10<br>\ntensorflow-gpu==1.5.0<br>\nscikit-image<br>\ndlib<br>\nface_recognition<br>\ntqdm</p>\n<p>and basically this is what I get everytime I try to start some kind of training. In this example faceswap.py training with <a href=\"https://github.com/deepfakes/faceswap\">https://github.com/deepfakes/faceswap</a>.<br>\nI can't see any change on nvidia-smi while this is going on, so I think the GPU is not actually used.</p>\n<p>Thank you in advance for any kind of help, I'm going mad about this :(</p>\n<pre><code>Loading Trainer from Model_Original plugin...\nStarting. Press \"Enter\" to stop training and save model\n2018-03-26 02:29:53.146441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-03-26 02:29:53.146998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-03-26 02:29:53.147230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.291\npciBusID: 0000:01:00.0\ntotalMemory: 3.95GiB freeMemory: 2.72GiB\n2018-03-26 02:29:53.147248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2)\n2018-03-26 02:29:54.731329: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.21GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.788791: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.20GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.832066: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 288.00MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.832100: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.29GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.837129: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 220.50MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:30:04.837415: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 3.00MiB**.  Current allocation summary follows.\n2018-03-26 02:30:04.837533: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): \tTotal Chunks: 49, Chunks in use: 48. 12.2KiB allocated for chunks. 12.0KiB in use in bin. 504B client-requested in use in bin.\n2018-03-26 02:30:04.837564: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): \tTotal Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837593: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): \tTotal Chunks: 13, Chunks in use: 13. 13.2KiB allocated for chunks. 13.2KiB in use in bin. 13.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837620: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): \tTotal Chunks: 12, Chunks in use: 12. 25.5KiB allocated for chunks. 25.5KiB in use in bin. 24.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837646: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): \tTotal Chunks: 17, Chunks in use: 17. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837673: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): \tTotal Chunks: 6, Chunks in use: 6. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837700: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): \tTotal Chunks: 9, Chunks in use: 8. 168.8KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837727: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): \tTotal Chunks: 6, Chunks in use: 6. 225.0KiB allocated for chunks. 225.0KiB in use in bin. 225.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837755: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837779: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.837817: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837854: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 597.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.837892: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): \tTotal Chunks: 6, Chunks in use: 6. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n2018-03-26 02:30:04.837931: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): \tTotal Chunks: 8, Chunks in use: 8. 23.62MiB allocated for chunks. 23.62MiB in use in bin. 22.75MiB client-requested in use in bin.\n2018-03-26 02:30:04.837971: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): \tTotal Chunks: 13, Chunks in use: 13. 58.25MiB allocated for chunks. 58.25MiB in use in bin. 53.62MiB client-requested in use in bin.\n2018-03-26 02:30:04.838012: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): \tTotal Chunks: 12, Chunks in use: 12. 123.00MiB allocated for chunks. 123.00MiB in use in bin. 123.00MiB client-requested in use in bin.\n2018-03-26 02:30:04.838046: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): \tTotal Chunks: 13, Chunks in use: 13. 222.00MiB allocated for chunks. 222.00MiB in use in bin. 222.00MiB client-requested in use in bin.\n2018-03-26 02:30:04.838075: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): \tTotal Chunks: 11, Chunks in use: 11. 461.50MiB allocated for chunks. 461.50MiB in use in bin. 442.00MiB client-requested in use in bin.\n2018-03-26 02:30:04.838100: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): \tTotal Chunks: 23, Chunks in use: 23. 1.50GiB allocated for chunks. 1.50GiB in use in bin. 1.47GiB client-requested in use in bin.\n2018-03-26 02:30:04.838124: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.838147: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.838172: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 3.00MiB was 2.00MiB, Chunk State: \n2018-03-26 02:30:04.838203: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0000 of size 1280\n2018-03-26 02:30:04.838229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0500 of size 256\n2018-03-26 02:30:04.838254: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0600 of size 256\n[...]\n2018-03-26 02:30:04.842321: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7949c2300 of size 67108864\n2018-03-26 02:30:04.842337: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7989c2300 of size 67108864\n2018-03-26 02:30:04.842353: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79c9c2300 of size 3145728\n2018-03-26 02:30:04.842371: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79ccc2300 of size 79944960\n2018-03-26 02:30:04.842390: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ed32200 of size 19200\n2018-03-26 02:30:04.842407: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72edaed00 of size 611328\n2018-03-26 02:30:04.842423: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ee45700 of size 256\n2018-03-26 02:30:04.842442: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: \n2018-03-26 02:30:04.842469: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 48 Chunks of size 256 totalling 12.0KiB\n2018-03-26 02:30:04.842490: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 512 totalling 3.0KiB\n2018-03-26 02:30:04.842509: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 12 Chunks of size 1024 totalling 12.0KiB\n2018-03-26 02:30:04.842527: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB\n2018-03-26 02:30:04.842547: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 11 Chunks of size 2048 totalling 22.0KiB\n2018-03-26 02:30:04.842565: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 3584 totalling 3.5KiB\n2018-03-26 02:30:04.842585: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 17 Chunks of size 4096 totalling 68.0KiB\n2018-03-26 02:30:04.842604: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 8192 totalling 48.0KiB\n[...]\n2018-03-26 02:30:04.843025: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 75497472 totalling 504.00MiB\n2018-03-26 02:30:04.843044: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 79944960 totalling 76.24MiB\n2018-03-26 02:30:04.843062: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 2.38GiB\n2018-03-26 02:30:04.843086: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: \nLimit:                  2555510784\nInUse:                  2554880000\nMaxInUse:               2554880256\nNumAllocs:                     303\nMaxAllocSize:            364109056\n\n2018-03-26 02:30:04.843151: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************\n2018-03-26 02:30:04.843199: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: **OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc**\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\n    return fn(*args)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\n    status, run_metadata)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 181, in processThread\n    raise e\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 161, in processThread\n    trainer.train_one_step(epoch, self.show if (save_iteration or self.save_now) else None)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Trainer.py\", line 26, in train_one_step\n    loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/training.py\", line 1839, in train_on_batch\n    outputs = self.train_function(ins)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\n    **self.session_kwargs)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\n    options, run_metadata)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'model_2/conv2d_9/convolution', defined at:\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 884, in _bootstrap\n    self._bootstrap_inner()\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 147, in processThread\n    model = PluginLoader.get_model(trainer)(get_folder(self.arguments.model_dir), self.arguments.gpus)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/AutoEncoder.py\", line 16, in __init__\n    self.initModel()\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Model.py\", line 22, in initModel\n    self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2061, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2212, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3195, in conv2d\n    data_format=tf_data_format)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 754, in convolution\n    return op(input, filter)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 838, in __call__\n    return self.conv_op(inp, filter)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 502, in __call__\n    return self.call(inp, filter)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 190, in __call__\n    name=self.name)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 639, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n[[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n[[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n</code></pre>", "body_text": "Hello there!\nSorry for the noob question, I'm new in the world of deep learning, especially with GPU processing.\nI'm trying to run Tensorflow-GPU but probably I'm doing something wrong.\nI tried many scripts with always the same result without any solution.\nI think I installed Cuda and cuDNN properly, as well tensorflow.\nThis is my system:\nUbuntu 16.04\nGPU: NVIDIA Geforce GTX 980\nRAM: 16 GB\nCuda 9.0, cuDNN 7.0.\nconda environment in the example reported below:\npython==3.6\npathlib==1.0.1\nscandir==1.6\nh5py==2.7.1\nKeras==2.1.2\nopencv-python==3.3.0.10\ntensorflow-gpu==1.5.0\nscikit-image\ndlib\nface_recognition\ntqdm\nand basically this is what I get everytime I try to start some kind of training. In this example faceswap.py training with https://github.com/deepfakes/faceswap.\nI can't see any change on nvidia-smi while this is going on, so I think the GPU is not actually used.\nThank you in advance for any kind of help, I'm going mad about this :(\nLoading Trainer from Model_Original plugin...\nStarting. Press \"Enter\" to stop training and save model\n2018-03-26 02:29:53.146441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2018-03-26 02:29:53.146998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-03-26 02:29:53.147230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.291\npciBusID: 0000:01:00.0\ntotalMemory: 3.95GiB freeMemory: 2.72GiB\n2018-03-26 02:29:53.147248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2)\n2018-03-26 02:29:54.731329: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.21GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.788791: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.20GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.832066: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 288.00MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.832100: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.29GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:29:54.837129: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 220.50MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\n2018-03-26 02:30:04.837415: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 3.00MiB**.  Current allocation summary follows.\n2018-03-26 02:30:04.837533: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): \tTotal Chunks: 49, Chunks in use: 48. 12.2KiB allocated for chunks. 12.0KiB in use in bin. 504B client-requested in use in bin.\n2018-03-26 02:30:04.837564: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): \tTotal Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837593: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): \tTotal Chunks: 13, Chunks in use: 13. 13.2KiB allocated for chunks. 13.2KiB in use in bin. 13.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837620: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): \tTotal Chunks: 12, Chunks in use: 12. 25.5KiB allocated for chunks. 25.5KiB in use in bin. 24.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837646: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): \tTotal Chunks: 17, Chunks in use: 17. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837673: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): \tTotal Chunks: 6, Chunks in use: 6. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837700: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): \tTotal Chunks: 9, Chunks in use: 8. 168.8KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837727: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): \tTotal Chunks: 6, Chunks in use: 6. 225.0KiB allocated for chunks. 225.0KiB in use in bin. 225.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837755: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837779: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.837817: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.\n2018-03-26 02:30:04.837854: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 597.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.837892: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): \tTotal Chunks: 6, Chunks in use: 6. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\n2018-03-26 02:30:04.837931: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): \tTotal Chunks: 8, Chunks in use: 8. 23.62MiB allocated for chunks. 23.62MiB in use in bin. 22.75MiB client-requested in use in bin.\n2018-03-26 02:30:04.837971: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): \tTotal Chunks: 13, Chunks in use: 13. 58.25MiB allocated for chunks. 58.25MiB in use in bin. 53.62MiB client-requested in use in bin.\n2018-03-26 02:30:04.838012: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): \tTotal Chunks: 12, Chunks in use: 12. 123.00MiB allocated for chunks. 123.00MiB in use in bin. 123.00MiB client-requested in use in bin.\n2018-03-26 02:30:04.838046: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): \tTotal Chunks: 13, Chunks in use: 13. 222.00MiB allocated for chunks. 222.00MiB in use in bin. 222.00MiB client-requested in use in bin.\n2018-03-26 02:30:04.838075: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): \tTotal Chunks: 11, Chunks in use: 11. 461.50MiB allocated for chunks. 461.50MiB in use in bin. 442.00MiB client-requested in use in bin.\n2018-03-26 02:30:04.838100: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): \tTotal Chunks: 23, Chunks in use: 23. 1.50GiB allocated for chunks. 1.50GiB in use in bin. 1.47GiB client-requested in use in bin.\n2018-03-26 02:30:04.838124: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.838147: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n2018-03-26 02:30:04.838172: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 3.00MiB was 2.00MiB, Chunk State: \n2018-03-26 02:30:04.838203: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0000 of size 1280\n2018-03-26 02:30:04.838229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0500 of size 256\n2018-03-26 02:30:04.838254: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0600 of size 256\n[...]\n2018-03-26 02:30:04.842321: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7949c2300 of size 67108864\n2018-03-26 02:30:04.842337: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7989c2300 of size 67108864\n2018-03-26 02:30:04.842353: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79c9c2300 of size 3145728\n2018-03-26 02:30:04.842371: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79ccc2300 of size 79944960\n2018-03-26 02:30:04.842390: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ed32200 of size 19200\n2018-03-26 02:30:04.842407: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72edaed00 of size 611328\n2018-03-26 02:30:04.842423: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ee45700 of size 256\n2018-03-26 02:30:04.842442: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: \n2018-03-26 02:30:04.842469: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 48 Chunks of size 256 totalling 12.0KiB\n2018-03-26 02:30:04.842490: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 512 totalling 3.0KiB\n2018-03-26 02:30:04.842509: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 12 Chunks of size 1024 totalling 12.0KiB\n2018-03-26 02:30:04.842527: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB\n2018-03-26 02:30:04.842547: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 11 Chunks of size 2048 totalling 22.0KiB\n2018-03-26 02:30:04.842565: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 3584 totalling 3.5KiB\n2018-03-26 02:30:04.842585: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 17 Chunks of size 4096 totalling 68.0KiB\n2018-03-26 02:30:04.842604: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 8192 totalling 48.0KiB\n[...]\n2018-03-26 02:30:04.843025: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 75497472 totalling 504.00MiB\n2018-03-26 02:30:04.843044: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 79944960 totalling 76.24MiB\n2018-03-26 02:30:04.843062: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 2.38GiB\n2018-03-26 02:30:04.843086: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: \nLimit:                  2555510784\nInUse:                  2554880000\nMaxInUse:               2554880256\nNumAllocs:                     303\nMaxAllocSize:            364109056\n\n2018-03-26 02:30:04.843151: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************\n2018-03-26 02:30:04.843199: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: **OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc**\nException in thread Thread-1:\nTraceback (most recent call last):\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\n    return fn(*args)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\n    status, run_metadata)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 181, in processThread\n    raise e\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 161, in processThread\n    trainer.train_one_step(epoch, self.show if (save_iteration or self.save_now) else None)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Trainer.py\", line 26, in train_one_step\n    loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/training.py\", line 1839, in train_on_batch\n    outputs = self.train_function(ins)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\n    **self.session_kwargs)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\n    run_metadata_ptr)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\n    options, run_metadata)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'model_2/conv2d_9/convolution', defined at:\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 884, in _bootstrap\n    self._bootstrap_inner()\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n    self.run()\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 147, in processThread\n    model = PluginLoader.get_model(trainer)(get_folder(self.arguments.model_dir), self.arguments.gpus)\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/AutoEncoder.py\", line 16, in __init__\n    self.initModel()\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Model.py\", line 22, in initModel\n    self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2061, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2212, in run_internal_graph\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 164, in call\n    dilation_rate=self.dilation_rate)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3195, in conv2d\n    data_format=tf_data_format)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 754, in convolution\n    return op(input, filter)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 838, in __call__\n    return self.conv_op(inp, filter)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 502, in __call__\n    return self.call(inp, filter)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 190, in __call__\n    name=self.name)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 639, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n[[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n[[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.", "body": "Hello there!\r\nSorry for the noob question, I'm new in the world of deep learning, especially with GPU processing.\r\n\r\nI'm trying to run Tensorflow-GPU but probably I'm doing something wrong.\r\nI tried many scripts with always the same result without any solution.\r\nI think I installed Cuda and cuDNN properly, as well tensorflow.\r\n\r\nThis is my system:\r\nUbuntu 16.04\r\nGPU: NVIDIA Geforce GTX 980\r\nRAM: 16 GB\r\nCuda 9.0, cuDNN 7.0.\r\n\r\nconda environment in the example reported below:\r\npython==3.6\r\npathlib==1.0.1\r\nscandir==1.6\r\nh5py==2.7.1\r\nKeras==2.1.2\r\nopencv-python==3.3.0.10\r\ntensorflow-gpu==1.5.0\r\nscikit-image\r\ndlib\r\nface_recognition\r\ntqdm\r\n\r\nand basically this is what I get everytime I try to start some kind of training. In this example faceswap.py training with https://github.com/deepfakes/faceswap.\r\nI can't see any change on nvidia-smi while this is going on, so I think the GPU is not actually used.\r\n\r\nThank you in advance for any kind of help, I'm going mad about this :(\r\n\r\n\r\n```\r\nLoading Trainer from Model_Original plugin...\r\nStarting. Press \"Enter\" to stop training and save model\r\n2018-03-26 02:29:53.146441: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2018-03-26 02:29:53.146998: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:895] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-03-26 02:29:53.147230: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1105] Found device 0 with properties: \r\nname: GeForce GTX 980 major: 5 minor: 2 memoryClockRate(GHz): 1.291\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 3.95GiB freeMemory: 2.72GiB\r\n2018-03-26 02:29:53.147248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1195] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0, compute capability: 5.2)\r\n2018-03-26 02:29:54.731329: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.21GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.788791: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.20GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.832066: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 288.00MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.832100: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 1.29GiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:29:54.837129: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 220.50MiB**. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.\r\n2018-03-26 02:30:04.837415: W tensorflow/core/common_runtime/bfc_allocator.cc:273] Allocator (GPU_0_bfc) **ran out of memory trying to allocate 3.00MiB**.  Current allocation summary follows.\r\n2018-03-26 02:30:04.837533: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (256): \tTotal Chunks: 49, Chunks in use: 48. 12.2KiB allocated for chunks. 12.0KiB in use in bin. 504B client-requested in use in bin.\r\n2018-03-26 02:30:04.837564: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (512): \tTotal Chunks: 6, Chunks in use: 6. 3.0KiB allocated for chunks. 3.0KiB in use in bin. 3.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837593: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1024): \tTotal Chunks: 13, Chunks in use: 13. 13.2KiB allocated for chunks. 13.2KiB in use in bin. 13.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837620: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2048): \tTotal Chunks: 12, Chunks in use: 12. 25.5KiB allocated for chunks. 25.5KiB in use in bin. 24.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837646: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4096): \tTotal Chunks: 17, Chunks in use: 17. 68.0KiB allocated for chunks. 68.0KiB in use in bin. 68.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837673: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8192): \tTotal Chunks: 6, Chunks in use: 6. 48.0KiB allocated for chunks. 48.0KiB in use in bin. 48.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837700: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16384): \tTotal Chunks: 9, Chunks in use: 8. 168.8KiB allocated for chunks. 150.0KiB in use in bin. 150.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837727: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (32768): \tTotal Chunks: 6, Chunks in use: 6. 225.0KiB allocated for chunks. 225.0KiB in use in bin. 225.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837755: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (65536): \tTotal Chunks: 6, Chunks in use: 6. 384.0KiB allocated for chunks. 384.0KiB in use in bin. 384.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837779: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (131072): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.837817: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 256.0KiB allocated for chunks. 256.0KiB in use in bin. 256.0KiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837854: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (524288): \tTotal Chunks: 1, Chunks in use: 0. 597.0KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.837892: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (1048576): \tTotal Chunks: 6, Chunks in use: 6. 6.75MiB allocated for chunks. 6.75MiB in use in bin. 6.75MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837931: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (2097152): \tTotal Chunks: 8, Chunks in use: 8. 23.62MiB allocated for chunks. 23.62MiB in use in bin. 22.75MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.837971: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (4194304): \tTotal Chunks: 13, Chunks in use: 13. 58.25MiB allocated for chunks. 58.25MiB in use in bin. 53.62MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838012: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (8388608): \tTotal Chunks: 12, Chunks in use: 12. 123.00MiB allocated for chunks. 123.00MiB in use in bin. 123.00MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838046: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (16777216): \tTotal Chunks: 13, Chunks in use: 13. 222.00MiB allocated for chunks. 222.00MiB in use in bin. 222.00MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838075: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (33554432): \tTotal Chunks: 11, Chunks in use: 11. 461.50MiB allocated for chunks. 461.50MiB in use in bin. 442.00MiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838100: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (67108864): \tTotal Chunks: 23, Chunks in use: 23. 1.50GiB allocated for chunks. 1.50GiB in use in bin. 1.47GiB client-requested in use in bin.\r\n2018-03-26 02:30:04.838124: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (134217728): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.838147: I tensorflow/core/common_runtime/bfc_allocator.cc:628] Bin (268435456): \tTotal Chunks: 0, Chunks in use: 0. 0B allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\r\n2018-03-26 02:30:04.838172: I tensorflow/core/common_runtime/bfc_allocator.cc:644] Bin for 3.00MiB was 2.00MiB, Chunk State: \r\n2018-03-26 02:30:04.838203: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0000 of size 1280\r\n2018-03-26 02:30:04.838229: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0500 of size 256\r\n2018-03-26 02:30:04.838254: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7093e0600 of size 256\r\n[...]\r\n2018-03-26 02:30:04.842321: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7949c2300 of size 67108864\r\n2018-03-26 02:30:04.842337: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x7989c2300 of size 67108864\r\n2018-03-26 02:30:04.842353: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79c9c2300 of size 3145728\r\n2018-03-26 02:30:04.842371: I tensorflow/core/common_runtime/bfc_allocator.cc:662] Chunk at 0x79ccc2300 of size 79944960\r\n2018-03-26 02:30:04.842390: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ed32200 of size 19200\r\n2018-03-26 02:30:04.842407: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72edaed00 of size 611328\r\n2018-03-26 02:30:04.842423: I tensorflow/core/common_runtime/bfc_allocator.cc:671] Free at 0x72ee45700 of size 256\r\n2018-03-26 02:30:04.842442: I tensorflow/core/common_runtime/bfc_allocator.cc:677]      Summary of in-use Chunks by size: \r\n2018-03-26 02:30:04.842469: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 48 Chunks of size 256 totalling 12.0KiB\r\n2018-03-26 02:30:04.842490: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 512 totalling 3.0KiB\r\n2018-03-26 02:30:04.842509: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 12 Chunks of size 1024 totalling 12.0KiB\r\n2018-03-26 02:30:04.842527: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 1280 totalling 1.2KiB\r\n2018-03-26 02:30:04.842547: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 11 Chunks of size 2048 totalling 22.0KiB\r\n2018-03-26 02:30:04.842565: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 3584 totalling 3.5KiB\r\n2018-03-26 02:30:04.842585: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 17 Chunks of size 4096 totalling 68.0KiB\r\n2018-03-26 02:30:04.842604: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 6 Chunks of size 8192 totalling 48.0KiB\r\n[...]\r\n2018-03-26 02:30:04.843025: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 7 Chunks of size 75497472 totalling 504.00MiB\r\n2018-03-26 02:30:04.843044: I tensorflow/core/common_runtime/bfc_allocator.cc:680] 1 Chunks of size 79944960 totalling 76.24MiB\r\n2018-03-26 02:30:04.843062: I tensorflow/core/common_runtime/bfc_allocator.cc:684] Sum Total of in-use chunks: 2.38GiB\r\n2018-03-26 02:30:04.843086: I tensorflow/core/common_runtime/bfc_allocator.cc:686] Stats: \r\nLimit:                  2555510784\r\nInUse:                  2554880000\r\nMaxInUse:               2554880256\r\nNumAllocs:                     303\r\nMaxAllocSize:            364109056\r\n\r\n2018-03-26 02:30:04.843151: W tensorflow/core/common_runtime/bfc_allocator.cc:277] ****************************************************************************************************\r\n2018-03-26 02:30:04.843199: W tensorflow/core/framework/op_kernel.cc:1198] Resource exhausted: **OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc**\r\nException in thread Thread-1:\r\nTraceback (most recent call last):\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1350, in _do_call\r\n    return fn(*args)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1329, in _run_fn\r\n    status, run_metadata)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 181, in processThread\r\n    raise e\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 161, in processThread\r\n    trainer.train_one_step(epoch, self.show if (save_iteration or self.save_now) else None)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Trainer.py\", line 26, in train_one_step\r\n    loss_A = self.model.autoencoder_A.train_on_batch(warped_A, target_A)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/training.py\", line 1839, in train_on_batch\r\n    outputs = self.train_function(ins)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 2357, in __call__\r\n    **self.session_kwargs)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 895, in run\r\n    run_metadata_ptr)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1128, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1344, in _do_run\r\n    options, run_metadata)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1363, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n\t [[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\t [[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n\r\nCaused by op 'model_2/conv2d_9/convolution', defined at:\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 884, in _bootstrap\r\n    self._bootstrap_inner()\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\r\n    self.run()\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/threading.py\", line 864, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/scripts/train.py\", line 147, in processThread\r\n    model = PluginLoader.get_model(trainer)(get_folder(self.arguments.model_dir), self.arguments.gpus)\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/AutoEncoder.py\", line 16, in __init__\r\n    self.initModel()\r\n  File \"/home/hunterwolf/AnacondaProjects/deepfakes-faceswap/plugins/Model_Original/Model.py\", line 22, in initModel\r\n    self.autoencoder_A = KerasModel(x, self.decoder_A(self.encoder(x)))\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 603, in __call__\r\n    output = self.call(inputs, **kwargs)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2061, in call\r\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/engine/topology.py\", line 2212, in run_internal_graph\r\n    output_tensors = _to_list(layer.call(computed_tensor, **kwargs))\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/layers/convolutional.py\", line 164, in call\r\n    dilation_rate=self.dilation_rate)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\", line 3195, in conv2d\r\n    data_format=tf_data_format)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 754, in convolution\r\n    return op(input, filter)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 838, in __call__\r\n    return self.conv_op(inp, filter)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 502, in __call__\r\n    return self.call(inp, filter)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 190, in __call__\r\n    name=self.name)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 639, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/home/hunterwolf/anaconda3/envs/faceswap/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,3,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\r\n[[Node: model_2/conv2d_9/convolution = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](model_2/pixel_shuffler_4/Reshape_1, conv2d_9/kernel/read)]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n[[Node: loss/mul/_211 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1608_loss/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\r\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\r\n\r\n```"}