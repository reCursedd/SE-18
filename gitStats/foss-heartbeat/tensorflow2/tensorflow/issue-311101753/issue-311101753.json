{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18230", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18230/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18230/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18230/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18230", "id": 311101753, "node_id": "MDU6SXNzdWUzMTExMDE3NTM=", "number": 18230, "title": "TypeError: unsupported format string passed to Tensor.__format__", "user": {"login": "wuchangming", "id": 5039340, "node_id": "MDQ6VXNlcjUwMzkzNDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5039340?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wuchangming", "html_url": "https://github.com/wuchangming", "followers_url": "https://api.github.com/users/wuchangming/followers", "following_url": "https://api.github.com/users/wuchangming/following{/other_user}", "gists_url": "https://api.github.com/users/wuchangming/gists{/gist_id}", "starred_url": "https://api.github.com/users/wuchangming/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wuchangming/subscriptions", "organizations_url": "https://api.github.com/users/wuchangming/orgs", "repos_url": "https://api.github.com/users/wuchangming/repos", "events_url": "https://api.github.com/users/wuchangming/events{/privacy}", "received_events_url": "https://api.github.com/users/wuchangming/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-04-04T06:33:13Z", "updated_at": "2018-04-04T06:51:59Z", "closed_at": "2018-04-04T06:51:59Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: NO</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac OS X  version: 10.12.1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:v1.7.0-3-g024aecf414 1.7.0</li>\n<li><strong>Python version</strong>: 3.6.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>:-</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:-</li>\n<li><strong>CUDA/cuDNN version</strong>: -</li>\n<li><strong>GPU model and memory</strong>:  Intel Iris Graphics 6100 1536 MB</li>\n<li><strong>Exact command to reproduce</strong>: python toy-dataset.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I run the toy dataset code in the <a href=\"https://www.tensorflow.org/programmers_guide/eager\" rel=\"nofollow\">Eager Execution Guide Document</a>, but I got an error.</p>\n<h3>Source code / logs</h3>\n<pre><code>WARNING:tensorflow:From /XXXXXXX/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nTraceback (most recent call last):\n  File \"toy-dataset.py\", line 31, in &lt;module&gt;\n    print(\"Initial loss: {:.3f}\".format(loss(W, B)))\nTypeError: unsupported format string passed to Tensor.__format__\n</code></pre>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import, division, print_function\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib.eager <span class=\"pl-k\">as</span> tfe\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> A toy dataset of points around 3 * x + 2</span>\n<span class=\"pl-c1\">NUM_EXAMPLES</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\ntraining_inputs <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">NUM_EXAMPLES</span>])\nnoise <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">NUM_EXAMPLES</span>])\ntraining_outputs <span class=\"pl-k\">=</span> training_inputs <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">+</span> noise\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">prediction</span>(<span class=\"pl-smi\">input</span>, <span class=\"pl-smi\">weight</span>, <span class=\"pl-smi\">bias</span>):\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">input</span> <span class=\"pl-k\">*</span> weight <span class=\"pl-k\">+</span> bias\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> A loss function using mean-squared error</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>(<span class=\"pl-smi\">weights</span>, <span class=\"pl-smi\">biases</span>):\n  error <span class=\"pl-k\">=</span> prediction(training_inputs, weights, biases) <span class=\"pl-k\">-</span> training_outputs\n  <span class=\"pl-k\">return</span> tf.reduce_mean(tf.square(error))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Return the derivative of loss with respect to weight and bias</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">grad</span>(<span class=\"pl-smi\">weights</span>, <span class=\"pl-smi\">biases</span>):\n  <span class=\"pl-k\">with</span> tfe.GradientTape() <span class=\"pl-k\">as</span> tape:\n    loss_value <span class=\"pl-k\">=</span> loss(weights, biases)\n  <span class=\"pl-k\">return</span> tape.gradient(loss_value, [weights, biases])\n\ntrain_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">200</span>\nlearning_rate <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.01</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Start with arbitrary values for W and B on the same batch of data</span>\nW <span class=\"pl-k\">=</span> tfe.Variable(<span class=\"pl-c1\">5</span>.)\nB <span class=\"pl-k\">=</span> tfe.Variable(<span class=\"pl-c1\">10</span>.)\n\ntf.Print(loss(W, B), [loss(W, B)], <span class=\"pl-v\">message</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>This is a: <span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Initial loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.3f</span>}</span><span class=\"pl-pds\">\"</span></span>.format(loss(W, B)))\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(train_steps):\n  dW, dB <span class=\"pl-k\">=</span> grad(W, B)\n  W.assign_sub(dW <span class=\"pl-k\">*</span> learning_rate)\n  B.assign_sub(dB <span class=\"pl-k\">*</span> learning_rate)\n  <span class=\"pl-k\">if</span> i <span class=\"pl-k\">%</span> <span class=\"pl-c1\">20</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> print(i, loss(W, B))</span>\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Loss at step <span class=\"pl-c1\">{<span class=\"pl-k\">:03d</span>}</span>: <span class=\"pl-c1\">{<span class=\"pl-k\">:.3f</span>}</span><span class=\"pl-pds\">\"</span></span>.format(i, loss(W, B)))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Final loss: <span class=\"pl-c1\">{<span class=\"pl-k\">:.3f</span>}</span><span class=\"pl-pds\">\"</span></span>.format(loss(W, B)))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W = <span class=\"pl-c1\">{}</span>, B = <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(W.numpy(), B.numpy()))</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): NO\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X  version: 10.12.1\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):v1.7.0-3-g024aecf414 1.7.0\nPython version: 3.6.4\nBazel version (if compiling from source):-\nGCC/Compiler version (if compiling from source):-\nCUDA/cuDNN version: -\nGPU model and memory:  Intel Iris Graphics 6100 1536 MB\nExact command to reproduce: python toy-dataset.py\n\nDescribe the problem\nI run the toy dataset code in the Eager Execution Guide Document, but I got an error.\nSource code / logs\nWARNING:tensorflow:From /XXXXXXX/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse the retry module or similar alternatives.\nTraceback (most recent call last):\n  File \"toy-dataset.py\", line 31, in <module>\n    print(\"Initial loss: {:.3f}\".format(loss(W, B)))\nTypeError: unsupported format string passed to Tensor.__format__\n\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\n\n# A toy dataset of points around 3 * x + 2\nNUM_EXAMPLES = 1000\ntraining_inputs = tf.random_normal([NUM_EXAMPLES])\nnoise = tf.random_normal([NUM_EXAMPLES])\ntraining_outputs = training_inputs * 3 + 2 + noise\ndef prediction(input, weight, bias):\n  return input * weight + bias\n\n# A loss function using mean-squared error\ndef loss(weights, biases):\n  error = prediction(training_inputs, weights, biases) - training_outputs\n  return tf.reduce_mean(tf.square(error))\n\n# Return the derivative of loss with respect to weight and bias\ndef grad(weights, biases):\n  with tfe.GradientTape() as tape:\n    loss_value = loss(weights, biases)\n  return tape.gradient(loss_value, [weights, biases])\n\ntrain_steps = 200\nlearning_rate = 0.01\n# Start with arbitrary values for W and B on the same batch of data\nW = tfe.Variable(5.)\nB = tfe.Variable(10.)\n\ntf.Print(loss(W, B), [loss(W, B)], message=\"This is a: \")\nprint(\"Initial loss: {:.3f}\".format(loss(W, B)))\n\nfor i in range(train_steps):\n  dW, dB = grad(W, B)\n  W.assign_sub(dW * learning_rate)\n  B.assign_sub(dB * learning_rate)\n  if i % 20 == 0:\n    # print(i, loss(W, B))\n    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\n\nprint(\"Final loss: {:.3f}\".format(loss(W, B)))\nprint(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: NO\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X  version: 10.12.1\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3.6.4\r\n- **Bazel version (if compiling from source)**:-\r\n- **GCC/Compiler version (if compiling from source)**:-\r\n- **CUDA/cuDNN version**: -\r\n- **GPU model and memory**:  Intel Iris Graphics 6100 1536 MB\r\n- **Exact command to reproduce**: python toy-dataset.py\r\n\r\n### Describe the problem\r\nI run the toy dataset code in the [Eager Execution Guide Document](https://www.tensorflow.org/programmers_guide/eager), but I got an error.  \r\n \r\n### Source code / logs\r\n```\r\nWARNING:tensorflow:From /XXXXXXX/tensorflow/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nUse the retry module or similar alternatives.\r\nTraceback (most recent call last):\r\n  File \"toy-dataset.py\", line 31, in <module>\r\n    print(\"Initial loss: {:.3f}\".format(loss(W, B)))\r\nTypeError: unsupported format string passed to Tensor.__format__\r\n```\r\n```python\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.eager as tfe\r\n\r\n# A toy dataset of points around 3 * x + 2\r\nNUM_EXAMPLES = 1000\r\ntraining_inputs = tf.random_normal([NUM_EXAMPLES])\r\nnoise = tf.random_normal([NUM_EXAMPLES])\r\ntraining_outputs = training_inputs * 3 + 2 + noise\r\ndef prediction(input, weight, bias):\r\n  return input * weight + bias\r\n\r\n# A loss function using mean-squared error\r\ndef loss(weights, biases):\r\n  error = prediction(training_inputs, weights, biases) - training_outputs\r\n  return tf.reduce_mean(tf.square(error))\r\n\r\n# Return the derivative of loss with respect to weight and bias\r\ndef grad(weights, biases):\r\n  with tfe.GradientTape() as tape:\r\n    loss_value = loss(weights, biases)\r\n  return tape.gradient(loss_value, [weights, biases])\r\n\r\ntrain_steps = 200\r\nlearning_rate = 0.01\r\n# Start with arbitrary values for W and B on the same batch of data\r\nW = tfe.Variable(5.)\r\nB = tfe.Variable(10.)\r\n\r\ntf.Print(loss(W, B), [loss(W, B)], message=\"This is a: \")\r\nprint(\"Initial loss: {:.3f}\".format(loss(W, B)))\r\n\r\nfor i in range(train_steps):\r\n  dW, dB = grad(W, B)\r\n  W.assign_sub(dW * learning_rate)\r\n  B.assign_sub(dB * learning_rate)\r\n  if i % 20 == 0:\r\n    # print(i, loss(W, B))\r\n    print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\r\n\r\nprint(\"Final loss: {:.3f}\".format(loss(W, B)))\r\nprint(\"W = {}, B = {}\".format(W.numpy(), B.numpy()))\r\n```"}