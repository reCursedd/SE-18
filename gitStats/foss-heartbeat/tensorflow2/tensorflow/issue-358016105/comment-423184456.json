{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423184456", "html_url": "https://github.com/tensorflow/tensorflow/issues/22142#issuecomment-423184456", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22142", "id": 423184456, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzE4NDQ1Ng==", "user": {"login": "manipopopo", "id": 14799222, "node_id": "MDQ6VXNlcjE0Nzk5MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/14799222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manipopopo", "html_url": "https://github.com/manipopopo", "followers_url": "https://api.github.com/users/manipopopo/followers", "following_url": "https://api.github.com/users/manipopopo/following{/other_user}", "gists_url": "https://api.github.com/users/manipopopo/gists{/gist_id}", "starred_url": "https://api.github.com/users/manipopopo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manipopopo/subscriptions", "organizations_url": "https://api.github.com/users/manipopopo/orgs", "repos_url": "https://api.github.com/users/manipopopo/repos", "events_url": "https://api.github.com/users/manipopopo/events{/privacy}", "received_events_url": "https://api.github.com/users/manipopopo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T13:32:51Z", "updated_at": "2018-09-20T13:38:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1962026\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yanboliang\">@yanboliang</a><br>\nThe following code generates video frames' features in two different ways.</p>\n<p>It calls <code>Conv2D</code> once to get all the frames' features during training. However, the input videos for evaluation could be very long, it uses <code>tf.while_loop</code> to apply <code>Conv2D</code> frame-by-frame so that the model can be placed on a GPU with limited memory.</p>\n<p><code>tf.while_loop</code> creates a name scope <code>while</code> which changes the names of <code>Conv2D</code> weights. We can't directly load the model checkpoints for evaluation without specifying <code>var_list</code> for <code>tf.train.Saver</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">NUM_FRAMES</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n\n<span class=\"pl-c1\">CONV_FILTERS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_graph_train</span>(<span class=\"pl-smi\">video_frames</span>):\n  model <span class=\"pl-k\">=</span> tf.keras.layers.Conv2D(<span class=\"pl-c1\">CONV_FILTERS</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>)\n\n  unfolded_video_frames <span class=\"pl-k\">=</span> tf.reshape(video_frames, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">720</span>, <span class=\"pl-c1\">1080</span>, <span class=\"pl-c1\">3</span>])\n  frames_features <span class=\"pl-k\">=</span> tf.reshape(\n      model(unfolded_video_frames),\n      [<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">NUM_FRAMES</span>, <span class=\"pl-c1\">720</span>, <span class=\"pl-c1\">1080</span>, <span class=\"pl-c1\">CONV_FILTERS</span>])\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> optimize loss(frames_features)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span>\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_graph_evaluate</span>(<span class=\"pl-smi\">video_frames</span>):\n  model <span class=\"pl-k\">=</span> tf.keras.layers.Conv2D(<span class=\"pl-c1\">CONV_FILTERS</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>same<span class=\"pl-pds\">'</span></span>)\n\n  num_frames <span class=\"pl-k\">=</span> tf.shape(video_frames)[<span class=\"pl-c1\">1</span>]\n  frame_ta <span class=\"pl-k\">=</span> tf.TensorArray(video_frames.dtype, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>num_frames)\n  frame_ta <span class=\"pl-k\">=</span> frame_ta.unstack(tf.transpose(video_frames, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>]))\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">process_frame_by_frame</span>(<span class=\"pl-smi\">frame_index</span>, <span class=\"pl-smi\">frame_features_ta</span>):\n    frame <span class=\"pl-k\">=</span> frame_ta.read(frame_index)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> the shape of frame is [BATCH_SIZE, 720, 1080, 3]</span>\n    <span class=\"pl-k\">return</span> frame_index <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, frame_features_ta.write(frame_index, model(frame))\n\n  _, frame_features_ta <span class=\"pl-k\">=</span> tf.while_loop(\n      <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">frame_index</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">_</span>: frame_index <span class=\"pl-k\">&lt;</span> num_frames,\n      process_frame_by_frame,\n      (<span class=\"pl-c1\">0</span>, tf.TensorArray(video_frames.dtype, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>num_frames)),\n      <span class=\"pl-v\">back_prop</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n  frame_features <span class=\"pl-k\">=</span> tf.transpose(frame_features_ta.stack(), [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> predict(frame_features)</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span>\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  <span class=\"pl-k\">with</span> tf.Graph().as_default():\n    video_frames <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">NUM_FRAMES</span>, <span class=\"pl-c1\">720</span>, <span class=\"pl-c1\">1080</span>, <span class=\"pl-c1\">3</span>])\n    build_graph_train(video_frames)\n\n    <span class=\"pl-c1\">print</span>(tf.global_variables())\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [&lt;tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32&gt;,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  &lt;tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32&gt;]</span>\n\n  <span class=\"pl-k\">with</span> tf.Graph().as_default():\n    video_frames <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-c1\">NUM_FRAMES</span>, <span class=\"pl-c1\">720</span>, <span class=\"pl-c1\">1080</span>, <span class=\"pl-c1\">3</span>])\n    build_graph_evaluate(video_frames)\n\n    <span class=\"pl-c1\">print</span>(tf.global_variables())\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [&lt;tf.Variable 'while/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32&gt;,</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>  &lt;tf.Variable 'while/conv2d/bias:0' shape=(32,) dtype=float32&gt;]</span></pre></div>", "body_text": "@yanboliang\nThe following code generates video frames' features in two different ways.\nIt calls Conv2D once to get all the frames' features during training. However, the input videos for evaluation could be very long, it uses tf.while_loop to apply Conv2D frame-by-frame so that the model can be placed on a GPU with limited memory.\ntf.while_loop creates a name scope while which changes the names of Conv2D weights. We can't directly load the model checkpoints for evaluation without specifying var_list for tf.train.Saver.\nimport tensorflow as tf\n\n\nBATCH_SIZE = 1\nNUM_FRAMES = 5\n\nCONV_FILTERS = 32\n\n\ndef build_graph_train(video_frames):\n  model = tf.keras.layers.Conv2D(CONV_FILTERS, 3, padding='same')\n\n  unfolded_video_frames = tf.reshape(video_frames, [-1, 720, 1080, 3])\n  frames_features = tf.reshape(\n      model(unfolded_video_frames),\n      [BATCH_SIZE, NUM_FRAMES, 720, 1080, CONV_FILTERS])\n\n  # optimize loss(frames_features)\n  # ...\n\n\ndef build_graph_evaluate(video_frames):\n  model = tf.keras.layers.Conv2D(CONV_FILTERS, 3, padding='same')\n\n  num_frames = tf.shape(video_frames)[1]\n  frame_ta = tf.TensorArray(video_frames.dtype, size=num_frames)\n  frame_ta = frame_ta.unstack(tf.transpose(video_frames, [1, 0, 2, 3, 4]))\n\n  def process_frame_by_frame(frame_index, frame_features_ta):\n    frame = frame_ta.read(frame_index)\n    # the shape of frame is [BATCH_SIZE, 720, 1080, 3]\n    return frame_index + 1, frame_features_ta.write(frame_index, model(frame))\n\n  _, frame_features_ta = tf.while_loop(\n      lambda frame_index, *_: frame_index < num_frames,\n      process_frame_by_frame,\n      (0, tf.TensorArray(video_frames.dtype, size=num_frames)),\n      back_prop=False)\n  frame_features = tf.transpose(frame_features_ta.stack(), [1, 0, 2, 3, 4])\n\n  # predict(frame_features)\n  # ...\n\n\nif __name__ == '__main__':\n  with tf.Graph().as_default():\n    video_frames = tf.zeros([BATCH_SIZE, NUM_FRAMES, 720, 1080, 3])\n    build_graph_train(video_frames)\n\n    print(tf.global_variables())\n    # [<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>,\n    #  <tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>]\n\n  with tf.Graph().as_default():\n    video_frames = tf.zeros([BATCH_SIZE, NUM_FRAMES, 720, 1080, 3])\n    build_graph_evaluate(video_frames)\n\n    print(tf.global_variables())\n    # [<tf.Variable 'while/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>,\n    #  <tf.Variable 'while/conv2d/bias:0' shape=(32,) dtype=float32>]", "body": "@yanboliang \r\nThe following code generates video frames' features in two different ways.\r\n\r\nIt calls `Conv2D` once to get all the frames' features during training. However, the input videos for evaluation could be very long, it uses `tf.while_loop` to apply `Conv2D` frame-by-frame so that the model can be placed on a GPU with limited memory.\r\n\r\n`tf.while_loop` creates a name scope `while` which changes the names of `Conv2D` weights. We can't directly load the model checkpoints for evaluation without specifying `var_list` for `tf.train.Saver`.\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n\r\nBATCH_SIZE = 1\r\nNUM_FRAMES = 5\r\n\r\nCONV_FILTERS = 32\r\n\r\n\r\ndef build_graph_train(video_frames):\r\n  model = tf.keras.layers.Conv2D(CONV_FILTERS, 3, padding='same')\r\n\r\n  unfolded_video_frames = tf.reshape(video_frames, [-1, 720, 1080, 3])\r\n  frames_features = tf.reshape(\r\n      model(unfolded_video_frames),\r\n      [BATCH_SIZE, NUM_FRAMES, 720, 1080, CONV_FILTERS])\r\n\r\n  # optimize loss(frames_features)\r\n  # ...\r\n\r\n\r\ndef build_graph_evaluate(video_frames):\r\n  model = tf.keras.layers.Conv2D(CONV_FILTERS, 3, padding='same')\r\n\r\n  num_frames = tf.shape(video_frames)[1]\r\n  frame_ta = tf.TensorArray(video_frames.dtype, size=num_frames)\r\n  frame_ta = frame_ta.unstack(tf.transpose(video_frames, [1, 0, 2, 3, 4]))\r\n\r\n  def process_frame_by_frame(frame_index, frame_features_ta):\r\n    frame = frame_ta.read(frame_index)\r\n    # the shape of frame is [BATCH_SIZE, 720, 1080, 3]\r\n    return frame_index + 1, frame_features_ta.write(frame_index, model(frame))\r\n\r\n  _, frame_features_ta = tf.while_loop(\r\n      lambda frame_index, *_: frame_index < num_frames,\r\n      process_frame_by_frame,\r\n      (0, tf.TensorArray(video_frames.dtype, size=num_frames)),\r\n      back_prop=False)\r\n  frame_features = tf.transpose(frame_features_ta.stack(), [1, 0, 2, 3, 4])\r\n\r\n  # predict(frame_features)\r\n  # ...\r\n\r\n\r\nif __name__ == '__main__':\r\n  with tf.Graph().as_default():\r\n    video_frames = tf.zeros([BATCH_SIZE, NUM_FRAMES, 720, 1080, 3])\r\n    build_graph_train(video_frames)\r\n\r\n    print(tf.global_variables())\r\n    # [<tf.Variable 'conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>,\r\n    #  <tf.Variable 'conv2d/bias:0' shape=(32,) dtype=float32>]\r\n\r\n  with tf.Graph().as_default():\r\n    video_frames = tf.zeros([BATCH_SIZE, NUM_FRAMES, 720, 1080, 3])\r\n    build_graph_evaluate(video_frames)\r\n\r\n    print(tf.global_variables())\r\n    # [<tf.Variable 'while/conv2d/kernel:0' shape=(3, 3, 3, 32) dtype=float32>,\r\n    #  <tf.Variable 'while/conv2d/bias:0' shape=(32,) dtype=float32>]\r\n```"}