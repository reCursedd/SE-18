{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/158854008", "pull_request_review_id": 85718023, "id": 158854008, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1ODg1NDAwOA==", "diff_hunk": "@@ -1144,13 +1149,28 @@ def __init__(self,\n             \"If provided, attention_layer_size must contain exactly one \"\n             \"integer per attention_mechanism, saw: %d vs %d\"\n             % (len(attention_layer_sizes), len(attention_mechanisms)))\n+      if attention_layer_activation is not None:\n+        attention_layer_activations = tuple(\n+            attention_layer_activation\n+            if isinstance(attention_layer_activation, (list, tuple))\n+            else (attention_layer_activation,))\n+      else:\n+        attention_layer_activations = tuple(None for _ in attention_layer_sizes)\n+      if len(attention_layer_activations) != len(attention_layer_sizes):\n+        raise ValueError(\n+            \"If provided, attention_layer_activation must contain exactly one \"\n+            \"activation function per attention layer, saw: %d vs %d\"\n+            % (len(attention_layer_activations), len(attention_layer_sizes)))\n+      attention_layers_param = zip(\n+          attention_layer_sizes, attention_layer_activations)\n       self._attention_layers = tuple(\n           layers_core.Dense(\n-              attention_layer_size,", "path": "tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py", "position": null, "original_position": 39, "commit_id": "aa40384dce1719348af505eaaae724eb78ba444a", "original_commit_id": "1ee4aec6244d8ac5be7aba84056ff73fa570034c", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "I wonder if it makes sense to just allow a list of attention_layers as an alternative to passing attention_layer_size.  Then users could pass their own Dense (or something else) here.  what do you think?", "created_at": "2017-12-27T18:46:13Z", "updated_at": "2018-03-29T14:40:05Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14974#discussion_r158854008", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14974", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/158854008"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14974#discussion_r158854008"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14974"}}, "body_html": "<p>I wonder if it makes sense to just allow a list of attention_layers as an alternative to passing attention_layer_size.  Then users could pass their own Dense (or something else) here.  what do you think?</p>", "body_text": "I wonder if it makes sense to just allow a list of attention_layers as an alternative to passing attention_layer_size.  Then users could pass their own Dense (or something else) here.  what do you think?"}