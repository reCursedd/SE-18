{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302386162", "html_url": "https://github.com/tensorflow/tensorflow/issues/9742#issuecomment-302386162", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9742", "id": 302386162, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjM4NjE2Mg==", "user": {"login": "sirfz", "id": 4741099, "node_id": "MDQ6VXNlcjQ3NDEwOTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4741099?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sirfz", "html_url": "https://github.com/sirfz", "followers_url": "https://api.github.com/users/sirfz/followers", "following_url": "https://api.github.com/users/sirfz/following{/other_user}", "gists_url": "https://api.github.com/users/sirfz/gists{/gist_id}", "starred_url": "https://api.github.com/users/sirfz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sirfz/subscriptions", "organizations_url": "https://api.github.com/users/sirfz/orgs", "repos_url": "https://api.github.com/users/sirfz/repos", "events_url": "https://api.github.com/users/sirfz/events{/privacy}", "received_events_url": "https://api.github.com/users/sirfz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-18T12:12:54Z", "updated_at": "2017-05-18T12:18:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> wow, the difference is huge. Initializing the variables now takes less than 3GB of memory, that's 2x less than what the binary version was taking and 4x less than the compiled one (using <code>tf.constant_initializer</code>). That's a huge difference! (edit: It's also way faster)</p>\n<p>However, the placeholder initialization workaround is unpractical in my case as we have wrapper classes that do the loading for us which we just initialize using <code>global_variables_initializer</code> so it'll require some design changes which isn't very feasible right now.</p>\n<p>I understand this is an issue with <code>protobuf</code> but how come when loading the variables in the compiled version of tensorflow we get so much more memory usage? Even though it's the same <code>protobuf</code> version used in both cases? Also should I file an issue with the <code>protobuf</code> team regarding this or are there ways to tune it?</p>\n<p>In case someone needs it, here's the code I used:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_layer</span>(<span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">output_size</span>, <span class=\"pl-smi\">name</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> supposedly loaded from a saved file</span>\n    W_pl <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (input_size, output_size))\n    W_val <span class=\"pl-k\">=</span> np.random.normal(<span class=\"pl-v\">scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(input_size, output_size)).astype(np.float32)\n    W <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>W_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(name),\n                        <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>W_pl,\n                        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>b_<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(name), <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(output_size,),\n                        <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32),\n                        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    <span class=\"pl-k\">return</span> (W_pl, W_val), W, b\n\nsessions <span class=\"pl-k\">=</span> []\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3</span>):\n    g <span class=\"pl-k\">=</span> tf.Graph()\n    <span class=\"pl-k\">with</span> g.as_default():\n        W1_pl, W1, b1 <span class=\"pl-k\">=</span> get_layer(<span class=\"pl-c1\">158238</span>, <span class=\"pl-c1\">900</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>1<span class=\"pl-pds\">'</span></span>)\n        W2_pl, W2, b2 <span class=\"pl-k\">=</span> get_layer(<span class=\"pl-c1\">900</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>2<span class=\"pl-pds\">'</span></span>)\n        W3_pl, W3, b3 <span class=\"pl-k\">=</span> get_layer(<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>3<span class=\"pl-pds\">'</span></span>)\n        init <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n    session <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>g)\n    session.run(init, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">dict</span>([W1_pl, W2_pl, W3_pl]))\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loaded <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(i)\n    sessions.append(session)</pre></div>", "body_text": "@yaroslavvb wow, the difference is huge. Initializing the variables now takes less than 3GB of memory, that's 2x less than what the binary version was taking and 4x less than the compiled one (using tf.constant_initializer). That's a huge difference! (edit: It's also way faster)\nHowever, the placeholder initialization workaround is unpractical in my case as we have wrapper classes that do the loading for us which we just initialize using global_variables_initializer so it'll require some design changes which isn't very feasible right now.\nI understand this is an issue with protobuf but how come when loading the variables in the compiled version of tensorflow we get so much more memory usage? Even though it's the same protobuf version used in both cases? Also should I file an issue with the protobuf team regarding this or are there ways to tune it?\nIn case someone needs it, here's the code I used:\nimport numpy as np\nimport tensorflow as tf\n\ndef get_layer(input_size, output_size, name):\n    # supposedly loaded from a saved file\n    W_pl = tf.placeholder(tf.float32, (input_size, output_size))\n    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\n    W = tf.get_variable(name='W_{}'.format(name),\n                        initializer=W_pl,\n                        dtype=tf.float32)\n    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\n                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\n                        dtype=tf.float32)\n    return (W_pl, W_val), W, b\n\nsessions = []\nfor i in range(3):\n    g = tf.Graph()\n    with g.as_default():\n        W1_pl, W1, b1 = get_layer(158238, 900, '1')\n        W2_pl, W2, b2 = get_layer(900, 1000, '2')\n        W3_pl, W3, b3 = get_layer(1000, 1, '3')\n        init = tf.global_variables_initializer()\n    session = tf.Session(graph=g)\n    session.run(init, feed_dict=dict([W1_pl, W2_pl, W3_pl]))\n    print 'Loaded {}'.format(i)\n    sessions.append(session)", "body": "@yaroslavvb wow, the difference is huge. Initializing the variables now takes less than 3GB of memory, that's 2x less than what the binary version was taking and 4x less than the compiled one (using `tf.constant_initializer`). That's a huge difference! (edit: It's also way faster)\r\n\r\nHowever, the placeholder initialization workaround is unpractical in my case as we have wrapper classes that do the loading for us which we just initialize using `global_variables_initializer` so it'll require some design changes which isn't very feasible right now.\r\n\r\nI understand this is an issue with `protobuf` but how come when loading the variables in the compiled version of tensorflow we get so much more memory usage? Even though it's the same `protobuf` version used in both cases? Also should I file an issue with the `protobuf` team regarding this or are there ways to tune it?\r\n\r\nIn case someone needs it, here's the code I used:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef get_layer(input_size, output_size, name):\r\n    # supposedly loaded from a saved file\r\n    W_pl = tf.placeholder(tf.float32, (input_size, output_size))\r\n    W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\r\n    W = tf.get_variable(name='W_{}'.format(name),\r\n                        initializer=W_pl,\r\n                        dtype=tf.float32)\r\n    b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\r\n                        initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\r\n                        dtype=tf.float32)\r\n    return (W_pl, W_val), W, b\r\n\r\nsessions = []\r\nfor i in range(3):\r\n    g = tf.Graph()\r\n    with g.as_default():\r\n        W1_pl, W1, b1 = get_layer(158238, 900, '1')\r\n        W2_pl, W2, b2 = get_layer(900, 1000, '2')\r\n        W3_pl, W3, b3 = get_layer(1000, 1, '3')\r\n        init = tf.global_variables_initializer()\r\n    session = tf.Session(graph=g)\r\n    session.run(init, feed_dict=dict([W1_pl, W2_pl, W3_pl]))\r\n    print 'Loaded {}'.format(i)\r\n    sessions.append(session)\r\n```"}