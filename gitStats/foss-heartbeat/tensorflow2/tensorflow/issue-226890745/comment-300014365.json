{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300014365", "html_url": "https://github.com/tensorflow/tensorflow/issues/9742#issuecomment-300014365", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9742", "id": 300014365, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDAxNDM2NQ==", "user": {"login": "yaroslavvb2", "id": 25833812, "node_id": "MDQ6VXNlcjI1ODMzODEy", "avatar_url": "https://avatars1.githubusercontent.com/u/25833812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb2", "html_url": "https://github.com/yaroslavvb2", "followers_url": "https://api.github.com/users/yaroslavvb2/followers", "following_url": "https://api.github.com/users/yaroslavvb2/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb2/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb2/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb2/orgs", "repos_url": "https://api.github.com/users/yaroslavvb2/repos", "events_url": "https://api.github.com/users/yaroslavvb2/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb2/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-08T22:59:42Z", "updated_at": "2017-05-08T22:59:42Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Could it be that one of the version is using Python implementation of\nprotobuf instead of cpp?\n\nTry this\npython -c \"from google.protobuf.internal import api_implementation;\nprint(api_implementation._default_implementation_type)\"\n\nIt should print \"cpp\"</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Mon, May 8, 2017 at 12:30 PM, Fayez ***@***.***&gt; wrote:\n I ran the profiler tool against the pip version and the compiled version.\n First, here's the printed messages for each:\n Pip version\n\n ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n Starting tracking the heap\n Starting tracking the heap\n tcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)\n Dumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)\n Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n Dumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)\n Dumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)\n Dumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)\n\n Compiled version\n\n ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n Starting tracking the heap\n Starting tracking the heap\n tcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)\n Dumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)\n Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n Dumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)\n Dumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)\n Dumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)\n Dumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)\n Dumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)\n\n Up until heap 4 in both cases, memory usage is the same and both graphs\n (generated by google-pprof tool) are exactly the same based on a quick\n look.\n\n At heap 5, the memory usage is still the same but in the compiled\n version's case I see some parallelization in the graph where there are 2\n nodes with ~540MB each instead of 1 node with 1090MB in the pip version's\n case.\n\n Heap 6 is where the memory usage doubles in the compiled version's case.\n In the pip version there's 1 big unnamed node with 1637.6MB. However, in\n the compiled version there are 3 nodes with ~500MB each (initcmath + 2\n other unnamed nodes) + a fourth large node called tensorflow\n PartialRunSetupRequest MergePartialFromCodedStream with 1093.8MB usage. I\n didn't see this particular node in any of the heap files of the\n pip-version. I can attach a screenshot but it becomes unreadable when I\n zoom out enough to view the whole thing. Let me know if you'd like me to\n attach the heap files.\n\n In heap 7 of the compiled version, that same node appears with double the\n memory as well (2183.7MB).\n\n I used a simplified version of my original snippet to generate the heap\n files:\n\n import numpy as npimport tensorflow as tf\n def get_layer(input_size, output_size, name):\n     # supposedly loaded from a saved file\n     W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\n     W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\n                         initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\n                         dtype=tf.float32)\n     b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\n                         initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\n                         dtype=tf.float32)\n     return W, b\n\n g = tf.Graph()with g.as_default():\n     W1, b1 = get_layer(158238, 900, '1')\n     W2, b2 = get_layer(900, 1000, '2')\n     W3, b3 = get_layer(1000, 1, '3')\n     init = tf.global_variables_initializer()\n session = tf.Session(graph=g)\n session.run(init)\n session.close()\n\n \u2014\n You are receiving this because you commented.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"226890745\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9742\" href=\"https://github.com/tensorflow/tensorflow/issues/9742#issuecomment-299966164\">#9742 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AYoxVHu9SJzrRAiqslcLQwpRRaD2Ku51ks5r321OgaJpZM4NTSLO\">https://github.com/notifications/unsubscribe-auth/AYoxVHu9SJzrRAiqslcLQwpRRaD2Ku51ks5r321OgaJpZM4NTSLO</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Could it be that one of the version is using Python implementation of\nprotobuf instead of cpp?\n\nTry this\npython -c \"from google.protobuf.internal import api_implementation;\nprint(api_implementation._default_implementation_type)\"\n\nIt should print \"cpp\"\n\u2026\nOn Mon, May 8, 2017 at 12:30 PM, Fayez ***@***.***> wrote:\n I ran the profiler tool against the pip version and the compiled version.\n First, here's the printed messages for each:\n Pip version\n\n ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n Starting tracking the heap\n Starting tracking the heap\n tcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)\n Dumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)\n Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n Dumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)\n Dumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)\n Dumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)\n\n Compiled version\n\n ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n Starting tracking the heap\n Starting tracking the heap\n tcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)\n Dumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)\n Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)\n Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n Dumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)\n Dumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)\n Dumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)\n Dumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)\n Dumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)\n\n Up until heap 4 in both cases, memory usage is the same and both graphs\n (generated by google-pprof tool) are exactly the same based on a quick\n look.\n\n At heap 5, the memory usage is still the same but in the compiled\n version's case I see some parallelization in the graph where there are 2\n nodes with ~540MB each instead of 1 node with 1090MB in the pip version's\n case.\n\n Heap 6 is where the memory usage doubles in the compiled version's case.\n In the pip version there's 1 big unnamed node with 1637.6MB. However, in\n the compiled version there are 3 nodes with ~500MB each (initcmath + 2\n other unnamed nodes) + a fourth large node called tensorflow\n PartialRunSetupRequest MergePartialFromCodedStream with 1093.8MB usage. I\n didn't see this particular node in any of the heap files of the\n pip-version. I can attach a screenshot but it becomes unreadable when I\n zoom out enough to view the whole thing. Let me know if you'd like me to\n attach the heap files.\n\n In heap 7 of the compiled version, that same node appears with double the\n memory as well (2183.7MB).\n\n I used a simplified version of my original snippet to generate the heap\n files:\n\n import numpy as npimport tensorflow as tf\n def get_layer(input_size, output_size, name):\n     # supposedly loaded from a saved file\n     W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\n     W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\n                         initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\n                         dtype=tf.float32)\n     b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\n                         initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\n                         dtype=tf.float32)\n     return W, b\n\n g = tf.Graph()with g.as_default():\n     W1, b1 = get_layer(158238, 900, '1')\n     W2, b2 = get_layer(900, 1000, '2')\n     W3, b3 = get_layer(1000, 1, '3')\n     init = tf.global_variables_initializer()\n session = tf.Session(graph=g)\n session.run(init)\n session.close()\n\n \u2014\n You are receiving this because you commented.\n Reply to this email directly, view it on GitHub\n <#9742 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AYoxVHu9SJzrRAiqslcLQwpRRaD2Ku51ks5r321OgaJpZM4NTSLO>\n .", "body": "Could it be that one of the version is using Python implementation of\nprotobuf instead of cpp?\n\nTry this\npython -c \"from google.protobuf.internal import api_implementation;\nprint(api_implementation._default_implementation_type)\"\n\nIt should print \"cpp\"\n\nOn Mon, May 8, 2017 at 12:30 PM, Fayez <notifications@github.com> wrote:\n\n> I ran the profiler tool against the pip version and the compiled version.\n> First, here's the printed messages for each:\n> Pip version\n>\n> ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n> Starting tracking the heap\n> Starting tracking the heap\n> tcmalloc: large alloc 1139318784 bytes == 0x4326000 @  0x7fa0319cfd1b 0x7fa02fa7835c 0x7fa02faafe73 0x7fa02fab2b19 0x7fa02fb2b1e8 0x7f9fdef7ece5 0x7f9fdefc1c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7fa0313d7830 0x49d9d9 (nil)\n> Dumping heap profile to /root/tf_profile.0001.heap (1387 MB allocated cumulatively, 1105 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0003.heap (3016 MB allocated cumulatively, 1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n> Dumping heap profile to /root/tf_profile.0005.heap (4676 MB allocated cumulatively, 1657 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0006.heap (5769 MB allocated cumulatively, 1657 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0007.heap (Exiting, 561 MB in use)\n>\n> Compiled version\n>\n> ~# LD_PRELOAD=\"/usr/lib/libtcmalloc.so.4\" HEAPPROFILE=/root/tf_profile python test.py\n> Starting tracking the heap\n> Starting tracking the heap\n> tcmalloc: large alloc 1139318784 bytes == 0x3c70000 @  0x7f4c16a35d1b 0x7f4c14ade35c 0x7f4c14b15e73 0x7f4c14b18b19 0x7f4c14b911e8 0x7f4bc3fd4ce5 0x7f4bc4017c00 0x4cada2 0x4c9d8f 0x4c2765 0x4c2509 0x4f1def 0x4ec652 0x4eae31 0x49e14a 0x7f4c1643d830 0x49d9d9 (nil)\n> Dumping heap profile to /root/tf_profile.0001.heap (1390 MB allocated cumulatively, 1105 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0002.heap (1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0003.heap (3020 MB allocated cumulatively, 1649 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0004.heap (2192 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0005.heap (4680 MB allocated cumulatively, 1657 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0006.heap (5777 MB allocated cumulatively, 2754 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0007.heap (6867 MB allocated cumulatively, 2751 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0008.heap (7960 MB allocated cumulatively, 2751 MB currently in use)\n> Dumping heap profile to /root/tf_profile.0009.heap (Exiting, 561 MB in use)\n>\n> Up until heap 4 in both cases, memory usage is the same and both graphs\n> (generated by google-pprof tool) are exactly the same based on a quick\n> look.\n>\n> At heap 5, the memory usage is still the same but in the compiled\n> version's case I see some parallelization in the graph where there are 2\n> nodes with ~540MB each instead of 1 node with 1090MB in the pip version's\n> case.\n>\n> Heap 6 is where the memory usage doubles in the compiled version's case.\n> In the pip version there's 1 big unnamed node with 1637.6MB. However, in\n> the compiled version there are 3 nodes with ~500MB each (initcmath + 2\n> other unnamed nodes) + a fourth large node called tensorflow\n> PartialRunSetupRequest MergePartialFromCodedStream with 1093.8MB usage. I\n> didn't see this particular node in any of the heap files of the\n> pip-version. I can attach a screenshot but it becomes unreadable when I\n> zoom out enough to view the whole thing. Let me know if you'd like me to\n> attach the heap files.\n>\n> In heap 7 of the compiled version, that same node appears with double the\n> memory as well (2183.7MB).\n>\n> I used a simplified version of my original snippet to generate the heap\n> files:\n>\n> import numpy as npimport tensorflow as tf\n> def get_layer(input_size, output_size, name):\n>     # supposedly loaded from a saved file\n>     W_val = np.random.normal(scale=0.1, size=(input_size, output_size)).astype(np.float32)\n>     W = tf.get_variable(name='W_{}'.format(name), shape=(input_size, output_size),\n>                         initializer=tf.constant_initializer(value=W_val, dtype=tf.float32),\n>                         dtype=tf.float32)\n>     b = tf.get_variable(name='b_{}'.format(name), shape=(output_size,),\n>                         initializer=tf.constant_initializer(value=0.1, dtype=tf.float32),\n>                         dtype=tf.float32)\n>     return W, b\n>\n> g = tf.Graph()with g.as_default():\n>     W1, b1 = get_layer(158238, 900, '1')\n>     W2, b2 = get_layer(900, 1000, '2')\n>     W3, b3 = get_layer(1000, 1, '3')\n>     init = tf.global_variables_initializer()\n> session = tf.Session(graph=g)\n> session.run(init)\n> session.close()\n>\n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9742#issuecomment-299966164>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AYoxVHu9SJzrRAiqslcLQwpRRaD2Ku51ks5r321OgaJpZM4NTSLO>\n> .\n>\n"}