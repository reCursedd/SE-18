{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316872607", "html_url": "https://github.com/tensorflow/tensorflow/issues/11637#issuecomment-316872607", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11637", "id": 316872607, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjg3MjYwNw==", "user": {"login": "liguanlin", "id": 23256131, "node_id": "MDQ6VXNlcjIzMjU2MTMx", "avatar_url": "https://avatars1.githubusercontent.com/u/23256131?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liguanlin", "html_url": "https://github.com/liguanlin", "followers_url": "https://api.github.com/users/liguanlin/followers", "following_url": "https://api.github.com/users/liguanlin/following{/other_user}", "gists_url": "https://api.github.com/users/liguanlin/gists{/gist_id}", "starred_url": "https://api.github.com/users/liguanlin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liguanlin/subscriptions", "organizations_url": "https://api.github.com/users/liguanlin/orgs", "repos_url": "https://api.github.com/users/liguanlin/repos", "events_url": "https://api.github.com/users/liguanlin/events{/privacy}", "received_events_url": "https://api.github.com/users/liguanlin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-21T01:05:04Z", "updated_at": "2017-07-21T01:05:04Z", "author_association": "NONE", "body_html": "<p>d_loss_real=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D),logits=D_log))<br>\nd_loss_fake=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(D_),logits=D_log_))<br>\nd_loss=d_loss_fake+d_loss_real<br>\ng_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D_),logits=D_log_))<br>\nboolean_error_real=tf.cast(tf.not_equal(tf.reshape(tf.to_int32(tf.round(D_log)),[batch_size,128]),tf.reshape(K,[batch_size,128])),tf.int32)<br>\nbit_error_real=tf.reduce_sum(boolean_error_real,[1])<br>\nboolean_error_fake = tf.cast(tf.not_equal(tf.reshape(tf.to_int32(tf.round(D_log_)),[batch_size,128]), tf.reshape(K, [batch_size, 128])), tf.int32)<br>\nbit_error_fake = tf.reduce_sum(boolean_error_fake, [1])</p>\n<pre><code>z_sum = tf.summary.histogram(\"z\", z)\nd_sum = tf.summary.histogram(\"d\", D)\nd__sum = tf.summary.histogram(\"d_\", D_)\nG_sum = tf.summary.histogram(\"G\", G)\n\nd_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\nd_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\nd_loss_sum = tf.summary.scalar(\"d_loss\", d_loss)\ng_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\nbit_error_real_sum=tf.summary.scalar(\"bit_error_real\",bit_error_real)\nbit_error_fake_sum=tf.summary.scalar(\"bit_error_fake\",bit_error_fake)\n\n# \u5408\u5e76\u5404\u81ea\u7684\u603b\u7ed3\ng_sum = tf.summary.merge([z_sum, d__sum, G_sum, d_loss_fake_sum, g_loss_sum])\nd_sum = tf.summary.merge([z_sum, d_sum, d_loss_real_sum, d_loss_sum])\n\n# \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff0c\u7528\u4e8e tf.train.Optimizer \u7684 var_list\nt_vars = tf.trainable_variables()\nd_vars = [var for var in t_vars if 'd_' in var.name]\ng_vars = [var for var in t_vars if 'g_' in var.name]\n\nsaver = tf.train.Saver()\n#tf.get_variable_scope().reuse_variables()\n# \u4f18\u5316\u7b97\u6cd5\u91c7\u7528 Adam\nwith tf.variable_scope(tf.get_variable_scope(),reuse=None):\n    d_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(d_loss, var_list=d_vars, global_step=global_step)\n    g_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(g_loss, var_list=g_vars, global_step=global_step)\n</code></pre>\n<p>yes,i did <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a></p>", "body_text": "d_loss_real=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D),logits=D_log))\nd_loss_fake=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(D_),logits=D_log_))\nd_loss=d_loss_fake+d_loss_real\ng_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D_),logits=D_log_))\nboolean_error_real=tf.cast(tf.not_equal(tf.reshape(tf.to_int32(tf.round(D_log)),[batch_size,128]),tf.reshape(K,[batch_size,128])),tf.int32)\nbit_error_real=tf.reduce_sum(boolean_error_real,[1])\nboolean_error_fake = tf.cast(tf.not_equal(tf.reshape(tf.to_int32(tf.round(D_log_)),[batch_size,128]), tf.reshape(K, [batch_size, 128])), tf.int32)\nbit_error_fake = tf.reduce_sum(boolean_error_fake, [1])\nz_sum = tf.summary.histogram(\"z\", z)\nd_sum = tf.summary.histogram(\"d\", D)\nd__sum = tf.summary.histogram(\"d_\", D_)\nG_sum = tf.summary.histogram(\"G\", G)\n\nd_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\nd_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\nd_loss_sum = tf.summary.scalar(\"d_loss\", d_loss)\ng_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\nbit_error_real_sum=tf.summary.scalar(\"bit_error_real\",bit_error_real)\nbit_error_fake_sum=tf.summary.scalar(\"bit_error_fake\",bit_error_fake)\n\n# \u5408\u5e76\u5404\u81ea\u7684\u603b\u7ed3\ng_sum = tf.summary.merge([z_sum, d__sum, G_sum, d_loss_fake_sum, g_loss_sum])\nd_sum = tf.summary.merge([z_sum, d_sum, d_loss_real_sum, d_loss_sum])\n\n# \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff0c\u7528\u4e8e tf.train.Optimizer \u7684 var_list\nt_vars = tf.trainable_variables()\nd_vars = [var for var in t_vars if 'd_' in var.name]\ng_vars = [var for var in t_vars if 'g_' in var.name]\n\nsaver = tf.train.Saver()\n#tf.get_variable_scope().reuse_variables()\n# \u4f18\u5316\u7b97\u6cd5\u91c7\u7528 Adam\nwith tf.variable_scope(tf.get_variable_scope(),reuse=None):\n    d_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(d_loss, var_list=d_vars, global_step=global_step)\n    g_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(g_loss, var_list=g_vars, global_step=global_step)\n\nyes,i did @drpngx", "body": "d_loss_real=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D),logits=D_log))\r\n    d_loss_fake=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(D_),logits=D_log_))\r\n    d_loss=d_loss_fake+d_loss_real\r\n    g_loss=tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(D_),logits=D_log_))\r\n    boolean_error_real=tf.cast(tf.not_equal(tf.reshape(tf.to_int32(tf.round(D_log)),[batch_size,128]),tf.reshape(K,[batch_size,128])),tf.int32)\r\n    bit_error_real=tf.reduce_sum(boolean_error_real,[1])\r\n    boolean_error_fake = tf.cast(tf.not_equal(tf.reshape(tf.to_int32(tf.round(D_log_)),[batch_size,128]), tf.reshape(K, [batch_size, 128])), tf.int32)\r\n    bit_error_fake = tf.reduce_sum(boolean_error_fake, [1])\r\n\r\n    z_sum = tf.summary.histogram(\"z\", z)\r\n    d_sum = tf.summary.histogram(\"d\", D)\r\n    d__sum = tf.summary.histogram(\"d_\", D_)\r\n    G_sum = tf.summary.histogram(\"G\", G)\r\n\r\n    d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\r\n    d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\r\n    d_loss_sum = tf.summary.scalar(\"d_loss\", d_loss)\r\n    g_loss_sum = tf.summary.scalar(\"g_loss\", g_loss)\r\n    bit_error_real_sum=tf.summary.scalar(\"bit_error_real\",bit_error_real)\r\n    bit_error_fake_sum=tf.summary.scalar(\"bit_error_fake\",bit_error_fake)\r\n\r\n    # \u5408\u5e76\u5404\u81ea\u7684\u603b\u7ed3\r\n    g_sum = tf.summary.merge([z_sum, d__sum, G_sum, d_loss_fake_sum, g_loss_sum])\r\n    d_sum = tf.summary.merge([z_sum, d_sum, d_loss_real_sum, d_loss_sum])\r\n\r\n    # \u751f\u6210\u5668\u548c\u5224\u522b\u5668\u8981\u66f4\u65b0\u7684\u53d8\u91cf\uff0c\u7528\u4e8e tf.train.Optimizer \u7684 var_list\r\n    t_vars = tf.trainable_variables()\r\n    d_vars = [var for var in t_vars if 'd_' in var.name]\r\n    g_vars = [var for var in t_vars if 'g_' in var.name]\r\n\r\n    saver = tf.train.Saver()\r\n    #tf.get_variable_scope().reuse_variables()\r\n    # \u4f18\u5316\u7b97\u6cd5\u91c7\u7528 Adam\r\n    with tf.variable_scope(tf.get_variable_scope(),reuse=None):\r\n        d_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(d_loss, var_list=d_vars, global_step=global_step)\r\n        g_optim = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.5).minimize(g_loss, var_list=g_vars, global_step=global_step)\r\n\r\nyes,i did @drpngx "}