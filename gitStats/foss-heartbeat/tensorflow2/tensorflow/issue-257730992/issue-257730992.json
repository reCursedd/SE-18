{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13041", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13041/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13041/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13041/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13041", "id": 257730992, "node_id": "MDU6SXNzdWUyNTc3MzA5OTI=", "number": 13041, "title": "Load model in C++ API and get \"from device: CUDA_ERROR_OUT_OF_MEMORY\" error", "user": {"login": "suzhaolong", "id": 5888412, "node_id": "MDQ6VXNlcjU4ODg0MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5888412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suzhaolong", "html_url": "https://github.com/suzhaolong", "followers_url": "https://api.github.com/users/suzhaolong/followers", "following_url": "https://api.github.com/users/suzhaolong/following{/other_user}", "gists_url": "https://api.github.com/users/suzhaolong/gists{/gist_id}", "starred_url": "https://api.github.com/users/suzhaolong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suzhaolong/subscriptions", "organizations_url": "https://api.github.com/users/suzhaolong/orgs", "repos_url": "https://api.github.com/users/suzhaolong/repos", "events_url": "https://api.github.com/users/suzhaolong/events{/privacy}", "received_events_url": "https://api.github.com/users/suzhaolong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-14T14:02:56Z", "updated_at": "2017-09-14T17:04:06Z", "closed_at": "2017-09-14T17:04:06Z", "author_association": "NONE", "body_html": "<p>My model is about 2.4GB\u3002In my inference step, I  want to load model by multi-processing method in each GPU. That means I try to make two process in one GPU and each load a model\u3002<br>\nAfter I make configuration of each session done, each session get about 5GB memory, But I still meet the \"from device: CUDA_ERROR_OUT_OF_MEMORY\"\u3002I am wondering\u3002\u3002\u3002 Asking for help</p>\n<h2><strong>GPU information:</strong></h2>\n<p>[search@qrwt01 /home/s/apps/qtfserverd/bin]$ nvidia-smi<br>\nThu Sep 14 21:42:48 2017<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  Tesla K80           Off  | 0000:08:00.0     Off |                    0 |<br>\n| N/A   48C    P0    61W / 149W |  11366MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+<br>\n|   1  Tesla K80           Off  | 0000:09:00.0     Off |                    0 |<br>\n| N/A   32C    P0    72W / 149W |  11359MiB / 11439MiB |      0%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID  Type  Process name                               Usage      |<br>\n|=============================================================================|<br>\n|    0     33056    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |<br>\n|    0     33057    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5515MiB |<br>\n|    1     33058    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |<br>\n|    1     33059    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5516MiB |<br>\n+-----------------------------------------------------------------------------+</p>\n<h2><strong>Session configuration:</strong></h2>\n<p>46 void* create_session(void* graph, std::string&amp; checkpoint_path,<br>\n47     int intra_op_threads, int inter_op_threads, std::string&amp; device_list) {<br>\n48     Session* session = NULL;<br>\n49     SessionOptions sess_opts;<br>\n50     //int NUM_THREADS = 8;<br>\n51     if (intra_op_threads &gt; 0) {<br>\n52         sess_opts.config.set_intra_op_parallelism_threads(intra_op_threads);<br>\n53     }<br>\n54     if (inter_op_threads &gt; 0) {<br>\n55         sess_opts.config.set_inter_op_parallelism_threads(inter_op_threads);<br>\n56     }<br>\n57<br>\n58     sess_opts.config.set_allow_soft_placement(true);<br>\n59     sess_opts.config.mutable_gpu_options()-&gt;set_visible_device_list(device_list);<br>\n60     sess_opts.config.mutable_gpu_options()-&gt;set_allocator_type(\"BFC\");<br>\n61     sess_opts.config.mutable_gpu_options()-&gt;set_per_process_gpu_memory_fraction(0.5);<br>\n62     sess_opts.config.mutable_gpu_options()-&gt;set_allow_growth(true);<br>\n63     Status status = NewSession(sess_opts, &amp;session);<br>\n64     if (!status.ok()) {<br>\n65         fprintf(stderr, \"Create Session Failed %s\\n\", status.ToString().c_str());<br>\n66         return NULL;<br>\n67     }</p>\n<h2><strong>Error information</strong></h2>\n<p>load /home/search/tensorflow/deploy_combine.model.meta graph to /gpu:1 success<br>\n2017-09-14 21:42:31.188212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:<br>\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235<br>\npciBusID: 0000:09:00.0<br>\ntotalMemory: 11.17GiB freeMemory: 11.05GiB<br>\n2017-09-14 21:42:31.188260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 1, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7)<br>\nqss_switch:1, lstm_switch:1<br>\nqss_switch:1, lstm_switch:1<br>\n2017-09-14 21:42:33.826598: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.58G (1701773312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.838694: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.43G (1531596032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.893832: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.903917: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.913843: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.924008: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.935385: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.946556: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\n2017-09-14 21:42:33.956340: E tensorflow/stream_executor/cuda/cuda_driver.</p>", "body_text": "My model is about 2.4GB\u3002In my inference step, I  want to load model by multi-processing method in each GPU. That means I try to make two process in one GPU and each load a model\u3002\nAfter I make configuration of each session done, each session get about 5GB memory, But I still meet the \"from device: CUDA_ERROR_OUT_OF_MEMORY\"\u3002I am wondering\u3002\u3002\u3002 Asking for help\nGPU information:\n[search@qrwt01 /home/s/apps/qtfserverd/bin]$ nvidia-smi\nThu Sep 14 21:42:48 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:08:00.0     Off |                    0 |\n| N/A   48C    P0    61W / 149W |  11366MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:09:00.0     Off |                    0 |\n| N/A   32C    P0    72W / 149W |  11359MiB / 11439MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0     33056    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |\n|    0     33057    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5515MiB |\n|    1     33058    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |\n|    1     33059    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5516MiB |\n+-----------------------------------------------------------------------------+\nSession configuration:\n46 void* create_session(void* graph, std::string& checkpoint_path,\n47     int intra_op_threads, int inter_op_threads, std::string& device_list) {\n48     Session* session = NULL;\n49     SessionOptions sess_opts;\n50     //int NUM_THREADS = 8;\n51     if (intra_op_threads > 0) {\n52         sess_opts.config.set_intra_op_parallelism_threads(intra_op_threads);\n53     }\n54     if (inter_op_threads > 0) {\n55         sess_opts.config.set_inter_op_parallelism_threads(inter_op_threads);\n56     }\n57\n58     sess_opts.config.set_allow_soft_placement(true);\n59     sess_opts.config.mutable_gpu_options()->set_visible_device_list(device_list);\n60     sess_opts.config.mutable_gpu_options()->set_allocator_type(\"BFC\");\n61     sess_opts.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);\n62     sess_opts.config.mutable_gpu_options()->set_allow_growth(true);\n63     Status status = NewSession(sess_opts, &session);\n64     if (!status.ok()) {\n65         fprintf(stderr, \"Create Session Failed %s\\n\", status.ToString().c_str());\n66         return NULL;\n67     }\nError information\nload /home/search/tensorflow/deploy_combine.model.meta graph to /gpu:1 success\n2017-09-14 21:42:31.188212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties:\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:09:00.0\ntotalMemory: 11.17GiB freeMemory: 11.05GiB\n2017-09-14 21:42:31.188260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7)\nqss_switch:1, lstm_switch:1\nqss_switch:1, lstm_switch:1\n2017-09-14 21:42:33.826598: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.58G (1701773312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.838694: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.43G (1531596032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.893832: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.903917: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.913843: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.924008: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.935385: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.946556: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\n2017-09-14 21:42:33.956340: E tensorflow/stream_executor/cuda/cuda_driver.", "body": "My model is about 2.4GB\u3002In my inference step, I  want to load model by multi-processing method in each GPU. That means I try to make two process in one GPU and each load a model\u3002\r\nAfter I make configuration of each session done, each session get about 5GB memory, But I still meet the \"from device: CUDA_ERROR_OUT_OF_MEMORY\"\u3002I am wondering\u3002\u3002\u3002 Asking for help\r\n\r\n##  **GPU information:**\r\n[search@qrwt01 /home/s/apps/qtfserverd/bin]$ nvidia-smi\r\nThu Sep 14 21:42:48 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.26                 Driver Version: 375.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:08:00.0     Off |                    0 |\r\n| N/A   48C    P0    61W / 149W |  11366MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:09:00.0     Off |                    0 |\r\n| N/A   32C    P0    72W / 149W |  11359MiB / 11439MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0     33056    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |\r\n|    0     33057    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5515MiB |\r\n|    1     33058    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5823MiB |\r\n|    1     33059    C   ...ome/s/apps/qtfserverd/etc/qtfserverd.conf  5516MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n\r\n## **Session configuration:**\r\n 46 void* create_session(void* graph, std::string& checkpoint_path,\r\n 47     int intra_op_threads, int inter_op_threads, std::string& device_list) {\r\n 48     Session* session = NULL;\r\n 49     SessionOptions sess_opts;\r\n 50     //int NUM_THREADS = 8;\r\n 51     if (intra_op_threads > 0) {\r\n 52         sess_opts.config.set_intra_op_parallelism_threads(intra_op_threads);\r\n 53     }\r\n 54     if (inter_op_threads > 0) {\r\n 55         sess_opts.config.set_inter_op_parallelism_threads(inter_op_threads);\r\n 56     }\r\n 57 \r\n 58     sess_opts.config.set_allow_soft_placement(true);\r\n 59     sess_opts.config.mutable_gpu_options()->set_visible_device_list(device_list);\r\n 60     sess_opts.config.mutable_gpu_options()->set_allocator_type(\"BFC\");\r\n 61     sess_opts.config.mutable_gpu_options()->set_per_process_gpu_memory_fraction(0.5);\r\n 62     sess_opts.config.mutable_gpu_options()->set_allow_growth(true);\r\n 63     Status status = NewSession(sess_opts, &session);\r\n 64     if (!status.ok()) {\r\n 65         fprintf(stderr, \"Create Session Failed %s\\n\", status.ToString().c_str());\r\n 66         return NULL;\r\n 67     }\r\n\r\n\r\n## **Error information**\r\nload /home/search/tensorflow/deploy_combine.model.meta graph to /gpu:1 success\r\n2017-09-14 21:42:31.188212: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:09:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.05GiB\r\n2017-09-14 21:42:31.188260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1055] Creating TensorFlow device (/device:GPU:0) -> (device: 1, name: Tesla K80, pci bus id: 0000:09:00.0, compute capability: 3.7)\r\nqss_switch:1, lstm_switch:1\r\nqss_switch:1, lstm_switch:1\r\n2017-09-14 21:42:33.826598: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.58G (1701773312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.838694: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 1.43G (1531596032 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.893832: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.903917: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.913843: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.924008: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.935385: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.946556: E tensorflow/stream_executor/cuda/cuda_driver.cc:936] failed to allocate 439.82M (461180672 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\n2017-09-14 21:42:33.956340: E tensorflow/stream_executor/cuda/cuda_driver.\r\n"}