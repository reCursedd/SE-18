{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19948", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19948/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19948/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19948/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19948", "id": 331695760, "node_id": "MDU6SXNzdWUzMzE2OTU3NjA=", "number": 19948, "title": "superfluous tf.reshape affacts output", "user": {"login": "tonyxty", "id": 3627229, "node_id": "MDQ6VXNlcjM2MjcyMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/3627229?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tonyxty", "html_url": "https://github.com/tonyxty", "followers_url": "https://api.github.com/users/tonyxty/followers", "following_url": "https://api.github.com/users/tonyxty/following{/other_user}", "gists_url": "https://api.github.com/users/tonyxty/gists{/gist_id}", "starred_url": "https://api.github.com/users/tonyxty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tonyxty/subscriptions", "organizations_url": "https://api.github.com/users/tonyxty/orgs", "repos_url": "https://api.github.com/users/tonyxty/repos", "events_url": "https://api.github.com/users/tonyxty/events{/privacy}", "received_events_url": "https://api.github.com/users/tonyxty/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-06-12T18:08:14Z", "updated_at": "2018-06-13T07:20:42Z", "closed_at": "2018-06-13T04:36:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 18.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.8.0-0-g93bc2e2072 1.8.0</li>\n<li><strong>Python version</strong>: Python 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: run the following code snippet</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>The code snippet attached, when run with different values for <code>reshape_iter</code>, produces different results.</p>\n<p>This should not happen since <code>reshape</code>-ing a tensor repeatedly to the same shape is, by definition, idempotent.  Could it be that <code>reshape</code> affects the internal state of the random number generator?</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tflearn\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">network</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">reshape_iter</span>):\n    _, height, width, c_dim <span class=\"pl-k\">=</span> x.shape\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(reshape_iter):\n        x <span class=\"pl-k\">=</span> tf.reshape(x, (<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, height <span class=\"pl-k\">*</span> width <span class=\"pl-k\">*</span> c_dim))\n    logits <span class=\"pl-k\">=</span> tflearn.fully_connected(x, <span class=\"pl-c1\">1</span>)\n    x <span class=\"pl-k\">=</span> tflearn.activation(logits, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> x, logits\n\n\nnp.random.seed(<span class=\"pl-c1\">123456</span>)\ntf.set_random_seed(<span class=\"pl-c1\">123456</span>)\n\ndisc_input <span class=\"pl-k\">=</span> tflearn.input_data(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>))\ndisc_output, logits <span class=\"pl-k\">=</span> network(disc_input, <span class=\"pl-v\">reshape_iter</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>)\n\nsample <span class=\"pl-k\">=</span> np.random.random((<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">3</span>))\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    disc_output_val, logits_val, W_val <span class=\"pl-k\">=</span> sess.run(\n            (disc_output, logits, logits.W), <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{disc_input: sample}\n    )\n\n<span class=\"pl-c1\">print</span>(disc_output_val.reshape((<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,)))\n<span class=\"pl-c1\">print</span>(W_val.reshape((<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,)))</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): v1.8.0-0-g93bc2e2072 1.8.0\nPython version: Python 3.6.5\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: run the following code snippet\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nThe code snippet attached, when run with different values for reshape_iter, produces different results.\nThis should not happen since reshape-ing a tensor repeatedly to the same shape is, by definition, idempotent.  Could it be that reshape affects the internal state of the random number generator?\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nimport tflearn\n\n\ndef network(x, reshape_iter):\n    _, height, width, c_dim = x.shape\n    for i in range(reshape_iter):\n        x = tf.reshape(x, (-1, height * width * c_dim))\n    logits = tflearn.fully_connected(x, 1)\n    x = tflearn.activation(logits, 'sigmoid')\n    return x, logits\n\n\nnp.random.seed(123456)\ntf.set_random_seed(123456)\n\ndisc_input = tflearn.input_data(shape=(None, 128, 128, 3))\ndisc_output, logits = network(disc_input, reshape_iter=5)\n\nsample = np.random.random((8, 128, 128, 3))\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    disc_output_val, logits_val, W_val = sess.run(\n            (disc_output, logits, logits.W), feed_dict={disc_input: sample}\n    )\n\nprint(disc_output_val.reshape((-1,)))\nprint(W_val.reshape((-1,)))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: v1.8.0-0-g93bc2e2072 1.8.0\r\n- **Python version**: Python 3.6.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: run the following code snippet\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nThe code snippet attached, when run with different values for `reshape_iter`, produces different results.\r\n\r\nThis should not happen since `reshape`-ing a tensor repeatedly to the same shape is, by definition, idempotent.  Could it be that `reshape` affects the internal state of the random number generator?\r\n\r\n### Source code / logs\r\n````python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tflearn\r\n\r\n\r\ndef network(x, reshape_iter):\r\n    _, height, width, c_dim = x.shape\r\n    for i in range(reshape_iter):\r\n        x = tf.reshape(x, (-1, height * width * c_dim))\r\n    logits = tflearn.fully_connected(x, 1)\r\n    x = tflearn.activation(logits, 'sigmoid')\r\n    return x, logits\r\n\r\n\r\nnp.random.seed(123456)\r\ntf.set_random_seed(123456)\r\n\r\ndisc_input = tflearn.input_data(shape=(None, 128, 128, 3))\r\ndisc_output, logits = network(disc_input, reshape_iter=5)\r\n\r\nsample = np.random.random((8, 128, 128, 3))\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    disc_output_val, logits_val, W_val = sess.run(\r\n            (disc_output, logits, logits.W), feed_dict={disc_input: sample}\r\n    )\r\n\r\nprint(disc_output_val.reshape((-1,)))\r\nprint(W_val.reshape((-1,)))\r\n````"}