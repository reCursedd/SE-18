{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347724430", "html_url": "https://github.com/tensorflow/tensorflow/issues/14613#issuecomment-347724430", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14613", "id": 347724430, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzcyNDQzMA==", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T01:41:58Z", "updated_at": "2017-11-29T01:43:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> I shared the graph just to illustrate that <code>MonitoredTrainingSession</code> is initialized repeatedly during a training. The graph looks that way because the checkpoint is being saved rarely. The problem is that there are no errors whatsoever.<br>\nIn the beginning I thought it maybe restarting due to OOMs and just not showing the errors, but I checked with bigger models and tf catches OOMs just fine.<br>\nMy setup is an alpha k8s cluster on GCP that I used successfully since it's release this summer:</p>\n<pre><code># Cluster settings.\nCLUSTER_NAME=#####\nNUM_NODES=10\nZONE=us-west1-b\nPROJECT=#####\nCLUSTER_VERSION=1.7.8-gke.0\nDISK_SIZE=75\nIMAGE_TYPE=COS\nMACHINE_TYPE=n1-highmem-4\nTYPE=nvidia-tesla-k80\nCOUNT=1\n\n# NodePool Settings.\nNP_NAME=ps-cpu\nNP_NUM_NODES=1\nNP_MACHINE_TYPE=n1-highmem-2\n\nsudo gcloud alpha container clusters create ${CLUSTER_NAME} \\\n    --num-nodes=${NUM_NODES} \\\n    --zone=${ZONE} \\\n    --project=${PROJECT} \\\n    --cluster-version=${CLUSTER_VERSION} \\\n    --disk-size=${DISK_SIZE} \\\n    --image-type=${IMAGE_TYPE} \\\n    --machine-type=${MACHINE_TYPE} \\\n    --accelerator=type=${TYPE},count=${COUNT} \\\n    --enable-kubernetes-alpha \\\n    --scopes storage-full\n\nsudo gcloud alpha config set container/cluster ${CLUSTER_NAME}\n\nsudo gcloud alpha container clusters get-credentials ${CLUSTER_NAME} \\\n    --zone=${ZONE}\n\n# This is a hack from github.com/ContainerEngine/accelerators/tree/master/cos-nvidia-gpu-installer\n# - the only possible way to expose GPUs of GKE nodes for now.\nsudo kubectl create -f https://raw.githubusercontent.com/ContainerEngine/accelerators/master/cos-nvidia-gpu-installer/daemonset.yaml\n\nsudo gcloud alpha container node-pools create ${NP_NAME} \\\n    --num-nodes=${NP_NUM_NODES} \\\n    --zone=${ZONE} \\\n    --disk-size=${DISK_SIZE} \\\n    --image-type=${IMAGE_TYPE} \\\n    --machine-type=${NP_MACHINE_TYPE} \\\n    --scopes storage-full \\\n    --cluster=${CLUSTER_NAME}\n</code></pre>\n<p>I have 1 chief, 8 training workers and 1 evaluation worker.<br>\nEverything is being controlled by <code>MonitoredTrainingSession</code> in a data-parallel way.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a> As I mentioned before, I tried this fix <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255772759\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12859\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/12859/hovercard?comment_id=327783827&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-327783827\">#12859 (comment)</a>, instead of <code> mon_sess._coordinated_creator.tf_sess.run()</code> and still got the same error. The latter is just for multi-task iterator initialization convenience. I haven't tried it again after <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>'s fix with <code>shared_name</code>, but I assume it's not the cause, as they both are due to session restarts. Though, I am going to add a proper <code>SessionRunHook</code> as you suggested, to eliminate the possibility of an internal leak.</p>\n<p>My biggest confusion is that there is no INFO on to why <code>MonitoredTrainingSession</code> restarts without warnings.</p>", "body_text": "@mrry I shared the graph just to illustrate that MonitoredTrainingSession is initialized repeatedly during a training. The graph looks that way because the checkpoint is being saved rarely. The problem is that there are no errors whatsoever.\nIn the beginning I thought it maybe restarting due to OOMs and just not showing the errors, but I checked with bigger models and tf catches OOMs just fine.\nMy setup is an alpha k8s cluster on GCP that I used successfully since it's release this summer:\n# Cluster settings.\nCLUSTER_NAME=#####\nNUM_NODES=10\nZONE=us-west1-b\nPROJECT=#####\nCLUSTER_VERSION=1.7.8-gke.0\nDISK_SIZE=75\nIMAGE_TYPE=COS\nMACHINE_TYPE=n1-highmem-4\nTYPE=nvidia-tesla-k80\nCOUNT=1\n\n# NodePool Settings.\nNP_NAME=ps-cpu\nNP_NUM_NODES=1\nNP_MACHINE_TYPE=n1-highmem-2\n\nsudo gcloud alpha container clusters create ${CLUSTER_NAME} \\\n    --num-nodes=${NUM_NODES} \\\n    --zone=${ZONE} \\\n    --project=${PROJECT} \\\n    --cluster-version=${CLUSTER_VERSION} \\\n    --disk-size=${DISK_SIZE} \\\n    --image-type=${IMAGE_TYPE} \\\n    --machine-type=${MACHINE_TYPE} \\\n    --accelerator=type=${TYPE},count=${COUNT} \\\n    --enable-kubernetes-alpha \\\n    --scopes storage-full\n\nsudo gcloud alpha config set container/cluster ${CLUSTER_NAME}\n\nsudo gcloud alpha container clusters get-credentials ${CLUSTER_NAME} \\\n    --zone=${ZONE}\n\n# This is a hack from github.com/ContainerEngine/accelerators/tree/master/cos-nvidia-gpu-installer\n# - the only possible way to expose GPUs of GKE nodes for now.\nsudo kubectl create -f https://raw.githubusercontent.com/ContainerEngine/accelerators/master/cos-nvidia-gpu-installer/daemonset.yaml\n\nsudo gcloud alpha container node-pools create ${NP_NAME} \\\n    --num-nodes=${NP_NUM_NODES} \\\n    --zone=${ZONE} \\\n    --disk-size=${DISK_SIZE} \\\n    --image-type=${IMAGE_TYPE} \\\n    --machine-type=${NP_MACHINE_TYPE} \\\n    --scopes storage-full \\\n    --cluster=${CLUSTER_NAME}\n\nI have 1 chief, 8 training workers and 1 evaluation worker.\nEverything is being controlled by MonitoredTrainingSession in a data-parallel way.\n@ispirmustafa As I mentioned before, I tried this fix #12859 (comment), instead of  mon_sess._coordinated_creator.tf_sess.run() and still got the same error. The latter is just for multi-task iterator initialization convenience. I haven't tried it again after @mrry's fix with shared_name, but I assume it's not the cause, as they both are due to session restarts. Though, I am going to add a proper SessionRunHook as you suggested, to eliminate the possibility of an internal leak.\nMy biggest confusion is that there is no INFO on to why MonitoredTrainingSession restarts without warnings.", "body": "@mrry I shared the graph just to illustrate that `MonitoredTrainingSession` is initialized repeatedly during a training. The graph looks that way because the checkpoint is being saved rarely. The problem is that there are no errors whatsoever. \r\nIn the beginning I thought it maybe restarting due to OOMs and just not showing the errors, but I checked with bigger models and tf catches OOMs just fine.\r\nMy setup is an alpha k8s cluster on GCP that I used successfully since it's release this summer:\r\n```\r\n# Cluster settings.\r\nCLUSTER_NAME=#####\r\nNUM_NODES=10\r\nZONE=us-west1-b\r\nPROJECT=#####\r\nCLUSTER_VERSION=1.7.8-gke.0\r\nDISK_SIZE=75\r\nIMAGE_TYPE=COS\r\nMACHINE_TYPE=n1-highmem-4\r\nTYPE=nvidia-tesla-k80\r\nCOUNT=1\r\n\r\n# NodePool Settings.\r\nNP_NAME=ps-cpu\r\nNP_NUM_NODES=1\r\nNP_MACHINE_TYPE=n1-highmem-2\r\n\r\nsudo gcloud alpha container clusters create ${CLUSTER_NAME} \\\r\n    --num-nodes=${NUM_NODES} \\\r\n    --zone=${ZONE} \\\r\n    --project=${PROJECT} \\\r\n    --cluster-version=${CLUSTER_VERSION} \\\r\n    --disk-size=${DISK_SIZE} \\\r\n    --image-type=${IMAGE_TYPE} \\\r\n    --machine-type=${MACHINE_TYPE} \\\r\n    --accelerator=type=${TYPE},count=${COUNT} \\\r\n    --enable-kubernetes-alpha \\\r\n    --scopes storage-full\r\n\r\nsudo gcloud alpha config set container/cluster ${CLUSTER_NAME}\r\n\r\nsudo gcloud alpha container clusters get-credentials ${CLUSTER_NAME} \\\r\n    --zone=${ZONE}\r\n\r\n# This is a hack from github.com/ContainerEngine/accelerators/tree/master/cos-nvidia-gpu-installer\r\n# - the only possible way to expose GPUs of GKE nodes for now.\r\nsudo kubectl create -f https://raw.githubusercontent.com/ContainerEngine/accelerators/master/cos-nvidia-gpu-installer/daemonset.yaml\r\n\r\nsudo gcloud alpha container node-pools create ${NP_NAME} \\\r\n    --num-nodes=${NP_NUM_NODES} \\\r\n    --zone=${ZONE} \\\r\n    --disk-size=${DISK_SIZE} \\\r\n    --image-type=${IMAGE_TYPE} \\\r\n    --machine-type=${NP_MACHINE_TYPE} \\\r\n    --scopes storage-full \\\r\n    --cluster=${CLUSTER_NAME}\r\n```\r\nI have 1 chief, 8 training workers and 1 evaluation worker.\r\nEverything is being controlled by `MonitoredTrainingSession` in a data-parallel way.\r\n\r\n@ispirmustafa As I mentioned before, I tried this fix https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-327783827, instead of `\r\nmon_sess._coordinated_creator.tf_sess.run()` and still got the same error. The latter is just for multi-task iterator initialization convenience. I haven't tried it again after @mrry's fix with `shared_name`, but I assume it's not the cause, as they both are due to session restarts. Though, I am going to add a proper `SessionRunHook` as you suggested, to eliminate the possibility of an internal leak.\r\n\r\nMy biggest confusion is that there is no INFO on to why `MonitoredTrainingSession` restarts without warnings."}