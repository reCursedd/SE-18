{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14613", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14613/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14613/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14613/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14613", "id": 274405576, "node_id": "MDU6SXNzdWUyNzQ0MDU1NzY=", "number": 14613, "title": "tf.data.Iterator.from_string_handle() breaking behaviour in r1.4 compared to r1.3.1", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "isaprykin", "id": 234070, "node_id": "MDQ6VXNlcjIzNDA3MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/234070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isaprykin", "html_url": "https://github.com/isaprykin", "followers_url": "https://api.github.com/users/isaprykin/followers", "following_url": "https://api.github.com/users/isaprykin/following{/other_user}", "gists_url": "https://api.github.com/users/isaprykin/gists{/gist_id}", "starred_url": "https://api.github.com/users/isaprykin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isaprykin/subscriptions", "organizations_url": "https://api.github.com/users/isaprykin/orgs", "repos_url": "https://api.github.com/users/isaprykin/repos", "events_url": "https://api.github.com/users/isaprykin/events{/privacy}", "received_events_url": "https://api.github.com/users/isaprykin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "isaprykin", "id": 234070, "node_id": "MDQ6VXNlcjIzNDA3MA==", "avatar_url": "https://avatars1.githubusercontent.com/u/234070?v=4", "gravatar_id": "", "url": "https://api.github.com/users/isaprykin", "html_url": "https://github.com/isaprykin", "followers_url": "https://api.github.com/users/isaprykin/followers", "following_url": "https://api.github.com/users/isaprykin/following{/other_user}", "gists_url": "https://api.github.com/users/isaprykin/gists{/gist_id}", "starred_url": "https://api.github.com/users/isaprykin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/isaprykin/subscriptions", "organizations_url": "https://api.github.com/users/isaprykin/orgs", "repos_url": "https://api.github.com/users/isaprykin/repos", "events_url": "https://api.github.com/users/isaprykin/events{/privacy}", "received_events_url": "https://api.github.com/users/isaprykin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2017-11-16T06:13:56Z", "updated_at": "2018-04-26T19:36:01Z", "closed_at": "2018-04-26T19:36:01Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: +</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.4.0-rc1-11-g130a514 1.4.0</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: -</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04</li>\n<li><strong>GPU model and memory</strong>: NVIDIA\u00ae Tesla\u00ae K80 (GCE)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Context:</h3>\n<p>Same setup/context as in the previous issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255772759\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12859\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/12859/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/12859\">#12859</a> including this fix <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"255772759\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12859\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/12859/hovercard?comment_id=327783827&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-327783827\">#12859 (comment)</a>.<br>\nIn addition the training is now in a Multi-task Learning mode, so feedable contrib Iterator was used to accommodate different datasets for each task.<br>\nSimplified snippet:</p>\n<div class=\"highlight highlight-source-python\"><pre>        <span class=\"pl-c1\">self</span>.handle <span class=\"pl-k\">=</span> tf.placeholder(tf.string, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[])\n\n        <span class=\"pl-c1\">...</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> targets -&gt; Multi-task training targets.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> datasets -&gt; dict of Multi-task target tf.Datasets.</span>\n        <span class=\"pl-k\">for</span> target <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.targets:\n            <span class=\"pl-c1\">self</span>.datasets[target] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.create_TFDataset()\n\n        <span class=\"pl-c1\">self</span>.iterator <span class=\"pl-k\">=</span> tf.contrib.data.Iterator.from_string_handle(\n            <span class=\"pl-c1\">self</span>.handle,\n            <span class=\"pl-c1\">self</span>.datasets[targets[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]].output_types,\n            <span class=\"pl-c1\">self</span>.datasets[targets[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]].output_shapes)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> iter_init_ops and iter_handles -&gt; init_ops &amp; handles per each task.</span>\n        <span class=\"pl-k\">for</span> target <span class=\"pl-k\">in</span> <span class=\"pl-c1\">self</span>.targets:\n            <span class=\"pl-c1\">self</span>.iterators[target] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.datasets[target].make_initializable_iterator()\n            <span class=\"pl-c1\">self</span>.iter_init_ops[target] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.iterators[target].initializer\n            <span class=\"pl-c1\">self</span>.iter_handles[target] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.iterators[target].string_handle()\n\n        <span class=\"pl-c1\">...</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Within tf.train.MonitoredTrainingSession as mon_sess.</span>\n        <span class=\"pl-k\">for</span> target <span class=\"pl-k\">in</span> targets:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get all target datasets handles.</span>\n            handle[target] <span class=\"pl-k\">=</span> mon_sess._coordinated_creator.tf_sess.run(\n                training_model.iter_handles[target])\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Init all target datasets.</span>\n            mon_sess._coordinated_creator.tf_sess.run(training_model.iter_init_ops[target])\n\n        <span class=\"pl-c1\">...</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Training step for a specific target.</span>\n        input_feed <span class=\"pl-k\">=</span> {<span class=\"pl-c1\">self</span>.handle:handle[target]}\n        output_feed <span class=\"pl-k\">=</span> [\n            <span class=\"pl-c1\">self</span>.update_ops[target],\n            <span class=\"pl-c1\">self</span>.losses[target], \n            <span class=\"pl-c1\">self</span>.metrics[target],\n        ]\n        outputs <span class=\"pl-k\">=</span> session.run(output_feed, input_feed, <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>options,\n                                             <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>run_metadata)</pre></div>\n<h3>Problem:</h3>\n<p>This system worked flawlessly for tf 1.3 &amp; tf 1.3.1.<br>\nAfter a planned update this week to tf 1.4 the following Error would appear at absolute random (it might appear after 5 seconds or after a few hours of training):</p>\n<div class=\"highlight highlight-text-shell-session\"><pre><span class=\"pl-c1\">...</span>\n<span class=\"pl-c1\">W tensorflow/core/framework/op_kernel.cc:1192] Unavailable: Endpoint read failed</span>\n<span class=\"pl-c1\">\t [[Node: Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w_S543 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:1/device:CPU:0\", send_device_incarnation=-458934800929363750, tensor_name=\"edge_9_Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/Select_G315 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=3243841103411587778, tensor_name=\"edge_4692_Training_Graph/Model/TARGET/TARGET_Metrics/Select\", tensor_type=DT_DOUBLE, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]</span>\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-c1\">W tensorflow/core/framework/op_kernel.cc:1192] Not found: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]</span>\n<span class=\"pl-c1\">...</span>\n<span class=\"pl-c1\">Traceback (most recent call last):</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call</span>\n<span class=\"pl-c1\">    return fn(*args)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn</span>\n<span class=\"pl-c1\">    status, run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__</span>\n<span class=\"pl-c1\">    c_api.TF_GetCode(self.status.status))</span>\n<span class=\"pl-c1\">tensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]</span>\n\n<span class=\"pl-c1\">During handling of the above exception, another exception occurred:</span>\n\n<span class=\"pl-c1\">Traceback (most recent call last):</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 684, in &lt;module&gt;</span>\n<span class=\"pl-c1\">    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run</span>\n<span class=\"pl-c1\">    _sys.exit(main(_sys.argv[:1] + flags_passthrough))</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 640, in main</span>\n<span class=\"pl-c1\">    create_job()</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 628, in create_job</span>\n<span class=\"pl-c1\">    run_worker(server, cluster)</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 429, in run_worker</span>\n<span class=\"pl-c1\">    profile=profile</span>\n<span class=\"pl-c1\">  File \"/HARNN.py\", line 1576, in step_dist_gpu</span>\n<span class=\"pl-c1\">    run_metadata=run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 521, in run</span>\n<span class=\"pl-c1\">    run_metadata=run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 892, in run</span>\n<span class=\"pl-c1\">    run_metadata=run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 967, in run</span>\n<span class=\"pl-c1\">    raise six.reraise(*original_exc_info)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise</span>\n<span class=\"pl-c1\">    raise value</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 952, in run</span>\n<span class=\"pl-c1\">    return self._sess.run(*args, **kwargs)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run</span>\n<span class=\"pl-c1\">    run_metadata=run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in run</span>\n<span class=\"pl-c1\">    return self._sess.run(*args, **kwargs)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 889, in run</span>\n<span class=\"pl-c1\">    run_metadata_ptr)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run</span>\n<span class=\"pl-c1\">    feed_dict_tensor, options, run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run</span>\n<span class=\"pl-c1\">    options, run_metadata)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call</span>\n<span class=\"pl-c1\">    raise type(e)(node_def, op, message)</span>\n<span class=\"pl-c1\">tensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]</span>\n\n<span class=\"pl-c1\">Caused by op 'Training_Graph/Model/IteratorFromStringHandle', defined at:</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 684, in &lt;module&gt;</span>\n<span class=\"pl-c1\">    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run</span>\n<span class=\"pl-c1\">    _sys.exit(main(_sys.argv[:1] + flags_passthrough))</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 640, in main</span>\n<span class=\"pl-c1\">    create_job()</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 628, in create_job</span>\n<span class=\"pl-c1\">    run_worker(server, cluster)</span>\n<span class=\"pl-c1\">  File \"dist_train.py\", line 272, in run_worker</span>\n<span class=\"pl-c1\">    training_model.build_graph()</span>\n<span class=\"pl-c1\">  File \"/HARNN.py\", line 221, in build_graph</span>\n<span class=\"pl-c1\">    self._init_dataset()</span>\n<span class=\"pl-c1\">  File \"/HARNN.py\", line 329, in _init_dataset</span>\n<span class=\"pl-c1\">    self.datasets[self.targets[-1]].output_shapes)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 189, in from_string_handle</span>\n<span class=\"pl-c1\">    output_shapes=nest.flatten(output_shapes))</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 662, in iterator_from_string_handle</span>\n<span class=\"pl-c1\">    output_types=output_types, output_shapes=output_shapes, name=name)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper</span>\n<span class=\"pl-c1\">    op_def=op_def)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op</span>\n<span class=\"pl-c1\">    op_def=op_def)</span>\n<span class=\"pl-c1\">  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__</span>\n<span class=\"pl-c1\">    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access</span>\n\n<span class=\"pl-c1\">NotFoundError (see above for traceback): Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]</span>\n<span class=\"pl-c1\">\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]</span></pre></div>\n<h3>Statement:</h3>\n<p>What exactly have changed in tf.data.Iterator between versions r1.4 and r1.3.1 that causes such an unpleasant behaviour? What can be done to counteract <code>NotFoundError</code>?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): +\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.4.0-rc1-11-g130a514 1.4.0\nPython version: Python 3.5.2\nBazel version (if compiling from source): -\nGCC/Compiler version (if compiling from source): -\nCUDA/cuDNN version: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04\nGPU model and memory: NVIDIA\u00ae Tesla\u00ae K80 (GCE)\nExact command to reproduce:\n\nContext:\nSame setup/context as in the previous issue #12859 including this fix #12859 (comment).\nIn addition the training is now in a Multi-task Learning mode, so feedable contrib Iterator was used to accommodate different datasets for each task.\nSimplified snippet:\n        self.handle = tf.placeholder(tf.string, shape=[])\n\n        ...\n\n        # targets -> Multi-task training targets.\n        # datasets -> dict of Multi-task target tf.Datasets.\n        for target in self.targets:\n            self.datasets[target] = self.create_TFDataset()\n\n        self.iterator = tf.contrib.data.Iterator.from_string_handle(\n            self.handle,\n            self.datasets[targets[-1]].output_types,\n            self.datasets[targets[-1]].output_shapes)\n\n        # iter_init_ops and iter_handles -> init_ops & handles per each task.\n        for target in self.targets:\n            self.iterators[target] = self.datasets[target].make_initializable_iterator()\n            self.iter_init_ops[target] = self.iterators[target].initializer\n            self.iter_handles[target] = self.iterators[target].string_handle()\n\n        ...\n        # Within tf.train.MonitoredTrainingSession as mon_sess.\n        for target in targets:\n            # Get all target datasets handles.\n            handle[target] = mon_sess._coordinated_creator.tf_sess.run(\n                training_model.iter_handles[target])\n            # Init all target datasets.\n            mon_sess._coordinated_creator.tf_sess.run(training_model.iter_init_ops[target])\n\n        ...\n        # Training step for a specific target.\n        input_feed = {self.handle:handle[target]}\n        output_feed = [\n            self.update_ops[target],\n            self.losses[target], \n            self.metrics[target],\n        ]\n        outputs = session.run(output_feed, input_feed, options=options,\n                                             run_metadata=run_metadata)\nProblem:\nThis system worked flawlessly for tf 1.3 & tf 1.3.1.\nAfter a planned update this week to tf 1.4 the following Error would appear at absolute random (it might appear after 5 seconds or after a few hours of training):\n...\nW tensorflow/core/framework/op_kernel.cc:1192] Unavailable: Endpoint read failed\n\t [[Node: Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w_S543 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:1/device:CPU:0\", send_device_incarnation=-458934800929363750, tensor_name=\"edge_9_Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/Select_G315 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=3243841103411587778, tensor_name=\"edge_4692_Training_Graph/Model/TARGET/TARGET_Metrics/Select\", tensor_type=DT_DOUBLE, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\n...\nW tensorflow/core/framework/op_kernel.cc:1192] Not found: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\n...\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\n    return fn(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\n    status, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\n    c_api.TF_GetCode(self.status.status))\ntensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"dist_train.py\", line 684, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"dist_train.py\", line 640, in main\n    create_job()\n  File \"dist_train.py\", line 628, in create_job\n    run_worker(server, cluster)\n  File \"dist_train.py\", line 429, in run_worker\n    profile=profile\n  File \"/HARNN.py\", line 1576, in step_dist_gpu\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\n    raise six.reraise(*original_exc_info)\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\n    return self._sess.run(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in run\n    return self._sess.run(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 889, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\n    options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\n\nCaused by op 'Training_Graph/Model/IteratorFromStringHandle', defined at:\n  File \"dist_train.py\", line 684, in <module>\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"dist_train.py\", line 640, in main\n    create_job()\n  File \"dist_train.py\", line 628, in create_job\n    run_worker(server, cluster)\n  File \"dist_train.py\", line 272, in run_worker\n    training_model.build_graph()\n  File \"/HARNN.py\", line 221, in build_graph\n    self._init_dataset()\n  File \"/HARNN.py\", line 329, in _init_dataset\n    self.datasets[self.targets[-1]].output_shapes)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 189, in from_string_handle\n    output_shapes=nest.flatten(output_shapes))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 662, in iterator_from_string_handle\n    output_types=output_types, output_shapes=output_shapes, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nNotFoundError (see above for traceback): Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\nStatement:\nWhat exactly have changed in tf.data.Iterator between versions r1.4 and r1.3.1 that causes such an unpleasant behaviour? What can be done to counteract NotFoundError?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: nvidia/cuda:8.0-cudnn6-devel-ubuntu16.04\r\n- **GPU model and memory**: NVIDIA\u00ae Tesla\u00ae K80 (GCE)\r\n- **Exact command to reproduce**:\r\n\r\n### Context:\r\nSame setup/context as in the previous issue https://github.com/tensorflow/tensorflow/issues/12859 including this fix https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-327783827.\r\nIn addition the training is now in a Multi-task Learning mode, so feedable contrib Iterator was used to accommodate different datasets for each task.\r\nSimplified snippet:\r\n~~~python\r\n        self.handle = tf.placeholder(tf.string, shape=[])\r\n\r\n        ...\r\n\r\n        # targets -> Multi-task training targets.\r\n        # datasets -> dict of Multi-task target tf.Datasets.\r\n        for target in self.targets:\r\n            self.datasets[target] = self.create_TFDataset()\r\n\r\n        self.iterator = tf.contrib.data.Iterator.from_string_handle(\r\n            self.handle,\r\n            self.datasets[targets[-1]].output_types,\r\n            self.datasets[targets[-1]].output_shapes)\r\n\r\n        # iter_init_ops and iter_handles -> init_ops & handles per each task.\r\n        for target in self.targets:\r\n            self.iterators[target] = self.datasets[target].make_initializable_iterator()\r\n            self.iter_init_ops[target] = self.iterators[target].initializer\r\n            self.iter_handles[target] = self.iterators[target].string_handle()\r\n\r\n        ...\r\n        # Within tf.train.MonitoredTrainingSession as mon_sess.\r\n        for target in targets:\r\n            # Get all target datasets handles.\r\n            handle[target] = mon_sess._coordinated_creator.tf_sess.run(\r\n                training_model.iter_handles[target])\r\n            # Init all target datasets.\r\n            mon_sess._coordinated_creator.tf_sess.run(training_model.iter_init_ops[target])\r\n\r\n        ...\r\n        # Training step for a specific target.\r\n        input_feed = {self.handle:handle[target]}\r\n        output_feed = [\r\n            self.update_ops[target],\r\n            self.losses[target], \r\n            self.metrics[target],\r\n        ]\r\n        outputs = session.run(output_feed, input_feed, options=options,\r\n                                             run_metadata=run_metadata)\r\n~~~\r\n### Problem:\r\nThis system worked flawlessly for tf 1.3 & tf 1.3.1. \r\nAfter a planned update this week to tf 1.4 the following Error would appear at absolute random (it might appear after 5 seconds or after a few hours of training):\r\n~~~console\r\n...\r\nW tensorflow/core/framework/op_kernel.cc:1192] Unavailable: Endpoint read failed\r\n\t [[Node: Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w_S543 = _Recv[_start_time=0, client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:ps/replica:0/task:1/device:CPU:0\", send_device_incarnation=-458934800929363750, tensor_name=\"edge_9_Model/Generate_BiRNN/BiRNN_Logic/bidirectional_rnn/fw/carry_w\", tensor_type=DT_FLOAT, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/Select_G315 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device_incarnation=3243841103411587778, tensor_name=\"edge_4692_Training_Graph/Model/TARGET/TARGET_Metrics/Select\", tensor_type=DT_DOUBLE, _device=\"/job:worker/replica:0/task:0/device:CPU:0\"]()]]\r\n...\r\nW tensorflow/core/framework/op_kernel.cc:1192] Not found: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n...\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1323, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1302, in _run_fn\r\n    status, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\", line 473, in __exit__\r\n    c_api.TF_GetCode(self.status.status))\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"dist_train.py\", line 684, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"dist_train.py\", line 640, in main\r\n    create_job()\r\n  File \"dist_train.py\", line 628, in create_job\r\n    run_worker(server, cluster)\r\n  File \"dist_train.py\", line 429, in run_worker\r\n    profile=profile\r\n  File \"/HARNN.py\", line 1576, in step_dist_gpu\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 521, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 892, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 967, in run\r\n    raise six.reraise(*original_exc_info)\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 952, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1024, in run\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 827, in run\r\n    return self._sess.run(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 889, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1120, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1317, in _do_run\r\n    options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1336, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n\r\nCaused by op 'Training_Graph/Model/IteratorFromStringHandle', defined at:\r\n  File \"dist_train.py\", line 684, in <module>\r\n    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"dist_train.py\", line 640, in main\r\n    create_job()\r\n  File \"dist_train.py\", line 628, in create_job\r\n    run_worker(server, cluster)\r\n  File \"dist_train.py\", line 272, in run_worker\r\n    training_model.build_graph()\r\n  File \"/HARNN.py\", line 221, in build_graph\r\n    self._init_dataset()\r\n  File \"/HARNN.py\", line 329, in _init_dataset\r\n    self.datasets[self.targets[-1]].output_shapes)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py\", line 189, in from_string_handle\r\n    output_shapes=nest.flatten(output_shapes))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 662, in iterator_from_string_handle\r\n    output_types=output_types, output_shapes=output_shapes, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nNotFoundError (see above for traceback): Resource worker/_3_Training_Graph/Model/Iterator/N10tensorflow12_GLOBAL__N_116IteratorResourceE does not exist.\r\n\t [[Node: Training_Graph/Model/IteratorFromStringHandle = IteratorFromStringHandle[output_shapes=[[?,?,?], [?,?,?], [?], [?,?], [?,?], [?], [?]], output_types=[DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_INT64, DT_FLOAT, DT_INT32], _device=\"/job:worker/replica:0/task:0/device:CPU:0\"](_recv_Training_Graph/Model/Placeholder_0)]]\r\n\t [[Node: Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26_G1265 = _Recv[client_terminated=false, recv_device=\"/job:worker/replica:0/task:0/device:GPU:0\", send_device=\"/job:worker/replica:0/task:0/device:CPU:0\", send_device_incarnation=-3380386273340330983, tensor_name=\"edge_4280_Training_Graph/Model/TARGET/TARGET_Metrics/DenseToDenseSetOperation_26\", tensor_type=DT_INT64, _device=\"/job:worker/replica:0/task:0/device:GPU:0\"]()]]\r\n~~~\r\n\r\n### Statement:\r\nWhat exactly have changed in tf.data.Iterator between versions r1.4 and r1.3.1 that causes such an unpleasant behaviour? What can be done to counteract `NotFoundError`?"}