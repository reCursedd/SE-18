{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19834", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19834/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19834/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19834/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19834", "id": 330244311, "node_id": "MDU6SXNzdWUzMzAyNDQzMTE=", "number": 19834, "title": "Attention Wrapper state error.", "user": {"login": "vsai121", "id": 23053697, "node_id": "MDQ6VXNlcjIzMDUzNjk3", "avatar_url": "https://avatars0.githubusercontent.com/u/23053697?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vsai121", "html_url": "https://github.com/vsai121", "followers_url": "https://api.github.com/users/vsai121/followers", "following_url": "https://api.github.com/users/vsai121/following{/other_user}", "gists_url": "https://api.github.com/users/vsai121/gists{/gist_id}", "starred_url": "https://api.github.com/users/vsai121/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vsai121/subscriptions", "organizations_url": "https://api.github.com/users/vsai121/orgs", "repos_url": "https://api.github.com/users/vsai121/repos", "events_url": "https://api.github.com/users/vsai121/events{/privacy}", "received_events_url": "https://api.github.com/users/vsai121/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-06-07T12:10:46Z", "updated_at": "2018-08-24T23:31:49Z", "closed_at": "2018-08-24T23:31:49Z", "author_association": "NONE", "body_html": "<p>Hello! I am implementing a seq2seq model with attention for multi-step time series forecasting. I am encountering an error with implementing attention for MultiRNN cell.</p>\n<p>System Information</p>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:   Yes , I have attached my code below.</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong> - Linux Ubuntu 16.04</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>: - Source</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>: -VERSION = 1.4.1</p>\n</li>\n<li>\n<p><strong>Python version</strong>: 2.7.12</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:  - NA</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>: COMPILER_VERSION = v1.4.0-19-ga52c8d9</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>: - NA</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>: - NA</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\nI am implementing a seq2seq model with attention for multi-step time series forecasting.</p>\n</li>\n</ul>\n<p>I am facing an issue with attention wrapper state.</p>\n<p>My code is given below.</p>\n<p>Function for defining the decoder</p>\n<pre><code>def decoder_network(attention_mechanism,dropout=config.dropout ):\n    cells = []\n    for i in range(config.num_stacked_layers)\n\n        lstm_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim)\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell, attention_mechanism=attention_mechanism, attention_layer_size=config.hidden_dim)\n        cells.append(decoder_cell)\n\n\n    cell = tf.contrib.rnn.MultiRNNCell(cells)\n    return cell,cells\n</code></pre>\n<p>Using the above function</p>\n<pre><code>cell,cells = decoder_network(attention_mechanism)\nnew_state = cells[0].zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state = initial_state)\n\nfor i, inp in enumerate(decoder_inputs):\n      output, new_state = cell(inp, state=new_state)\n</code></pre>\n<p>However, I am facing the following error :</p>\n<pre><code>Traceback (most recent call last):\n  File \"seq2seq.py\", line 351, in &lt;module&gt;\n    train()\n  File \"seq2seq.py\", line 274, in train\n    rnn_model = build_train_graph(feed_previous=True)\n  File \"seq2seq.py\", line 247, in build_train_graph\n    dec_outputs, dec_memory = _basic_rnn_seq2seq(enc_inp, dec_inp, cell, Why , by , feed_previous=feed_previous)\n  File \"seq2seq.py\", line 165, in _basic_rnn_seq2seq\n    return _rnn_decoder(decoder_inputs, enc_state, attention_mechanism, Why , by ,_loop_function)\n  File \"seq2seq.py\", line 147, in _rnn_decoder\n    output, new_state = cell(inp, state=new_state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1066, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 1289, in call\n    \"Received type %s instead.\"  % type(state))\nTypeError: Expected state to be instance of AttentionWrapperState. Received type &lt;class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'&gt; instead.\n</code></pre>\n<p>I have tried printing the type of new state and i got the following type</p>\n<p><code>    ('new_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=&lt;tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/add_9:0' shape=(?, 40) dtype=float32&gt;, h=&lt;tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/mul_14:0' shape=(?, 40) dtype=float32&gt;),), attention=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(244, 40) dtype=float32&gt;, time=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32&gt;, alignments=&lt;tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(244, 5) dtype=float32&gt;, alignment_history=()))</code><br>\nEven though new state is an instance of AttentionWrapperState it still gives an error.</p>\n<p>Thanks in advance for your replies .</p>", "body_text": "Hello! I am implementing a seq2seq model with attention for multi-step time series forecasting. I am encountering an error with implementing attention for MultiRNN cell.\nSystem Information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):   Yes , I have attached my code below.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04) - Linux Ubuntu 16.04\n\n\nTensorFlow installed from (source or binary): - Source\n\n\nTensorFlow version (use command below): -VERSION = 1.4.1\n\n\nPython version: 2.7.12\n\n\nBazel version (if compiling from source):  - NA\n\n\nGCC/Compiler version (if compiling from source): COMPILER_VERSION = v1.4.0-19-ga52c8d9\n\n\nCUDA/cuDNN version: - NA\n\n\nGPU model and memory: - NA\n\n\nExact command to reproduce:\nI am implementing a seq2seq model with attention for multi-step time series forecasting.\n\n\nI am facing an issue with attention wrapper state.\nMy code is given below.\nFunction for defining the decoder\ndef decoder_network(attention_mechanism,dropout=config.dropout ):\n    cells = []\n    for i in range(config.num_stacked_layers)\n\n        lstm_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim)\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell, attention_mechanism=attention_mechanism, attention_layer_size=config.hidden_dim)\n        cells.append(decoder_cell)\n\n\n    cell = tf.contrib.rnn.MultiRNNCell(cells)\n    return cell,cells\n\nUsing the above function\ncell,cells = decoder_network(attention_mechanism)\nnew_state = cells[0].zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state = initial_state)\n\nfor i, inp in enumerate(decoder_inputs):\n      output, new_state = cell(inp, state=new_state)\n\nHowever, I am facing the following error :\nTraceback (most recent call last):\n  File \"seq2seq.py\", line 351, in <module>\n    train()\n  File \"seq2seq.py\", line 274, in train\n    rnn_model = build_train_graph(feed_previous=True)\n  File \"seq2seq.py\", line 247, in build_train_graph\n    dec_outputs, dec_memory = _basic_rnn_seq2seq(enc_inp, dec_inp, cell, Why , by , feed_previous=feed_previous)\n  File \"seq2seq.py\", line 165, in _basic_rnn_seq2seq\n    return _rnn_decoder(decoder_inputs, enc_state, attention_mechanism, Why , by ,_loop_function)\n  File \"seq2seq.py\", line 147, in _rnn_decoder\n    output, new_state = cell(inp, state=new_state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1066, in call\n    cur_inp, new_state = cell(cur_inp, cur_state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\n    return super(RNNCell, self).__call__(inputs, state)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 1289, in call\n    \"Received type %s instead.\"  % type(state))\nTypeError: Expected state to be instance of AttentionWrapperState. Received type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'> instead.\n\nI have tried printing the type of new state and i got the following type\n    ('new_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/add_9:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/mul_14:0' shape=(?, 40) dtype=float32>),), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(244, 40) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(244, 5) dtype=float32>, alignment_history=()))\nEven though new state is an instance of AttentionWrapperState it still gives an error.\nThanks in advance for your replies .", "body": "Hello! I am implementing a seq2seq model with attention for multi-step time series forecasting. I am encountering an error with implementing attention for MultiRNN cell.\r\n\r\nSystem Information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:   Yes , I have attached my code below.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)** - Linux Ubuntu 16.04\r\n\r\n- **TensorFlow installed from (source or binary)**: - Source\r\n\r\n- **TensorFlow version (use command below)**: -VERSION = 1.4.1\r\n\r\n- **Python version**: 2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:  - NA\r\n\r\n- **GCC/Compiler version (if compiling from source)**: COMPILER_VERSION = v1.4.0-19-ga52c8d9\r\n\r\n- **CUDA/cuDNN version**: - NA\r\n\r\n- **GPU model and memory**: - NA\r\n\r\n- **Exact command to reproduce**:\r\nI am implementing a seq2seq model with attention for multi-step time series forecasting.\r\n\r\nI am facing an issue with attention wrapper state.\r\n\r\nMy code is given below.\r\n\r\nFunction for defining the decoder \r\n\r\n\r\n    def decoder_network(attention_mechanism,dropout=config.dropout ):\r\n        cells = []\r\n        for i in range(config.num_stacked_layers)\r\n    \r\n            lstm_cell = tf.contrib.rnn.LSTMCell(config.hidden_dim)\r\n            decoder_cell = tf.contrib.seq2seq.AttentionWrapper(lstm_cell, attention_mechanism=attention_mechanism, attention_layer_size=config.hidden_dim)\r\n            cells.append(decoder_cell)\r\n    \r\n    \r\n        cell = tf.contrib.rnn.MultiRNNCell(cells)\r\n        return cell,cells\r\n\t\t\r\n\t\t\r\n\r\nUsing the above function\r\n\r\n    cell,cells = decoder_network(attention_mechanism)\r\n    new_state = cells[0].zero_state(batch_size=batch_size, dtype=tf.float32).clone(cell_state = initial_state)\r\n\r\n\tfor i, inp in enumerate(decoder_inputs):\r\n\t      output, new_state = cell(inp, state=new_state)\r\n\t\r\n\r\n\r\nHowever, I am facing the following error :\r\n\r\n    Traceback (most recent call last):\r\n      File \"seq2seq.py\", line 351, in <module>\r\n        train()\r\n      File \"seq2seq.py\", line 274, in train\r\n        rnn_model = build_train_graph(feed_previous=True)\r\n      File \"seq2seq.py\", line 247, in build_train_graph\r\n        dec_outputs, dec_memory = _basic_rnn_seq2seq(enc_inp, dec_inp, cell, Why , by , feed_previous=feed_previous)\r\n      File \"seq2seq.py\", line 165, in _basic_rnn_seq2seq\r\n        return _rnn_decoder(decoder_inputs, enc_state, attention_mechanism, Why , by ,_loop_function)\r\n      File \"seq2seq.py\", line 147, in _rnn_decoder\r\n        output, new_state = cell(inp, state=new_state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n        return super(RNNCell, self).__call__(inputs, state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n        outputs = self.call(inputs, *args, **kwargs)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 1066, in call\r\n        cur_inp, new_state = cell(cur_inp, cur_state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.py\", line 183, in __call__\r\n        return super(RNNCell, self).__call__(inputs, state)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.py\", line 575, in __call__\r\n        outputs = self.call(inputs, *args, **kwargs)\r\n      File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\", line 1289, in call\r\n        \"Received type %s instead.\"  % type(state))\r\n    TypeError: Expected state to be instance of AttentionWrapperState. Received type <class 'tensorflow.python.ops.rnn_cell_impl.LSTMStateTuple'> instead.\r\n    \r\n\r\n\r\nI have tried printing the type of new state and i got the following type\r\n\r\n`    \r\n    ('new_state', AttentionWrapperState(cell_state=(LSTMStateTuple(c=<tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/add_9:0' shape=(?, 40) dtype=float32>, h=<tf.Tensor 'rnn/rnn/multi_rnn_cell/cell_0/cell_0/lstm_cell/mul_14:0' shape=(?, 40) dtype=float32>),), attention=<tf.Tensor 'AttentionWrapperZeroState/zeros_1:0' shape=(244, 40) dtype=float32>, time=<tf.Tensor 'AttentionWrapperZeroState/zeros:0' shape=() dtype=int32>, alignments=<tf.Tensor 'AttentionWrapperZeroState/zeros_2:0' shape=(244, 5) dtype=float32>, alignment_history=()))\r\n    `\r\nEven though new state is an instance of AttentionWrapperState it still gives an error.\r\n\r\nThanks in advance for your replies ."}