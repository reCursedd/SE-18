{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/631", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/631/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/631/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/631/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/631", "id": 123984682, "node_id": "MDU6SXNzdWUxMjM5ODQ2ODI=", "number": 631, "title": "Provide sugar for insuring gradient computation of an op is put on the desired device", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2015-12-27T16:07:47Z", "updated_at": "2018-04-02T20:56:21Z", "closed_at": "2018-04-02T20:56:21Z", "author_association": "NONE", "body_html": "<p>Currently, it is somewhat involved to insure that a set of ops are put on the desired device for both the forward and backward (gradient) computations. For example if I have a function that defines its own scope like:</p>\n<pre><code>def my_op(inputs, name=None):\n    with tf.op_scope([inputs], name, 'my_op') as scope:\n        ...\n</code></pre>\n<p>Simply using the following code would not place its gradient calculations on the specified device:</p>\n<pre><code>with tf.device('/gpu:0'):\n    my_op(...)\n</code></pre>\n<p>Instead, the only way I got the above to really work is to define a device function that not only checks the name of the op but also all incoming nodes (because TF adds a lot of additional operations that are not explicitly named under the scope), like this:</p>\n<pre><code>def _device_function(op):\n    if ('my_op' in op.name) or any('my_op' in node.name for node in op.inputs):\n        return \"/gpu:0\"\n    else:\n        return None\n</code></pre>\n<p>This is unnecessarily involved and may very well be error-prone. Something like a boolean <code>grad</code> option for <code>tf.device</code> so that the user can simply specify <code>tf.device(..., grad=True)</code> would be far cleaner.</p>", "body_text": "Currently, it is somewhat involved to insure that a set of ops are put on the desired device for both the forward and backward (gradient) computations. For example if I have a function that defines its own scope like:\ndef my_op(inputs, name=None):\n    with tf.op_scope([inputs], name, 'my_op') as scope:\n        ...\n\nSimply using the following code would not place its gradient calculations on the specified device:\nwith tf.device('/gpu:0'):\n    my_op(...)\n\nInstead, the only way I got the above to really work is to define a device function that not only checks the name of the op but also all incoming nodes (because TF adds a lot of additional operations that are not explicitly named under the scope), like this:\ndef _device_function(op):\n    if ('my_op' in op.name) or any('my_op' in node.name for node in op.inputs):\n        return \"/gpu:0\"\n    else:\n        return None\n\nThis is unnecessarily involved and may very well be error-prone. Something like a boolean grad option for tf.device so that the user can simply specify tf.device(..., grad=True) would be far cleaner.", "body": "Currently, it is somewhat involved to insure that a set of ops are put on the desired device for both the forward and backward (gradient) computations. For example if I have a function that defines its own scope like:\n\n```\ndef my_op(inputs, name=None):\n    with tf.op_scope([inputs], name, 'my_op') as scope:\n        ...\n```\n\nSimply using the following code would not place its gradient calculations on the specified device:\n\n```\nwith tf.device('/gpu:0'):\n    my_op(...)\n```\n\nInstead, the only way I got the above to really work is to define a device function that not only checks the name of the op but also all incoming nodes (because TF adds a lot of additional operations that are not explicitly named under the scope), like this:\n\n```\ndef _device_function(op):\n    if ('my_op' in op.name) or any('my_op' in node.name for node in op.inputs):\n        return \"/gpu:0\"\n    else:\n        return None\n```\n\nThis is unnecessarily involved and may very well be error-prone. Something like a boolean `grad` option for `tf.device` so that the user can simply specify `tf.device(..., grad=True)` would be far cleaner.\n"}