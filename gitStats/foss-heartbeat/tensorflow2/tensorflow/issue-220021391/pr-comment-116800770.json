{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/116800770", "pull_request_review_id": 38451024, "id": 116800770, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjgwMDc3MA==", "diff_hunk": "@@ -97,9 +84,114 @@ class FFTGPUBase : public OpKernel {\n   virtual bool IsForward() const = 0;\n   virtual bool IsReal() const = 0;\n \n- private:\n+  // The function that actually computes the FFT.\n+  virtual void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n+                     Tensor* out) = 0;\n+};\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+template <typename Device, typename TInput, typename TOutput,\n+          int FFTResultType, int FFTDir, int FFTRank>\n+struct FFTFunctor {\n+  void operator()(const Device& d,\n+                  typename TTypes<TOutput, FFTRank + 1>::Tensor output,\n+                  typename TTypes<TInput, FFTRank + 1>::Tensor input) {\n+    // Create the axes (which are always trailing).\n+    auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n+    // Evaluate the fft on the specified device.\n+    output.device(d) = input.template fft<FFTResultType, FFTDir>(axes);\n+  }\n+};\n+\n+template <bool Forward, bool _Real, int FFTRank>\n+class FFTCPU : public FFTBase {\n+ public:\n+  using FFTBase::FFTBase;\n+ protected:\n+  int Rank() const override { return FFTRank; }\n+  bool IsForward() const override { return Forward; }\n+  bool IsReal() const override { return _Real; }\n+\n   void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n-             Tensor* out) {\n+             Tensor* out) override {\n+    // Create the axes (which are always trailing).\n+    auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n+    auto device = ctx->eigen_device<CPUDevice>();\n+\n+    if (!IsReal()) {\n+      auto input = ((Tensor) in).flat_inner_dims<complex64, FFTRank + 1>();\n+      // Compute the FFT using eigen.\n+      auto output = out->flat_inner_dims<complex64, FFTRank + 1>();\n+      output.device(device) = input.template fft<Eigen::BothParts,\n+        Forward ? Eigen::FFT_FORWARD : Eigen::FFT_REVERSE>(axes);\n+    }\n+    else {\n+      if (IsForward()) {\n+        auto input = ((Tensor) in).flat_inner_dims<float, FFTRank + 1>();\n+        auto output = out->flat_inner_dims<complex64, FFTRank + 1>();\n+        Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\n+\n+        // Compute the full FFT using a temporary tensor.\n+        Tensor temp;\n+        OP_REQUIRES_OK(ctx,ctx->allocate_temp(\n+          DataTypeToEnum<complex64>::v(), in.shape(), &temp\n+        ));\n+        auto full_fft = temp.flat_inner_dims<complex64, FFTRank + 1>();\n+        full_fft.device(device) = input.template fft<Eigen::BothParts,\n+          Eigen::FFT_FORWARD>(axes);\n+\n+        // Slice away the negative frequency components\n+        output.device(device) = full_fft.slice(startIndices, output.dimensions());\n+      }\n+      else {\n+        // TODO: reconstruct the full fft and take the inverse\n+        ctx->CtxFailureWithWarning(errors::Unimplemented(\n+          \"IRFFT is not implemented as a CPU kernel\"\n+        ));\n+      }\n+    }\n+  }\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"FFT\").Device(DEVICE_CPU), FFTCPU<true, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"FFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"FFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, false, 3>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 3>);\n+\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT\").Device(DEVICE_CPU), FFTCPU<true, true, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, true, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, true, 3>);\n+\n+#if GOOGLE_CUDA", "path": "tensorflow/core/kernels/fft_ops.cc", "position": 115, "original_position": 130, "commit_id": "e6da919d85a75ba32258b3e578d8dbe0dd49188e", "original_commit_id": "9e3eab071785d9b0e4d8cfcee127a5696a857ae8", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "(this fails because you are including this inside namespace tensorflow.", "created_at": "2017-05-16T17:05:18Z", "updated_at": "2017-05-17T11:28:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r116800770", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/116800770"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r116800770"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029"}}, "body_html": "<p>(this fails because you are including this inside namespace tensorflow.</p>", "body_text": "(this fails because you are including this inside namespace tensorflow.", "in_reply_to_id": 116799537}