{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/116803994", "pull_request_review_id": 38454657, "id": 116803994, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExNjgwMzk5NA==", "diff_hunk": "@@ -97,9 +84,114 @@ class FFTGPUBase : public OpKernel {\n   virtual bool IsForward() const = 0;\n   virtual bool IsReal() const = 0;\n \n- private:\n+  // The function that actually computes the FFT.\n+  virtual void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n+                     Tensor* out) = 0;\n+};\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+template <typename Device, typename TInput, typename TOutput,\n+          int FFTResultType, int FFTDir, int FFTRank>\n+struct FFTFunctor {\n+  void operator()(const Device& d,\n+                  typename TTypes<TOutput, FFTRank + 1>::Tensor output,\n+                  typename TTypes<TInput, FFTRank + 1>::Tensor input) {\n+    // Create the axes (which are always trailing).\n+    auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n+    // Evaluate the fft on the specified device.\n+    output.device(d) = input.template fft<FFTResultType, FFTDir>(axes);\n+  }\n+};\n+\n+template <bool Forward, bool _Real, int FFTRank>\n+class FFTCPU : public FFTBase {\n+ public:\n+  using FFTBase::FFTBase;\n+ protected:\n+  int Rank() const override { return FFTRank; }\n+  bool IsForward() const override { return Forward; }\n+  bool IsReal() const override { return _Real; }\n+\n   void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n-             Tensor* out) {\n+             Tensor* out) override {\n+    // Create the axes (which are always trailing).\n+    auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n+    auto device = ctx->eigen_device<CPUDevice>();\n+\n+    if (!IsReal()) {\n+      auto input = ((Tensor) in).flat_inner_dims<complex64, FFTRank + 1>();\n+      // Compute the FFT using eigen.\n+      auto output = out->flat_inner_dims<complex64, FFTRank + 1>();\n+      output.device(device) = input.template fft<Eigen::BothParts,\n+        Forward ? Eigen::FFT_FORWARD : Eigen::FFT_REVERSE>(axes);\n+    }\n+    else {\n+      if (IsForward()) {\n+        auto input = ((Tensor) in).flat_inner_dims<float, FFTRank + 1>();\n+        auto output = out->flat_inner_dims<complex64, FFTRank + 1>();\n+        Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\n+\n+        // Compute the full FFT using a temporary tensor.\n+        Tensor temp;\n+        OP_REQUIRES_OK(ctx,ctx->allocate_temp(\n+          DataTypeToEnum<complex64>::v(), in.shape(), &temp\n+        ));\n+        auto full_fft = temp.flat_inner_dims<complex64, FFTRank + 1>();\n+        full_fft.device(device) = input.template fft<Eigen::BothParts,\n+          Eigen::FFT_FORWARD>(axes);\n+\n+        // Slice away the negative frequency components\n+        output.device(device) = full_fft.slice(startIndices, output.dimensions());\n+      }\n+      else {\n+        // TODO: reconstruct the full fft and take the inverse\n+        ctx->CtxFailureWithWarning(errors::Unimplemented(\n+          \"IRFFT is not implemented as a CPU kernel\"\n+        ));\n+      }\n+    }\n+  }\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"FFT\").Device(DEVICE_CPU), FFTCPU<true, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"FFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"FFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, false, 3>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 3>);\n+\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT\").Device(DEVICE_CPU), FFTCPU<true, true, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, true, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, true, 3>);\n+\n+#if GOOGLE_CUDA", "path": "tensorflow/core/kernels/fft_ops.cc", "position": 115, "original_position": 130, "commit_id": "e6da919d85a75ba32258b3e578d8dbe0dd49188e", "original_commit_id": "9e3eab071785d9b0e4d8cfcee127a5696a857ae8", "user": {"login": "tillahoffmann", "id": 966348, "node_id": "MDQ6VXNlcjk2NjM0OA==", "avatar_url": "https://avatars2.githubusercontent.com/u/966348?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tillahoffmann", "html_url": "https://github.com/tillahoffmann", "followers_url": "https://api.github.com/users/tillahoffmann/followers", "following_url": "https://api.github.com/users/tillahoffmann/following{/other_user}", "gists_url": "https://api.github.com/users/tillahoffmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tillahoffmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tillahoffmann/subscriptions", "organizations_url": "https://api.github.com/users/tillahoffmann/orgs", "repos_url": "https://api.github.com/users/tillahoffmann/repos", "events_url": "https://api.github.com/users/tillahoffmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tillahoffmann/received_events", "type": "User", "site_admin": false}, "body": "Ok, given that there is both CPU and GPU code, would the following work?\r\n\r\n```cpp\r\nnamespace tensorflow {\r\n// CPU stuff\r\n}\r\n\r\n#if GOOGLE_CUDA\r\n#include \"tensorflow/core/platform/stream_executor.h\"\r\n\r\nnamespace tensorflow {\r\n// GPU stuff\r\n}\r\n#endif\r\n```", "created_at": "2017-05-16T17:19:20Z", "updated_at": "2017-05-17T11:28:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r116803994", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/116803994"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r116803994"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029"}}, "body_html": "<p>Ok, given that there is both CPU and GPU code, would the following work?</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-k\">namespace</span> <span class=\"pl-en\">tensorflow</span> {\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> CPU stuff</span>\n}\n\n#<span class=\"pl-k\">if</span> GOOGLE_CUDA\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/platform/stream_executor.h<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">tensorflow</span> {\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> GPU stuff</span>\n}\n#<span class=\"pl-k\">endif</span></pre></div>", "body_text": "Ok, given that there is both CPU and GPU code, would the following work?\nnamespace tensorflow {\n// CPU stuff\n}\n\n#if GOOGLE_CUDA\n#include \"tensorflow/core/platform/stream_executor.h\"\n\nnamespace tensorflow {\n// GPU stuff\n}\n#endif", "in_reply_to_id": 116799537}