{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111290665", "pull_request_review_id": 32525932, "id": 111290665, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMTI5MDY2NQ==", "diff_hunk": "@@ -97,9 +84,141 @@ class FFTGPUBase : public OpKernel {\n   virtual bool IsForward() const = 0;\n   virtual bool IsReal() const = 0;\n \n- private:\n+  // The function that actually computes the FFT.\n+  virtual void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n+                     Tensor* out) = 0;\n+};\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+template <typename Device, typename TInput, typename TOutput,\n+          int FFTResultType, int FFTDir, int FFTRank>\n+struct FFTFunctor {\n+  void operator()(const Device& d,\n+                  typename TTypes<TOutput, FFTRank + 1>::Tensor output,\n+                  typename TTypes<TInput, FFTRank + 1>::Tensor input) {\n+    // Create the axes (which are always trailing).\n+    auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n+    // Evaluate the fft on the specified device.\n+    output.device(d) = input.template fft<FFTResultType, FFTDir>(axes);\n+  }\n+};\n+\n+template <bool Forward, bool _Real, int FFTRank>\n+class FFTCPU : public FFTBase {\n+ public:\n+  using FFTBase::FFTBase;\n+ protected:\n+  int Rank() const override { return FFTRank; }\n+  bool IsForward() const override { return Forward; }\n+  bool IsReal() const override { return _Real; }\n+\n+  void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n+             Tensor* out) override {\n+    if (!IsReal()) {\n+      auto input = ((Tensor) in).flat_inner_dims<complex64, FFTRank + 1>();\n+\n+      // Apply the functor.\n+      FFTFunctor<CPUDevice, complex64, complex64, Eigen::BothParts,\n+                Forward ? Eigen::FFT_FORWARD : Eigen::FFT_REVERSE,\n+                FFTRank> functor;\n+      functor(ctx->eigen_device<CPUDevice>(),\n+              out->flat_inner_dims<complex64, FFTRank + 1>(), input);\n+    }\n+    else {\n+      if (IsForward()) {\n+        auto input = ((Tensor) in).flat_inner_dims<float, FFTRank + 1>();\n+        // Create a temporary placeholder for the full FFT.\n+        Tensor temp;\n+        OP_REQUIRES_OK(ctx,ctx->allocate_temp(\n+          DataTypeToEnum<complex64>::v(), in.shape(), &temp\n+        ));\n+        auto full_fft = temp.flat_inner_dims<complex64, FFTRank + 1>();\n+        // Apply the functor.\n+        FFTFunctor<CPUDevice, float, complex64, Eigen::BothParts,\n+                  Eigen::FFT_FORWARD, FFTRank> functor;\n+        functor(ctx->eigen_device<CPUDevice>(), full_fft, input);\n+        // Create zero indices for slicing.\n+        Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\n+        // Convert output to tensor and get tensor size for slicing.\n+        auto output = out->flat_inner_dims<complex64, FFTRank + 1>();\n+        auto sizes = output.dimensions();\n+        // Slice the full FFT to get the non-negative frequency components only.\n+        output.slice(startIndices, sizes) =\n+          full_fft.slice(startIndices, sizes);\n+      }\n+      else {\n+        auto input = ((Tensor) in).flat_inner_dims<complex64, FFTRank + 1>();\n+        // The first dimension contains the zero-frequency component which we\n+        // do not want to duplicate. So we reconstruct the complex signal by\n+        // (1) slicing from the second element, (2) reversing the order,\n+        // (3) taking the complex conjugate, (4) concatenating with the original\n+        // input. Note that for an even input length, the last element is the\n+        // Nyquist frequency which we also do not want to duplicate.\n+        Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\n+        startIndices[FFTRank] = 1;\n+        auto sizes = input.dimensions();\n+        if (sizes[FFTRank] % 2 == 0) {\n+          sizes[FFTRank] -= 1;\n+        }\n+        auto cc = input.slice(startIndices, sizes).conjugate()\n+          .reverse(FFTRank);\n+        auto full_fft = input.concatenate(cc, FFTRank);\n+\n+        // Evaluate the IFFT\n+        auto output = out->flat_inner_dims<float, FFTRank + 1>();\n+        FFTFunctor<CPUDevice, complex64, float, Eigen::RealPart,\n+                  Eigen::FFT_REVERSE, FFTRank> functor;\n+        functor(ctx->eigen_device<CPUDevice>(), output, full_fft);\n+      }\n+    }\n+  }\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"FFT\").Device(DEVICE_CPU), FFTCPU<true, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"FFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"FFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, false, 3>);\n+REGISTER_KERNEL_BUILDER(Name(\"IFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 3>);\n+\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT\").Device(DEVICE_CPU), FFTCPU<true, true, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"IRFFT\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 1>);\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, true, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"IRFFT2D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 2>);\n+REGISTER_KERNEL_BUILDER(Name(\"RFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<true, true, 3>);\n+REGISTER_KERNEL_BUILDER(Name(\"IRFFT3D\").Device(DEVICE_CPU),\n+                        FFTCPU<false, false, 3>);\n+\n+#if GOOGLE_CUDA\n+#include \"tensorflow/core/platform/stream_executor.h\"", "path": "tensorflow/core/kernels/fft_ops.cc", "position": null, "original_position": 157, "commit_id": "e6da919d85a75ba32258b3e578d8dbe0dd49188e", "original_commit_id": "cfce2b036ea33aeb2742fb4219e75c2652dad3c7", "user": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "body": "Shouldn't we move this at the top of the file to avoid including stream_executor.h in the tensorflow namespace ?", "created_at": "2017-04-13T00:43:04Z", "updated_at": "2017-05-17T11:28:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r111290665", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111290665"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r111290665"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029"}}, "body_html": "<p>Shouldn't we move this at the top of the file to avoid including stream_executor.h in the tensorflow namespace ?</p>", "body_text": "Shouldn't we move this at the top of the file to avoid including stream_executor.h in the tensorflow namespace ?"}