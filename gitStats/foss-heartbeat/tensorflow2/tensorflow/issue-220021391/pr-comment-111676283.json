{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111676283", "pull_request_review_id": 32937199, "id": 111676283, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMTY3NjI4Mw==", "diff_hunk": "@@ -97,9 +84,141 @@ class FFTGPUBase : public OpKernel {\n   virtual bool IsForward() const = 0;\n   virtual bool IsReal() const = 0;\n \n- private:\n+  // The function that actually computes the FFT.\n+  virtual void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n+                     Tensor* out) = 0;\n+};\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+template <typename Device, typename TInput, typename TOutput,\n+          int FFTResultType, int FFTDir, int FFTRank>\n+struct FFTFunctor {\n+  void operator()(const Device& d,\n+                  typename TTypes<TOutput, FFTRank + 1>::Tensor output,\n+                  typename TTypes<TInput, FFTRank + 1>::Tensor input) {\n+    // Create the axes (which are always trailing).\n+    auto axes = Eigen::ArrayXi::LinSpaced(FFTRank, 1, FFTRank);\n+    // Evaluate the fft on the specified device.\n+    output.device(d) = input.template fft<FFTResultType, FFTDir>(axes);\n+  }\n+};\n+\n+template <bool Forward, bool _Real, int FFTRank>\n+class FFTCPU : public FFTBase {\n+ public:\n+  using FFTBase::FFTBase;\n+ protected:\n+  int Rank() const override { return FFTRank; }\n+  bool IsForward() const override { return Forward; }\n+  bool IsReal() const override { return _Real; }\n+\n+  void DoFFT(OpKernelContext* ctx, const Tensor& in, uint64* fft_shape,\n+             Tensor* out) override {\n+    if (!IsReal()) {\n+      auto input = ((Tensor) in).flat_inner_dims<complex64, FFTRank + 1>();\n+\n+      // Apply the functor.\n+      FFTFunctor<CPUDevice, complex64, complex64, Eigen::BothParts,\n+                Forward ? Eigen::FFT_FORWARD : Eigen::FFT_REVERSE,\n+                FFTRank> functor;\n+      functor(ctx->eigen_device<CPUDevice>(),\n+              out->flat_inner_dims<complex64, FFTRank + 1>(), input);\n+    }\n+    else {\n+      if (IsForward()) {\n+        auto input = ((Tensor) in).flat_inner_dims<float, FFTRank + 1>();\n+        // Create a temporary placeholder for the full FFT.\n+        Tensor temp;\n+        OP_REQUIRES_OK(ctx,ctx->allocate_temp(\n+          DataTypeToEnum<complex64>::v(), in.shape(), &temp\n+        ));\n+        auto full_fft = temp.flat_inner_dims<complex64, FFTRank + 1>();\n+        // Apply the functor.\n+        FFTFunctor<CPUDevice, float, complex64, Eigen::BothParts,\n+                  Eigen::FFT_FORWARD, FFTRank> functor;\n+        functor(ctx->eigen_device<CPUDevice>(), full_fft, input);\n+        // Create zero indices for slicing.\n+        Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\n+        // Convert output to tensor and get tensor size for slicing.\n+        auto output = out->flat_inner_dims<complex64, FFTRank + 1>();\n+        auto sizes = output.dimensions();\n+        // Slice the full FFT to get the non-negative frequency components only.\n+        output.slice(startIndices, sizes) =\n+          full_fft.slice(startIndices, sizes);\n+      }\n+      else {\n+        auto input = ((Tensor) in).flat_inner_dims<complex64, FFTRank + 1>();\n+        // The first dimension contains the zero-frequency component which we\n+        // do not want to duplicate. So we reconstruct the complex signal by\n+        // (1) slicing from the second element, (2) reversing the order,\n+        // (3) taking the complex conjugate, (4) concatenating with the original\n+        // input. Note that for an even input length, the last element is the\n+        // Nyquist frequency which we also do not want to duplicate.\n+        Eigen::DSizes<Eigen::DenseIndex, FFTRank + 1> startIndices;\n+        startIndices[FFTRank] = 1;\n+        auto sizes = input.dimensions();\n+        if (sizes[FFTRank] % 2 == 0) {\n+          sizes[FFTRank] -= 1;\n+        }\n+        auto cc = input.slice(startIndices, sizes).conjugate()\n+          .reverse(FFTRank);\n+        auto full_fft = input.concatenate(cc, FFTRank);\n+\n+        // Evaluate the IFFT\n+        auto output = out->flat_inner_dims<float, FFTRank + 1>();\n+        FFTFunctor<CPUDevice, complex64, float, Eigen::RealPart,\n+                  Eigen::FFT_REVERSE, FFTRank> functor;\n+        functor(ctx->eigen_device<CPUDevice>(), output, full_fft);", "path": "tensorflow/core/kernels/fft_ops.cc", "position": null, "original_position": 126, "commit_id": "e6da919d85a75ba32258b3e578d8dbe0dd49188e", "original_commit_id": "cfce2b036ea33aeb2742fb4219e75c2652dad3c7", "user": {"login": "rryan", "id": 26527, "node_id": "MDQ6VXNlcjI2NTI3", "avatar_url": "https://avatars3.githubusercontent.com/u/26527?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rryan", "html_url": "https://github.com/rryan", "followers_url": "https://api.github.com/users/rryan/followers", "following_url": "https://api.github.com/users/rryan/following{/other_user}", "gists_url": "https://api.github.com/users/rryan/gists{/gist_id}", "starred_url": "https://api.github.com/users/rryan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rryan/subscriptions", "organizations_url": "https://api.github.com/users/rryan/orgs", "repos_url": "https://api.github.com/users/rryan/repos", "events_url": "https://api.github.com/users/rryan/events{/privacy}", "received_events_url": "https://api.github.com/users/rryan/received_events", "type": "User", "site_admin": false}, "body": "> I was wondering whether it makes sense to stick to the functor paradigm in this case because the functors are just a templated wrapper around Eigen which are not used anywhere else.\r\n\r\nI was thinking it would cut down on instantiations of the Eigen expression in the .o, but I just noticed that forward/backward is part of the TensorFFT template arguments, so we can't reuse the same C2C code path for forward and backward (unless we did the constant scaling for the backward outside of FFTFunctor and always did C2C forward inside of FFTFunctor). Dropping the functor is fine w/ me, but I'll defer to @benoitsteiner.\r\n\r\n> What are your thoughts on the suggested implementation in https://github.com/tensorflow/tensorflow/pull/9029#issuecomment-292505411. And do you have any thoughts on the resulting [compiler error](http://stackoverflow.com/questions/43364086/struggling-to-evaluate-operation-in-the-tensor-module-of-eigen)? \r\n\r\nArgh, sorry -- not sure how I missed your comment, you were already a step ahead of me. \r\n\r\nOff hand without trying to compile your example, that smells like TensorFFT doesn't like having a float tensor as its input. \r\n\r\nThough from skimming [TensorFFT](https://bitbucket.org/eigen/eigen/src/f3a22f35b0444e03a1f65941800b9a2283de1398/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h?at=default&fileviewer=file-view-default#TensorFFT.h-30), it kind of looks like it's meant to convert `T` to `std::complex<T>` on the fly.\r\n\r\nHave you already tried casting?\r\n\r\n```c++\r\n auto op = input\r\n  .cast<std::complex<float>>()\r\n  .template fft<Eigen::BothParts, Eigen::FFT_FORWARD>(axes)\r\n  .slice(startIndices, sizes);\r\n```", "created_at": "2017-04-16T01:21:39Z", "updated_at": "2017-05-17T11:28:15Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r111676283", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111676283"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9029#discussion_r111676283"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9029"}}, "body_html": "<blockquote>\n<p>I was wondering whether it makes sense to stick to the functor paradigm in this case because the functors are just a templated wrapper around Eigen which are not used anywhere else.</p>\n</blockquote>\n<p>I was thinking it would cut down on instantiations of the Eigen expression in the .o, but I just noticed that forward/backward is part of the TensorFFT template arguments, so we can't reuse the same C2C code path for forward and backward (unless we did the constant scaling for the backward outside of FFTFunctor and always did C2C forward inside of FFTFunctor). Dropping the functor is fine w/ me, but I'll defer to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a>.</p>\n<blockquote>\n<p>What are your thoughts on the suggested implementation in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"220021391\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9029\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/9029/hovercard?comment_id=292505411&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/9029#issuecomment-292505411\">#9029 (comment)</a>. And do you have any thoughts on the resulting <a href=\"http://stackoverflow.com/questions/43364086/struggling-to-evaluate-operation-in-the-tensor-module-of-eigen\" rel=\"nofollow\">compiler error</a>?</p>\n</blockquote>\n<p>Argh, sorry -- not sure how I missed your comment, you were already a step ahead of me.</p>\n<p>Off hand without trying to compile your example, that smells like TensorFFT doesn't like having a float tensor as its input.</p>\n<p>Though from skimming <a href=\"https://bitbucket.org/eigen/eigen/src/f3a22f35b0444e03a1f65941800b9a2283de1398/unsupported/Eigen/CXX11/src/Tensor/TensorFFT.h?at=default&amp;fileviewer=file-view-default#TensorFFT.h-30\" rel=\"nofollow\">TensorFFT</a>, it kind of looks like it's meant to convert <code>T</code> to <code>std::complex&lt;T&gt;</code> on the fly.</p>\n<p>Have you already tried casting?</p>\n<div class=\"highlight highlight-source-c++\"><pre> <span class=\"pl-k\">auto</span> op = input\n  .cast&lt;std::complex&lt;<span class=\"pl-k\">float</span>&gt;&gt;()\n  .<span class=\"pl-k\">template </span>fft&lt;Eigen::BothParts, Eigen::FFT_FORWARD&gt;(axes)\n  .slice(startIndices, sizes);</pre></div>", "body_text": "I was wondering whether it makes sense to stick to the functor paradigm in this case because the functors are just a templated wrapper around Eigen which are not used anywhere else.\n\nI was thinking it would cut down on instantiations of the Eigen expression in the .o, but I just noticed that forward/backward is part of the TensorFFT template arguments, so we can't reuse the same C2C code path for forward and backward (unless we did the constant scaling for the backward outside of FFTFunctor and always did C2C forward inside of FFTFunctor). Dropping the functor is fine w/ me, but I'll defer to @benoitsteiner.\n\nWhat are your thoughts on the suggested implementation in #9029 (comment). And do you have any thoughts on the resulting compiler error?\n\nArgh, sorry -- not sure how I missed your comment, you were already a step ahead of me.\nOff hand without trying to compile your example, that smells like TensorFFT doesn't like having a float tensor as its input.\nThough from skimming TensorFFT, it kind of looks like it's meant to convert T to std::complex<T> on the fly.\nHave you already tried casting?\n auto op = input\n  .cast<std::complex<float>>()\n  .template fft<Eigen::BothParts, Eigen::FFT_FORWARD>(axes)\n  .slice(startIndices, sizes);", "in_reply_to_id": 111649584}