{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430814019", "html_url": "https://github.com/tensorflow/tensorflow/issues/22995#issuecomment-430814019", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22995", "id": 430814019, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDgxNDAxOQ==", "user": {"login": "alanchiao", "id": 4323109, "node_id": "MDQ6VXNlcjQzMjMxMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/4323109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanchiao", "html_url": "https://github.com/alanchiao", "followers_url": "https://api.github.com/users/alanchiao/followers", "following_url": "https://api.github.com/users/alanchiao/following{/other_user}", "gists_url": "https://api.github.com/users/alanchiao/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanchiao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanchiao/subscriptions", "organizations_url": "https://api.github.com/users/alanchiao/orgs", "repos_url": "https://api.github.com/users/alanchiao/repos", "events_url": "https://api.github.com/users/alanchiao/events{/privacy}", "received_events_url": "https://api.github.com/users/alanchiao/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T22:39:24Z", "updated_at": "2018-10-17T22:39:24Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5137261\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Yu-Hang\">@Yu-Hang</a> : it is as jazzystring1@ suggested. With post-training quantization, the inputs and outputs continue to be float32.</p>\n<p>The code you pointed to in the example demo app is using a \"fully-quantized\" models, created with the process described <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md\">here</a> as opposed to post-training quantization.</p>\n<p>I think this is somewhere where we could improve the documentation around the demo, now that there are more variants to how we do quantization.</p>", "body_text": "@Yu-Hang : it is as jazzystring1@ suggested. With post-training quantization, the inputs and outputs continue to be float32.\nThe code you pointed to in the example demo app is using a \"fully-quantized\" models, created with the process described here as opposed to post-training quantization.\nI think this is somewhere where we could improve the documentation around the demo, now that there are more variants to how we do quantization.", "body": "@Yu-Hang : it is as jazzystring1@ suggested. With post-training quantization, the inputs and outputs continue to be float32. \r\n\r\nThe code you pointed to in the example demo app is using a \"fully-quantized\" models, created with the process described [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/quantize/README.md) as opposed to post-training quantization.\r\n\r\nI think this is somewhere where we could improve the documentation around the demo, now that there are more variants to how we do quantization.\r\n\r\n"}