{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22995", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22995/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22995/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22995/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22995", "id": 370307395, "node_id": "MDU6SXNzdWUzNzAzMDczOTU=", "number": 22995, "title": "Model is still floating point after Post-training quantization ", "user": {"login": "Yu-Hang", "id": 5137261, "node_id": "MDQ6VXNlcjUxMzcyNjE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5137261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yu-Hang", "html_url": "https://github.com/Yu-Hang", "followers_url": "https://api.github.com/users/Yu-Hang/followers", "following_url": "https://api.github.com/users/Yu-Hang/following{/other_user}", "gists_url": "https://api.github.com/users/Yu-Hang/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yu-Hang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yu-Hang/subscriptions", "organizations_url": "https://api.github.com/users/Yu-Hang/orgs", "repos_url": "https://api.github.com/users/Yu-Hang/repos", "events_url": "https://api.github.com/users/Yu-Hang/events{/privacy}", "received_events_url": "https://api.github.com/users/Yu-Hang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "alanchiao", "id": 4323109, "node_id": "MDQ6VXNlcjQzMjMxMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/4323109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanchiao", "html_url": "https://github.com/alanchiao", "followers_url": "https://api.github.com/users/alanchiao/followers", "following_url": "https://api.github.com/users/alanchiao/following{/other_user}", "gists_url": "https://api.github.com/users/alanchiao/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanchiao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanchiao/subscriptions", "organizations_url": "https://api.github.com/users/alanchiao/orgs", "repos_url": "https://api.github.com/users/alanchiao/repos", "events_url": "https://api.github.com/users/alanchiao/events{/privacy}", "received_events_url": "https://api.github.com/users/alanchiao/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alanchiao", "id": 4323109, "node_id": "MDQ6VXNlcjQzMjMxMDk=", "avatar_url": "https://avatars2.githubusercontent.com/u/4323109?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanchiao", "html_url": "https://github.com/alanchiao", "followers_url": "https://api.github.com/users/alanchiao/followers", "following_url": "https://api.github.com/users/alanchiao/following{/other_user}", "gists_url": "https://api.github.com/users/alanchiao/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanchiao/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanchiao/subscriptions", "organizations_url": "https://api.github.com/users/alanchiao/orgs", "repos_url": "https://api.github.com/users/alanchiao/repos", "events_url": "https://api.github.com/users/alanchiao/events{/privacy}", "received_events_url": "https://api.github.com/users/alanchiao/received_events", "type": "User", "site_admin": false}, {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-10-15T19:20:56Z", "updated_at": "2018-10-18T19:00:22Z", "closed_at": "2018-10-18T19:00:22Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nAndroid 7</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nSamsung Galaxy S6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.11.0 on PC, 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on Android</li>\n<li><strong>Python version</strong>: 2.7.15</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>: tfLite.run(imgData, labelProb);</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>After applying post-training quantization, my custom CNN model was shrinked to 1/4 of its original size (from 56.1MB to 14MB). I put the image(100x100x3) that is to be predicted into ByteBuffer as 100x100x3=30,000 bytes. However, I got the following error during inference:</p>\n<pre><code>**java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 120000 bytes and a ByteBuffer with 30000 bytes.**\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:136)\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.recongnize(TFClassifier_Lite.java:102)\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.process_by_batch(TFClassifier_Lite.java:145)\n        at gov.nih.nlm.malaria_screener.Cells.runCells(Cells.java:269)\n        at gov.nih.nlm.malaria_screener.CameraActivity.ProcessThinSmearImage(CameraActivity.java:1020)\n        at gov.nih.nlm.malaria_screener.CameraActivity.access$600(CameraActivity.java:75)\n        at gov.nih.nlm.malaria_screener.CameraActivity$8.run(CameraActivity.java:810)\n        at java.lang.Thread.run(Thread.java:762) \n</code></pre>\n<p>The imput image size to the model is: 100x100x3. I'm currently predicting one image at a time. So, if I'm making the Bytebuffer: 100x100x3 = 30,000 bytes. However, the log info above says the TensorFlowLite buffer has 120,000 bytes. This makes me suspect that the converted tflite model is still in float format. Is this expected behavior? How can I get a quantized model that take input image in 8 pit precision like it does in the <a href=\"https://github.com/tensorflow/tensorflow/blob/307c83106445ab2c52847f08d35a66c51aff19d9/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L163\">example</a> from TensorFlow official repository ?</p>\n<p>In the example code, the ByteBuffer used as input for tflite.run() is in 8 bit precision for the quantized model.<br>\nBut I also read from the google doc saying, \"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels.\" This two instances seems to contradict each other.</p>\n<h3>Source code / logs</h3>\n<pre><code>private static final int BATCH_SIZE = 1;\n\nprivate static final int DIM_IMG_SIZE = 100;\n\nprivate static final int DIM_PIXEL_SIZE = 3;\n\nprivate static final int BYTE_NUM = 1;\n\nimgData = ByteBuffer.allocateDirect(BYTE_NUM * BATCH_SIZE * DIM_IMG_SIZE * DIM_IMG_SIZE * DIM_PIXEL_SIZE);\nimgData.order(ByteOrder.nativeOrder());\n\n... ...\n\nint pixel = 0;\n\n        for (int i = 0; i &lt; DIM_IMG_SIZE; ++i) {\n            for (int j = 0; j &lt; DIM_IMG_SIZE; ++j) {\n\n                final int val = intValues[pixel++];\n\n                imgData.put((byte)((val &gt;&gt; 16) &amp; 0xFF));\n                imgData.put((byte)((val &gt;&gt; 8) &amp; 0xFF));\n                imgData.put((byte)(val &amp; 0xFF));\n\n//                imgData.putFloat(((val &gt;&gt; 16) &amp; 0xFF) / 255.0f);\n//                imgData.putFloat(((val &gt;&gt; 8) &amp; 0xFF) / 255.0f);\n//                imgData.putFloat((val &amp; 0xFF) / 255.0f);\n\n            }\n        }\n\n... ...\n\ntfLite.run(imgData, labelProb);\n</code></pre>\n<p>Post-training quantization code:</p>\n<pre><code>import tensorflow as tf\nimport sys\nimport os\n\nsaved_model_dir = '/home/yuh5/Downloads/malaria_thinsmear.h5.pb'\n\ninput_arrays = [\"input_2\"]\n\noutput_arrays = [\"output_node0\"]\n\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(saved_model_dir, input_arrays, output_arrays)\n\nconverter.post_training_quantize = True\n\ntflite_model = converter.convert()\nopen(\"thinSmear_100.tflite\", \"wb\").write(tflite_model)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nAndroid 7\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nSamsung Galaxy S6\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.11.0 on PC, 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on Android\nPython version: 2.7.15\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce: tfLite.run(imgData, labelProb);\n\nDescribe the problem\nAfter applying post-training quantization, my custom CNN model was shrinked to 1/4 of its original size (from 56.1MB to 14MB). I put the image(100x100x3) that is to be predicted into ByteBuffer as 100x100x3=30,000 bytes. However, I got the following error during inference:\n**java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 120000 bytes and a ByteBuffer with 30000 bytes.**\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:136)\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.recongnize(TFClassifier_Lite.java:102)\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.process_by_batch(TFClassifier_Lite.java:145)\n        at gov.nih.nlm.malaria_screener.Cells.runCells(Cells.java:269)\n        at gov.nih.nlm.malaria_screener.CameraActivity.ProcessThinSmearImage(CameraActivity.java:1020)\n        at gov.nih.nlm.malaria_screener.CameraActivity.access$600(CameraActivity.java:75)\n        at gov.nih.nlm.malaria_screener.CameraActivity$8.run(CameraActivity.java:810)\n        at java.lang.Thread.run(Thread.java:762) \n\nThe imput image size to the model is: 100x100x3. I'm currently predicting one image at a time. So, if I'm making the Bytebuffer: 100x100x3 = 30,000 bytes. However, the log info above says the TensorFlowLite buffer has 120,000 bytes. This makes me suspect that the converted tflite model is still in float format. Is this expected behavior? How can I get a quantized model that take input image in 8 pit precision like it does in the example from TensorFlow official repository ?\nIn the example code, the ByteBuffer used as input for tflite.run() is in 8 bit precision for the quantized model.\nBut I also read from the google doc saying, \"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels.\" This two instances seems to contradict each other.\nSource code / logs\nprivate static final int BATCH_SIZE = 1;\n\nprivate static final int DIM_IMG_SIZE = 100;\n\nprivate static final int DIM_PIXEL_SIZE = 3;\n\nprivate static final int BYTE_NUM = 1;\n\nimgData = ByteBuffer.allocateDirect(BYTE_NUM * BATCH_SIZE * DIM_IMG_SIZE * DIM_IMG_SIZE * DIM_PIXEL_SIZE);\nimgData.order(ByteOrder.nativeOrder());\n\n... ...\n\nint pixel = 0;\n\n        for (int i = 0; i < DIM_IMG_SIZE; ++i) {\n            for (int j = 0; j < DIM_IMG_SIZE; ++j) {\n\n                final int val = intValues[pixel++];\n\n                imgData.put((byte)((val >> 16) & 0xFF));\n                imgData.put((byte)((val >> 8) & 0xFF));\n                imgData.put((byte)(val & 0xFF));\n\n//                imgData.putFloat(((val >> 16) & 0xFF) / 255.0f);\n//                imgData.putFloat(((val >> 8) & 0xFF) / 255.0f);\n//                imgData.putFloat((val & 0xFF) / 255.0f);\n\n            }\n        }\n\n... ...\n\ntfLite.run(imgData, labelProb);\n\nPost-training quantization code:\nimport tensorflow as tf\nimport sys\nimport os\n\nsaved_model_dir = '/home/yuh5/Downloads/malaria_thinsmear.h5.pb'\n\ninput_arrays = [\"input_2\"]\n\noutput_arrays = [\"output_node0\"]\n\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(saved_model_dir, input_arrays, output_arrays)\n\nconverter.post_training_quantize = True\n\ntflite_model = converter.convert()\nopen(\"thinSmear_100.tflite\", \"wb\").write(tflite_model)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nSamsung Galaxy S6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0 on PC, 'org.tensorflow:tensorflow-lite:0.0.0-nightly' on Android \r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: tfLite.run(imgData, labelProb);\r\n\r\n### Describe the problem\r\nAfter applying post-training quantization, my custom CNN model was shrinked to 1/4 of its original size (from 56.1MB to 14MB). I put the image(100x100x3) that is to be predicted into ByteBuffer as 100x100x3=30,000 bytes. However, I got the following error during inference:\r\n\r\n```\r\n**java.lang.IllegalArgumentException: Cannot convert between a TensorFlowLite buffer with 120000 bytes and a ByteBuffer with 30000 bytes.**\r\n        at org.tensorflow.lite.Tensor.throwExceptionIfTypeIsIncompatible(Tensor.java:221)\r\n        at org.tensorflow.lite.Tensor.setTo(Tensor.java:93)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:136)\r\n        at org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:216)\r\n        at org.tensorflow.lite.Interpreter.run(Interpreter.java:195)\r\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.recongnize(TFClassifier_Lite.java:102)\r\n        at gov.nih.nlm.malaria_screener.imageProcessing.TFClassifier_Lite.process_by_batch(TFClassifier_Lite.java:145)\r\n        at gov.nih.nlm.malaria_screener.Cells.runCells(Cells.java:269)\r\n        at gov.nih.nlm.malaria_screener.CameraActivity.ProcessThinSmearImage(CameraActivity.java:1020)\r\n        at gov.nih.nlm.malaria_screener.CameraActivity.access$600(CameraActivity.java:75)\r\n        at gov.nih.nlm.malaria_screener.CameraActivity$8.run(CameraActivity.java:810)\r\n        at java.lang.Thread.run(Thread.java:762) \r\n```\r\n\r\nThe imput image size to the model is: 100x100x3. I'm currently predicting one image at a time. So, if I'm making the Bytebuffer: 100x100x3 = 30,000 bytes. However, the log info above says the TensorFlowLite buffer has 120,000 bytes. This makes me suspect that the converted tflite model is still in float format. Is this expected behavior? How can I get a quantized model that take input image in 8 pit precision like it does in the [example](https://github.com/tensorflow/tensorflow/blob/307c83106445ab2c52847f08d35a66c51aff19d9/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java#L163) from TensorFlow official repository ?\r\n\r\nIn the example code, the ByteBuffer used as input for tflite.run() is in 8 bit precision for the quantized model. \r\nBut I also read from the google doc saying, \"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels.\" This two instances seems to contradict each other. \r\n\r\n### Source code / logs\r\n\r\n```\r\nprivate static final int BATCH_SIZE = 1;\r\n\r\nprivate static final int DIM_IMG_SIZE = 100;\r\n\r\nprivate static final int DIM_PIXEL_SIZE = 3;\r\n\r\nprivate static final int BYTE_NUM = 1;\r\n\r\nimgData = ByteBuffer.allocateDirect(BYTE_NUM * BATCH_SIZE * DIM_IMG_SIZE * DIM_IMG_SIZE * DIM_PIXEL_SIZE);\r\nimgData.order(ByteOrder.nativeOrder());\r\n\r\n... ...\r\n\r\nint pixel = 0;\r\n\r\n        for (int i = 0; i < DIM_IMG_SIZE; ++i) {\r\n            for (int j = 0; j < DIM_IMG_SIZE; ++j) {\r\n\r\n                final int val = intValues[pixel++];\r\n\r\n                imgData.put((byte)((val >> 16) & 0xFF));\r\n                imgData.put((byte)((val >> 8) & 0xFF));\r\n                imgData.put((byte)(val & 0xFF));\r\n\r\n//                imgData.putFloat(((val >> 16) & 0xFF) / 255.0f);\r\n//                imgData.putFloat(((val >> 8) & 0xFF) / 255.0f);\r\n//                imgData.putFloat((val & 0xFF) / 255.0f);\r\n\r\n            }\r\n        }\r\n\r\n... ...\r\n\r\ntfLite.run(imgData, labelProb);\r\n```\r\n\r\nPost-training quantization code:\r\n```\r\nimport tensorflow as tf\r\nimport sys\r\nimport os\r\n\r\nsaved_model_dir = '/home/yuh5/Downloads/malaria_thinsmear.h5.pb'\r\n\r\ninput_arrays = [\"input_2\"]\r\n\r\noutput_arrays = [\"output_node0\"]\r\n\r\nconverter = tf.contrib.lite.TocoConverter.from_frozen_graph(saved_model_dir, input_arrays, output_arrays)\r\n\r\nconverter.post_training_quantize = True\r\n\r\ntflite_model = converter.convert()\r\nopen(\"thinSmear_100.tflite\", \"wb\").write(tflite_model)\r\n```\r\n\r\n"}