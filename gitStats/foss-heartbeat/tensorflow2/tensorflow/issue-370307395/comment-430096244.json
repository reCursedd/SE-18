{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430096244", "html_url": "https://github.com/tensorflow/tensorflow/issues/22995#issuecomment-430096244", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22995", "id": 430096244, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDA5NjI0NA==", "user": {"login": "jazzystring1", "id": 33471627, "node_id": "MDQ6VXNlcjMzNDcxNjI3", "avatar_url": "https://avatars1.githubusercontent.com/u/33471627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jazzystring1", "html_url": "https://github.com/jazzystring1", "followers_url": "https://api.github.com/users/jazzystring1/followers", "following_url": "https://api.github.com/users/jazzystring1/following{/other_user}", "gists_url": "https://api.github.com/users/jazzystring1/gists{/gist_id}", "starred_url": "https://api.github.com/users/jazzystring1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jazzystring1/subscriptions", "organizations_url": "https://api.github.com/users/jazzystring1/orgs", "repos_url": "https://api.github.com/users/jazzystring1/repos", "events_url": "https://api.github.com/users/jazzystring1/events{/privacy}", "received_events_url": "https://api.github.com/users/jazzystring1/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-16T04:24:05Z", "updated_at": "2018-10-16T04:24:05Z", "author_association": "NONE", "body_html": "<p>If I'm not mistaken, it's an expected behavior because according to the documentation (<a href=\"https://www.tensorflow.org/performance/post_training_quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/post_training_quantization</a>)<br>\n\"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels. This conversion is done once and cached to reduce latency. \". So it comes out that the model should still be treated as floating point model.</p>", "body_text": "If I'm not mistaken, it's an expected behavior because according to the documentation (https://www.tensorflow.org/performance/post_training_quantization)\n\"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels. This conversion is done once and cached to reduce latency. \". So it comes out that the model should still be treated as floating point model.", "body": "If I'm not mistaken, it's an expected behavior because according to the documentation (https://www.tensorflow.org/performance/post_training_quantization)\r\n\"At inference, weights are converted from 8-bits of precision to floating-point and computed using floating point kernels. This conversion is done once and cached to reduce latency. \". So it comes out that the model should still be treated as floating point model."}