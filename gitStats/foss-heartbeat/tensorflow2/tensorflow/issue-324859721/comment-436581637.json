{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436581637", "html_url": "https://github.com/tensorflow/tensorflow/issues/19431#issuecomment-436581637", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19431", "id": 436581637, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjU4MTYzNw==", "user": {"login": "SubhashKsr", "id": 11551745, "node_id": "MDQ6VXNlcjExNTUxNzQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/11551745?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SubhashKsr", "html_url": "https://github.com/SubhashKsr", "followers_url": "https://api.github.com/users/SubhashKsr/followers", "following_url": "https://api.github.com/users/SubhashKsr/following{/other_user}", "gists_url": "https://api.github.com/users/SubhashKsr/gists{/gist_id}", "starred_url": "https://api.github.com/users/SubhashKsr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SubhashKsr/subscriptions", "organizations_url": "https://api.github.com/users/SubhashKsr/orgs", "repos_url": "https://api.github.com/users/SubhashKsr/repos", "events_url": "https://api.github.com/users/SubhashKsr/events{/privacy}", "received_events_url": "https://api.github.com/users/SubhashKsr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T10:41:28Z", "updated_at": "2018-11-07T10:41:28Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=25754898\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrehentz\">@andrehentz</a> did you mean the following when you said, \"<strong>If you are looking into weight-only quantization (and are well aware of the possible accuracy degradation) you can pass --quantize_weights=true to toco.</strong>\"</p>\n<p>Please help me understand this better.</p>\n<pre><code>bazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=/tmp/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen_quantized_graph.pb \\\n  --output_file=/tmp/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen_quantized_graph.lite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --input_shapes=1,224,224,3 \\\n  --mean_values=128 \\\n  --std_values=128 \\\n  --input_arrays=\"input\" \\\n  --output_arrays=\"MobilenetV1/Predictions/Reshape_1\" \\\n  //--inference_type=QUANTIZED_UINT8 \\\n  --quantize_weights=true \\\n  --default_ranges_min=0 \\\n  --default_ranges_max=6\n</code></pre>\n<p>Please help me solve this error ?</p>\n<pre><code>2018-11-07 16:44:16.082872: F tensorflow/lite/toco/graph_transformations/resolve_batch_normalization.cc:45] Check failed: IsConstantParameterArray(*model, bn_op-&gt;inputs[1]) &amp;&amp; IsConstantParameterArray(*model, bn_op-&gt;inputs[2]) &amp;&amp; IsConstantParameterArray(*model, bn_op-&gt;inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\nAbort trap: 6\n</code></pre>\n<p>I found the possible explanation for the issue here in <a href=\"https://github.com/tensorflow/tensorflow/issues/15336\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/15336/hovercard\">#15336</a></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave (10.14 (18A391))</li>\n<li>TensorFlow installed from (source or binary): source</li>\n<li>TensorFlow version : 1.11.0</li>\n<li>Python version: 2.7.10</li>\n<li>Bazel version : 0.19.0<br>\n-[GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.0.42)] on darwin</li>\n<li>CUDA/cuDNN version: N/A (build without support CUDA)</li>\n<li>GPU model and memory: N/A (build without support CUDA)</li>\n<li>Exact command to reproduce:</li>\n</ul>\n<pre><code>./toco --input_format=TENSORFLOW_GRAPHDEF --input_file=/Users/......./Documents/data/output_graph.pb --output_format=TFLITE --output_file=/Users/......./Documents/data/facenet_model_quantized.lite --quantize_weights=true --input_arrays=input --output_arrays=embeddings --input_shapes=1,160,160,3 --mean_values=128 --std_values=128 --default_ranges_min=0 --default_ranges_max=6\n</code></pre>\n<p>This is the <a href=\"https://drive.google.com/file/d/1q4MVVTGKYxPOGhjiVz_BUCzEgR76LD-m/view?usp=sharing\" rel=\"nofollow\">model</a>, I have been trying to convert to .lite format</p>", "body_text": "@andrehentz did you mean the following when you said, \"If you are looking into weight-only quantization (and are well aware of the possible accuracy degradation) you can pass --quantize_weights=true to toco.\"\nPlease help me understand this better.\nbazel run --config=opt \\\n  //tensorflow/contrib/lite/toco:toco -- \\\n  --input_file=/tmp/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen_quantized_graph.pb \\\n  --output_file=/tmp/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen_quantized_graph.lite \\\n  --input_format=TENSORFLOW_GRAPHDEF \\\n  --output_format=TFLITE \\\n  --input_shapes=1,224,224,3 \\\n  --mean_values=128 \\\n  --std_values=128 \\\n  --input_arrays=\"input\" \\\n  --output_arrays=\"MobilenetV1/Predictions/Reshape_1\" \\\n  //--inference_type=QUANTIZED_UINT8 \\\n  --quantize_weights=true \\\n  --default_ranges_min=0 \\\n  --default_ranges_max=6\n\nPlease help me solve this error ?\n2018-11-07 16:44:16.082872: F tensorflow/lite/toco/graph_transformations/resolve_batch_normalization.cc:45] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\nAbort trap: 6\n\nI found the possible explanation for the issue here in #15336\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave (10.14 (18A391))\nTensorFlow installed from (source or binary): source\nTensorFlow version : 1.11.0\nPython version: 2.7.10\nBazel version : 0.19.0\n-[GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.0.42)] on darwin\nCUDA/cuDNN version: N/A (build without support CUDA)\nGPU model and memory: N/A (build without support CUDA)\nExact command to reproduce:\n\n./toco --input_format=TENSORFLOW_GRAPHDEF --input_file=/Users/......./Documents/data/output_graph.pb --output_format=TFLITE --output_file=/Users/......./Documents/data/facenet_model_quantized.lite --quantize_weights=true --input_arrays=input --output_arrays=embeddings --input_shapes=1,160,160,3 --mean_values=128 --std_values=128 --default_ranges_min=0 --default_ranges_max=6\n\nThis is the model, I have been trying to convert to .lite format", "body": "@andrehentz did you mean the following when you said, \"**If you are looking into weight-only quantization (and are well aware of the possible accuracy degradation) you can pass --quantize_weights=true to toco.**\"\r\n\r\nPlease help me understand this better.\r\n\r\n```\r\nbazel run --config=opt \\\r\n  //tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=/tmp/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen_quantized_graph.pb \\\r\n  --output_file=/tmp/mobilenet_v1_1.0_224/mobilenet_v1_1.0_224_frozen_quantized_graph.lite \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shapes=1,224,224,3 \\\r\n  --mean_values=128 \\\r\n  --std_values=128 \\\r\n  --input_arrays=\"input\" \\\r\n  --output_arrays=\"MobilenetV1/Predictions/Reshape_1\" \\\r\n  //--inference_type=QUANTIZED_UINT8 \\\r\n  --quantize_weights=true \\\r\n  --default_ranges_min=0 \\\r\n  --default_ranges_max=6\r\n```\r\nPlease help me solve this error ?\r\n\r\n```\r\n2018-11-07 16:44:16.082872: F tensorflow/lite/toco/graph_transformations/resolve_batch_normalization.cc:45] Check failed: IsConstantParameterArray(*model, bn_op->inputs[1]) && IsConstantParameterArray(*model, bn_op->inputs[2]) && IsConstantParameterArray(*model, bn_op->inputs[3]) Batch normalization resolution requires that mean, multiplier and offset arrays be constant.\r\nAbort trap: 6\r\n```\r\n\r\nI found the possible explanation for the issue here in [#15336](https://github.com/tensorflow/tensorflow/issues/15336)\r\n\r\n\r\n**System information**\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS Mojave (10.14 (18A391))\r\n- TensorFlow installed from (source or binary): source\r\n- TensorFlow version : 1.11.0\r\n- Python version: 2.7.10\r\n- Bazel version : 0.19.0\r\n-[GCC 4.2.1 Compatible Apple LLVM 10.0.0 (clang-1000.0.42)] on darwin\r\n- CUDA/cuDNN version: N/A (build without support CUDA)\r\n- GPU model and memory: N/A (build without support CUDA)\r\n- Exact command to reproduce:\r\n\r\n```\r\n./toco --input_format=TENSORFLOW_GRAPHDEF --input_file=/Users/......./Documents/data/output_graph.pb --output_format=TFLITE --output_file=/Users/......./Documents/data/facenet_model_quantized.lite --quantize_weights=true --input_arrays=input --output_arrays=embeddings --input_shapes=1,160,160,3 --mean_values=128 --std_values=128 --default_ranges_min=0 --default_ranges_max=6\r\n```\r\n\r\nThis is the [model](https://drive.google.com/file/d/1q4MVVTGKYxPOGhjiVz_BUCzEgR76LD-m/view?usp=sharing), I have been trying to convert to .lite format\r\n"}