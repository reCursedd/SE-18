{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351407919", "html_url": "https://github.com/tensorflow/tensorflow/issues/9445#issuecomment-351407919", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9445", "id": 351407919, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTQwNzkxOQ==", "user": {"login": "mirh", "id": 9366725, "node_id": "MDQ6VXNlcjkzNjY3MjU=", "avatar_url": "https://avatars1.githubusercontent.com/u/9366725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mirh", "html_url": "https://github.com/mirh", "followers_url": "https://api.github.com/users/mirh/followers", "following_url": "https://api.github.com/users/mirh/following{/other_user}", "gists_url": "https://api.github.com/users/mirh/gists{/gist_id}", "starred_url": "https://api.github.com/users/mirh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mirh/subscriptions", "organizations_url": "https://api.github.com/users/mirh/orgs", "repos_url": "https://api.github.com/users/mirh/repos", "events_url": "https://api.github.com/users/mirh/events{/privacy}", "received_events_url": "https://api.github.com/users/mirh/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-13T14:32:28Z", "updated_at": "2017-12-13T14:33:49Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>I believe the computecpp_info tool will describe platforms we test internally as supported, most other platforms as \"untested\" and those we know won't work as \"unsupported\". The documentation could be clearer on that point.</p>\n</blockquote>\n<p>Cool, that would really be neat.</p>\n<blockquote>\n<p>SPIR 1.2 and SPIR-V are very distinct formats. It might well be easier for them not to have to support 1.2 in favour of developing other IR formats, like SPIR-V.</p>\n</blockquote>\n<p>Well, then in turn you'd be dumping OpenCL 1.2 support altogether.<br>\nWhich to be really honest, at least atm is the biggest selling point of all the affair.</p>\n<blockquote>\n<p>Which AMD note 4 is that exactly? When I look at the historical docs, it looks like that is the one saying that we can't do AMD on 16.04, no?</p>\n</blockquote>\n<p>Yes, and notice how it's still there, mentioned in the table notes.. Yet no description is currently available.<br>\np.s. I just noticed it kind of moved inside AMD-2 now.</p>\n<blockquote>\n<p>I can try to find a spare machine to test the GPUPRO driver but I don't actually have compatible hardware myself, so this might not happen.</p>\n</blockquote>\n<p>FTR folks over at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115928097\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/22/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/22\">#22</a> are calling it a day.<br>\nI wouldn't know if it was something fixed on theirs or yours part though.</p>\n<blockquote>\n<p>We could also add some notes about the Fedora and Arch patches though that seems more flaky than I'd be comfortable suggesting.</p>\n</blockquote>\n<p>I can totally see it.<br>\nMaybe just simply something between brackets at the end of current note could do it too?<br>\nWith or without even links, if you don't feel at ease.</p>\n<blockquote>\n<p>What I will say as a final thought is that we are planning to continue updating the compiler to tip Clang, which will increase the number of IR formats that we can support (for example, recently we enabled PTX output from compute++, and while it's early days we can run some basic samples on nvidia hardware).</p>\n</blockquote>\n<p>Absolutely, keep going.<br>\nWith that, AFAIK you are the most (possibly only) cross-vendor library/path out there for accelerating ML/DNN.</p>", "body_text": "I believe the computecpp_info tool will describe platforms we test internally as supported, most other platforms as \"untested\" and those we know won't work as \"unsupported\". The documentation could be clearer on that point.\n\nCool, that would really be neat.\n\nSPIR 1.2 and SPIR-V are very distinct formats. It might well be easier for them not to have to support 1.2 in favour of developing other IR formats, like SPIR-V.\n\nWell, then in turn you'd be dumping OpenCL 1.2 support altogether.\nWhich to be really honest, at least atm is the biggest selling point of all the affair.\n\nWhich AMD note 4 is that exactly? When I look at the historical docs, it looks like that is the one saying that we can't do AMD on 16.04, no?\n\nYes, and notice how it's still there, mentioned in the table notes.. Yet no description is currently available.\np.s. I just noticed it kind of moved inside AMD-2 now.\n\nI can try to find a spare machine to test the GPUPRO driver but I don't actually have compatible hardware myself, so this might not happen.\n\nFTR folks over at #22 are calling it a day.\nI wouldn't know if it was something fixed on theirs or yours part though.\n\nWe could also add some notes about the Fedora and Arch patches though that seems more flaky than I'd be comfortable suggesting.\n\nI can totally see it.\nMaybe just simply something between brackets at the end of current note could do it too?\nWith or without even links, if you don't feel at ease.\n\nWhat I will say as a final thought is that we are planning to continue updating the compiler to tip Clang, which will increase the number of IR formats that we can support (for example, recently we enabled PTX output from compute++, and while it's early days we can run some basic samples on nvidia hardware).\n\nAbsolutely, keep going.\nWith that, AFAIK you are the most (possibly only) cross-vendor library/path out there for accelerating ML/DNN.", "body": "> I believe the computecpp_info tool will describe platforms we test internally as supported, most other platforms as \"untested\" and those we know won't work as \"unsupported\". The documentation could be clearer on that point.\r\n\r\nCool, that would really be neat. \r\n\r\n> SPIR 1.2 and SPIR-V are very distinct formats. It might well be easier for them not to have to support 1.2 in favour of developing other IR formats, like SPIR-V.\r\n\r\nWell, then in turn you'd be dumping OpenCL 1.2 support altogether. \r\nWhich to be really honest, at least atm is the biggest selling point of all the affair. \r\n\r\n> Which AMD note 4 is that exactly? When I look at the historical docs, it looks like that is the one saying that we can't do AMD on 16.04, no?\r\n\r\nYes, and notice how it's still there, mentioned in the table notes.. Yet no description is currently available. \r\np.s. I just noticed it kind of moved inside AMD-2 now. \r\n\r\n> I can try to find a spare machine to test the GPUPRO driver but I don't actually have compatible hardware myself, so this might not happen.\r\n\r\nFTR folks over at #22 are calling it a day. \r\nI wouldn't know if it was something fixed on theirs or yours part though. \r\n\r\n> We could also add some notes about the Fedora and Arch patches though that seems more flaky than I'd be comfortable suggesting.\r\n\r\nI can totally see it. \r\nMaybe just simply something between brackets at the end of current note could do it too?\r\nWith or without even links, if you don't feel at ease. \r\n\r\n> What I will say as a final thought is that we are planning to continue updating the compiler to tip Clang, which will increase the number of IR formats that we can support (for example, recently we enabled PTX output from compute++, and while it's early days we can run some basic samples on nvidia hardware).\r\n\r\nAbsolutely, keep going. \r\nWith that, AFAIK you are the most (possibly only) cross-vendor library/path out there for accelerating ML/DNN. "}