{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21271", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21271/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21271/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21271/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21271", "id": 346143103, "node_id": "MDU6SXNzdWUzNDYxNDMxMDM=", "number": 21271, "title": "tf.nn.softmax_cross_entropy_with_logits_v2 returns wrong value with soft labels", "user": {"login": "fenghou1st", "id": 8263678, "node_id": "MDQ6VXNlcjgyNjM2Nzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/8263678?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fenghou1st", "html_url": "https://github.com/fenghou1st", "followers_url": "https://api.github.com/users/fenghou1st/followers", "following_url": "https://api.github.com/users/fenghou1st/following{/other_user}", "gists_url": "https://api.github.com/users/fenghou1st/gists{/gist_id}", "starred_url": "https://api.github.com/users/fenghou1st/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fenghou1st/subscriptions", "organizations_url": "https://api.github.com/users/fenghou1st/orgs", "repos_url": "https://api.github.com/users/fenghou1st/repos", "events_url": "https://api.github.com/users/fenghou1st/events{/privacy}", "received_events_url": "https://api.github.com/users/fenghou1st/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-07-31T11:16:53Z", "updated_at": "2018-08-01T05:45:56Z", "closed_at": "2018-08-01T05:43:23Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: Yes</li>\n<li><strong>OS Platform and Distribution</strong>: Linux CentOS 7.4</li>\n<li><strong>Mobile device if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from</strong>: binary</li>\n<li><strong>TensorFlow version</strong>: v1.9.0-0-g25c197e023 1.9.0</li>\n<li><strong>Python version</strong>: Python 3.6.3</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>GCC/Compiler version</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0 / 7.0.5</li>\n<li><strong>GPU model and memory</strong>:  GeForce 940MX, 4GB</li>\n<li><strong>Exact command to reproduce</strong>: save the codes below as a .py file, and run it with command-line something like <code>python3 test.py</code>.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I use soft labels (for example, [0.2, 0.8] instead of [0, 1]) in a CNN model, in which I use <code>tf.nn.softmax_cross_entropy_with_logits_v2</code> for loss computing. But when I trained the model, the loss became +inf in 10 steps, so I debugged the codes and found that the problem was caused by <code>tf.nn.softmax_cross_entropy_with_logits_v2</code>.<br>\nSo I implemented the softmax and cross_entropy process separately, then the returned value seemed to make sense.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ntf.enable_eager_execution()\ntf.executing_eagerly()\n\nlogits <span class=\"pl-k\">=</span> [[<span class=\"pl-k\">-</span><span class=\"pl-c1\">4885.4614</span>, <span class=\"pl-c1\">4878.027</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5188.321</span>, <span class=\"pl-c1\">5179.7915</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4121.558</span>, <span class=\"pl-c1\">4114.9995</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5165.612</span>, <span class=\"pl-c1\">5157.5044</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4152.7183</span>, <span class=\"pl-c1\">4145.978</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5175.428</span>, <span class=\"pl-c1\">5167.603</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4514.224</span>, <span class=\"pl-c1\">4506.477</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4752.5854</span>, <span class=\"pl-c1\">4745.2524</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5580.9463</span>, <span class=\"pl-c1\">5572.2275</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5164.766</span>, <span class=\"pl-c1\">5156.6685</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4273.686</span>, <span class=\"pl-c1\">4266.31</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4886.724</span>, <span class=\"pl-c1\">4879.5757</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5216.2935</span>, <span class=\"pl-c1\">5208.269</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5411.082</span>, <span class=\"pl-c1\">5402.344</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">6057.239</span>, <span class=\"pl-c1\">6048.3647</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5314.882</span>, <span class=\"pl-c1\">5306.708</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5674.2505</span>, <span class=\"pl-c1\">5664.6436</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5650.7827</span>, <span class=\"pl-c1\">5642.1997</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4301.4194</span>, <span class=\"pl-c1\">4294.5957</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5156.4683</span>, <span class=\"pl-c1\">5148.283</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5032.6797</span>, <span class=\"pl-c1\">5024.821</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5072.1533</span>, <span class=\"pl-c1\">5064.013</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4129.488</span>, <span class=\"pl-c1\">4123.4355</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4915.1147</span>, <span class=\"pl-c1\">4907.8643</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5256.5747</span>, <span class=\"pl-c1\">5248.351</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">5297.694</span>, <span class=\"pl-c1\">5289.386</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4979.939</span>, <span class=\"pl-c1\">4971.691</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4895.983</span>, <span class=\"pl-c1\">4887.4897</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4757.732</span>, <span class=\"pl-c1\">4749.2886</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4871.5654</span>, <span class=\"pl-c1\">4863.604</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4772.0356</span>, <span class=\"pl-c1\">4763.9414</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">4528.853</span>, <span class=\"pl-c1\">4521.4424</span>],]\nlabels = [[9.6226019e-01, 3.7739828e-02], [9.5367432e-07, 9.9999905e-01], [9.9017835e-01, 9.8216468e-03], [8.3446503e-07, 9.9999917e-01], [9.9999952e-01, 4.9012118e-07], [3.6466300e-02, 9.6353370e-01], [3.2732385e-01, 6.7267615e-01], [2.1918458e-01, 7.8081542e-01], [3.4707606e-02, 9.6529239e-01], [1.3036132e-03, 9.9869639e-01], [4.1835755e-01, 5.8164245e-01], [5.0599152e-01, 4.9400848e-01], [9.9629784e-01, 3.7021455e-03], [1.9747615e-03, 9.9802524e-01], [8.2850456e-06, 9.9999171e-01], [8.1463850e-01, 1.8536153e-01], [7.6112747e-03, 9.9238873e-01], [9.4729370e-01, 5.2706327e-02], [9.4496381e-01, 5.5036198e-02], [3.3008814e-02, 9.6699119e-01], [1.0292470e-02, 9.8970753e-01], [9.9998099e-01, 1.9039073e-05], [7.5116873e-01, 2.4883127e-01], [9.9973243e-01, 2.6756502e-04], [3.1858683e-04, 9.9968141e-01], [3.6358833e-05, 9.9996364e-01], [9.3631679e-01, 6.3683234e-02], [7.9292524e-01, 2.0707479e-01], [9.9999642e-01, 3.6055078e-06], [4.9336654e-01, 5.0663346e-01], [6.9030523e-03, 9.9309695e-01], [9.9974275e-01, 2.5725813e-04],]\n\nlogits <span class=\"pl-k\">=</span> np.array(logits, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\nlabels <span class=\"pl-k\">=</span> np.array(labels, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Check each row of labels is a valid probability distribution.</span>\nlabels_sum <span class=\"pl-k\">=</span> tf.reduce_sum(labels, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n<span class=\"pl-c1\">print</span>(labels_sum)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Cross entropy computed by tf.nn.softmax_cross_entropy_with_logits_v2</span>\ncross_entropy <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits_v2(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\ncross_entropy_mean <span class=\"pl-k\">=</span> tf.reduce_mean(cross_entropy)\n<span class=\"pl-c1\">print</span>(cross_entropy_mean)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Cross entropy computed separately</span>\nsoftmax <span class=\"pl-k\">=</span> tf.nn.softmax(logits)\ncross_entropy_mean <span class=\"pl-k\">=</span> tf.reduce_mean(<span class=\"pl-k\">-</span>tf.reduce_sum(softmax <span class=\"pl-k\">*</span> tf.log(labels), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>))\n<span class=\"pl-c1\">print</span>(cross_entropy_mean)</pre></div>\n<p><strong>Output:</strong></p>\n<pre><code>tf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(32,), dtype=float32)\ntf.Tensor(4538.922, shape=(), dtype=float32)\ntf.Tensor(2.621081, shape=(), dtype=float32)\n</code></pre>", "body_text": "System information\n\nHave I written custom code: Yes\nOS Platform and Distribution: Linux CentOS 7.4\nMobile device if the issue happens on mobile device: N/A\nTensorFlow installed from: binary\nTensorFlow version: v1.9.0-0-g25c197e023 1.9.0\nPython version: Python 3.6.3\nBazel version: N/A\nGCC/Compiler version: N/A\nCUDA/cuDNN version: 9.0 / 7.0.5\nGPU model and memory:  GeForce 940MX, 4GB\nExact command to reproduce: save the codes below as a .py file, and run it with command-line something like python3 test.py.\n\nDescribe the problem\nI use soft labels (for example, [0.2, 0.8] instead of [0, 1]) in a CNN model, in which I use tf.nn.softmax_cross_entropy_with_logits_v2 for loss computing. But when I trained the model, the loss became +inf in 10 steps, so I debugged the codes and found that the problem was caused by tf.nn.softmax_cross_entropy_with_logits_v2.\nSo I implemented the softmax and cross_entropy process separately, then the returned value seemed to make sense.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\n\ntf.enable_eager_execution()\ntf.executing_eagerly()\n\nlogits = [[-4885.4614, 4878.027], [-5188.321, 5179.7915], [-4121.558, 4114.9995], [-5165.612, 5157.5044], [-4152.7183, 4145.978], [-5175.428, 5167.603], [-4514.224, 4506.477], [-4752.5854, 4745.2524], [-5580.9463, 5572.2275], [-5164.766, 5156.6685], [-4273.686, 4266.31], [-4886.724, 4879.5757], [-5216.2935, 5208.269], [-5411.082, 5402.344], [-6057.239, 6048.3647], [-5314.882, 5306.708], [-5674.2505, 5664.6436], [-5650.7827, 5642.1997], [-4301.4194, 4294.5957], [-5156.4683, 5148.283], [-5032.6797, 5024.821], [-5072.1533, 5064.013], [-4129.488, 4123.4355], [-4915.1147, 4907.8643], [-5256.5747, 5248.351], [-5297.694, 5289.386], [-4979.939, 4971.691], [-4895.983, 4887.4897], [-4757.732, 4749.2886], [-4871.5654, 4863.604], [-4772.0356, 4763.9414], [-4528.853, 4521.4424],]\nlabels = [[9.6226019e-01, 3.7739828e-02], [9.5367432e-07, 9.9999905e-01], [9.9017835e-01, 9.8216468e-03], [8.3446503e-07, 9.9999917e-01], [9.9999952e-01, 4.9012118e-07], [3.6466300e-02, 9.6353370e-01], [3.2732385e-01, 6.7267615e-01], [2.1918458e-01, 7.8081542e-01], [3.4707606e-02, 9.6529239e-01], [1.3036132e-03, 9.9869639e-01], [4.1835755e-01, 5.8164245e-01], [5.0599152e-01, 4.9400848e-01], [9.9629784e-01, 3.7021455e-03], [1.9747615e-03, 9.9802524e-01], [8.2850456e-06, 9.9999171e-01], [8.1463850e-01, 1.8536153e-01], [7.6112747e-03, 9.9238873e-01], [9.4729370e-01, 5.2706327e-02], [9.4496381e-01, 5.5036198e-02], [3.3008814e-02, 9.6699119e-01], [1.0292470e-02, 9.8970753e-01], [9.9998099e-01, 1.9039073e-05], [7.5116873e-01, 2.4883127e-01], [9.9973243e-01, 2.6756502e-04], [3.1858683e-04, 9.9968141e-01], [3.6358833e-05, 9.9996364e-01], [9.3631679e-01, 6.3683234e-02], [7.9292524e-01, 2.0707479e-01], [9.9999642e-01, 3.6055078e-06], [4.9336654e-01, 5.0663346e-01], [6.9030523e-03, 9.9309695e-01], [9.9974275e-01, 2.5725813e-04],]\n\nlogits = np.array(logits, dtype=np.float32)\nlabels = np.array(labels, dtype=np.float32)\n\n# Check each row of labels is a valid probability distribution.\nlabels_sum = tf.reduce_sum(labels, axis=-1)\nprint(labels_sum)\n\n# Cross entropy computed by tf.nn.softmax_cross_entropy_with_logits_v2\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\nprint(cross_entropy_mean)\n\n# Cross entropy computed separately\nsoftmax = tf.nn.softmax(logits)\ncross_entropy_mean = tf.reduce_mean(-tf.reduce_sum(softmax * tf.log(labels), axis=-1))\nprint(cross_entropy_mean)\nOutput:\ntf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(32,), dtype=float32)\ntf.Tensor(4538.922, shape=(), dtype=float32)\ntf.Tensor(2.621081, shape=(), dtype=float32)", "body": "### System information\r\n- **Have I written custom code**: Yes\r\n- **OS Platform and Distribution**: Linux CentOS 7.4\r\n- **Mobile device if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from**: binary\r\n- **TensorFlow version**: v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: Python 3.6.3\r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: 9.0 / 7.0.5\r\n- **GPU model and memory**:  GeForce 940MX, 4GB\r\n- **Exact command to reproduce**: save the codes below as a .py file, and run it with command-line something like `python3 test.py`.\r\n\r\n### Describe the problem\r\nI use soft labels (for example, [0.2, 0.8] instead of [0, 1]) in a CNN model, in which I use `tf.nn.softmax_cross_entropy_with_logits_v2` for loss computing. But when I trained the model, the loss became +inf in 10 steps, so I debugged the codes and found that the problem was caused by `tf.nn.softmax_cross_entropy_with_logits_v2`.\r\nSo I implemented the softmax and cross_entropy process separately, then the returned value seemed to make sense. \r\n\r\n### Source code / logs\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ntf.executing_eagerly()\r\n\r\nlogits = [[-4885.4614, 4878.027], [-5188.321, 5179.7915], [-4121.558, 4114.9995], [-5165.612, 5157.5044], [-4152.7183, 4145.978], [-5175.428, 5167.603], [-4514.224, 4506.477], [-4752.5854, 4745.2524], [-5580.9463, 5572.2275], [-5164.766, 5156.6685], [-4273.686, 4266.31], [-4886.724, 4879.5757], [-5216.2935, 5208.269], [-5411.082, 5402.344], [-6057.239, 6048.3647], [-5314.882, 5306.708], [-5674.2505, 5664.6436], [-5650.7827, 5642.1997], [-4301.4194, 4294.5957], [-5156.4683, 5148.283], [-5032.6797, 5024.821], [-5072.1533, 5064.013], [-4129.488, 4123.4355], [-4915.1147, 4907.8643], [-5256.5747, 5248.351], [-5297.694, 5289.386], [-4979.939, 4971.691], [-4895.983, 4887.4897], [-4757.732, 4749.2886], [-4871.5654, 4863.604], [-4772.0356, 4763.9414], [-4528.853, 4521.4424],]\r\nlabels = [[9.6226019e-01, 3.7739828e-02], [9.5367432e-07, 9.9999905e-01], [9.9017835e-01, 9.8216468e-03], [8.3446503e-07, 9.9999917e-01], [9.9999952e-01, 4.9012118e-07], [3.6466300e-02, 9.6353370e-01], [3.2732385e-01, 6.7267615e-01], [2.1918458e-01, 7.8081542e-01], [3.4707606e-02, 9.6529239e-01], [1.3036132e-03, 9.9869639e-01], [4.1835755e-01, 5.8164245e-01], [5.0599152e-01, 4.9400848e-01], [9.9629784e-01, 3.7021455e-03], [1.9747615e-03, 9.9802524e-01], [8.2850456e-06, 9.9999171e-01], [8.1463850e-01, 1.8536153e-01], [7.6112747e-03, 9.9238873e-01], [9.4729370e-01, 5.2706327e-02], [9.4496381e-01, 5.5036198e-02], [3.3008814e-02, 9.6699119e-01], [1.0292470e-02, 9.8970753e-01], [9.9998099e-01, 1.9039073e-05], [7.5116873e-01, 2.4883127e-01], [9.9973243e-01, 2.6756502e-04], [3.1858683e-04, 9.9968141e-01], [3.6358833e-05, 9.9996364e-01], [9.3631679e-01, 6.3683234e-02], [7.9292524e-01, 2.0707479e-01], [9.9999642e-01, 3.6055078e-06], [4.9336654e-01, 5.0663346e-01], [6.9030523e-03, 9.9309695e-01], [9.9974275e-01, 2.5725813e-04],]\r\n\r\nlogits = np.array(logits, dtype=np.float32)\r\nlabels = np.array(labels, dtype=np.float32)\r\n\r\n# Check each row of labels is a valid probability distribution.\r\nlabels_sum = tf.reduce_sum(labels, axis=-1)\r\nprint(labels_sum)\r\n\r\n# Cross entropy computed by tf.nn.softmax_cross_entropy_with_logits_v2\r\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\r\ncross_entropy_mean = tf.reduce_mean(cross_entropy)\r\nprint(cross_entropy_mean)\r\n\r\n# Cross entropy computed separately\r\nsoftmax = tf.nn.softmax(logits)\r\ncross_entropy_mean = tf.reduce_mean(-tf.reduce_sum(softmax * tf.log(labels), axis=-1))\r\nprint(cross_entropy_mean)\r\n```\r\n\r\n**Output:**\r\n```\r\ntf.Tensor([1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.], shape=(32,), dtype=float32)\r\ntf.Tensor(4538.922, shape=(), dtype=float32)\r\ntf.Tensor(2.621081, shape=(), dtype=float32)\r\n```"}