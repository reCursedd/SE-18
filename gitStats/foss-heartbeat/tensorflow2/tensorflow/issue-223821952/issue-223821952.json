{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9415", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9415/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9415/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9415/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9415", "id": 223821952, "node_id": "MDU6SXNzdWUyMjM4MjE5NTI=", "number": 9415, "title": "XLA \"resnet 50\" style example crash during graph building", "user": {"login": "DavidNorman", "id": 606831, "node_id": "MDQ6VXNlcjYwNjgzMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/606831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNorman", "html_url": "https://github.com/DavidNorman", "followers_url": "https://api.github.com/users/DavidNorman/followers", "following_url": "https://api.github.com/users/DavidNorman/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNorman/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNorman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNorman/subscriptions", "organizations_url": "https://api.github.com/users/DavidNorman/orgs", "repos_url": "https://api.github.com/users/DavidNorman/repos", "events_url": "https://api.github.com/users/DavidNorman/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNorman/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-24T13:34:20Z", "updated_at": "2017-05-13T08:58:20Z", "closed_at": "2017-05-13T08:58:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Note: The test code below was cobbled together from various un-credited sources.</p>\n<p>The example crashes, during the building of the reduce_mean.   It fails with quite a large stack, maybe some sort of stack overflow?  Without the last two 'block's (marked with #*****) the compilation succeeds.</p>\n<p>Perhaps it is as simple as it failing on my laptop (OS/X, 16GB RAM).</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef _get_variable(name, shape, initializer, dtype=tf.float32):\n\n  return tf.get_variable(name,\n                         shape=shape,\n                         initializer=initializer,\n                         dtype=dtype)\ndef inference(x):\n\n  with tf.variable_scope('scale1', use_resource=True):\n    x = conv(x, 7, 2, 64)\n    x = tf.nn.relu(x)\n\n  with tf.variable_scope('max_pool', use_resource=True):\n    x = max_pool(x, ksize=3, stride=2)\n\n  with tf.variable_scope('scale2-1', use_resource=True):\n    x = block(x, 1, 64, 256)\n\n  with tf.variable_scope('scale2-2', use_resource=True):\n    x = block(x, 1, 64, 256)\n\n  with tf.variable_scope('scale2-3', use_resource=True):\n    x = block(x, 1, 64, 256)\n\n\n\n  with tf.variable_scope('scale3-1', use_resource=True):\n    x = block(x, 2, 128, 512)\n\n  with tf.variable_scope('scale3-2', use_resource=True):\n    x = block(x, 1, 128, 512)\n\n  with tf.variable_scope('scale3-3', use_resource=True):\n    x = block(x, 1, 128, 512)\n\n  with tf.variable_scope('scale3-4', use_resource=True):\n    x = block(x, 1, 128, 512)\n\n\n\n  with tf.variable_scope('scale4-1', use_resource=True):\n    x = block(x, 2, 256, 1024)\n\n  with tf.variable_scope('scale4-2', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-3', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-4', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-5', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-6', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n\n\n  with tf.variable_scope('scale5-1', use_resource=True):\n    x = block(x, 2, 512, 2048)\n\n  with tf.variable_scope('scale5-2', use_resource=True):  #*****\n    x = block(x, 1, 512, 2048)\n\n  with tf.variable_scope('scale5-3', use_resource=True): #*****\n    x = block(x, 1, 512, 2048)\n\n  x = tf.reduce_mean(x, reduction_indices=[1, 2])\n\n  with tf.variable_scope('fc', use_resource=True):\n    x = fc(x, 1000)\n\n  return x\n\n\ndef block(x, first_stride, internal_filters, final_filters):\n  shape_in = x.get_shape()\n\n  shortcut = x\n\n  with tf.variable_scope('a', use_resource=True):\n    x = conv(x, 1, first_stride, internal_filters)\n    x = tf.nn.relu(x)\n\n  with tf.variable_scope('b', use_resource=True):\n    x = conv(x, 3, 1, internal_filters)\n    x = tf.nn.relu(x)\n\n  with tf.variable_scope('c', use_resource=True):\n    x = conv(x, 1, 1, final_filters)\n\n  with tf.variable_scope('shortcut', use_resource=True):\n    pad = int(x.get_shape()[-1] - shape_in[-1])\n    kernel = np.reshape(np.concatenate((np.identity(shape_in[-1], dtype=np.float32),\n                                        np.zeros([pad, shape_in[-1]]))),\n                        [1, 1, shape_in[-1], final_filters])\n\n    shortcut = tf.nn.conv2d(shortcut,\n                            kernel,\n                            [1,first_stride,first_stride,1],\n                            padding='SAME')\n\n  return tf.nn.relu(x + shortcut)\n\n\ndef fc(x, num_units_out):\n  num_units_in = x.get_shape()[1]\n  weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n\n  weights = _get_variable('weights', shape=[num_units_in, num_units_out],\n                          initializer=weights_initializer)\n  biases = _get_variable('biases', shape=[num_units_out],\n                         initializer=tf.constant_initializer(0.0))\n\n  x = tf.nn.xw_plus_b(x, weights, biases)\n\n  return x\n\ndef conv(x, ksize, stride, filters_out):\n\n  filters_in = x.get_shape()[-1]\n  shape = [ksize, ksize, filters_in, filters_out]\n  initializer = tf.truncated_normal_initializer(stddev=0.1)\n\n  weights = _get_variable('weights', shape=shape, initializer=initializer)\n  return tf.nn.conv2d(x,\n                      weights,\n                      [1, stride, stride, 1],\n                      padding='SAME')\n\n\ndef max_pool(x, ksize=3, stride=2):\n  return tf.nn.max_pool(x,\n                        ksize=[1, ksize, ksize, 1],\n                        strides=[1, stride, stride, 1],\n                        padding='SAME')\n\n\n#\n# Main code\n#\n\nwith tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\n  # Inputs\n  x = tf.placeholder(tf.float32, shape=[2, 224, 224, 4])\n\n  # Inference\n  logits = inference(x)\n\nsess = tf.InteractiveSession()\n\nsess.run(tf.global_variables_initializer())\n\ntraining_data = np.zeros([2, 224, 224, 4]);\n\nsess.run(logits, feed_dict={x: training_data})\n\nsess.close()\n\n</code></pre>", "body_text": "Note: The test code below was cobbled together from various un-credited sources.\nThe example crashes, during the building of the reduce_mean.   It fails with quite a large stack, maybe some sort of stack overflow?  Without the last two 'block's (marked with #*****) the compilation succeeds.\nPerhaps it is as simple as it failing on my laptop (OS/X, 16GB RAM).\nimport tensorflow as tf\nimport numpy as np\n\ndef _get_variable(name, shape, initializer, dtype=tf.float32):\n\n  return tf.get_variable(name,\n                         shape=shape,\n                         initializer=initializer,\n                         dtype=dtype)\ndef inference(x):\n\n  with tf.variable_scope('scale1', use_resource=True):\n    x = conv(x, 7, 2, 64)\n    x = tf.nn.relu(x)\n\n  with tf.variable_scope('max_pool', use_resource=True):\n    x = max_pool(x, ksize=3, stride=2)\n\n  with tf.variable_scope('scale2-1', use_resource=True):\n    x = block(x, 1, 64, 256)\n\n  with tf.variable_scope('scale2-2', use_resource=True):\n    x = block(x, 1, 64, 256)\n\n  with tf.variable_scope('scale2-3', use_resource=True):\n    x = block(x, 1, 64, 256)\n\n\n\n  with tf.variable_scope('scale3-1', use_resource=True):\n    x = block(x, 2, 128, 512)\n\n  with tf.variable_scope('scale3-2', use_resource=True):\n    x = block(x, 1, 128, 512)\n\n  with tf.variable_scope('scale3-3', use_resource=True):\n    x = block(x, 1, 128, 512)\n\n  with tf.variable_scope('scale3-4', use_resource=True):\n    x = block(x, 1, 128, 512)\n\n\n\n  with tf.variable_scope('scale4-1', use_resource=True):\n    x = block(x, 2, 256, 1024)\n\n  with tf.variable_scope('scale4-2', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-3', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-4', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-5', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n  with tf.variable_scope('scale4-6', use_resource=True):\n    x = block(x, 1, 256, 1024)\n\n\n\n  with tf.variable_scope('scale5-1', use_resource=True):\n    x = block(x, 2, 512, 2048)\n\n  with tf.variable_scope('scale5-2', use_resource=True):  #*****\n    x = block(x, 1, 512, 2048)\n\n  with tf.variable_scope('scale5-3', use_resource=True): #*****\n    x = block(x, 1, 512, 2048)\n\n  x = tf.reduce_mean(x, reduction_indices=[1, 2])\n\n  with tf.variable_scope('fc', use_resource=True):\n    x = fc(x, 1000)\n\n  return x\n\n\ndef block(x, first_stride, internal_filters, final_filters):\n  shape_in = x.get_shape()\n\n  shortcut = x\n\n  with tf.variable_scope('a', use_resource=True):\n    x = conv(x, 1, first_stride, internal_filters)\n    x = tf.nn.relu(x)\n\n  with tf.variable_scope('b', use_resource=True):\n    x = conv(x, 3, 1, internal_filters)\n    x = tf.nn.relu(x)\n\n  with tf.variable_scope('c', use_resource=True):\n    x = conv(x, 1, 1, final_filters)\n\n  with tf.variable_scope('shortcut', use_resource=True):\n    pad = int(x.get_shape()[-1] - shape_in[-1])\n    kernel = np.reshape(np.concatenate((np.identity(shape_in[-1], dtype=np.float32),\n                                        np.zeros([pad, shape_in[-1]]))),\n                        [1, 1, shape_in[-1], final_filters])\n\n    shortcut = tf.nn.conv2d(shortcut,\n                            kernel,\n                            [1,first_stride,first_stride,1],\n                            padding='SAME')\n\n  return tf.nn.relu(x + shortcut)\n\n\ndef fc(x, num_units_out):\n  num_units_in = x.get_shape()[1]\n  weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\n\n  weights = _get_variable('weights', shape=[num_units_in, num_units_out],\n                          initializer=weights_initializer)\n  biases = _get_variable('biases', shape=[num_units_out],\n                         initializer=tf.constant_initializer(0.0))\n\n  x = tf.nn.xw_plus_b(x, weights, biases)\n\n  return x\n\ndef conv(x, ksize, stride, filters_out):\n\n  filters_in = x.get_shape()[-1]\n  shape = [ksize, ksize, filters_in, filters_out]\n  initializer = tf.truncated_normal_initializer(stddev=0.1)\n\n  weights = _get_variable('weights', shape=shape, initializer=initializer)\n  return tf.nn.conv2d(x,\n                      weights,\n                      [1, stride, stride, 1],\n                      padding='SAME')\n\n\ndef max_pool(x, ksize=3, stride=2):\n  return tf.nn.max_pool(x,\n                        ksize=[1, ksize, ksize, 1],\n                        strides=[1, stride, stride, 1],\n                        padding='SAME')\n\n\n#\n# Main code\n#\n\nwith tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\n  # Inputs\n  x = tf.placeholder(tf.float32, shape=[2, 224, 224, 4])\n\n  # Inference\n  logits = inference(x)\n\nsess = tf.InteractiveSession()\n\nsess.run(tf.global_variables_initializer())\n\ntraining_data = np.zeros([2, 224, 224, 4]);\n\nsess.run(logits, feed_dict={x: training_data})\n\nsess.close()", "body": "Note: The test code below was cobbled together from various un-credited sources.\r\n\r\nThe example crashes, during the building of the reduce_mean.   It fails with quite a large stack, maybe some sort of stack overflow?  Without the last two 'block's (marked with #*****) the compilation succeeds.\r\n\r\nPerhaps it is as simple as it failing on my laptop (OS/X, 16GB RAM).\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef _get_variable(name, shape, initializer, dtype=tf.float32):\r\n\r\n  return tf.get_variable(name,\r\n                         shape=shape,\r\n                         initializer=initializer,\r\n                         dtype=dtype)\r\ndef inference(x):\r\n\r\n  with tf.variable_scope('scale1', use_resource=True):\r\n    x = conv(x, 7, 2, 64)\r\n    x = tf.nn.relu(x)\r\n\r\n  with tf.variable_scope('max_pool', use_resource=True):\r\n    x = max_pool(x, ksize=3, stride=2)\r\n\r\n  with tf.variable_scope('scale2-1', use_resource=True):\r\n    x = block(x, 1, 64, 256)\r\n\r\n  with tf.variable_scope('scale2-2', use_resource=True):\r\n    x = block(x, 1, 64, 256)\r\n\r\n  with tf.variable_scope('scale2-3', use_resource=True):\r\n    x = block(x, 1, 64, 256)\r\n\r\n\r\n\r\n  with tf.variable_scope('scale3-1', use_resource=True):\r\n    x = block(x, 2, 128, 512)\r\n\r\n  with tf.variable_scope('scale3-2', use_resource=True):\r\n    x = block(x, 1, 128, 512)\r\n\r\n  with tf.variable_scope('scale3-3', use_resource=True):\r\n    x = block(x, 1, 128, 512)\r\n\r\n  with tf.variable_scope('scale3-4', use_resource=True):\r\n    x = block(x, 1, 128, 512)\r\n\r\n\r\n\r\n  with tf.variable_scope('scale4-1', use_resource=True):\r\n    x = block(x, 2, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-2', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-3', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-4', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-5', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n  with tf.variable_scope('scale4-6', use_resource=True):\r\n    x = block(x, 1, 256, 1024)\r\n\r\n\r\n\r\n  with tf.variable_scope('scale5-1', use_resource=True):\r\n    x = block(x, 2, 512, 2048)\r\n\r\n  with tf.variable_scope('scale5-2', use_resource=True):  #*****\r\n    x = block(x, 1, 512, 2048)\r\n\r\n  with tf.variable_scope('scale5-3', use_resource=True): #*****\r\n    x = block(x, 1, 512, 2048)\r\n\r\n  x = tf.reduce_mean(x, reduction_indices=[1, 2])\r\n\r\n  with tf.variable_scope('fc', use_resource=True):\r\n    x = fc(x, 1000)\r\n\r\n  return x\r\n\r\n\r\ndef block(x, first_stride, internal_filters, final_filters):\r\n  shape_in = x.get_shape()\r\n\r\n  shortcut = x\r\n\r\n  with tf.variable_scope('a', use_resource=True):\r\n    x = conv(x, 1, first_stride, internal_filters)\r\n    x = tf.nn.relu(x)\r\n\r\n  with tf.variable_scope('b', use_resource=True):\r\n    x = conv(x, 3, 1, internal_filters)\r\n    x = tf.nn.relu(x)\r\n\r\n  with tf.variable_scope('c', use_resource=True):\r\n    x = conv(x, 1, 1, final_filters)\r\n\r\n  with tf.variable_scope('shortcut', use_resource=True):\r\n    pad = int(x.get_shape()[-1] - shape_in[-1])\r\n    kernel = np.reshape(np.concatenate((np.identity(shape_in[-1], dtype=np.float32),\r\n                                        np.zeros([pad, shape_in[-1]]))),\r\n                        [1, 1, shape_in[-1], final_filters])\r\n\r\n    shortcut = tf.nn.conv2d(shortcut,\r\n                            kernel,\r\n                            [1,first_stride,first_stride,1],\r\n                            padding='SAME')\r\n\r\n  return tf.nn.relu(x + shortcut)\r\n\r\n\r\ndef fc(x, num_units_out):\r\n  num_units_in = x.get_shape()[1]\r\n  weights_initializer = tf.truncated_normal_initializer(stddev=0.01)\r\n\r\n  weights = _get_variable('weights', shape=[num_units_in, num_units_out],\r\n                          initializer=weights_initializer)\r\n  biases = _get_variable('biases', shape=[num_units_out],\r\n                         initializer=tf.constant_initializer(0.0))\r\n\r\n  x = tf.nn.xw_plus_b(x, weights, biases)\r\n\r\n  return x\r\n\r\ndef conv(x, ksize, stride, filters_out):\r\n\r\n  filters_in = x.get_shape()[-1]\r\n  shape = [ksize, ksize, filters_in, filters_out]\r\n  initializer = tf.truncated_normal_initializer(stddev=0.1)\r\n\r\n  weights = _get_variable('weights', shape=shape, initializer=initializer)\r\n  return tf.nn.conv2d(x,\r\n                      weights,\r\n                      [1, stride, stride, 1],\r\n                      padding='SAME')\r\n\r\n\r\ndef max_pool(x, ksize=3, stride=2):\r\n  return tf.nn.max_pool(x,\r\n                        ksize=[1, ksize, ksize, 1],\r\n                        strides=[1, stride, stride, 1],\r\n                        padding='SAME')\r\n\r\n\r\n#\r\n# Main code\r\n#\r\n\r\nwith tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\r\n  # Inputs\r\n  x = tf.placeholder(tf.float32, shape=[2, 224, 224, 4])\r\n\r\n  # Inference\r\n  logits = inference(x)\r\n\r\nsess = tf.InteractiveSession()\r\n\r\nsess.run(tf.global_variables_initializer())\r\n\r\ntraining_data = np.zeros([2, 224, 224, 4]);\r\n\r\nsess.run(logits, feed_dict={x: training_data})\r\n\r\nsess.close()\r\n\r\n```"}