{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17555", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17555/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17555/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17555/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17555", "id": 303505050, "node_id": "MDU6SXNzdWUzMDM1MDUwNTA=", "number": 17555, "title": "Nested while_loop does not work for automatic gradient derivation", "user": {"login": "shengc", "id": 940628, "node_id": "MDQ6VXNlcjk0MDYyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/940628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shengc", "html_url": "https://github.com/shengc", "followers_url": "https://api.github.com/users/shengc/followers", "following_url": "https://api.github.com/users/shengc/following{/other_user}", "gists_url": "https://api.github.com/users/shengc/gists{/gist_id}", "starred_url": "https://api.github.com/users/shengc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shengc/subscriptions", "organizations_url": "https://api.github.com/users/shengc/orgs", "repos_url": "https://api.github.com/users/shengc/repos", "events_url": "https://api.github.com/users/shengc/events{/privacy}", "received_events_url": "https://api.github.com/users/shengc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-03-08T14:46:59Z", "updated_at": "2018-03-09T03:58:53Z", "closed_at": "2018-03-09T03:58:52Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes  I have</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: OSX 10.11.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: conda-forge</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0</li>\n<li><strong>Python version</strong>: 3.6.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre>cell_fw <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\ncell_bw <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">inp</span>):\n    (outputs_fw, outputs_bw), _ <span class=\"pl-k\">=</span> \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>inp[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n        <span class=\"pl-k\">return</span> tf.concat([outputs_fw, outputs_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\noutputs <span class=\"pl-k\">=</span> tf.map_fn(fn, (embedded, words_length), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)</pre></div>\n<h3>Describe the problem</h3>\n<p>This problem seems having to do with the nested <code>while_loop</code>. I have a 4-D tensor that I want to run through a bidirectional GRU with, so I used <code>tf.map_fn</code> to map the sub tensors (which are 3-D tensor now) to <code>bidirectional_rnn</code> with the same <code>GRU</code> cells. This was going fine until I asked <code>TensorFlow</code> to generate the automatic gradient descent, which threw error suggesting <code>while_loop</code> cannot be inside another <code>while_loop</code>.</p>\n<p><strong>Reproducible code is here</strong>:</p>\n<div class=\"highlight highlight-source-python\"><pre>num_words <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\nembedding_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nhidden_dim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\nnum_class <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20</span>\n\nwords <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>words<span class=\"pl-pds\">'</span></span>)\nwords_length <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_length<span class=\"pl-pds\">'</span></span>)\nsentences_length <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sentences_length<span class=\"pl-pds\">'</span></span>)\nlabels <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>labels<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>embeddings<span class=\"pl-pds\">'</span></span>):\n    embedding <span class=\"pl-k\">=</span> \\\n        tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>parameter<span class=\"pl-pds\">'</span></span>, \n                        <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(num_words, embedding_dim), \n                        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    embedded  <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(embedding, words, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>lookup<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_lstm<span class=\"pl-pds\">'</span></span>):\n    cell_fw <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\n    cell_bw <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">fn</span>(<span class=\"pl-smi\">inp</span>):\n        (outputs_fw, outputs_bw), _ <span class=\"pl-k\">=</span> \\\n            tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>inp[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n        <span class=\"pl-k\">return</span> tf.concat([outputs_fw, outputs_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n    outputs <span class=\"pl-k\">=</span> tf.map_fn(fn, (embedded, words_length), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_attention<span class=\"pl-pds\">'</span></span>):\n    hidden <span class=\"pl-k\">=</span> tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>hidden_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh)\n    attention <span class=\"pl-k\">=</span> tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n    attention <span class=\"pl-k\">=</span> tf.transpose(tf.nn.softmax(tf.transpose(attention, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>])), <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>])\noutputs <span class=\"pl-k\">=</span> tf.reduce_sum(outputs <span class=\"pl-k\">*</span> attention, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sentence_lstm<span class=\"pl-pds\">'</span></span>):\n    cell_fw <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\n    cell_bw <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>hidden_dim)\n    (outputs_fw, outputs_bw), _ <span class=\"pl-k\">=</span> \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, outputs, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sentences_length, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\noutputs <span class=\"pl-k\">=</span> tf.concat([outputs_fw, outputs_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>sentence_attention<span class=\"pl-pds\">'</span></span>):\n    hidden <span class=\"pl-k\">=</span> tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>hidden_dim <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.nn.tanh)\n    attention <span class=\"pl-k\">=</span> tf.layers.dense(hidden, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n    attention <span class=\"pl-k\">=</span> tf.transpose(tf.nn.softmax(tf.transpose(attention, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])), <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>])\noutputs <span class=\"pl-k\">=</span> tf.reduce_sum(outputs <span class=\"pl-k\">*</span> attention, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\nlogits <span class=\"pl-k\">=</span> tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>num_class, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\nloss <span class=\"pl-k\">=</span> tf.reduce_sum(tf.one_hot(labels, num_class) <span class=\"pl-k\">*</span> tf.nn.softmax(logits), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>)\ntraining_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>).minimize(loss)</pre></div>\n<p><strong>Error</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">INFO</span>:tensorflow:Cannot use <span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">as</span> <span class=\"pl-c1\">input</span> to <span class=\"pl-s\"><span class=\"pl-pds\">'</span>gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc<span class=\"pl-pds\">'</span></span> because <span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">is</span> <span class=\"pl-k\">in</span> a <span class=\"pl-k\">while</span> loop.\n\ngradients<span class=\"pl-k\">/</span>words_lstm<span class=\"pl-k\">/</span><span class=\"pl-c1\">map</span><span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>bidirectional_rnn<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>TensorArrayWrite<span class=\"pl-k\">/</span>TensorArrayWriteV3_grad<span class=\"pl-k\">/</span>TensorArrayReadV3<span class=\"pl-k\">/</span>f_acc <span class=\"pl-k\">while</span> context: <span class=\"pl-c1\">None</span>\nwords_lstm<span class=\"pl-k\">/</span><span class=\"pl-c1\">map</span><span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>bidirectional_rnn<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span>strided_slice_1 <span class=\"pl-k\">while</span> context: words_lstm<span class=\"pl-k\">/</span><span class=\"pl-c1\">map</span><span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>while_context\n\nTraceback <span class=\"pl-k\">for</span> gradients<span class=\"pl-k\">/</span>words_lstm<span class=\"pl-k\">/</span><span class=\"pl-c1\">map</span><span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>bidirectional_rnn<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>TensorArrayWrite<span class=\"pl-k\">/</span>TensorArrayWriteV3_grad<span class=\"pl-k\">/</span>TensorArrayReadV3<span class=\"pl-k\">/</span>f_acc:\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/bin/ipython<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">6</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    sys.exit(IPython.start_ipython())\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">125</span>, <span class=\"pl-k\">in</span> start_ipython\n    <span class=\"pl-k\">return</span> launch_new_instance(<span class=\"pl-v\">argv</span><span class=\"pl-k\">=</span>argv, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">658</span>, <span class=\"pl-k\">in</span> launch_instance\n    app.start()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">356</span>, <span class=\"pl-k\">in</span> start\n    <span class=\"pl-c1\">self</span>.shell.mainloop()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">480</span>, <span class=\"pl-k\">in</span> mainloop\n    <span class=\"pl-c1\">self</span>.interact()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">471</span>, <span class=\"pl-k\">in</span> interact\n    <span class=\"pl-c1\">self</span>.run_cell(code, <span class=\"pl-v\">store_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2728</span>, <span class=\"pl-k\">in</span> run_cell\n    interactivity<span class=\"pl-k\">=</span>interactivity, compiler<span class=\"pl-k\">=</span>compiler, result<span class=\"pl-k\">=</span>result)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2856</span>, <span class=\"pl-k\">in</span> run_ast_nodes\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.run_code(code, result):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2910</span>, <span class=\"pl-k\">in</span> run_code\n    <span class=\"pl-c1\">exec</span>(code_obj, <span class=\"pl-c1\">self</span>.user_global_ns, <span class=\"pl-c1\">self</span>.user_ns)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;ipython-input-3-cda6375ef3e5&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    get_ipython().run_line_magic(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>paste<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2095</span>, <span class=\"pl-k\">in</span> run_line_magic\n    result <span class=\"pl-k\">=</span> fn(<span class=\"pl-k\">*</span>args,<span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;decorator-gen-27&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2</span>, <span class=\"pl-k\">in</span> paste\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">187</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-k\">lambda</span>&gt;\n    call <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">f</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">a</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">k</span>: f(<span class=\"pl-k\">*</span>a, <span class=\"pl-k\">**</span>k)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">199</span>, <span class=\"pl-k\">in</span> paste\n    <span class=\"pl-c1\">self</span>.store_or_execute(block, name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">57</span>, <span class=\"pl-k\">in</span> store_or_execute\n    <span class=\"pl-c1\">self</span>.shell.run_cell(b)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2728</span>, <span class=\"pl-k\">in</span> run_cell\n    interactivity<span class=\"pl-k\">=</span>interactivity, compiler<span class=\"pl-k\">=</span>compiler, result<span class=\"pl-k\">=</span>result)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2850</span>, <span class=\"pl-k\">in</span> run_ast_nodes\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.run_code(code, result):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2910</span>, <span class=\"pl-k\">in</span> run_code\n    <span class=\"pl-c1\">exec</span>(code_obj, <span class=\"pl-c1\">self</span>.user_global_ns, <span class=\"pl-c1\">self</span>.user_ns)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;ipython-input-3-0176473fb528&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">40</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    training_op <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>).minimize(loss)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">355</span>, <span class=\"pl-k\">in</span> minimize\n    grad_loss<span class=\"pl-k\">=</span>grad_loss)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">456</span>, <span class=\"pl-k\">in</span> compute_gradients\n    colocate_gradients_with_ops<span class=\"pl-k\">=</span>colocate_gradients_with_ops)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">609</span>, <span class=\"pl-k\">in</span> gradients\n    grad_scope, op, func_call, <span class=\"pl-k\">lambda</span>: grad_fn(op, <span class=\"pl-k\">*</span>out_grads))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">375</span>, <span class=\"pl-k\">in</span> _MaybeCompile\n    <span class=\"pl-k\">return</span> grad_fn()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Exit early</span>\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">609</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-k\">lambda</span>&gt;\n    grad_scope, op, func_call, <span class=\"pl-k\">lambda</span>: grad_fn(op, <span class=\"pl-k\">*</span>out_grads))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">131</span>, <span class=\"pl-k\">in</span> _TensorArrayWriteGrad\n    grad <span class=\"pl-k\">=</span> g.read(index)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">859</span>, <span class=\"pl-k\">in</span> read\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._implementation.read(index, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">259</span>, <span class=\"pl-k\">in</span> read\n    name<span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">4498</span>, <span class=\"pl-k\">in</span> _tensor_array_read_v3\n    dtype<span class=\"pl-k\">=</span>dtype, name<span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">787</span>, <span class=\"pl-k\">in</span> _apply_op_helper\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">3160</span>, <span class=\"pl-k\">in</span> create_op\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1674</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">self</span>._control_flow_context.AddOp(<span class=\"pl-c1\">self</span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2251</span>, <span class=\"pl-k\">in</span> AddOp\n    <span class=\"pl-c1\">self</span>._AddOpInternal(op)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2274</span>, <span class=\"pl-k\">in</span> _AddOpInternal\n    real_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.AddValue(x)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2207</span>, <span class=\"pl-k\">in</span> AddValue\n    real_val <span class=\"pl-k\">=</span> grad_ctxt.grad_state.GetRealValue(val)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1050</span>, <span class=\"pl-k\">in</span> GetRealValue\n    history_value <span class=\"pl-k\">=</span> cur_grad_state.AddForwardAccumulator(cur_value)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">908</span>, <span class=\"pl-k\">in</span> AddForwardAccumulator\n    name<span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>f_acc<span class=\"pl-pds\">\"</span></span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">3578</span>, <span class=\"pl-k\">in</span> _stack_v2\n    stack_name<span class=\"pl-k\">=</span>stack_name, name<span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">787</span>, <span class=\"pl-k\">in</span> _apply_op_helper\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">3160</span>, <span class=\"pl-k\">in</span> create_op\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1625</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">self</span>._traceback <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._graph._extract_stack()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=protected-access</span>\n\nTraceback <span class=\"pl-k\">for</span> words_lstm<span class=\"pl-k\">/</span><span class=\"pl-c1\">map</span><span class=\"pl-k\">/</span><span class=\"pl-k\">while</span><span class=\"pl-k\">/</span>bidirectional_rnn<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span>fw<span class=\"pl-k\">/</span>strided_slice_1:\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/bin/ipython<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">6</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    sys.exit(IPython.start_ipython())\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">125</span>, <span class=\"pl-k\">in</span> start_ipython\n    <span class=\"pl-k\">return</span> launch_new_instance(<span class=\"pl-v\">argv</span><span class=\"pl-k\">=</span>argv, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">658</span>, <span class=\"pl-k\">in</span> launch_instance\n    app.start()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">356</span>, <span class=\"pl-k\">in</span> start\n    <span class=\"pl-c1\">self</span>.shell.mainloop()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">480</span>, <span class=\"pl-k\">in</span> mainloop\n    <span class=\"pl-c1\">self</span>.interact()\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">471</span>, <span class=\"pl-k\">in</span> interact\n    <span class=\"pl-c1\">self</span>.run_cell(code, <span class=\"pl-v\">store_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2728</span>, <span class=\"pl-k\">in</span> run_cell\n    interactivity<span class=\"pl-k\">=</span>interactivity, compiler<span class=\"pl-k\">=</span>compiler, result<span class=\"pl-k\">=</span>result)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2856</span>, <span class=\"pl-k\">in</span> run_ast_nodes\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.run_code(code, result):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2910</span>, <span class=\"pl-k\">in</span> run_code\n    <span class=\"pl-c1\">exec</span>(code_obj, <span class=\"pl-c1\">self</span>.user_global_ns, <span class=\"pl-c1\">self</span>.user_ns)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;ipython-input-3-cda6375ef3e5&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    get_ipython().run_line_magic(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>paste<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2095</span>, <span class=\"pl-k\">in</span> run_line_magic\n    result <span class=\"pl-k\">=</span> fn(<span class=\"pl-k\">*</span>args,<span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;decorator-gen-27&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2</span>, <span class=\"pl-k\">in</span> paste\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">187</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-k\">lambda</span>&gt;\n    call <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">f</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">a</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">k</span>: f(<span class=\"pl-k\">*</span>a, <span class=\"pl-k\">**</span>k)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">199</span>, <span class=\"pl-k\">in</span> paste\n    <span class=\"pl-c1\">self</span>.store_or_execute(block, name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">57</span>, <span class=\"pl-k\">in</span> store_or_execute\n    <span class=\"pl-c1\">self</span>.shell.run_cell(b)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2728</span>, <span class=\"pl-k\">in</span> run_cell\n    interactivity<span class=\"pl-k\">=</span>interactivity, compiler<span class=\"pl-k\">=</span>compiler, result<span class=\"pl-k\">=</span>result)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2850</span>, <span class=\"pl-k\">in</span> run_ast_nodes\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.run_code(code, result):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2910</span>, <span class=\"pl-k\">in</span> run_code\n    <span class=\"pl-c1\">exec</span>(code_obj, <span class=\"pl-c1\">self</span>.user_global_ns, <span class=\"pl-c1\">self</span>.user_ns)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;ipython-input-3-0176473fb528&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">22</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    outputs <span class=\"pl-k\">=</span> tf.map_fn(fn, (embedded, words_length), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">409</span>, <span class=\"pl-k\">in</span> map_fn\n    swap_memory<span class=\"pl-k\">=</span>swap_memory)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2934</span>, <span class=\"pl-k\">in</span> while_loop\n    result <span class=\"pl-k\">=</span> loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2720</span>, <span class=\"pl-k\">in</span> BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">2662</span>, <span class=\"pl-k\">in</span> _BuildLoop\n    body_result <span class=\"pl-k\">=</span> body(<span class=\"pl-k\">*</span>packed_vars_for_body)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">399</span>, <span class=\"pl-k\">in</span> compute\n    packed_fn_values <span class=\"pl-k\">=</span> fn(packed_values)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>&lt;ipython-input-3-0176473fb528&gt;<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">20</span>, <span class=\"pl-k\">in</span> fn\n    (outputs_fw, outputs_bw), _ <span class=\"pl-k\">=</span>             tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>inp[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">414</span>, <span class=\"pl-k\">in</span> bidirectional_dynamic_rnn\n    time_major<span class=\"pl-k\">=</span>time_major, scope<span class=\"pl-k\">=</span>fw_scope)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">629</span>, <span class=\"pl-k\">in</span> dynamic_rnn\n    dtype<span class=\"pl-k\">=</span>dtype)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">688</span>, <span class=\"pl-k\">in</span> _dynamic_rnn_loop\n    time_steps <span class=\"pl-k\">=</span> input_shape[<span class=\"pl-c1\">0</span>]\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">573</span>, <span class=\"pl-k\">in</span> _slice_helper\n    name<span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">737</span>, <span class=\"pl-k\">in</span> strided_slice\n    shrink_axis_mask<span class=\"pl-k\">=</span>shrink_axis_mask)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">5501</span>, <span class=\"pl-k\">in</span> strided_slice\n    name<span class=\"pl-k\">=</span>name)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">787</span>, <span class=\"pl-k\">in</span> _apply_op_helper\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">3160</span>, <span class=\"pl-k\">in</span> create_op\n    op_def<span class=\"pl-k\">=</span>op_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">1625</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    <span class=\"pl-c1\">self</span>._traceback <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._graph._extract_stack()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=protected-access</span>\n\n\n<span class=\"pl-ii\">--------------------------------------------------------------------------</span><span class=\"pl-k\">-</span>\n<span class=\"pl-c1\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>gradients_impl.py <span class=\"pl-k\">in</span> _MaybeCompile(scope, op, func, grad_fn)\n    <span class=\"pl-c1\">369</span>     <span class=\"pl-k\">try</span>:\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">370</span>       xla_compile <span class=\"pl-k\">=</span> op.get_attr(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_XlaCompile<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">371</span>       xla_separate_compiled_gradients <span class=\"pl-k\">=</span> op.get_attr(\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>ops.py <span class=\"pl-k\">in</span> get_attr(<span class=\"pl-c1\">self</span>, name)\n   <span class=\"pl-c1\">2172</span>         <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">2173</span>             <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>No attr named '<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>' in <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">self</span>._node_def))\n   <span class=\"pl-c1\">2174</span>       <span class=\"pl-v\">x</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._node_def.attr[name]\n\n<span class=\"pl-c1\">ValueError</span>: No attr named <span class=\"pl-s\"><span class=\"pl-pds\">'</span>_XlaCompile<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">in</span> name: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3<span class=\"pl-pds\">\"</span></span>\nop: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>TensorArrayWriteV3<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c1\">input</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c1\">input</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_1<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c1\">input</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>words_lstm/map/while/bidirectional_rnn/fw/fw/while/Select<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c1\">input</span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_2<span class=\"pl-pds\">\"</span></span>\nattr {\n  key: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>T<span class=\"pl-pds\">\"</span></span>\n  value {\n    <span class=\"pl-c1\">type</span>: <span class=\"pl-c1\">DT_FLOAT</span>\n  }\n}\nattr {\n  key: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_class<span class=\"pl-pds\">\"</span></span>\n  value {\n    <span class=\"pl-c1\">list</span> {\n      s: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loc:@words_lstm/map/while/bidirectional_rnn/fw/fw/while/gru_cell/add<span class=\"pl-pds\">\"</span></span>\n    }\n  }\n}\n\n\nDuring handling of the above exception, another exception occurred:\n\n<span class=\"pl-c1\">ValueError</span>                                Traceback (most recent call last)\n<span class=\"pl-k\">&lt;</span>ipython<span class=\"pl-k\">-</span><span class=\"pl-c1\">input</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span><span class=\"pl-k\">-</span><span class=\"pl-ii\">0176473fb528</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>()\n     <span class=\"pl-c1\">38</span> <span class=\"pl-v\">logits</span> <span class=\"pl-k\">=</span> tf.layers.dense(outputs, <span class=\"pl-v\">units</span><span class=\"pl-k\">=</span>num_class, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n     <span class=\"pl-c1\">39</span> <span class=\"pl-v\">loss</span> <span class=\"pl-k\">=</span> tf.reduce_sum(tf.one_hot(labels, num_class) <span class=\"pl-k\">*</span> tf.nn.softmax(logits), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-ii\">--</span><span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">40</span> <span class=\"pl-v\">training_op</span> <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>).minimize(loss)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>training<span class=\"pl-k\">/</span>optimizer.py <span class=\"pl-k\">in</span> minimize(<span class=\"pl-c1\">self</span>, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    <span class=\"pl-c1\">353</span>         <span class=\"pl-v\">aggregation_method</span><span class=\"pl-k\">=</span>aggregation_method,\n    <span class=\"pl-c1\">354</span>         <span class=\"pl-v\">colocate_gradients_with_ops</span><span class=\"pl-k\">=</span>colocate_gradients_with_ops,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">355</span>         <span class=\"pl-v\">grad_loss</span><span class=\"pl-k\">=</span>grad_loss)\n    <span class=\"pl-c1\">356</span>\n    <span class=\"pl-c1\">357</span>     vars_with_grad <span class=\"pl-k\">=</span> [v <span class=\"pl-k\">for</span> g, v <span class=\"pl-k\">in</span> grads_and_vars <span class=\"pl-k\">if</span> g <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>]\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>training<span class=\"pl-k\">/</span>optimizer.py <span class=\"pl-k\">in</span> compute_gradients(<span class=\"pl-c1\">self</span>, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\n    <span class=\"pl-c1\">454</span>         gate_gradients<span class=\"pl-k\">=</span>(gate_gradients <span class=\"pl-k\">==</span> Optimizer.<span class=\"pl-c1\">GATE_OP</span>),\n    <span class=\"pl-c1\">455</span>         aggregation_method<span class=\"pl-k\">=</span>aggregation_method,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">456</span>         colocate_gradients_with_ops<span class=\"pl-k\">=</span>colocate_gradients_with_ops)\n    <span class=\"pl-c1\">457</span>     <span class=\"pl-k\">if</span> gate_gradients <span class=\"pl-k\">==</span> Optimizer.<span class=\"pl-c1\">GATE_GRAPH</span>:\n    <span class=\"pl-c1\">458</span>       grads <span class=\"pl-k\">=</span> control_flow_ops.tuple(grads)\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>gradients_impl.py <span class=\"pl-k\">in</span> gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\n    <span class=\"pl-c1\">607</span>                 <span class=\"pl-c\"><span class=\"pl-c\">#</span> functions.</span>\n    <span class=\"pl-c1\">608</span>                 in_grads <span class=\"pl-k\">=</span> _MaybeCompile(\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">609</span>                     grad_scope, op, func_call, <span class=\"pl-k\">lambda</span>: grad_fn(op, <span class=\"pl-k\">*</span>out_grads))\n    <span class=\"pl-c1\">610</span>               <span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">611</span>                 <span class=\"pl-c\"><span class=\"pl-c\">#</span> For function call ops, we add a 'SymbolicGradient'</span>\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>gradients_impl.py <span class=\"pl-k\">in</span> _MaybeCompile(scope, op, func, grad_fn)\n    <span class=\"pl-c1\">373</span>       xla_scope <span class=\"pl-k\">=</span> op.get_attr(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_XlaScope<span class=\"pl-pds\">\"</span></span>).decode()\n    <span class=\"pl-c1\">374</span>     <span class=\"pl-k\">except</span> <span class=\"pl-c1\">ValueError</span>:\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">375</span>       <span class=\"pl-k\">return</span> grad_fn()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Exit early</span>\n    <span class=\"pl-c1\">376</span>\n    <span class=\"pl-c1\">377</span>   <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> xla_compile:\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>gradients_impl.py <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span><span class=\"pl-k\">lambda</span>&gt;()\n    <span class=\"pl-c1\">607</span>                 <span class=\"pl-c\"><span class=\"pl-c\">#</span> functions.</span>\n    <span class=\"pl-c1\">608</span>                 in_grads <span class=\"pl-k\">=</span> _MaybeCompile(\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">609</span>                     grad_scope, op, func_call, <span class=\"pl-k\">lambda</span>: grad_fn(op, <span class=\"pl-k\">*</span>out_grads))\n    <span class=\"pl-c1\">610</span>               <span class=\"pl-k\">else</span>:\n    <span class=\"pl-c1\">611</span>                 <span class=\"pl-c\"><span class=\"pl-c\">#</span> For function call ops, we add a 'SymbolicGradient'</span>\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>tensor_array_grad.py <span class=\"pl-k\">in</span> _TensorArrayWriteGrad(op, flow)\n    <span class=\"pl-c1\">129</span>                                     colocate_with_first_write_call<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-c1\">130</span>        .grad(<span class=\"pl-v\">source</span><span class=\"pl-k\">=</span>grad_source, <span class=\"pl-v\">flow</span><span class=\"pl-k\">=</span>flow))\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">131</span>   grad <span class=\"pl-k\">=</span> g.read(index)\n    <span class=\"pl-c1\">132</span>   <span class=\"pl-k\">return</span> [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>, grad, flow]\n    <span class=\"pl-c1\">133</span>\n\n<span class=\"pl-k\">~</span><span class=\"pl-k\">/</span>anaconda<span class=\"pl-k\">/</span>envs<span class=\"pl-k\">/</span>py36<span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>tensor_array_ops.py <span class=\"pl-k\">in</span> read(<span class=\"pl-c1\">self</span>, index, name)\n    <span class=\"pl-c1\">857</span>       The tensor at index <span class=\"pl-bu\">`index`</span>.\n    <span class=\"pl-c1\">858</span>     <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">--&gt; 859     return self._implementation.read(index, name=name)</span>\n<span class=\"pl-s\">    860</span>\n<span class=\"pl-s\">    861   @tf_should_use.should_use_result</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)</span>\n<span class=\"pl-s\">    257         flow_in=self._flow,</span>\n<span class=\"pl-s\">    258         dtype=self._dtype,</span>\n<span class=\"pl-s\">--&gt; 259         name=name)</span>\n<span class=\"pl-s\">    260     if self._element_shape:</span>\n<span class=\"pl-s\">    261       value.set_shape(self._element_shape[0].dims)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _tensor_array_read_v3(handle, index, flow_in, dtype, name)</span>\n<span class=\"pl-s\">   4496     _, _, _op = _op_def_lib._apply_op_helper(</span>\n<span class=\"pl-s\">   4497         \"TensorArrayReadV3\", handle=handle, index=index, flow_in=flow_in,</span>\n<span class=\"pl-s\">-&gt; 4498         dtype=dtype, name=name)</span>\n<span class=\"pl-s\">   4499     _result = _op.outputs[:]</span>\n<span class=\"pl-s\">   4500     _inputs_flat = _op.inputs</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)</span>\n<span class=\"pl-s\">    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,</span>\n<span class=\"pl-s\">    786                          input_types=input_types, attrs=attr_protos,</span>\n<span class=\"pl-s\">--&gt; 787                          op_def=op_def)</span>\n<span class=\"pl-s\">    788       return output_structure, op_def.is_stateful, op</span>\n<span class=\"pl-s\">    789</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)</span>\n<span class=\"pl-s\">   3158         input_types=input_types,</span>\n<span class=\"pl-s\">   3159         original_op=self._default_original_op,</span>\n<span class=\"pl-s\">-&gt; 3160         op_def=op_def)</span>\n<span class=\"pl-s\">   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,</span>\n<span class=\"pl-s\">   3162                            compute_device=compute_device)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)</span>\n<span class=\"pl-s\">   1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)</span>\n<span class=\"pl-s\">   1673     if self._control_flow_context is not None:</span>\n<span class=\"pl-s\">-&gt; 1674       self._control_flow_context.AddOp(self)</span>\n<span class=\"pl-s\">   1675     self._recompute_node_def()</span>\n<span class=\"pl-s\">   1676</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddOp(self, op)</span>\n<span class=\"pl-s\">   2249             op_input_ctxt._AddOpInternal(op)</span>\n<span class=\"pl-s\">   2250             return</span>\n<span class=\"pl-s\">-&gt; 2251     self._AddOpInternal(op)</span>\n<span class=\"pl-s\">   2252</span>\n<span class=\"pl-s\">   2253   def _AddOpInternal(self, op):</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _AddOpInternal(self, op)</span>\n<span class=\"pl-s\">   2272       for index in range(len(op.inputs)):</span>\n<span class=\"pl-s\">   2273         x = op.inputs[index]</span>\n<span class=\"pl-s\">-&gt; 2274         real_x = self.AddValue(x)</span>\n<span class=\"pl-s\">   2275         if real_x != x:</span>\n<span class=\"pl-s\">   2276           op._update_input(index, real_x)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddValue(self, val)</span>\n<span class=\"pl-s\">   2205               forward_ctxt = forward_ctxt.GetWhileContext()</span>\n<span class=\"pl-s\">   2206           if forward_ctxt == grad_ctxt.grad_state.forward_context:</span>\n<span class=\"pl-s\">-&gt; 2207             real_val = grad_ctxt.grad_state.GetRealValue(val)</span>\n<span class=\"pl-s\">   2208             self._external_values[val.name] = real_val</span>\n<span class=\"pl-s\">   2209             return real_val</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in GetRealValue(self, value)</span>\n<span class=\"pl-s\">   1048           # Record the history of this value in forward_ctxt.</span>\n<span class=\"pl-s\">   1049           self._grad_context.Exit()</span>\n<span class=\"pl-s\">-&gt; 1050           history_value = cur_grad_state.AddForwardAccumulator(cur_value)</span>\n<span class=\"pl-s\">   1051           self._grad_context.Enter()</span>\n<span class=\"pl-s\">   1052           break</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddForwardAccumulator(self, value, dead_branch)</span>\n<span class=\"pl-s\">    906             max_size=maximum_iterations,</span>\n<span class=\"pl-s\">    907             elem_type=value.dtype.base_dtype,</span>\n<span class=\"pl-s\">--&gt; 908             name=\"f_acc\")</span>\n<span class=\"pl-s\">    909         # pylint: enable=protected-access</span>\n<span class=\"pl-s\">    910       if curr_ctxt: curr_ctxt.Exit()</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _stack_v2(max_size, elem_type, stack_name, name)</span>\n<span class=\"pl-s\">   3576     _, _, _op = _op_def_lib._apply_op_helper(</span>\n<span class=\"pl-s\">   3577         \"StackV2\", max_size=max_size, elem_type=elem_type,</span>\n<span class=\"pl-s\">-&gt; 3578         stack_name=stack_name, name=name)</span>\n<span class=\"pl-s\">   3579     _result = _op.outputs[:]</span>\n<span class=\"pl-s\">   3580     _inputs_flat = _op.inputs</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)</span>\n<span class=\"pl-s\">    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,</span>\n<span class=\"pl-s\">    786                          input_types=input_types, attrs=attr_protos,</span>\n<span class=\"pl-s\">--&gt; 787                          op_def=op_def)</span>\n<span class=\"pl-s\">    788       return output_structure, op_def.is_stateful, op</span>\n<span class=\"pl-s\">    789</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)</span>\n<span class=\"pl-s\">   3158         input_types=input_types,</span>\n<span class=\"pl-s\">   3159         original_op=self._default_original_op,</span>\n<span class=\"pl-s\">-&gt; 3160         op_def=op_def)</span>\n<span class=\"pl-s\">   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,</span>\n<span class=\"pl-s\">   3162                            compute_device=compute_device)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)</span>\n<span class=\"pl-s\">   1670     self._control_flow_context = g._get_control_flow_context()  # pylint: disable=protected-access</span>\n<span class=\"pl-s\">   1671     for input_tensor in self.inputs:</span>\n<span class=\"pl-s\">-&gt; 1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)</span>\n<span class=\"pl-s\">   1673     if self._control_flow_context is not None:</span>\n<span class=\"pl-s\">   1674       self._control_flow_context.AddOp(self)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py in CheckInputFromValidContext(op, input_op)</span>\n<span class=\"pl-s\">    198         input_op.name, \"\".join(traceback.format_list(input_op.traceback)))</span>\n<span class=\"pl-s\">    199     logging.info(log_msg)</span>\n<span class=\"pl-s\">--&gt; 200     raise ValueError(error_msg + \" See info log for more details.\")</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">ValueError: Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop. See info log for more details.</span></pre></div>\n<h3>Source code / logs</h3>\n<p>source code is here,<br>\n<a href=\"https://github.com/shengc/tf-han/blob/master/hierarchical-attention-network.ipynb\">https://github.com/shengc/tf-han/blob/master/hierarchical-attention-network.ipynb</a><br>\ncalling <code>HierarchicalAttentionNetwork()._make_graph_batch(graph)</code> will produce the above error</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes  I have\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OSX 10.11.6\nTensorFlow installed from (source or binary): conda-forge\nTensorFlow version (use command below): 1.5.0\nPython version: 3.6.0\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:\n\ncell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\ncell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\ndef fn(inp):\n    (outputs_fw, outputs_bw), _ = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\n        return tf.concat([outputs_fw, outputs_bw], axis=2)\noutputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\nDescribe the problem\nThis problem seems having to do with the nested while_loop. I have a 4-D tensor that I want to run through a bidirectional GRU with, so I used tf.map_fn to map the sub tensors (which are 3-D tensor now) to bidirectional_rnn with the same GRU cells. This was going fine until I asked TensorFlow to generate the automatic gradient descent, which threw error suggesting while_loop cannot be inside another while_loop.\nReproducible code is here:\nnum_words = 1000\nembedding_dim = 100\nhidden_dim = 100\nnum_class = 20\n\nwords = tf.placeholder(tf.int32, [None, None, None], name='words')\nwords_length = tf.placeholder(tf.int32, [None, None], name='words_length')\nsentences_length = tf.placeholder(tf.int32, [None], name='sentences_length')\nlabels = tf.placeholder(tf.int32, [None], name='labels')\n\nwith tf.variable_scope('embeddings'):\n    embedding = \\\n        tf.get_variable('parameter', \n                        shape=(num_words, embedding_dim), \n                        dtype=tf.float32, trainable=True)\n    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')\nwith tf.variable_scope('words_lstm'):\n    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\n    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\n    def fn(inp):\n        (outputs_fw, outputs_bw), _ = \\\n            tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\n        return tf.concat([outputs_fw, outputs_bw], axis=2)\n    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\nwith tf.variable_scope('words_attention'):\n    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\n    attention = tf.layers.dense(outputs, units=1, activation=None)\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 1, 3, 2])), perm=[0, 1, 3, 2])\noutputs = tf.reduce_sum(outputs * attention, axis=2)\nwith tf.variable_scope('sentence_lstm'):\n    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\n    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\n    (outputs_fw, outputs_bw), _ = \\\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, outputs, sequence_length=sentences_length, dtype=tf.float32)\noutputs = tf.concat([outputs_fw, outputs_bw], axis=2)\nwith tf.variable_scope('sentence_attention'):\n    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\n    attention = tf.layers.dense(hidden, units=1, activation=None)\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])\noutputs = tf.reduce_sum(outputs * attention, axis=1)\nlogits = tf.layers.dense(outputs, units=num_class, activation=None)\nloss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')\ntraining_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\nError\nINFO:tensorflow:Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop.\n\ngradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc while context: None\nwords_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1 while context: words_lstm/map/while/while_context\n\nTraceback for gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc:\n  File \"/Users/shengc/anaconda/envs/py36/bin/ipython\", line 6, in <module>\n    sys.exit(IPython.start_ipython())\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\n    return launch_new_instance(argv=argv, **kwargs)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\n    self.shell.mainloop()\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 480, in mainloop\n    self.interact()\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 471, in interact\n    self.run_cell(code, store_history=True)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-cda6375ef3e5>\", line 1, in <module>\n    get_ipython().run_line_magic('paste', '')\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\n    result = fn(*args,**kwargs)\n  File \"<decorator-gen-27>\", line 2, in paste\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 199, in paste\n    self.store_or_execute(block, name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 57, in store_or_execute\n    self.shell.run_cell(b)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-0176473fb528>\", line 40, in <module>\n    training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\n    grad_loss=grad_loss)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 131, in _TensorArrayWriteGrad\n    grad = g.read(index)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 859, in read\n    return self._implementation.read(index, name=name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 259, in read\n    name=name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4498, in _tensor_array_read_v3\n    dtype=dtype, name=name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1674, in __init__\n    self._control_flow_context.AddOp(self)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2251, in AddOp\n    self._AddOpInternal(op)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2274, in _AddOpInternal\n    real_x = self.AddValue(x)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2207, in AddValue\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1050, in GetRealValue\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 908, in AddForwardAccumulator\n    name=\"f_acc\")\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3578, in _stack_v2\n    stack_name=stack_name, name=name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nTraceback for words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1:\n  File \"/Users/shengc/anaconda/envs/py36/bin/ipython\", line 6, in <module>\n    sys.exit(IPython.start_ipython())\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\n    return launch_new_instance(argv=argv, **kwargs)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\n    self.shell.mainloop()\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 480, in mainloop\n    self.interact()\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 471, in interact\n    self.run_cell(code, store_history=True)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-cda6375ef3e5>\", line 1, in <module>\n    get_ipython().run_line_magic('paste', '')\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\n    result = fn(*args,**kwargs)\n  File \"<decorator-gen-27>\", line 2, in paste\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 199, in paste\n    self.store_or_execute(block, name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 57, in store_or_execute\n    self.shell.run_cell(b)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-3-0176473fb528>\", line 22, in <module>\n    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 409, in map_fn\n    swap_memory=swap_memory)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\n    body_result = body(*packed_vars_for_body)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 399, in compute\n    packed_fn_values = fn(packed_values)\n  File \"<ipython-input-3-0176473fb528>\", line 20, in fn\n    (outputs_fw, outputs_bw), _ =             tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 414, in bidirectional_dynamic_rnn\n    time_major=time_major, scope=fw_scope)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 629, in dynamic_rnn\n    dtype=dtype)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 688, in _dynamic_rnn_loop\n    time_steps = input_shape[0]\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 573, in _slice_helper\n    name=name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 737, in strided_slice\n    shrink_axis_mask=shrink_axis_mask)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5501, in strided_slice\n    name=name)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\n    369     try:\n--> 370       xla_compile = op.get_attr(\"_XlaCompile\")\n    371       xla_separate_compiled_gradients = op.get_attr(\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\n   2172         raise ValueError(\n-> 2173             \"No attr named '\" + name + \"' in \" + str(self._node_def))\n   2174       x = self._node_def.attr[name]\n\nValueError: No attr named '_XlaCompile' in name: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3\"\nop: \"TensorArrayWriteV3\"\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter\"\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_1\"\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Select\"\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_2\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"_class\"\n  value {\n    list {\n      s: \"loc:@words_lstm/map/while/bidirectional_rnn/fw/fw/while/gru_cell/add\"\n    }\n  }\n}\n\n\nDuring handling of the above exception, another exception occurred:\n\nValueError                                Traceback (most recent call last)\n<ipython-input-3-0176473fb528> in <module>()\n     38 logits = tf.layers.dense(outputs, units=num_class, activation=None)\n     39 loss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')\n---> 40 training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\n    353         aggregation_method=aggregation_method,\n    354         colocate_gradients_with_ops=colocate_gradients_with_ops,\n--> 355         grad_loss=grad_loss)\n    356\n    357     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\n    454         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\n    455         aggregation_method=aggregation_method,\n--> 456         colocate_gradients_with_ops=colocate_gradients_with_ops)\n    457     if gate_gradients == Optimizer.GATE_GRAPH:\n    458       grads = control_flow_ops.tuple(grads)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\n    607                 # functions.\n    608                 in_grads = _MaybeCompile(\n--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n    610               else:\n    611                 # For function call ops, we add a 'SymbolicGradient'\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\n    373       xla_scope = op.get_attr(\"_XlaScope\").decode()\n    374     except ValueError:\n--> 375       return grad_fn()  # Exit early\n    376\n    377   if not xla_compile:\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()\n    607                 # functions.\n    608                 in_grads = _MaybeCompile(\n--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n    610               else:\n    611                 # For function call ops, we add a 'SymbolicGradient'\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py in _TensorArrayWriteGrad(op, flow)\n    129                                     colocate_with_first_write_call=False)\n    130        .grad(source=grad_source, flow=flow))\n--> 131   grad = g.read(index)\n    132   return [None, None, grad, flow]\n    133\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)\n    857       The tensor at index `index`.\n    858     \"\"\"\n--> 859     return self._implementation.read(index, name=name)\n    860\n    861   @tf_should_use.should_use_result\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)\n    257         flow_in=self._flow,\n    258         dtype=self._dtype,\n--> 259         name=name)\n    260     if self._element_shape:\n    261       value.set_shape(self._element_shape[0].dims)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _tensor_array_read_v3(handle, index, flow_in, dtype, name)\n   4496     _, _, _op = _op_def_lib._apply_op_helper(\n   4497         \"TensorArrayReadV3\", handle=handle, index=index, flow_in=flow_in,\n-> 4498         dtype=dtype, name=name)\n   4499     _result = _op.outputs[:]\n   4500     _inputs_flat = _op.inputs\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    786                          input_types=input_types, attrs=attr_protos,\n--> 787                          op_def=op_def)\n    788       return output_structure, op_def.is_stateful, op\n    789\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   3158         input_types=input_types,\n   3159         original_op=self._default_original_op,\n-> 3160         op_def=op_def)\n   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,\n   3162                            compute_device=compute_device)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\n   1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\n   1673     if self._control_flow_context is not None:\n-> 1674       self._control_flow_context.AddOp(self)\n   1675     self._recompute_node_def()\n   1676\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddOp(self, op)\n   2249             op_input_ctxt._AddOpInternal(op)\n   2250             return\n-> 2251     self._AddOpInternal(op)\n   2252\n   2253   def _AddOpInternal(self, op):\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _AddOpInternal(self, op)\n   2272       for index in range(len(op.inputs)):\n   2273         x = op.inputs[index]\n-> 2274         real_x = self.AddValue(x)\n   2275         if real_x != x:\n   2276           op._update_input(index, real_x)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddValue(self, val)\n   2205               forward_ctxt = forward_ctxt.GetWhileContext()\n   2206           if forward_ctxt == grad_ctxt.grad_state.forward_context:\n-> 2207             real_val = grad_ctxt.grad_state.GetRealValue(val)\n   2208             self._external_values[val.name] = real_val\n   2209             return real_val\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in GetRealValue(self, value)\n   1048           # Record the history of this value in forward_ctxt.\n   1049           self._grad_context.Exit()\n-> 1050           history_value = cur_grad_state.AddForwardAccumulator(cur_value)\n   1051           self._grad_context.Enter()\n   1052           break\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddForwardAccumulator(self, value, dead_branch)\n    906             max_size=maximum_iterations,\n    907             elem_type=value.dtype.base_dtype,\n--> 908             name=\"f_acc\")\n    909         # pylint: enable=protected-access\n    910       if curr_ctxt: curr_ctxt.Exit()\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _stack_v2(max_size, elem_type, stack_name, name)\n   3576     _, _, _op = _op_def_lib._apply_op_helper(\n   3577         \"StackV2\", max_size=max_size, elem_type=elem_type,\n-> 3578         stack_name=stack_name, name=name)\n   3579     _result = _op.outputs[:]\n   3580     _inputs_flat = _op.inputs\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n    786                          input_types=input_types, attrs=attr_protos,\n--> 787                          op_def=op_def)\n    788       return output_structure, op_def.is_stateful, op\n    789\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\n   3158         input_types=input_types,\n   3159         original_op=self._default_original_op,\n-> 3160         op_def=op_def)\n   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,\n   3162                            compute_device=compute_device)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\n   1670     self._control_flow_context = g._get_control_flow_context()  # pylint: disable=protected-access\n   1671     for input_tensor in self.inputs:\n-> 1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\n   1673     if self._control_flow_context is not None:\n   1674       self._control_flow_context.AddOp(self)\n\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py in CheckInputFromValidContext(op, input_op)\n    198         input_op.name, \"\".join(traceback.format_list(input_op.traceback)))\n    199     logging.info(log_msg)\n--> 200     raise ValueError(error_msg + \" See info log for more details.\")\n\nValueError: Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop. See info log for more details.\nSource code / logs\nsource code is here,\nhttps://github.com/shengc/tf-han/blob/master/hierarchical-attention-network.ipynb\ncalling HierarchicalAttentionNetwork()._make_graph_batch(graph) will produce the above error", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes  I have\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OSX 10.11.6\r\n- **TensorFlow installed from (source or binary)**: conda-forge\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n```python\r\ncell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\ncell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\ndef fn(inp):\r\n    (outputs_fw, outputs_bw), _ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\r\n        return tf.concat([outputs_fw, outputs_bw], axis=2)\r\noutputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\r\n```\r\n\r\n### Describe the problem\r\nThis problem seems having to do with the nested `while_loop`. I have a 4-D tensor that I want to run through a bidirectional GRU with, so I used `tf.map_fn` to map the sub tensors (which are 3-D tensor now) to `bidirectional_rnn` with the same `GRU` cells. This was going fine until I asked `TensorFlow` to generate the automatic gradient descent, which threw error suggesting `while_loop` cannot be inside another `while_loop`. \r\n\r\n**Reproducible code is here**:\r\n```python\r\nnum_words = 1000\r\nembedding_dim = 100\r\nhidden_dim = 100\r\nnum_class = 20\r\n\r\nwords = tf.placeholder(tf.int32, [None, None, None], name='words')\r\nwords_length = tf.placeholder(tf.int32, [None, None], name='words_length')\r\nsentences_length = tf.placeholder(tf.int32, [None], name='sentences_length')\r\nlabels = tf.placeholder(tf.int32, [None], name='labels')\r\n\r\nwith tf.variable_scope('embeddings'):\r\n    embedding = \\\r\n        tf.get_variable('parameter', \r\n                        shape=(num_words, embedding_dim), \r\n                        dtype=tf.float32, trainable=True)\r\n    embedded  = tf.nn.embedding_lookup(embedding, words, name='lookup')\r\nwith tf.variable_scope('words_lstm'):\r\n    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    def fn(inp):\r\n        (outputs_fw, outputs_bw), _ = \\\r\n            tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\r\n        return tf.concat([outputs_fw, outputs_bw], axis=2)\r\n    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\r\nwith tf.variable_scope('words_attention'):\r\n    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = tf.layers.dense(outputs, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 1, 3, 2])), perm=[0, 1, 3, 2])\r\noutputs = tf.reduce_sum(outputs * attention, axis=2)\r\nwith tf.variable_scope('sentence_lstm'):\r\n    cell_fw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    cell_bw = tf.nn.rnn_cell.GRUCell(num_units=hidden_dim)\r\n    (outputs_fw, outputs_bw), _ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, outputs, sequence_length=sentences_length, dtype=tf.float32)\r\noutputs = tf.concat([outputs_fw, outputs_bw], axis=2)\r\nwith tf.variable_scope('sentence_attention'):\r\n    hidden = tf.layers.dense(outputs, units=hidden_dim * 2, activation=tf.nn.tanh)\r\n    attention = tf.layers.dense(hidden, units=1, activation=None)\r\n    attention = tf.transpose(tf.nn.softmax(tf.transpose(attention, perm=[0, 2, 1])), perm=[0, 2, 1])\r\noutputs = tf.reduce_sum(outputs * attention, axis=1)\r\nlogits = tf.layers.dense(outputs, units=num_class, activation=None)\r\nloss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')\r\ntraining_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n```\r\n\r\n**Error**\r\n```python\r\nINFO:tensorflow:Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop.\r\n\r\ngradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc while context: None\r\nwords_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1 while context: words_lstm/map/while/while_context\r\n\r\nTraceback for gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc:\r\n  File \"/Users/shengc/anaconda/envs/py36/bin/ipython\", line 6, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-cda6375ef3e5>\", line 1, in <module>\r\n    get_ipython().run_line_magic('paste', '')\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\r\n    result = fn(*args,**kwargs)\r\n  File \"<decorator-gen-27>\", line 2, in paste\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\r\n    call = lambda f, *a, **k: f(*a, **k)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 199, in paste\r\n    self.store_or_execute(block, name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 57, in store_or_execute\r\n    self.shell.run_cell(b)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-0176473fb528>\", line 40, in <module>\r\n    training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 355, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py\", line 456, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 375, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 609, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py\", line 131, in _TensorArrayWriteGrad\r\n    grad = g.read(index)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 859, in read\r\n    return self._implementation.read(index, name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py\", line 259, in read\r\n    name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 4498, in _tensor_array_read_v3\r\n    dtype=dtype, name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1674, in __init__\r\n    self._control_flow_context.AddOp(self)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2251, in AddOp\r\n    self._AddOpInternal(op)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2274, in _AddOpInternal\r\n    real_x = self.AddValue(x)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2207, in AddValue\r\n    real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1050, in GetRealValue\r\n    history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 908, in AddForwardAccumulator\r\n    name=\"f_acc\")\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py\", line 3578, in _stack_v2\r\n    stack_name=stack_name, name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nTraceback for words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1:\r\n  File \"/Users/shengc/anaconda/envs/py36/bin/ipython\", line 6, in <module>\r\n    sys.exit(IPython.start_ipython())\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/__init__.py\", line 125, in start_ipython\r\n    return launch_new_instance(argv=argv, **kwargs)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\r\n    app.start()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/ipapp.py\", line 356, in start\r\n    self.shell.mainloop()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 480, in mainloop\r\n    self.interact()\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/interactiveshell.py\", line 471, in interact\r\n    self.run_cell(code, store_history=True)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-cda6375ef3e5>\", line 1, in <module>\r\n    get_ipython().run_line_magic('paste', '')\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2095, in run_line_magic\r\n    result = fn(*args,**kwargs)\r\n  File \"<decorator-gen-27>\", line 2, in paste\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/magic.py\", line 187, in <lambda>\r\n    call = lambda f, *a, **k: f(*a, **k)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 199, in paste\r\n    self.store_or_execute(block, name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/terminal/magics.py\", line 57, in store_or_execute\r\n    self.shell.run_cell(b)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\r\n    interactivity=interactivity, compiler=compiler, result=result)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\r\n    if self.run_code(code, result):\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\r\n    exec(code_obj, self.user_global_ns, self.user_ns)\r\n  File \"<ipython-input-3-0176473fb528>\", line 22, in <module>\r\n    outputs = tf.map_fn(fn, (embedded, words_length), dtype=tf.float32)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 409, in map_fn\r\n    swap_memory=swap_memory)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2934, in while_loop\r\n    result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2720, in BuildLoop\r\n    pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2662, in _BuildLoop\r\n    body_result = body(*packed_vars_for_body)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py\", line 399, in compute\r\n    packed_fn_values = fn(packed_values)\r\n  File \"<ipython-input-3-0176473fb528>\", line 20, in fn\r\n    (outputs_fw, outputs_bw), _ =             tf.nn.bidirectional_dynamic_rnn(cell_fw, cell_bw, inp[0], sequence_length=inp[1], dtype=tf.float32)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 414, in bidirectional_dynamic_rnn\r\n    time_major=time_major, scope=fw_scope)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 629, in dynamic_rnn\r\n    dtype=dtype)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\", line 688, in _dynamic_rnn_loop\r\n    time_steps = input_shape[0]\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 573, in _slice_helper\r\n    name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py\", line 737, in strided_slice\r\n    shrink_axis_mask=shrink_axis_mask)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 5501, in strided_slice\r\n    name=name)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\r\n    op_def=op_def)\r\n  File \"/Users/shengc/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    369     try:\r\n--> 370       xla_compile = op.get_attr(\"_XlaCompile\")\r\n    371       xla_separate_compiled_gradients = op.get_attr(\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in get_attr(self, name)\r\n   2172         raise ValueError(\r\n-> 2173             \"No attr named '\" + name + \"' in \" + str(self._node_def))\r\n   2174       x = self._node_def.attr[name]\r\n\r\nValueError: No attr named '_XlaCompile' in name: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3\"\r\nop: \"TensorArrayWriteV3\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3/Enter\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_1\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Select\"\r\ninput: \"words_lstm/map/while/bidirectional_rnn/fw/fw/while/Identity_2\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"_class\"\r\n  value {\r\n    list {\r\n      s: \"loc:@words_lstm/map/while/bidirectional_rnn/fw/fw/while/gru_cell/add\"\r\n    }\r\n  }\r\n}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-0176473fb528> in <module>()\r\n     38 logits = tf.layers.dense(outputs, units=num_class, activation=None)\r\n     39 loss = tf.reduce_sum(tf.one_hot(labels, num_class) * tf.nn.softmax(logits), name='loss')\r\n---> 40 training_op = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in minimize(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\r\n    353         aggregation_method=aggregation_method,\r\n    354         colocate_gradients_with_ops=colocate_gradients_with_ops,\r\n--> 355         grad_loss=grad_loss)\r\n    356\r\n    357     vars_with_grad = [v for g, v in grads_and_vars if g is not None]\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/optimizer.py in compute_gradients(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\r\n    454         gate_gradients=(gate_gradients == Optimizer.GATE_OP),\r\n    455         aggregation_method=aggregation_method,\r\n--> 456         colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n    457     if gate_gradients == Optimizer.GATE_GRAPH:\r\n    458       grads = control_flow_ops.tuple(grads)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in gradients(ys, xs, grad_ys, name, colocate_gradients_with_ops, gate_gradients, aggregation_method, stop_gradients)\r\n    607                 # functions.\r\n    608                 in_grads = _MaybeCompile(\r\n--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n    610               else:\r\n    611                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in _MaybeCompile(scope, op, func, grad_fn)\r\n    373       xla_scope = op.get_attr(\"_XlaScope\").decode()\r\n    374     except ValueError:\r\n--> 375       return grad_fn()  # Exit early\r\n    376\r\n    377   if not xla_compile:\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py in <lambda>()\r\n    607                 # functions.\r\n    608                 in_grads = _MaybeCompile(\r\n--> 609                     grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n    610               else:\r\n    611                 # For function call ops, we add a 'SymbolicGradient'\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_grad.py in _TensorArrayWriteGrad(op, flow)\r\n    129                                     colocate_with_first_write_call=False)\r\n    130        .grad(source=grad_source, flow=flow))\r\n--> 131   grad = g.read(index)\r\n    132   return [None, None, grad, flow]\r\n    133\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)\r\n    857       The tensor at index `index`.\r\n    858     \"\"\"\r\n--> 859     return self._implementation.read(index, name=name)\r\n    860\r\n    861   @tf_should_use.should_use_result\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/tensor_array_ops.py in read(self, index, name)\r\n    257         flow_in=self._flow,\r\n    258         dtype=self._dtype,\r\n--> 259         name=name)\r\n    260     if self._element_shape:\r\n    261       value.set_shape(self._element_shape[0].dims)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _tensor_array_read_v3(handle, index, flow_in, dtype, name)\r\n   4496     _, _, _op = _op_def_lib._apply_op_helper(\r\n   4497         \"TensorArrayReadV3\", handle=handle, index=index, flow_in=flow_in,\r\n-> 4498         dtype=dtype, name=name)\r\n   4499     _result = _op.outputs[:]\r\n   4500     _inputs_flat = _op.inputs\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   3158         input_types=input_types,\r\n   3159         original_op=self._default_original_op,\r\n-> 3160         op_def=op_def)\r\n   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,\r\n   3162                            compute_device=compute_device)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\r\n   1673     if self._control_flow_context is not None:\r\n-> 1674       self._control_flow_context.AddOp(self)\r\n   1675     self._recompute_node_def()\r\n   1676\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddOp(self, op)\r\n   2249             op_input_ctxt._AddOpInternal(op)\r\n   2250             return\r\n-> 2251     self._AddOpInternal(op)\r\n   2252\r\n   2253   def _AddOpInternal(self, op):\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in _AddOpInternal(self, op)\r\n   2272       for index in range(len(op.inputs)):\r\n   2273         x = op.inputs[index]\r\n-> 2274         real_x = self.AddValue(x)\r\n   2275         if real_x != x:\r\n   2276           op._update_input(index, real_x)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddValue(self, val)\r\n   2205               forward_ctxt = forward_ctxt.GetWhileContext()\r\n   2206           if forward_ctxt == grad_ctxt.grad_state.forward_context:\r\n-> 2207             real_val = grad_ctxt.grad_state.GetRealValue(val)\r\n   2208             self._external_values[val.name] = real_val\r\n   2209             return real_val\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in GetRealValue(self, value)\r\n   1048           # Record the history of this value in forward_ctxt.\r\n   1049           self._grad_context.Exit()\r\n-> 1050           history_value = cur_grad_state.AddForwardAccumulator(cur_value)\r\n   1051           self._grad_context.Enter()\r\n   1052           break\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py in AddForwardAccumulator(self, value, dead_branch)\r\n    906             max_size=maximum_iterations,\r\n    907             elem_type=value.dtype.base_dtype,\r\n--> 908             name=\"f_acc\")\r\n    909         # pylint: enable=protected-access\r\n    910       if curr_ctxt: curr_ctxt.Exit()\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/gen_data_flow_ops.py in _stack_v2(max_size, elem_type, stack_name, name)\r\n   3576     _, _, _op = _op_def_lib._apply_op_helper(\r\n   3577         \"StackV2\", max_size=max_size, elem_type=elem_type,\r\n-> 3578         stack_name=stack_name, name=name)\r\n   3579     _result = _op.outputs[:]\r\n   3580     _inputs_flat = _op.inputs\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py in _apply_op_helper(self, op_type_name, name, **keywords)\r\n    785         op = g.create_op(op_type_name, inputs, output_types, name=scope,\r\n    786                          input_types=input_types, attrs=attr_protos,\r\n--> 787                          op_def=op_def)\r\n    788       return output_structure, op_def.is_stateful, op\r\n    789\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in create_op(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_shapes, compute_device)\r\n   3158         input_types=input_types,\r\n   3159         original_op=self._default_original_op,\r\n-> 3160         op_def=op_def)\r\n   3161     self._create_op_helper(ret, compute_shapes=compute_shapes,\r\n   3162                            compute_device=compute_device)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in __init__(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\r\n   1670     self._control_flow_context = g._get_control_flow_context()  # pylint: disable=protected-access\r\n   1671     for input_tensor in self.inputs:\r\n-> 1672       control_flow_util.CheckInputFromValidContext(self, input_tensor.op)\r\n   1673     if self._control_flow_context is not None:\r\n   1674       self._control_flow_context.AddOp(self)\r\n\r\n~/anaconda/envs/py36/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_util.py in CheckInputFromValidContext(op, input_op)\r\n    198         input_op.name, \"\".join(traceback.format_list(input_op.traceback)))\r\n    199     logging.info(log_msg)\r\n--> 200     raise ValueError(error_msg + \" See info log for more details.\")\r\n\r\nValueError: Cannot use 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' as input to 'gradients/words_lstm/map/while/bidirectional_rnn/fw/fw/while/TensorArrayWrite/TensorArrayWriteV3_grad/TensorArrayReadV3/f_acc' because 'words_lstm/map/while/bidirectional_rnn/fw/fw/strided_slice_1' is in a while loop. See info log for more details.\r\n```\r\n\r\n### Source code / logs\r\nsource code is here,\r\nhttps://github.com/shengc/tf-han/blob/master/hierarchical-attention-network.ipynb\r\ncalling `HierarchicalAttentionNetwork()._make_graph_batch(graph)` will produce the above error"}