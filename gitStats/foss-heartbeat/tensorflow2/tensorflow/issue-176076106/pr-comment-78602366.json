{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/78602366", "pull_request_review_id": null, "id": 78602366, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDc4NjAyMzY2", "diff_hunk": "@@ -0,0 +1,121 @@\n+# Adding Support to New Hardware in TensorFlow\n+\n+New compute hardware is be release frequently and \n+it is of great interest to do machine learning on \n+this hardware. Much of this hardware is designed \n+with machine learning in mind and had features \n+beyond the normal processor. Taking advantage of\n+these novel features is important to make the \n+processor be used to its fullest. For example, the\n+[Intel Xeon Phi](http://www.intel.com/content/www/us/en/processors/xeon/xeon-phi-detail.html),\n+[KnuPath Hermosa](https://www.knupath.com/products/hermosa-processors/),\n+[IBM TrueNorth](http://researchweb.watson.ibm.com/articles/brain-chip.shtml), etc.\n+Custom support for each of these different processors\n+is needed to make TensorFlow capable of using them.\n+\n+This is a walk through of how to add support for \n+new hardware to TensorFlow. It is assumed that you\n+can link C/C++ code that uses your particular hardware\n+into TensorFlow. For simplicity, I will link\n+code that implement standard BLAS calls.\n+\n+## TensorFlow Class Structure\n+\n+The class structure for Devices,\n+* [class DeviceBase](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/device_base.h)\n+  * [class Device: public DeviceBase](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/device.h)\n+    * [class LocalDevice: public Device](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/local_device.h)\n+      * [class BaseGPUDevice: public LocalDevice](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.h)\n+      \t* [class GPUDevice: public BaseGPUDevice](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device_factory.cc)\n+      * [class ThreadPoolDevice: public LocalDevice](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/threadpool_device.h)\n+    * [class RemoteDevice: public Device](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/distributed_runtime/remote_device.cc)\n+\n+\n+There is also a structure referred to as a Device Factory,\n+* [class DeviceFactory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/device_factory.h)\n+  * [class ThreadPoolDeviceFactory: public Device Factory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/threadpool_device_factory.cc)\n+  * [class BaseGPUDeviceFactory : public DeviceFactory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device.h)\n+    * [class GPUDeviceFactory: public BaseGPUDeviceFactory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device_factory.cc)\n+  * [class GPUCompatibleCPUDeviceFactory: public DeviceFactory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/gpu/gpu_device_factory.cc)\n+\n+\n+It is clear that TensorFlow is currently built to support 3 major \n+types of devices: CPU, GPU, Distributed. We will walk throught the \n+creation of a new kind of device for each of these classes and leave \n+it to the reader to decide which general type of device his/her \n+hardware falls into best.\n+\n+\n+## New ThreadPool\n+\n+We would like to create a new hardware interface that we can use in\n+standard TensorFlow operations. For example, we might want a ficticious\n+device called an SPU that I can use like,\n+\n+```python\n+import tensorflow as tf\n+\n+# Creates a graph.                                                              \n+with tf.device('/spu:0'):\n+  a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n+  b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n+  c = tf.matmul(a, b)\n+# Creates a session with log_device_placement set to True.                      \n+sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n+# Runs the op.                                                                  \n+print sess.run(c)\n+```\n+\n+with output,\n+```bash\n+$ python test.py \n+Device mapping: no known devices.\n+I tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n+\n+MatMul: /job:localhost/replica:0/task:0/spu:0\n+I tensorflow/core/common_runtime/simple_placer.cc:818] MatMul: /job:localhost/replica:0/task:0/spu:0\n+b: /job:localhost/replica:0/task:0/spu:0\n+I tensorflow/core/common_runtime/simple_placer.cc:818] b: /job:localhost/replica:0/task:0/spu:0\n+a: /job:localhost/replica:0/task:0/spu:0\n+I tensorflow/core/common_runtime/simple_placer.cc:818] a: /job:localhost/replica:0/task:0/spu:0\n+[[ 22.  28.]\n+ [ 49.  64.]]\n+```\n+\n+To do so, we need to implement a few classes and modify the C++\n+source to TensorFlow. Detail of this are found [here](fake_device.md),\n+but as a high level overview, you need to create or modify the following,\n+\n+* `/configure` - Modify build parameters\n+* `/tensorflow/core/common_runtime/device_factory.cc` - Add bindings for the SPU device factory\n+* `/tensorflow/core/common_runtime/device_set.cc` - Define a priority ordering for the devices", "path": "tensorflow/g3doc/hardware/adding_support/index.md", "position": 91, "original_position": 91, "commit_id": "36e0cdf04f294bfd51931d4f78e291590ed0d3ec", "original_commit_id": "36e0cdf04f294bfd51931d4f78e291590ed0d3ec", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "For now let's leave this out, it's not necessary at the moment\n", "created_at": "2016-09-13T17:07:30Z", "updated_at": "2016-09-13T17:07:30Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4305#discussion_r78602366", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4305", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/78602366"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4305#discussion_r78602366"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4305"}}, "body_html": "<p>For now let's leave this out, it's not necessary at the moment</p>", "body_text": "For now let's leave this out, it's not necessary at the moment"}