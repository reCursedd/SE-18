{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11411", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11411/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11411/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11411/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11411", "id": 241760279, "node_id": "MDU6SXNzdWUyNDE3NjAyNzk=", "number": 11411, "title": "Fetching data in Distributed Tensorflow has too much latency ", "user": {"login": "ahaider3", "id": 8320911, "node_id": "MDQ6VXNlcjgzMjA5MTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/8320911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahaider3", "html_url": "https://github.com/ahaider3", "followers_url": "https://api.github.com/users/ahaider3/followers", "following_url": "https://api.github.com/users/ahaider3/following{/other_user}", "gists_url": "https://api.github.com/users/ahaider3/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahaider3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahaider3/subscriptions", "organizations_url": "https://api.github.com/users/ahaider3/orgs", "repos_url": "https://api.github.com/users/ahaider3/repos", "events_url": "https://api.github.com/users/ahaider3/events{/privacy}", "received_events_url": "https://api.github.com/users/ahaider3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 33, "created_at": "2017-07-10T15:50:23Z", "updated_at": "2018-03-26T20:14:33Z", "closed_at": "2018-03-26T20:14:33Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n</ul>\n<p><a href=\"https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\">https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d</a></p>\n<p>The above is a simple benchmark which tests the overhead of distributed TF. It fetches a configurable sized variable from the parameter server and does a matmul on the worker. It also does a matmul from a locally stored variable on the worker. The time difference between these two operations would be the overhead I am measuring.</p>\n<ul>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Redhat</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntensorflow 1.2</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\npython 2.7.7</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n</li>\n</ul>\n<p>python matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=ps &amp;</p>\n<p>python matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=worker</p>\n<p>By increasing batch_size, the timing difference between local/remote computation eventually becomes negligible.<br>\nHowever, for small batch sizes the overhead can become 2x/3x:</p>\n<p>For example, here are two runs for different model/dataset sizes:</p>\n<blockquote>\n<p>128 features, batch size of 32, hidden layer size of 256  returns:<br>\nLocal GEMM Time:  0.0002624  Network Fetch GEMM Time: 0.0006798</p>\n</blockquote>\n<blockquote>\n<p>256 features, batch size of 128, hidden layer size of 128  returns:<br>\nLocal GEMM Time: 0.0002995 Network Fetch GEMM Time: 0.0006124</p>\n</blockquote>\n<h3>Describe the problem</h3>\n<p>Distributed tensorflow introduces overhead due to its communication stack. By overhead I mean the additional time required for workers to receive data from parameter servers when compared to doing the same computation without fetching any remote data.</p>\n<p>This is a problem because due to this overhead I have to use 2/3 nodes to just provide performance on-par with non-distributed (single process) tensorflow. The number of nodes required to be on-par with single process TF increases further when I use gpus.</p>\n<p>Fetching small variables provides a constant overhead which limits scaling and efficiency .<br>\nThis overhead creates two issues in Distributed Tensorflow:</p>\n<ol>\n<li>I have to add several workers just to equal the performance of a single process.</li>\n<li>The overhead of fetching model parameters doesn't scale but the amount of computation does<br>\ndecrease as I add more workers. Thus, once I get to a moderately small batch size for each worker I can't scale because the constant overhead of fetching remote model parameters.</li>\n</ol>\n<p>There have been several issues posted with distributed tensorflow. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"193706605\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6116\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6116/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6116\">#6116</a> is an improvement to large tensor transfer while this problem exists for small tensors. <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"178173180\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4498\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4498/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4498\">#4498</a> might have been caused by CPU performance bottleneck and not network. However for my problem, network transfer is definitely the bottleneck. I have tried using RDMA and have seen minimal benefit.</p>\n<h3>Source code / logs</h3>\n<p><a href=\"https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\">https://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\n\nhttps://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\nThe above is a simple benchmark which tests the overhead of distributed TF. It fetches a configurable sized variable from the parameter server and does a matmul on the worker. It also does a matmul from a locally stored variable on the worker. The time difference between these two operations would be the overhead I am measuring.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Redhat\n\n\nTensorFlow installed from (source or binary):\nsource\n\n\nTensorFlow version (use command below):\ntensorflow 1.2\n\n\nPython version:\npython 2.7.7\n\n\nExact command to reproduce:\n\n\npython matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=ps &\npython matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=worker\nBy increasing batch_size, the timing difference between local/remote computation eventually becomes negligible.\nHowever, for small batch sizes the overhead can become 2x/3x:\nFor example, here are two runs for different model/dataset sizes:\n\n128 features, batch size of 32, hidden layer size of 256  returns:\nLocal GEMM Time:  0.0002624  Network Fetch GEMM Time: 0.0006798\n\n\n256 features, batch size of 128, hidden layer size of 128  returns:\nLocal GEMM Time: 0.0002995 Network Fetch GEMM Time: 0.0006124\n\nDescribe the problem\nDistributed tensorflow introduces overhead due to its communication stack. By overhead I mean the additional time required for workers to receive data from parameter servers when compared to doing the same computation without fetching any remote data.\nThis is a problem because due to this overhead I have to use 2/3 nodes to just provide performance on-par with non-distributed (single process) tensorflow. The number of nodes required to be on-par with single process TF increases further when I use gpus.\nFetching small variables provides a constant overhead which limits scaling and efficiency .\nThis overhead creates two issues in Distributed Tensorflow:\n\nI have to add several workers just to equal the performance of a single process.\nThe overhead of fetching model parameters doesn't scale but the amount of computation does\ndecrease as I add more workers. Thus, once I get to a moderately small batch size for each worker I can't scale because the constant overhead of fetching remote model parameters.\n\nThere have been several issues posted with distributed tensorflow. #6116 is an improvement to large tensor transfer while this problem exists for small tensors. #4498 might have been caused by CPU performance bottleneck and not network. However for my problem, network transfer is definitely the bottleneck. I have tried using RDMA and have seen minimal benefit.\nSource code / logs\nhttps://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n\r\nhttps://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\r\n\r\nThe above is a simple benchmark which tests the overhead of distributed TF. It fetches a configurable sized variable from the parameter server and does a matmul on the worker. It also does a matmul from a locally stored variable on the worker. The time difference between these two operations would be the overhead I am measuring. \r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Redhat  \r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\ntensorflow 1.2 \r\n- **Python version**: \r\npython 2.7.7\r\n\r\n\r\n\r\n- **Exact command to reproduce**:\r\n\r\npython matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=ps &\r\n\r\npython matmul_benchmark.py --num_features=256 --batch_size=128 --num_hidden=64 --job_name=worker\r\n\r\nBy increasing batch_size, the timing difference between local/remote computation eventually becomes negligible. \r\nHowever, for small batch sizes the overhead can become 2x/3x:\r\n\r\nFor example, here are two runs for different model/dataset sizes:\r\n\r\n> 128 features, batch size of 32, hidden layer size of 256  returns:\r\nLocal GEMM Time:  0.0002624  Network Fetch GEMM Time: 0.0006798\r\n\r\n> 256 features, batch size of 128, hidden layer size of 128  returns:\r\nLocal GEMM Time: 0.0002995 Network Fetch GEMM Time: 0.0006124\r\n\r\n### Describe the problem\r\nDistributed tensorflow introduces overhead due to its communication stack. By overhead I mean the additional time required for workers to receive data from parameter servers when compared to doing the same computation without fetching any remote data. \r\n\r\nThis is a problem because due to this overhead I have to use 2/3 nodes to just provide performance on-par with non-distributed (single process) tensorflow. The number of nodes required to be on-par with single process TF increases further when I use gpus. \r\n\r\n\r\nFetching small variables provides a constant overhead which limits scaling and efficiency . \r\nThis overhead creates two issues in Distributed Tensorflow:\r\n1) I have to add several workers just to equal the performance of a single process. \r\n2) The overhead of fetching model parameters doesn't scale but the amount of computation does \r\ndecrease as I add more workers. Thus, once I get to a moderately small batch size for each worker I can't scale because the constant overhead of fetching remote model parameters. \r\n\r\nThere have been several issues posted with distributed tensorflow. #6116 is an improvement to large tensor transfer while this problem exists for small tensors. #4498 might have been caused by CPU performance bottleneck and not network. However for my problem, network transfer is definitely the bottleneck. I have tried using RDMA and have seen minimal benefit. \r\n\r\n### Source code / logs\r\nhttps://gist.github.com/ahaider3/ae4f6d2d790d963a93b346bb0138a41d\r\n\r\n\r\n\r\n"}