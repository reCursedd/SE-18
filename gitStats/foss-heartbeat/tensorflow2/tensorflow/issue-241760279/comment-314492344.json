{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314492344", "html_url": "https://github.com/tensorflow/tensorflow/issues/11411#issuecomment-314492344", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11411", "id": 314492344, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDQ5MjM0NA==", "user": {"login": "ahaider3", "id": 8320911, "node_id": "MDQ6VXNlcjgzMjA5MTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/8320911?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahaider3", "html_url": "https://github.com/ahaider3", "followers_url": "https://api.github.com/users/ahaider3/followers", "following_url": "https://api.github.com/users/ahaider3/following{/other_user}", "gists_url": "https://api.github.com/users/ahaider3/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahaider3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahaider3/subscriptions", "organizations_url": "https://api.github.com/users/ahaider3/orgs", "repos_url": "https://api.github.com/users/ahaider3/repos", "events_url": "https://api.github.com/users/ahaider3/events{/privacy}", "received_events_url": "https://api.github.com/users/ahaider3/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-11T16:03:56Z", "updated_at": "2017-07-11T16:04:51Z", "author_association": "NONE", "body_html": "<p>Without fetching model parameters it only takes a few hundred microseconds. Yes, that's the problem I have. My compute time per iteration is low and so going to distributed requires double/triple the time per iteration due to the fixed overhead.</p>\n<p>I didn't see much benefit when I was testing with <code>grpc+verbs</code>, but that was with larger tensors. Is there a difference between <code>grpc+mpi</code> or <code>grpc+verbs</code> when compared to <code>grpc</code> besides the actual communication protocol? Are tensors still serialized in the same manner? The actual communication across the network is not the bottleneck from my testing.</p>", "body_text": "Without fetching model parameters it only takes a few hundred microseconds. Yes, that's the problem I have. My compute time per iteration is low and so going to distributed requires double/triple the time per iteration due to the fixed overhead.\nI didn't see much benefit when I was testing with grpc+verbs, but that was with larger tensors. Is there a difference between grpc+mpi or grpc+verbs when compared to grpc besides the actual communication protocol? Are tensors still serialized in the same manner? The actual communication across the network is not the bottleneck from my testing.", "body": "Without fetching model parameters it only takes a few hundred microseconds. Yes, that's the problem I have. My compute time per iteration is low and so going to distributed requires double/triple the time per iteration due to the fixed overhead. \r\n\r\nI didn't see much benefit when I was testing with `grpc+verbs`, but that was with larger tensors. Is there a difference between `grpc+mpi` or `grpc+verbs` when compared to `grpc` besides the actual communication protocol? Are tensors still serialized in the same manner? The actual communication across the network is not the bottleneck from my testing. "}