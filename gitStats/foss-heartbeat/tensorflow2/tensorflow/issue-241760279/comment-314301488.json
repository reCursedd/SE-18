{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/314301488", "html_url": "https://github.com/tensorflow/tensorflow/issues/11411#issuecomment-314301488", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11411", "id": 314301488, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNDMwMTQ4OA==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-11T02:03:36Z", "updated_at": "2017-07-11T02:03:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I see both of your model size and batch size is small (128/256?). What's the time spent on computation each round before fetching model parameters from PS? If it only spends tens to hundreds of microseconds, I don't think the current distributed runtime could reach a performance on par to that of a single process, as there's a fixed overhead on setting up interprocess communication.</p>", "body_text": "I see both of your model size and batch size is small (128/256?). What's the time spent on computation each round before fetching model parameters from PS? If it only spends tens to hundreds of microseconds, I don't think the current distributed runtime could reach a performance on par to that of a single process, as there's a fixed overhead on setting up interprocess communication.", "body": "I see both of your model size and batch size is small (128/256?). What's the time spent on computation each round before fetching model parameters from PS? If it only spends tens to hundreds of microseconds, I don't think the current distributed runtime could reach a performance on par to that of a single process, as there's a fixed overhead on setting up interprocess communication."}