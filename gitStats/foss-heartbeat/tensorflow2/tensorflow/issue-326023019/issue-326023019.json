{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19524", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19524/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19524/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19524/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19524", "id": 326023019, "node_id": "MDU6SXNzdWUzMjYwMjMwMTk=", "number": 19524, "title": "Compiling C++ inference with -O1 produces wrong results", "user": {"login": "mkravchik", "id": 5304779, "node_id": "MDQ6VXNlcjUzMDQ3Nzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/5304779?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mkravchik", "html_url": "https://github.com/mkravchik", "followers_url": "https://api.github.com/users/mkravchik/followers", "following_url": "https://api.github.com/users/mkravchik/following{/other_user}", "gists_url": "https://api.github.com/users/mkravchik/gists{/gist_id}", "starred_url": "https://api.github.com/users/mkravchik/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mkravchik/subscriptions", "organizations_url": "https://api.github.com/users/mkravchik/orgs", "repos_url": "https://api.github.com/users/mkravchik/repos", "events_url": "https://api.github.com/users/mkravchik/events{/privacy}", "received_events_url": "https://api.github.com/users/mkravchik/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-05-24T08:49:51Z", "updated_at": "2018-06-11T13:57:56Z", "closed_at": "2018-06-11T13:57:19Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNot in the tensorflow, but I have a code that utilizes C++ API</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux VirtualBox 4.13.0-41-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115986171\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/46\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/46/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/46\">#46</a>~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux - running in VirtualBox on a Windows Host</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nSource</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntf.VERSION = 1.8.0-rc1<br>\ntf.GIT_VERSION = v1.8.0-rc1-1239-gd0f5bc1<br>\ntf.COMPILER_VERSION = v1.8.0-rc1-1239-gd0f5bc1<br>\nSanity check: array([1], dtype=int32)</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 2.7.12</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\nBuild label: 0.13.0<br>\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)<br>\nBuild timestamp: 1525078013620<br>\nBuild timestamp as int: 1525078013620</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nNo</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nNo</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\nPlease see below<br>\nYou can collect some of this information using our environment capture script:</p>\n</li>\n</ul>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.<br>\nI'm using a model I built in Python to run inference in C++ and compare the results to the ones I got when running it in Python. I save the exact batches I fed in Python and feed them in C++. When I use the default build settings I get very close results. However, I've found that matrix calclulations I performs with the results are incredibly slow (3 minutes for matrix division of 2 1000 by 5 matrices - I will submit a separate issue for that). So I tried to compile with -O1.<br>\nThe result returned by running the inference very different now from the expected one.<br>\nPlease clarify what is going on here.</p>\n<p>So without optimization I get:<br>\nLoadModel passed!<br>\nIteration 1<br>\noutputs[0]: Tensor&lt;type: float shape: [1,5] values: [0.928591609 0.129222199 1.01102]...&gt; &lt;== Inferred by C++<br>\noutput_tensor Tensor&lt;type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...&gt; &lt;== Inferred by Python</p>\n<p>With -O1:<br>\nLoadModel passed!<br>\nIteration 1<br>\noutputs[0]: Tensor&lt;type: float shape: [1,5] values: [0.907290399 0.124970555 1]...&gt; &lt;== Inferred by C++</p>\n<p>output_tensor Tensor&lt;type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...&gt;&lt;== Inferred by Python</p>\n<p>diff is bigger than expected: 0.0445271</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/2034485/optimization_problem_tensorflow.zip\">optimization_problem_tensorflow.zip</a></p>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNot in the tensorflow, but I have a code that utilizes C++ API\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux VirtualBox 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux - running in VirtualBox on a Windows Host\n\n\nTensorFlow installed from (source or binary):\nSource\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.8.0-rc1\ntf.GIT_VERSION = v1.8.0-rc1-1239-gd0f5bc1\ntf.COMPILER_VERSION = v1.8.0-rc1-1239-gd0f5bc1\nSanity check: array([1], dtype=int32)\n\n\nPython version:\nPython 2.7.12\n\n\nBazel version (if compiling from source):\nBuild label: 0.13.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\nBuild timestamp: 1525078013620\nBuild timestamp as int: 1525078013620\n\n\nGCC/Compiler version (if compiling from source):\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\n\n\nCUDA/cuDNN version:\nNo\n\n\nGPU model and memory:\nNo\n\n\nExact command to reproduce:\nPlease see below\nYou can collect some of this information using our environment capture script:\n\n\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nI'm using a model I built in Python to run inference in C++ and compare the results to the ones I got when running it in Python. I save the exact batches I fed in Python and feed them in C++. When I use the default build settings I get very close results. However, I've found that matrix calclulations I performs with the results are incredibly slow (3 minutes for matrix division of 2 1000 by 5 matrices - I will submit a separate issue for that). So I tried to compile with -O1.\nThe result returned by running the inference very different now from the expected one.\nPlease clarify what is going on here.\nSo without optimization I get:\nLoadModel passed!\nIteration 1\noutputs[0]: Tensor<type: float shape: [1,5] values: [0.928591609 0.129222199 1.01102]...> <== Inferred by C++\noutput_tensor Tensor<type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...> <== Inferred by Python\nWith -O1:\nLoadModel passed!\nIteration 1\noutputs[0]: Tensor<type: float shape: [1,5] values: [0.907290399 0.124970555 1]...> <== Inferred by C++\noutput_tensor Tensor<type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...><== Inferred by Python\ndiff is bigger than expected: 0.0445271\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\noptimization_problem_tensorflow.zip", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nNot in the tensorflow, but I have a code that utilizes C++ API\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux VirtualBox 4.13.0-41-generic #46~16.04.1-Ubuntu SMP Thu May 3 10:06:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux - running in VirtualBox on a Windows Host\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nSource\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.8.0-rc1\r\ntf.GIT_VERSION = v1.8.0-rc1-1239-gd0f5bc1\r\ntf.COMPILER_VERSION = v1.8.0-rc1-1239-gd0f5bc1\r\nSanity check: array([1], dtype=int32)\r\n- **Python version**: \r\nPython 2.7.12\r\n\r\n- **Bazel version (if compiling from source)**:\r\nBuild label: 0.13.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Mon Oct 18 21:33:40 +50297 (1525078013620)\r\nBuild timestamp: 1525078013620\r\nBuild timestamp as int: 1525078013620\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609\r\n\r\n- **CUDA/cuDNN version**:\r\nNo\r\n\r\n- **GPU model and memory**:\r\nNo\r\n\r\n- **Exact command to reproduce**:\r\nPlease see below\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\nI'm using a model I built in Python to run inference in C++ and compare the results to the ones I got when running it in Python. I save the exact batches I fed in Python and feed them in C++. When I use the default build settings I get very close results. However, I've found that matrix calclulations I performs with the results are incredibly slow (3 minutes for matrix division of 2 1000 by 5 matrices - I will submit a separate issue for that). So I tried to compile with -O1. \r\nThe result returned by running the inference very different now from the expected one.\r\nPlease clarify what is going on here.\r\n\r\nSo without optimization I get:\r\nLoadModel passed!\r\nIteration 1\r\noutputs[0]: Tensor<type: float shape: [1,5] values: [0.928591609 0.129222199 1.01102]...> <== Inferred by C++\r\noutput_tensor Tensor<type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...> <== Inferred by Python\r\n\r\nWith -O1:\r\nLoadModel passed!\r\nIteration 1\r\noutputs[0]: Tensor<type: float shape: [1,5] values: [0.907290399 0.124970555 1]...> <== Inferred by C++\r\n\r\noutput_tensor Tensor<type: float shape: [1,1,5] values: [[0.928591728 0.129222214 1.01102006]]...><== Inferred by Python\r\n\r\ndiff is bigger than expected: 0.0445271\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n[optimization_problem_tensorflow.zip](https://github.com/tensorflow/tensorflow/files/2034485/optimization_problem_tensorflow.zip)\r\n\r\n"}