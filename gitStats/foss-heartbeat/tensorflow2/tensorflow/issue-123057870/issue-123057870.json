{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/551", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/551/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/551/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/551/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/551", "id": 123057870, "node_id": "MDU6SXNzdWUxMjMwNTc4NzA=", "number": 551, "title": "Lock contention in thread pool for CPU-based tensor operations", "user": {"login": "jeremybarnes", "id": 112556, "node_id": "MDQ6VXNlcjExMjU1Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/112556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jeremybarnes", "html_url": "https://github.com/jeremybarnes", "followers_url": "https://api.github.com/users/jeremybarnes/followers", "following_url": "https://api.github.com/users/jeremybarnes/following{/other_user}", "gists_url": "https://api.github.com/users/jeremybarnes/gists{/gist_id}", "starred_url": "https://api.github.com/users/jeremybarnes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jeremybarnes/subscriptions", "organizations_url": "https://api.github.com/users/jeremybarnes/orgs", "repos_url": "https://api.github.com/users/jeremybarnes/repos", "events_url": "https://api.github.com/users/jeremybarnes/events{/privacy}", "received_events_url": "https://api.github.com/users/jeremybarnes/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2015-12-19T03:14:49Z", "updated_at": "2016-05-21T13:34:18Z", "closed_at": "2016-05-21T13:34:18Z", "author_association": "NONE", "body_html": "<p>Running the December 15 Inception model, with either serial or parallel inference steps, is showing serious lock contention on the thread pool lock, with about 50% of the CPU being spent in futex operations:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/112556/11910933/8f720e80-a5d3-11e5-9a03-33ad7985fe3f.png\"><img width=\"1296\" alt=\"jeremy_khan____projects_mldb\" src=\"https://cloud.githubusercontent.com/assets/112556/11910933/8f720e80-a5d3-11e5-9a03-33ad7985fe3f.png\" style=\"max-width:100%;\"></a></p>\n<p>As an experiment, replacing the mutex with a basic spinlock reduces latency by around 35% (whilst obviously increasing CPU usage due to all of the CPUs constantly spinning even with nothing to do).  Obviously a pure spinlock is not a viable solution, but it shows that there is some scope for improvement.</p>\n<p>This leads to three related questions:</p>\n<ol>\n<li>Would it be possible to allow the platform to provide its own thread pool implementation or factory?  The ThreadPool methods are virtual, so it would be possible to make it pluggable, except that the class is directly created wherever used.  In our case we have a separate thread pool that has a broadly compatible interface and we'd like to share with TensorFlow.</li>\n<li>Would it make sense to reduce the granularity of the Eigen tensor operation multithreading?</li>\n<li>If not, is there any interest in a lock free thread pool implementation?  Or is this something that somebody is already working on?</li>\n</ol>", "body_text": "Running the December 15 Inception model, with either serial or parallel inference steps, is showing serious lock contention on the thread pool lock, with about 50% of the CPU being spent in futex operations:\n\nAs an experiment, replacing the mutex with a basic spinlock reduces latency by around 35% (whilst obviously increasing CPU usage due to all of the CPUs constantly spinning even with nothing to do).  Obviously a pure spinlock is not a viable solution, but it shows that there is some scope for improvement.\nThis leads to three related questions:\n\nWould it be possible to allow the platform to provide its own thread pool implementation or factory?  The ThreadPool methods are virtual, so it would be possible to make it pluggable, except that the class is directly created wherever used.  In our case we have a separate thread pool that has a broadly compatible interface and we'd like to share with TensorFlow.\nWould it make sense to reduce the granularity of the Eigen tensor operation multithreading?\nIf not, is there any interest in a lock free thread pool implementation?  Or is this something that somebody is already working on?", "body": "Running the December 15 Inception model, with either serial or parallel inference steps, is showing serious lock contention on the thread pool lock, with about 50% of the CPU being spent in futex operations:\n\n<img width=\"1296\" alt=\"jeremy_khan____projects_mldb\" src=\"https://cloud.githubusercontent.com/assets/112556/11910933/8f720e80-a5d3-11e5-9a03-33ad7985fe3f.png\">\n\nAs an experiment, replacing the mutex with a basic spinlock reduces latency by around 35% (whilst obviously increasing CPU usage due to all of the CPUs constantly spinning even with nothing to do).  Obviously a pure spinlock is not a viable solution, but it shows that there is some scope for improvement.\n\nThis leads to three related questions:\n1.  Would it be possible to allow the platform to provide its own thread pool implementation or factory?  The ThreadPool methods are virtual, so it would be possible to make it pluggable, except that the class is directly created wherever used.  In our case we have a separate thread pool that has a broadly compatible interface and we'd like to share with TensorFlow.\n2.  Would it make sense to reduce the granularity of the Eigen tensor operation multithreading?\n3.  If not, is there any interest in a lock free thread pool implementation?  Or is this something that somebody is already working on?\n"}