{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12855", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12855/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12855/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12855/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12855", "id": 255755638, "node_id": "MDU6SXNzdWUyNTU3NTU2Mzg=", "number": 12855, "title": "[Feature Request]Read the last batch in Dataset", "user": {"login": "TomorrowIsAnOtherDay", "id": 25046619, "node_id": "MDQ6VXNlcjI1MDQ2NjE5", "avatar_url": "https://avatars0.githubusercontent.com/u/25046619?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TomorrowIsAnOtherDay", "html_url": "https://github.com/TomorrowIsAnOtherDay", "followers_url": "https://api.github.com/users/TomorrowIsAnOtherDay/followers", "following_url": "https://api.github.com/users/TomorrowIsAnOtherDay/following{/other_user}", "gists_url": "https://api.github.com/users/TomorrowIsAnOtherDay/gists{/gist_id}", "starred_url": "https://api.github.com/users/TomorrowIsAnOtherDay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TomorrowIsAnOtherDay/subscriptions", "organizations_url": "https://api.github.com/users/TomorrowIsAnOtherDay/orgs", "repos_url": "https://api.github.com/users/TomorrowIsAnOtherDay/repos", "events_url": "https://api.github.com/users/TomorrowIsAnOtherDay/events{/privacy}", "received_events_url": "https://api.github.com/users/TomorrowIsAnOtherDay/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-09-06T22:01:06Z", "updated_at": "2017-09-07T00:45:48Z", "closed_at": "2017-09-07T00:05:32Z", "author_association": "NONE", "body_html": "<h2>The programmer guide provides an example of work flow like this</h2>\n<p>`filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]<br>\ndataset = tf.contrib.data.TFRecordDataset(filenames)<br>\ndataset = dataset.map(...)<br>\ndataset = dataset.batch(32)<br>\niterator = dataset.make_initializable_iterator()<br>\nnext_element = iterator.get_next()</p>\n<h3>Compute for 100 epochs.</h3>\n<p>for _ in range(100):<br>\nsess.run(iterator.initializer)<br>\nwhile True:<br>\ntry:<br>\nsess.run(next_element)<br>\nexcept tf.errors.OutOfRangeError:<br>\nbreak`</p>\n<h2>if we have 33 examples in dataset then we will miss the last example, Is there an API to adjust the batch size automatically(e.g. in this case 1) in order to feed all the examples into model?</h2>", "body_text": "The programmer guide provides an example of work flow like this\n`filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\ndataset = tf.contrib.data.TFRecordDataset(filenames)\ndataset = dataset.map(...)\ndataset = dataset.batch(32)\niterator = dataset.make_initializable_iterator()\nnext_element = iterator.get_next()\nCompute for 100 epochs.\nfor _ in range(100):\nsess.run(iterator.initializer)\nwhile True:\ntry:\nsess.run(next_element)\nexcept tf.errors.OutOfRangeError:\nbreak`\nif we have 33 examples in dataset then we will miss the last example, Is there an API to adjust the batch size automatically(e.g. in this case 1) in order to feed all the examples into model?", "body": "## The programmer guide provides an example of work flow like this\r\n\r\n\r\n`filenames = [\"/var/data/file1.tfrecord\", \"/var/data/file2.tfrecord\"]\r\ndataset = tf.contrib.data.TFRecordDataset(filenames)\r\ndataset = dataset.map(...)\r\ndataset = dataset.batch(32)\r\niterator = dataset.make_initializable_iterator()\r\nnext_element = iterator.get_next()\r\n\r\n### Compute for 100 epochs.\r\nfor _ in range(100):\r\n  sess.run(iterator.initializer)\r\n  while True:\r\n    try:\r\n      sess.run(next_element)\r\n    except tf.errors.OutOfRangeError:\r\n      break`\r\n\r\n## if we have 33 examples in dataset then we will miss the last example, Is there an API to adjust the batch size automatically(e.g. in this case 1) in order to feed all the examples into model? "}