{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22329", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22329/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22329/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22329/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22329", "id": 361033399, "node_id": "MDU6SXNzdWUzNjEwMzMzOTk=", "number": 22329, "title": "Error when loading model from InputStream in Tensorflow Lite", "user": {"login": "Bracewind", "id": 40767899, "node_id": "MDQ6VXNlcjQwNzY3ODk5", "avatar_url": "https://avatars3.githubusercontent.com/u/40767899?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bracewind", "html_url": "https://github.com/Bracewind", "followers_url": "https://api.github.com/users/Bracewind/followers", "following_url": "https://api.github.com/users/Bracewind/following{/other_user}", "gists_url": "https://api.github.com/users/Bracewind/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bracewind/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bracewind/subscriptions", "organizations_url": "https://api.github.com/users/Bracewind/orgs", "repos_url": "https://api.github.com/users/Bracewind/repos", "events_url": "https://api.github.com/users/Bracewind/events{/privacy}", "received_events_url": "https://api.github.com/users/Bracewind/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-17T20:35:56Z", "updated_at": "2018-09-19T15:25:52Z", "closed_at": "2018-09-19T15:25:51Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 18.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: Samsung Galaxy, android 6.0 API 23</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: gradle</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow-android:1.9.0</li>\n<li><strong>Python version</strong>: no python</li>\n<li><strong>Bazel version (if compiling from source)</strong>: no bazel</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: no compiler</li>\n<li><strong>CUDA/cuDNN version</strong>: no version</li>\n<li><strong>GPU model and memory</strong>: Mobile</li>\n<li><strong>Exact command to reproduce</strong>: initialize tensorflowInferenceInterface with a InputBuffer &gt; 17 MO</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I use Tensorflow Lite since Tensorflow mobile is not ready for production yet.</p>\n<p>When I tried to directly load Inception V3 from a file from External storage, it returns a OOM error</p>\n<p>By looking at the code, I saw :<br>\n<code>byte[] buf = new byte[16384];</code><br>\nat the line 143 of TensorflowInferenceInterface</p>\n<p>but my network uses more than 16384<br>\nI think it should use baosInitSize<br>\n<code>int baosInitSize = is.available() &gt; 16384 ? is.available() : 16384;</code> at line 140</p>\n<h3>Source code / logs</h3>\n<p>java.lang.OutOfMemoryError: Failed to allocate a 87514870 byte allocation with 16773184 free bytes and 29MB until OOM</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Samsung Galaxy, android 6.0 API 23\nTensorFlow installed from (source or binary): gradle\nTensorFlow version (use command below): tensorflow-android:1.9.0\nPython version: no python\nBazel version (if compiling from source): no bazel\nGCC/Compiler version (if compiling from source): no compiler\nCUDA/cuDNN version: no version\nGPU model and memory: Mobile\nExact command to reproduce: initialize tensorflowInferenceInterface with a InputBuffer > 17 MO\n\nDescribe the problem\nI use Tensorflow Lite since Tensorflow mobile is not ready for production yet.\nWhen I tried to directly load Inception V3 from a file from External storage, it returns a OOM error\nBy looking at the code, I saw :\nbyte[] buf = new byte[16384];\nat the line 143 of TensorflowInferenceInterface\nbut my network uses more than 16384\nI think it should use baosInitSize\nint baosInitSize = is.available() > 16384 ? is.available() : 16384; at line 140\nSource code / logs\njava.lang.OutOfMemoryError: Failed to allocate a 87514870 byte allocation with 16773184 free bytes and 29MB until OOM", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Samsung Galaxy, android 6.0 API 23 \r\n- **TensorFlow installed from (source or binary)**: gradle\r\n- **TensorFlow version (use command below)**: tensorflow-android:1.9.0\r\n- **Python version**: no python\r\n- **Bazel version (if compiling from source)**: no bazel\r\n- **GCC/Compiler version (if compiling from source)**: no compiler\r\n- **CUDA/cuDNN version**: no version\r\n- **GPU model and memory**: Mobile\r\n- **Exact command to reproduce**: initialize tensorflowInferenceInterface with a InputBuffer > 17 MO\r\n\r\n### Describe the problem\r\nI use Tensorflow Lite since Tensorflow mobile is not ready for production yet.\r\n\r\nWhen I tried to directly load Inception V3 from a file from External storage, it returns a OOM error\r\n\r\nBy looking at the code, I saw : \r\n      `byte[] buf = new byte[16384];` \r\nat the line 143 of TensorflowInferenceInterface\r\n\r\nbut my network uses more than 16384\r\nI think it should use baosInitSize\r\n      `int baosInitSize = is.available() > 16384 ? is.available() : 16384;` at line 140\r\n\r\n### Source code / logs\r\njava.lang.OutOfMemoryError: Failed to allocate a 87514870 byte allocation with 16773184 free bytes and 29MB until OOM\r\n"}