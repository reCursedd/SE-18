{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422846332", "html_url": "https://github.com/tensorflow/tensorflow/issues/22329#issuecomment-422846332", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22329", "id": 422846332, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjg0NjMzMg==", "user": {"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T15:25:51Z", "updated_at": "2018-09-19T15:25:51Z", "author_association": "MEMBER", "body_html": "<p><code>TensorflowInferenceInterface</code> is actually a component of TensorFlow Mobile. If you want to use TensorFlow Lite (which <em>is</em> ready for production, and is the officially recommended path for mobile deployment), you can follow this <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md#java\">API guide</a> and this <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/demo_android.md\">example</a>.</p>\n<p>As for the issue you're facing, the code in <code>TensorFlowInterface</code> looks fine; <code>buf</code> is intentionally restricted in size to avoid extremely large temporary buffer allocations. Note the way it's used as a streaming buffer. Have you tried setting the <code>largeHeap</code> <a href=\"https://developer.android.com/guide/topics/manifest/application-element#largeHeap\" rel=\"nofollow\">manifest attribute</a>?</p>\n<p>In any case, I would strongly encourage taking a look at this <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/demo_android.md\">example</a> for TensorFlow Lite; it uses MobileNet for classification, a model specifically optimized for resource-constrained devices, and should have a much smaller memory (and latency) footprint than Inception. TensorFlow Lite also allows you to use a MappedByteBuffer when providing the model, which avoids using the JVM heap entirely (see this <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L183\">snippet of code</a>).</p>", "body_text": "TensorflowInferenceInterface is actually a component of TensorFlow Mobile. If you want to use TensorFlow Lite (which is ready for production, and is the officially recommended path for mobile deployment), you can follow this API guide and this example.\nAs for the issue you're facing, the code in TensorFlowInterface looks fine; buf is intentionally restricted in size to avoid extremely large temporary buffer allocations. Note the way it's used as a streaming buffer. Have you tried setting the largeHeap manifest attribute?\nIn any case, I would strongly encourage taking a look at this example for TensorFlow Lite; it uses MobileNet for classification, a model specifically optimized for resource-constrained devices, and should have a much smaller memory (and latency) footprint than Inception. TensorFlow Lite also allows you to use a MappedByteBuffer when providing the model, which avoids using the JVM heap entirely (see this snippet of code).", "body": "`TensorflowInferenceInterface` is actually a component of TensorFlow Mobile. If you want to use TensorFlow Lite (which *is* ready for production, and is the officially recommended path for mobile deployment), you can follow this [API guide](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/apis.md#java) and this [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/demo_android.md).\r\n\r\nAs for the issue you're facing, the code in `TensorFlowInterface` looks fine; `buf` is intentionally restricted in size to avoid extremely large temporary buffer allocations. Note the way it's used as a streaming buffer. Have you tried setting the `largeHeap` [manifest attribute](https://developer.android.com/guide/topics/manifest/application-element#largeHeap)?\r\n\r\nIn any case, I would strongly encourage taking a look at this [example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/demo_android.md) for TensorFlow Lite; it uses MobileNet for classification, a model specifically optimized for resource-constrained devices, and should have a much smaller memory (and latency) footprint than Inception. TensorFlow Lite also allows you to use a MappedByteBuffer when providing the model, which avoids using the JVM heap entirely (see this [snippet of code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/java/demo/app/src/main/java/com/example/android/tflitecamerademo/ImageClassifier.java#L183))."}