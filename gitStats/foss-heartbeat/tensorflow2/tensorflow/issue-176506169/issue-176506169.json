{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4342", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4342/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4342/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4342/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4342", "id": 176506169, "node_id": "MDU6SXNzdWUxNzY1MDYxNjk=", "number": 4342, "title": "Invert gradient op", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-09-12T22:58:23Z", "updated_at": "2016-09-15T22:05:34Z", "closed_at": "2016-09-15T22:05:34Z", "author_association": "NONE", "body_html": "<p>This one should be pretty simple. It would be nice to have an op that acts as the identity function in feedforward, but during backpropagation it propagates the negative of the gradients. This is useful when joining two datasets as it allows for the suppression of features that distinguish between domains. Its referred to as a gradient reversal layer in <a href=\"http://arxiv.org/pdf/1505.07818v4.pdf\" rel=\"nofollow\">http://arxiv.org/pdf/1505.07818v4.pdf</a></p>", "body_text": "This one should be pretty simple. It would be nice to have an op that acts as the identity function in feedforward, but during backpropagation it propagates the negative of the gradients. This is useful when joining two datasets as it allows for the suppression of features that distinguish between domains. Its referred to as a gradient reversal layer in http://arxiv.org/pdf/1505.07818v4.pdf", "body": "This one should be pretty simple. It would be nice to have an op that acts as the identity function in feedforward, but during backpropagation it propagates the negative of the gradients. This is useful when joining two datasets as it allows for the suppression of features that distinguish between domains. Its referred to as a gradient reversal layer in [http://arxiv.org/pdf/1505.07818v4.pdf](http://arxiv.org/pdf/1505.07818v4.pdf)\n"}