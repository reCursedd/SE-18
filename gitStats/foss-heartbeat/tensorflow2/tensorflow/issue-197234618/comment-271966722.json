{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271966722", "html_url": "https://github.com/tensorflow/tensorflow/issues/6460#issuecomment-271966722", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6460", "id": 271966722, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTk2NjcyMg==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-11T19:19:09Z", "updated_at": "2017-01-11T19:19:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=592670\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/concretevitamin\">@concretevitamin</a> Thanks for the hopefulness, but alas I don't think any of my recent optimizations will make this better.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9952727\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Bobrosoft98\">@Bobrosoft98</a> What is your CPU utilization like on the PS machine when running Adam with one worker? From a quick glance it looks like the ops duration might be dilating because of contention for the threadpools (in particular the intra-op parallelism) that do parallel work in a TF server. If 30 workers all attempts to send their gradients at approximately the same time, there will be 30x the amount of work to do at the PS.</p>\n<p>One possible mitigation would be to use <code>tf.train.SyncReplicasOptimizer</code> to wrap the Adam optimizer. This would have the benefit that all of the gradients from the different workers would be aggregated (relatively cheaply) before the Adam update rule is evaluated. This should cut down on the amount of work, compared to an asynchronous setup that processes the incoming batches separately. (It's not a free lunch... the synchronous replication is more vulnerable to stragglers, and a little more tricky to set up... but it might be worth it for your case.)</p>", "body_text": "@concretevitamin Thanks for the hopefulness, but alas I don't think any of my recent optimizations will make this better.\n@Bobrosoft98 What is your CPU utilization like on the PS machine when running Adam with one worker? From a quick glance it looks like the ops duration might be dilating because of contention for the threadpools (in particular the intra-op parallelism) that do parallel work in a TF server. If 30 workers all attempts to send their gradients at approximately the same time, there will be 30x the amount of work to do at the PS.\nOne possible mitigation would be to use tf.train.SyncReplicasOptimizer to wrap the Adam optimizer. This would have the benefit that all of the gradients from the different workers would be aggregated (relatively cheaply) before the Adam update rule is evaluated. This should cut down on the amount of work, compared to an asynchronous setup that processes the incoming batches separately. (It's not a free lunch... the synchronous replication is more vulnerable to stragglers, and a little more tricky to set up... but it might be worth it for your case.)", "body": "@concretevitamin Thanks for the hopefulness, but alas I don't think any of my recent optimizations will make this better.\r\n\r\n@Bobrosoft98 What is your CPU utilization like on the PS machine when running Adam with one worker? From a quick glance it looks like the ops duration might be dilating because of contention for the threadpools (in particular the intra-op parallelism) that do parallel work in a TF server. If 30 workers all attempts to send their gradients at approximately the same time, there will be 30x the amount of work to do at the PS.\r\n\r\nOne possible mitigation would be to use `tf.train.SyncReplicasOptimizer` to wrap the Adam optimizer. This would have the benefit that all of the gradients from the different workers would be aggregated (relatively cheaply) before the Adam update rule is evaluated. This should cut down on the amount of work, compared to an asynchronous setup that processes the incoming batches separately. (It's not a free lunch... the synchronous replication is more vulnerable to stragglers, and a little more tricky to set up... but it might be worth it for your case.)"}