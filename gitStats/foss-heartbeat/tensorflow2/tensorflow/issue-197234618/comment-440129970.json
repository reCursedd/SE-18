{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/440129970", "html_url": "https://github.com/tensorflow/tensorflow/issues/6460#issuecomment-440129970", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6460", "id": 440129970, "node_id": "MDEyOklzc3VlQ29tbWVudDQ0MDEyOTk3MA==", "user": {"login": "mpjlu", "id": 13826327, "node_id": "MDQ6VXNlcjEzODI2MzI3", "avatar_url": "https://avatars1.githubusercontent.com/u/13826327?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpjlu", "html_url": "https://github.com/mpjlu", "followers_url": "https://api.github.com/users/mpjlu/followers", "following_url": "https://api.github.com/users/mpjlu/following{/other_user}", "gists_url": "https://api.github.com/users/mpjlu/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpjlu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpjlu/subscriptions", "organizations_url": "https://api.github.com/users/mpjlu/orgs", "repos_url": "https://api.github.com/users/mpjlu/repos", "events_url": "https://api.github.com/users/mpjlu/events{/privacy}", "received_events_url": "https://api.github.com/users/mpjlu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-20T04:00:19Z", "updated_at": "2018-11-20T04:00:19Z", "author_association": "NONE", "body_html": "<p>SparseAdam in pytorch is same as Tensorflow LazyAdam. The convergence of LazyAdam is lower than Adam.  The main problem is if the gradient is 0, we don't update m, and v.<br>\nActually, m and v should be updated when other weights are updated.  This is why LazyAdam convergence is low.<br>\nI have proposed a method to fix this issue. The performance is about the same as LazyAdam, and the convergence is the same as Adam.<br>\nCould you please review my method:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"379596611\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/23668\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/23668/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/23668\">#23668</a></p>", "body_text": "SparseAdam in pytorch is same as Tensorflow LazyAdam. The convergence of LazyAdam is lower than Adam.  The main problem is if the gradient is 0, we don't update m, and v.\nActually, m and v should be updated when other weights are updated.  This is why LazyAdam convergence is low.\nI have proposed a method to fix this issue. The performance is about the same as LazyAdam, and the convergence is the same as Adam.\nCould you please review my method:\n#23668", "body": "SparseAdam in pytorch is same as Tensorflow LazyAdam. The convergence of LazyAdam is lower than Adam.  The main problem is if the gradient is 0, we don't update m, and v. \r\nActually, m and v should be updated when other weights are updated.  This is why LazyAdam convergence is low.\r\nI have proposed a method to fix this issue. The performance is about the same as LazyAdam, and the convergence is the same as Adam.\r\nCould you please review my method: \r\nhttps://github.com/tensorflow/tensorflow/issues/23668\r\n"}