{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225039228", "html_url": "https://github.com/tensorflow/tensorflow/issues/842#issuecomment-225039228", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/842", "id": 225039228, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTAzOTIyOA==", "user": {"login": "kingtaurus", "id": 2761482, "node_id": "MDQ6VXNlcjI3NjE0ODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/2761482?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kingtaurus", "html_url": "https://github.com/kingtaurus", "followers_url": "https://api.github.com/users/kingtaurus/followers", "following_url": "https://api.github.com/users/kingtaurus/following{/other_user}", "gists_url": "https://api.github.com/users/kingtaurus/gists{/gist_id}", "starred_url": "https://api.github.com/users/kingtaurus/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kingtaurus/subscriptions", "organizations_url": "https://api.github.com/users/kingtaurus/orgs", "repos_url": "https://api.github.com/users/kingtaurus/repos", "events_url": "https://api.github.com/users/kingtaurus/events{/privacy}", "received_events_url": "https://api.github.com/users/kingtaurus/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-09T21:57:29Z", "updated_at": "2016-06-09T21:57:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Quick workaround;<br>\nI believe some of visualization features of this tool can be handled directly within tensorflow (and combined with tensorboard) with a bit of effort.</p>\n<p>Issues:</p>\n<ol>\n<li>the layout is not ideal (and there are issues with layout of images in general);</li>\n<li>extra code means an increased chance for mistakes;</li>\n<li>it seems to make the processing much slower :);</li>\n<li>it doesn't take into account max_pooling (or other spatial contracting operations);</li>\n</ol>\n<p>This provides visualization of output images of each layer (which is part of the requirements - but is in-efficient  <g-emoji class=\"g-emoji\" alias=\"-1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44e.png\">\ud83d\udc4e</g-emoji> );</p>\n<pre><code>W_conv1 = weight_variable([3,3,1,128])\n#f_x,f_y,depth, number of filters\nb_conv1 = bias_variable([128])\ncnn_layer_1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\nW_conv2 = weight_variable([3,3,128,128])\nb_conv2 = bias_variable([128])\ncnn_layer_2 = tf.nn.relu(conv2d(cnn_layer_1, W_conv2) + b_conv2)\n\nW_conv3 = weight_variable([3,3,128,128])\nb_conv3 = bias_variable([128])\ncnn_layer_3 = tf.nn.relu(conv2d(cnn_layer_2, W_conv3) + b_conv3)\n\n#grab only the first element of the batch and 16 filters\nlayer1_image1 = cnn_layer_1[0:1, :, :, 0:16]\nlayer2_image1 = cnn_layer_2[0:1, :, :, 0:16]\nlayer3_image1 = cnn_layer_3[0:1, :, :, 0:16]\n\nlayer1_image1 = tf.transpose(layer1_image1, perm=[3,1,2,0])\nlayer2_image1 = tf.transpose(layer2_image1, perm=[3,1,2,0])\nlayer3_image1 = tf.transpose(layer3_image1, perm=[3,1,2,0])\n\nlayer_combine_1 = tf.concat(2, [layer3_image1, layer2_image1, layer1_image1])\nlist_lc1 = tf.split(0, 16, layer_combine_1)\nlayer_combine_1 = tf.concat(1, list_lc1)\n\ntf.image_summary(\"filtered_images_1\", layer_combine_1)\n# combine this summary with tensorboard (and you get a decent output);\n</code></pre>\n<p>Looks like (using MNIST):<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/2761482/15947688/ba56104a-2e51-11e6-8dfc-69c9e3f4b1b3.png\"><img src=\"https://cloud.githubusercontent.com/assets/2761482/15947688/ba56104a-2e51-11e6-8dfc-69c9e3f4b1b3.png\" alt=\"tensorboard-tiled-filter-output\" style=\"max-width:100%;\"></a></p>\n<p>The other parts would requires a little bit more work (some involve backprop).</p>", "body_text": "Quick workaround;\nI believe some of visualization features of this tool can be handled directly within tensorflow (and combined with tensorboard) with a bit of effort.\nIssues:\n\nthe layout is not ideal (and there are issues with layout of images in general);\nextra code means an increased chance for mistakes;\nit seems to make the processing much slower :);\nit doesn't take into account max_pooling (or other spatial contracting operations);\n\nThis provides visualization of output images of each layer (which is part of the requirements - but is in-efficient  \ud83d\udc4e );\nW_conv1 = weight_variable([3,3,1,128])\n#f_x,f_y,depth, number of filters\nb_conv1 = bias_variable([128])\ncnn_layer_1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\nW_conv2 = weight_variable([3,3,128,128])\nb_conv2 = bias_variable([128])\ncnn_layer_2 = tf.nn.relu(conv2d(cnn_layer_1, W_conv2) + b_conv2)\n\nW_conv3 = weight_variable([3,3,128,128])\nb_conv3 = bias_variable([128])\ncnn_layer_3 = tf.nn.relu(conv2d(cnn_layer_2, W_conv3) + b_conv3)\n\n#grab only the first element of the batch and 16 filters\nlayer1_image1 = cnn_layer_1[0:1, :, :, 0:16]\nlayer2_image1 = cnn_layer_2[0:1, :, :, 0:16]\nlayer3_image1 = cnn_layer_3[0:1, :, :, 0:16]\n\nlayer1_image1 = tf.transpose(layer1_image1, perm=[3,1,2,0])\nlayer2_image1 = tf.transpose(layer2_image1, perm=[3,1,2,0])\nlayer3_image1 = tf.transpose(layer3_image1, perm=[3,1,2,0])\n\nlayer_combine_1 = tf.concat(2, [layer3_image1, layer2_image1, layer1_image1])\nlist_lc1 = tf.split(0, 16, layer_combine_1)\nlayer_combine_1 = tf.concat(1, list_lc1)\n\ntf.image_summary(\"filtered_images_1\", layer_combine_1)\n# combine this summary with tensorboard (and you get a decent output);\n\nLooks like (using MNIST):\n\nThe other parts would requires a little bit more work (some involve backprop).", "body": "Quick workaround;\nI believe some of visualization features of this tool can be handled directly within tensorflow (and combined with tensorboard) with a bit of effort.\n\nIssues: \n1. the layout is not ideal (and there are issues with layout of images in general);\n2. extra code means an increased chance for mistakes;\n3. it seems to make the processing much slower :);\n4. it doesn't take into account max_pooling (or other spatial contracting operations);\n\nThis provides visualization of output images of each layer (which is part of the requirements - but is in-efficient  :-1: );\n\n```\nW_conv1 = weight_variable([3,3,1,128])\n#f_x,f_y,depth, number of filters\nb_conv1 = bias_variable([128])\ncnn_layer_1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\nW_conv2 = weight_variable([3,3,128,128])\nb_conv2 = bias_variable([128])\ncnn_layer_2 = tf.nn.relu(conv2d(cnn_layer_1, W_conv2) + b_conv2)\n\nW_conv3 = weight_variable([3,3,128,128])\nb_conv3 = bias_variable([128])\ncnn_layer_3 = tf.nn.relu(conv2d(cnn_layer_2, W_conv3) + b_conv3)\n\n#grab only the first element of the batch and 16 filters\nlayer1_image1 = cnn_layer_1[0:1, :, :, 0:16]\nlayer2_image1 = cnn_layer_2[0:1, :, :, 0:16]\nlayer3_image1 = cnn_layer_3[0:1, :, :, 0:16]\n\nlayer1_image1 = tf.transpose(layer1_image1, perm=[3,1,2,0])\nlayer2_image1 = tf.transpose(layer2_image1, perm=[3,1,2,0])\nlayer3_image1 = tf.transpose(layer3_image1, perm=[3,1,2,0])\n\nlayer_combine_1 = tf.concat(2, [layer3_image1, layer2_image1, layer1_image1])\nlist_lc1 = tf.split(0, 16, layer_combine_1)\nlayer_combine_1 = tf.concat(1, list_lc1)\n\ntf.image_summary(\"filtered_images_1\", layer_combine_1)\n# combine this summary with tensorboard (and you get a decent output);\n```\n\nLooks like (using MNIST):\n![tensorboard-tiled-filter-output](https://cloud.githubusercontent.com/assets/2761482/15947688/ba56104a-2e51-11e6-8dfc-69c9e3f4b1b3.png)\n\nThe other parts would requires a little bit more work (some involve backprop).\n"}