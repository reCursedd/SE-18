{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260310293", "html_url": "https://github.com/tensorflow/tensorflow/issues/5543#issuecomment-260310293", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5543", "id": 260310293, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDMxMDI5Mw==", "user": {"login": "DavidNorman", "id": 606831, "node_id": "MDQ6VXNlcjYwNjgzMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/606831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNorman", "html_url": "https://github.com/DavidNorman", "followers_url": "https://api.github.com/users/DavidNorman/followers", "following_url": "https://api.github.com/users/DavidNorman/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNorman/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNorman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNorman/subscriptions", "organizations_url": "https://api.github.com/users/DavidNorman/orgs", "repos_url": "https://api.github.com/users/DavidNorman/repos", "events_url": "https://api.github.com/users/DavidNorman/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNorman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-14T11:17:48Z", "updated_at": "2016-11-14T11:17:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant:</p>\n<pre><code>  // 1) If the destination tensor is not an int32 tensor, and has HOST_MEMORY\n  // constraint, do not replace it.\n  // 2) If the destination tensor is an int32 tensor, but has DEVICE_MEMORY\n  // constraint, do not replace it.\n</code></pre>\n<p>Why would you want to keep nodes that perform arithmetic on a device unnecessarily?  I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding (although on a neural network I would hope that the result would work around that).</p>\n<p>In the case of integers, I can't decide why constant folding would always take place. Could you describe why it would be limited to HOST_MEMORY only?</p>", "body_text": "I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant:\n  // 1) If the destination tensor is not an int32 tensor, and has HOST_MEMORY\n  // constraint, do not replace it.\n  // 2) If the destination tensor is an int32 tensor, but has DEVICE_MEMORY\n  // constraint, do not replace it.\n\nWhy would you want to keep nodes that perform arithmetic on a device unnecessarily?  I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding (although on a neural network I would hope that the result would work around that).\nIn the case of integers, I can't decide why constant folding would always take place. Could you describe why it would be limited to HOST_MEMORY only?", "body": "I am also wondering what is the rationale behind the memory constraints in the ReplaceTensorWithConstant:\n\n```\n  // 1) If the destination tensor is not an int32 tensor, and has HOST_MEMORY\n  // constraint, do not replace it.\n  // 2) If the destination tensor is an int32 tensor, but has DEVICE_MEMORY\n  // constraint, do not replace it.\n```\n\nWhy would you want to keep nodes that perform arithmetic on a device unnecessarily?  I can imagine that you might consider that it is important to do floating point arithmetic on the target platform because of differences in rounding (although on a neural network I would hope that the result would work around that).\n\nIn the case of integers, I can't decide why constant folding would always take place. Could you describe why it would be limited to HOST_MEMORY only?  \n"}