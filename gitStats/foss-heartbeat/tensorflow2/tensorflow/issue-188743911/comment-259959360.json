{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259959360", "html_url": "https://github.com/tensorflow/tensorflow/issues/5543#issuecomment-259959360", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5543", "id": 259959360, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTk1OTM2MA==", "user": {"login": "DavidNorman", "id": 606831, "node_id": "MDQ6VXNlcjYwNjgzMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/606831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNorman", "html_url": "https://github.com/DavidNorman", "followers_url": "https://api.github.com/users/DavidNorman/followers", "following_url": "https://api.github.com/users/DavidNorman/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNorman/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNorman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNorman/subscriptions", "organizations_url": "https://api.github.com/users/DavidNorman/orgs", "repos_url": "https://api.github.com/users/DavidNorman/repos", "events_url": "https://api.github.com/users/DavidNorman/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNorman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-11T13:41:13Z", "updated_at": "2016-11-11T13:41:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One more example.  This is the 'getting_started.py' that is in the TF documentation.  I have modified it so that it uses placeholders, rather than allowing the X and Y data to be constants.</p>\n<pre><code>Graph Before #nodes 67 #edges 109\n|| \n|| () -&gt; () {\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()\n||   n12 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 1&gt;]()\n||   n14 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()\n||   n16 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n18 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n19 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()\n||   n20 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()\n||   n22 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()\n||   n24 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [] values: 1&gt;]()\n||   n32 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n33 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n42 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n43 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()\n||   n54 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()\n||   n55 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n63 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 0.5&gt;]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 2&gt;]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n28, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n\nReplacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:ipu:0\"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant\n\nGraph After #nodes 68 #edges 110\n|| \n|| () -&gt; () {\n||   n67 = Const[dtype=float, value=Tensor&lt;type: float shape: [100] values: 0.01 0.01 0.01...&gt;]()\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()\n||   n12 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 1&gt;]()\n||   n14 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()\n||   n16 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n18 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n19 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [0] values: &gt;]()\n||   n20 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()\n||   n22 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 0&gt;]()\n||   n24 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [] values: 1&gt;]()\n||   n32 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n33 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n42 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n43 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()\n||   n54 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 1&gt;]()\n||   n55 = Const[dtype=int32, value=Tensor&lt;type: int32 shape: [1] values: 100&gt;]()\n||   n63 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 0.5&gt;]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor&lt;type: float shape: [] values: 2&gt;]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n67, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n</code></pre>\n<p>Node n29, which has a control dependency on n28 is the problem.  n28 was the Div which was replaced.</p>", "body_text": "One more example.  This is the 'getting_started.py' that is in the TF documentation.  I have modified it so that it uses placeholders, rather than allowing the X and Y data to be constants.\nGraph Before #nodes 67 #edges 109\n|| \n|| () -> () {\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()\n||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()\n||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n28, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n\nReplacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:ipu:0\"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant\n\nGraph After #nodes 68 #edges 110\n|| \n|| () -> () {\n||   n67 = Const[dtype=float, value=Tensor<type: float shape: [100] values: 0.01 0.01 0.01...>]()\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()\n||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()\n||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n67, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n\nNode n29, which has a control dependency on n28 is the problem.  n28 was the Div which was replaced.", "body": "One more example.  This is the 'getting_started.py' that is in the TF documentation.  I have modified it so that it uses placeholders, rather than allowing the X and Y data to be constants.\n\n```\nGraph Before #nodes 67 #edges 109\n|| \n|| () -> () {\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()\n||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()\n||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n28, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n\nReplacing {name:'gradients/Mean_grad/truediv' id:28 op device:{/job:localhost/replica:0/task:0/device:ipu:0} def:{gradients/Mean_grad/truediv = Div[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:ipu:0\"](gradients/Mean_grad/Tile, gradients/Mean_grad/Cast)}} :: 0 with a constant\n\nGraph After #nodes 68 #edges 110\n|| \n|| () -> () {\n||   n67 = Const[dtype=float, value=Tensor<type: float shape: [100] values: 0.01 0.01 0.01...>]()\n||   n2 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_110__recv_Placeholder_1_0\", tensor_type=float]()\n||   n3 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:ipu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_109__recv_Placeholder_0\", tensor_type=float]()\n||   n4 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n7 = Variable[container=\"\", dtype=float, shape=[1], shared_name=\"\"]()\n||   n11 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n12 = Const[dtype=float, value=Tensor<type: float shape: [] values: 1>]()\n||   n14 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n16 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n18 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n19 = Const[dtype=int32, value=Tensor<type: int32 shape: [0] values: >]()\n||   n20 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n22 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 0>]()\n||   n24 = Const[dtype=int32, value=Tensor<type: int32 shape: [] values: 1>]()\n||   n32 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n33 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n42 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n43 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n54 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 1>]()\n||   n55 = Const[dtype=int32, value=Tensor<type: int32 shape: [1] values: 100>]()\n||   n63 = Const[dtype=float, value=Tensor<type: float shape: [] values: 0.5>]()\n||   n5 = Identity[T=float, _class=[\"loc:@Variable\"]](n4)\n||   n8 = Identity[T=float, _class=[\"loc:@Variable_1\"]](n7)\n||   n13 = Fill[T=float](n11, n12)\n||   n21 = Prod[T=int32, Tidx=int32, keep_dims=false](n18, n20)\n||   n23 = Prod[T=int32, Tidx=int32, keep_dims=false](n19, n22)\n||   n34 = BroadcastGradientArgs[T=int32](n32, n33)\n||   n44 = BroadcastGradientArgs[T=int32](n42, n43)\n||   n56 = BroadcastGradientArgs[T=int32](n54, n55)\n||   n6 = Mul[T=float](n5, n3)\n||   n15 = Reshape[T=float, Tshape=int32](n13, n14)\n||   n25 = Maximum[T=int32](n23, n24)\n||   n9 = Add[T=float](n6, n8)\n||   n17 = Tile[T=float, Tmultiples=int32](n15, n16)\n||   n26 = Div[T=int32](n21, n25)\n||   n10 = Sub[T=float](n9, n2)\n||   n27 = Cast[DstT=float, SrcT=int32](n26)\n||   n28 = Div[T=float](n17, n27)\n||   n29 = Const[dtype=float, value=Tensor<type: float shape: [] values: 2>]() @ n28\n||   n30 = Mul[T=float](n29, n10)\n||   n31 = Mul[T=float](n67, n30)\n||   n35 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34)\n||   n37 = Sum[T=float, Tidx=int32, keep_dims=false](n31, n34:1)\n||   n36 = Reshape[T=float, Tshape=int32](n35, n32)\n||   n38 = Neg[T=float](n37)\n||   n39 = Reshape[T=float, Tshape=int32](n38, n33)\n||   n40 = NoOp() @ n36, n39\n||   n41 = Identity[T=float, _class=[\"loc:@gradients/sub_grad/Reshape\"]](n36) @ n40\n||   n45 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44)\n||   n47 = Sum[T=float, Tidx=int32, keep_dims=false](n41, n44:1)\n||   n46 = Reshape[T=float, Tshape=int32](n45, n42)\n||   n48 = Reshape[T=float, Tshape=int32](n47, n43)\n||   n49 = NoOp() @ n46, n48\n||   n50 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape\"]](n46) @ n49\n||   n53 = Identity[T=float, _class=[\"loc:@gradients/add_grad/Reshape_1\"]](n48) @ n49\n||   n51 = Mul[T=float](n50, n3)\n||   n52 = Mul[T=float](n5, n50)\n||   n65 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable_1\"], use_locking=false](n7, n63, n53)\n||   n57 = Sum[T=float, Tidx=int32, keep_dims=false](n51, n56)\n||   n59 = Sum[T=float, Tidx=int32, keep_dims=false](n52, n56:1)\n||   n58 = Reshape[T=float, Tshape=int32](n57, n54)\n||   n60 = Reshape[T=float, Tshape=int32](n59, n55)\n||   n61 = NoOp() @ n58, n60\n||   n62 = Identity[T=float, _class=[\"loc:@gradients/mul_grad/Reshape\"]](n58) @ n61\n||   n64 = ApplyGradientDescent[T=float, _class=[\"loc:@Variable\"], use_locking=false](n4, n63, n62)\n||   n66 = NoOp() @ n64, n65\n|| }\n|| \n\n```\n\nNode n29, which has a control dependency on n28 is the problem.  n28 was the Div which was replaced.\n"}