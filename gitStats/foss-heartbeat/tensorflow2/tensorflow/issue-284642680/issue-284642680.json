{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15651", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15651/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15651/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15651/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15651", "id": 284642680, "node_id": "MDU6SXNzdWUyODQ2NDI2ODA=", "number": 15651, "title": "Not found: FeedInputs: unable to find feed output Mul", "user": {"login": "amirjamez", "id": 8922398, "node_id": "MDQ6VXNlcjg5MjIzOTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/8922398?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amirjamez", "html_url": "https://github.com/amirjamez", "followers_url": "https://api.github.com/users/amirjamez/followers", "following_url": "https://api.github.com/users/amirjamez/following{/other_user}", "gists_url": "https://api.github.com/users/amirjamez/gists{/gist_id}", "starred_url": "https://api.github.com/users/amirjamez/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amirjamez/subscriptions", "organizations_url": "https://api.github.com/users/amirjamez/orgs", "repos_url": "https://api.github.com/users/amirjamez/repos", "events_url": "https://api.github.com/users/amirjamez/events{/privacy}", "received_events_url": "https://api.github.com/users/amirjamez/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, {"login": "rockyrhodes", "id": 7253968, "node_id": "MDQ6VXNlcjcyNTM5Njg=", "avatar_url": "https://avatars2.githubusercontent.com/u/7253968?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rockyrhodes", "html_url": "https://github.com/rockyrhodes", "followers_url": "https://api.github.com/users/rockyrhodes/followers", "following_url": "https://api.github.com/users/rockyrhodes/following{/other_user}", "gists_url": "https://api.github.com/users/rockyrhodes/gists{/gist_id}", "starred_url": "https://api.github.com/users/rockyrhodes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rockyrhodes/subscriptions", "organizations_url": "https://api.github.com/users/rockyrhodes/orgs", "repos_url": "https://api.github.com/users/rockyrhodes/repos", "events_url": "https://api.github.com/users/rockyrhodes/events{/privacy}", "received_events_url": "https://api.github.com/users/rockyrhodes/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-12-27T03:41:28Z", "updated_at": "2018-11-20T07:51:52Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20085789\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/satok16\">@satok16</a> I have already set up and was able to run <code>hexagon_graph_execution</code> on my hvx board, however, when I tried to use my own <a href=\"https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models\">inception-v3 pre-trained model</a> that I froze using <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">this quantization method</a>, I am receiving this error:</p>\n<pre><code>[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\nnative : hexagon_graph_execution_test.cc:519 Fuse and run inception v3 on hexagon with tf runtime\nnative : hexagon_graph_execution_test.cc:94 Hexagon controller version is 90\nnative : hexagon_graph_execution_test.cc:142 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\nnative : hexagon_graph_execution_test.cc:148 header size = 54\nnative : hexagon_graph_execution_test.cc:150 image size = 40\nnative : hexagon_graph_execution_test.cc:152 width = 299\nnative : hexagon_graph_execution_test.cc:154 height = -299\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\nt1(loading image time)=0.026770\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\nAborted\n</code></pre>\n<p>Do you know if the issue is because of an incorrect input argument here:</p>\n<pre><code>curl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\n  tar -C tensorflow/examples/label_image/data -xz\nbazel build tensorflow/tools/graph_transforms:transform_graph\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\n  --out_graph=/tmp/quantized_graph.pb \\\n  **--inputs=input \\**\n  --outputs=InceptionV3/Predictions/Reshape_1 \\\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\n    strip_unused_nodes sort_by_execution_order'\n</code></pre>\n<p>I found on [this] (<a href=\"https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input\" rel=\"nofollow\">https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input</a>) and also [this]<a href=\"url\">https://github.com/tensorflow/tensorflow/issues/2883#issuecomment-226591095</a> posts that we might have to use <code>Mul</code> instead. Tried that with no success. Interestingly, when I test my frozen_quantized graph with:</p>\n<p><code>bazel-bin/tensorflow/examples/label_image/label_image --graph=/tmp/my_inception_quantized_graph_hvx.pb</code></p>\n<p>I receive similar results compared to a non-quantized version, so it shows that my frozen_quantized is not faulty. Can you verify the issue here?<br>\nWas the file <code>https://storage.googleapis.com/download.tensorflow.org/models/tensorflow_inception_v3_stripped_optimized_quantized.pb</code> used in the original hvx hexgon_graph_execution produced differently?</p>", "body_text": "@satok16 I have already set up and was able to run hexagon_graph_execution on my hvx board, however, when I tried to use my own inception-v3 pre-trained model that I froze using this quantization method, I am receiving this error:\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\nnative : hexagon_graph_execution_test.cc:519 Fuse and run inception v3 on hexagon with tf runtime\nnative : hexagon_graph_execution_test.cc:94 Hexagon controller version is 90\nnative : hexagon_graph_execution_test.cc:142 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\nnative : hexagon_graph_execution_test.cc:148 header size = 54\nnative : hexagon_graph_execution_test.cc:150 image size = 40\nnative : hexagon_graph_execution_test.cc:152 width = 299\nnative : hexagon_graph_execution_test.cc:154 height = -299\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\nt1(loading image time)=0.026770\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\nAborted\n\nDo you know if the issue is because of an incorrect input argument here:\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\n  tar -C tensorflow/examples/label_image/data -xz\nbazel build tensorflow/tools/graph_transforms:transform_graph\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\n  --out_graph=/tmp/quantized_graph.pb \\\n  **--inputs=input \\**\n  --outputs=InceptionV3/Predictions/Reshape_1 \\\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\n    strip_unused_nodes sort_by_execution_order'\n\nI found on [this] (https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input) and also [this]https://github.com/tensorflow/tensorflow/issues/2883#issuecomment-226591095 posts that we might have to use Mul instead. Tried that with no success. Interestingly, when I test my frozen_quantized graph with:\nbazel-bin/tensorflow/examples/label_image/label_image --graph=/tmp/my_inception_quantized_graph_hvx.pb\nI receive similar results compared to a non-quantized version, so it shows that my frozen_quantized is not faulty. Can you verify the issue here?\nWas the file https://storage.googleapis.com/download.tensorflow.org/models/tensorflow_inception_v3_stripped_optimized_quantized.pb used in the original hvx hexgon_graph_execution produced differently?", "body": "@satok16 I have already set up and was able to run `hexagon_graph_execution` on my hvx board, however, when I tried to use my own [inception-v3 pre-trained model](https://github.com/tensorflow/models/tree/master/research/slim#pre-trained-models) that I froze using [this quantization method](https://www.tensorflow.org/performance/quantization), I am receiving this error:\r\n\r\n```\r\n[ RUN      ] GraphTransferer.RunInceptionV3OnHexagonExampleWithTfRuntime\r\nnative : hexagon_graph_execution_test.cc:519 Fuse and run inception v3 on hexagon with tf runtime\r\nnative : hexagon_graph_execution_test.cc:94 Hexagon controller version is 90\r\nnative : hexagon_graph_execution_test.cc:142 Read /data/local/tmp/img_299x299.bmp, size = 269156bytes\r\nnative : hexagon_graph_execution_test.cc:148 header size = 54\r\nnative : hexagon_graph_execution_test.cc:150 image size = 40\r\nnative : hexagon_graph_execution_test.cc:152 width = 299\r\nnative : hexagon_graph_execution_test.cc:154 height = -299\r\nnative : hexagon_graph_execution_test.cc:533 Ioading image finished.\r\nt1(loading image time)=0.026770\r\nnative : hexagon_graph_execution_test.cc:546 Build fused graph\r\nnative : remote_fused_graph_execute_utils.cc:259 Error during inference: Not found: FeedInputs: unable to find feed output Mul\r\nnative : graph_transfer_utils.cc:110 Check failed: status.ok()\r\nAborted\r\n```\r\nDo you know if the issue is because of an incorrect input argument here: \r\n\r\n```\r\ncurl -L \"https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz\" |\r\n  tar -C tensorflow/examples/label_image/data -xz\r\nbazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n  --in_graph=tensorflow/examples/label_image/data/inception_v3_2016_08_28_frozen.pb \\\r\n  --out_graph=/tmp/quantized_graph.pb \\\r\n  **--inputs=input \\**\r\n  --outputs=InceptionV3/Predictions/Reshape_1 \\\r\n  --transforms='add_default_attributes strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n    remove_nodes(op=Identity, op=CheckNumerics) fold_constants(ignore_errors=true)\r\n    fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes\r\n    strip_unused_nodes sort_by_execution_order'\r\n```\r\n\r\nI found on [this] (https://stackoverflow.com/questions/43022516/tensorflow-inception-feedinputs-unable-to-find-feed-output-input) and also [this][https://github.com/tensorflow/tensorflow/issues/2883#issuecomment-226591095](url) posts that we might have to use `Mul` instead. Tried that with no success. Interestingly, when I test my frozen_quantized graph with:\r\n\r\n`bazel-bin/tensorflow/examples/label_image/label_image --graph=/tmp/my_inception_quantized_graph_hvx.pb`\r\n\r\nI receive similar results compared to a non-quantized version, so it shows that my frozen_quantized is not faulty. Can you verify the issue here? \r\nWas the file `https://storage.googleapis.com/download.tensorflow.org/models/tensorflow_inception_v3_stripped_optimized_quantized.pb` used in the original hvx hexgon_graph_execution produced differently?"}