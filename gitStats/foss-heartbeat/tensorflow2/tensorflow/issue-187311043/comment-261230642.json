{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261230642", "html_url": "https://github.com/tensorflow/tensorflow/issues/5396#issuecomment-261230642", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5396", "id": 261230642, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTIzMDY0Mg==", "user": {"login": "pltrdy", "id": 6375843, "node_id": "MDQ6VXNlcjYzNzU4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6375843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pltrdy", "html_url": "https://github.com/pltrdy", "followers_url": "https://api.github.com/users/pltrdy/followers", "following_url": "https://api.github.com/users/pltrdy/following{/other_user}", "gists_url": "https://api.github.com/users/pltrdy/gists{/gist_id}", "starred_url": "https://api.github.com/users/pltrdy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pltrdy/subscriptions", "organizations_url": "https://api.github.com/users/pltrdy/orgs", "repos_url": "https://api.github.com/users/pltrdy/repos", "events_url": "https://api.github.com/users/pltrdy/events{/privacy}", "received_events_url": "https://api.github.com/users/pltrdy/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-17T12:06:01Z", "updated_at": "2016-11-17T14:43:04Z", "author_association": "NONE", "body_html": "<p><strong>Edit:</strong><br>\nEven simpler, I don't get why this code fails (does not terminate nor load cpu/gpu blablabla)</p>\n<pre><code>import tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\nl = tf.placeholder(tf.int32)\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=l, shuffle=False)\nslice_end = range_q.dequeue()\n\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nwith tf.Session() as sess:\n    print(sess.run(y, feed_dict={l: 5}))\n</code></pre>\n<hr>\n<p>I am still having trouble with this ptb_producer. My point now is just not to pass data on graph building (i.e. not in the PTBModel object creation) but on execution through placeholder.</p>\n<p>Thus I modified PTBInput the following way:</p>\n<pre><code>class PTBInput(object):\n  \"\"\"The input data.\"\"\"\n\n  def __init__(self, config, data=None, name=None):\n    self.data = data #tf.placeholder(tf.int32, shape=[None], name=\"data\")\n    datalen = tf.size(self.data)\n\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.epoch_size = ((datalen // batch_size) - 1) // num_steps\n    self.input_data, self.targets = reader.producer(\n        self.data, batch_size, num_steps, name=name)\n</code></pre>\n<p>and I try to run:</p>\n<pre><code>from ptb_word_lm import PTBModel, SmallConfig\nimport reader\n\nconfig = SmallConfig() \ndata_path='./data'\ntrain_data, valid_data, test_data, word_to_id = reader.ptb_raw_data(data_path)\nwith tf.Graph().as_default():\n   initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n    with tf.name_scope(\"Train\"):\n        with tf.variable_scope(\"Model\", reuse=False, initializer=initializer):\n            m = Model(is_training=True, config=config)\n\n    with tf.Session()  as session:\n        m.assign_lr(session, 1.0)\n        state = session.run(m.initial_state)\n        vals = session.run([m.cost, m.final_state], feed_dict={m.initial_state: state, m.data: train_data})\n</code></pre>\n<p>Which should run one training epoch.<br>\nThis code is minimalistic but still does not work. As always, no CPU nor GPU load, just waiting...</p>\n<p>The only difference is that 'data' is not an array anymore but a Tensorf (placeholder), don't see which this induces errors.</p>", "body_text": "Edit:\nEven simpler, I don't get why this code fails (does not terminate nor load cpu/gpu blablabla)\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\nl = tf.placeholder(tf.int32)\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=l, shuffle=False)\nslice_end = range_q.dequeue()\n\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nwith tf.Session() as sess:\n    print(sess.run(y, feed_dict={l: 5}))\n\n\nI am still having trouble with this ptb_producer. My point now is just not to pass data on graph building (i.e. not in the PTBModel object creation) but on execution through placeholder.\nThus I modified PTBInput the following way:\nclass PTBInput(object):\n  \"\"\"The input data.\"\"\"\n\n  def __init__(self, config, data=None, name=None):\n    self.data = data #tf.placeholder(tf.int32, shape=[None], name=\"data\")\n    datalen = tf.size(self.data)\n\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.epoch_size = ((datalen // batch_size) - 1) // num_steps\n    self.input_data, self.targets = reader.producer(\n        self.data, batch_size, num_steps, name=name)\n\nand I try to run:\nfrom ptb_word_lm import PTBModel, SmallConfig\nimport reader\n\nconfig = SmallConfig() \ndata_path='./data'\ntrain_data, valid_data, test_data, word_to_id = reader.ptb_raw_data(data_path)\nwith tf.Graph().as_default():\n   initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n    with tf.name_scope(\"Train\"):\n        with tf.variable_scope(\"Model\", reuse=False, initializer=initializer):\n            m = Model(is_training=True, config=config)\n\n    with tf.Session()  as session:\n        m.assign_lr(session, 1.0)\n        state = session.run(m.initial_state)\n        vals = session.run([m.cost, m.final_state], feed_dict={m.initial_state: state, m.data: train_data})\n\nWhich should run one training epoch.\nThis code is minimalistic but still does not work. As always, no CPU nor GPU load, just waiting...\nThe only difference is that 'data' is not an array anymore but a Tensorf (placeholder), don't see which this induces errors.", "body": "**Edit:**\nEven simpler, I don't get why this code fails (does not terminate nor load cpu/gpu blablabla)\n\n```\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\nl = tf.placeholder(tf.int32)\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=l, shuffle=False)\nslice_end = range_q.dequeue()\n\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nwith tf.Session() as sess:\n    print(sess.run(y, feed_dict={l: 5}))\n```\n\n---\n\nI am still having trouble with this ptb_producer. My point now is just not to pass data on graph building (i.e. not in the PTBModel object creation) but on execution through placeholder.\n\nThus I modified PTBInput the following way:\n\n```\nclass PTBInput(object):\n  \"\"\"The input data.\"\"\"\n\n  def __init__(self, config, data=None, name=None):\n    self.data = data #tf.placeholder(tf.int32, shape=[None], name=\"data\")\n    datalen = tf.size(self.data)\n\n    self.batch_size = batch_size = config.batch_size\n    self.num_steps = num_steps = config.num_steps\n    self.epoch_size = ((datalen // batch_size) - 1) // num_steps\n    self.input_data, self.targets = reader.producer(\n        self.data, batch_size, num_steps, name=name)\n```\n\nand I try to run:\n\n```\nfrom ptb_word_lm import PTBModel, SmallConfig\nimport reader\n\nconfig = SmallConfig() \ndata_path='./data'\ntrain_data, valid_data, test_data, word_to_id = reader.ptb_raw_data(data_path)\nwith tf.Graph().as_default():\n   initializer = tf.random_uniform_initializer(-config.init_scale,config.init_scale)\n    with tf.name_scope(\"Train\"):\n        with tf.variable_scope(\"Model\", reuse=False, initializer=initializer):\n            m = Model(is_training=True, config=config)\n\n    with tf.Session()  as session:\n        m.assign_lr(session, 1.0)\n        state = session.run(m.initial_state)\n        vals = session.run([m.cost, m.final_state], feed_dict={m.initial_state: state, m.data: train_data})\n```\n\nWhich should run one training epoch. \nThis code is minimalistic but still does not work. As always, no CPU nor GPU load, just waiting...\n\nThe only difference is that 'data' is not an array anymore but a Tensorf (placeholder), don't see which this induces errors.\n"}