{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5396", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5396/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5396/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5396/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5396", "id": 187311043, "node_id": "MDU6SXNzdWUxODczMTEwNDM=", "number": 5396, "title": "tf.train.range_input_producer(epoch_size, shuffle=True) does not terminate", "user": {"login": "pltrdy", "id": 6375843, "node_id": "MDQ6VXNlcjYzNzU4NDM=", "avatar_url": "https://avatars3.githubusercontent.com/u/6375843?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pltrdy", "html_url": "https://github.com/pltrdy", "followers_url": "https://api.github.com/users/pltrdy/followers", "following_url": "https://api.github.com/users/pltrdy/following{/other_user}", "gists_url": "https://api.github.com/users/pltrdy/gists{/gist_id}", "starred_url": "https://api.github.com/users/pltrdy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pltrdy/subscriptions", "organizations_url": "https://api.github.com/users/pltrdy/orgs", "repos_url": "https://api.github.com/users/pltrdy/repos", "events_url": "https://api.github.com/users/pltrdy/events{/privacy}", "received_events_url": "https://api.github.com/users/pltrdy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-11-04T11:30:33Z", "updated_at": "2017-08-15T08:21:56Z", "closed_at": "2016-11-14T18:32:12Z", "author_association": "NONE", "body_html": "<p>Hey guys,</p>\n<p>While working on language modeling using PTB dataset  <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py\">(as shown in TensorFlow tutorial)</a> I had an issue with <code>ptb_producer </code><a href=\"http://stackoverflow.com/questions/40406469/tf-nn-embedding-lookup-does-nothing-blocking-the-execution-but-no-associated-cp\" rel=\"nofollow\">(see on StackOverflow)</a>.</p>\n<p>Long story short, it turns out that <code>ptb_producer</code> never terminates.</p>\n<p>To debug, I tried to 'manually' run <code>ptb_producer</code> with the code below.</p>\n<pre><code>import tensorflow as tf\n\nlearning_rate = 1.0\nnum_layers = 2\nnum_steps = 20\nhidden_size = 2\nepochs = 50\nbatch_size = 20\nvocab_size = 10000\n\n# Loading data\nimport tensorflow.models.rnn.ptb.reader as reader\n\nptb_data_path = '&lt;path&gt;/tensorflow/ptb_data/simple-examples/data/'\n(train, valid, test, _) = reader.ptb_raw_data(ptb_data_path)\n\nraw_data = train\nname = None\n\n# Exact content of PTBProducer\nwith tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n    with tf.control_dependencies([assertion]):\n        epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = tf.slice(data, [0, i * num_steps], [batch_size, num_steps])\n    y = tf.slice(data, [0, i * num_steps + 1], [batch_size, num_steps])\n</code></pre>\n<p>And then running to find out where thing is getting into troubles<br>\n<code>sess = tf.Session()</code> then <code>sess.run(epoch_size)</code> gives <code>2323</code></p>\n<p>Running <code>sess.run(tf.train.range_input_producer(epoch_size, shuffle=True).dequeue())</code> just waits (it does not terminate nor induce CPU load, i.e. same problem than when I run the whole code).</p>\n<p>In fact, even <code>queue = tf.train.range_input_producer(epoch_size, shuffle=True)</code> (which can be intermediate step for previous syntax) does nothing. There is the problem line, still don't know why</p>\n<p>I am not sure how this issue could be related to be, thus it might be a bug.</p>\n<h1>Config Information</h1>\n<ul>\n<li><code>tf.__version__</code> = '0.11.0rc2'</li>\n<li>python 3.5.2</li>\n<li>Ubuntu 16.04</li>\n</ul>", "body_text": "Hey guys,\nWhile working on language modeling using PTB dataset  (as shown in TensorFlow tutorial) I had an issue with ptb_producer (see on StackOverflow).\nLong story short, it turns out that ptb_producer never terminates.\nTo debug, I tried to 'manually' run ptb_producer with the code below.\nimport tensorflow as tf\n\nlearning_rate = 1.0\nnum_layers = 2\nnum_steps = 20\nhidden_size = 2\nepochs = 50\nbatch_size = 20\nvocab_size = 10000\n\n# Loading data\nimport tensorflow.models.rnn.ptb.reader as reader\n\nptb_data_path = '<path>/tensorflow/ptb_data/simple-examples/data/'\n(train, valid, test, _) = reader.ptb_raw_data(ptb_data_path)\n\nraw_data = train\nname = None\n\n# Exact content of PTBProducer\nwith tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\n\n    data_len = tf.size(raw_data)\n    batch_len = data_len // batch_size\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\n                      [batch_size, batch_len])\n\n    epoch_size = (batch_len - 1) // num_steps\n    assertion = tf.assert_positive(\n        epoch_size,\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\n    with tf.control_dependencies([assertion]):\n        epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\n\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\n    x = tf.slice(data, [0, i * num_steps], [batch_size, num_steps])\n    y = tf.slice(data, [0, i * num_steps + 1], [batch_size, num_steps])\n\nAnd then running to find out where thing is getting into troubles\nsess = tf.Session() then sess.run(epoch_size) gives 2323\nRunning sess.run(tf.train.range_input_producer(epoch_size, shuffle=True).dequeue()) just waits (it does not terminate nor induce CPU load, i.e. same problem than when I run the whole code).\nIn fact, even queue = tf.train.range_input_producer(epoch_size, shuffle=True) (which can be intermediate step for previous syntax) does nothing. There is the problem line, still don't know why\nI am not sure how this issue could be related to be, thus it might be a bug.\nConfig Information\n\ntf.__version__ = '0.11.0rc2'\npython 3.5.2\nUbuntu 16.04", "body": "Hey guys,\r\n\r\nWhile working on language modeling using PTB dataset  [(as shown in TensorFlow tutorial)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py) I had an issue with `ptb_producer `[(see on StackOverflow)](http://stackoverflow.com/questions/40406469/tf-nn-embedding-lookup-does-nothing-blocking-the-execution-but-no-associated-cp).\r\n\r\nLong story short, it turns out that `ptb_producer` never terminates.\r\n\r\nTo debug, I tried to 'manually' run `ptb_producer` with the code below.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nlearning_rate = 1.0\r\nnum_layers = 2\r\nnum_steps = 20\r\nhidden_size = 2\r\nepochs = 50\r\nbatch_size = 20\r\nvocab_size = 10000\r\n\r\n# Loading data\r\nimport tensorflow.models.rnn.ptb.reader as reader\r\n\r\nptb_data_path = '<path>/tensorflow/ptb_data/simple-examples/data/'\r\n(train, valid, test, _) = reader.ptb_raw_data(ptb_data_path)\r\n\r\nraw_data = train\r\nname = None\r\n\r\n# Exact content of PTBProducer\r\nwith tf.name_scope(name, \"PTBProducer\", [raw_data, batch_size, num_steps]):\r\n    raw_data = tf.convert_to_tensor(raw_data, name=\"raw_data\", dtype=tf.int32)\r\n\r\n    data_len = tf.size(raw_data)\r\n    batch_len = data_len // batch_size\r\n    data = tf.reshape(raw_data[0 : batch_size * batch_len],\r\n                      [batch_size, batch_len])\r\n\r\n    epoch_size = (batch_len - 1) // num_steps\r\n    assertion = tf.assert_positive(\r\n        epoch_size,\r\n        message=\"epoch_size == 0, decrease batch_size or num_steps\")\r\n    with tf.control_dependencies([assertion]):\r\n        epoch_size = tf.identity(epoch_size, name=\"epoch_size\")\r\n\r\n    i = tf.train.range_input_producer(epoch_size, shuffle=False).dequeue()\r\n    x = tf.slice(data, [0, i * num_steps], [batch_size, num_steps])\r\n    y = tf.slice(data, [0, i * num_steps + 1], [batch_size, num_steps])\r\n```\r\n\r\nAnd then running to find out where thing is getting into troubles\r\n`sess = tf.Session()` then `sess.run(epoch_size)` gives `2323`\r\n\r\n\r\n\r\nRunning `sess.run(tf.train.range_input_producer(epoch_size, shuffle=True).dequeue())` just waits (it does not terminate nor induce CPU load, i.e. same problem than when I run the whole code).\r\n\r\nIn fact, even `queue = tf.train.range_input_producer(epoch_size, shuffle=True)` (which can be intermediate step for previous syntax) does nothing. There is the problem line, still don't know why\r\n\r\nI am not sure how this issue could be related to be, thus it might be a bug.\r\n\r\n# Config Information\r\n- `tf.__version__` = '0.11.0rc2'\r\n- python 3.5.2\r\n- Ubuntu 16.04 \r\n"}