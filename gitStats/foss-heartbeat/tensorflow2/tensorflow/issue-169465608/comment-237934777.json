{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237934777", "html_url": "https://github.com/tensorflow/tensorflow/issues/3649#issuecomment-237934777", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3649", "id": 237934777, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzkzNDc3Nw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-05T18:58:56Z", "updated_at": "2016-08-05T18:58:56Z", "author_association": "MEMBER", "body_html": "<p>First, thanks for reporting this issue.  There is something going on that looks like a possible bug.</p>\n<p>Second, writing good performance benchmarks that measure what you intend, and not something else, is  hard.  I'm still not completely confident I've gotten it right.  However, I'm going to suggest an alternative benchmark program, then explain why.   It's not precisely what I run on my system, and you may need to mess with the python a bit to get it to run on yours.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/404387/benchmark.txt\">benchmark.txt</a></p>\n<p>My CPU is a dual-socket Haswell, and my GPU is a GTX Titan X.  Here's the output I get:</p>\n<p>dev cpu size 1024 logical and: 0.002729<br>\ndev cpu size 1024 integer add: 0.002192<br>\ndev cpu size 1024 float32 add: 0.002473<br>\ndev cpu size 1024 logical red: 0.004275<br>\ndev cpu size 1024 integer red: 0.005276<br>\ndev cpu size 1024 float32 red: 0.004385</p>\n<p>dev gpu size 1024 logical and: 0.068111<br>\ndev gpu size 1024 integer add: 0.004291<br>\ndev gpu size 1024 float32 add: 0.003737<br>\ndev gpu size 1024 logical red: 1.761069     &lt;&lt;&lt;&lt; ANOMALY<br>\ndev gpu size 1024 integer red: 0.007658<br>\ndev gpu size 1024 float32 red: 0.006458</p>\n<p>dev cpu size 1048576 logical and: 0.018126<br>\ndev cpu size 1048576 integer add: 0.035838<br>\ndev cpu size 1048576 float32 add: 0.039299<br>\ndev cpu size 1048576 logical red: 0.027802<br>\ndev cpu size 1048576 integer red: 0.048078<br>\ndev cpu size 1048576 float32 red: 0.053308</p>\n<p>dev gpu size 1048576 logical and: 0.007452<br>\ndev gpu size 1048576 integer add: 0.015339<br>\ndev gpu size 1048576 float32 add: 0.010211<br>\ndev gpu size 1048576 logical red: 0.009259<br>\ndev gpu size 1048576 integer red: 0.021310<br>\ndev gpu size 1048576 float32 red: 0.011330</p>\n<p>dev cpu size 10485760 logical and: 0.089549<br>\ndev cpu size 10485760 integer add: 0.251854<br>\ndev cpu size 10485760 float32 add: 0.292280<br>\ndev cpu size 10485760 logical red: 0.089407<br>\ndev cpu size 10485760 integer red: 0.270315<br>\ndev cpu size 10485760 float32 red: 0.283756</p>\n<p>dev gpu size 10485760 logical and: 0.073739<br>\ndev gpu size 10485760 integer add: 0.148312<br>\ndev gpu size 10485760 float32 add: 0.051901<br>\ndev gpu size 10485760 logical red: 0.026299<br>\ndev gpu size 10485760 integer red: 0.169700<br>\ndev gpu size 10485760 float32 red: 0.053469</p>\n<p>The main issues I found with your program are:</p>\n<ol>\n<li>Beware of session.run() overhead.  Instead of looping many times over session.run(), its better to run one graph that's expensive enough to get a significant time measurement.</li>\n<li>Beware of data copying.  If you run a single Op on a GPU, you may be mostly measuring the time it takes to copy inputs to the GPU, and maybe the result back off.</li>\n<li>Beware of cache effects.  For some Ops, effective choreography of data through the cache is an important aspect of efficiency, so we really need to test cold cache performance, not just loop over execution while all data stays in cache.</li>\n</ol>\n<p>So my approach is to build a test graph for each Op consisting of many similar instances of the Op application, where the inputs are nearly all device-local.  We still get Op-dispatch overhead in the measured time, which is going to be relatively more significant for small tensors.</p>\n<p>My program compares elementwise Add and And, in addition to full-tensor reductions over boolean, int32, and float32 types, for both CPU and GPU.  What I'm seeing is that all these GPU operations are a bit faster than the CPU operations on large tensors, as expected, but on small tensors they are strangely slower, with a very large performance anomaly for boolean logical reduction of a small tensor.</p>\n<p>I'm going to open up an internal bug ticket on this issue.</p>\n<p>I would be interested if you see something significantly different on your system, or if you can find some flaw in the design of my benchmark program.</p>", "body_text": "First, thanks for reporting this issue.  There is something going on that looks like a possible bug.\nSecond, writing good performance benchmarks that measure what you intend, and not something else, is  hard.  I'm still not completely confident I've gotten it right.  However, I'm going to suggest an alternative benchmark program, then explain why.   It's not precisely what I run on my system, and you may need to mess with the python a bit to get it to run on yours.\nbenchmark.txt\nMy CPU is a dual-socket Haswell, and my GPU is a GTX Titan X.  Here's the output I get:\ndev cpu size 1024 logical and: 0.002729\ndev cpu size 1024 integer add: 0.002192\ndev cpu size 1024 float32 add: 0.002473\ndev cpu size 1024 logical red: 0.004275\ndev cpu size 1024 integer red: 0.005276\ndev cpu size 1024 float32 red: 0.004385\ndev gpu size 1024 logical and: 0.068111\ndev gpu size 1024 integer add: 0.004291\ndev gpu size 1024 float32 add: 0.003737\ndev gpu size 1024 logical red: 1.761069     <<<< ANOMALY\ndev gpu size 1024 integer red: 0.007658\ndev gpu size 1024 float32 red: 0.006458\ndev cpu size 1048576 logical and: 0.018126\ndev cpu size 1048576 integer add: 0.035838\ndev cpu size 1048576 float32 add: 0.039299\ndev cpu size 1048576 logical red: 0.027802\ndev cpu size 1048576 integer red: 0.048078\ndev cpu size 1048576 float32 red: 0.053308\ndev gpu size 1048576 logical and: 0.007452\ndev gpu size 1048576 integer add: 0.015339\ndev gpu size 1048576 float32 add: 0.010211\ndev gpu size 1048576 logical red: 0.009259\ndev gpu size 1048576 integer red: 0.021310\ndev gpu size 1048576 float32 red: 0.011330\ndev cpu size 10485760 logical and: 0.089549\ndev cpu size 10485760 integer add: 0.251854\ndev cpu size 10485760 float32 add: 0.292280\ndev cpu size 10485760 logical red: 0.089407\ndev cpu size 10485760 integer red: 0.270315\ndev cpu size 10485760 float32 red: 0.283756\ndev gpu size 10485760 logical and: 0.073739\ndev gpu size 10485760 integer add: 0.148312\ndev gpu size 10485760 float32 add: 0.051901\ndev gpu size 10485760 logical red: 0.026299\ndev gpu size 10485760 integer red: 0.169700\ndev gpu size 10485760 float32 red: 0.053469\nThe main issues I found with your program are:\n\nBeware of session.run() overhead.  Instead of looping many times over session.run(), its better to run one graph that's expensive enough to get a significant time measurement.\nBeware of data copying.  If you run a single Op on a GPU, you may be mostly measuring the time it takes to copy inputs to the GPU, and maybe the result back off.\nBeware of cache effects.  For some Ops, effective choreography of data through the cache is an important aspect of efficiency, so we really need to test cold cache performance, not just loop over execution while all data stays in cache.\n\nSo my approach is to build a test graph for each Op consisting of many similar instances of the Op application, where the inputs are nearly all device-local.  We still get Op-dispatch overhead in the measured time, which is going to be relatively more significant for small tensors.\nMy program compares elementwise Add and And, in addition to full-tensor reductions over boolean, int32, and float32 types, for both CPU and GPU.  What I'm seeing is that all these GPU operations are a bit faster than the CPU operations on large tensors, as expected, but on small tensors they are strangely slower, with a very large performance anomaly for boolean logical reduction of a small tensor.\nI'm going to open up an internal bug ticket on this issue.\nI would be interested if you see something significantly different on your system, or if you can find some flaw in the design of my benchmark program.", "body": "First, thanks for reporting this issue.  There is something going on that looks like a possible bug.\n\nSecond, writing good performance benchmarks that measure what you intend, and not something else, is  hard.  I'm still not completely confident I've gotten it right.  However, I'm going to suggest an alternative benchmark program, then explain why.   It's not precisely what I run on my system, and you may need to mess with the python a bit to get it to run on yours.\n\n[benchmark.txt](https://github.com/tensorflow/tensorflow/files/404387/benchmark.txt)\n\nMy CPU is a dual-socket Haswell, and my GPU is a GTX Titan X.  Here's the output I get:\n\ndev cpu size 1024 logical and: 0.002729\ndev cpu size 1024 integer add: 0.002192\ndev cpu size 1024 float32 add: 0.002473\ndev cpu size 1024 logical red: 0.004275\ndev cpu size 1024 integer red: 0.005276\ndev cpu size 1024 float32 red: 0.004385\n\ndev gpu size 1024 logical and: 0.068111\ndev gpu size 1024 integer add: 0.004291\ndev gpu size 1024 float32 add: 0.003737\ndev gpu size 1024 logical red: 1.761069     <<<< ANOMALY\ndev gpu size 1024 integer red: 0.007658\ndev gpu size 1024 float32 red: 0.006458\n\ndev cpu size 1048576 logical and: 0.018126\ndev cpu size 1048576 integer add: 0.035838\ndev cpu size 1048576 float32 add: 0.039299\ndev cpu size 1048576 logical red: 0.027802\ndev cpu size 1048576 integer red: 0.048078\ndev cpu size 1048576 float32 red: 0.053308\n\ndev gpu size 1048576 logical and: 0.007452\ndev gpu size 1048576 integer add: 0.015339\ndev gpu size 1048576 float32 add: 0.010211\ndev gpu size 1048576 logical red: 0.009259\ndev gpu size 1048576 integer red: 0.021310\ndev gpu size 1048576 float32 red: 0.011330\n\ndev cpu size 10485760 logical and: 0.089549\ndev cpu size 10485760 integer add: 0.251854\ndev cpu size 10485760 float32 add: 0.292280\ndev cpu size 10485760 logical red: 0.089407\ndev cpu size 10485760 integer red: 0.270315\ndev cpu size 10485760 float32 red: 0.283756\n\ndev gpu size 10485760 logical and: 0.073739\ndev gpu size 10485760 integer add: 0.148312\ndev gpu size 10485760 float32 add: 0.051901\ndev gpu size 10485760 logical red: 0.026299\ndev gpu size 10485760 integer red: 0.169700\ndev gpu size 10485760 float32 red: 0.053469\n\nThe main issues I found with your program are:\n 1) Beware of session.run() overhead.  Instead of looping many times over session.run(), its better to run one graph that's expensive enough to get a significant time measurement.\n 2) Beware of data copying.  If you run a single Op on a GPU, you may be mostly measuring the time it takes to copy inputs to the GPU, and maybe the result back off.  \n 3) Beware of cache effects.  For some Ops, effective choreography of data through the cache is an important aspect of efficiency, so we really need to test cold cache performance, not just loop over execution while all data stays in cache.\n\nSo my approach is to build a test graph for each Op consisting of many similar instances of the Op application, where the inputs are nearly all device-local.  We still get Op-dispatch overhead in the measured time, which is going to be relatively more significant for small tensors.\n\nMy program compares elementwise Add and And, in addition to full-tensor reductions over boolean, int32, and float32 types, for both CPU and GPU.  What I'm seeing is that all these GPU operations are a bit faster than the CPU operations on large tensors, as expected, but on small tensors they are strangely slower, with a very large performance anomaly for boolean logical reduction of a small tensor.  \n\nI'm going to open up an internal bug ticket on this issue.  \n\nI would be interested if you see something significantly different on your system, or if you can find some flaw in the design of my benchmark program.\n"}