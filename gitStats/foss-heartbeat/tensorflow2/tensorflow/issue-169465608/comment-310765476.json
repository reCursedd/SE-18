{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310765476", "html_url": "https://github.com/tensorflow/tensorflow/issues/3649#issuecomment-310765476", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3649", "id": 310765476, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDc2NTQ3Ng==", "user": {"login": "tartavull", "id": 4648166, "node_id": "MDQ6VXNlcjQ2NDgxNjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/4648166?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tartavull", "html_url": "https://github.com/tartavull", "followers_url": "https://api.github.com/users/tartavull/followers", "following_url": "https://api.github.com/users/tartavull/following{/other_user}", "gists_url": "https://api.github.com/users/tartavull/gists{/gist_id}", "starred_url": "https://api.github.com/users/tartavull/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tartavull/subscriptions", "organizations_url": "https://api.github.com/users/tartavull/orgs", "repos_url": "https://api.github.com/users/tartavull/repos", "events_url": "https://api.github.com/users/tartavull/events{/privacy}", "received_events_url": "https://api.github.com/users/tartavull/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-23T20:29:23Z", "updated_at": "2017-06-26T13:19:33Z", "author_association": "NONE", "body_html": "<p>I have also found reduce_all to be surprisingly slow on the GPU.<br>\nAlthough my benchmark is not as correct as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> I think it shows that a quick and dirty kernel performs orders of magnitude faster.</p>\n<h3>log output</h3>\n<pre><code>2017-06-23 15:58:53.100418: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100457: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100461: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.473227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\npciBusID 0000:02:00.0\nTotal memory: 11.90GiB\nFree memory: 11.76GiB\n2017-06-23 15:58:53.473278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \n2017-06-23 15:58:53.473284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \n2017-06-23 15:58:53.473296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\ndifferent\ntf.reduce_all /gpu:0 16.2250158787\ntf.reduce_all_user_defined /gpu:0 0.0130350589752\ntf.reduce_all /gpu:0 16.276045084\ntf.reduce_all_user_defined /gpu:0 0.012246131897\ntf.reduce_all /gpu:0 16.2119369507\ntf.reduce_all_user_defined /gpu:0 0.0138440132141\nsame\ntf.reduce_all /gpu:0 16.3995730877\ntf.reduce_all_user_defined /gpu:0 0.00978088378906\ntf.reduce_all /gpu:0 16.5090448856\ntf.reduce_all_user_defined /gpu:0 0.0134818553925\ntf.reduce_all /gpu:0 16.4032981396\ntf.reduce_all_user_defined /gpu:0 0.0142848491669\n</code></pre>\n<h3>test.py</h3>\n<pre><code>from __future__ import print_function\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\nimport numpy as np\nimport tensorflow as tf\nimport timeit\nimport functools\nfrom copy import deepcopy\n\nreduce_equal_module = tf.load_op_library('./reduce_all_user_defined.so')\ntf.reduce_all_user_defined = reduce_equal_module.reduce_all_user_defined\n\ndef compare(obj_0, obj_1, r):\n    assert tf.reduce_all(\n        tf.equal(obj_0, obj_1)).eval() == r\n\ndef compare_fast(obj_0, obj_1, r):\n    assert tf.reduce_all_user_defined(\n        tf.equal(obj_0, obj_1)).eval() == r \n\nwith tf.Session() as sess:\n\n    for device in ['/gpu:0']:\n        with tf.device(device):\n            for eq in [False, True]:\n\n                rand_0 = np.random.uniform(low=0., high=100., size=(100,100,100,100))\n                rand_1 = deepcopy(rand_0)\n                rand_1[-1] += 1.0\n\n                ph_0 = tf.placeholder(rand_0.dtype, shape=rand_0.shape)\n                var_0 = tf.Variable(ph_0, trainable=False, collections=[])\n\n                ph_1 = tf.placeholder(rand_1.dtype, shape=rand_1.shape)\n                var_1 = tf.Variable(ph_1, trainable=False, collections=[])\n                \n                if eq:\n                    print ('same')\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_0 })\n                else:\n                    print ('different')\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_1 })\n\n                for i in range(3):\n                    t = timeit.Timer(functools.partial(compare, var_0, var_1, eq)) \n                    print('tf.reduce_all '+ device + ' ' +  str(t.timeit(1)))\n                    \n                    t = timeit.Timer(functools.partial(compare_fast, var_0, var_1, eq)) \n                    print('tf.reduce_all_user_defined '+ device + ' ' +  str(t.timeit(1)))\n</code></pre>\n<h3>reduce_all_user_defined.cc</h3>\n<pre><code>#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n\nusing namespace tensorflow;\n\nREGISTER_OP(\"ReduceAllUserDefined\")\n    .Input(\"input: bool\")\n    .Output(\"output: bool\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      c-&gt;set_output(0,  c-&gt;Scalar());\n      return Status::OK();\n    });\n\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int n, bool* d_out);\n\nclass ReduceAllUserDefinedOpGPU : public OpKernel {\n public:\n  explicit ReduceAllUserDefinedOpGPU(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* c) override {\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(c, c-&gt;allocate_output(0, TensorShape({}), &amp;output));\n\n    auto in =  c-&gt;input(0).flat&lt;bool&gt;();\n    ReduceKernelLauncher(in.data(),\n                         in.size(),\n                         output-&gt;flat&lt;bool&gt;().data());\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"ReduceAllUserDefined\").Device(DEVICE_GPU), ReduceAllUserDefinedOpGPU);\n</code></pre>\n<h3>reduce_all_user_defined_gpu.cu.cc</h3>\n<pre><code>#if GOOGLE_CUDA\n#define EIGEN_USE_GPU\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n\ntemplate&lt;class T&gt;\nstruct SharedMemory\n{\n    __device__ inline operator       T *()\n    {\n        extern __shared__ int __smem[];\n        return (T *)__smem;\n    }\n\n    __device__ inline operator const T *() const\n    {\n        extern __shared__ int __smem[];\n        return (T *)__smem;\n    }\n};\n\n\ntemplate &lt;unsigned int blockSize&gt;\n__global__ void ReduceAll(const bool* g_idata, bool* g_odata,const unsigned int n)\n{\n    bool *sdata = SharedMemory&lt;bool&gt;();\n\n    // perform first level of reduction,\n    // reading from global memory, writing to shared memory\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockSize*2 + threadIdx.x;\n    unsigned int gridSize = blockSize*2*gridDim.x;\n\n    bool all = true;\n    // we reduce multiple elements per thread.  The number is determined by the\n    // number of active thread blocks (via gridDim).  More blocks will result\n    // in a larger gridSize and therefore fewer elements per thread\n    while (i &lt; n)\n    {\n        all &amp;= g_idata[i];\n\n        // ensure we don't read out of bounds -- this is optimized away for powerOf2 sized arrays\n        if (i + blockSize &lt; n)\n            all &amp;= g_idata[i+blockSize];\n        i += gridSize;\n    }\n\n    // each thread puts its local sum into shared memory\n    sdata[tid] = all;\n    __syncthreads();\n\n\n    // do reduction in shared mem\n    if ((blockSize &gt;= 512) &amp;&amp; (tid &lt; 256))\n        sdata[tid] &amp;= sdata[tid + 256];\n\n    __syncthreads();\n    if ((blockSize &gt;= 256) &amp;&amp;(tid &lt; 128))\n        sdata[tid] &amp;= sdata[tid + 128];\n\n     __syncthreads();\n\n    if ((blockSize &gt;= 128) &amp;&amp; (tid &lt;  64))\n       sdata[tid] &amp;= sdata[tid +  64];\n\n    __syncthreads();\n    if ((blockSize &gt;= 64) &amp;&amp; (tid &lt; 32))\n        sdata[tid] &amp;= sdata[tid + 32];\n\n    __syncthreads();\n    if ((blockSize &gt;= 32) &amp;&amp; (tid &lt; 16))\n        sdata[tid] &amp;= sdata[tid + 16];\n\n    __syncthreads();\n    if ((blockSize &gt;= 16) &amp;&amp; (tid &lt;  8))\n        sdata[tid] &amp;= sdata[tid +  8];\n\n    __syncthreads();\n    if ((blockSize &gt;= 8) &amp;&amp; (tid &lt;  4))\n        sdata[tid] &amp;= sdata[tid +  4];\n\n    __syncthreads();\n    if ((blockSize &gt;= 4) &amp;&amp; (tid &lt;  2))\n        sdata[tid] &amp;= sdata[tid +  2];\n\n    __syncthreads();\n    if ((blockSize &gt;= 2) &amp;&amp; ( tid &lt;  1))\n        sdata[tid] &amp;= sdata[tid +  1];\n\n    __syncthreads();\n    // write result for this block to global mem\n    if (tid == 0) g_odata[blockIdx.x] = sdata[tid];\n}\n\nunsigned int ilog2(int n) {\n    // integer log 2\n    unsigned int l = 0;\n    while (n &gt;&gt;= 1) ++l;\n    return l;\n}\n\nstd::tuple&lt;int, int&gt; chooseBlockAndThreads(const unsigned int size) {\n    const int maxThreads = 256;\n    const int maxBlocks = 64;\n    int blocks = min(maxBlocks, size/(maxThreads*2) + 1);\n    int threads = 0; // number of threads per block\n    if (blocks == 1) {\n        threads = pow(2, ilog2(size));\n    } else {\n        threads = maxThreads;\n    }\n    return std::make_tuple(blocks, threads);\n}\n\nvoid reduce(int blocks, int threads, const bool* d_in, bool* d_odata,const unsigned int size) {\n    // when there is only one warp per block, we need to allocate two warps\n    // worth of shared memory so that we don't index shared memory out of bounds\n    dim3 dimBlock(threads, 1, 1);\n    dim3 dimGrid(blocks, 1, 1);\n\n    int smemSize = (threads &lt;= 32) ? 2 * threads * sizeof(bool) : threads * sizeof(bool);\n    switch (threads)\n    {\n        case 256:\n            ReduceAll&lt;256&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case 128:\n            ReduceAll&lt;128&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case 64:\n            ReduceAll&lt; 64&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case 32:\n            ReduceAll&lt; 32&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case 16:\n            ReduceAll&lt; 16&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case  8:\n            ReduceAll&lt;  8&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case  4:\n            ReduceAll&lt;  4&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case  2:\n            ReduceAll&lt;  2&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n\n        case  1:\n            ReduceAll&lt;  1&gt;&lt;&lt;&lt; dimGrid, dimBlock, smemSize &gt;&gt;&gt;(d_in, d_odata, size);\n            break;\n    }\n}\n\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int size, bool* d_out) {\n\n    auto t = chooseBlockAndThreads(size);\n    int threads = std::get&lt;0&gt;(t);\n    int blocks = std::get&lt;1&gt;(t);\n\n\n    bool* d_odata = nullptr;\n    cudaMalloc((void **) &amp;d_odata, blocks*sizeof(bool));\n    cudaMemset(d_odata, 0, blocks*sizeof(bool));\n\n    reduce(blocks, threads, d_in, d_odata, size);\n    \n    bool* h_odata = (bool*) malloc(blocks*sizeof(bool));\n    cudaMemcpy(h_odata, d_odata, blocks*sizeof(bool), cudaMemcpyDeviceToHost);\n\n    bool h_out = true;\n    for(unsigned int b = 0; b &lt; blocks; b++) {\n        h_out &amp;= h_odata[b];\n    }\n    cudaMemcpy(d_out, &amp;h_out, sizeof(bool), cudaMemcpyHostToDevice);\n\n    free(h_odata);\n    cudaFree(d_odata);\n}\n\n#endif\n</code></pre>\n<h3>compilation</h3>\n<pre><code>TF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') \n\nnvcc -std=c++11 -c -o reduce_all_user_defined_gpu.cu.o reduce_all_user_defined_gpu.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -D_MWAITXINTRIN_H_INCLUDED --expt-relaxed-constexpr -Wno-deprecated-gpu-targets\ng++ -std=c++11 -shared -o reduce_all_user_defined.so reduce_all_user_defined.cc  \\\nreduce_all_user_defined_gpu.cu.o -I $TF_INC -fPIC -D_GLIBCXX_USE_CXX11_ABI=0\n</code></pre>", "body_text": "I have also found reduce_all to be surprisingly slow on the GPU.\nAlthough my benchmark is not as correct as @ppwwyyxx I think it shows that a quick and dirty kernel performs orders of magnitude faster.\nlog output\n2017-06-23 15:58:53.100418: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100457: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.100461: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-06-23 15:58:53.473227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \nname: TITAN X (Pascal)\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\npciBusID 0000:02:00.0\nTotal memory: 11.90GiB\nFree memory: 11.76GiB\n2017-06-23 15:58:53.473278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \n2017-06-23 15:58:53.473284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \n2017-06-23 15:58:53.473296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\ndifferent\ntf.reduce_all /gpu:0 16.2250158787\ntf.reduce_all_user_defined /gpu:0 0.0130350589752\ntf.reduce_all /gpu:0 16.276045084\ntf.reduce_all_user_defined /gpu:0 0.012246131897\ntf.reduce_all /gpu:0 16.2119369507\ntf.reduce_all_user_defined /gpu:0 0.0138440132141\nsame\ntf.reduce_all /gpu:0 16.3995730877\ntf.reduce_all_user_defined /gpu:0 0.00978088378906\ntf.reduce_all /gpu:0 16.5090448856\ntf.reduce_all_user_defined /gpu:0 0.0134818553925\ntf.reduce_all /gpu:0 16.4032981396\ntf.reduce_all_user_defined /gpu:0 0.0142848491669\n\ntest.py\nfrom __future__ import print_function\n\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\nimport numpy as np\nimport tensorflow as tf\nimport timeit\nimport functools\nfrom copy import deepcopy\n\nreduce_equal_module = tf.load_op_library('./reduce_all_user_defined.so')\ntf.reduce_all_user_defined = reduce_equal_module.reduce_all_user_defined\n\ndef compare(obj_0, obj_1, r):\n    assert tf.reduce_all(\n        tf.equal(obj_0, obj_1)).eval() == r\n\ndef compare_fast(obj_0, obj_1, r):\n    assert tf.reduce_all_user_defined(\n        tf.equal(obj_0, obj_1)).eval() == r \n\nwith tf.Session() as sess:\n\n    for device in ['/gpu:0']:\n        with tf.device(device):\n            for eq in [False, True]:\n\n                rand_0 = np.random.uniform(low=0., high=100., size=(100,100,100,100))\n                rand_1 = deepcopy(rand_0)\n                rand_1[-1] += 1.0\n\n                ph_0 = tf.placeholder(rand_0.dtype, shape=rand_0.shape)\n                var_0 = tf.Variable(ph_0, trainable=False, collections=[])\n\n                ph_1 = tf.placeholder(rand_1.dtype, shape=rand_1.shape)\n                var_1 = tf.Variable(ph_1, trainable=False, collections=[])\n                \n                if eq:\n                    print ('same')\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_0 })\n                else:\n                    print ('different')\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_1 })\n\n                for i in range(3):\n                    t = timeit.Timer(functools.partial(compare, var_0, var_1, eq)) \n                    print('tf.reduce_all '+ device + ' ' +  str(t.timeit(1)))\n                    \n                    t = timeit.Timer(functools.partial(compare_fast, var_0, var_1, eq)) \n                    print('tf.reduce_all_user_defined '+ device + ' ' +  str(t.timeit(1)))\n\nreduce_all_user_defined.cc\n#include \"tensorflow/core/framework/op.h\"\n#include \"tensorflow/core/framework/shape_inference.h\"\n#include \"tensorflow/core/framework/op_kernel.h\"\n\nusing namespace tensorflow;\n\nREGISTER_OP(\"ReduceAllUserDefined\")\n    .Input(\"input: bool\")\n    .Output(\"output: bool\")\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\n      c->set_output(0,  c->Scalar());\n      return Status::OK();\n    });\n\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int n, bool* d_out);\n\nclass ReduceAllUserDefinedOpGPU : public OpKernel {\n public:\n  explicit ReduceAllUserDefinedOpGPU(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* c) override {\n\n    Tensor* output = nullptr;\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &output));\n\n    auto in =  c->input(0).flat<bool>();\n    ReduceKernelLauncher(in.data(),\n                         in.size(),\n                         output->flat<bool>().data());\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"ReduceAllUserDefined\").Device(DEVICE_GPU), ReduceAllUserDefinedOpGPU);\n\nreduce_all_user_defined_gpu.cu.cc\n#if GOOGLE_CUDA\n#define EIGEN_USE_GPU\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n\ntemplate<class T>\nstruct SharedMemory\n{\n    __device__ inline operator       T *()\n    {\n        extern __shared__ int __smem[];\n        return (T *)__smem;\n    }\n\n    __device__ inline operator const T *() const\n    {\n        extern __shared__ int __smem[];\n        return (T *)__smem;\n    }\n};\n\n\ntemplate <unsigned int blockSize>\n__global__ void ReduceAll(const bool* g_idata, bool* g_odata,const unsigned int n)\n{\n    bool *sdata = SharedMemory<bool>();\n\n    // perform first level of reduction,\n    // reading from global memory, writing to shared memory\n    unsigned int tid = threadIdx.x;\n    unsigned int i = blockIdx.x*blockSize*2 + threadIdx.x;\n    unsigned int gridSize = blockSize*2*gridDim.x;\n\n    bool all = true;\n    // we reduce multiple elements per thread.  The number is determined by the\n    // number of active thread blocks (via gridDim).  More blocks will result\n    // in a larger gridSize and therefore fewer elements per thread\n    while (i < n)\n    {\n        all &= g_idata[i];\n\n        // ensure we don't read out of bounds -- this is optimized away for powerOf2 sized arrays\n        if (i + blockSize < n)\n            all &= g_idata[i+blockSize];\n        i += gridSize;\n    }\n\n    // each thread puts its local sum into shared memory\n    sdata[tid] = all;\n    __syncthreads();\n\n\n    // do reduction in shared mem\n    if ((blockSize >= 512) && (tid < 256))\n        sdata[tid] &= sdata[tid + 256];\n\n    __syncthreads();\n    if ((blockSize >= 256) &&(tid < 128))\n        sdata[tid] &= sdata[tid + 128];\n\n     __syncthreads();\n\n    if ((blockSize >= 128) && (tid <  64))\n       sdata[tid] &= sdata[tid +  64];\n\n    __syncthreads();\n    if ((blockSize >= 64) && (tid < 32))\n        sdata[tid] &= sdata[tid + 32];\n\n    __syncthreads();\n    if ((blockSize >= 32) && (tid < 16))\n        sdata[tid] &= sdata[tid + 16];\n\n    __syncthreads();\n    if ((blockSize >= 16) && (tid <  8))\n        sdata[tid] &= sdata[tid +  8];\n\n    __syncthreads();\n    if ((blockSize >= 8) && (tid <  4))\n        sdata[tid] &= sdata[tid +  4];\n\n    __syncthreads();\n    if ((blockSize >= 4) && (tid <  2))\n        sdata[tid] &= sdata[tid +  2];\n\n    __syncthreads();\n    if ((blockSize >= 2) && ( tid <  1))\n        sdata[tid] &= sdata[tid +  1];\n\n    __syncthreads();\n    // write result for this block to global mem\n    if (tid == 0) g_odata[blockIdx.x] = sdata[tid];\n}\n\nunsigned int ilog2(int n) {\n    // integer log 2\n    unsigned int l = 0;\n    while (n >>= 1) ++l;\n    return l;\n}\n\nstd::tuple<int, int> chooseBlockAndThreads(const unsigned int size) {\n    const int maxThreads = 256;\n    const int maxBlocks = 64;\n    int blocks = min(maxBlocks, size/(maxThreads*2) + 1);\n    int threads = 0; // number of threads per block\n    if (blocks == 1) {\n        threads = pow(2, ilog2(size));\n    } else {\n        threads = maxThreads;\n    }\n    return std::make_tuple(blocks, threads);\n}\n\nvoid reduce(int blocks, int threads, const bool* d_in, bool* d_odata,const unsigned int size) {\n    // when there is only one warp per block, we need to allocate two warps\n    // worth of shared memory so that we don't index shared memory out of bounds\n    dim3 dimBlock(threads, 1, 1);\n    dim3 dimGrid(blocks, 1, 1);\n\n    int smemSize = (threads <= 32) ? 2 * threads * sizeof(bool) : threads * sizeof(bool);\n    switch (threads)\n    {\n        case 256:\n            ReduceAll<256><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case 128:\n            ReduceAll<128><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case 64:\n            ReduceAll< 64><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case 32:\n            ReduceAll< 32><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case 16:\n            ReduceAll< 16><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case  8:\n            ReduceAll<  8><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case  4:\n            ReduceAll<  4><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case  2:\n            ReduceAll<  2><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n\n        case  1:\n            ReduceAll<  1><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\n            break;\n    }\n}\n\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int size, bool* d_out) {\n\n    auto t = chooseBlockAndThreads(size);\n    int threads = std::get<0>(t);\n    int blocks = std::get<1>(t);\n\n\n    bool* d_odata = nullptr;\n    cudaMalloc((void **) &d_odata, blocks*sizeof(bool));\n    cudaMemset(d_odata, 0, blocks*sizeof(bool));\n\n    reduce(blocks, threads, d_in, d_odata, size);\n    \n    bool* h_odata = (bool*) malloc(blocks*sizeof(bool));\n    cudaMemcpy(h_odata, d_odata, blocks*sizeof(bool), cudaMemcpyDeviceToHost);\n\n    bool h_out = true;\n    for(unsigned int b = 0; b < blocks; b++) {\n        h_out &= h_odata[b];\n    }\n    cudaMemcpy(d_out, &h_out, sizeof(bool), cudaMemcpyHostToDevice);\n\n    free(h_odata);\n    cudaFree(d_odata);\n}\n\n#endif\n\ncompilation\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') \n\nnvcc -std=c++11 -c -o reduce_all_user_defined_gpu.cu.o reduce_all_user_defined_gpu.cu.cc \\\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -D_MWAITXINTRIN_H_INCLUDED --expt-relaxed-constexpr -Wno-deprecated-gpu-targets\ng++ -std=c++11 -shared -o reduce_all_user_defined.so reduce_all_user_defined.cc  \\\nreduce_all_user_defined_gpu.cu.o -I $TF_INC -fPIC -D_GLIBCXX_USE_CXX11_ABI=0", "body": "I have also found reduce_all to be surprisingly slow on the GPU.\r\nAlthough my benchmark is not as correct as @ppwwyyxx I think it shows that a quick and dirty kernel performs orders of magnitude faster.\r\n\r\n### log output\r\n```\r\n2017-06-23 15:58:53.100418: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100447: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100452: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100457: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.100461: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-06-23 15:58:53.473227: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: \r\nname: TITAN X (Pascal)\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.531\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.90GiB\r\nFree memory: 11.76GiB\r\n2017-06-23 15:58:53.473278: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 \r\n2017-06-23 15:58:53.473284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y \r\n2017-06-23 15:58:53.473296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0)\r\ndifferent\r\ntf.reduce_all /gpu:0 16.2250158787\r\ntf.reduce_all_user_defined /gpu:0 0.0130350589752\r\ntf.reduce_all /gpu:0 16.276045084\r\ntf.reduce_all_user_defined /gpu:0 0.012246131897\r\ntf.reduce_all /gpu:0 16.2119369507\r\ntf.reduce_all_user_defined /gpu:0 0.0138440132141\r\nsame\r\ntf.reduce_all /gpu:0 16.3995730877\r\ntf.reduce_all_user_defined /gpu:0 0.00978088378906\r\ntf.reduce_all /gpu:0 16.5090448856\r\ntf.reduce_all_user_defined /gpu:0 0.0134818553925\r\ntf.reduce_all /gpu:0 16.4032981396\r\ntf.reduce_all_user_defined /gpu:0 0.0142848491669\r\n```\r\n### test.py\r\n```\r\nfrom __future__ import print_function\r\n\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport timeit\r\nimport functools\r\nfrom copy import deepcopy\r\n\r\nreduce_equal_module = tf.load_op_library('./reduce_all_user_defined.so')\r\ntf.reduce_all_user_defined = reduce_equal_module.reduce_all_user_defined\r\n\r\ndef compare(obj_0, obj_1, r):\r\n    assert tf.reduce_all(\r\n        tf.equal(obj_0, obj_1)).eval() == r\r\n\r\ndef compare_fast(obj_0, obj_1, r):\r\n    assert tf.reduce_all_user_defined(\r\n        tf.equal(obj_0, obj_1)).eval() == r \r\n\r\nwith tf.Session() as sess:\r\n\r\n    for device in ['/gpu:0']:\r\n        with tf.device(device):\r\n            for eq in [False, True]:\r\n\r\n                rand_0 = np.random.uniform(low=0., high=100., size=(100,100,100,100))\r\n                rand_1 = deepcopy(rand_0)\r\n                rand_1[-1] += 1.0\r\n\r\n                ph_0 = tf.placeholder(rand_0.dtype, shape=rand_0.shape)\r\n                var_0 = tf.Variable(ph_0, trainable=False, collections=[])\r\n\r\n                ph_1 = tf.placeholder(rand_1.dtype, shape=rand_1.shape)\r\n                var_1 = tf.Variable(ph_1, trainable=False, collections=[])\r\n                \r\n                if eq:\r\n                    print ('same')\r\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\r\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_0 })\r\n                else:\r\n                    print ('different')\r\n                    sess.run(var_0.initializer, feed_dict={ ph_0: rand_0 })\r\n                    sess.run(var_1.initializer, feed_dict={ ph_1: rand_1 })\r\n\r\n                for i in range(3):\r\n                    t = timeit.Timer(functools.partial(compare, var_0, var_1, eq)) \r\n                    print('tf.reduce_all '+ device + ' ' +  str(t.timeit(1)))\r\n                    \r\n                    t = timeit.Timer(functools.partial(compare_fast, var_0, var_1, eq)) \r\n                    print('tf.reduce_all_user_defined '+ device + ' ' +  str(t.timeit(1)))\r\n```\r\n### reduce_all_user_defined.cc\r\n```\r\n#include \"tensorflow/core/framework/op.h\"\r\n#include \"tensorflow/core/framework/shape_inference.h\"\r\n#include \"tensorflow/core/framework/op_kernel.h\"\r\n\r\nusing namespace tensorflow;\r\n\r\nREGISTER_OP(\"ReduceAllUserDefined\")\r\n    .Input(\"input: bool\")\r\n    .Output(\"output: bool\")\r\n    .SetShapeFn([](::tensorflow::shape_inference::InferenceContext* c) {\r\n      c->set_output(0,  c->Scalar());\r\n      return Status::OK();\r\n    });\r\n\r\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int n, bool* d_out);\r\n\r\nclass ReduceAllUserDefinedOpGPU : public OpKernel {\r\n public:\r\n  explicit ReduceAllUserDefinedOpGPU(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* c) override {\r\n\r\n    Tensor* output = nullptr;\r\n    OP_REQUIRES_OK(c, c->allocate_output(0, TensorShape({}), &output));\r\n\r\n    auto in =  c->input(0).flat<bool>();\r\n    ReduceKernelLauncher(in.data(),\r\n                         in.size(),\r\n                         output->flat<bool>().data());\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"ReduceAllUserDefined\").Device(DEVICE_GPU), ReduceAllUserDefinedOpGPU);\r\n```\r\n### reduce_all_user_defined_gpu.cu.cc\r\n```\r\n#if GOOGLE_CUDA\r\n#define EIGEN_USE_GPU\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n\r\ntemplate<class T>\r\nstruct SharedMemory\r\n{\r\n    __device__ inline operator       T *()\r\n    {\r\n        extern __shared__ int __smem[];\r\n        return (T *)__smem;\r\n    }\r\n\r\n    __device__ inline operator const T *() const\r\n    {\r\n        extern __shared__ int __smem[];\r\n        return (T *)__smem;\r\n    }\r\n};\r\n\r\n\r\ntemplate <unsigned int blockSize>\r\n__global__ void ReduceAll(const bool* g_idata, bool* g_odata,const unsigned int n)\r\n{\r\n    bool *sdata = SharedMemory<bool>();\r\n\r\n    // perform first level of reduction,\r\n    // reading from global memory, writing to shared memory\r\n    unsigned int tid = threadIdx.x;\r\n    unsigned int i = blockIdx.x*blockSize*2 + threadIdx.x;\r\n    unsigned int gridSize = blockSize*2*gridDim.x;\r\n\r\n    bool all = true;\r\n    // we reduce multiple elements per thread.  The number is determined by the\r\n    // number of active thread blocks (via gridDim).  More blocks will result\r\n    // in a larger gridSize and therefore fewer elements per thread\r\n    while (i < n)\r\n    {\r\n        all &= g_idata[i];\r\n\r\n        // ensure we don't read out of bounds -- this is optimized away for powerOf2 sized arrays\r\n        if (i + blockSize < n)\r\n            all &= g_idata[i+blockSize];\r\n        i += gridSize;\r\n    }\r\n\r\n    // each thread puts its local sum into shared memory\r\n    sdata[tid] = all;\r\n    __syncthreads();\r\n\r\n\r\n    // do reduction in shared mem\r\n    if ((blockSize >= 512) && (tid < 256))\r\n        sdata[tid] &= sdata[tid + 256];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 256) &&(tid < 128))\r\n        sdata[tid] &= sdata[tid + 128];\r\n\r\n     __syncthreads();\r\n\r\n    if ((blockSize >= 128) && (tid <  64))\r\n       sdata[tid] &= sdata[tid +  64];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 64) && (tid < 32))\r\n        sdata[tid] &= sdata[tid + 32];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 32) && (tid < 16))\r\n        sdata[tid] &= sdata[tid + 16];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 16) && (tid <  8))\r\n        sdata[tid] &= sdata[tid +  8];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 8) && (tid <  4))\r\n        sdata[tid] &= sdata[tid +  4];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 4) && (tid <  2))\r\n        sdata[tid] &= sdata[tid +  2];\r\n\r\n    __syncthreads();\r\n    if ((blockSize >= 2) && ( tid <  1))\r\n        sdata[tid] &= sdata[tid +  1];\r\n\r\n    __syncthreads();\r\n    // write result for this block to global mem\r\n    if (tid == 0) g_odata[blockIdx.x] = sdata[tid];\r\n}\r\n\r\nunsigned int ilog2(int n) {\r\n    // integer log 2\r\n    unsigned int l = 0;\r\n    while (n >>= 1) ++l;\r\n    return l;\r\n}\r\n\r\nstd::tuple<int, int> chooseBlockAndThreads(const unsigned int size) {\r\n    const int maxThreads = 256;\r\n    const int maxBlocks = 64;\r\n    int blocks = min(maxBlocks, size/(maxThreads*2) + 1);\r\n    int threads = 0; // number of threads per block\r\n    if (blocks == 1) {\r\n        threads = pow(2, ilog2(size));\r\n    } else {\r\n        threads = maxThreads;\r\n    }\r\n    return std::make_tuple(blocks, threads);\r\n}\r\n\r\nvoid reduce(int blocks, int threads, const bool* d_in, bool* d_odata,const unsigned int size) {\r\n    // when there is only one warp per block, we need to allocate two warps\r\n    // worth of shared memory so that we don't index shared memory out of bounds\r\n    dim3 dimBlock(threads, 1, 1);\r\n    dim3 dimGrid(blocks, 1, 1);\r\n\r\n    int smemSize = (threads <= 32) ? 2 * threads * sizeof(bool) : threads * sizeof(bool);\r\n    switch (threads)\r\n    {\r\n        case 256:\r\n            ReduceAll<256><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 128:\r\n            ReduceAll<128><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 64:\r\n            ReduceAll< 64><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 32:\r\n            ReduceAll< 32><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case 16:\r\n            ReduceAll< 16><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  8:\r\n            ReduceAll<  8><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  4:\r\n            ReduceAll<  4><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  2:\r\n            ReduceAll<  2><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n\r\n        case  1:\r\n            ReduceAll<  1><<< dimGrid, dimBlock, smemSize >>>(d_in, d_odata, size);\r\n            break;\r\n    }\r\n}\r\n\r\nvoid ReduceKernelLauncher(const bool* d_in, const unsigned int size, bool* d_out) {\r\n\r\n    auto t = chooseBlockAndThreads(size);\r\n    int threads = std::get<0>(t);\r\n    int blocks = std::get<1>(t);\r\n\r\n\r\n    bool* d_odata = nullptr;\r\n    cudaMalloc((void **) &d_odata, blocks*sizeof(bool));\r\n    cudaMemset(d_odata, 0, blocks*sizeof(bool));\r\n\r\n    reduce(blocks, threads, d_in, d_odata, size);\r\n    \r\n    bool* h_odata = (bool*) malloc(blocks*sizeof(bool));\r\n    cudaMemcpy(h_odata, d_odata, blocks*sizeof(bool), cudaMemcpyDeviceToHost);\r\n\r\n    bool h_out = true;\r\n    for(unsigned int b = 0; b < blocks; b++) {\r\n        h_out &= h_odata[b];\r\n    }\r\n    cudaMemcpy(d_out, &h_out, sizeof(bool), cudaMemcpyHostToDevice);\r\n\r\n    free(h_odata);\r\n    cudaFree(d_odata);\r\n}\r\n\r\n#endif\r\n```\r\n### compilation\r\n```\r\nTF_INC=$(python -c 'import tensorflow as tf; print(tf.sysconfig.get_include())') \r\n\r\nnvcc -std=c++11 -c -o reduce_all_user_defined_gpu.cu.o reduce_all_user_defined_gpu.cu.cc \\\r\n-I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -D_MWAITXINTRIN_H_INCLUDED --expt-relaxed-constexpr -Wno-deprecated-gpu-targets\r\ng++ -std=c++11 -shared -o reduce_all_user_defined.so reduce_all_user_defined.cc  \\\r\nreduce_all_user_defined_gpu.cu.o -I $TF_INC -fPIC -D_GLIBCXX_USE_CXX11_ABI=0\r\n```"}