{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12808", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12808/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12808/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12808/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12808", "id": 255168047, "node_id": "MDU6SXNzdWUyNTUxNjgwNDc=", "number": 12808, "title": "tensorflow performance issue for map_fn and gather", "user": {"login": "patelprateek", "id": 22586349, "node_id": "MDQ6VXNlcjIyNTg2MzQ5", "avatar_url": "https://avatars3.githubusercontent.com/u/22586349?v=4", "gravatar_id": "", "url": "https://api.github.com/users/patelprateek", "html_url": "https://github.com/patelprateek", "followers_url": "https://api.github.com/users/patelprateek/followers", "following_url": "https://api.github.com/users/patelprateek/following{/other_user}", "gists_url": "https://api.github.com/users/patelprateek/gists{/gist_id}", "starred_url": "https://api.github.com/users/patelprateek/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/patelprateek/subscriptions", "organizations_url": "https://api.github.com/users/patelprateek/orgs", "repos_url": "https://api.github.com/users/patelprateek/repos", "events_url": "https://api.github.com/users/patelprateek/events{/privacy}", "received_events_url": "https://api.github.com/users/patelprateek/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-09-05T06:03:21Z", "updated_at": "2017-09-12T00:04:44Z", "closed_at": "2017-09-12T00:04:44Z", "author_association": "NONE", "body_html": "<p>I am trying to understand more about certain surprising results i see in implementing a tf graph .<br>\nThe graph i am working with is just a forest (bunch of trees). This is just a plain forward inference graph , and nothing related to training. I am sharing the snippets for 2 implementation</p>\n<p>code snippet 1:</p>\n<pre><code>with tf.name_scope(\"main\"):\n   \n    def get_tree_output(offset):\n        loop_vars = (offset,)\n        leaf_indice = tf.while_loop(cond,\n                                    body,\n                                    loop_vars,\n                                    back_prop=False,\n                                    parallel_iterations=1,\n                                    name=\"while_loop\")\n        tree_score = tf.gather(score_tensor, leaf_indice, name=\"tree-scores\")\n        output = tf.add(tree_score, output)\n\n    leaf_indices = tf.map_fn(get_tree_output,\n                             tree_offsets_tensor,\n                             dtype=INT_TYPE,\n                             parallel_iterations=n_trees,\n                             back_prop=False,\n                             name=\"tree-scores\")\n\n    tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\n\n    output = tf.reduce_sum(tree_scores, name=\"sum-output\")\n    output = tf.sigmoid(output, name=\"sigmoid-output\")\n</code></pre>\n<p>code snippet 2:</p>\n<pre><code>with tf.name_scope(\"main\"):\n    tree_offsets_tensor = tf.constant(tree_offsets, dtype=INT_TYPE, name=\"tree_offsets_tensor\")\n    loop_vars = (tree_offsets_tensor,)\n    leaf_indices = tf.while_loop(cond,\n                                 body,\n                                 loop_vars,\n                                 back_prop=False,\n                                 parallel_iterations=n_trees,\n                                 name=\"while_loop\")\n\n    tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\n\n    output = tf.reduce_sum(tree_scores, name=\"sum-output\")\n    output = tf.sigmoid(output, name=\"sigmoid-output\")\n</code></pre>\n<p>The rest of the code is exactly the same  : the constant tensors , variables, condition and body for the while loop. thread and parallelism was also the same in both case<br>\ncode snippet2 :  takes about 500 micro sec to do inference<br>\ncode snippet 1 : take about  12 milli sec to do inference</p>\n<p>The difference is that in snippet 1 , I use <code>map_fn</code> to operate on <code>tree_offset_tensor</code>, where as in snippet 2 , I get rid of that <code>map_fn</code>, and just directly use that tensor, so as I understand in snippet1 <code>get_tree_output</code> method gets called with one element from <code>tree_offset_tensor</code>, we are  having multiple <code>while_loop</code> for each individual offset value, whereas in snippet 2 we just have one <code>while_loop</code> that just takes multiple offset values (basically the offset_tensor).</p>\n<p>I also tried another variation for snippet , instead of using the map_fn  I write a hand written for loop</p>\n<p>code snippet 1 (variation for loop) :</p>\n<pre><code>output = 0\nwith tf.name_scope(\"main\"):\n    for offset in tree_offsets:\n        loop_vars = (offset,)  # offset here is a scalar \n        leaf_indice = tf.while_loop(cond,\n                                    body,\n                                    loop_vars,\n                                    back_prop=False,\n                                    parallel_iterations=1,\n                                    name=\"while_loop\")\n        tree_score = tf.gather(score_tensor, leaf_indice, name=\"tree-scores\")\n        output = tf.add(tree_score, output)\n\n    #leaf_indices = tf.map_fn(get_tree_output,\n    #    tree_offsets_tensor, dtype=INT_TYPE,\n    #    parallel_iterations=n_trees, back_prop=False,\n    #    name=\"tree-scores\")\n\n    #tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\n\n    #output = tf.reduce_sum(tree_scores, name=\"sum-output\")\n    output = tf.sigmoid(output, name=\"sigmoid-output\")\n</code></pre>\n<p>This gives minor improvement :  9 millisec<br>\nThe while condition does a bunch of gather operation , so if i use map_fn or the \"for loop\" the gather operates on a bunch of scalars instead of tensor of offset . Why is the code 20-40x slower , is the usage wrong or are there any caveats here ? Any help in understanding or optimizing this is appreciated</p>", "body_text": "I am trying to understand more about certain surprising results i see in implementing a tf graph .\nThe graph i am working with is just a forest (bunch of trees). This is just a plain forward inference graph , and nothing related to training. I am sharing the snippets for 2 implementation\ncode snippet 1:\nwith tf.name_scope(\"main\"):\n   \n    def get_tree_output(offset):\n        loop_vars = (offset,)\n        leaf_indice = tf.while_loop(cond,\n                                    body,\n                                    loop_vars,\n                                    back_prop=False,\n                                    parallel_iterations=1,\n                                    name=\"while_loop\")\n        tree_score = tf.gather(score_tensor, leaf_indice, name=\"tree-scores\")\n        output = tf.add(tree_score, output)\n\n    leaf_indices = tf.map_fn(get_tree_output,\n                             tree_offsets_tensor,\n                             dtype=INT_TYPE,\n                             parallel_iterations=n_trees,\n                             back_prop=False,\n                             name=\"tree-scores\")\n\n    tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\n\n    output = tf.reduce_sum(tree_scores, name=\"sum-output\")\n    output = tf.sigmoid(output, name=\"sigmoid-output\")\n\ncode snippet 2:\nwith tf.name_scope(\"main\"):\n    tree_offsets_tensor = tf.constant(tree_offsets, dtype=INT_TYPE, name=\"tree_offsets_tensor\")\n    loop_vars = (tree_offsets_tensor,)\n    leaf_indices = tf.while_loop(cond,\n                                 body,\n                                 loop_vars,\n                                 back_prop=False,\n                                 parallel_iterations=n_trees,\n                                 name=\"while_loop\")\n\n    tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\n\n    output = tf.reduce_sum(tree_scores, name=\"sum-output\")\n    output = tf.sigmoid(output, name=\"sigmoid-output\")\n\nThe rest of the code is exactly the same  : the constant tensors , variables, condition and body for the while loop. thread and parallelism was also the same in both case\ncode snippet2 :  takes about 500 micro sec to do inference\ncode snippet 1 : take about  12 milli sec to do inference\nThe difference is that in snippet 1 , I use map_fn to operate on tree_offset_tensor, where as in snippet 2 , I get rid of that map_fn, and just directly use that tensor, so as I understand in snippet1 get_tree_output method gets called with one element from tree_offset_tensor, we are  having multiple while_loop for each individual offset value, whereas in snippet 2 we just have one while_loop that just takes multiple offset values (basically the offset_tensor).\nI also tried another variation for snippet , instead of using the map_fn  I write a hand written for loop\ncode snippet 1 (variation for loop) :\noutput = 0\nwith tf.name_scope(\"main\"):\n    for offset in tree_offsets:\n        loop_vars = (offset,)  # offset here is a scalar \n        leaf_indice = tf.while_loop(cond,\n                                    body,\n                                    loop_vars,\n                                    back_prop=False,\n                                    parallel_iterations=1,\n                                    name=\"while_loop\")\n        tree_score = tf.gather(score_tensor, leaf_indice, name=\"tree-scores\")\n        output = tf.add(tree_score, output)\n\n    #leaf_indices = tf.map_fn(get_tree_output,\n    #    tree_offsets_tensor, dtype=INT_TYPE,\n    #    parallel_iterations=n_trees, back_prop=False,\n    #    name=\"tree-scores\")\n\n    #tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\n\n    #output = tf.reduce_sum(tree_scores, name=\"sum-output\")\n    output = tf.sigmoid(output, name=\"sigmoid-output\")\n\nThis gives minor improvement :  9 millisec\nThe while condition does a bunch of gather operation , so if i use map_fn or the \"for loop\" the gather operates on a bunch of scalars instead of tensor of offset . Why is the code 20-40x slower , is the usage wrong or are there any caveats here ? Any help in understanding or optimizing this is appreciated", "body": "I am trying to understand more about certain surprising results i see in implementing a tf graph .\r\nThe graph i am working with is just a forest (bunch of trees). This is just a plain forward inference graph , and nothing related to training. I am sharing the snippets for 2 implementation\r\n\r\ncode snippet 1: \r\n\r\n    with tf.name_scope(\"main\"):\r\n       \r\n        def get_tree_output(offset):\r\n            loop_vars = (offset,)\r\n            leaf_indice = tf.while_loop(cond,\r\n                                        body,\r\n                                        loop_vars,\r\n                                        back_prop=False,\r\n                                        parallel_iterations=1,\r\n                                        name=\"while_loop\")\r\n            tree_score = tf.gather(score_tensor, leaf_indice, name=\"tree-scores\")\r\n            output = tf.add(tree_score, output)\r\n\r\n        leaf_indices = tf.map_fn(get_tree_output,\r\n                                 tree_offsets_tensor,\r\n                                 dtype=INT_TYPE,\r\n                                 parallel_iterations=n_trees,\r\n                                 back_prop=False,\r\n                                 name=\"tree-scores\")\r\n\r\n        tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\r\n\r\n        output = tf.reduce_sum(tree_scores, name=\"sum-output\")\r\n        output = tf.sigmoid(output, name=\"sigmoid-output\")\r\n\r\n\r\ncode snippet 2:\r\n\r\n    with tf.name_scope(\"main\"):\r\n        tree_offsets_tensor = tf.constant(tree_offsets, dtype=INT_TYPE, name=\"tree_offsets_tensor\")\r\n        loop_vars = (tree_offsets_tensor,)\r\n        leaf_indices = tf.while_loop(cond,\r\n                                     body,\r\n                                     loop_vars,\r\n                                     back_prop=False,\r\n                                     parallel_iterations=n_trees,\r\n                                     name=\"while_loop\")\r\n\r\n        tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\r\n\r\n        output = tf.reduce_sum(tree_scores, name=\"sum-output\")\r\n        output = tf.sigmoid(output, name=\"sigmoid-output\")\r\n\r\n\r\n\r\nThe rest of the code is exactly the same  : the constant tensors , variables, condition and body for the while loop. thread and parallelism was also the same in both case\r\ncode snippet2 :  takes about 500 micro sec to do inference \r\ncode snippet 1 : take about  12 milli sec to do inference \r\n\r\nThe difference is that in snippet 1 , I use `map_fn` to operate on `tree_offset_tensor`, where as in snippet 2 , I get rid of that `map_fn`, and just directly use that tensor, so as I understand in snippet1 `get_tree_output` method gets called with one element from `tree_offset_tensor`, we are  having multiple `while_loop` for each individual offset value, whereas in snippet 2 we just have one `while_loop` that just takes multiple offset values (basically the offset_tensor). \r\n\r\nI also tried another variation for snippet , instead of using the map_fn  I write a hand written for loop\r\n\r\ncode snippet 1 (variation for loop) :\r\n\r\n    output = 0\r\n    with tf.name_scope(\"main\"):\r\n        for offset in tree_offsets:\r\n            loop_vars = (offset,)  # offset here is a scalar \r\n            leaf_indice = tf.while_loop(cond,\r\n                                        body,\r\n                                        loop_vars,\r\n                                        back_prop=False,\r\n                                        parallel_iterations=1,\r\n                                        name=\"while_loop\")\r\n            tree_score = tf.gather(score_tensor, leaf_indice, name=\"tree-scores\")\r\n            output = tf.add(tree_score, output)\r\n\r\n        #leaf_indices = tf.map_fn(get_tree_output,\r\n        #    tree_offsets_tensor, dtype=INT_TYPE,\r\n        #    parallel_iterations=n_trees, back_prop=False,\r\n        #    name=\"tree-scores\")\r\n\r\n        #tree_scores = tf.gather(score_tensor, leaf_indices, name=\"tree-scores\")\r\n\r\n        #output = tf.reduce_sum(tree_scores, name=\"sum-output\")\r\n        output = tf.sigmoid(output, name=\"sigmoid-output\")\r\n\r\nThis gives minor improvement :  9 millisec\r\nThe while condition does a bunch of gather operation , so if i use map_fn or the \"for loop\" the gather operates on a bunch of scalars instead of tensor of offset . Why is the code 20-40x slower , is the usage wrong or are there any caveats here ? Any help in understanding or optimizing this is appreciated"}