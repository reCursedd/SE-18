{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4581", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4581/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4581/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4581/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4581", "id": 179164358, "node_id": "MDU6SXNzdWUxNzkxNjQzNTg=", "number": 4581, "title": "How can user control the communication in distributed tensorflow", "user": {"login": "zhougr1993", "id": 7897672, "node_id": "MDQ6VXNlcjc4OTc2NzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/7897672?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhougr1993", "html_url": "https://github.com/zhougr1993", "followers_url": "https://api.github.com/users/zhougr1993/followers", "following_url": "https://api.github.com/users/zhougr1993/following{/other_user}", "gists_url": "https://api.github.com/users/zhougr1993/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhougr1993/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhougr1993/subscriptions", "organizations_url": "https://api.github.com/users/zhougr1993/orgs", "repos_url": "https://api.github.com/users/zhougr1993/repos", "events_url": "https://api.github.com/users/zhougr1993/events{/privacy}", "received_events_url": "https://api.github.com/users/zhougr1993/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-09-26T08:10:58Z", "updated_at": "2016-10-04T01:29:38Z", "closed_at": "2016-10-04T01:29:38Z", "author_association": "NONE", "body_html": "<p>sorry to trouble.<br>\nWhen I try to train model with ps and worker. I found that TensorFlow passed the whole gradients of each op between ps and each worker.<br>\nIf I explicitly calculate the gradient of inputs, which is defined as placeholder, TensorFlow passed these gradients. While the batch size of the inputs is too big, it makes the time of communication too long.<br>\nIs there any way to choose the parameters which user want to pass between ps and worker.</p>", "body_text": "sorry to trouble.\nWhen I try to train model with ps and worker. I found that TensorFlow passed the whole gradients of each op between ps and each worker.\nIf I explicitly calculate the gradient of inputs, which is defined as placeholder, TensorFlow passed these gradients. While the batch size of the inputs is too big, it makes the time of communication too long.\nIs there any way to choose the parameters which user want to pass between ps and worker.", "body": "sorry to trouble.\nWhen I try to train model with ps and worker. I found that TensorFlow passed the whole gradients of each op between ps and each worker. \nIf I explicitly calculate the gradient of inputs, which is defined as placeholder, TensorFlow passed these gradients. While the batch size of the inputs is too big, it makes the time of communication too long.\nIs there any way to choose the parameters which user want to pass between ps and worker.\n"}