{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/436505529", "html_url": "https://github.com/tensorflow/tensorflow/issues/22081#issuecomment-436505529", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22081", "id": 436505529, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNjUwNTUyOQ==", "user": {"login": "wangshuaizs", "id": 21264093, "node_id": "MDQ6VXNlcjIxMjY0MDkz", "avatar_url": "https://avatars2.githubusercontent.com/u/21264093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangshuaizs", "html_url": "https://github.com/wangshuaizs", "followers_url": "https://api.github.com/users/wangshuaizs/followers", "following_url": "https://api.github.com/users/wangshuaizs/following{/other_user}", "gists_url": "https://api.github.com/users/wangshuaizs/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangshuaizs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangshuaizs/subscriptions", "organizations_url": "https://api.github.com/users/wangshuaizs/orgs", "repos_url": "https://api.github.com/users/wangshuaizs/repos", "events_url": "https://api.github.com/users/wangshuaizs/events{/privacy}", "received_events_url": "https://api.github.com/users/wangshuaizs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-07T05:01:08Z", "updated_at": "2018-11-07T05:01:08Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a><br>\nIt compiled successfully this time. but it seems my training source code is not compatible with master. Here is the log:</p>\n<pre><code>2018-11-07 12:49:40.514512: I tensorflow/contrib/gdr/gdr_memory_manager.cc:228] RDMA server is listening on 10.10.11.7:3333\n2018-11-07 12:49:40.514610: I tensorflow/contrib/gdr/gdr_memory_manager.cc:90] NUMA node for device: mlx4_0 is 1\n2018-11-07 12:49:40.514643: I tensorflow/contrib/gdr/gdr_memory_manager.cc:266] Instrumenting CPU allocator(s)\n2018-11-07 12:49:43.439704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\npciBusID: 0000:83:00.0\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\n2018-11-07 12:49:43.439772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n2018-11-07 12:49:43.441047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-07 12:49:43.441076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-07 12:49:43.441088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-07 12:49:43.441642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11519 MB memory) -&gt; physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\n2018-11-07 12:49:43.446125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; 10.10.11.2:2222}\n2018-11-07 12:49:43.446158: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:3333}\n2018-11-07 12:49:43.452238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:3333\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/shuai/test/yolo_tensorflow/yolo/yolo_net.py:346: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From train.py:239: __init__ (from tensorflow.python.training.sync_replicas_optimizer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThe `SyncReplicaOptimizer` is deprecated. For synchrononous training, please use [Distribution Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py:346: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From train.py:251: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease switch to tf.train.MonitoredTrainingSession\nWorker 0: Initializing session...\n2018-11-07 12:49:45.947153: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session cdc6fedbc4db30df with config: gpu_options { } allow_soft_placement: true\n2018-11-07 12:49:51.843039: I tensorflow/contrib/gdr/gdr_memory_manager.cc:685] RDMA endpoint connected to rdma://10.10.11.2:2222\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nWorker 0: Session initialization complete.\nStarting chief queue runner and running init_tokens_op\nStart distributed training ...\nTraceback (most recent call last):\n  File \"train.py\", line 358, in &lt;module&gt;\n    main()\n  File \"train.py\", line 353, in main\n    distributed_train(params)\n  File \"train.py\", line 315, in distributed_train\n    sess.run(train_op, feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'reduction_type' not in Op&lt;name=ConditionalAccumulator; signature= -&gt; handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true&gt;; NodeDef: sync_replicas/conditional_accumulator = ConditionalAccumulator[_class=[\"loc:@sync_replicas/AccumulatorApplyGradient\"], container=\"\", dtype=DT_FLOAT, reduction_type=\"MEAN\", shape=[7,7,3,64], shared_name=\"yolo/conv_2/weights:0/grad_accum\", _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n</code></pre>\n<p>Though no <code>Cannot register memory region</code> is printed, I don't know if it works.<br>\nps: <code>verbs</code> failed to compile with tf-master. Would you mind making a patch based on r1.10?</p>\n<p>thanks</p>", "body_text": "@byronyi\nIt compiled successfully this time. but it seems my training source code is not compatible with master. Here is the log:\n2018-11-07 12:49:40.514512: I tensorflow/contrib/gdr/gdr_memory_manager.cc:228] RDMA server is listening on 10.10.11.7:3333\n2018-11-07 12:49:40.514610: I tensorflow/contrib/gdr/gdr_memory_manager.cc:90] NUMA node for device: mlx4_0 is 1\n2018-11-07 12:49:40.514643: I tensorflow/contrib/gdr/gdr_memory_manager.cc:266] Instrumenting CPU allocator(s)\n2018-11-07 12:49:43.439704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\npciBusID: 0000:83:00.0\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\n2018-11-07 12:49:43.439772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\n2018-11-07 12:49:43.441047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-11-07 12:49:43.441076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\n2018-11-07 12:49:43.441088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\n2018-11-07 12:49:43.441642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11519 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\n2018-11-07 12:49:43.446125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.11.2:2222}\n2018-11-07 12:49:43.446158: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}\n2018-11-07 12:49:43.452238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:3333\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\nWARNING:tensorflow:From /home/shuai/test/yolo_tensorflow/yolo/yolo_net.py:346: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nWARNING:tensorflow:From train.py:239: __init__ (from tensorflow.python.training.sync_replicas_optimizer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThe `SyncReplicaOptimizer` is deprecated. For synchrononous training, please use [Distribution Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py:346: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\nInstructions for updating:\nTo construct input pipelines, use the `tf.data` module.\nWARNING:tensorflow:From train.py:251: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease switch to tf.train.MonitoredTrainingSession\nWorker 0: Initializing session...\n2018-11-07 12:49:45.947153: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session cdc6fedbc4db30df with config: gpu_options { } allow_soft_placement: true\n2018-11-07 12:49:51.843039: I tensorflow/contrib/gdr/gdr_memory_manager.cc:685] RDMA endpoint connected to rdma://10.10.11.2:2222\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\nWorker 0: Session initialization complete.\nStarting chief queue runner and running init_tokens_op\nStart distributed training ...\nTraceback (most recent call last):\n  File \"train.py\", line 358, in <module>\n    main()\n  File \"train.py\", line 353, in main\n    distributed_train(params)\n  File \"train.py\", line 315, in distributed_train\n    sess.run(train_op, feed_dict=feed_dict)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'reduction_type' not in Op<name=ConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true>; NodeDef: sync_replicas/conditional_accumulator = ConditionalAccumulator[_class=[\"loc:@sync_replicas/AccumulatorApplyGradient\"], container=\"\", dtype=DT_FLOAT, reduction_type=\"MEAN\", shape=[7,7,3,64], shared_name=\"yolo/conv_2/weights:0/grad_accum\", _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\n\nThough no Cannot register memory region is printed, I don't know if it works.\nps: verbs failed to compile with tf-master. Would you mind making a patch based on r1.10?\nthanks", "body": "@byronyi \r\nIt compiled successfully this time. but it seems my training source code is not compatible with master. Here is the log:\r\n```\r\n2018-11-07 12:49:40.514512: I tensorflow/contrib/gdr/gdr_memory_manager.cc:228] RDMA server is listening on 10.10.11.7:3333\r\n2018-11-07 12:49:40.514610: I tensorflow/contrib/gdr/gdr_memory_manager.cc:90] NUMA node for device: mlx4_0 is 1\r\n2018-11-07 12:49:40.514643: I tensorflow/contrib/gdr/gdr_memory_manager.cc:266] Instrumenting CPU allocator(s)\r\n2018-11-07 12:49:43.439704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties:\r\nname: Tesla K40c major: 3 minor: 5 memoryClockRate(GHz): 0.745\r\npciBusID: 0000:83:00.0\r\ntotalMemory: 11.92GiB freeMemory: 11.84GiB\r\n2018-11-07 12:49:43.439772: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\r\n2018-11-07 12:49:43.441047: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-11-07 12:49:43.441076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0\r\n2018-11-07 12:49:43.441088: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N\r\n2018-11-07 12:49:43.441642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:worker/replica:0/task:0/device:GPU:0 with 11519 MB memory) -> physical GPU (device: 0, name: Tesla K40c, pci bus id: 0000:83:00.0, compute capability: 3.5)\r\n2018-11-07 12:49:43.446125: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job ps -> {0 -> 10.10.11.2:2222}\r\n2018-11-07 12:49:43.446158: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:222] Initialize GrpcChannelCache for job worker -> {0 -> localhost:3333}\r\n2018-11-07 12:49:43.452238: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:381] Started server with target: grpc://localhost:3333\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nColocations handled automatically by placer.\r\nWARNING:tensorflow:From /home/shuai/test/yolo_tensorflow/yolo/yolo_net.py:346: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nWARNING:tensorflow:From train.py:239: __init__ (from tensorflow.python.training.sync_replicas_optimizer) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nThe `SyncReplicaOptimizer` is deprecated. For synchrononous training, please use [Distribution Strategies](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/distribute).\r\nWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/training/sync_replicas_optimizer.py:346: __init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nTo construct input pipelines, use the `tf.data` module.\r\nWARNING:tensorflow:From train.py:251: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease switch to tf.train.MonitoredTrainingSession\r\nWorker 0: Initializing session...\r\n2018-11-07 12:49:45.947153: I tensorflow/core/distributed_runtime/master_session.cc:1161] Start master session cdc6fedbc4db30df with config: gpu_options { } allow_soft_placement: true\r\n2018-11-07 12:49:51.843039: I tensorflow/contrib/gdr/gdr_memory_manager.cc:685] RDMA endpoint connected to rdma://10.10.11.2:2222\r\nWARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\r\nWorker 0: Session initialization complete.\r\nStarting chief queue runner and running init_tokens_op\r\nStart distributed training ...\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 358, in <module>\r\n    main()\r\n  File \"train.py\", line 353, in main\r\n    distributed_train(params)\r\n  File \"train.py\", line 315, in distributed_train\r\n    sess.run(train_op, feed_dict=feed_dict)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 929, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1152, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1328, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1348, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef mentions attr 'reduction_type' not in Op<name=ConditionalAccumulator; signature= -> handle:Ref(string); attr=dtype:type,allowed=[DT_FLOAT, DT_DOUBLE, DT_INT32, DT_UINT8, DT_INT16, ..., DT_UINT16, DT_COMPLEX128, DT_HALF, DT_UINT32, DT_UINT64]; attr=shape:shape; attr=container:string,default=\"\"; attr=shared_name:string,default=\"\"; is_stateful=true>; NodeDef: sync_replicas/conditional_accumulator = ConditionalAccumulator[_class=[\"loc:@sync_replicas/AccumulatorApplyGradient\"], container=\"\", dtype=DT_FLOAT, reduction_type=\"MEAN\", shape=[7,7,3,64], shared_name=\"yolo/conv_2/weights:0/grad_accum\", _device=\"/job:ps/replica:0/task:0/device:CPU:0\"](). (Check whether your GraphDef-interpreting binary is up to date with your GraphDef-generating binary.).\r\n```\r\nThough no ```Cannot register memory region``` is printed, I don't know if it works.\r\nps: ```verbs``` failed to compile with tf-master. Would you mind making a patch based on r1.10? \r\n\r\nthanks"}