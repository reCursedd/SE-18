{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13434", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13434/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13434/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13434/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13434", "id": 261936895, "node_id": "MDU6SXNzdWUyNjE5MzY4OTU=", "number": 13434, "title": "Tensorflow does NOT utilize the memory from two GPUs in Windows 10", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-10-01T18:46:40Z", "updated_at": "2017-10-09T22:57:58Z", "closed_at": "2017-10-03T04:50:28Z", "author_association": "NONE", "body_html": "<p>Tensorflow Version 1.3.0<br>\nOS: Windows 10<br>\nGPUs: Nvidia Quadro M4000 * 2 with 8G GPU memory for each<br>\nGPU modes: one for WDDM, one for TCC</p>\n<p>I tested the official codes at <a href=\"https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py\">https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py</a></p>\n<p>I just add the GPU constraints in the main function as:</p>\n<pre><code>def main(unused_argv):\n  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n\n      # For this line, visible_divice_list set to only \"0\" and \"0, 1\" can only support the same batch_size\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1')) \n\n  resnet_classifier = tf.estimator.Estimator(\n\t  model_fn=imagenet_model_fn, model_dir=FLAGS.model_dir,\n\t  config=tf.contrib.learn.RunConfig(session_config=config))\n\n  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):\n\ttensors_to_log = {\n\t\t'learning_rate': 'learning_rate',\n\t\t'cross_entropy': 'cross_entropy',\n\t\t'train_accuracy': 'train_accuracy'\n\t}\n\n\tlogging_hook = tf.train.LoggingTensorHook(\n\t\ttensors=tensors_to_log, every_n_iter=100)\n\n\tprint('Starting a training cycle.')\n\tresnet_classifier.train(\n\t\tinput_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN),\n\t\tsteps=FLAGS.first_cycle_steps or FLAGS.steps_per_eval,\n\t\thooks=[logging_hook])\n\tFLAGS.first_cycle_steps = None\n\n\tprint('Starting to evaluate.')\n\teval_results = resnet_classifier.evaluate(\n\t  input_fn=lambda: input_fn(tf.estimator.ModeKeys.EVAL))\n\tprint(eval_results)\n</code></pre>\n<p>In the training process, if I set the visible device list to \"0, 1\" or \"0\" only, both can run successfully with batch_size=48, but BOTH failed with batch_size=49! This indicates that the second GPU's memory is not utilized, as batch size could not be bigger when using two GPUs. I have use Nvidia-smi to confirm that only one or two GPUs are used in the above experiments.</p>\n<p>My questions are:</p>\n<ol>\n<li>Is there any way that I can use bigger batch_size when using two GPUs?</li>\n<li>If the answer for Q1 is No in Windows, is there any way to do it in Linux? I am not familiar with Linux. In Linux, can I set all GPUs to TCC mode? Will the batch size be bigger when two GPUs are both in TCC mode?</li>\n</ol>\n<p>Thank you.</p>", "body_text": "Tensorflow Version 1.3.0\nOS: Windows 10\nGPUs: Nvidia Quadro M4000 * 2 with 8G GPU memory for each\nGPU modes: one for WDDM, one for TCC\nI tested the official codes at https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py\nI just add the GPU constraints in the main function as:\ndef main(unused_argv):\n  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\n\n      # For this line, visible_divice_list set to only \"0\" and \"0, 1\" can only support the same batch_size\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1')) \n\n  resnet_classifier = tf.estimator.Estimator(\n\t  model_fn=imagenet_model_fn, model_dir=FLAGS.model_dir,\n\t  config=tf.contrib.learn.RunConfig(session_config=config))\n\n  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):\n\ttensors_to_log = {\n\t\t'learning_rate': 'learning_rate',\n\t\t'cross_entropy': 'cross_entropy',\n\t\t'train_accuracy': 'train_accuracy'\n\t}\n\n\tlogging_hook = tf.train.LoggingTensorHook(\n\t\ttensors=tensors_to_log, every_n_iter=100)\n\n\tprint('Starting a training cycle.')\n\tresnet_classifier.train(\n\t\tinput_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN),\n\t\tsteps=FLAGS.first_cycle_steps or FLAGS.steps_per_eval,\n\t\thooks=[logging_hook])\n\tFLAGS.first_cycle_steps = None\n\n\tprint('Starting to evaluate.')\n\teval_results = resnet_classifier.evaluate(\n\t  input_fn=lambda: input_fn(tf.estimator.ModeKeys.EVAL))\n\tprint(eval_results)\n\nIn the training process, if I set the visible device list to \"0, 1\" or \"0\" only, both can run successfully with batch_size=48, but BOTH failed with batch_size=49! This indicates that the second GPU's memory is not utilized, as batch size could not be bigger when using two GPUs. I have use Nvidia-smi to confirm that only one or two GPUs are used in the above experiments.\nMy questions are:\n\nIs there any way that I can use bigger batch_size when using two GPUs?\nIf the answer for Q1 is No in Windows, is there any way to do it in Linux? I am not familiar with Linux. In Linux, can I set all GPUs to TCC mode? Will the batch size be bigger when two GPUs are both in TCC mode?\n\nThank you.", "body": "Tensorflow Version 1.3.0\r\nOS: Windows 10\r\nGPUs: Nvidia Quadro M4000 * 2 with 8G GPU memory for each\r\nGPU modes: one for WDDM, one for TCC\r\n\r\nI tested the official codes at https://github.com/tensorflow/models/blob/master/official/resnet/imagenet_main.py\r\n\r\nI just add the GPU constraints in the main function as:\r\n\r\n\tdef main(unused_argv):\r\n\t  os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\r\n\r\n          # For this line, visible_divice_list set to only \"0\" and \"0, 1\" can only support the same batch_size\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1')) \r\n\r\n\t  resnet_classifier = tf.estimator.Estimator(\r\n\t\t  model_fn=imagenet_model_fn, model_dir=FLAGS.model_dir,\r\n\t\t  config=tf.contrib.learn.RunConfig(session_config=config))\r\n\r\n\t  for cycle in range(FLAGS.train_steps // FLAGS.steps_per_eval):\r\n\t\ttensors_to_log = {\r\n\t\t\t'learning_rate': 'learning_rate',\r\n\t\t\t'cross_entropy': 'cross_entropy',\r\n\t\t\t'train_accuracy': 'train_accuracy'\r\n\t\t}\r\n\r\n\t\tlogging_hook = tf.train.LoggingTensorHook(\r\n\t\t\ttensors=tensors_to_log, every_n_iter=100)\r\n\r\n\t\tprint('Starting a training cycle.')\r\n\t\tresnet_classifier.train(\r\n\t\t\tinput_fn=lambda: input_fn(tf.estimator.ModeKeys.TRAIN),\r\n\t\t\tsteps=FLAGS.first_cycle_steps or FLAGS.steps_per_eval,\r\n\t\t\thooks=[logging_hook])\r\n\t\tFLAGS.first_cycle_steps = None\r\n\r\n\t\tprint('Starting to evaluate.')\r\n\t\teval_results = resnet_classifier.evaluate(\r\n\t\t  input_fn=lambda: input_fn(tf.estimator.ModeKeys.EVAL))\r\n\t\tprint(eval_results)\r\n\r\nIn the training process, if I set the visible device list to \"0, 1\" or \"0\" only, both can run successfully with batch_size=48, but BOTH failed with batch_size=49! This indicates that the second GPU's memory is not utilized, as batch size could not be bigger when using two GPUs. I have use Nvidia-smi to confirm that only one or two GPUs are used in the above experiments.\r\n\r\nMy questions are:\r\n1. Is there any way that I can use bigger batch_size when using two GPUs?\r\n2. If the answer for Q1 is No in Windows, is there any way to do it in Linux? I am not familiar with Linux. In Linux, can I set all GPUs to TCC mode? Will the batch size be bigger when two GPUs are both in TCC mode? \r\n\r\nThank you."}