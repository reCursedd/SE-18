{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335311100", "html_url": "https://github.com/tensorflow/tensorflow/issues/13434#issuecomment-335311100", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13434", "id": 335311100, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTMxMTEwMA==", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-09T22:51:40Z", "updated_at": "2017-10-09T22:51:40Z", "author_association": "NONE", "body_html": "<p>I finally made a successful running codes which now use double batch size on both GPUs now. I have double checked that both GPUs are in use now. Thanks for all the help. The revised codes are:</p>\n<pre><code>def average_gradients(tower_grads):\n  \"\"\"Calculate the average gradient for each shared variable across all towers.\n  Note that this function provides a synchronization point across all towers.\n  Args:\n\ttower_grads: List of lists of (gradient, variable) tuples. The outer list\n\t  is over individual gradients. The inner list is over the gradient\n\t  calculation for each tower.\n  Returns:\n\t List of pairs of (gradient, variable) where the gradient has been averaged\n\t across all towers.\n  \"\"\"\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n\t# Note that each grad_and_vars looks like the following:\n\t#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n\tgrads = []\n\tfor g, _ in grad_and_vars:\n\t  # Add 0 dimension to the gradients to represent the tower.\n\t  expanded_g = tf.expand_dims(g, 0)\n\n\t  # Append on a 'tower' dimension which we will average over below.\n\t  grads.append(expanded_g)\n\n\t# Average over the 'tower' dimension.\n\tgrad = tf.concat(axis=0, values=grads)\n\tgrad = tf.reduce_mean(grad, 0)\n\n\t# Keep in mind that the Variables are redundant because they are shared\n\t# across towers. So .. we will just return the first tower's pointer to\n\t# the Variable.\n\tv = grad_and_vars[0][1]\n\tgrad_and_var = (grad, v)\n\taverage_grads.append(grad_and_var)\n  return average_grads\n\n\ndef imagenet_model_fn(features, labels, mode):\n  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\n  tf.summary.image('images', features, max_outputs=6)\n\n  with tf.device('/cpu:0'):\n\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\n\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\n\t\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  global_step = tf.train.get_or_create_global_step()\n\n\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\n\t  boundaries = [\n\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\n\t  values = [\n\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n\t  learning_rate = tf.train.piecewise_constant(\n\t\ttf.cast(global_step, tf.int32), boundaries, values)\n\n\t  # Create a tensor named learning_rate for logging purposes.\n\t  tf.identity(learning_rate, name='learning_rate')\n\t  tf.summary.scalar('learning_rate', learning_rate)\n\n\t  optimizer = tf.train.MomentumOptimizer(\n\t\tlearning_rate=learning_rate,\n\t\tmomentum=_MOMENTUM)\n\n\ttower_grads = []\n\ttower_cross_entropy = []\n\ttower_reg_loss = []\n\ttower_accuracy = []\n\n\twith tf.variable_scope(tf.get_variable_scope()):\n\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\n\t\t_DEVICE_LIST, split_batch, split_labels)):\n\t\twith tf.device(device):\n\t\t  with tf.name_scope('device_%d' % dev_idx):\n\t\t\tlogits = network(inputs=device_features,\n\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n\n\t\t\ttf.get_variable_scope().reuse_variables()\n\t  \n\t\t\tpredictions = {\n\t\t\t  'classes': tf.argmax(logits, axis=1),\n\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n\t\t\t}\n\n\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\n\t\t\t  logits=logits, onehot_labels=device_labels)\n\t\t\ttower_cross_entropy.append(cross_entropy)\n\n\t\t\treg_loss = 0.5 * FLAGS.weight_decay * tf.add_n(\n\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n\t\t\ttower_reg_loss.append(reg_loss)            \n\n\t\t\tloss = cross_entropy + reg_loss\n\n\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \n\t\t\t  grads = optimizer.compute_gradients(loss)\n\t\t\t  tower_grads.append(grads)\n\n\t\t\taccuracy = tf.metrics.accuracy(\n\t\t\t  tf.argmax(device_labels, axis=1), predictions['classes'])\n\t\t\tmetrics = {'accuracy': accuracy}\n\t\t\ttower_accuracy.append(accuracy[1])\n\n\tcross_entropy = tf.add_n(tower_cross_entropy)\n\ttf.identity(cross_entropy, name='cross_entropy')\n\ttf.summary.scalar('cross_entropy', cross_entropy)\n\n\treg_loss = tf.add_n(tower_reg_loss)\n\ttf.summary.scalar('reg_loss', reg_loss)\n\n\tloss = cross_entropy + reg_loss\n\ttf.summary.scalar('total_loss', loss)\n\t  \n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  accuracy = tf.reduce_mean(tower_accuracy)\n\t  tf.identity(accuracy, name='train_accuracy')\n\t  tf.summary.scalar('train_accuracy', accuracy)\n\t  \n\t  # Batch norm requires update_ops to be added as a train_op dependency.\n\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\t  with tf.control_dependencies(update_ops):\n\t\tgrads = average_gradients(tower_grads)\n\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\n\telse:\n\t  train_op = None\n\n\treturn tf.estimator.EstimatorSpec(\n\t  mode=mode,\n\t  predictions=predictions,\n\t  loss=loss,\n\t  train_op=train_op,\n\t  eval_metric_ops=metrics)\n</code></pre>", "body_text": "I finally made a successful running codes which now use double batch size on both GPUs now. I have double checked that both GPUs are in use now. Thanks for all the help. The revised codes are:\ndef average_gradients(tower_grads):\n  \"\"\"Calculate the average gradient for each shared variable across all towers.\n  Note that this function provides a synchronization point across all towers.\n  Args:\n\ttower_grads: List of lists of (gradient, variable) tuples. The outer list\n\t  is over individual gradients. The inner list is over the gradient\n\t  calculation for each tower.\n  Returns:\n\t List of pairs of (gradient, variable) where the gradient has been averaged\n\t across all towers.\n  \"\"\"\n  average_grads = []\n  for grad_and_vars in zip(*tower_grads):\n\t# Note that each grad_and_vars looks like the following:\n\t#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n\tgrads = []\n\tfor g, _ in grad_and_vars:\n\t  # Add 0 dimension to the gradients to represent the tower.\n\t  expanded_g = tf.expand_dims(g, 0)\n\n\t  # Append on a 'tower' dimension which we will average over below.\n\t  grads.append(expanded_g)\n\n\t# Average over the 'tower' dimension.\n\tgrad = tf.concat(axis=0, values=grads)\n\tgrad = tf.reduce_mean(grad, 0)\n\n\t# Keep in mind that the Variables are redundant because they are shared\n\t# across towers. So .. we will just return the first tower's pointer to\n\t# the Variable.\n\tv = grad_and_vars[0][1]\n\tgrad_and_var = (grad, v)\n\taverage_grads.append(grad_and_var)\n  return average_grads\n\n\ndef imagenet_model_fn(features, labels, mode):\n  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\n  tf.summary.image('images', features, max_outputs=6)\n\n  with tf.device('/cpu:0'):\n\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\n\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\n\t\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  global_step = tf.train.get_or_create_global_step()\n\n\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\n\t  boundaries = [\n\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\n\t  values = [\n\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n\t  learning_rate = tf.train.piecewise_constant(\n\t\ttf.cast(global_step, tf.int32), boundaries, values)\n\n\t  # Create a tensor named learning_rate for logging purposes.\n\t  tf.identity(learning_rate, name='learning_rate')\n\t  tf.summary.scalar('learning_rate', learning_rate)\n\n\t  optimizer = tf.train.MomentumOptimizer(\n\t\tlearning_rate=learning_rate,\n\t\tmomentum=_MOMENTUM)\n\n\ttower_grads = []\n\ttower_cross_entropy = []\n\ttower_reg_loss = []\n\ttower_accuracy = []\n\n\twith tf.variable_scope(tf.get_variable_scope()):\n\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\n\t\t_DEVICE_LIST, split_batch, split_labels)):\n\t\twith tf.device(device):\n\t\t  with tf.name_scope('device_%d' % dev_idx):\n\t\t\tlogits = network(inputs=device_features,\n\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n\n\t\t\ttf.get_variable_scope().reuse_variables()\n\t  \n\t\t\tpredictions = {\n\t\t\t  'classes': tf.argmax(logits, axis=1),\n\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n\t\t\t}\n\n\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\n\t\t\t  logits=logits, onehot_labels=device_labels)\n\t\t\ttower_cross_entropy.append(cross_entropy)\n\n\t\t\treg_loss = 0.5 * FLAGS.weight_decay * tf.add_n(\n\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n\t\t\ttower_reg_loss.append(reg_loss)            \n\n\t\t\tloss = cross_entropy + reg_loss\n\n\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \n\t\t\t  grads = optimizer.compute_gradients(loss)\n\t\t\t  tower_grads.append(grads)\n\n\t\t\taccuracy = tf.metrics.accuracy(\n\t\t\t  tf.argmax(device_labels, axis=1), predictions['classes'])\n\t\t\tmetrics = {'accuracy': accuracy}\n\t\t\ttower_accuracy.append(accuracy[1])\n\n\tcross_entropy = tf.add_n(tower_cross_entropy)\n\ttf.identity(cross_entropy, name='cross_entropy')\n\ttf.summary.scalar('cross_entropy', cross_entropy)\n\n\treg_loss = tf.add_n(tower_reg_loss)\n\ttf.summary.scalar('reg_loss', reg_loss)\n\n\tloss = cross_entropy + reg_loss\n\ttf.summary.scalar('total_loss', loss)\n\t  \n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  accuracy = tf.reduce_mean(tower_accuracy)\n\t  tf.identity(accuracy, name='train_accuracy')\n\t  tf.summary.scalar('train_accuracy', accuracy)\n\t  \n\t  # Batch norm requires update_ops to be added as a train_op dependency.\n\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\t  with tf.control_dependencies(update_ops):\n\t\tgrads = average_gradients(tower_grads)\n\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\n\telse:\n\t  train_op = None\n\n\treturn tf.estimator.EstimatorSpec(\n\t  mode=mode,\n\t  predictions=predictions,\n\t  loss=loss,\n\t  train_op=train_op,\n\t  eval_metric_ops=metrics)", "body": "I finally made a successful running codes which now use double batch size on both GPUs now. I have double checked that both GPUs are in use now. Thanks for all the help. The revised codes are:\r\n\r\n\tdef average_gradients(tower_grads):\r\n\t  \"\"\"Calculate the average gradient for each shared variable across all towers.\r\n\t  Note that this function provides a synchronization point across all towers.\r\n\t  Args:\r\n\t\ttower_grads: List of lists of (gradient, variable) tuples. The outer list\r\n\t\t  is over individual gradients. The inner list is over the gradient\r\n\t\t  calculation for each tower.\r\n\t  Returns:\r\n\t\t List of pairs of (gradient, variable) where the gradient has been averaged\r\n\t\t across all towers.\r\n\t  \"\"\"\r\n\t  average_grads = []\r\n\t  for grad_and_vars in zip(*tower_grads):\r\n\t\t# Note that each grad_and_vars looks like the following:\r\n\t\t#   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\r\n\t\tgrads = []\r\n\t\tfor g, _ in grad_and_vars:\r\n\t\t  # Add 0 dimension to the gradients to represent the tower.\r\n\t\t  expanded_g = tf.expand_dims(g, 0)\r\n\r\n\t\t  # Append on a 'tower' dimension which we will average over below.\r\n\t\t  grads.append(expanded_g)\r\n\r\n\t\t# Average over the 'tower' dimension.\r\n\t\tgrad = tf.concat(axis=0, values=grads)\r\n\t\tgrad = tf.reduce_mean(grad, 0)\r\n\r\n\t\t# Keep in mind that the Variables are redundant because they are shared\r\n\t\t# across towers. So .. we will just return the first tower's pointer to\r\n\t\t# the Variable.\r\n\t\tv = grad_and_vars[0][1]\r\n\t\tgrad_and_var = (grad, v)\r\n\t\taverage_grads.append(grad_and_var)\r\n\t  return average_grads\r\n\r\n\r\n\tdef imagenet_model_fn(features, labels, mode):\r\n\t  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\r\n\t  tf.summary.image('images', features, max_outputs=6)\r\n\r\n\t  with tf.device('/cpu:0'):\r\n\t\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\r\n\t\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\r\n\t\t\r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  global_step = tf.train.get_or_create_global_step()\r\n\r\n\t\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\r\n\t\t  boundaries = [\r\n\t\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\r\n\t\t  values = [\r\n\t\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\r\n\t\t  learning_rate = tf.train.piecewise_constant(\r\n\t\t\ttf.cast(global_step, tf.int32), boundaries, values)\r\n\r\n\t\t  # Create a tensor named learning_rate for logging purposes.\r\n\t\t  tf.identity(learning_rate, name='learning_rate')\r\n\t\t  tf.summary.scalar('learning_rate', learning_rate)\r\n\r\n\t\t  optimizer = tf.train.MomentumOptimizer(\r\n\t\t\tlearning_rate=learning_rate,\r\n\t\t\tmomentum=_MOMENTUM)\r\n\r\n\t\ttower_grads = []\r\n\t\ttower_cross_entropy = []\r\n\t\ttower_reg_loss = []\r\n\t\ttower_accuracy = []\r\n\r\n\t\twith tf.variable_scope(tf.get_variable_scope()):\r\n\t\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\r\n\t\t\t_DEVICE_LIST, split_batch, split_labels)):\r\n\t\t\twith tf.device(device):\r\n\t\t\t  with tf.name_scope('device_%d' % dev_idx):\r\n\t\t\t\tlogits = network(inputs=device_features,\r\n\t\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\r\n\t\t\t\ttf.get_variable_scope().reuse_variables()\r\n\t\t  \r\n\t\t\t\tpredictions = {\r\n\t\t\t\t  'classes': tf.argmax(logits, axis=1),\r\n\t\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n\t\t\t\t}\r\n\r\n\t\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\r\n\t\t\t\t  logits=logits, onehot_labels=device_labels)\r\n\t\t\t\ttower_cross_entropy.append(cross_entropy)\r\n\r\n\t\t\t\treg_loss = 0.5 * FLAGS.weight_decay * tf.add_n(\r\n\t\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\r\n\t\t\t\ttower_reg_loss.append(reg_loss)            \r\n\r\n\t\t\t\tloss = cross_entropy + reg_loss\r\n\r\n\t\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \r\n\t\t\t\t  grads = optimizer.compute_gradients(loss)\r\n\t\t\t\t  tower_grads.append(grads)\r\n\r\n\t\t\t\taccuracy = tf.metrics.accuracy(\r\n\t\t\t\t  tf.argmax(device_labels, axis=1), predictions['classes'])\r\n\t\t\t\tmetrics = {'accuracy': accuracy}\r\n\t\t\t\ttower_accuracy.append(accuracy[1])\r\n\r\n\t\tcross_entropy = tf.add_n(tower_cross_entropy)\r\n\t\ttf.identity(cross_entropy, name='cross_entropy')\r\n\t\ttf.summary.scalar('cross_entropy', cross_entropy)\r\n\r\n\t\treg_loss = tf.add_n(tower_reg_loss)\r\n\t\ttf.summary.scalar('reg_loss', reg_loss)\r\n\r\n\t\tloss = cross_entropy + reg_loss\r\n\t\ttf.summary.scalar('total_loss', loss)\r\n\t\t  \r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  accuracy = tf.reduce_mean(tower_accuracy)\r\n\t\t  tf.identity(accuracy, name='train_accuracy')\r\n\t\t  tf.summary.scalar('train_accuracy', accuracy)\r\n\t\t  \r\n\t\t  # Batch norm requires update_ops to be added as a train_op dependency.\r\n\t\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t  with tf.control_dependencies(update_ops):\r\n\t\t\tgrads = average_gradients(tower_grads)\r\n\t\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\r\n\t\telse:\r\n\t\t  train_op = None\r\n\r\n\t\treturn tf.estimator.EstimatorSpec(\r\n\t\t  mode=mode,\r\n\t\t  predictions=predictions,\r\n\t\t  loss=loss,\r\n\t\t  train_op=train_op,\r\n\t\t  eval_metric_ops=metrics)"}