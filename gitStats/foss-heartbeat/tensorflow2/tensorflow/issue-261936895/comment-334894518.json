{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334894518", "html_url": "https://github.com/tensorflow/tensorflow/issues/13434#issuecomment-334894518", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13434", "id": 334894518, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDg5NDUxOA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-06T23:49:13Z", "updated_at": "2017-10-06T23:49:13Z", "author_association": "MEMBER", "body_html": "<p>Could you be a bit more specific on what the problem is?<br>\nIn particular, please fill out all the details asked for in the new issue template, which include lots of metadata about your setup and instructions to reproduce the problem (for example, a single command line that one could run to reproduce the problem).</p>\n<p>From what is described, it is hard to determine what the problem may be. For example, what error are you getting? How big are the input tensors?  Are there any other live tensors/variables etc. that are holding on to memory in GPU (hard to tell since I can't see the full code that's running this)? Is it possible that your <code>features</code> and <code>labels</code> tensors are placed on CPU and you're running out of host memory?</p>\n<p>For example, to demonstrate that you can do more with two GPUs than one, consider the following:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">GPU_MEMORY_BYTES</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">30</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assuming your GPU has 8GB of memory, adjust accordingly</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Number of float32 elements (4 bytes) that consume 80% of GPU memory</span>\n<span class=\"pl-c1\">NUM_ELEMS</span> <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">GPU_MEMORY_BYTES</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">10</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">4</span>\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n  t0 <span class=\"pl-k\">=</span> tf.ones([<span class=\"pl-c1\">NUM_ELEMS</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensor that consumes 80% of GPU0's memory</span>\n  s0 <span class=\"pl-k\">=</span> tf.reduce_sum(t0)\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:1<span class=\"pl-pds\">\"</span></span>):\n  t1 <span class=\"pl-k\">=</span> tf.ones([<span class=\"pl-c1\">NUM_ELEMS</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensor that consumes 80% of GPU1's memory</span>\n  s1 <span class=\"pl-k\">=</span> tf.reduce_sum(t1)\ns <span class=\"pl-k\">=</span> tf.add(s0, s1)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  <span class=\"pl-c1\">print</span>(sess.run(s))</pre></div>\n<p>Computing <code>s</code> requires more than one GPU worth of memory, but with the computation and input split between two GPUs, this will succeed. As opposed to trying to do the following on a single GPU:</p>\n<pre><code>with tf.device(\"/gpu:0\"):\n  t = tf.ones([2 * NUM_ELEMS])\n  s = tf.reduced_sum(t)\nwith tf.Session() as sess:\n  print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\n</code></pre>", "body_text": "Could you be a bit more specific on what the problem is?\nIn particular, please fill out all the details asked for in the new issue template, which include lots of metadata about your setup and instructions to reproduce the problem (for example, a single command line that one could run to reproduce the problem).\nFrom what is described, it is hard to determine what the problem may be. For example, what error are you getting? How big are the input tensors?  Are there any other live tensors/variables etc. that are holding on to memory in GPU (hard to tell since I can't see the full code that's running this)? Is it possible that your features and labels tensors are placed on CPU and you're running out of host memory?\nFor example, to demonstrate that you can do more with two GPUs than one, consider the following:\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\n\n# Number of float32 elements (4 bytes) that consume 80% of GPU memory\nNUM_ELEMS = (8 * GPU_MEMORY_BYTES / 10) / 4\n\nwith tf.device(\"/gpu:0\"):\n  t0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\n  s0 = tf.reduce_sum(t0)\nwith tf.device(\"/gpu:1\"):\n  t1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\n  s1 = tf.reduce_sum(t1)\ns = tf.add(s0, s1)\n\nwith tf.Session() as sess:\n  print(sess.run(s))\nComputing s requires more than one GPU worth of memory, but with the computation and input split between two GPUs, this will succeed. As opposed to trying to do the following on a single GPU:\nwith tf.device(\"/gpu:0\"):\n  t = tf.ones([2 * NUM_ELEMS])\n  s = tf.reduced_sum(t)\nwith tf.Session() as sess:\n  print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU", "body": "Could you be a bit more specific on what the problem is?\r\nIn particular, please fill out all the details asked for in the new issue template, which include lots of metadata about your setup and instructions to reproduce the problem (for example, a single command line that one could run to reproduce the problem).\r\n\r\nFrom what is described, it is hard to determine what the problem may be. For example, what error are you getting? How big are the input tensors?  Are there any other live tensors/variables etc. that are holding on to memory in GPU (hard to tell since I can't see the full code that's running this)? Is it possible that your `features` and `labels` tensors are placed on CPU and you're running out of host memory?\r\n\r\nFor example, to demonstrate that you can do more with two GPUs than one, consider the following:\r\n\r\n```python\r\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n# Number of float32 elements (4 bytes) that consume 80% of GPU memory\r\nNUM_ELEMS = (8 * GPU_MEMORY_BYTES / 10) / 4\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n  t0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\r\n  s0 = tf.reduce_sum(t0)\r\nwith tf.device(\"/gpu:1\"):\r\n  t1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\r\n  s1 = tf.reduce_sum(t1)\r\ns = tf.add(s0, s1)\r\n\r\nwith tf.Session() as sess:\r\n  print(sess.run(s))\r\n```\r\nComputing `s` requires more than one GPU worth of memory, but with the computation and input split between two GPUs, this will succeed. As opposed to trying to do the following on a single GPU:\r\n\r\n```\r\nwith tf.device(\"/gpu:0\"):\r\n  t = tf.ones([2 * NUM_ELEMS])\r\n  s = tf.reduced_sum(t)\r\nwith tf.Session() as sess:\r\n  print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n```\r\n "}