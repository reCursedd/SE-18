{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/151277617", "pull_request_review_id": 76957269, "id": 151277617, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE1MTI3NzYxNw==", "diff_hunk": "@@ -0,0 +1,195 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/math_ops.cc.\n+\n+// This file uses MKL CBLAS batched xGEMM for acceleration of TF Batch\n+// Matrix-Matrix Multiplication (MatMul) operations.\n+// We currently register this kernel only for MKL supported data\n+// types (float, double, complex64, complex128). The macro INTEL_MKL is defined\n+// by the build system only when MKL is chosen as an option at configure stage\n+// and when it is undefined at build time, this file becomes an empty\n+// compilation unit\n+\n+#define EIGEN_USE_THREADS\n+\n+#if defined(INTEL_MKL)\n+#include <vector>\n+#include \"mkl_cblas.h\"\n+#include \"tensorflow/core/framework/op.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/type_traits.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/fill_functor.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+template <typename Device, typename Scalar>\n+class BatchMatMulMkl : public OpKernel {\n+ public:\n+  explicit BatchMatMulMkl(OpKernelConstruction *context) : OpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"adj_x\", &adj_x_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"adj_y\", &adj_y_));\n+  }\n+\n+  virtual ~BatchMatMulMkl() {}\n+\n+  void Compute(OpKernelContext *ctx) override {\n+    const Tensor &in0 = ctx->input(0);\n+    const Tensor &in1 = ctx->input(1);\n+    OP_REQUIRES(ctx, in0.dims() == in1.dims(),\n+                errors::InvalidArgument(\"In[0] and In[1] has different ndims: \",\n+                                        in0.shape().DebugString(), \" vs. \",\n+                                        in1.shape().DebugString()));\n+    const int ndims = in0.dims();\n+    OP_REQUIRES(\n+        ctx, ndims >= 2,\n+        errors::InvalidArgument(\"In[0] and In[1] ndims must be >= 2: \", ndims));\n+    TensorShape out_shape;\n+    for (int i = 0; i < ndims - 2; ++i) {\n+      OP_REQUIRES(ctx, in0.dim_size(i) == in1.dim_size(i),\n+                  errors::InvalidArgument(\"In[0].dim(\", i, \") and In[1].dim(\",\n+                                          i, \") must be the same: \",\n+                                          in0.shape().DebugString(), \" vs \",\n+                                          in1.shape().DebugString()));\n+      out_shape.AddDim(in0.dim_size(i));\n+    }\n+    auto n = (ndims == 2) ? 1 : out_shape.num_elements();\n+    auto d0 = in0.dim_size(ndims - 2);\n+    auto d1 = in0.dim_size(ndims - 1);\n+    Tensor in0_reshaped;\n+    CHECK(in0_reshaped.CopyFrom(in0, TensorShape({n, d0, d1})));\n+    auto d2 = in1.dim_size(ndims - 2);\n+    auto d3 = in1.dim_size(ndims - 1);\n+    Tensor in1_reshaped;\n+    CHECK(in1_reshaped.CopyFrom(in1, TensorShape({n, d2, d3})));\n+    if (adj_x_) std::swap(d0, d1);\n+    if (adj_y_) std::swap(d2, d3);\n+    OP_REQUIRES(ctx, d1 == d2,\n+                errors::InvalidArgument(\n+                    \"In[0] mismatch In[1] shape: \", d1, \" vs. \", d2, \": \",\n+                    in0.shape().DebugString(), \" \", in1.shape().DebugString(),\n+                    \" \", adj_x_, \" \", adj_y_));\n+    out_shape.AddDim(d0);\n+    out_shape.AddDim(d3);\n+    Tensor *out = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(0, out_shape, &out));\n+    if (out->NumElements() == 0) {\n+      return;\n+    }\n+    if (in0.NumElements() == 0 || in1.NumElements() == 0) {\n+      functor::SetZeroFunctor<Device, Scalar> f;\n+      f(ctx->eigen_device<Device>(), out->flat<Scalar>());\n+      return;\n+    }\n+\n+    const uint64 M = in0_reshaped.dim_size(adj_x_ ? 2 : 1);\n+    const uint64 K = in0_reshaped.dim_size(adj_x_ ? 1 : 2);\n+    const uint64 N = in1_reshaped.dim_size(adj_y_ ? 1 : 2);\n+    auto in0_ptr = (in0.template flat<Scalar>().data());\n+    auto in1_ptr = (in1.template flat<Scalar>().data());\n+    auto out_ptr = (out->template flat<Scalar>().data());\n+    std::vector<CBLAS_TRANSPOSE> transa_array;\n+    std::vector<CBLAS_TRANSPOSE> transb_array;\n+    std::vector<MKL_INT> m_array;\n+    std::vector<MKL_INT> n_array;\n+    std::vector<MKL_INT> k_array;\n+    std::vector<Scalar> alpha_array;\n+    std::vector<Scalar> beta_array;\n+    std::vector<const Scalar *> a_array;\n+    std::vector<const Scalar *> b_array;\n+    std::vector<Scalar *> c_array;\n+    std::vector<MKL_INT> lda_array;\n+    std::vector<MKL_INT> ldb_array;\n+    std::vector<MKL_INT> ldc_array;\n+    std::vector<MKL_INT> group_size;\n+    for (int64 i = 0; i < n; i++) {\n+      transa_array.push_back(adj_x_ ? CblasTrans : CblasNoTrans);\n+      transb_array.push_back(adj_y_ ? CblasTrans : CblasNoTrans);\n+      m_array.push_back(M);\n+      n_array.push_back(N);\n+      k_array.push_back(K);\n+      alpha_array.push_back(1.0);\n+      beta_array.push_back(0.0);\n+      a_array.push_back(in0_ptr + i * M * K);\n+      b_array.push_back(in1_ptr + i * K * N);\n+      c_array.push_back(out_ptr + i * M * N);\n+      lda_array.push_back(adj_x_ ? M : K);\n+      ldb_array.push_back(adj_y_ ? K : N);\n+      ldc_array.push_back(N);\n+    }\n+    group_size.push_back(n);\n+    MklCblasGemmBatch(CblasRowMajor, &transa_array[0], &transb_array[0],\n+                      &m_array[0], &n_array[0], &k_array[0], &alpha_array[0],\n+                      &a_array[0], &lda_array[0], &b_array[0], &ldb_array[0],\n+                      &beta_array[0], &c_array[0], &ldc_array[0], 1,\n+                      &group_size[0]);\n+  }\n+\n+ private:\n+  bool adj_x_;\n+  bool adj_y_;\n+\n+  void MklCblasGemmBatch(const CBLAS_LAYOUT Layout,\n+                         const CBLAS_TRANSPOSE *TransA_Array,\n+                         const CBLAS_TRANSPOSE *TransB_Array,\n+                         const MKL_INT *M_Array, const MKL_INT *N_Array,\n+                         const MKL_INT *K_Array, const float *alpha_Array,\n+                         const float **A_Array, const MKL_INT *lda_Array,\n+                         const float **B_Array, const MKL_INT *ldb_Array,\n+                         const float *beta_Array, float **C_Array,\n+                         const MKL_INT *ldc_Array, const MKL_INT group_count,\n+                         const MKL_INT *group_size) {\n+    cblas_sgemm_batch(Layout, TransA_Array, TransB_Array, M_Array, N_Array,\n+                      K_Array, alpha_Array, A_Array, lda_Array, B_Array,\n+                      ldb_Array, beta_Array, C_Array, ldc_Array, group_count,\n+                      group_size);\n+  }\n+\n+  void MklCblasGemmBatch(const CBLAS_LAYOUT Layout,\n+                         const CBLAS_TRANSPOSE *TransA_Array,\n+                         const CBLAS_TRANSPOSE *TransB_Array,\n+                         const MKL_INT *M_Array, const MKL_INT *N_Array,\n+                         const MKL_INT *K_Array, const double *alpha_Array,\n+                         const double **A_Array, const MKL_INT *lda_Array,\n+                         const double **B_Array, const MKL_INT *ldb_Array,\n+                         const double *beta_Array, double **C_Array,\n+                         const MKL_INT *ldc_Array, const MKL_INT group_count,\n+                         const MKL_INT *group_size) {\n+    cblas_dgemm_batch(Layout, TransA_Array, TransB_Array, M_Array, N_Array,\n+                      K_Array, alpha_Array, A_Array, lda_Array, B_Array,\n+                      ldb_Array, beta_Array, C_Array, ldc_Array, group_count,\n+                      group_size);\n+  }\n+};\n+\n+#define REGISTER_BATCH_MATMUL_MKL(TYPE)                                 \\\n+  REGISTER_KERNEL_BUILDER(                                              \\\n+      Name(\"BatchMatMul\").Device(DEVICE_CPU).TypeConstraint<TYPE>(\"T\"), \\\n+      BatchMatMulMkl<CPUDevice, TYPE>)\n+\n+TF_CALL_float(REGISTER_BATCH_MATMUL_MKL);\n+TF_CALL_double(REGISTER_BATCH_MATMUL_MKL);", "path": "tensorflow/core/kernels/mkl_batch_matmul_op.cc", "position": 233, "original_position": 192, "commit_id": "582822d5596985b4b6497acec2850a71b5e4239f", "original_commit_id": "83d8cd8624598b4f61a03f7bcc68949e263411dd", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "Please add support for complex as well.", "created_at": "2017-11-15T22:59:20Z", "updated_at": "2017-11-17T22:14:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/14335#discussion_r151277617", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14335", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/151277617"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/14335#discussion_r151277617"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/14335"}}, "body_html": "<p>Please add support for complex as well.</p>", "body_text": "Please add support for complex as well."}