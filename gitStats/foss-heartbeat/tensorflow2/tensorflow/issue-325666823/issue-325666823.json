{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19497", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19497/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19497/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19497/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19497", "id": 325666823, "node_id": "MDU6SXNzdWUzMjU2NjY4MjM=", "number": 19497, "title": "NHWC convolution sometimes incorrectly considered NCHW", "user": {"login": "tdanyluk", "id": 9149812, "node_id": "MDQ6VXNlcjkxNDk4MTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/9149812?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tdanyluk", "html_url": "https://github.com/tdanyluk", "followers_url": "https://api.github.com/users/tdanyluk/followers", "following_url": "https://api.github.com/users/tdanyluk/following{/other_user}", "gists_url": "https://api.github.com/users/tdanyluk/gists{/gist_id}", "starred_url": "https://api.github.com/users/tdanyluk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tdanyluk/subscriptions", "organizations_url": "https://api.github.com/users/tdanyluk/orgs", "repos_url": "https://api.github.com/users/tdanyluk/repos", "events_url": "https://api.github.com/users/tdanyluk/events{/privacy}", "received_events_url": "https://api.github.com/users/tdanyluk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-05-23T11:39:39Z", "updated_at": "2018-10-03T07:39:56Z", "closed_at": "2018-06-07T01:10:06Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7.0</li>\n<li><strong>Python version</strong>:  3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:  9.0.176 / 7.0.5</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 1050 Ti/PCIe/SSE2 4GB</li>\n<li><strong>Exact command to reproduce</strong>: custom script</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When using dilated conv2d and bias_add, the conv2d is incorrectly considered NCHW, so the normal dilation format is not accepted. Strangely when there is no bias_add, the problem does not happen.</p>\n<h3>Source code / logs</h3>\n<p>This gives ERROR:</p>\n<pre><code>import tensorflow as tf \nimport numpy as np\n\ndef test_add_conv_transform1():\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\n    \n    conv = tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\n\n    bias1 = tf.get_variable(name='bias', shape=[16], dtype=tf.float32)\n    return tf.nn.bias_add(conv, bias1)\n        \no = test_add_conv_transform1()\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\n</code></pre>\n<p>tensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support dilations in the batch and depth dimensions.<br>\n[[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 4, 4, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, filter/read)]]</p>\n<p>Interestingly this works as intended:</p>\n<pre><code>import tensorflow as tf \nimport numpy as np\n\ndef test_add_conv_transform1():\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\n\n    return tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\n        \no = test_add_conv_transform1()\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.7.0\nPython version:  3.5.2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:  9.0.176 / 7.0.5\nGPU model and memory: GeForce GTX 1050 Ti/PCIe/SSE2 4GB\nExact command to reproduce: custom script\n\nDescribe the problem\nWhen using dilated conv2d and bias_add, the conv2d is incorrectly considered NCHW, so the normal dilation format is not accepted. Strangely when there is no bias_add, the problem does not happen.\nSource code / logs\nThis gives ERROR:\nimport tensorflow as tf \nimport numpy as np\n\ndef test_add_conv_transform1():\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\n    \n    conv = tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\n\n    bias1 = tf.get_variable(name='bias', shape=[16], dtype=tf.float32)\n    return tf.nn.bias_add(conv, bias1)\n        \no = test_add_conv_transform1()\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\n\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support dilations in the batch and depth dimensions.\n[[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 4, 4, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, filter/read)]]\nInterestingly this works as intended:\nimport tensorflow as tf \nimport numpy as np\n\ndef test_add_conv_transform1():\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\n\n    return tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\n        \no = test_add_conv_transform1()\nwith tf.Session() as sess:\n    init = tf.global_variables_initializer()\n    sess.run(init)\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**:  3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:  9.0.176 / 7.0.5\r\n- **GPU model and memory**: GeForce GTX 1050 Ti/PCIe/SSE2 4GB\r\n- **Exact command to reproduce**: custom script\r\n\r\n### Describe the problem\r\nWhen using dilated conv2d and bias_add, the conv2d is incorrectly considered NCHW, so the normal dilation format is not accepted. Strangely when there is no bias_add, the problem does not happen.\r\n\r\n### Source code / logs\r\nThis gives ERROR:\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\ndef test_add_conv_transform1():\r\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\r\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\r\n    \r\n    conv = tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\r\n\r\n    bias1 = tf.get_variable(name='bias', shape=[16], dtype=tf.float32)\r\n    return tf.nn.bias_add(conv, bias1)\r\n        \r\no = test_add_conv_transform1()\r\nwith tf.Session() as sess:\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\r\n```\r\n\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support dilations in the batch and depth dimensions.\r\n         [[Node: Conv2D = Conv2D[T=DT_FLOAT, data_format=\"NCHW\", dilations=[1, 4, 4, 1], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Conv2D-0-TransposeNHWCToNCHW-LayoutOptimizer, filter/read)]]\r\n\r\nInterestingly this works as intended:\r\n```\r\nimport tensorflow as tf \r\nimport numpy as np\r\n\r\ndef test_add_conv_transform1():\r\n    input_ = tf.placeholder(tf.float32, shape=[1, 64, 64, 3], name=\"input\")\r\n    filter_ = tf.get_variable(dtype=tf.float32, shape=[4, 4, 3, 16], name=\"filter\")\r\n\r\n    return tf.nn.conv2d(input_, filter_, strides=[1, 1, 1, 1], padding='VALID', dilations=[1, 4, 4, 1])\r\n        \r\no = test_add_conv_transform1()\r\nwith tf.Session() as sess:\r\n    init = tf.global_variables_initializer()\r\n    sess.run(init)\r\n    sess.run(o, {\"input:0\": np.random.random([1, 64, 64, 3])})\r\n```\r\n"}