{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318973117", "html_url": "https://github.com/tensorflow/tensorflow/issues/11882#issuecomment-318973117", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11882", "id": 318973117, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODk3MzExNw==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-31T05:41:22Z", "updated_at": "2017-07-31T05:41:22Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a>, Do you have any thoughts on this. I made a slightly more elaborate test case that computes relative error between the gpu and cpu version and two runs of the cpu version. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16274959\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jnjaby\">@jnjaby</a>'s fear that gpu is not deterministic seems due to non-determinism in reduction (probably), because I only see 1e-5 error between two runs on the gpu. But I see a ~5770. relative  error between the gpu vs cpu. Could you please take a look:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\nnp.random.seed(<span class=\"pl-c1\">1234</span>)\nconv_ <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">56</span>)\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>):\n    bottom <span class=\"pl-k\">=</span> tf.constant(conv_, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    weight <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weight_cpu<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">56</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.001</span>))\n    bias <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias_cpu<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>np.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32))\n\n    conv <span class=\"pl-k\">=</span> tf.nn.conv2d_transpose(bottom, weight, [<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">19</span>, <span class=\"pl-c1\">19</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    conv_cpu <span class=\"pl-k\">=</span> tf.nn.bias_add(conv, bias)\n\n\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n    bottom <span class=\"pl-k\">=</span> tf.constant(conv_, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    weight <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weight_gpu<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">56</span>], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0.001</span>))\n    bias <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias_gpu<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>np.zeros(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32))\n\n    conv <span class=\"pl-k\">=</span> tf.nn.conv2d_transpose(bottom, weight, [<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">19</span>, <span class=\"pl-c1\">19</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    conv_gpu <span class=\"pl-k\">=</span> tf.nn.bias_add(conv, bias)\n\n\nsess <span class=\"pl-k\">=</span> tf.Session()\nsess.run(tf.global_variables_initializer())\ncpu_a<span class=\"pl-k\">=</span>sess.run(conv_cpu)\ngpu_a<span class=\"pl-k\">=</span>sess.run(conv_gpu)\ngpu_b<span class=\"pl-k\">=</span>sess.run(conv_gpu)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">rel_error</span>(<span class=\"pl-smi\">a</span>,<span class=\"pl-smi\">ref</span>):\n  <span class=\"pl-k\">return</span> np.max(np.abs((ref<span class=\"pl-k\">-</span>a)<span class=\"pl-k\">/</span>ref))\n<span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relerror gpu_a vs cpu <span class=\"pl-c1\">%f</span> relerror gpu_b vs cpu 2 <span class=\"pl-c1\">%f</span><span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>(rel_error(gpu_a, cpu_a), rel_error(gpu_b, cpu_a)))\n<span class=\"pl-c1\">print</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relerror gpu_a vs. gpu_b <span class=\"pl-c1\">%f</span> <span class=\"pl-pds\">'</span></span><span class=\"pl-k\">%</span>(rel_error(gpu_a, gpu_b)))</pre></div>", "body_text": "@zheng-xq, Do you have any thoughts on this. I made a slightly more elaborate test case that computes relative error between the gpu and cpu version and two runs of the cpu version. @jnjaby's fear that gpu is not deterministic seems due to non-determinism in reduction (probably), because I only see 1e-5 error between two runs on the gpu. But I see a ~5770. relative  error between the gpu vs cpu. Could you please take a look:\nimport tensorflow as tf\nimport numpy as np\n\nnp.random.seed(1234)\nconv_ = np.random.randn(10, 7, 7, 56)\n\nwith tf.device(\"/cpu:0\"):\n    bottom = tf.constant(conv_, dtype=tf.float32)\n    weight = tf.get_variable(\"weight_cpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\n    bias = tf.get_variable(\"bias_cpu\", initializer=np.zeros(1, dtype=np.float32))\n\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\n    conv_cpu = tf.nn.bias_add(conv, bias)\n\n\nwith tf.device('/gpu:0'):\n    bottom = tf.constant(conv_, dtype=tf.float32)\n    weight = tf.get_variable(\"weight_gpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\n    bias = tf.get_variable(\"bias_gpu\", initializer=np.zeros(1, dtype=np.float32))\n\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\n    conv_gpu = tf.nn.bias_add(conv, bias)\n\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\ncpu_a=sess.run(conv_cpu)\ngpu_a=sess.run(conv_gpu)\ngpu_b=sess.run(conv_gpu)\n\ndef rel_error(a,ref):\n  return np.max(np.abs((ref-a)/ref))\nprint ('relerror gpu_a vs cpu %f relerror gpu_b vs cpu 2 %f'%(rel_error(gpu_a, cpu_a), rel_error(gpu_b, cpu_a)))\nprint ('relerror gpu_a vs. gpu_b %f '%(rel_error(gpu_a, gpu_b)))", "body": "@zheng-xq, Do you have any thoughts on this. I made a slightly more elaborate test case that computes relative error between the gpu and cpu version and two runs of the cpu version. @jnjaby's fear that gpu is not deterministic seems due to non-determinism in reduction (probably), because I only see 1e-5 error between two runs on the gpu. But I see a ~5770. relative  error between the gpu vs cpu. Could you please take a look:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nnp.random.seed(1234)\r\nconv_ = np.random.randn(10, 7, 7, 56)\r\n\r\nwith tf.device(\"/cpu:0\"):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.get_variable(\"weight_cpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    bias = tf.get_variable(\"bias_cpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_cpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nwith tf.device('/gpu:0'):\r\n    bottom = tf.constant(conv_, dtype=tf.float32)\r\n    weight = tf.get_variable(\"weight_gpu\", [9, 9, 1, 56], initializer=tf.random_normal_initializer(0, 0.001))\r\n    bias = tf.get_variable(\"bias_gpu\", initializer=np.zeros(1, dtype=np.float32))\r\n\r\n    conv = tf.nn.conv2d_transpose(bottom, weight, [10, 19, 19, 1], [1, 3, 3, 1], padding='SAME')\r\n    conv_gpu = tf.nn.bias_add(conv, bias)\r\n\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\ncpu_a=sess.run(conv_cpu)\r\ngpu_a=sess.run(conv_gpu)\r\ngpu_b=sess.run(conv_gpu)\r\n\r\ndef rel_error(a,ref):\r\n  return np.max(np.abs((ref-a)/ref))\r\nprint ('relerror gpu_a vs cpu %f relerror gpu_b vs cpu 2 %f'%(rel_error(gpu_a, cpu_a), rel_error(gpu_b, cpu_a)))\r\nprint ('relerror gpu_a vs. gpu_b %f '%(rel_error(gpu_a, gpu_b)))\r\n```"}