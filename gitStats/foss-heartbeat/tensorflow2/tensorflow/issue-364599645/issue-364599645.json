{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22570", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22570/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22570/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22570/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22570", "id": 364599645, "node_id": "MDU6SXNzdWUzNjQ1OTk2NDU=", "number": 22570, "title": "Keras eager execution: Accessing the model DeferredTensors", "user": {"login": "nairouz", "id": 10966954, "node_id": "MDQ6VXNlcjEwOTY2OTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/10966954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairouz", "html_url": "https://github.com/nairouz", "followers_url": "https://api.github.com/users/nairouz/followers", "following_url": "https://api.github.com/users/nairouz/following{/other_user}", "gists_url": "https://api.github.com/users/nairouz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairouz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairouz/subscriptions", "organizations_url": "https://api.github.com/users/nairouz/orgs", "repos_url": "https://api.github.com/users/nairouz/repos", "events_url": "https://api.github.com/users/nairouz/events{/privacy}", "received_events_url": "https://api.github.com/users/nairouz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-27T18:27:55Z", "updated_at": "2018-11-14T19:26:58Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Colab</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Colab</li>\n<li><strong>TensorFlow version (use command below)</strong>:  1.11.0-rc2</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Colab</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: Colab</li>\n<li><strong>CUDA/cuDNN version</strong>: Colab</li>\n<li><strong>GPU model and memory</strong>: Colab</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have a custom loss function which requires some specific tensors to be computed based on other tensors generated from the model itself.  Unfortunately, this can not be done in eager mode.</p>\n<p>Please find below a toy example that illustrates the issue. The code runs without problems in the graph mode.<br>\nI really need a workaround. Could you please help.</p>\n<p>Thanks.</p>\n<h3>Source code</h3>\n<pre><code>import tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as K\nimport numpy as np\nfrom tensorflow.keras.layers import Dense, Input, Layer\nfrom tensorflow.keras.models import Model\n\ntf.enable_eager_execution()\n\ndef custom_loss_wrapper(input_tensor):\n    def custom_loss(y_true, y_pred):\n        return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\n    return custom_loss\n  \ninput_tensor = Input(shape=(10,))\nhidden = Dense(100, activation='relu')(input_tensor)\nout = Dense(1, activation='sigmoid')(hidden)\nmodel = Model(input_tensor, out)\nmodel.compile(loss=custom_loss_wrapper(input_tensor), optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\n\nnp.random.seed(0)\nX = np.random.random((3, 10)).astype(np.float32)\nY1 = np.random.random((3, 1)).astype(np.float32)\nmodel.fit(x=X, y=Y1, batch_size=1, epochs=10)\n</code></pre>\n<h3>logs</h3>\n<p>Epoch 1/10</p>\n<hr>\n<p>ValueError                                Traceback (most recent call last)<br>\n in ()<br>\n22 X = np.random.random((3, 10)).astype(np.float32)<br>\n23 Y1 = np.random.random((3, 1)).astype(np.float32)<br>\n---&gt; 24 model.fit(x=X, y=Y1, batch_size=1, epochs=10)</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)<br>\n1577           initial_epoch=initial_epoch,<br>\n1578           steps_per_epoch=steps_per_epoch,<br>\n-&gt; 1579           validation_steps=validation_steps)<br>\n1580     elif self._distribution_strategy:<br>\n1581       return training_distributed.fit_loop(</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)<br>\n696           validation_steps=validation_steps,<br>\n697           do_validation=do_validation,<br>\n--&gt; 698           batch_size=batch_size)<br>\n699       callbacks.on_epoch_end(epoch, epoch_logs)<br>\n700       if callbacks.model.stop_training:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in iterator_fit_loop(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)<br>\n248     # Train model.<br>\n249     outs, loss, loss_metrics, masks = _process_single_batch(<br>\n--&gt; 250         model, x, y, sample_weights=sample_weights, training=True)<br>\n251     outs = generic_utils.to_list(outs)<br>\n252</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)<br>\n502           targets,<br>\n503           sample_weights=sample_weights,<br>\n--&gt; 504           training=training)<br>\n505       if loss is None:<br>\n506         raise ValueError('The model cannot be run '</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)<br>\n110       with backend.name_scope(model.output_names[i] + '_loss'):<br>\n111         output_loss = weighted_masked_fn(<br>\n--&gt; 112             targets[i], outs[i], weights, mask=mask)<br>\n113       # If the number of outputs is 1 then we don't append the loss metric<br>\n114       # associated with each model output. When there are multiple outputs</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in weighted(y_true, y_pred, weights, mask)<br>\n569     \"\"\"<br>\n570     # score_array has ndim &gt;= 2<br>\n--&gt; 571     score_array = fn(y_true, y_pred)<br>\n572     if mask is not None:<br>\n573       mask = math_ops.cast(mask, y_pred.dtype)</p>\n<p> in custom_loss(y_true, y_pred)<br>\n10 def custom_loss_wrapper(input_tensor):<br>\n11     def custom_loss(y_true, y_pred):<br>\n---&gt; 12         return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)<br>\n13     return custom_loss<br>\n14</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in mean(x, axis, keepdims)<br>\n1727   if x.dtype.base_dtype == dtypes_module.bool:<br>\n1728     x = math_ops.cast(x, floatx())<br>\n-&gt; 1729   return math_ops.reduce_mean(x, axis, keepdims)<br>\n1730<br>\n1731</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)<br>\n486                 'in a future version' if date is None else ('after %s' % date),<br>\n487                 instructions)<br>\n--&gt; 488       return func(*args, **kwargs)<br>\n489     return tf_decorator.make_decorator(func, new_func, 'deprecated',<br>\n490                                        _add_deprecated_arg_notice_to_docstring(</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_mean(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)<br>\n1488                                    input_tensor,<br>\n1489                                    _ReductionDims(input_tensor, axis,<br>\n-&gt; 1490                                                   reduction_indices),<br>\n1491                                    keepdims,<br>\n1492                                    name=name))</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _ReductionDims(x, axis, reduction_indices)<br>\n1270<br>\n1271     # Otherwise, we rely on Range and Rank to do the right thing at run-time.<br>\n-&gt; 1272     return range(0, array_ops.rank(x))<br>\n1273<br>\n1274</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank(input, name)<br>\n366   @end_compatibility<br>\n367   \"\"\"<br>\n--&gt; 368   return rank_internal(input, name, optimize=True)<br>\n369<br>\n370</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank_internal(input, name, optimize)<br>\n386       return gen_array_ops.size(input.dense_shape, name=name)<br>\n387     else:<br>\n--&gt; 388       input_tensor = ops.convert_to_tensor(input)<br>\n389       input_shape = input_tensor.get_shape()<br>\n390       if optimize and input_shape.ndims is not None:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)<br>\n1046       name=name,<br>\n1047       preferred_dtype=preferred_dtype,<br>\n-&gt; 1048       as_ref=False)<br>\n1049<br>\n1050</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)<br>\n1142<br>\n1143     if ret is None:<br>\n-&gt; 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)<br>\n1145<br>\n1146     if ret is NotImplemented:</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)<br>\n226                                          as_ref=False):<br>\n227   _ = as_ref<br>\n--&gt; 228   return constant(v, dtype=dtype, name=name)<br>\n229<br>\n230</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)<br>\n176   ctx = context.context()<br>\n177   if ctx.executing_eagerly():<br>\n--&gt; 178     t = convert_to_eager_tensor(value, ctx, dtype)<br>\n179     if shape is None:<br>\n180       return t</p>\n<p>/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)<br>\n111     return t<br>\n112   else:<br>\n--&gt; 113     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)<br>\n114<br>\n115</p>\n<p>ValueError: Attempt to convert a value (&lt;DeferredTensor 'input_2' shape=(?, 10) dtype=float32&gt;) with an unsupported type (&lt;class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'&gt;) to a Tensor.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Colab\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): Colab\nTensorFlow version (use command below):  1.11.0-rc2\nPython version: 3.6\nBazel version (if compiling from source): Colab\nGCC/Compiler version (if compiling from source): Colab\nCUDA/cuDNN version: Colab\nGPU model and memory: Colab\nExact command to reproduce:\n\nDescribe the problem\nI have a custom loss function which requires some specific tensors to be computed based on other tensors generated from the model itself.  Unfortunately, this can not be done in eager mode.\nPlease find below a toy example that illustrates the issue. The code runs without problems in the graph mode.\nI really need a workaround. Could you please help.\nThanks.\nSource code\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.backend as K\nimport numpy as np\nfrom tensorflow.keras.layers import Dense, Input, Layer\nfrom tensorflow.keras.models import Model\n\ntf.enable_eager_execution()\n\ndef custom_loss_wrapper(input_tensor):\n    def custom_loss(y_true, y_pred):\n        return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\n    return custom_loss\n  \ninput_tensor = Input(shape=(10,))\nhidden = Dense(100, activation='relu')(input_tensor)\nout = Dense(1, activation='sigmoid')(hidden)\nmodel = Model(input_tensor, out)\nmodel.compile(loss=custom_loss_wrapper(input_tensor), optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\n\nnp.random.seed(0)\nX = np.random.random((3, 10)).astype(np.float32)\nY1 = np.random.random((3, 1)).astype(np.float32)\nmodel.fit(x=X, y=Y1, batch_size=1, epochs=10)\n\nlogs\nEpoch 1/10\n\nValueError                                Traceback (most recent call last)\n in ()\n22 X = np.random.random((3, 10)).astype(np.float32)\n23 Y1 = np.random.random((3, 1)).astype(np.float32)\n---> 24 model.fit(x=X, y=Y1, batch_size=1, epochs=10)\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n1577           initial_epoch=initial_epoch,\n1578           steps_per_epoch=steps_per_epoch,\n-> 1579           validation_steps=validation_steps)\n1580     elif self._distribution_strategy:\n1581       return training_distributed.fit_loop(\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)\n696           validation_steps=validation_steps,\n697           do_validation=do_validation,\n--> 698           batch_size=batch_size)\n699       callbacks.on_epoch_end(epoch, epoch_logs)\n700       if callbacks.model.stop_training:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in iterator_fit_loop(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)\n248     # Train model.\n249     outs, loss, loss_metrics, masks = _process_single_batch(\n--> 250         model, x, y, sample_weights=sample_weights, training=True)\n251     outs = generic_utils.to_list(outs)\n252\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)\n502           targets,\n503           sample_weights=sample_weights,\n--> 504           training=training)\n505       if loss is None:\n506         raise ValueError('The model cannot be run '\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)\n110       with backend.name_scope(model.output_names[i] + '_loss'):\n111         output_loss = weighted_masked_fn(\n--> 112             targets[i], outs[i], weights, mask=mask)\n113       # If the number of outputs is 1 then we don't append the loss metric\n114       # associated with each model output. When there are multiple outputs\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in weighted(y_true, y_pred, weights, mask)\n569     \"\"\"\n570     # score_array has ndim >= 2\n--> 571     score_array = fn(y_true, y_pred)\n572     if mask is not None:\n573       mask = math_ops.cast(mask, y_pred.dtype)\n in custom_loss(y_true, y_pred)\n10 def custom_loss_wrapper(input_tensor):\n11     def custom_loss(y_true, y_pred):\n---> 12         return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\n13     return custom_loss\n14\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in mean(x, axis, keepdims)\n1727   if x.dtype.base_dtype == dtypes_module.bool:\n1728     x = math_ops.cast(x, floatx())\n-> 1729   return math_ops.reduce_mean(x, axis, keepdims)\n1730\n1731\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\n486                 'in a future version' if date is None else ('after %s' % date),\n487                 instructions)\n--> 488       return func(*args, **kwargs)\n489     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n490                                        _add_deprecated_arg_notice_to_docstring(\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_mean(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\n1488                                    input_tensor,\n1489                                    _ReductionDims(input_tensor, axis,\n-> 1490                                                   reduction_indices),\n1491                                    keepdims,\n1492                                    name=name))\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _ReductionDims(x, axis, reduction_indices)\n1270\n1271     # Otherwise, we rely on Range and Rank to do the right thing at run-time.\n-> 1272     return range(0, array_ops.rank(x))\n1273\n1274\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank(input, name)\n366   @end_compatibility\n367   \"\"\"\n--> 368   return rank_internal(input, name, optimize=True)\n369\n370\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank_internal(input, name, optimize)\n386       return gen_array_ops.size(input.dense_shape, name=name)\n387     else:\n--> 388       input_tensor = ops.convert_to_tensor(input)\n389       input_shape = input_tensor.get_shape()\n390       if optimize and input_shape.ndims is not None:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\n1046       name=name,\n1047       preferred_dtype=preferred_dtype,\n-> 1048       as_ref=False)\n1049\n1050\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n1142\n1143     if ret is None:\n-> 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n1145\n1146     if ret is NotImplemented:\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\n226                                          as_ref=False):\n227   _ = as_ref\n--> 228   return constant(v, dtype=dtype, name=name)\n229\n230\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\n176   ctx = context.context()\n177   if ctx.executing_eagerly():\n--> 178     t = convert_to_eager_tensor(value, ctx, dtype)\n179     if shape is None:\n180       return t\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\n111     return t\n112   else:\n--> 113     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\n114\n115\nValueError: Attempt to convert a value (<DeferredTensor 'input_2' shape=(?, 10) dtype=float32>) with an unsupported type (<class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>) to a Tensor.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: \r\n- **TensorFlow installed from (source or binary)**: Colab\r\n- **TensorFlow version (use command below)**:  1.11.0-rc2\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: Colab\r\n- **GCC/Compiler version (if compiling from source)**: Colab\r\n- **CUDA/cuDNN version**: Colab\r\n- **GPU model and memory**: Colab\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a custom loss function which requires some specific tensors to be computed based on other tensors generated from the model itself.  Unfortunately, this can not be done in eager mode.\r\n\r\nPlease find below a toy example that illustrates the issue. The code runs without problems in the graph mode.\r\nI really need a workaround. Could you please help.\r\n\r\nThanks.\r\n\r\n### Source code \r\n```\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras\r\nimport tensorflow.keras.backend as K\r\nimport numpy as np\r\nfrom tensorflow.keras.layers import Dense, Input, Layer\r\nfrom tensorflow.keras.models import Model\r\n\r\ntf.enable_eager_execution()\r\n\r\ndef custom_loss_wrapper(input_tensor):\r\n    def custom_loss(y_true, y_pred):\r\n        return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\r\n    return custom_loss\r\n  \r\ninput_tensor = Input(shape=(10,))\r\nhidden = Dense(100, activation='relu')(input_tensor)\r\nout = Dense(1, activation='sigmoid')(hidden)\r\nmodel = Model(input_tensor, out)\r\nmodel.compile(loss=custom_loss_wrapper(input_tensor), optimizer=tf.train.AdamOptimizer(learning_rate=0.001))\r\n\r\nnp.random.seed(0)\r\nX = np.random.random((3, 10)).astype(np.float32)\r\nY1 = np.random.random((3, 1)).astype(np.float32)\r\nmodel.fit(x=X, y=Y1, batch_size=1, epochs=10)\r\n```\r\n\r\n### logs\r\nEpoch 1/10\r\n\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-3-9dbf3db39e42> in <module>()\r\n     22 X = np.random.random((3, 10)).astype(np.float32)\r\n     23 Y1 = np.random.random((3, 1)).astype(np.float32)\r\n---> 24 model.fit(x=X, y=Y1, batch_size=1, epochs=10)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1577           initial_epoch=initial_epoch,\r\n   1578           steps_per_epoch=steps_per_epoch,\r\n-> 1579           validation_steps=validation_steps)\r\n   1580     elif self._distribution_strategy:\r\n   1581       return training_distributed.fit_loop(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in fit_loop(model, inputs, targets, sample_weights, class_weight, val_inputs, val_targets, val_sample_weights, batch_size, epochs, verbose, callbacks, shuffle, initial_epoch, steps_per_epoch, validation_steps)\r\n    696           validation_steps=validation_steps,\r\n    697           do_validation=do_validation,\r\n--> 698           batch_size=batch_size)\r\n    699       callbacks.on_epoch_end(epoch, epoch_logs)\r\n    700       if callbacks.model.stop_training:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in iterator_fit_loop(model, inputs, class_weight, steps_per_epoch, epoch_logs, val_inputs, val_targets, val_sample_weights, epochs, verbose, callbacks, validation_steps, do_validation, batch_size)\r\n    248     # Train model.\r\n    249     outs, loss, loss_metrics, masks = _process_single_batch(\r\n--> 250         model, x, y, sample_weights=sample_weights, training=True)\r\n    251     outs = generic_utils.to_list(outs)\r\n    252 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _process_single_batch(model, inputs, targets, sample_weights, training)\r\n    502           targets,\r\n    503           sample_weights=sample_weights,\r\n--> 504           training=training)\r\n    505       if loss is None:\r\n    506         raise ValueError('The model cannot be run '\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_eager.py in _model_loss(model, inputs, targets, sample_weights, training)\r\n    110       with backend.name_scope(model.output_names[i] + '_loss'):\r\n    111         output_loss = weighted_masked_fn(\r\n--> 112             targets[i], outs[i], weights, mask=mask)\r\n    113       # If the number of outputs is 1 then we don't append the loss metric\r\n    114       # associated with each model output. When there are multiple outputs\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_utils.py in weighted(y_true, y_pred, weights, mask)\r\n    569     \"\"\"\r\n    570     # score_array has ndim >= 2\r\n--> 571     score_array = fn(y_true, y_pred)\r\n    572     if mask is not None:\r\n    573       mask = math_ops.cast(mask, y_pred.dtype)\r\n\r\n<ipython-input-3-9dbf3db39e42> in custom_loss(y_true, y_pred)\r\n     10 def custom_loss_wrapper(input_tensor):\r\n     11     def custom_loss(y_true, y_pred):\r\n---> 12         return K.binary_crossentropy(y_true, y_pred) + K.mean(input_tensor)\r\n     13     return custom_loss\r\n     14 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py in mean(x, axis, keepdims)\r\n   1727   if x.dtype.base_dtype == dtypes_module.bool:\r\n   1728     x = math_ops.cast(x, floatx())\r\n-> 1729   return math_ops.reduce_mean(x, axis, keepdims)\r\n   1730 \r\n   1731 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py in new_func(*args, **kwargs)\r\n    486                 'in a future version' if date is None else ('after %s' % date),\r\n    487                 instructions)\r\n--> 488       return func(*args, **kwargs)\r\n    489     return tf_decorator.make_decorator(func, new_func, 'deprecated',\r\n    490                                        _add_deprecated_arg_notice_to_docstring(\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in reduce_mean(input_tensor, axis, keepdims, name, reduction_indices, keep_dims)\r\n   1488                                    input_tensor,\r\n   1489                                    _ReductionDims(input_tensor, axis,\r\n-> 1490                                                   reduction_indices),\r\n   1491                                    keepdims,\r\n   1492                                    name=name))\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _ReductionDims(x, axis, reduction_indices)\r\n   1270 \r\n   1271     # Otherwise, we rely on Range and Rank to do the right thing at run-time.\r\n-> 1272     return range(0, array_ops.rank(x))\r\n   1273 \r\n   1274 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank(input, name)\r\n    366   @end_compatibility\r\n    367   \"\"\"\r\n--> 368   return rank_internal(input, name, optimize=True)\r\n    369 \r\n    370 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_ops.py in rank_internal(input, name, optimize)\r\n    386       return gen_array_ops.size(input.dense_shape, name=name)\r\n    387     else:\r\n--> 388       input_tensor = ops.convert_to_tensor(input)\r\n    389       input_shape = input_tensor.get_shape()\r\n    390       if optimize and input_shape.ndims is not None:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n   1046       name=name,\r\n   1047       preferred_dtype=preferred_dtype,\r\n-> 1048       as_ref=False)\r\n   1049 \r\n   1050 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1142 \r\n   1143     if ret is None:\r\n-> 1144       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1145 \r\n   1146     if ret is NotImplemented:\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in _constant_tensor_conversion_function(v, dtype, name, as_ref)\r\n    226                                          as_ref=False):\r\n    227   _ = as_ref\r\n--> 228   return constant(v, dtype=dtype, name=name)\r\n    229 \r\n    230 \r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in constant(value, dtype, shape, name, verify_shape)\r\n    176   ctx = context.context()\r\n    177   if ctx.executing_eagerly():\r\n--> 178     t = convert_to_eager_tensor(value, ctx, dtype)\r\n    179     if shape is None:\r\n    180       return t\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/constant_op.py in convert_to_eager_tensor(value, ctx, dtype)\r\n    111     return t\r\n    112   else:\r\n--> 113     return ops.EagerTensor(value, context=handle, device=device, dtype=dtype)\r\n    114 \r\n    115 \r\n\r\nValueError: Attempt to convert a value (<DeferredTensor 'input_2' shape=(?, 10) dtype=float32>) with an unsupported type (<class 'tensorflow.python.keras.engine.base_layer.DeferredTensor'>) to a Tensor."}