{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215856113", "html_url": "https://github.com/tensorflow/tensorflow/issues/2158#issuecomment-215856113", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2158", "id": 215856113, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTg1NjExMw==", "user": {"login": "rafaljozefowicz", "id": 2983769, "node_id": "MDQ6VXNlcjI5ODM3Njk=", "avatar_url": "https://avatars1.githubusercontent.com/u/2983769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rafaljozefowicz", "html_url": "https://github.com/rafaljozefowicz", "followers_url": "https://api.github.com/users/rafaljozefowicz/followers", "following_url": "https://api.github.com/users/rafaljozefowicz/following{/other_user}", "gists_url": "https://api.github.com/users/rafaljozefowicz/gists{/gist_id}", "starred_url": "https://api.github.com/users/rafaljozefowicz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rafaljozefowicz/subscriptions", "organizations_url": "https://api.github.com/users/rafaljozefowicz/orgs", "repos_url": "https://api.github.com/users/rafaljozefowicz/repos", "events_url": "https://api.github.com/users/rafaljozefowicz/events{/privacy}", "received_events_url": "https://api.github.com/users/rafaljozefowicz/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-29T19:31:10Z", "updated_at": "2016-04-29T19:31:10Z", "author_association": "NONE", "body_html": "<p>in <code>seq2seq.sequence_loss_by_example</code> the averaging happens across timesteps but only if the timesteps are specified as a list of elements.</p>\n<p>In PTB case we concatenate all the steps before calling that function and <code>log_perps</code> ([batch_size * num_steps]) is divided by tf.ones([batch_size * num_steps]) which is esentially a no-op.</p>\n<p>Instead of calling sequence_loss_by_example we could directly call sparse_cross_entropy_with_logits because that's the only thing of interest here. In the past, that function was more complicated and we didn't want to copy its logic into ptb_word_lm.py. Feel free to send a PR.</p>\n<p>Hope that makes sense!</p>", "body_text": "in seq2seq.sequence_loss_by_example the averaging happens across timesteps but only if the timesteps are specified as a list of elements.\nIn PTB case we concatenate all the steps before calling that function and log_perps ([batch_size * num_steps]) is divided by tf.ones([batch_size * num_steps]) which is esentially a no-op.\nInstead of calling sequence_loss_by_example we could directly call sparse_cross_entropy_with_logits because that's the only thing of interest here. In the past, that function was more complicated and we didn't want to copy its logic into ptb_word_lm.py. Feel free to send a PR.\nHope that makes sense!", "body": "in `seq2seq.sequence_loss_by_example` the averaging happens across timesteps but only if the timesteps are specified as a list of elements.\n\nIn PTB case we concatenate all the steps before calling that function and `log_perps` ([batch_size \\* num_steps]) is divided by tf.ones([batch_size \\* num_steps]) which is esentially a no-op.\n\nInstead of calling sequence_loss_by_example we could directly call sparse_cross_entropy_with_logits because that's the only thing of interest here. In the past, that function was more complicated and we didn't want to copy its logic into ptb_word_lm.py. Feel free to send a PR.\n\nHope that makes sense!\n"}