{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2158", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2158/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2158/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2158/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2158", "id": 151736401, "node_id": "MDU6SXNzdWUxNTE3MzY0MDE=", "number": 2158, "title": "Division by num_steps when calculating language model perplexity", "user": {"login": "giancds", "id": 3305726, "node_id": "MDQ6VXNlcjMzMDU3MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/3305726?v=4", "gravatar_id": "", "url": "https://api.github.com/users/giancds", "html_url": "https://github.com/giancds", "followers_url": "https://api.github.com/users/giancds/followers", "following_url": "https://api.github.com/users/giancds/following{/other_user}", "gists_url": "https://api.github.com/users/giancds/gists{/gist_id}", "starred_url": "https://api.github.com/users/giancds/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/giancds/subscriptions", "organizations_url": "https://api.github.com/users/giancds/orgs", "repos_url": "https://api.github.com/users/giancds/repos", "events_url": "https://api.github.com/users/giancds/events{/privacy}", "received_events_url": "https://api.github.com/users/giancds/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-04-28T20:32:21Z", "updated_at": "2016-10-10T18:27:59Z", "closed_at": "2016-10-10T18:27:59Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>I think I found something on the language modeling example that might be a bug:</p>\n<p>As pointed in the tutorial, the perplexity can obtained from the averaged cross entropy. The code show <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L260\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L263\">here</a> the division of the costs by <code>num_iters</code>.</p>\n<p>Nevertheless, the seq2seq.sequence_loss_by_example is averaging losses across timesteps <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L871\">(here)</a>. As no value is passed to the parameter <code>average_across_timesteps</code> I'm assuming it is using the default value <code>True</code>.</p>\n<p>After that, the model is further averaging the losses by the batch size <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L133\">here</a>.</p>\n<p>If I'm not wrong (and please correct me if I am), there is no need to divide the costs by <code>num_iters</code> as we already averaged over timesteps and the batch. There is no reference in the tutorial regarding why one should do that. If it is something that we should do it would be nice to include an instruction in the tutorial or a comment in the code. In addition, the Seq2seq tutorial is also showing the perplexity calculation and does not have any further division by the number of steps representing the length of the sequence, but by the number of steps between two verbosity points.</p>\n<p>Also, imo, if the intention is averaging, the <code>step</code> parameter <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L249\">here</a>  is the one that should be added to <code>iters</code>.</p>", "body_text": "Hi all,\nI think I found something on the language modeling example that might be a bug:\nAs pointed in the tutorial, the perplexity can obtained from the averaged cross entropy. The code show here and here the division of the costs by num_iters.\nNevertheless, the seq2seq.sequence_loss_by_example is averaging losses across timesteps (here). As no value is passed to the parameter average_across_timesteps I'm assuming it is using the default value True.\nAfter that, the model is further averaging the losses by the batch size here.\nIf I'm not wrong (and please correct me if I am), there is no need to divide the costs by num_iters as we already averaged over timesteps and the batch. There is no reference in the tutorial regarding why one should do that. If it is something that we should do it would be nice to include an instruction in the tutorial or a comment in the code. In addition, the Seq2seq tutorial is also showing the perplexity calculation and does not have any further division by the number of steps representing the length of the sequence, but by the number of steps between two verbosity points.\nAlso, imo, if the intention is averaging, the step parameter here  is the one that should be added to iters.", "body": "Hi all,\n\nI think I found something on the language modeling example that might be a bug: \n\nAs pointed in the tutorial, the perplexity can obtained from the averaged cross entropy. The code show [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L260) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L263) the division of the costs by `num_iters`.\n\nNevertheless, the seq2seq.sequence_loss_by_example is averaging losses across timesteps [(here)](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L871). As no value is passed to the parameter `average_across_timesteps` I'm assuming it is using the default value `True`.\n\nAfter that, the model is further averaging the losses by the batch size [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L133).\n\nIf I'm not wrong (and please correct me if I am), there is no need to divide the costs by `num_iters` as we already averaged over timesteps and the batch. There is no reference in the tutorial regarding why one should do that. If it is something that we should do it would be nice to include an instruction in the tutorial or a comment in the code. In addition, the Seq2seq tutorial is also showing the perplexity calculation and does not have any further division by the number of steps representing the length of the sequence, but by the number of steps between two verbosity points. \n\nAlso, imo, if the intention is averaging, the `step` parameter [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/ptb/ptb_word_lm.py#L249)  is the one that should be added to `iters`.\n"}