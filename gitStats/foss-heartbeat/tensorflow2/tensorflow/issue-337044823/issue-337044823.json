{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20418", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20418/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20418/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20418/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20418", "id": 337044823, "node_id": "MDU6SXNzdWUzMzcwNDQ4MjM=", "number": 20418, "title": "Error when using tf.contrib.metrics.count in eval_metric_ops of tf.estimator.EstimatorSpec", "user": {"login": "tgenin", "id": 6623268, "node_id": "MDQ6VXNlcjY2MjMyNjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/6623268?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tgenin", "html_url": "https://github.com/tgenin", "followers_url": "https://api.github.com/users/tgenin/followers", "following_url": "https://api.github.com/users/tgenin/following{/other_user}", "gists_url": "https://api.github.com/users/tgenin/gists{/gist_id}", "starred_url": "https://api.github.com/users/tgenin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tgenin/subscriptions", "organizations_url": "https://api.github.com/users/tgenin/orgs", "repos_url": "https://api.github.com/users/tgenin/repos", "events_url": "https://api.github.com/users/tgenin/events{/privacy}", "received_events_url": "https://api.github.com/users/tgenin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-06-29T15:34:48Z", "updated_at": "2018-08-01T22:44:54Z", "closed_at": "2018-08-01T22:44:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac Os 10.13.5</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install</li>\n<li><strong>TensorFlow version (use command below)</strong>:  tested on v1.8.0-0-g93bc2e2072 1.8.0 and v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: no gpu</li>\n<li><strong>Exact command to reproduce</strong>: see description</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hello,</p>\n<p>It seems that tf.contrib.metrics.count does not work into<br>\neval_metric_ops of tf.estimator.EstimatorSpec during validation<br>\n(I just wanted to check the size of my validation set when debugging)</p>\n<p>It raises</p>\n<pre><code>~/.virtualenvs/python3/lib/python3.5/site-packages/tensorflow/python/estimator/model_fn.py in _check_is_tensor_or_operation(x, name)\n    388 def _check_is_tensor_or_operation(x, name):\n    389   if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):\n--&gt; 390     raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))\n    391 \n    392 \n\nTypeError: eval_metric_ops[count] must be Operation or Tensor, given: &lt;tf.Variable 'count/count:0' shape=() dtype=float32_ref&gt;\n</code></pre>\n<p>It seems that the first returned argument (count_) in<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3722\">https://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3722</a></p>\n<p>is type <code>&lt;tf.Variable 'count/count:0' shape=() dtype=float32_ref&gt;</code></p>\n<p>Shoud not it be converted into</p>\n<p><code>&lt;tf.Tensor 'count/count/read:0' shape=() dtype=float32&gt;</code></p>\n<p>using math_ops.float32() (or something else ?) like in other metrics definitions ?</p>\n<h3>Source code / logs</h3>\n<p>Somewhere in my generate_model_fn():</p>\n<pre><code>        if mode == tf.estimator.ModeKeys.EVAL:\n            eval_metrics = _create_evaluation_metrics(outputs, labels)\n            return tf.estimator.EstimatorSpec(mode, loss=loss, \n                                              eval_metric_ops=eval_metrics)\n</code></pre>\n<p>with</p>\n<pre><code>def _create_evaluation_metrics(outputs, labels):\n    count = tf.contrib.metrics.count(outputs)\n    return {\n        \"count\": count\n    }\n</code></pre>\n<p>raises the <code>TypeError: eval_metric_ops[count] must be Operation or Tensor, given: &lt;tf.Variable 'count/count:0' shape=() dtype=float32_ref&gt;</code> during validation phase</p>\n<p>but works with</p>\n<pre><code>from tensorflow.python.ops import math_ops\n\ndef _create_evaluation_metrics(outputs, labels):\n    count, op = tf.contrib.metrics.count(outputs)\n    return {\n        \"count\": (math_ops.to_float(count), op)\n    }\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac Os 10.13.5\nTensorFlow installed from (source or binary): pip install\nTensorFlow version (use command below):  tested on v1.8.0-0-g93bc2e2072 1.8.0 and v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1\nPython version: 3.5.2\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: no gpu\nExact command to reproduce: see description\n\nDescribe the problem\nHello,\nIt seems that tf.contrib.metrics.count does not work into\neval_metric_ops of tf.estimator.EstimatorSpec during validation\n(I just wanted to check the size of my validation set when debugging)\nIt raises\n~/.virtualenvs/python3/lib/python3.5/site-packages/tensorflow/python/estimator/model_fn.py in _check_is_tensor_or_operation(x, name)\n    388 def _check_is_tensor_or_operation(x, name):\n    389   if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):\n--> 390     raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))\n    391 \n    392 \n\nTypeError: eval_metric_ops[count] must be Operation or Tensor, given: <tf.Variable 'count/count:0' shape=() dtype=float32_ref>\n\nIt seems that the first returned argument (count_) in\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3722\nis type <tf.Variable 'count/count:0' shape=() dtype=float32_ref>\nShoud not it be converted into\n<tf.Tensor 'count/count/read:0' shape=() dtype=float32>\nusing math_ops.float32() (or something else ?) like in other metrics definitions ?\nSource code / logs\nSomewhere in my generate_model_fn():\n        if mode == tf.estimator.ModeKeys.EVAL:\n            eval_metrics = _create_evaluation_metrics(outputs, labels)\n            return tf.estimator.EstimatorSpec(mode, loss=loss, \n                                              eval_metric_ops=eval_metrics)\n\nwith\ndef _create_evaluation_metrics(outputs, labels):\n    count = tf.contrib.metrics.count(outputs)\n    return {\n        \"count\": count\n    }\n\nraises the TypeError: eval_metric_ops[count] must be Operation or Tensor, given: <tf.Variable 'count/count:0' shape=() dtype=float32_ref> during validation phase\nbut works with\nfrom tensorflow.python.ops import math_ops\n\ndef _create_evaluation_metrics(outputs, labels):\n    count, op = tf.contrib.metrics.count(outputs)\n    return {\n        \"count\": (math_ops.to_float(count), op)\n    }", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac Os 10.13.5\r\n- **TensorFlow installed from (source or binary)**: pip install\r\n- **TensorFlow version (use command below)**:  tested on v1.8.0-0-g93bc2e2072 1.8.0 and v1.9.0-rc0-35-g17d6639b55 1.9.0-rc1\r\n- **Python version**: 3.5.2\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: no gpu\r\n- **Exact command to reproduce**: see description\r\n\r\n### Describe the problem\r\n\r\nHello,\r\n\r\nIt seems that tf.contrib.metrics.count does not work into\r\neval_metric_ops of tf.estimator.EstimatorSpec during validation\r\n(I just wanted to check the size of my validation set when debugging)\r\n\r\nIt raises\r\n```\r\n~/.virtualenvs/python3/lib/python3.5/site-packages/tensorflow/python/estimator/model_fn.py in _check_is_tensor_or_operation(x, name)\r\n    388 def _check_is_tensor_or_operation(x, name):\r\n    389   if not (isinstance(x, ops.Operation) or isinstance(x, ops.Tensor)):\r\n--> 390     raise TypeError('{} must be Operation or Tensor, given: {}'.format(name, x))\r\n    391 \r\n    392 \r\n\r\nTypeError: eval_metric_ops[count] must be Operation or Tensor, given: <tf.Variable 'count/count:0' shape=() dtype=float32_ref>\r\n```\r\n\r\nIt seems that the first returned argument (count_) in \r\nhttps://github.com/tensorflow/tensorflow/blob/r1.9/tensorflow/contrib/metrics/python/ops/metric_ops.py#L3722\r\n\r\nis type `<tf.Variable 'count/count:0' shape=() dtype=float32_ref>`\r\n\r\nShoud not it be converted into \r\n\r\n`<tf.Tensor 'count/count/read:0' shape=() dtype=float32>`\r\n\r\nusing math_ops.float32() (or something else ?) like in other metrics definitions ?\r\n\r\n### Source code / logs\r\n\r\nSomewhere in my generate_model_fn():\r\n\r\n```\r\n        if mode == tf.estimator.ModeKeys.EVAL:\r\n            eval_metrics = _create_evaluation_metrics(outputs, labels)\r\n            return tf.estimator.EstimatorSpec(mode, loss=loss, \r\n                                              eval_metric_ops=eval_metrics)\r\n```\r\n\r\nwith \r\n\r\n```\r\ndef _create_evaluation_metrics(outputs, labels):\r\n    count = tf.contrib.metrics.count(outputs)\r\n    return {\r\n        \"count\": count\r\n    }\r\n```\r\n\r\nraises the `TypeError: eval_metric_ops[count] must be Operation or Tensor, given: <tf.Variable 'count/count:0' shape=() dtype=float32_ref>` during validation phase\r\n\r\nbut works with\r\n\r\n```\r\nfrom tensorflow.python.ops import math_ops\r\n\r\ndef _create_evaluation_metrics(outputs, labels):\r\n    count, op = tf.contrib.metrics.count(outputs)\r\n    return {\r\n        \"count\": (math_ops.to_float(count), op)\r\n    }\r\n```"}