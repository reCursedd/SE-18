{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12420", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12420/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12420/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12420/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12420", "id": 251459946, "node_id": "MDU6SXNzdWUyNTE0NTk5NDY=", "number": 12420, "title": "ValueError: No attr named '_XlaCompile' and AttributeError: 'NoneType' object has no attribute 'back_prop' with tf.while_loop and templates", "user": {"login": "3rd3", "id": 2372391, "node_id": "MDQ6VXNlcjIzNzIzOTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/2372391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/3rd3", "html_url": "https://github.com/3rd3", "followers_url": "https://api.github.com/users/3rd3/followers", "following_url": "https://api.github.com/users/3rd3/following{/other_user}", "gists_url": "https://api.github.com/users/3rd3/gists{/gist_id}", "starred_url": "https://api.github.com/users/3rd3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/3rd3/subscriptions", "organizations_url": "https://api.github.com/users/3rd3/orgs", "repos_url": "https://api.github.com/users/3rd3/repos", "events_url": "https://api.github.com/users/3rd3/events{/privacy}", "received_events_url": "https://api.github.com/users/3rd3/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-08-20T01:25:47Z", "updated_at": "2017-08-25T21:47:53Z", "closed_at": "2017-08-25T21:47:53Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2, 1.3</li>\n<li><strong>Python version</strong>: 3.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 5, 6</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 Ti</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I run <code>tf.train.Optimizer().minimize(loss)</code> I receive the error message below. The code is a bit unwieldy, so I just provide the overall structure of my code:</p>\n<pre><code>x = { ... }\ndef some_template(..):\n  bar(x[..])\ndef foo(..):\n  return foobar(x[..])\ntemplate = tf.make_template(\"my_template\", some_template)\ndef loop_body(inputs, ..):\n  for i in range(x[..]):\n    net = foo(inputs)\n    net = template(net)\n  return inputs, ..\nnet, *_ = tf.while_loop(cond, loop_body, vars)\n</code></pre>\n<p>The functions and templates that are called in the loop body access some Python variables from the outer scope (read-only, i.e. without side-effects), and they are, of course, expected to be constant at run-time. The graph construction seems to work perfectly fine. Only when I construct the first minimization operation, it fails at the first graph node that makes use of a variable (<code>Model/ModuleB_0/b_46/</code> in this case). One thing that might be unusual about my graph is that it uses an earlier part of the graph as target for the outputs, so the loss is defined as a function of <code>output_node</code> and <code>tf.stop_gradient(earlier_node)</code>. What might maybe also be interesting is that the <code>frame_name</code> below contains the same path concatenated twice <code>Model/ModuleB_0/while/Model/ModuleB_0/while/</code>. I also have some <code>trainable=True</code> variables which are not in the subgraph of the training op, but they are not in the <code>var_list</code> argument for the training op, so that should be fine. Is this a bug or am I doing something wrong?</p>\n<h3>Source code / logs</h3>\n<pre><code>Traceback (most recent call last):\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.\ny\", line 343, in _MaybeCompile\n    xla_compile = op.get_attr(\"_XlaCompile\")\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\",\nine 1705, in get_attr\n    str(self._node_def))\nValueError: No attr named '_XlaCompile' in name: \"Model/ModuleB_0/while/Level_4/BottomRight/Conv2D\nayer_2/BiasAdd/Enter\"\nop: \"Enter\"\ninput: \"Model/ModuleB_0/b_46/read\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"frame_name\"\n  value {\n    s: \"Model/ModuleB_0/while/Model/ModuleB_0/while/\"\n  }\n}\nattr {\n  key: \"is_constant\"\n  value {\n    b: true\n  }\n}\nattr {\n  key: \"parallel_iterations\"\n  value {\n    i: 10\n  }\n}\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"main.py\", line 98, in &lt;module&gt;\n    tf.app.run()\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", li\nne 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"main.py\", line 61, in main\n    model = Model(tf.flags.FLAGS.__flags)\n  File \"/media/other_dir/me/Code/model/model.py\", line 193, in __init__\n    scope=\"Model/ModuleA_%i\" % i)\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\ny\", line 315, in minimize\n    grad_loss=grad_loss)\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\ny\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\ny\", line 542, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\ny\", line 348, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\ny\", line 542, in &lt;lambda&gt;\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_gra\nd.py\", line 208, in _EnterGrad\n    if not grad_ctxt.back_prop:\nAttributeError: 'NoneType' object has no attribute 'back_prop'\n\n</code></pre>", "body_text": "System information\n\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow version (use command below): 1.2, 1.3\nPython version: 3.4\nCUDA/cuDNN version: 5, 6\nGPU model and memory: GTX 1080 Ti\n\nDescribe the problem\nWhen I run tf.train.Optimizer().minimize(loss) I receive the error message below. The code is a bit unwieldy, so I just provide the overall structure of my code:\nx = { ... }\ndef some_template(..):\n  bar(x[..])\ndef foo(..):\n  return foobar(x[..])\ntemplate = tf.make_template(\"my_template\", some_template)\ndef loop_body(inputs, ..):\n  for i in range(x[..]):\n    net = foo(inputs)\n    net = template(net)\n  return inputs, ..\nnet, *_ = tf.while_loop(cond, loop_body, vars)\n\nThe functions and templates that are called in the loop body access some Python variables from the outer scope (read-only, i.e. without side-effects), and they are, of course, expected to be constant at run-time. The graph construction seems to work perfectly fine. Only when I construct the first minimization operation, it fails at the first graph node that makes use of a variable (Model/ModuleB_0/b_46/ in this case). One thing that might be unusual about my graph is that it uses an earlier part of the graph as target for the outputs, so the loss is defined as a function of output_node and tf.stop_gradient(earlier_node). What might maybe also be interesting is that the frame_name below contains the same path concatenated twice Model/ModuleB_0/while/Model/ModuleB_0/while/. I also have some trainable=True variables which are not in the subgraph of the training op, but they are not in the var_list argument for the training op, so that should be fine. Is this a bug or am I doing something wrong?\nSource code / logs\nTraceback (most recent call last):\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.\ny\", line 343, in _MaybeCompile\n    xla_compile = op.get_attr(\"_XlaCompile\")\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\",\nine 1705, in get_attr\n    str(self._node_def))\nValueError: No attr named '_XlaCompile' in name: \"Model/ModuleB_0/while/Level_4/BottomRight/Conv2D\nayer_2/BiasAdd/Enter\"\nop: \"Enter\"\ninput: \"Model/ModuleB_0/b_46/read\"\nattr {\n  key: \"T\"\n  value {\n    type: DT_FLOAT\n  }\n}\nattr {\n  key: \"frame_name\"\n  value {\n    s: \"Model/ModuleB_0/while/Model/ModuleB_0/while/\"\n  }\n}\nattr {\n  key: \"is_constant\"\n  value {\n    b: true\n  }\n}\nattr {\n  key: \"parallel_iterations\"\n  value {\n    i: 10\n  }\n}\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"main.py\", line 98, in <module>\n    tf.app.run()\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", li\nne 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"main.py\", line 61, in main\n    model = Model(tf.flags.FLAGS.__flags)\n  File \"/media/other_dir/me/Code/model/model.py\", line 193, in __init__\n    scope=\"Model/ModuleA_%i\" % i)\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\ny\", line 315, in minimize\n    grad_loss=grad_loss)\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\ny\", line 386, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\ny\", line 542, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\ny\", line 348, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\ny\", line 542, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_gra\nd.py\", line 208, in _EnterGrad\n    if not grad_ctxt.back_prop:\nAttributeError: 'NoneType' object has no attribute 'back_prop'", "body": "### System information\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow version (use command below)**: 1.2, 1.3\r\n- **Python version**: 3.4\r\n- **CUDA/cuDNN version**: 5, 6\r\n- **GPU model and memory**: GTX 1080 Ti\r\n\r\n### Describe the problem\r\n\r\nWhen I run `tf.train.Optimizer().minimize(loss)` I receive the error message below. The code is a bit unwieldy, so I just provide the overall structure of my code:\r\n\r\n```\r\nx = { ... }\r\ndef some_template(..):\r\n  bar(x[..])\r\ndef foo(..):\r\n  return foobar(x[..])\r\ntemplate = tf.make_template(\"my_template\", some_template)\r\ndef loop_body(inputs, ..):\r\n  for i in range(x[..]):\r\n    net = foo(inputs)\r\n    net = template(net)\r\n  return inputs, ..\r\nnet, *_ = tf.while_loop(cond, loop_body, vars)\r\n```\r\n\r\nThe functions and templates that are called in the loop body access some Python variables from the outer scope (read-only, i.e. without side-effects), and they are, of course, expected to be constant at run-time. The graph construction seems to work perfectly fine. Only when I construct the first minimization operation, it fails at the first graph node that makes use of a variable (`Model/ModuleB_0/b_46/` in this case). One thing that might be unusual about my graph is that it uses an earlier part of the graph as target for the outputs, so the loss is defined as a function of `output_node` and `tf.stop_gradient(earlier_node)`. What might maybe also be interesting is that the `frame_name` below contains the same path concatenated twice `Model/ModuleB_0/while/Model/ModuleB_0/while/`. I also have some `trainable=True` variables which are not in the subgraph of the training op, but they are not in the `var_list` argument for the training op, so that should be fine. Is this a bug or am I doing something wrong?\r\n\r\n### Source code / logs\r\n```\r\nTraceback (most recent call last):\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.\r\ny\", line 343, in _MaybeCompile\r\n    xla_compile = op.get_attr(\"_XlaCompile\")\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/framework/ops.py\",\r\nine 1705, in get_attr\r\n    str(self._node_def))\r\nValueError: No attr named '_XlaCompile' in name: \"Model/ModuleB_0/while/Level_4/BottomRight/Conv2D\r\nayer_2/BiasAdd/Enter\"\r\nop: \"Enter\"\r\ninput: \"Model/ModuleB_0/b_46/read\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"frame_name\"\r\n  value {\r\n    s: \"Model/ModuleB_0/while/Model/ModuleB_0/while/\"\r\n  }\r\n}\r\nattr {\r\n  key: \"is_constant\"\r\n  value {\r\n    b: true\r\n  }\r\n}\r\nattr {\r\n  key: \"parallel_iterations\"\r\n  value {\r\n    i: 10\r\n  }\r\n}\r\n\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 98, in <module>\r\n    tf.app.run()\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/platform/app.py\", li\r\nne 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"main.py\", line 61, in main\r\n    model = Model(tf.flags.FLAGS.__flags)\r\n  File \"/media/other_dir/me/Code/model/model.py\", line 193, in __init__\r\n    scope=\"Model/ModuleA_%i\" % i)\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\r\ny\", line 315, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/training/optimizer.p\r\ny\", line 386, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\r\ny\", line 542, in gradients\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\r\ny\", line 348, in _MaybeCompile\r\n    return grad_fn()  # Exit early\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/gradients_impl.p\r\ny\", line 542, in <lambda>\r\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\r\n  File \"/media/other_dir/me/venv3/lib/python3.4/site-packages/tensorflow/python/ops/control_flow_gra\r\nd.py\", line 208, in _EnterGrad\r\n    if not grad_ctxt.back_prop:\r\nAttributeError: 'NoneType' object has no attribute 'back_prop'\r\n\r\n```"}