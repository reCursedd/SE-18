{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/408859436", "html_url": "https://github.com/tensorflow/tensorflow/issues/20942#issuecomment-408859436", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20942", "id": 408859436, "node_id": "MDEyOklzc3VlQ29tbWVudDQwODg1OTQzNg==", "user": {"login": "JRMeyer", "id": 8389864, "node_id": "MDQ6VXNlcjgzODk4NjQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/8389864?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JRMeyer", "html_url": "https://github.com/JRMeyer", "followers_url": "https://api.github.com/users/JRMeyer/followers", "following_url": "https://api.github.com/users/JRMeyer/following{/other_user}", "gists_url": "https://api.github.com/users/JRMeyer/gists{/gist_id}", "starred_url": "https://api.github.com/users/JRMeyer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JRMeyer/subscriptions", "organizations_url": "https://api.github.com/users/JRMeyer/orgs", "repos_url": "https://api.github.com/users/JRMeyer/repos", "events_url": "https://api.github.com/users/JRMeyer/events{/privacy}", "received_events_url": "https://api.github.com/users/JRMeyer/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-30T13:16:53Z", "updated_at": "2018-07-30T13:16:53Z", "author_association": "NONE", "body_html": "<p>Chiming in here:</p>\n<p>I've been using <code>KMeansClustering</code> with a single GPU, and I <strong>do</strong> see activity on the GPU<br>\nwhen I look via <code>nvidia-smi</code>, so now after reading this thread I'm puzzled because I'm using<br>\nthe standard pre-canned <code>KMeansClustering</code>.</p>\n<p>But, that's not why I found this issue. I'm here because I'd like to distribute training of <code>KmeansClustering</code> via <code>tf.contrib.distribute.MirroredStrategy</code>.</p>\n<p>I've tried to set up distributed training with the following code:</p>\n<pre><code>import tensorflow as tf\nimport multiprocessing\n\ndef parse_fn(record):\n    '''\n    features={\n        'feats': tf.FixedLenFeature([], tf.string)\n    }\n    parsed = tf.parse_single_example(record, features)\n    feats= tf.convert_to_tensor(tf.decode_raw(parsed['feats'], tf.float64))\n\n    return {'feats': feats}                                                                                                                                 \n\n\n\ndef my_input_fn(tfrecords_path, model):\n    dataset = (\n        tf.data.TFRecordDataset(tfrecords_path)\n        .apply(\n            tf.contrib.data.map_and_batch(\n                map_func=parse_fn,\n                batch_size=4096,\n                num_parallel_batches=multiprocessing.cpu_count()\n            )\n        )\n        .prefetch(4096)\n    )\n\n    return dataset\n\n\n### Multi-GPU training config ###                                                                                                                                                                           \ndistribution = tf.contrib.distribute.MirroredStrategy()\nrun_config = tf.estimator.RunConfig(train_distribute=distribution)\n\ntrain_spec_kmeans = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))\neval_spec_kmeans = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\n\nKMeansEstimator = tf.contrib.factorization.KMeansClustering(\n    num_clusters=1024,\n    feature_columns = [tf.feature_column.numeric_column(\n        key='feats',\n        dtype=tf.float64,\n        shape=(806,),\n    )],          \n    model_dir = '/tmp/tf',\n    use_mini_batch = True,\n    config = run_config)\n\n\ntf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)\n</code></pre>\n<p>However, when I set up the training to use <code> tf.contrib.distribute.MirroredStrategy</code>, it crashes<br>\nwith the following Error:</p>\n<pre><code>2018-07-30 11:42:05.948549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\n2018-07-30 11:42:05.948629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-30 11:42:05.948646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \n2018-07-30 11:42:05.948660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \n2018-07-30 11:42:05.948669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \n2018-07-30 11:42:05.948676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \n2018-07-30 11:42:05.948687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \n2018-07-30 11:42:05.949163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n2018-07-30 11:42:05.949304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -&gt; physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n2018-07-30 11:42:05.949936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -&gt; physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n2018-07-30 11:42:05.950061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -&gt; physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\n    c_op = c_api.TF_FinishOperation(op_desc)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'cond/cond_1/concat' (op: 'ConcatV2') with input shapes: [0], [?,806], [].\n</code></pre>\n<p>In this case, TF finds all four of my GPUs, but the shape of the data is off.</p>\n<p>I run this exact same code without <code>MirroredStrategy</code> everything works fine (but I do have to add an <code>Iterator</code> object back in).</p>\n<p>Thoughts?</p>", "body_text": "Chiming in here:\nI've been using KMeansClustering with a single GPU, and I do see activity on the GPU\nwhen I look via nvidia-smi, so now after reading this thread I'm puzzled because I'm using\nthe standard pre-canned KMeansClustering.\nBut, that's not why I found this issue. I'm here because I'd like to distribute training of KmeansClustering via tf.contrib.distribute.MirroredStrategy.\nI've tried to set up distributed training with the following code:\nimport tensorflow as tf\nimport multiprocessing\n\ndef parse_fn(record):\n    '''\n    features={\n        'feats': tf.FixedLenFeature([], tf.string)\n    }\n    parsed = tf.parse_single_example(record, features)\n    feats= tf.convert_to_tensor(tf.decode_raw(parsed['feats'], tf.float64))\n\n    return {'feats': feats}                                                                                                                                 \n\n\n\ndef my_input_fn(tfrecords_path, model):\n    dataset = (\n        tf.data.TFRecordDataset(tfrecords_path)\n        .apply(\n            tf.contrib.data.map_and_batch(\n                map_func=parse_fn,\n                batch_size=4096,\n                num_parallel_batches=multiprocessing.cpu_count()\n            )\n        )\n        .prefetch(4096)\n    )\n\n    return dataset\n\n\n### Multi-GPU training config ###                                                                                                                                                                           \ndistribution = tf.contrib.distribute.MirroredStrategy()\nrun_config = tf.estimator.RunConfig(train_distribute=distribution)\n\ntrain_spec_kmeans = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))\neval_spec_kmeans = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\n\nKMeansEstimator = tf.contrib.factorization.KMeansClustering(\n    num_clusters=1024,\n    feature_columns = [tf.feature_column.numeric_column(\n        key='feats',\n        dtype=tf.float64,\n        shape=(806,),\n    )],          \n    model_dir = '/tmp/tf',\n    use_mini_batch = True,\n    config = run_config)\n\n\ntf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)\n\nHowever, when I set up the training to use  tf.contrib.distribute.MirroredStrategy, it crashes\nwith the following Error:\n2018-07-30 11:42:05.948549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\n2018-07-30 11:42:05.948629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-30 11:42:05.948646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \n2018-07-30 11:42:05.948660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \n2018-07-30 11:42:05.948669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \n2018-07-30 11:42:05.948676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \n2018-07-30 11:42:05.948687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \n2018-07-30 11:42:05.949163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\n2018-07-30 11:42:05.949304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\n2018-07-30 11:42:05.949936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\n2018-07-30 11:42:05.950061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\nTraceback (most recent call last):\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\n    c_op = c_api.TF_FinishOperation(op_desc)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'cond/cond_1/concat' (op: 'ConcatV2') with input shapes: [0], [?,806], [].\n\nIn this case, TF finds all four of my GPUs, but the shape of the data is off.\nI run this exact same code without MirroredStrategy everything works fine (but I do have to add an Iterator object back in).\nThoughts?", "body": "Chiming in here:\r\n\r\nI've been using `KMeansClustering` with a single GPU, and I **do** see activity on the GPU\r\nwhen I look via `nvidia-smi`, so now after reading this thread I'm puzzled because I'm using\r\nthe standard pre-canned `KMeansClustering`.\r\n\r\nBut, that's not why I found this issue. I'm here because I'd like to distribute training of `KmeansClustering` via `tf.contrib.distribute.MirroredStrategy`.\r\n\r\nI've tried to set up distributed training with the following code:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport multiprocessing\r\n\r\ndef parse_fn(record):\r\n    '''\r\n    features={\r\n        'feats': tf.FixedLenFeature([], tf.string)\r\n    }\r\n    parsed = tf.parse_single_example(record, features)\r\n    feats= tf.convert_to_tensor(tf.decode_raw(parsed['feats'], tf.float64))\r\n\r\n    return {'feats': feats}                                                                                                                                 \r\n\r\n\r\n\r\ndef my_input_fn(tfrecords_path, model):\r\n    dataset = (\r\n        tf.data.TFRecordDataset(tfrecords_path)\r\n        .apply(\r\n            tf.contrib.data.map_and_batch(\r\n                map_func=parse_fn,\r\n                batch_size=4096,\r\n                num_parallel_batches=multiprocessing.cpu_count()\r\n            )\r\n        )\r\n        .prefetch(4096)\r\n    )\r\n\r\n    return dataset\r\n\r\n\r\n### Multi-GPU training config ###                                                                                                                                                                           \r\ndistribution = tf.contrib.distribute.MirroredStrategy()\r\nrun_config = tf.estimator.RunConfig(train_distribute=distribution)\r\n\r\ntrain_spec_kmeans = tf.estimator.TrainSpec(input_fn = lambda: my_input_fn('train.tfrecords'))\r\neval_spec_kmeans = tf.estimator.EvalSpec(input_fn = lambda: my_input_fn('eval.tfrecords') )\r\n\r\nKMeansEstimator = tf.contrib.factorization.KMeansClustering(\r\n    num_clusters=1024,\r\n    feature_columns = [tf.feature_column.numeric_column(\r\n        key='feats',\r\n        dtype=tf.float64,\r\n        shape=(806,),\r\n    )],          \r\n    model_dir = '/tmp/tf',\r\n    use_mini_batch = True,\r\n    config = run_config)\r\n\r\n\r\ntf.estimator.train_and_evaluate(KMeansEstimator, train_spec_kmeans, eval_spec_kmeans)\r\n```\r\n\r\n\r\nHowever, when I set up the training to use ` tf.contrib.distribute.MirroredStrategy`, it crashes\r\nwith the following Error:\r\n\r\n\r\n```\r\n2018-07-30 11:42:05.948549: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1435] Adding visible gpu devices: 0, 1, 2, 3\r\n2018-07-30 11:42:05.948629: I tensorflow/core/common_runtime/gpu/gpu_device.cc:923] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 11:42:05.948646: I tensorflow/core/common_runtime/gpu/gpu_device.cc:929]      0 1 2 3 \r\n2018-07-30 11:42:05.948660: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 0:   N Y Y Y \r\n2018-07-30 11:42:05.948669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 1:   Y N Y Y \r\n2018-07-30 11:42:05.948676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 2:   Y Y N Y \r\n2018-07-30 11:42:05.948687: I tensorflow/core/common_runtime/gpu/gpu_device.cc:942] 3:   Y Y Y N \r\n2018-07-30 11:42:05.949163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:0 with 14867 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0)\r\n2018-07-30 11:42:05.949304: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:1 with 14867 MB memory) -> physical GPU (device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0)\r\n2018-07-30 11:42:05.949936: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:2 with 14867 MB memory) -> physical GPU (device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0)\r\n2018-07-30 11:42:05.950061: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1053] Created TensorFlow device (/device:GPU:3 with 14867 MB memory) -> physical GPU (device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\r\nTraceback (most recent call last):\r\n  File \"/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1567, in _create_c_op\r\n    c_op = c_api.TF_FinishOperation(op_desc)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Shape must be rank 1 but is rank 2 for 'cond/cond_1/concat' (op: 'ConcatV2') with input shapes: [0], [?,806], [].\r\n```\r\n\r\nIn this case, TF finds all four of my GPUs, but the shape of the data is off. \r\n\r\nI run this exact same code without `MirroredStrategy` everything works fine (but I do have to add an `Iterator` object back in).\r\n\r\nThoughts?\r\n\r\n"}