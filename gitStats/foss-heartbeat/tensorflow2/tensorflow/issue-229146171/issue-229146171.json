{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9944", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9944/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9944/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9944/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9944", "id": 229146171, "node_id": "MDU6SXNzdWUyMjkxNDYxNzE=", "number": 9944, "title": "Distributed tensorflow - possible memory leak on Linux", "user": {"login": "leelassen", "id": 5662195, "node_id": "MDQ6VXNlcjU2NjIxOTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/5662195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leelassen", "html_url": "https://github.com/leelassen", "followers_url": "https://api.github.com/users/leelassen/followers", "following_url": "https://api.github.com/users/leelassen/following{/other_user}", "gists_url": "https://api.github.com/users/leelassen/gists{/gist_id}", "starred_url": "https://api.github.com/users/leelassen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leelassen/subscriptions", "organizations_url": "https://api.github.com/users/leelassen/orgs", "repos_url": "https://api.github.com/users/leelassen/repos", "events_url": "https://api.github.com/users/leelassen/events{/privacy}", "received_events_url": "https://api.github.com/users/leelassen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-16T19:38:41Z", "updated_at": "2017-05-17T10:57:23Z", "closed_at": "2017-05-17T08:32:49Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1.rc2 (v1.1.0-rc2-675-gd0042ed)</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 8/5.0</li>\n<li><strong>GPU model and memory</strong>: Geforce670 2GB (GPU not used in example)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<p><code>import tensorflow as tf</code><br>\n<code>server = tf.train.Server.create_local_server()</code><br>\n<code>session = tf.Session(server.target)</code><br>\n<code>c = tf.constant(\"Hello, distributed TensorFlow!\")</code><br>\n<code>init = tf.global_variables_initializer()</code><br>\n<code>session.run(init)</code><br>\n<code>session.graph.finalize()</code><br>\n<code>with session as sess:</code><br>\n<code></code> <code>while True:</code><br>\n<code>  </code>  <code>  </code>  <code>out = sess.run(c)</code></p>\n<h3>Describe the problem</h3>\n<p>Running the above example, memory usage keeps slowly increasing. If the session is initialized as non-distributed with:</p>\n<p>session = tf.Session()</p>\n<p>The amount of memory used remains stable. This is tested on several machines both with binary and source code installations. Could someone else verify, if this is a problem with tensorflow.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.1.rc2 (v1.1.0-rc2-675-gd0042ed)\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: 8/5.0\nGPU model and memory: Geforce670 2GB (GPU not used in example)\nExact command to reproduce:\n\nimport tensorflow as tf\nserver = tf.train.Server.create_local_server()\nsession = tf.Session(server.target)\nc = tf.constant(\"Hello, distributed TensorFlow!\")\ninit = tf.global_variables_initializer()\nsession.run(init)\nsession.graph.finalize()\nwith session as sess:\n while True:\n        out = sess.run(c)\nDescribe the problem\nRunning the above example, memory usage keeps slowly increasing. If the session is initialized as non-distributed with:\nsession = tf.Session()\nThe amount of memory used remains stable. This is tested on several machines both with binary and source code installations. Could someone else verify, if this is a problem with tensorflow.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.1.rc2 (v1.1.0-rc2-675-gd0042ed)\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8/5.0\r\n- **GPU model and memory**: Geforce670 2GB (GPU not used in example)\r\n- **Exact command to reproduce**:\r\n\r\n`import tensorflow as tf`\r\n`server = tf.train.Server.create_local_server()`\r\n`session = tf.Session(server.target)`\r\n`c = tf.constant(\"Hello, distributed TensorFlow!\")`\r\n`init = tf.global_variables_initializer()`\r\n`session.run(init)`\r\n`session.graph.finalize()`\r\n`with session as sess:`\r\n`  ` `while True:`\r\n`    `  `    `  `out = sess.run(c)`\r\n\r\n### Describe the problem\r\nRunning the above example, memory usage keeps slowly increasing. If the session is initialized as non-distributed with:\r\n\r\nsession = tf.Session()\r\n\r\nThe amount of memory used remains stable. This is tested on several machines both with binary and source code installations. Could someone else verify, if this is a problem with tensorflow.\r\n"}