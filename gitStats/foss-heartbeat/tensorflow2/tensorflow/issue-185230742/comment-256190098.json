{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/256190098", "html_url": "https://github.com/tensorflow/tensorflow/issues/5205#issuecomment-256190098", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5205", "id": 256190098, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NjE5MDA5OA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-25T22:02:02Z", "updated_at": "2016-10-25T22:02:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>As far as TensorFlow programs go, this is a pessimal case. The underlying assumption in TensorFlow is (approximately) that you'll build a graph once and then call <code>sess.run()</code> on (various parts of) it multiple times. The tax of calling <code>TF_ExtendGraph</code> is paid once on the first use of a graph, and subsequent uses are much cheaper. If you must use a loop <em>within</em> your TensorFlow graph, there are constructs like <code>tf.while_loop()</code> (and, similarly, <code>tf.cond()</code> for if-statements) that you can use.</p>\n<p>You can make this program much faster by constructing the graph once and using (e.g.) a placeholder to feed in the value that changes in each iteration. For example, the following should do the trick:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">Trajectory_Fun</span>(<span class=\"pl-smi\">tspan</span>, <span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">b</span>, <span class=\"pl-smi\">session</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">server</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-k\">if</span> session<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span>:\n        <span class=\"pl-k\">if</span> server<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span>:\n            sess <span class=\"pl-k\">=</span> tf.Session()\n        <span class=\"pl-k\">else</span>:\n            sess <span class=\"pl-k\">=</span> tf.Session(server.target)       \n    <span class=\"pl-k\">else</span>:\n        sess <span class=\"pl-k\">=</span> session\n    B <span class=\"pl-k\">=</span> np.zeros(np.size(tspan), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float64)\n    B[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> b\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the TensorFlow graph once and reuse it in each iteration of the for loop.</span>\n    r_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[])\n    out_t <span class=\"pl-k\">=</span> tf.trace(tf.random_normal((<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">4</span>), r_placeholder, <span class=\"pl-c1\">1.0</span>))\n\n    <span class=\"pl-k\">for</span> i, t <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(tspan):\n        r <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-c1\">1</span>)\n        <span class=\"pl-k\">if</span> r<span class=\"pl-k\">&gt;</span>a:\n            c <span class=\"pl-k\">=</span> sess.run(out_t, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{r_placeholder: r})\n        <span class=\"pl-k\">else</span>:\n            c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.0</span>\n        B[i] <span class=\"pl-k\">=</span> c\n    <span class=\"pl-k\">if</span> session<span class=\"pl-k\">==</span><span class=\"pl-c1\">None</span>:\n        sess.close()\n    <span class=\"pl-k\">return</span> B</pre></div>\n<p>You could potentially make this even more efficient by using a TF loop and making fewer calls to <code>sess.run()</code>, but the general principle is the same: reuse the graph multiple times to get the benefit of TensorFlow.</p>", "body_text": "As far as TensorFlow programs go, this is a pessimal case. The underlying assumption in TensorFlow is (approximately) that you'll build a graph once and then call sess.run() on (various parts of) it multiple times. The tax of calling TF_ExtendGraph is paid once on the first use of a graph, and subsequent uses are much cheaper. If you must use a loop within your TensorFlow graph, there are constructs like tf.while_loop() (and, similarly, tf.cond() for if-statements) that you can use.\nYou can make this program much faster by constructing the graph once and using (e.g.) a placeholder to feed in the value that changes in each iteration. For example, the following should do the trick:\ndef Trajectory_Fun(tspan, a, b, session=None, server=None):\n    if session==None:\n        if server==None:\n            sess = tf.Session()\n        else:\n            sess = tf.Session(server.target)       \n    else:\n        sess = session\n    B = np.zeros(np.size(tspan), dtype=np.float64)\n    B[0] = b\n\n    # Define the TensorFlow graph once and reuse it in each iteration of the for loop.\n    r_placeholder = tf.placeholder(tf.float32, shape=[])\n    out_t = tf.trace(tf.random_normal((4, 4), r_placeholder, 1.0))\n\n    for i, t in enumerate(tspan):\n        r = np.random.rand(1)\n        if r>a:\n            c = sess.run(out_t, feed_dict={r_placeholder: r})\n        else:\n            c = 0.0\n        B[i] = c\n    if session==None:\n        sess.close()\n    return B\nYou could potentially make this even more efficient by using a TF loop and making fewer calls to sess.run(), but the general principle is the same: reuse the graph multiple times to get the benefit of TensorFlow.", "body": "As far as TensorFlow programs go, this is a pessimal case. The underlying assumption in TensorFlow is (approximately) that you'll build a graph once and then call `sess.run()` on (various parts of) it multiple times. The tax of calling `TF_ExtendGraph` is paid once on the first use of a graph, and subsequent uses are much cheaper. If you must use a loop _within_ your TensorFlow graph, there are constructs like `tf.while_loop()` (and, similarly, `tf.cond()` for if-statements) that you can use.\n\nYou can make this program much faster by constructing the graph once and using (e.g.) a placeholder to feed in the value that changes in each iteration. For example, the following should do the trick:\n\n``` python\ndef Trajectory_Fun(tspan, a, b, session=None, server=None):\n    if session==None:\n        if server==None:\n            sess = tf.Session()\n        else:\n            sess = tf.Session(server.target)       \n    else:\n        sess = session\n    B = np.zeros(np.size(tspan), dtype=np.float64)\n    B[0] = b\n\n    # Define the TensorFlow graph once and reuse it in each iteration of the for loop.\n    r_placeholder = tf.placeholder(tf.float32, shape=[])\n    out_t = tf.trace(tf.random_normal((4, 4), r_placeholder, 1.0))\n\n    for i, t in enumerate(tspan):\n        r = np.random.rand(1)\n        if r>a:\n            c = sess.run(out_t, feed_dict={r_placeholder: r})\n        else:\n            c = 0.0\n        B[i] = c\n    if session==None:\n        sess.close()\n    return B\n```\n\nYou could potentially make this even more efficient by using a TF loop and making fewer calls to `sess.run()`, but the general principle is the same: reuse the graph multiple times to get the benefit of TensorFlow.\n"}