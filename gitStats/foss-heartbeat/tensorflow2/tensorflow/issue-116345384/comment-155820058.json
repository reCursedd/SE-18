{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155820058", "html_url": "https://github.com/tensorflow/tensorflow/issues/139#issuecomment-155820058", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/139", "id": 155820058, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTgyMDA1OA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-11T15:38:34Z", "updated_at": "2015-11-11T15:38:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It looks like the problem arises because (i) <code>tf.nn.moments</code> relies on knowing the fully-defined shape for its argument, and (ii) the <code>x</code> argument that you passed to <code>tf.nn.moments</code> has one or more undefined dimensions.</p>\n<p>This can arise when shape inference doesn't have enough information to infer all of the dimensions in your tensor. In this case, the best thing to do is the following:</p>\n<div class=\"highlight highlight-source-python\"><pre>x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">...</span>\naxes <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">...</span>]\nx.set_shape([size_in_dim_0, <span class=\"pl-c1\">...</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Each `size_in_dim_i` must be an integer.</span>\nx_moments <span class=\"pl-k\">=</span> tf.nn.moments(x, axes)</pre></div>\n<p>If the size in any of those dimensions is dynamic, the current implementation of <code>tf.nn.moments()</code> will not work, but it is possible to add a custom version that does the trick:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">moments</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">axes</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Version of tf.nn.moments that supports variable-sized tensors.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  N.B. The rank must be known statically for this version to work.</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">with</span> tf.op_scope([x], name, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>moments<span class=\"pl-pds\">\"</span></span>):\n    x <span class=\"pl-k\">=</span> tf.convert_to_tensor(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>)\n    divisor <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">1.0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>x.dtype)\n    x_dynamic_shape <span class=\"pl-k\">=</span> tf.shape(x)\n    <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(x.get_shape().ndims):\n      <span class=\"pl-k\">if</span> d <span class=\"pl-k\">in</span> axes:\n        divisor <span class=\"pl-k\">=</span> divisor <span class=\"pl-k\">*</span> x_dynamic_shape[d]\n    divisor <span class=\"pl-k\">=</span> tf.inv(divisor)\n    mean <span class=\"pl-k\">=</span> math_ops.mul(math_ops.reduce_sum(x, axes), divisor, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mean<span class=\"pl-pds\">\"</span></span>)\n    var <span class=\"pl-k\">=</span> math_ops.mul(math_ops.reduce_sum(math_ops.square(x <span class=\"pl-k\">-</span> mean), axes),\n                       divisor, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>variance<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">return</span> mean, var</pre></div>\n<p>A version that works with dynamic rank tensors would also be possible, but it would be quite a bit more complicated.</p>\n<p>It would also be possible to replace the <code>divisor</code> computation with a call to <code>tf.mean()</code>, but as the comment in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L539\">the implementation</a> mentions, there are sum performance issues when using <code>tf.mean()</code> on GPU at present.</p>", "body_text": "It looks like the problem arises because (i) tf.nn.moments relies on knowing the fully-defined shape for its argument, and (ii) the x argument that you passed to tf.nn.moments has one or more undefined dimensions.\nThis can arise when shape inference doesn't have enough information to infer all of the dimensions in your tensor. In this case, the best thing to do is the following:\nx = ...\naxes = [...]\nx.set_shape([size_in_dim_0, ...])  # Each `size_in_dim_i` must be an integer.\nx_moments = tf.nn.moments(x, axes)\nIf the size in any of those dimensions is dynamic, the current implementation of tf.nn.moments() will not work, but it is possible to add a custom version that does the trick:\ndef moments(x, axes, name=None):\n  \"\"\"Version of tf.nn.moments that supports variable-sized tensors.\n\n  N.B. The rank must be known statically for this version to work.\n  \"\"\"\n  with tf.op_scope([x], name, \"moments\"):\n    x = tf.convert_to_tensor(x, name=\"x\")\n    divisor = tf.constant(1.0, dtype=x.dtype)\n    x_dynamic_shape = tf.shape(x)\n    for d in xrange(x.get_shape().ndims):\n      if d in axes:\n        divisor = divisor * x_dynamic_shape[d]\n    divisor = tf.inv(divisor)\n    mean = math_ops.mul(math_ops.reduce_sum(x, axes), divisor, name=\"mean\")\n    var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x - mean), axes),\n                       divisor, name=\"variance\")\n    return mean, var\nA version that works with dynamic rank tensors would also be possible, but it would be quite a bit more complicated.\nIt would also be possible to replace the divisor computation with a call to tf.mean(), but as the comment in the implementation mentions, there are sum performance issues when using tf.mean() on GPU at present.", "body": "It looks like the problem arises because (i) `tf.nn.moments` relies on knowing the fully-defined shape for its argument, and (ii) the `x` argument that you passed to `tf.nn.moments` has one or more undefined dimensions.\n\nThis can arise when shape inference doesn't have enough information to infer all of the dimensions in your tensor. In this case, the best thing to do is the following:\n\n``` python\nx = ...\naxes = [...]\nx.set_shape([size_in_dim_0, ...])  # Each `size_in_dim_i` must be an integer.\nx_moments = tf.nn.moments(x, axes)\n```\n\nIf the size in any of those dimensions is dynamic, the current implementation of `tf.nn.moments()` will not work, but it is possible to add a custom version that does the trick:\n\n``` python\ndef moments(x, axes, name=None):\n  \"\"\"Version of tf.nn.moments that supports variable-sized tensors.\n\n  N.B. The rank must be known statically for this version to work.\n  \"\"\"\n  with tf.op_scope([x], name, \"moments\"):\n    x = tf.convert_to_tensor(x, name=\"x\")\n    divisor = tf.constant(1.0, dtype=x.dtype)\n    x_dynamic_shape = tf.shape(x)\n    for d in xrange(x.get_shape().ndims):\n      if d in axes:\n        divisor = divisor * x_dynamic_shape[d]\n    divisor = tf.inv(divisor)\n    mean = math_ops.mul(math_ops.reduce_sum(x, axes), divisor, name=\"mean\")\n    var = math_ops.mul(math_ops.reduce_sum(math_ops.square(x - mean), axes),\n                       divisor, name=\"variance\")\n    return mean, var\n```\n\nA version that works with dynamic rank tensors would also be possible, but it would be quite a bit more complicated.\n\nIt would also be possible to replace the `divisor` computation with a call to `tf.mean()`, but as the comment in [the implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn.py#L539) mentions, there are sum performance issues when using `tf.mean()` on GPU at present.\n"}