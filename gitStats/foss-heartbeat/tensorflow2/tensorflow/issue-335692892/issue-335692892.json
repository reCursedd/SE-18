{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20299", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20299/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20299/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20299/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20299", "id": 335692892, "node_id": "MDU6SXNzdWUzMzU2OTI4OTI=", "number": 20299, "title": "Layer norm for cudnnlstm?", "user": {"login": "gaussleescorpio", "id": 17249981, "node_id": "MDQ6VXNlcjE3MjQ5OTgx", "avatar_url": "https://avatars0.githubusercontent.com/u/17249981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaussleescorpio", "html_url": "https://github.com/gaussleescorpio", "followers_url": "https://api.github.com/users/gaussleescorpio/followers", "following_url": "https://api.github.com/users/gaussleescorpio/following{/other_user}", "gists_url": "https://api.github.com/users/gaussleescorpio/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaussleescorpio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaussleescorpio/subscriptions", "organizations_url": "https://api.github.com/users/gaussleescorpio/orgs", "repos_url": "https://api.github.com/users/gaussleescorpio/repos", "events_url": "https://api.github.com/users/gaussleescorpio/events{/privacy}", "received_events_url": "https://api.github.com/users/gaussleescorpio/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "protoget", "id": 5117188, "node_id": "MDQ6VXNlcjUxMTcxODg=", "avatar_url": "https://avatars1.githubusercontent.com/u/5117188?v=4", "gravatar_id": "", "url": "https://api.github.com/users/protoget", "html_url": "https://github.com/protoget", "followers_url": "https://api.github.com/users/protoget/followers", "following_url": "https://api.github.com/users/protoget/following{/other_user}", "gists_url": "https://api.github.com/users/protoget/gists{/gist_id}", "starred_url": "https://api.github.com/users/protoget/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/protoget/subscriptions", "organizations_url": "https://api.github.com/users/protoget/orgs", "repos_url": "https://api.github.com/users/protoget/repos", "events_url": "https://api.github.com/users/protoget/events{/privacy}", "received_events_url": "https://api.github.com/users/protoget/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-06-26T07:31:29Z", "updated_at": "2018-11-20T13:28:37Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I am training a very large and deep lstm-RNN. Using cudnnlstm can save memories and speed up the training process. However, I need some extra functions such as layer norm, adding attention wrapper, etc.<br>\nIs there a way to achieve this? Should I rewrite the c++ code and recompile? If so, which file of the source code contains the cudnnlstm implementation?</p>\n<p>PS: In the documentation, there is only one bit mentioning topics related to the layer norm of cudnn. It is in the performance guides. It only tells developers there is no layer norm for cudnnlstm. However, layer norm is very important for a decent lstm network. I would like to know the method to adding the layer norm algorithm similar to the LayerNormBasicLstm class. I explored a bit via trying to scoop out the params (weights and biases) of cudnn. It seems the shape of those two are not specific in the process of building the graph.</p>\n<p><strong>Have I written custom code</strong><br>\nNot yet<br>\n<strong>OS Platform and Distribution</strong><br>\nUbuntu 16.04<br>\n<strong>TensorFlow installed from</strong><br>\npip<br>\n<strong>TensorFlow version</strong><br>\n1.8.0<br>\n<strong>Bazel version</strong><br>\nN/A<br>\n<strong>CUDA/cuDNN version</strong><br>\nCuda 8.0<br>\n<strong>GPU model and memory</strong><br>\nNvidia GTX 1080<br>\n<strong>Exact command to reproduce</strong><br>\nN/A</p>", "body_text": "I am training a very large and deep lstm-RNN. Using cudnnlstm can save memories and speed up the training process. However, I need some extra functions such as layer norm, adding attention wrapper, etc.\nIs there a way to achieve this? Should I rewrite the c++ code and recompile? If so, which file of the source code contains the cudnnlstm implementation?\nPS: In the documentation, there is only one bit mentioning topics related to the layer norm of cudnn. It is in the performance guides. It only tells developers there is no layer norm for cudnnlstm. However, layer norm is very important for a decent lstm network. I would like to know the method to adding the layer norm algorithm similar to the LayerNormBasicLstm class. I explored a bit via trying to scoop out the params (weights and biases) of cudnn. It seems the shape of those two are not specific in the process of building the graph.\nHave I written custom code\nNot yet\nOS Platform and Distribution\nUbuntu 16.04\nTensorFlow installed from\npip\nTensorFlow version\n1.8.0\nBazel version\nN/A\nCUDA/cuDNN version\nCuda 8.0\nGPU model and memory\nNvidia GTX 1080\nExact command to reproduce\nN/A", "body": "I am training a very large and deep lstm-RNN. Using cudnnlstm can save memories and speed up the training process. However, I need some extra functions such as layer norm, adding attention wrapper, etc.\r\nIs there a way to achieve this? Should I rewrite the c++ code and recompile? If so, which file of the source code contains the cudnnlstm implementation?\r\n\r\nPS: In the documentation, there is only one bit mentioning topics related to the layer norm of cudnn. It is in the performance guides. It only tells developers there is no layer norm for cudnnlstm. However, layer norm is very important for a decent lstm network. I would like to know the method to adding the layer norm algorithm similar to the LayerNormBasicLstm class. I explored a bit via trying to scoop out the params (weights and biases) of cudnn. It seems the shape of those two are not specific in the process of building the graph.\r\n\r\n**Have I written custom code**\r\nNot yet\r\n**OS Platform and Distribution**\r\nUbuntu 16.04\r\n**TensorFlow installed from**\r\npip \r\n**TensorFlow version**\r\n1.8.0\r\n**Bazel version**\r\nN/A\r\n**CUDA/cuDNN version**\r\nCuda 8.0  \r\n**GPU model and memory**\r\nNvidia GTX 1080\r\n**Exact command to reproduce**\r\nN/A\r\n\r\n"}