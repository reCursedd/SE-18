{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22161", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22161/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22161/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22161/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22161", "id": 358273520, "node_id": "MDU6SXNzdWUzNTgyNzM1MjA=", "number": 22161, "title": "ValueError on using tf.contrib.avro with tf.keras", "user": {"login": "Nithanaroy", "id": 670556, "node_id": "MDQ6VXNlcjY3MDU1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/670556?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nithanaroy", "html_url": "https://github.com/Nithanaroy", "followers_url": "https://api.github.com/users/Nithanaroy/followers", "following_url": "https://api.github.com/users/Nithanaroy/following{/other_user}", "gists_url": "https://api.github.com/users/Nithanaroy/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nithanaroy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nithanaroy/subscriptions", "organizations_url": "https://api.github.com/users/Nithanaroy/orgs", "repos_url": "https://api.github.com/users/Nithanaroy/repos", "events_url": "https://api.github.com/users/Nithanaroy/events{/privacy}", "received_events_url": "https://api.github.com/users/Nithanaroy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-09-08T07:38:05Z", "updated_at": "2018-10-29T21:31:51Z", "closed_at": "2018-09-08T20:19:53Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: RHEL 7</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: NA</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Yes</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: Please see the source below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>On using <a href=\"https://github.com/tensorflow/tensorflow/pull/18224\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/18224/hovercard\">tf.contrib.avro</a> with tf.keras, it throws a ValueError. The dataset created using tf.contrib.avro produces the results correctly but when used as training data for tf.keras.model during fit, it fails saying - <code>ValueError: Input 0 of layer dense0 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]</code></p>\n<h3>Source code / logs</h3>\n<p>Here is the test code to reproduce the error:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.keras.layers <span class=\"pl-k\">import</span> Conv1D, MaxPool1D, Flatten, Dense, Dropout, Input, BatchNormalization, concatenate\n<span class=\"pl-k\">from</span> tensorflow.keras.optimizers <span class=\"pl-k\">import</span> Adam\n\n<span class=\"pl-k\">from</span> tensorflow.contrib.avro.python.parse_avro_record <span class=\"pl-k\">import</span> parse_avro_record\n<span class=\"pl-k\">from</span> tensorflow.contrib.avro.python.avro_record_dataset <span class=\"pl-k\">import</span> <span class=\"pl-k\">*</span>\n\n\n<span class=\"pl-k\">from</span> avro.io <span class=\"pl-k\">import</span> DatumReader, DatumWriter, BinaryDecoder, BinaryEncoder\n<span class=\"pl-k\">from</span> avro.schema <span class=\"pl-k\">import</span> parse\n<span class=\"pl-k\">from</span> StringIO <span class=\"pl-k\">import</span> StringIO\n<span class=\"pl-k\">from</span> fastavro <span class=\"pl-k\">import</span> writer, parse_schema\n<span class=\"pl-k\">import</span> random, json, glob, os\n\nschema <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>{\"doc\": \"Simple example.\",</span>\n<span class=\"pl-s\">             \"namespace\": \"com.linkedin.demo.person\",</span>\n<span class=\"pl-s\">             \"type\": \"record\",</span>\n<span class=\"pl-s\">             \"name\": \"data_row\",</span>\n<span class=\"pl-s\">             \"fields\": [</span>\n<span class=\"pl-s\">               {\"name\": \"index\", \"type\": \"long\"},</span>\n<span class=\"pl-s\">               {\"name\": \"first_name\", \"type\": \"string\"},</span>\n<span class=\"pl-s\">               {\"name\": \"age\", \"type\": \"int\"},</span>\n<span class=\"pl-s\">               {\"name\": \"gender\", \"type\": \"int\"}</span>\n<span class=\"pl-s\">            ]}<span class=\"pl-pds\">'''</span></span>\n\ndata <span class=\"pl-k\">=</span> [{<span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">0</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>first_name<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Karl<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>age<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">22</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gender<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">0</span>},\n        {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">1</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>first_name<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Julia<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>age<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">42</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gender<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1</span>},\n        {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">2</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>first_name<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Liberty<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>age<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">90</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gender<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">1</span>}]\n\nfeatures <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>index<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], tf.int64),\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>first_name<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], tf.string),\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>age<span class=\"pl-pds\">'</span></span>: tf.FixedLenFeature([], tf.int32)}\n\n<span class=\"pl-c1\">RECORDS_PER_FILE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000</span>\n<span class=\"pl-c1\">FEATURE_LIST</span> <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>index<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>age<span class=\"pl-pds\">\"</span></span>]\nexclude_features <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>first_name<span class=\"pl-pds\">\"</span></span>]\n<span class=\"pl-c1\">RESPONSE_FIELD</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gender<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-c1\">DATA_SCHEMA</span> <span class=\"pl-k\">=</span> schema\n<span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">data_generator</span>(<span class=\"pl-smi\">output_directory</span>, <span class=\"pl-smi\">num_files</span>, <span class=\"pl-smi\">records_per_file</span>):\n    parsed_schema <span class=\"pl-k\">=</span> parse_schema(json.loads(<span class=\"pl-c1\">DATA_SCHEMA</span>))\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_files):\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Generating file <span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i))\n        records <span class=\"pl-k\">=</span> [{}] <span class=\"pl-k\">*</span> records_per_file\n        <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(records_per_file):\n            records[j] <span class=\"pl-k\">=</span> {\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>index<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">int</span>( random.random() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">100</span> ),\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>first_name<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>,\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>age<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">int</span>( random.random() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">95</span> ),\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gender<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-c1\">int</span>( random.random() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">10</span> ) <span class=\"pl-k\">%</span> <span class=\"pl-c1\">2</span>\n            }\n        \n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>( output_directory <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.avro<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> out:\n            writer(out, parsed_schema, records)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_input_fn</span>(<span class=\"pl-smi\">data_files</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">num_parallel</span>, <span class=\"pl-smi\">shuffle_size</span>, <span class=\"pl-smi\">repeat_times</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Create the input function for training and evaluation.<span class=\"pl-pds\">\"\"\"</span></span>\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the function for parsing avro records into TensorFlow tensors (features, label).</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_parse_function</span>(<span class=\"pl-smi\">serialized</span>):\n        parser_spec <span class=\"pl-k\">=</span> {f: tf.FixedLenFeature([], tf.float32, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>) <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> <span class=\"pl-c1\">FEATURE_LIST</span>}\n        parser_spec[<span class=\"pl-c1\">RESPONSE_FIELD</span>] <span class=\"pl-k\">=</span> tf.FixedLenFeature([], tf.int32, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        parsed <span class=\"pl-k\">=</span> parse_avro_record(<span class=\"pl-v\">serialized</span><span class=\"pl-k\">=</span>serialized, <span class=\"pl-v\">schema</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DATA_SCHEMA</span>, <span class=\"pl-v\">features</span><span class=\"pl-k\">=</span>parser_spec)\n\n        features <span class=\"pl-k\">=</span> {f: parsed[f] <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> <span class=\"pl-c1\">FEATURE_LIST</span> <span class=\"pl-k\">if</span> f <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> exclude_features}\n        label <span class=\"pl-k\">=</span> parsed.pop(<span class=\"pl-c1\">RESPONSE_FIELD</span>)\n        <span class=\"pl-k\">return</span> features, label\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the dataset and apply the parse function (must apply batch before parsing).</span>\n    dataset <span class=\"pl-k\">=</span> AvroRecordDataset(<span class=\"pl-v\">filenames</span><span class=\"pl-k\">=</span>data_files)\n    dataset <span class=\"pl-k\">=</span> dataset.batch(batch_size).map(_parse_function, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>num_parallel)\n\n    <span class=\"pl-k\">if</span> repeat_times <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        dataset <span class=\"pl-k\">=</span> dataset.cache().repeat(repeat_times)\n    <span class=\"pl-k\">if</span> shuffle_size <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        dataset <span class=\"pl-k\">=</span> dataset.shuffle(shuffle_size)\n        \n    dataset <span class=\"pl-k\">=</span> dataset.prefetch(<span class=\"pl-c1\">2</span>)\n\n    <span class=\"pl-k\">return</span> dataset\n    \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">deep_model</span>(<span class=\"pl-smi\">layer_names</span>):\n    inputs <span class=\"pl-k\">=</span> [Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>layer_name) <span class=\"pl-k\">for</span> layer_name <span class=\"pl-k\">in</span> layer_names]\n    x <span class=\"pl-k\">=</span> concatenate(inputs, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input_layer<span class=\"pl-pds\">\"</span></span>)\n    x <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dense0<span class=\"pl-pds\">\"</span></span>)(x)\n    outputs <span class=\"pl-k\">=</span> Dense(<span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>softmax<span class=\"pl-pds\">'</span></span>)(x)\n    \n    <span class=\"pl-k\">return</span> tf.keras.Model(inputs, outputs)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_data_generator</span>(<span class=\"pl-smi\">output_directory</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Running data generator<span class=\"pl-pds\">\"</span></span>)\n    data_generator(output_directory, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">RECORDS_PER_FILE</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train_using_fit</span>(<span class=\"pl-smi\">data_dir</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Running the training loop using model.fit<span class=\"pl-pds\">\"</span></span>)\n    files <span class=\"pl-k\">=</span> tf.gfile.Glob(os.path.join(data_dir, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.avro<span class=\"pl-pds\">\"</span></span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> can be generated using data_generator() above</span>\n    \n    model <span class=\"pl-k\">=</span> deep_model([f <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> <span class=\"pl-c1\">FEATURE_LIST</span> <span class=\"pl-k\">if</span> f <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> exclude_features])\n    optimizer <span class=\"pl-k\">=</span> Adam(<span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>, <span class=\"pl-v\">clipvalue</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>categorical_crossentropy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])\n    model.summary()\n    \n    training_dataset <span class=\"pl-k\">=</span> create_input_fn(<span class=\"pl-v\">data_files</span><span class=\"pl-k\">=</span>files, \n                                       <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-v\">num_parallel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">shuffle_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n    model.fit(\n        training_dataset.make_one_shot_iterator(),\n        <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">RECORDS_PER_FILE</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">len</span>(files)) <span class=\"pl-k\">//</span> <span class=\"pl-c1\">BATCH_SIZE</span>,\n        <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train_using_estimator</span>(<span class=\"pl-smi\">data_dir</span>):\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Running the training loop using keras estimators<span class=\"pl-pds\">\"</span></span>)\n    files <span class=\"pl-k\">=</span> tf.gfile.Glob(os.path.join(data_dir, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>*.avro<span class=\"pl-pds\">\"</span></span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> can be generated using data_generator() above</span>\n    \n    model <span class=\"pl-k\">=</span> deep_model([f <span class=\"pl-k\">for</span> f <span class=\"pl-k\">in</span> <span class=\"pl-c1\">FEATURE_LIST</span> <span class=\"pl-k\">if</span> f <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> exclude_features])\n    optimizer <span class=\"pl-k\">=</span> Adam(<span class=\"pl-v\">lr</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>, <span class=\"pl-v\">clipvalue</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>)\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>categorical_crossentropy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>optimizer, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])\n    model.summary()\n\n    training_dataset <span class=\"pl-k\">=</span> create_input_fn(<span class=\"pl-v\">data_files</span><span class=\"pl-k\">=</span>files, \n                                       <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">BATCH_SIZE</span>, <span class=\"pl-v\">num_parallel</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">shuffle_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n\n    estimator <span class=\"pl-k\">=</span> tf.keras.estimator.model_to_estimator(model) <span class=\"pl-c\"><span class=\"pl-c\">#</span> can provide distributed training strategy</span>\n    estimator.train(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: training_dataset.make_one_shot_iterator().get_next(), <span class=\"pl-v\">max_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">20</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    data_dir <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./data/mock_data/<span class=\"pl-pds\">\"</span></span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> make sure this folder exists</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> test_data_generator(data_dir) # need to run this only once to generate dummy data</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> train_using_fit(data_dir) # approach 1 for training</span>\n    train_using_estimator(data_dir) <span class=\"pl-c\"><span class=\"pl-c\">#</span> approach 2 for training</span>\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): RHEL 7\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: NA\nTensorFlow installed from (source or binary): Yes\nTensorFlow version (use command below): 1.9.0\nPython version: 2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: Please see the source below\n\nDescribe the problem\nOn using tf.contrib.avro with tf.keras, it throws a ValueError. The dataset created using tf.contrib.avro produces the results correctly but when used as training data for tf.keras.model during fit, it fails saying - ValueError: Input 0 of layer dense0 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nSource code / logs\nHere is the test code to reproduce the error:\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Dropout, Input, BatchNormalization, concatenate\nfrom tensorflow.keras.optimizers import Adam\n\nfrom tensorflow.contrib.avro.python.parse_avro_record import parse_avro_record\nfrom tensorflow.contrib.avro.python.avro_record_dataset import *\n\n\nfrom avro.io import DatumReader, DatumWriter, BinaryDecoder, BinaryEncoder\nfrom avro.schema import parse\nfrom StringIO import StringIO\nfrom fastavro import writer, parse_schema\nimport random, json, glob, os\n\nschema = '''{\"doc\": \"Simple example.\",\n             \"namespace\": \"com.linkedin.demo.person\",\n             \"type\": \"record\",\n             \"name\": \"data_row\",\n             \"fields\": [\n               {\"name\": \"index\", \"type\": \"long\"},\n               {\"name\": \"first_name\", \"type\": \"string\"},\n               {\"name\": \"age\", \"type\": \"int\"},\n               {\"name\": \"gender\", \"type\": \"int\"}\n            ]}'''\n\ndata = [{'index': 0, 'first_name': \"Karl\", 'age': 22, \"gender\": 0},\n        {'index': 1, 'first_name': \"Julia\", 'age': 42, \"gender\": 1},\n        {'index': 2, 'first_name': \"Liberty\", 'age': 90, \"gender\": 1}]\n\nfeatures = {'index': tf.FixedLenFeature([], tf.int64),\n            'first_name': tf.FixedLenFeature([], tf.string),\n            'age': tf.FixedLenFeature([], tf.int32)}\n\nRECORDS_PER_FILE = 1000\nFEATURE_LIST = [\"index\", \"age\"]\nexclude_features = [\"first_name\"]\nRESPONSE_FIELD = \"gender\"\nDATA_SCHEMA = schema\nBATCH_SIZE = 10\n\ndef data_generator(output_directory, num_files, records_per_file):\n    parsed_schema = parse_schema(json.loads(DATA_SCHEMA))\n\n    for i in range(num_files):\n        print(\"Generating file \" + str(i))\n        records = [{}] * records_per_file\n        for j in range(records_per_file):\n            records[j] = {\n                \"index\": int( random.random() * 100 ),\n                \"first_name\": \"\",\n                \"age\": int( random.random() * 95 ),\n                \"gender\": int( random.random() * 10 ) % 2\n            }\n        \n        with open( output_directory + str(i) + '.avro', 'wb') as out:\n            writer(out, parsed_schema, records)\n\ndef create_input_fn(data_files, batch_size, num_parallel, shuffle_size, repeat_times=-1):\n    \"\"\"Create the input function for training and evaluation.\"\"\"\n    \n    # Define the function for parsing avro records into TensorFlow tensors (features, label).\n    def _parse_function(serialized):\n        parser_spec = {f: tf.FixedLenFeature([], tf.float32, default_value=0) for f in FEATURE_LIST}\n        parser_spec[RESPONSE_FIELD] = tf.FixedLenFeature([], tf.int32, default_value=0)\n        parsed = parse_avro_record(serialized=serialized, schema=DATA_SCHEMA, features=parser_spec)\n\n        features = {f: parsed[f] for f in FEATURE_LIST if f not in exclude_features}\n        label = parsed.pop(RESPONSE_FIELD)\n        return features, label\n\n    # Create the dataset and apply the parse function (must apply batch before parsing).\n    dataset = AvroRecordDataset(filenames=data_files)\n    dataset = dataset.batch(batch_size).map(_parse_function, num_parallel_calls=num_parallel)\n\n    if repeat_times is not None:\n        dataset = dataset.cache().repeat(repeat_times)\n    if shuffle_size is not None:\n        dataset = dataset.shuffle(shuffle_size)\n        \n    dataset = dataset.prefetch(2)\n\n    return dataset\n    \ndef deep_model(layer_names):\n    inputs = [Input(shape=(1,), name=layer_name) for layer_name in layer_names]\n    x = concatenate(inputs, name=\"input_layer\")\n    x = Dense(1, activation='relu', name=\"dense0\")(x)\n    outputs = Dense(2, activation='softmax')(x)\n    \n    return tf.keras.Model(inputs, outputs)\n\ndef test_data_generator(output_directory):\n    print(\"Running data generator\")\n    data_generator(output_directory, 10, RECORDS_PER_FILE)\n\ndef train_using_fit(data_dir):\n    print(\"Running the training loop using model.fit\")\n    files = tf.gfile.Glob(os.path.join(data_dir, \"*.avro\")) # can be generated using data_generator() above\n    \n    model = deep_model([f for f in FEATURE_LIST if f not in exclude_features])\n    optimizer = Adam(lr=0.001, clipvalue=0.5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.summary()\n    \n    training_dataset = create_input_fn(data_files=files, \n                                       batch_size=BATCH_SIZE, num_parallel=5, shuffle_size=None)\n    model.fit(\n        training_dataset.make_one_shot_iterator(),\n        steps_per_epoch=(RECORDS_PER_FILE * len(files)) // BATCH_SIZE,\n        epochs=2)\n\ndef train_using_estimator(data_dir):\n    print(\"Running the training loop using keras estimators\")\n    files = tf.gfile.Glob(os.path.join(data_dir, \"*.avro\")) # can be generated using data_generator() above\n    \n    model = deep_model([f for f in FEATURE_LIST if f not in exclude_features])\n    optimizer = Adam(lr=0.001, clipvalue=0.5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n    model.summary()\n\n    training_dataset = create_input_fn(data_files=files, \n                                       batch_size=BATCH_SIZE, num_parallel=5, shuffle_size=None)\n\n    estimator = tf.keras.estimator.model_to_estimator(model) # can provide distributed training strategy\n    estimator.train(input_fn=lambda: training_dataset.make_one_shot_iterator().get_next(), max_steps=20)\n\n\ndef main():\n    data_dir = \"./data/mock_data/\" # make sure this folder exists\n    # test_data_generator(data_dir) # need to run this only once to generate dummy data\n    # train_using_fit(data_dir) # approach 1 for training\n    train_using_estimator(data_dir) # approach 2 for training\n\nif __name__ == '__main__':\n    main()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: RHEL 7\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: NA\r\n- **TensorFlow installed from (source or binary)**: Yes\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: \r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: Please see the source below\r\n\r\n### Describe the problem\r\nOn using [tf.contrib.avro](https://github.com/tensorflow/tensorflow/pull/18224) with tf.keras, it throws a ValueError. The dataset created using tf.contrib.avro produces the results correctly but when used as training data for tf.keras.model during fit, it fails saying - `ValueError: Input 0 of layer dense0 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]`\r\n\r\n### Source code / logs\r\nHere is the test code to reproduce the error:\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.keras.layers import Conv1D, MaxPool1D, Flatten, Dense, Dropout, Input, BatchNormalization, concatenate\r\nfrom tensorflow.keras.optimizers import Adam\r\n\r\nfrom tensorflow.contrib.avro.python.parse_avro_record import parse_avro_record\r\nfrom tensorflow.contrib.avro.python.avro_record_dataset import *\r\n\r\n\r\nfrom avro.io import DatumReader, DatumWriter, BinaryDecoder, BinaryEncoder\r\nfrom avro.schema import parse\r\nfrom StringIO import StringIO\r\nfrom fastavro import writer, parse_schema\r\nimport random, json, glob, os\r\n\r\nschema = '''{\"doc\": \"Simple example.\",\r\n             \"namespace\": \"com.linkedin.demo.person\",\r\n             \"type\": \"record\",\r\n             \"name\": \"data_row\",\r\n             \"fields\": [\r\n               {\"name\": \"index\", \"type\": \"long\"},\r\n               {\"name\": \"first_name\", \"type\": \"string\"},\r\n               {\"name\": \"age\", \"type\": \"int\"},\r\n               {\"name\": \"gender\", \"type\": \"int\"}\r\n            ]}'''\r\n\r\ndata = [{'index': 0, 'first_name': \"Karl\", 'age': 22, \"gender\": 0},\r\n        {'index': 1, 'first_name': \"Julia\", 'age': 42, \"gender\": 1},\r\n        {'index': 2, 'first_name': \"Liberty\", 'age': 90, \"gender\": 1}]\r\n\r\nfeatures = {'index': tf.FixedLenFeature([], tf.int64),\r\n            'first_name': tf.FixedLenFeature([], tf.string),\r\n            'age': tf.FixedLenFeature([], tf.int32)}\r\n\r\nRECORDS_PER_FILE = 1000\r\nFEATURE_LIST = [\"index\", \"age\"]\r\nexclude_features = [\"first_name\"]\r\nRESPONSE_FIELD = \"gender\"\r\nDATA_SCHEMA = schema\r\nBATCH_SIZE = 10\r\n\r\ndef data_generator(output_directory, num_files, records_per_file):\r\n    parsed_schema = parse_schema(json.loads(DATA_SCHEMA))\r\n\r\n    for i in range(num_files):\r\n        print(\"Generating file \" + str(i))\r\n        records = [{}] * records_per_file\r\n        for j in range(records_per_file):\r\n            records[j] = {\r\n                \"index\": int( random.random() * 100 ),\r\n                \"first_name\": \"\",\r\n                \"age\": int( random.random() * 95 ),\r\n                \"gender\": int( random.random() * 10 ) % 2\r\n            }\r\n        \r\n        with open( output_directory + str(i) + '.avro', 'wb') as out:\r\n            writer(out, parsed_schema, records)\r\n\r\ndef create_input_fn(data_files, batch_size, num_parallel, shuffle_size, repeat_times=-1):\r\n    \"\"\"Create the input function for training and evaluation.\"\"\"\r\n    \r\n    # Define the function for parsing avro records into TensorFlow tensors (features, label).\r\n    def _parse_function(serialized):\r\n        parser_spec = {f: tf.FixedLenFeature([], tf.float32, default_value=0) for f in FEATURE_LIST}\r\n        parser_spec[RESPONSE_FIELD] = tf.FixedLenFeature([], tf.int32, default_value=0)\r\n        parsed = parse_avro_record(serialized=serialized, schema=DATA_SCHEMA, features=parser_spec)\r\n\r\n        features = {f: parsed[f] for f in FEATURE_LIST if f not in exclude_features}\r\n        label = parsed.pop(RESPONSE_FIELD)\r\n        return features, label\r\n\r\n    # Create the dataset and apply the parse function (must apply batch before parsing).\r\n    dataset = AvroRecordDataset(filenames=data_files)\r\n    dataset = dataset.batch(batch_size).map(_parse_function, num_parallel_calls=num_parallel)\r\n\r\n    if repeat_times is not None:\r\n        dataset = dataset.cache().repeat(repeat_times)\r\n    if shuffle_size is not None:\r\n        dataset = dataset.shuffle(shuffle_size)\r\n        \r\n    dataset = dataset.prefetch(2)\r\n\r\n    return dataset\r\n    \r\ndef deep_model(layer_names):\r\n    inputs = [Input(shape=(1,), name=layer_name) for layer_name in layer_names]\r\n    x = concatenate(inputs, name=\"input_layer\")\r\n    x = Dense(1, activation='relu', name=\"dense0\")(x)\r\n    outputs = Dense(2, activation='softmax')(x)\r\n    \r\n    return tf.keras.Model(inputs, outputs)\r\n\r\ndef test_data_generator(output_directory):\r\n    print(\"Running data generator\")\r\n    data_generator(output_directory, 10, RECORDS_PER_FILE)\r\n\r\ndef train_using_fit(data_dir):\r\n    print(\"Running the training loop using model.fit\")\r\n    files = tf.gfile.Glob(os.path.join(data_dir, \"*.avro\")) # can be generated using data_generator() above\r\n    \r\n    model = deep_model([f for f in FEATURE_LIST if f not in exclude_features])\r\n    optimizer = Adam(lr=0.001, clipvalue=0.5)\r\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n    model.summary()\r\n    \r\n    training_dataset = create_input_fn(data_files=files, \r\n                                       batch_size=BATCH_SIZE, num_parallel=5, shuffle_size=None)\r\n    model.fit(\r\n        training_dataset.make_one_shot_iterator(),\r\n        steps_per_epoch=(RECORDS_PER_FILE * len(files)) // BATCH_SIZE,\r\n        epochs=2)\r\n\r\ndef train_using_estimator(data_dir):\r\n    print(\"Running the training loop using keras estimators\")\r\n    files = tf.gfile.Glob(os.path.join(data_dir, \"*.avro\")) # can be generated using data_generator() above\r\n    \r\n    model = deep_model([f for f in FEATURE_LIST if f not in exclude_features])\r\n    optimizer = Adam(lr=0.001, clipvalue=0.5)\r\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\r\n    model.summary()\r\n\r\n    training_dataset = create_input_fn(data_files=files, \r\n                                       batch_size=BATCH_SIZE, num_parallel=5, shuffle_size=None)\r\n\r\n    estimator = tf.keras.estimator.model_to_estimator(model) # can provide distributed training strategy\r\n    estimator.train(input_fn=lambda: training_dataset.make_one_shot_iterator().get_next(), max_steps=20)\r\n\r\n\r\ndef main():\r\n    data_dir = \"./data/mock_data/\" # make sure this folder exists\r\n    # test_data_generator(data_dir) # need to run this only once to generate dummy data\r\n    # train_using_fit(data_dir) # approach 1 for training\r\n    train_using_estimator(data_dir) # approach 2 for training\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\n"}