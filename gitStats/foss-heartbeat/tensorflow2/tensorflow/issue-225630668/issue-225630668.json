{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9590", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9590/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9590/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9590/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9590", "id": 225630668, "node_id": "MDU6SXNzdWUyMjU2MzA2Njg=", "number": 9590, "title": "Memory Leak from Deep Learning Training Step? (Finalized Graph)", "user": {"login": "JamesKostas", "id": 22646371, "node_id": "MDQ6VXNlcjIyNjQ2Mzcx", "avatar_url": "https://avatars3.githubusercontent.com/u/22646371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JamesKostas", "html_url": "https://github.com/JamesKostas", "followers_url": "https://api.github.com/users/JamesKostas/followers", "following_url": "https://api.github.com/users/JamesKostas/following{/other_user}", "gists_url": "https://api.github.com/users/JamesKostas/gists{/gist_id}", "starred_url": "https://api.github.com/users/JamesKostas/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JamesKostas/subscriptions", "organizations_url": "https://api.github.com/users/JamesKostas/orgs", "repos_url": "https://api.github.com/users/JamesKostas/repos", "events_url": "https://api.github.com/users/JamesKostas/events{/privacy}", "received_events_url": "https://api.github.com/users/JamesKostas/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-05-02T08:50:57Z", "updated_at": "2017-05-13T00:07:40Z", "closed_at": "2017-05-13T00:07:40Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes, although my code is somewhat based on the MNIST deep learning tutorial.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 14.04 VERSION=\"14.04.5 LTS, Trusty Tahr\"</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary (I think, not 100% sure since it's been a few months since install. How can I check?)</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n('v0.11.0-2614-g14aeb08-dirty', '0.12.0-rc0')</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCUDA Version 8.0.44</li>\n<li><strong>GPU model and memory</strong>:<br>\nGeForce GTX 780M 4GB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\n<code>self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I apologize if this is not an actual bug; I am relatively new to TensorFlow. I have posted on StackOverflow <a href=\"http://stackoverflow.com/questions/43695085/tensorflow-deep-learning-memory-leak\" rel=\"nofollow\">here</a> and the only response I have gotten suggests filing a bug report here.</p>\n<p>If I comment the sess.run line above, <em>and only that line</em>, out (but still do all my pre-processing and validation/testing and such for a few thousand training batches), the memory leak does not happen.</p>\n<p>The leak is on the order of a few GB per hour (I am running Ubuntu, and have 16GB RAM + 16GB swap; the system becomes very laggy and unresponsive after 1-3 hours of running, when about 1/3-1/2 the RAM is used, which is a bit weird to me since I still have lots of RAM and the CPU is mostly free when this happens...)</p>\n<p>Here is some of the initializer code (only run once, at the beginning) if it is relevant:</p>\n<pre><code>\n    with tf.name_scope('after_final_layer') as scope:\n        self.layer1 = weights[\"wc1\"]\n        self.y_conv = network(self.x, weights, biases, self.keepratio)['out']\n        variable_summaries(self.y_conv)\n        # Note: Don't add a softmax reducer in the network if you are going to use this\n        # cross-entropy function\n        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_true, name = \"softmax/cross_ent\"), name = \"reduce_mean\")\n        self.train_step = tf.train.AdamOptimizer(learning_rate, name = \"Adam_Optimizer\").minimize(self.cross_entropy)\n\n        self.prediction = tf.argmax(self.y_conv, 1)\n        self.correct_prediction = tf.equal(self.prediction, tf.argmax(self.y_true, 1))\n\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n\n        if tensorboard:\n            # Merge all the summaries and write them out to the directory below\n            self.merged = tf.summary.merge_all()\n            self.my_writer = tf.summary.FileWriter('/home/james/PycharmProjects/AI_Final/my_tensorboard', graph=self.sess.graph)\n\n        # self.sess.run(tf.initialize_all_variables()) #old outdated way to do below\n        tf.global_variables_initializer().run(session=self.sess)\n        self.sess.graph.finalize() #make sure nothing new is added to graph\n\n</code></pre>\n<p>Notice that I have finalized the graph, so nothing new should be added to it.</p>\n<p>Am I doing something wrong/is this expected behavior, or is this a real bug?</p>\n<p>I have attached the source code as well (two .py files in directory).  <strong>Note: I am happy put in the work to reduce the source to a minimal recreation of the bug, but first I'd like verification that 1) this would be helpful (i.e. that the above info is not enough) and that 2) this is probably a bug, and not just an obvious beginner mistake on my part.</strong></p>\n<p>Thank you in advance.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/969906/source.zip\">source.zip</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes, although my code is somewhat based on the MNIST deep learning tutorial.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 14.04 VERSION=\"14.04.5 LTS, Trusty Tahr\"\nTensorFlow installed from (source or binary):\nbinary (I think, not 100% sure since it's been a few months since install. How can I check?)\nTensorFlow version (use command below):\n('v0.11.0-2614-g14aeb08-dirty', '0.12.0-rc0')\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nCUDA Version 8.0.44\nGPU model and memory:\nGeForce GTX 780M 4GB\nExact command to reproduce:\nself.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})\n\nDescribe the problem\nI apologize if this is not an actual bug; I am relatively new to TensorFlow. I have posted on StackOverflow here and the only response I have gotten suggests filing a bug report here.\nIf I comment the sess.run line above, and only that line, out (but still do all my pre-processing and validation/testing and such for a few thousand training batches), the memory leak does not happen.\nThe leak is on the order of a few GB per hour (I am running Ubuntu, and have 16GB RAM + 16GB swap; the system becomes very laggy and unresponsive after 1-3 hours of running, when about 1/3-1/2 the RAM is used, which is a bit weird to me since I still have lots of RAM and the CPU is mostly free when this happens...)\nHere is some of the initializer code (only run once, at the beginning) if it is relevant:\n\n    with tf.name_scope('after_final_layer') as scope:\n        self.layer1 = weights[\"wc1\"]\n        self.y_conv = network(self.x, weights, biases, self.keepratio)['out']\n        variable_summaries(self.y_conv)\n        # Note: Don't add a softmax reducer in the network if you are going to use this\n        # cross-entropy function\n        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_true, name = \"softmax/cross_ent\"), name = \"reduce_mean\")\n        self.train_step = tf.train.AdamOptimizer(learning_rate, name = \"Adam_Optimizer\").minimize(self.cross_entropy)\n\n        self.prediction = tf.argmax(self.y_conv, 1)\n        self.correct_prediction = tf.equal(self.prediction, tf.argmax(self.y_true, 1))\n\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\n\n        if tensorboard:\n            # Merge all the summaries and write them out to the directory below\n            self.merged = tf.summary.merge_all()\n            self.my_writer = tf.summary.FileWriter('/home/james/PycharmProjects/AI_Final/my_tensorboard', graph=self.sess.graph)\n\n        # self.sess.run(tf.initialize_all_variables()) #old outdated way to do below\n        tf.global_variables_initializer().run(session=self.sess)\n        self.sess.graph.finalize() #make sure nothing new is added to graph\n\n\nNotice that I have finalized the graph, so nothing new should be added to it.\nAm I doing something wrong/is this expected behavior, or is this a real bug?\nI have attached the source code as well (two .py files in directory).  Note: I am happy put in the work to reduce the source to a minimal recreation of the bug, but first I'd like verification that 1) this would be helpful (i.e. that the above info is not enough) and that 2) this is probably a bug, and not just an obvious beginner mistake on my part.\nThank you in advance.\nsource.zip", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes, although my code is somewhat based on the MNIST deep learning tutorial.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04 VERSION=\"14.04.5 LTS, Trusty Tahr\"\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary (I think, not 100% sure since it's been a few months since install. How can I check?)\r\n- **TensorFlow version (use command below)**:\r\n('v0.11.0-2614-g14aeb08-dirty', '0.12.0-rc0')\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nCUDA Version 8.0.44\r\n- **GPU model and memory**:\r\nGeForce GTX 780M 4GB\r\n- **Exact command to reproduce**:\r\n`self.sess.run(self.train_step, feed_dict={self.x: trainingdata, self.y_true: traininglabels, self.keepratio: self.training_keep_rate})`\r\n\r\n### Describe the problem\r\n\r\nI apologize if this is not an actual bug; I am relatively new to TensorFlow. I have posted on StackOverflow [here](http://stackoverflow.com/questions/43695085/tensorflow-deep-learning-memory-leak) and the only response I have gotten suggests filing a bug report here.\r\n\r\nIf I comment the sess.run line above, _and only that line_, out (but still do all my pre-processing and validation/testing and such for a few thousand training batches), the memory leak does not happen.\r\n\r\nThe leak is on the order of a few GB per hour (I am running Ubuntu, and have 16GB RAM + 16GB swap; the system becomes very laggy and unresponsive after 1-3 hours of running, when about 1/3-1/2 the RAM is used, which is a bit weird to me since I still have lots of RAM and the CPU is mostly free when this happens...)\r\n\r\nHere is some of the initializer code (only run once, at the beginning) if it is relevant:\r\n```\r\n\r\n    with tf.name_scope('after_final_layer') as scope:\r\n        self.layer1 = weights[\"wc1\"]\r\n        self.y_conv = network(self.x, weights, biases, self.keepratio)['out']\r\n        variable_summaries(self.y_conv)\r\n        # Note: Don't add a softmax reducer in the network if you are going to use this\r\n        # cross-entropy function\r\n        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(self.y_conv, self.y_true, name = \"softmax/cross_ent\"), name = \"reduce_mean\")\r\n        self.train_step = tf.train.AdamOptimizer(learning_rate, name = \"Adam_Optimizer\").minimize(self.cross_entropy)\r\n\r\n        self.prediction = tf.argmax(self.y_conv, 1)\r\n        self.correct_prediction = tf.equal(self.prediction, tf.argmax(self.y_true, 1))\r\n\r\n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_prediction, tf.float32))\r\n\r\n        if tensorboard:\r\n            # Merge all the summaries and write them out to the directory below\r\n            self.merged = tf.summary.merge_all()\r\n            self.my_writer = tf.summary.FileWriter('/home/james/PycharmProjects/AI_Final/my_tensorboard', graph=self.sess.graph)\r\n\r\n        # self.sess.run(tf.initialize_all_variables()) #old outdated way to do below\r\n        tf.global_variables_initializer().run(session=self.sess)\r\n        self.sess.graph.finalize() #make sure nothing new is added to graph\r\n\r\n```\r\nNotice that I have finalized the graph, so nothing new should be added to it.\r\n\r\nAm I doing something wrong/is this expected behavior, or is this a real bug?\r\n\r\nI have attached the source code as well (two .py files in directory).  **Note: I am happy put in the work to reduce the source to a minimal recreation of the bug, but first I'd like verification that 1) this would be helpful (i.e. that the above info is not enough) and that 2) this is probably a bug, and not just an obvious beginner mistake on my part.**\r\n\r\nThank you in advance.\r\n\r\n[source.zip](https://github.com/tensorflow/tensorflow/files/969906/source.zip)"}