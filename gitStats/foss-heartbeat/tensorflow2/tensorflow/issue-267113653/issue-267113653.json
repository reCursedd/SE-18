{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13854", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13854/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13854/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13854/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13854", "id": 267113653, "node_id": "MDU6SXNzdWUyNjcxMTM2NTM=", "number": 13854, "title": "tf.train.SyncReplicasOptimizer training does not start", "user": {"login": "onyamaa", "id": 32953264, "node_id": "MDQ6VXNlcjMyOTUzMjY0", "avatar_url": "https://avatars3.githubusercontent.com/u/32953264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/onyamaa", "html_url": "https://github.com/onyamaa", "followers_url": "https://api.github.com/users/onyamaa/followers", "following_url": "https://api.github.com/users/onyamaa/following{/other_user}", "gists_url": "https://api.github.com/users/onyamaa/gists{/gist_id}", "starred_url": "https://api.github.com/users/onyamaa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/onyamaa/subscriptions", "organizations_url": "https://api.github.com/users/onyamaa/orgs", "repos_url": "https://api.github.com/users/onyamaa/repos", "events_url": "https://api.github.com/users/onyamaa/events{/privacy}", "received_events_url": "https://api.github.com/users/onyamaa/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-10-20T09:20:04Z", "updated_at": "2018-01-04T00:02:34Z", "closed_at": "2018-01-04T00:02:34Z", "author_association": "NONE", "body_html": "<p>System information</p>\n<ul>\n<li>**Have I written custom code **: Yes</li>\n<li>**OS Platform and Distribution **: Linux Ubuntu 16.04</li>\n<li>**TensorFlow installed from **: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow-gpu==1.3.0</li>\n<li><strong>Python version</strong>: Python 3.6.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>: not installed</li>\n<li><strong>CUDA/cuDNN version</strong>: Cuda compilation tools, release 8.0, V8.0.61</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Quadro P5000 16GB</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nwrite actual IP addresses instead of  ip_address1 and ip_address2</li>\n</ul>\n<p>( on machine 1 )<br>\n$  python trainer.py <br>\n--replicas_num=1 <br>\n--ps_hosts=ip_address1:2222 <br>\n--worker_hosts=ip_address2:2223 <br>\n--job_name=ps --task_index=0</p>\n<p>( on machine 2 )<br>\n$  python trainer.py <br>\n--replicas_num=1 <br>\n--ps_hosts=ip_address1:2222 <br>\n--worker_hosts=ip_address2:2223 <br>\n--job_name=worker --task_index=0</p>\n<h3>Describe the problem</h3>\n<p>I'm trying to train an below model with distributed synchronized training.<br>\nI tried, 1 ps and 1 worker, 1 ps and 2 workers, 1 ps and 3 workers.<br>\nIt is a sample code similar to <a href=\"https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program\" rel=\"nofollow\">https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program</a></p>\n<p>If I comment out line 71 shown below training became asynchronized trainig and there is no problem.<br>\nopt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)</p>\n<p>But if I run it with line 71 not commented out training will not start.<br>\n(I followed <a href=\"https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage</a> to make it synchronized.)</p>\n<p>There is no error. But trainig will not continue. sess.run() (line 96) never ends.<br>\nIs there any suggestions what might be the problem ?</p>\n<h3>Source code / logs</h3>\n<h4>Source code</h4>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> slim\n<span class=\"pl-k\">from</span> tensorflow.examples.tutorials.mnist <span class=\"pl-k\">import</span> input_data\n\nflags <span class=\"pl-k\">=</span> tf.flags\nflags.DEFINE_string(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps_hosts<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Comma-separated list of hostname:port pairs<span class=\"pl-pds\">\"</span></span>)\nflags.DEFINE_string(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker_hosts<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Comma-separated list of hostname:port pairs<span class=\"pl-pds\">\"</span></span>)\nflags.DEFINE_string(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>job_name<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>One of 'ps', 'worker'<span class=\"pl-pds\">\"</span></span>)\nflags.DEFINE_integer(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task_index<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Index of task within the job<span class=\"pl-pds\">\"</span></span>)\nflags.DEFINE_integer(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>replicas_num<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Number of replicas<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> flags.<span class=\"pl-c1\">FLAGS</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> config</span>\n    <span class=\"pl-c1\">BATCH_SIZE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n    <span class=\"pl-c1\">TRAINING_STEPS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5000</span>\n    <span class=\"pl-c1\">PRINT_EVERY</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n    <span class=\"pl-c1\">LOG_DIR</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/tmp/<span class=\"pl-pds\">\"</span></span>\n\n    ps_hosts <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.ps_hosts.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-pds\">\"</span></span>)\n    worker_hosts <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.worker_hosts.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cluster specification</span>\n    cluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>: ps_hosts, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: worker_hosts})\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> start a server for a specific task</span>\n    server <span class=\"pl-k\">=</span> tf.train.Server(cluster,\n                             <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.job_name,\n                             <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.task_index)\n\n    mnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/tmp/MNIST_data<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">net</span>(<span class=\"pl-smi\">x</span>):\n        x_image <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">28</span>, <span class=\"pl-c1\">1</span>])\n        net <span class=\"pl-k\">=</span> slim.layers.conv2d(x_image, <span class=\"pl-c1\">32</span>, [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv1<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.layers.max_pool2d(net, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool1<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.layers.conv2d(net, <span class=\"pl-c1\">64</span>, [<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">5</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv2<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.layers.max_pool2d(net, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>pool2<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.layers.flatten(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>flatten<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.layers.fully_connected(net, <span class=\"pl-c1\">500</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fully_connected<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.layers.fully_connected(net, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>pred<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">return</span> net\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>job_name     = <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">FLAGS</span>.job_name)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task_index   = <span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">FLAGS</span>.task_index)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>replicas_num = <span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">FLAGS</span>.replicas_num)\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ps<span class=\"pl-pds\">\"</span></span>:\n        server.join()\n    <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">FLAGS</span>.job_name <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Between-graph replication</span>\n        <span class=\"pl-k\">with</span> tf.device(tf.train.replica_device_setter(\n                <span class=\"pl-v\">worker_device</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/job:worker/task:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">FLAGS</span>.task_index,\n                <span class=\"pl-v\">cluster</span><span class=\"pl-k\">=</span>cluster)):\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> count the number of updates</span>\n            global_step <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step<span class=\"pl-pds\">'</span></span>, [],\n                                          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0</span>),\n                                          <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n            x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x-input<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> target 10 output classes</span>\n            y_ <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y-input<span class=\"pl-pds\">\"</span></span>)\n            y <span class=\"pl-k\">=</span> net(x)\n\n            cross_entropy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>y, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>y_))\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))</span>\n\n            opt <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">1e-4</span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> for synchronous training</span>\n            opt <span class=\"pl-k\">=</span> tf.train.SyncReplicasOptimizer(opt, <span class=\"pl-v\">replicas_to_aggregate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.replicas_num, <span class=\"pl-v\">total_num_replicas</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.replicas_num)\n\n            train_step <span class=\"pl-k\">=</span> opt.minimize(cross_entropy, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step)\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0))</span>\n\n            correct_prediction <span class=\"pl-k\">=</span> tf.equal(tf.argmax(y, <span class=\"pl-c1\">1</span>), tf.argmax(y_, <span class=\"pl-c1\">1</span>))\n            accuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n            init_op <span class=\"pl-k\">=</span> tf.global_variables_initializer()\n\n        sv <span class=\"pl-k\">=</span> tf.train.Supervisor(<span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">FLAGS</span>.task_index <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>),\n                                 <span class=\"pl-v\">logdir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LOG_DIR</span>,\n                                 <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step,\n                                 <span class=\"pl-v\">init_op</span><span class=\"pl-k\">=</span>init_op)\n\n        <span class=\"pl-k\">with</span> sv.managed_session(server.target) <span class=\"pl-k\">as</span> sess:\n            step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n\n            test <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n            <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> sv.should_stop() <span class=\"pl-k\">and</span> step <span class=\"pl-k\">&lt;=</span> <span class=\"pl-c1\">TRAINING_STEPS</span>:\n                batch_x, batch_y <span class=\"pl-k\">=</span> mnist.train.next_batch(<span class=\"pl-c1\">BATCH_SIZE</span>)\n\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--------- run_step: test <span class=\"pl-c1\">%d</span> ----------<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> test)\n                test <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n                _, acc, step <span class=\"pl-k\">=</span> sess.run([train_step, accuracy, global_step],\n                                        <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: batch_x, y_: batch_y})\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--------- run_step: test <span class=\"pl-c1\">%d</span> ----------<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> test)\n                test <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n                <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">PRINT_EVERY</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Worker : <span class=\"pl-c1\">{}</span>, Step: <span class=\"pl-c1\">{}</span>, Accuracy (batch): <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.\\\n                          <span class=\"pl-c1\">format</span>(<span class=\"pl-c1\">FLAGS</span>.task_index, step, acc))\n\n            test_acc <span class=\"pl-k\">=</span> sess.run(accuracy, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: mnist.test.images, y_: mnist.test.labels})\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Test-Accuracy: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(test_acc))\n\n        sv.stop()\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    tf.app.run(<span class=\"pl-v\">main</span><span class=\"pl-k\">=</span>main)\n</pre></div>\n<h4>Logs</h4>\n<p>I got logs shown below from the parameter server and the worker</p>\n<p>from the parameter server:</p>\n<div class=\"highlight highlight-source-shell\"><pre>...\njob_name     = ps\ntask_index   = 0\nreplicas_num = 1</pre></div>\n<p>from the worker:</p>\n<div class=\"highlight highlight-source-shell\"><pre>...\njob_name     = worker\ntask_index   = 0\nreplicas_num = 1\n2017-10-20 17:54:43.891111: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 10bf51b4613ac80d with config:\n--------- run_step: <span class=\"pl-c1\">test</span> 0 ----------\n</pre></div>", "body_text": "System information\n\n**Have I written custom code **: Yes\n**OS Platform and Distribution **: Linux Ubuntu 16.04\n**TensorFlow installed from **: binary\nTensorFlow version (use command below): tensorflow-gpu==1.3.0\nPython version: Python 3.6.2\nBazel version (if compiling from source): not installed\nCUDA/cuDNN version: Cuda compilation tools, release 8.0, V8.0.61\nGPU model and memory: NVIDIA Quadro P5000 16GB\nExact command to reproduce:\nwrite actual IP addresses instead of  ip_address1 and ip_address2\n\n( on machine 1 )\n$  python trainer.py \n--replicas_num=1 \n--ps_hosts=ip_address1:2222 \n--worker_hosts=ip_address2:2223 \n--job_name=ps --task_index=0\n( on machine 2 )\n$  python trainer.py \n--replicas_num=1 \n--ps_hosts=ip_address1:2222 \n--worker_hosts=ip_address2:2223 \n--job_name=worker --task_index=0\nDescribe the problem\nI'm trying to train an below model with distributed synchronized training.\nI tried, 1 ps and 1 worker, 1 ps and 2 workers, 1 ps and 3 workers.\nIt is a sample code similar to https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program\nIf I comment out line 71 shown below training became asynchronized trainig and there is no problem.\nopt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)\nBut if I run it with line 71 not commented out training will not start.\n(I followed https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage to make it synchronized.)\nThere is no error. But trainig will not continue. sess.run() (line 96) never ends.\nIs there any suggestions what might be the problem ?\nSource code / logs\nSource code\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nflags = tf.flags\nflags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\nflags.DEFINE_integer(\"replicas_num\", 3, \"Number of replicas\")\nFLAGS = flags.FLAGS\n\ndef main(_):\n    # config\n    BATCH_SIZE = 10\n    TRAINING_STEPS = 5000\n    PRINT_EVERY = 100\n    LOG_DIR = \"/tmp/\"\n\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\n\n    # cluster specification\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n\n    # start a server for a specific task\n    server = tf.train.Server(cluster,\n                             job_name=FLAGS.job_name,\n                             task_index=FLAGS.task_index)\n\n    mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\n\n    def net(x):\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\n        net = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')\n        net = slim.layers.max_pool2d(net, [2, 2], scope='pool1')\n        net = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')\n        net = slim.layers.max_pool2d(net, [2, 2], scope='pool2')\n        net = slim.layers.flatten(net, scope='flatten')\n        net = slim.layers.fully_connected(net, 500, scope='fully_connected')\n        net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='pred')\n        return net\n\n    print(\"job_name     = %s\" % FLAGS.job_name)\n    print(\"task_index   = %d\" % FLAGS.task_index)\n    print(\"replicas_num = %d\" % FLAGS.replicas_num)\n\n    if FLAGS.job_name == \"ps\":\n        server.join()\n    elif FLAGS.job_name == \"worker\":\n        # Between-graph replication\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n                cluster=cluster)):\n            # count the number of updates\n            global_step = tf.get_variable('global_step', [],\n                                          initializer=tf.constant_initializer(0),\n                                          trainable=False)\n\n            x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\n            # target 10 output classes\n            y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\n            y = net(x)\n\n            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\n            #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n\n            opt = tf.train.AdamOptimizer(1e-4)\n\n            # for synchronous training\n            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)\n\n            train_step = opt.minimize(cross_entropy, global_step=global_step)\n            #sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0))\n\n            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n            init_op = tf.global_variables_initializer()\n\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                                 logdir=LOG_DIR,\n                                 global_step=global_step,\n                                 init_op=init_op)\n\n        with sv.managed_session(server.target) as sess:\n            step = 0\n\n            test = 0\n            while not sv.should_stop() and step <= TRAINING_STEPS:\n                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\n\n                print(\"--------- run_step: test %d ----------\" % test)\n                test += 1\n\n                _, acc, step = sess.run([train_step, accuracy, global_step],\n                                        feed_dict={x: batch_x, y_: batch_y})\n                print(\"--------- run_step: test %d ----------\" % test)\n                test += 1\n\n                if step % PRINT_EVERY == 0:\n                    print(\"Worker : {}, Step: {}, Accuracy (batch): {}\".\\\n                          format(FLAGS.task_index, step, acc))\n\n            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\n            print(\"Test-Accuracy: {}\".format(test_acc))\n\n        sv.stop()\n\n\nif __name__ == \"__main__\":\n    tf.app.run(main=main)\n\nLogs\nI got logs shown below from the parameter server and the worker\nfrom the parameter server:\n...\njob_name     = ps\ntask_index   = 0\nreplicas_num = 1\nfrom the worker:\n...\njob_name     = worker\ntask_index   = 0\nreplicas_num = 1\n2017-10-20 17:54:43.891111: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 10bf51b4613ac80d with config:\n--------- run_step: test 0 ----------", "body": "System information\r\n- **Have I written custom code **: Yes\r\n- **OS Platform and Distribution **: Linux Ubuntu 16.04\r\n- **TensorFlow installed from **: binary\r\n- **TensorFlow version (use command below)**: tensorflow-gpu==1.3.0\r\n- **Python version**: Python 3.6.2\r\n- **Bazel version (if compiling from source)**: not installed\r\n- **CUDA/cuDNN version**: Cuda compilation tools, release 8.0, V8.0.61\r\n- **GPU model and memory**: NVIDIA Quadro P5000 16GB\r\n- **Exact command to reproduce**:\r\nwrite actual IP addresses instead of  ip_address1 and ip_address2 \r\n\r\n( on machine 1 )\r\n$  python trainer.py \\\r\n    --replicas_num=1 \\\r\n    --ps_hosts=ip_address1:2222 \\\r\n    --worker_hosts=ip_address2:2223 \\\r\n    --job_name=ps --task_index=0\r\n\r\n( on machine 2 )\r\n$  python trainer.py \\\r\n    --replicas_num=1 \\\r\n    --ps_hosts=ip_address1:2222 \\\r\n    --worker_hosts=ip_address2:2223 \\\r\n    --job_name=worker --task_index=0\r\n\r\n### Describe the problem\r\nI'm trying to train an below model with distributed synchronized training.\r\nI tried, 1 ps and 1 worker, 1 ps and 2 workers, 1 ps and 3 workers.\r\nIt is a sample code similar to https://www.tensorflow.org/deploy/distributed#putting_it_all_together_example_trainer_program\r\n\r\nIf I comment out line 71 shown below training became asynchronized trainig and there is no problem.\r\nopt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)\r\n\r\nBut if I run it with line 71 not commented out training will not start.\r\n(I followed https://www.tensorflow.org/api_docs/python/tf/train/SyncReplicasOptimizer#usage to make it synchronized.)\r\n\r\nThere is no error. But trainig will not continue. sess.run() (line 96) never ends.\r\nIs there any suggestions what might be the problem ?\r\n\r\n### Source code / logs\r\n#### Source code\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nflags = tf.flags\r\nflags.DEFINE_string(\"ps_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"worker_hosts\", \"\", \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"job_name\", \"\", \"One of 'ps', 'worker'\")\r\nflags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\nflags.DEFINE_integer(\"replicas_num\", 3, \"Number of replicas\")\r\nFLAGS = flags.FLAGS\r\n\r\ndef main(_):\r\n    # config\r\n    BATCH_SIZE = 10\r\n    TRAINING_STEPS = 5000\r\n    PRINT_EVERY = 100\r\n    LOG_DIR = \"/tmp/\"\r\n\r\n    ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n    worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n\r\n    # cluster specification\r\n    cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n\r\n    # start a server for a specific task\r\n    server = tf.train.Server(cluster,\r\n                             job_name=FLAGS.job_name,\r\n                             task_index=FLAGS.task_index)\r\n\r\n    mnist = input_data.read_data_sets('/tmp/MNIST_data', one_hot=True)\r\n\r\n    def net(x):\r\n        x_image = tf.reshape(x, [-1, 28, 28, 1])\r\n        net = slim.layers.conv2d(x_image, 32, [5, 5], scope='conv1')\r\n        net = slim.layers.max_pool2d(net, [2, 2], scope='pool1')\r\n        net = slim.layers.conv2d(net, 64, [5, 5], scope='conv2')\r\n        net = slim.layers.max_pool2d(net, [2, 2], scope='pool2')\r\n        net = slim.layers.flatten(net, scope='flatten')\r\n        net = slim.layers.fully_connected(net, 500, scope='fully_connected')\r\n        net = slim.layers.fully_connected(net, 10, activation_fn=None, scope='pred')\r\n        return net\r\n\r\n    print(\"job_name     = %s\" % FLAGS.job_name)\r\n    print(\"task_index   = %d\" % FLAGS.task_index)\r\n    print(\"replicas_num = %d\" % FLAGS.replicas_num)\r\n\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n        # Between-graph replication\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n                cluster=cluster)):\r\n            # count the number of updates\r\n            global_step = tf.get_variable('global_step', [],\r\n                                          initializer=tf.constant_initializer(0),\r\n                                          trainable=False)\r\n\r\n            x = tf.placeholder(tf.float32, shape=[None, 784], name=\"x-input\")\r\n            # target 10 output classes\r\n            y_ = tf.placeholder(tf.float32, shape=[None, 10], name=\"y-input\")\r\n            y = net(x)\r\n\r\n            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y, labels=y_))\r\n            #cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\r\n\r\n            opt = tf.train.AdamOptimizer(1e-4)\r\n\r\n            # for synchronous training\r\n            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=FLAGS.replicas_num, total_num_replicas=FLAGS.replicas_num)\r\n\r\n            train_step = opt.minimize(cross_entropy, global_step=global_step)\r\n            #sync_replicas_hook = opt.make_session_run_hook(is_chief=(FLAGS.task_index == 0))\r\n\r\n            correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\r\n            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n            init_op = tf.global_variables_initializer()\r\n\r\n        sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                                 logdir=LOG_DIR,\r\n                                 global_step=global_step,\r\n                                 init_op=init_op)\r\n\r\n        with sv.managed_session(server.target) as sess:\r\n            step = 0\r\n\r\n            test = 0\r\n            while not sv.should_stop() and step <= TRAINING_STEPS:\r\n                batch_x, batch_y = mnist.train.next_batch(BATCH_SIZE)\r\n\r\n                print(\"--------- run_step: test %d ----------\" % test)\r\n                test += 1\r\n\r\n                _, acc, step = sess.run([train_step, accuracy, global_step],\r\n                                        feed_dict={x: batch_x, y_: batch_y})\r\n                print(\"--------- run_step: test %d ----------\" % test)\r\n                test += 1\r\n\r\n                if step % PRINT_EVERY == 0:\r\n                    print(\"Worker : {}, Step: {}, Accuracy (batch): {}\".\\\r\n                          format(FLAGS.task_index, step, acc))\r\n\r\n            test_acc = sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels})\r\n            print(\"Test-Accuracy: {}\".format(test_acc))\r\n\r\n        sv.stop()\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run(main=main)\r\n\r\n```\r\n#### Logs\r\nI got logs shown below from the parameter server and the worker \r\n\r\nfrom the parameter server:\r\n```bash\r\n...\r\njob_name     = ps\r\ntask_index   = 0\r\nreplicas_num = 1\r\n```\r\nfrom the worker:\r\n```bash\r\n...\r\njob_name     = worker\r\ntask_index   = 0\r\nreplicas_num = 1\r\n2017-10-20 17:54:43.891111: I tensorflow/core/distributed_runtime/master_session.cc:998] Start master session 10bf51b4613ac80d with config:\r\n--------- run_step: test 0 ----------\r\n\r\n```\r\n"}