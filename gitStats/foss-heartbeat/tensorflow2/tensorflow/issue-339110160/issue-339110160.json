{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20608", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20608/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20608/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20608/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20608", "id": 339110160, "node_id": "MDU6SXNzdWUzMzkxMTAxNjA=", "number": 20608, "title": "Allow restoring subgraph from checkpoint for weight sharing between different models", "user": {"login": "tremblerz", "id": 10557215, "node_id": "MDQ6VXNlcjEwNTU3MjE1", "avatar_url": "https://avatars1.githubusercontent.com/u/10557215?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tremblerz", "html_url": "https://github.com/tremblerz", "followers_url": "https://api.github.com/users/tremblerz/followers", "following_url": "https://api.github.com/users/tremblerz/following{/other_user}", "gists_url": "https://api.github.com/users/tremblerz/gists{/gist_id}", "starred_url": "https://api.github.com/users/tremblerz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tremblerz/subscriptions", "organizations_url": "https://api.github.com/users/tremblerz/orgs", "repos_url": "https://api.github.com/users/tremblerz/repos", "events_url": "https://api.github.com/users/tremblerz/events{/privacy}", "received_events_url": "https://api.github.com/users/tremblerz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-07-07T02:26:56Z", "updated_at": "2018-11-22T18:57:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOS Sierra</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.7.0-3-g024aecf414 1.7.0</li>\n<li><strong>Python version</strong>: 3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: Read the summary below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>There are many scenarios where we want to do parameter sharing between two different models (let's call <code>A</code> and <code>B</code>). If <code>A</code> is trained and checkpointed, we can either load all of the variables of <code>A</code> or selectively load variables from A's checkpoint by passing a list <code>L</code> containing all shared params. But the second way is not helpful if we don't know <code>L</code> beforehand. Can we have a way by which we can share variables just based on the common subset between two graphs.<br>\nPS - I am assuming that parameters of <code>A</code> and <code>B</code> follow same naming convention in scopes.</p>\n<h3>Source code / logs</h3>\n<p>Here is what I was trying to do using metagraph but this also doesn't seem to work. Please note that I deliberately do <code>tf.reset_default_graph()</code> as objective is to share parameter with a checkpointed model. Sorry lots of redundant lines in the code.</p>\n<pre><code>import tensorflow as tf\nmnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n\ndef model_1(features, labels):\n    with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\n        out = tf.reshape(features, [-1, 28, 28, 1])\n    with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.conv2d(\n           inputs=out,\n           filters=32,\n           kernel_size=[5, 5],\n           padding=\"same\",\n           activation=tf.nn.relu)\n    with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.conv2d(\n           inputs=out,\n           filters=32,\n           kernel_size=[3, 3],\n           padding=\"same\",\n           activation=tf.nn.relu)\n    with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\n    with tf.variable_scope(\"conv_4_3x3\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.conv2d(\n           inputs=out,\n           filters=64,\n           kernel_size=[3, 3],\n           padding=\"same\",\n           activation=tf.nn.relu)\n    with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\n        flattened = tf.reshape(out, [-1, 14 * 14 * 64])\n        logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\n    softmax = tf.nn.softmax(logits, name=\"softmax\")\n\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\n    return loss, train_op\n\ndef model_2(features, labels, old_graph):\n    with old_graph.as_default():\n        with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\n            out = tf.reshape(features, [-1, 28, 28, 1])\n        with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.conv2d(\n               inputs=out,\n               filters=32,\n               kernel_size=[5, 5],\n               padding=\"same\",\n               activation=tf.nn.relu)\n        with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.conv2d(\n               inputs=out,\n               filters=32,\n               kernel_size=[3, 3],\n               padding=\"same\",\n               activation=tf.nn.relu)\n        with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\n        # This is the place where model 1 and 2 are actually different\n        with tf.variable_scope(\"conv_4_5x5\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.conv2d(\n               inputs=out,\n               filters=64,\n               kernel_size=[5, 5],\n               padding=\"same\",\n               activation=tf.nn.relu)\n        with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\n            flattened = tf.reshape(out, [-1, 14 * 14 * 64])\n            logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\n        softmax = tf.nn.softmax(logits, name=\"softmax\")\n\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\n    return loss, train_op\n\nfeatures = tf.placeholder(shape=[None, 784], dtype=tf.float32, name=\"features\")\nlabels = tf.placeholder(shape=[None], dtype=tf.int32, name=\"labels\")\n\nsteps = 0\nloss_op, train_op = model_1(features, labels)\n\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_1\") as sess:\n    while steps &lt;= 20:\n        features_, labels_ = mnist.train.next_batch(100)\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\n            \"features:0\": features_, \"labels:0\": labels_})\n        if steps % 5 == 0:\n            print(\"loss at step {} = {}\".format(steps, loss))\n        steps += 1\n\ntf.reset_default_graph()\ntf.train.import_meta_graph(\"./model_1/model.ckpt-\"+str(steps)+\".meta\")\nold_graph = tf.get_default_graph()\n\nsteps = 0\nloss_op, train_op = model_2(features, labels, old_graph)\n\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_2\") as sess:\n    while steps &lt;= 20:\n        features_, labels_ = mnist.train.next_batch(100)\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\n            \"features:0\": features_, \"labels:0\": labels_})\n        if steps % 5 == 0:\n            print(\"loss at step {} = {}\".format(steps, loss))\n        steps += 1\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS Sierra\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.7.0-3-g024aecf414 1.7.0\nPython version: 3\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: Read the summary below\n\nDescribe the problem\nThere are many scenarios where we want to do parameter sharing between two different models (let's call A and B). If A is trained and checkpointed, we can either load all of the variables of A or selectively load variables from A's checkpoint by passing a list L containing all shared params. But the second way is not helpful if we don't know L beforehand. Can we have a way by which we can share variables just based on the common subset between two graphs.\nPS - I am assuming that parameters of A and B follow same naming convention in scopes.\nSource code / logs\nHere is what I was trying to do using metagraph but this also doesn't seem to work. Please note that I deliberately do tf.reset_default_graph() as objective is to share parameter with a checkpointed model. Sorry lots of redundant lines in the code.\nimport tensorflow as tf\nmnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n\ndef model_1(features, labels):\n    with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\n        out = tf.reshape(features, [-1, 28, 28, 1])\n    with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.conv2d(\n           inputs=out,\n           filters=32,\n           kernel_size=[5, 5],\n           padding=\"same\",\n           activation=tf.nn.relu)\n    with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.conv2d(\n           inputs=out,\n           filters=32,\n           kernel_size=[3, 3],\n           padding=\"same\",\n           activation=tf.nn.relu)\n    with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\n    with tf.variable_scope(\"conv_4_3x3\", reuse=tf.AUTO_REUSE):\n        out = tf.layers.conv2d(\n           inputs=out,\n           filters=64,\n           kernel_size=[3, 3],\n           padding=\"same\",\n           activation=tf.nn.relu)\n    with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\n        flattened = tf.reshape(out, [-1, 14 * 14 * 64])\n        logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\n    softmax = tf.nn.softmax(logits, name=\"softmax\")\n\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\n    return loss, train_op\n\ndef model_2(features, labels, old_graph):\n    with old_graph.as_default():\n        with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\n            out = tf.reshape(features, [-1, 28, 28, 1])\n        with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.conv2d(\n               inputs=out,\n               filters=32,\n               kernel_size=[5, 5],\n               padding=\"same\",\n               activation=tf.nn.relu)\n        with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.conv2d(\n               inputs=out,\n               filters=32,\n               kernel_size=[3, 3],\n               padding=\"same\",\n               activation=tf.nn.relu)\n        with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\n        # This is the place where model 1 and 2 are actually different\n        with tf.variable_scope(\"conv_4_5x5\", reuse=tf.AUTO_REUSE):\n            out = tf.layers.conv2d(\n               inputs=out,\n               filters=64,\n               kernel_size=[5, 5],\n               padding=\"same\",\n               activation=tf.nn.relu)\n        with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\n            flattened = tf.reshape(out, [-1, 14 * 14 * 64])\n            logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\n        softmax = tf.nn.softmax(logits, name=\"softmax\")\n\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\n    return loss, train_op\n\nfeatures = tf.placeholder(shape=[None, 784], dtype=tf.float32, name=\"features\")\nlabels = tf.placeholder(shape=[None], dtype=tf.int32, name=\"labels\")\n\nsteps = 0\nloss_op, train_op = model_1(features, labels)\n\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_1\") as sess:\n    while steps <= 20:\n        features_, labels_ = mnist.train.next_batch(100)\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\n            \"features:0\": features_, \"labels:0\": labels_})\n        if steps % 5 == 0:\n            print(\"loss at step {} = {}\".format(steps, loss))\n        steps += 1\n\ntf.reset_default_graph()\ntf.train.import_meta_graph(\"./model_1/model.ckpt-\"+str(steps)+\".meta\")\nold_graph = tf.get_default_graph()\n\nsteps = 0\nloss_op, train_op = model_2(features, labels, old_graph)\n\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_2\") as sess:\n    while steps <= 20:\n        features_, labels_ = mnist.train.next_batch(100)\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\n            \"features:0\": features_, \"labels:0\": labels_})\n        if steps % 5 == 0:\n            print(\"loss at step {} = {}\".format(steps, loss))\n        steps += 1", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS Sierra\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.7.0-3-g024aecf414 1.7.0\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: Read the summary below\r\n\r\n### Describe the problem\r\nThere are many scenarios where we want to do parameter sharing between two different models (let's call `A` and `B`). If `A` is trained and checkpointed, we can either load all of the variables of `A` or selectively load variables from A's checkpoint by passing a list `L` containing all shared params. But the second way is not helpful if we don't know `L` beforehand. Can we have a way by which we can share variables just based on the common subset between two graphs.\r\nPS - I am assuming that parameters of `A` and `B` follow same naming convention in scopes.\r\n\r\n### Source code / logs\r\nHere is what I was trying to do using metagraph but this also doesn't seem to work. Please note that I deliberately do `tf.reset_default_graph()` as objective is to share parameter with a checkpointed model. Sorry lots of redundant lines in the code.\r\n```\r\nimport tensorflow as tf\r\nmnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\r\n\r\ndef model_1(features, labels):\r\n    with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\r\n        out = tf.reshape(features, [-1, 28, 28, 1])\r\n    with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.conv2d(\r\n           inputs=out,\r\n           filters=32,\r\n           kernel_size=[5, 5],\r\n           padding=\"same\",\r\n           activation=tf.nn.relu)\r\n    with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.conv2d(\r\n           inputs=out,\r\n           filters=32,\r\n           kernel_size=[3, 3],\r\n           padding=\"same\",\r\n           activation=tf.nn.relu)\r\n    with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\r\n    with tf.variable_scope(\"conv_4_3x3\", reuse=tf.AUTO_REUSE):\r\n        out = tf.layers.conv2d(\r\n           inputs=out,\r\n           filters=64,\r\n           kernel_size=[3, 3],\r\n           padding=\"same\",\r\n           activation=tf.nn.relu)\r\n    with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\r\n        flattened = tf.reshape(out, [-1, 14 * 14 * 64])\r\n        logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\r\n    softmax = tf.nn.softmax(logits, name=\"softmax\")\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\r\n    return loss, train_op\r\n\r\ndef model_2(features, labels, old_graph):\r\n    with old_graph.as_default():\r\n        with tf.variable_scope(\"input\", reuse=tf.AUTO_REUSE):\r\n            out = tf.reshape(features, [-1, 28, 28, 1])\r\n        with tf.variable_scope(\"conv_1_5x5\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.conv2d(\r\n               inputs=out,\r\n               filters=32,\r\n               kernel_size=[5, 5],\r\n               padding=\"same\",\r\n               activation=tf.nn.relu)\r\n        with tf.variable_scope(\"conv_2_3x3\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.conv2d(\r\n               inputs=out,\r\n               filters=32,\r\n               kernel_size=[3, 3],\r\n               padding=\"same\",\r\n               activation=tf.nn.relu)\r\n        with tf.variable_scope(\"pool_3_2x2\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.max_pooling2d(inputs=out, pool_size=[2, 2], strides=2)\r\n        # This is the place where model 1 and 2 are actually different\r\n        with tf.variable_scope(\"conv_4_5x5\", reuse=tf.AUTO_REUSE):\r\n            out = tf.layers.conv2d(\r\n               inputs=out,\r\n               filters=64,\r\n               kernel_size=[5, 5],\r\n               padding=\"same\",\r\n               activation=tf.nn.relu)\r\n        with tf.variable_scope(\"dense\", reuse=tf.AUTO_REUSE):\r\n            flattened = tf.reshape(out, [-1, 14 * 14 * 64])\r\n            logits = tf.layers.dense(flattened, units=10, activation=tf.nn.relu)\r\n        softmax = tf.nn.softmax(logits, name=\"softmax\")\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\r\n    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_or_create_global_step())\r\n    return loss, train_op\r\n\r\nfeatures = tf.placeholder(shape=[None, 784], dtype=tf.float32, name=\"features\")\r\nlabels = tf.placeholder(shape=[None], dtype=tf.int32, name=\"labels\")\r\n\r\nsteps = 0\r\nloss_op, train_op = model_1(features, labels)\r\n\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_1\") as sess:\r\n    while steps <= 20:\r\n        features_, labels_ = mnist.train.next_batch(100)\r\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\r\n            \"features:0\": features_, \"labels:0\": labels_})\r\n        if steps % 5 == 0:\r\n            print(\"loss at step {} = {}\".format(steps, loss))\r\n        steps += 1\r\n\r\ntf.reset_default_graph()\r\ntf.train.import_meta_graph(\"./model_1/model.ckpt-\"+str(steps)+\".meta\")\r\nold_graph = tf.get_default_graph()\r\n\r\nsteps = 0\r\nloss_op, train_op = model_2(features, labels, old_graph)\r\n\r\nwith tf.train.MonitoredTrainingSession(checkpoint_dir=\"./model_2\") as sess:\r\n    while steps <= 20:\r\n        features_, labels_ = mnist.train.next_batch(100)\r\n        loss, _ = sess.run([loss_op, train_op], feed_dict={\r\n            \"features:0\": features_, \"labels:0\": labels_})\r\n        if steps % 5 == 0:\r\n            print(\"loss at step {} = {}\".format(steps, loss))\r\n        steps += 1\r\n\r\n"}