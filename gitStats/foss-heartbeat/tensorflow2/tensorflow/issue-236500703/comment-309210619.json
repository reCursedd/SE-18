{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309210619", "html_url": "https://github.com/tensorflow/tensorflow/issues/10767#issuecomment-309210619", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10767", "id": 309210619, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTIxMDYxOQ==", "user": {"login": "gfolego", "id": 8110317, "node_id": "MDQ6VXNlcjgxMTAzMTc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8110317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gfolego", "html_url": "https://github.com/gfolego", "followers_url": "https://api.github.com/users/gfolego/followers", "following_url": "https://api.github.com/users/gfolego/following{/other_user}", "gists_url": "https://api.github.com/users/gfolego/gists{/gist_id}", "starred_url": "https://api.github.com/users/gfolego/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gfolego/subscriptions", "organizations_url": "https://api.github.com/users/gfolego/orgs", "repos_url": "https://api.github.com/users/gfolego/repos", "events_url": "https://api.github.com/users/gfolego/events{/privacy}", "received_events_url": "https://api.github.com/users/gfolego/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-17T11:53:52Z", "updated_at": "2017-06-17T11:53:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> Sure! I'll use ImageNet as an example, but my case is quite similar.<br>\nLet's say I have the ILSVRC2017 dataset, and I'm working on image classification only, so my main objective is to get a high top 5 accuracy.<br>\nNow, I'm analyzing a CNN's performance given a specific number of training images, which will vary from very few to the complete training set. The motivation behind this analysis is to determine the best trade-off between size and accuracy, so we know how many images we need when collecting a new dataset.</p>\n<p>In particular, ILSVRC2017 training dataset (for classification/localization) has between 732 and 1300 images for each class. If I use as few as 5 from each class for training a (large) CNN from scratch, this will most likely result in a CNN with random predictions or maybe predicting always the same class. So the top 5 accuracy for this model will be artificially (and incorrectly, IMHO) high.</p>\n<p>So, if I'm early stopping or choosing a model based on this metric, I'll get a CNN that doesn't work in practice. Also, based on this approach, training with 5 samples will most likely achieve better top 5 accuracy than training with the complete set.</p>\n<p>Please note that I'm not the first one experiencing this, as indicated in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"234166884\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10489\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/10489/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/10489\">#10489</a> and in this issue on Keras (using TF's in_top_k) <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"219625928\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/keras-team/keras/issues/6167\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/6167/hovercard\" href=\"https://github.com/keras-team/keras/issues/6167\">keras-team/keras#6167</a></p>\n<p>TL;DR: If a CNN predicts the same probability for every class, its top 5 accuracy in ILSVRC2017 classification is 100%, which is naturally incorrect.</p>\n<p>Thanks!</p>", "body_text": "@aselle Sure! I'll use ImageNet as an example, but my case is quite similar.\nLet's say I have the ILSVRC2017 dataset, and I'm working on image classification only, so my main objective is to get a high top 5 accuracy.\nNow, I'm analyzing a CNN's performance given a specific number of training images, which will vary from very few to the complete training set. The motivation behind this analysis is to determine the best trade-off between size and accuracy, so we know how many images we need when collecting a new dataset.\nIn particular, ILSVRC2017 training dataset (for classification/localization) has between 732 and 1300 images for each class. If I use as few as 5 from each class for training a (large) CNN from scratch, this will most likely result in a CNN with random predictions or maybe predicting always the same class. So the top 5 accuracy for this model will be artificially (and incorrectly, IMHO) high.\nSo, if I'm early stopping or choosing a model based on this metric, I'll get a CNN that doesn't work in practice. Also, based on this approach, training with 5 samples will most likely achieve better top 5 accuracy than training with the complete set.\nPlease note that I'm not the first one experiencing this, as indicated in #10489 and in this issue on Keras (using TF's in_top_k) keras-team/keras#6167\nTL;DR: If a CNN predicts the same probability for every class, its top 5 accuracy in ILSVRC2017 classification is 100%, which is naturally incorrect.\nThanks!", "body": "@aselle Sure! I'll use ImageNet as an example, but my case is quite similar.\r\nLet's say I have the ILSVRC2017 dataset, and I'm working on image classification only, so my main objective is to get a high top 5 accuracy.\r\nNow, I'm analyzing a CNN's performance given a specific number of training images, which will vary from very few to the complete training set. The motivation behind this analysis is to determine the best trade-off between size and accuracy, so we know how many images we need when collecting a new dataset.\r\n\r\nIn particular, ILSVRC2017 training dataset (for classification/localization) has between 732 and 1300 images for each class. If I use as few as 5 from each class for training a (large) CNN from scratch, this will most likely result in a CNN with random predictions or maybe predicting always the same class. So the top 5 accuracy for this model will be artificially (and incorrectly, IMHO) high.\r\n\r\nSo, if I'm early stopping or choosing a model based on this metric, I'll get a CNN that doesn't work in practice. Also, based on this approach, training with 5 samples will most likely achieve better top 5 accuracy than training with the complete set.\r\n\r\nPlease note that I'm not the first one experiencing this, as indicated in #10489 and in this issue on Keras (using TF's in_top_k) https://github.com/fchollet/keras/issues/6167\r\n\r\nTL;DR: If a CNN predicts the same probability for every class, its top 5 accuracy in ILSVRC2017 classification is 100%, which is naturally incorrect.\r\n\r\nThanks!"}