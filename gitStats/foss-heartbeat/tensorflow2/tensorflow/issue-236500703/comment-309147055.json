{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/309147055", "html_url": "https://github.com/tensorflow/tensorflow/issues/10767#issuecomment-309147055", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10767", "id": 309147055, "node_id": "MDEyOklzc3VlQ29tbWVudDMwOTE0NzA1NQ==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-16T22:17:20Z", "updated_at": "2017-06-16T22:17:20Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8110317\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gfolego\">@gfolego</a>, could you give some motivation for why this is needed. It seems you worry that there is a trivial null-space to the loss function where you return constant 1's as your logits which causes zero loss since every class is in top_k. However, with random weight initialization this doesn't seem likely to be found with sgd since the gradient will not reliably target that.</p>", "body_text": "@gfolego, could you give some motivation for why this is needed. It seems you worry that there is a trivial null-space to the loss function where you return constant 1's as your logits which causes zero loss since every class is in top_k. However, with random weight initialization this doesn't seem likely to be found with sgd since the gradient will not reliably target that.", "body": "@gfolego, could you give some motivation for why this is needed. It seems you worry that there is a trivial null-space to the loss function where you return constant 1's as your logits which causes zero loss since every class is in top_k. However, with random weight initialization this doesn't seem likely to be found with sgd since the gradient will not reliably target that."}