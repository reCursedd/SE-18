{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6675", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6675/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6675/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6675/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6675", "id": 199081285, "node_id": "MDU6SXNzdWUxOTkwODEyODU=", "number": 6675, "title": "Creating an auxiliary CUDA Stream for an op", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-01-05T22:55:56Z", "updated_at": "2018-01-18T23:05:16Z", "closed_at": "2017-01-06T17:55:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am writing an op which (sadly) requires using multiple CUDA streams to be efficient. Specifically, I need to have one auxiliary CUDA stream on whatever device the op is being executed; without this stream, the operations in this op will get entangled with operations in the rest of the graph, because the op <em>has</em> to occasionally wait for its GPU kernels to complete before it can do things on CPU.</p>\n<p>If I need to have an auxiliary CUDA stream besides the primary TensorFlow stream, one limited <em>just</em>  to the invocation of this op, how can I do so?</p>\n<p>(To clarify, the CUDA stream should be long-lived across multiple invocations of the op, probably, just that it shouldn't the same stream as used by the rest of the graph)</p>", "body_text": "I am writing an op which (sadly) requires using multiple CUDA streams to be efficient. Specifically, I need to have one auxiliary CUDA stream on whatever device the op is being executed; without this stream, the operations in this op will get entangled with operations in the rest of the graph, because the op has to occasionally wait for its GPU kernels to complete before it can do things on CPU.\nIf I need to have an auxiliary CUDA stream besides the primary TensorFlow stream, one limited just  to the invocation of this op, how can I do so?\n(To clarify, the CUDA stream should be long-lived across multiple invocations of the op, probably, just that it shouldn't the same stream as used by the rest of the graph)", "body": "I am writing an op which (sadly) requires using multiple CUDA streams to be efficient. Specifically, I need to have one auxiliary CUDA stream on whatever device the op is being executed; without this stream, the operations in this op will get entangled with operations in the rest of the graph, because the op _has_ to occasionally wait for its GPU kernels to complete before it can do things on CPU.\r\n\r\nIf I need to have an auxiliary CUDA stream besides the primary TensorFlow stream, one limited *just*  to the invocation of this op, how can I do so?\r\n\r\n(To clarify, the CUDA stream should be long-lived across multiple invocations of the op, probably, just that it shouldn't the same stream as used by the rest of the graph)"}