{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10284", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10284/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10284/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10284/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10284", "id": 232197670, "node_id": "MDU6SXNzdWUyMzIxOTc2NzA=", "number": 10284, "title": "Using fixed_unigram_candidate_sampler + nce_loss with reserved_ids emits NaN outputs", "user": {"login": "shuuki4", "id": 12455653, "node_id": "MDQ6VXNlcjEyNDU1NjUz", "avatar_url": "https://avatars2.githubusercontent.com/u/12455653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shuuki4", "html_url": "https://github.com/shuuki4", "followers_url": "https://api.github.com/users/shuuki4/followers", "following_url": "https://api.github.com/users/shuuki4/following{/other_user}", "gists_url": "https://api.github.com/users/shuuki4/gists{/gist_id}", "starred_url": "https://api.github.com/users/shuuki4/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shuuki4/subscriptions", "organizations_url": "https://api.github.com/users/shuuki4/orgs", "repos_url": "https://api.github.com/users/shuuki4/repos", "events_url": "https://api.github.com/users/shuuki4/events{/privacy}", "received_events_url": "https://api.github.com/users/shuuki4/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-30T09:45:10Z", "updated_at": "2017-06-02T00:52:02Z", "closed_at": "2017-06-02T00:52:02Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: From Source,</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.1.0-rc2-773-g7fa0cf3</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 8.0, cuDNN 5</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using <code>tf.nn.nce_loss</code> (to be precise, <code>_compute_sampled_logits</code> function with argument <code>subtract_log_q=True</code>) with <code>tf.nn.fixed_unigram_candidate_sampler(num_reserved_ids&gt;0)</code> + inputs with reserved ids gives NaN/inf logit output.</p>\n<p>The cause for this NaN seems to be the <code>true_expected_count</code> return value of <code>tf.nn.fixed_unigram_candidate_sampler</code> for ids in range <code>[0, num_reserved_ids)</code>, since sampler returns expected count 0.0 for these ids. When the <code>subtract_log_q</code> argument of <code>_compute_sampled_logits</code> is zero, log value of expected count for these ids become inf or NaN. I used reserved ids for UNK and PAD (since <code>nce_loss</code> does not support variable number of target classes yet), using these ids in input was inevitable.</p>\n<p>Possible solution would be adding/cliiping log input by small epsilon. Will there be any better solution?</p>\n<h3>Source code / logs</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nbatch_size = 3\nnum_true = 4\nnum_classes = 5\nnum_sampled = 5\nembed_dim = 5\n\ntrue_classes = tf.constant(\n    np.array(\n        [[3, 1, 2, 0],\n         [2, 0, 0, 0],\n         [2, 4, 3, 0]]),\n    dtype=tf.int64)\n\nsampled_values = tf.nn.fixed_unigram_candidate_sampler(\n    true_classes=true_classes,\n    num_true=num_true,\n    num_sampled=num_sampled,\n    unique=False,\n    range_max=num_classes,\n    num_reserved_ids=1,\n    unigrams=[10, 10, 10, 10]\n)\nsampled_ids, true_expected_count, sampled_expected_count = sampled_values\n\nloss = tf.reduce_mean(\n    tf.nn.nce_loss(\n        weights=tf.ones([num_classes, embed_dim], dtype=tf.float32),\n        biases=tf.zeros([num_classes], dtype=tf.float32),\n        labels=true_classes,\n        inputs=tf.ones([batch_size, embed_dim], dtype=tf.float32),\n        num_sampled=num_sampled,\n        num_classes=num_classes,\n        num_true=num_true,\n        sampled_values=sampled_values\n    )\n)\n\nwith tf.Session() as sess:\n    loss_value, true_count = sess.run([loss, true_expected_count])\n    print('Loss: {:.4f}'.format(loss_value))\n    print('Min True Count: {:.4f}'.format(np.amin(true_count)))\n\n&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;\nLoss: nan\nMin True Count: 0.0000\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 14.04\nTensorFlow installed from (source or binary): From Source,\nTensorFlow version (use command below): v1.1.0-rc2-773-g7fa0cf3\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: CUDA 8.0, cuDNN 5\n\nDescribe the problem\nUsing tf.nn.nce_loss (to be precise, _compute_sampled_logits function with argument subtract_log_q=True) with tf.nn.fixed_unigram_candidate_sampler(num_reserved_ids>0) + inputs with reserved ids gives NaN/inf logit output.\nThe cause for this NaN seems to be the true_expected_count return value of tf.nn.fixed_unigram_candidate_sampler for ids in range [0, num_reserved_ids), since sampler returns expected count 0.0 for these ids. When the subtract_log_q argument of _compute_sampled_logits is zero, log value of expected count for these ids become inf or NaN. I used reserved ids for UNK and PAD (since nce_loss does not support variable number of target classes yet), using these ids in input was inevitable.\nPossible solution would be adding/cliiping log input by small epsilon. Will there be any better solution?\nSource code / logs\nimport tensorflow as tf\nimport numpy as np\n\nbatch_size = 3\nnum_true = 4\nnum_classes = 5\nnum_sampled = 5\nembed_dim = 5\n\ntrue_classes = tf.constant(\n    np.array(\n        [[3, 1, 2, 0],\n         [2, 0, 0, 0],\n         [2, 4, 3, 0]]),\n    dtype=tf.int64)\n\nsampled_values = tf.nn.fixed_unigram_candidate_sampler(\n    true_classes=true_classes,\n    num_true=num_true,\n    num_sampled=num_sampled,\n    unique=False,\n    range_max=num_classes,\n    num_reserved_ids=1,\n    unigrams=[10, 10, 10, 10]\n)\nsampled_ids, true_expected_count, sampled_expected_count = sampled_values\n\nloss = tf.reduce_mean(\n    tf.nn.nce_loss(\n        weights=tf.ones([num_classes, embed_dim], dtype=tf.float32),\n        biases=tf.zeros([num_classes], dtype=tf.float32),\n        labels=true_classes,\n        inputs=tf.ones([batch_size, embed_dim], dtype=tf.float32),\n        num_sampled=num_sampled,\n        num_classes=num_classes,\n        num_true=num_true,\n        sampled_values=sampled_values\n    )\n)\n\nwith tf.Session() as sess:\n    loss_value, true_count = sess.run([loss, true_expected_count])\n    print('Loss: {:.4f}'.format(loss_value))\n    print('Min True Count: {:.4f}'.format(np.amin(true_count)))\n\n>>>>>>>>\nLoss: nan\nMin True Count: 0.0000", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: From Source,\r\n- **TensorFlow version (use command below)**: v1.1.0-rc2-773-g7fa0cf3\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: CUDA 8.0, cuDNN 5\r\n\r\n### Describe the problem\r\nUsing `tf.nn.nce_loss` (to be precise, `_compute_sampled_logits` function with argument `subtract_log_q=True`) with `tf.nn.fixed_unigram_candidate_sampler(num_reserved_ids>0)` + inputs with reserved ids gives NaN/inf logit output.\r\n\r\nThe cause for this NaN seems to be the `true_expected_count` return value of `tf.nn.fixed_unigram_candidate_sampler` for ids in range `[0, num_reserved_ids)`, since sampler returns expected count 0.0 for these ids. When the `subtract_log_q` argument of `_compute_sampled_logits` is zero, log value of expected count for these ids become inf or NaN. I used reserved ids for UNK and PAD (since `nce_loss` does not support variable number of target classes yet), using these ids in input was inevitable.\r\n\r\nPossible solution would be adding/cliiping log input by small epsilon. Will there be any better solution?\r\n\r\n### Source code / logs\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nbatch_size = 3\r\nnum_true = 4\r\nnum_classes = 5\r\nnum_sampled = 5\r\nembed_dim = 5\r\n\r\ntrue_classes = tf.constant(\r\n    np.array(\r\n        [[3, 1, 2, 0],\r\n         [2, 0, 0, 0],\r\n         [2, 4, 3, 0]]),\r\n    dtype=tf.int64)\r\n\r\nsampled_values = tf.nn.fixed_unigram_candidate_sampler(\r\n    true_classes=true_classes,\r\n    num_true=num_true,\r\n    num_sampled=num_sampled,\r\n    unique=False,\r\n    range_max=num_classes,\r\n    num_reserved_ids=1,\r\n    unigrams=[10, 10, 10, 10]\r\n)\r\nsampled_ids, true_expected_count, sampled_expected_count = sampled_values\r\n\r\nloss = tf.reduce_mean(\r\n    tf.nn.nce_loss(\r\n        weights=tf.ones([num_classes, embed_dim], dtype=tf.float32),\r\n        biases=tf.zeros([num_classes], dtype=tf.float32),\r\n        labels=true_classes,\r\n        inputs=tf.ones([batch_size, embed_dim], dtype=tf.float32),\r\n        num_sampled=num_sampled,\r\n        num_classes=num_classes,\r\n        num_true=num_true,\r\n        sampled_values=sampled_values\r\n    )\r\n)\r\n\r\nwith tf.Session() as sess:\r\n    loss_value, true_count = sess.run([loss, true_expected_count])\r\n    print('Loss: {:.4f}'.format(loss_value))\r\n    print('Min True Count: {:.4f}'.format(np.amin(true_count)))\r\n\r\n>>>>>>>>\r\nLoss: nan\r\nMin True Count: 0.0000\r\n```"}