{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289392416", "html_url": "https://github.com/tensorflow/tensorflow/issues/8746#issuecomment-289392416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8746", "id": 289392416, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTM5MjQxNg==", "user": {"login": "MicaelCarvalho", "id": 17184992, "node_id": "MDQ6VXNlcjE3MTg0OTky", "avatar_url": "https://avatars3.githubusercontent.com/u/17184992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MicaelCarvalho", "html_url": "https://github.com/MicaelCarvalho", "followers_url": "https://api.github.com/users/MicaelCarvalho/followers", "following_url": "https://api.github.com/users/MicaelCarvalho/following{/other_user}", "gists_url": "https://api.github.com/users/MicaelCarvalho/gists{/gist_id}", "starred_url": "https://api.github.com/users/MicaelCarvalho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MicaelCarvalho/subscriptions", "organizations_url": "https://api.github.com/users/MicaelCarvalho/orgs", "repos_url": "https://api.github.com/users/MicaelCarvalho/repos", "events_url": "https://api.github.com/users/MicaelCarvalho/events{/privacy}", "received_events_url": "https://api.github.com/users/MicaelCarvalho/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-27T08:52:07Z", "updated_at": "2017-03-27T08:54:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The problem isn't the Optimizer, it's your loss. It should return the mean loss, not the sum. If you're doing an L2 regression, for instance, it should look like this:</p>\n<pre><code>l_value = tf.pow(tf.abs(ground_truth - predict), 2) # distance for each individual position of the output matrix of shape = (n_examples, example_data_size)\nregression_loss = tf.reduce_sum(l_value, axis=1) # distance per example, shape = (n_examples, 1)\ntotal_regression_loss = tf.reduce_mean(regression_loss) # mean distance of all examples, shape = (1)\n</code></pre>\n<p>PS: <code>tf.abs</code> is used for convenience, so you can replace the L2 loss for another one (like L1) without having to worry about sign changes, which would yield results in the complex plane.</p>", "body_text": "The problem isn't the Optimizer, it's your loss. It should return the mean loss, not the sum. If you're doing an L2 regression, for instance, it should look like this:\nl_value = tf.pow(tf.abs(ground_truth - predict), 2) # distance for each individual position of the output matrix of shape = (n_examples, example_data_size)\nregression_loss = tf.reduce_sum(l_value, axis=1) # distance per example, shape = (n_examples, 1)\ntotal_regression_loss = tf.reduce_mean(regression_loss) # mean distance of all examples, shape = (1)\n\nPS: tf.abs is used for convenience, so you can replace the L2 loss for another one (like L1) without having to worry about sign changes, which would yield results in the complex plane.", "body": "The problem isn't the Optimizer, it's your loss. It should return the mean loss, not the sum. If you're doing an L2 regression, for instance, it should look like this:\r\n\r\n```\r\nl_value = tf.pow(tf.abs(ground_truth - predict), 2) # distance for each individual position of the output matrix of shape = (n_examples, example_data_size)\r\nregression_loss = tf.reduce_sum(l_value, axis=1) # distance per example, shape = (n_examples, 1)\r\ntotal_regression_loss = tf.reduce_mean(regression_loss) # mean distance of all examples, shape = (1)\r\n```\r\n\r\nPS: `tf.abs` is used for convenience, so you can replace the L2 loss for another one (like L1) without having to worry about sign changes, which would yield results in the complex plane."}