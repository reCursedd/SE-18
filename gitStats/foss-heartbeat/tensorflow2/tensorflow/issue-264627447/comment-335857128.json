{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335857128", "html_url": "https://github.com/tensorflow/tensorflow/issues/13636#issuecomment-335857128", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13636", "id": 335857128, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTg1NzEyOA==", "user": {"login": "DEKHTIARJonathan", "id": 10923599, "node_id": "MDQ6VXNlcjEwOTIzNTk5", "avatar_url": "https://avatars2.githubusercontent.com/u/10923599?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DEKHTIARJonathan", "html_url": "https://github.com/DEKHTIARJonathan", "followers_url": "https://api.github.com/users/DEKHTIARJonathan/followers", "following_url": "https://api.github.com/users/DEKHTIARJonathan/following{/other_user}", "gists_url": "https://api.github.com/users/DEKHTIARJonathan/gists{/gist_id}", "starred_url": "https://api.github.com/users/DEKHTIARJonathan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DEKHTIARJonathan/subscriptions", "organizations_url": "https://api.github.com/users/DEKHTIARJonathan/orgs", "repos_url": "https://api.github.com/users/DEKHTIARJonathan/repos", "events_url": "https://api.github.com/users/DEKHTIARJonathan/events{/privacy}", "received_events_url": "https://api.github.com/users/DEKHTIARJonathan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T15:51:40Z", "updated_at": "2017-10-11T15:52:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks to the work in Keras, I managed to design a workaround:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">int_shape</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">tuple</span>(x.get_shape().as_list())\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">ValueError</span>:\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">None</span>\n\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>])\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x_shape:<span class=\"pl-pds\">\"</span></span>, x.get_shape()) <span class=\"pl-c\"><span class=\"pl-c\">#</span> x_shape: (?, 32, 32, 1)</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>###################</span>\n\nscale_factor   <span class=\"pl-k\">=</span> (<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>)\n\noriginal_shape <span class=\"pl-k\">=</span> int_shape(x)\n\nnew_shape      <span class=\"pl-k\">=</span> tf.shape(x)[<span class=\"pl-c1\">1</span>:<span class=\"pl-c1\">3</span>]\nnew_shape      <span class=\"pl-k\">=</span> tf.multiply(new_shape, scale_factor) <span class=\"pl-c\"><span class=\"pl-c\">#</span> gives (64, 64)</span>\n\nresized_data <span class=\"pl-k\">=</span> tf.image.resize_nearest_neighbor(\n    <span class=\"pl-v\">images</span>        <span class=\"pl-k\">=</span> x,\n    <span class=\"pl-v\">size</span>          <span class=\"pl-k\">=</span> new_shape,\n    <span class=\"pl-v\">align_corners</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>,\n    <span class=\"pl-v\">name</span>          <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>resized_data:<span class=\"pl-pds\">\"</span></span>, resized_data.get_shape()) <span class=\"pl-c\"><span class=\"pl-c\">#</span> resized_data: (?, ?, ?, 1)</span>\n\n<span class=\"pl-c1\">print</span>(resized_data.shape)\n\nresized_data.set_shape(\n    (\n        <span class=\"pl-c1\">None</span>, \n        original_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> scale_factor[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">if</span> original_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>,\n        original_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> scale_factor[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">if</span> original_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> <span class=\"pl-c1\">None</span>, \n        <span class=\"pl-c1\">None</span>\n    )\n)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>################### NOW THE PROBLEMATIC #################</span>\n\nflatten_tensor <span class=\"pl-k\">=</span> tf.contrib.layers.flatten(<span class=\"pl-v\">inputs</span> <span class=\"pl-k\">=</span> resized_data)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>flatten_tensor:<span class=\"pl-pds\">\"</span></span>, flatten_tensor.get_shape()) <span class=\"pl-c\"><span class=\"pl-c\">#</span> resized_data: (?, ?)</span>\n\nfailing_layer <span class=\"pl-k\">=</span> tf.contrib.layers.fully_connected(\n    <span class=\"pl-v\">inputs</span>        <span class=\"pl-k\">=</span> flatten_tensor,\n    <span class=\"pl-v\">num_outputs</span>   <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>,\n    <span class=\"pl-v\">activation_fn</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span>\n)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>#############################################</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>############ LAUNCH THE SESSION #############</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>#############################################</span>\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    \n    tf.global_variables_initializer().run()\n    \n    rslt <span class=\"pl-k\">=</span> sess.run(resized_data, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: np.ones((<span class=\"pl-c1\">666</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">1</span>))})\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Rslt Shape:<span class=\"pl-pds\">\"</span></span>, rslt.shape)\n    \n    failing_layer <span class=\"pl-k\">=</span> sess.run(failing_layer, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{x: np.ones((<span class=\"pl-c1\">666</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">1</span>))})\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>failing_layer Shape:<span class=\"pl-pds\">\"</span></span>, failing_layer.shape)</pre></div>\n<p>Do you think, a PR should fix this ? Or is it maybe normal behavior ...</p>", "body_text": "Thanks to the work in Keras, I managed to design a workaround:\nimport numpy as np\nimport tensorflow as tf\n\ndef int_shape(x):\n    try:\n        return tuple(x.get_shape().as_list())\n    except ValueError:\n        return None\n\nx = tf.placeholder(tf.float32, [None, 32, 32, 1])\n\nprint(\"x_shape:\", x.get_shape()) # x_shape: (?, 32, 32, 1)\n\n####################\n\nscale_factor   = (2, 2)\n\noriginal_shape = int_shape(x)\n\nnew_shape      = tf.shape(x)[1:3]\nnew_shape      = tf.multiply(new_shape, scale_factor) # gives (64, 64)\n\nresized_data = tf.image.resize_nearest_neighbor(\n    images        = x,\n    size          = new_shape,\n    align_corners = None,\n    name          = None\n)\n\nprint(\"resized_data:\", resized_data.get_shape()) # resized_data: (?, ?, ?, 1)\n\nprint(resized_data.shape)\n\nresized_data.set_shape(\n    (\n        None, \n        original_shape[1] * scale_factor[0] if original_shape[1] is not None else None,\n        original_shape[2] * scale_factor[1] if original_shape[2] is not None else None, \n        None\n    )\n)\n\n#################### NOW THE PROBLEMATIC #################\n\nflatten_tensor = tf.contrib.layers.flatten(inputs = resized_data)\nprint(\"flatten_tensor:\", flatten_tensor.get_shape()) # resized_data: (?, ?)\n\nfailing_layer = tf.contrib.layers.fully_connected(\n    inputs        = flatten_tensor,\n    num_outputs   = 1,\n    activation_fn = None\n)\n\n##############################################\n############# LAUNCH THE SESSION #############\n##############################################\n\nwith tf.Session() as sess:\n    \n    tf.global_variables_initializer().run()\n    \n    rslt = sess.run(resized_data, feed_dict={x: np.ones((666, 32, 32,1))})\n    print(\"Rslt Shape:\", rslt.shape)\n    \n    failing_layer = sess.run(failing_layer, feed_dict={x: np.ones((666, 32, 32,1))})\n    print(\"failing_layer Shape:\", failing_layer.shape)\nDo you think, a PR should fix this ? Or is it maybe normal behavior ...", "body": "Thanks to the work in Keras, I managed to design a workaround:\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef int_shape(x):\r\n    try:\r\n        return tuple(x.get_shape().as_list())\r\n    except ValueError:\r\n        return None\r\n\r\nx = tf.placeholder(tf.float32, [None, 32, 32, 1])\r\n\r\nprint(\"x_shape:\", x.get_shape()) # x_shape: (?, 32, 32, 1)\r\n\r\n####################\r\n\r\nscale_factor   = (2, 2)\r\n\r\noriginal_shape = int_shape(x)\r\n\r\nnew_shape      = tf.shape(x)[1:3]\r\nnew_shape      = tf.multiply(new_shape, scale_factor) # gives (64, 64)\r\n\r\nresized_data = tf.image.resize_nearest_neighbor(\r\n    images        = x,\r\n    size          = new_shape,\r\n    align_corners = None,\r\n    name          = None\r\n)\r\n\r\nprint(\"resized_data:\", resized_data.get_shape()) # resized_data: (?, ?, ?, 1)\r\n\r\nprint(resized_data.shape)\r\n\r\nresized_data.set_shape(\r\n    (\r\n        None, \r\n        original_shape[1] * scale_factor[0] if original_shape[1] is not None else None,\r\n        original_shape[2] * scale_factor[1] if original_shape[2] is not None else None, \r\n        None\r\n    )\r\n)\r\n\r\n#################### NOW THE PROBLEMATIC #################\r\n\r\nflatten_tensor = tf.contrib.layers.flatten(inputs = resized_data)\r\nprint(\"flatten_tensor:\", flatten_tensor.get_shape()) # resized_data: (?, ?)\r\n\r\nfailing_layer = tf.contrib.layers.fully_connected(\r\n    inputs        = flatten_tensor,\r\n    num_outputs   = 1,\r\n    activation_fn = None\r\n)\r\n\r\n##############################################\r\n############# LAUNCH THE SESSION #############\r\n##############################################\r\n\r\nwith tf.Session() as sess:\r\n    \r\n    tf.global_variables_initializer().run()\r\n    \r\n    rslt = sess.run(resized_data, feed_dict={x: np.ones((666, 32, 32,1))})\r\n    print(\"Rslt Shape:\", rslt.shape)\r\n    \r\n    failing_layer = sess.run(failing_layer, feed_dict={x: np.ones((666, 32, 32,1))})\r\n    print(\"failing_layer Shape:\", failing_layer.shape)\r\n```\r\n\r\nDo you think, a PR should fix this ? Or is it maybe normal behavior ..."}