{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320933893", "html_url": "https://github.com/tensorflow/tensorflow/issues/12071#issuecomment-320933893", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12071", "id": 320933893, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDkzMzg5Mw==", "user": {"login": "oduerr", "id": 4604020, "node_id": "MDQ6VXNlcjQ2MDQwMjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/4604020?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oduerr", "html_url": "https://github.com/oduerr", "followers_url": "https://api.github.com/users/oduerr/followers", "following_url": "https://api.github.com/users/oduerr/following{/other_user}", "gists_url": "https://api.github.com/users/oduerr/gists{/gist_id}", "starred_url": "https://api.github.com/users/oduerr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oduerr/subscriptions", "organizations_url": "https://api.github.com/users/oduerr/orgs", "repos_url": "https://api.github.com/users/oduerr/repos", "events_url": "https://api.github.com/users/oduerr/events{/privacy}", "received_events_url": "https://api.github.com/users/oduerr/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-08T11:57:16Z", "updated_at": "2017-08-08T11:57:16Z", "author_association": "NONE", "body_html": "<p>First <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> thanks a lot for code for the stable version of the gradient (I did not know that it's possible to overwrite the gradient calculation).</p>\n<p>I agree that the corner case 0 is not really defined but the same is true for the gradient of a ReLU at 0. For <code>float32</code> exact 0 is not a singular event, you might get it by rounding errors and then the whole optimization is broken. Furthermore (even more problematic) there are instabilities in the gradient of <code>tf.norm</code> for small values, e.g the <code>Inf</code> for e-19, which you could encounter in an optimization process containing the norm.  In my case part of the loss function was <code>exp(-norm(x-y))</code> (it took quite some time to nail down the problem to the gradient of the tf.norm()). Therefore, I think this is a real bug and not just a pure mathematical problem and deserves fixing.</p>\n<p>I did not include this in my original report, but the problem is also present for non-scalars. See</p>\n<pre><code>X = tf.placeholder(tf.float32, shape=(1,3))\nZ = tf.norm(X, ord='euclidean', axis=1, name='logit')\npik = tf.nn.softmax(logits=Z)\nres = tf.reduce_sum(pik)\nvar_grad = tf.gradients(res, [X])\n\nwith tf.Session() as sess:\n    X_ = np.array([\n        [1e-19, 1e-19, 0]\n    ], dtype=np.float32)\n\n    sess.run(tf.global_variables_initializer())\n    print(sess.run((res, var_grad), feed_dict={X:X_}))\n</code></pre>\n<p>which gives <code>(nan, nan, nan)</code>.</p>", "body_text": "First @yaroslavvb thanks a lot for code for the stable version of the gradient (I did not know that it's possible to overwrite the gradient calculation).\nI agree that the corner case 0 is not really defined but the same is true for the gradient of a ReLU at 0. For float32 exact 0 is not a singular event, you might get it by rounding errors and then the whole optimization is broken. Furthermore (even more problematic) there are instabilities in the gradient of tf.norm for small values, e.g the Inf for e-19, which you could encounter in an optimization process containing the norm.  In my case part of the loss function was exp(-norm(x-y)) (it took quite some time to nail down the problem to the gradient of the tf.norm()). Therefore, I think this is a real bug and not just a pure mathematical problem and deserves fixing.\nI did not include this in my original report, but the problem is also present for non-scalars. See\nX = tf.placeholder(tf.float32, shape=(1,3))\nZ = tf.norm(X, ord='euclidean', axis=1, name='logit')\npik = tf.nn.softmax(logits=Z)\nres = tf.reduce_sum(pik)\nvar_grad = tf.gradients(res, [X])\n\nwith tf.Session() as sess:\n    X_ = np.array([\n        [1e-19, 1e-19, 0]\n    ], dtype=np.float32)\n\n    sess.run(tf.global_variables_initializer())\n    print(sess.run((res, var_grad), feed_dict={X:X_}))\n\nwhich gives (nan, nan, nan).", "body": "First @yaroslavvb thanks a lot for code for the stable version of the gradient (I did not know that it's possible to overwrite the gradient calculation).\r\n\r\nI agree that the corner case 0 is not really defined but the same is true for the gradient of a ReLU at 0. For `float32` exact 0 is not a singular event, you might get it by rounding errors and then the whole optimization is broken. Furthermore (even more problematic) there are instabilities in the gradient of `tf.norm` for small values, e.g the `Inf` for e-19, which you could encounter in an optimization process containing the norm.  In my case part of the loss function was `exp(-norm(x-y))` (it took quite some time to nail down the problem to the gradient of the tf.norm()). Therefore, I think this is a real bug and not just a pure mathematical problem and deserves fixing.\r\n\r\nI did not include this in my original report, but the problem is also present for non-scalars. See\r\n```\r\nX = tf.placeholder(tf.float32, shape=(1,3))\r\nZ = tf.norm(X, ord='euclidean', axis=1, name='logit')\r\npik = tf.nn.softmax(logits=Z)\r\nres = tf.reduce_sum(pik)\r\nvar_grad = tf.gradients(res, [X])\r\n\r\nwith tf.Session() as sess:\r\n    X_ = np.array([\r\n        [1e-19, 1e-19, 0]\r\n    ], dtype=np.float32)\r\n\r\n    sess.run(tf.global_variables_initializer())\r\n    print(sess.run((res, var_grad), feed_dict={X:X_}))\r\n```\r\nwhich gives `(nan, nan, nan)`. \r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n"}