{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320797203", "html_url": "https://github.com/tensorflow/tensorflow/issues/12071#issuecomment-320797203", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12071", "id": 320797203, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDc5NzIwMw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-07T22:23:49Z", "updated_at": "2017-08-07T22:48:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thinking philosophically, the general problem is that computational graph ends up with things like<code>a/a</code> where <code>a</code> is 0. Numerically it's undefined, but the limit exists. Similar issue exists with gradient of tf.select (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"157280822\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2540\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2540/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2540\">#2540</a>) and gradient of <code>tf.exp(-tf.exp(x))</code></p>\n<p>You have to do some algebraic massaging to get numerically defined result.</p>\n<p>In your particular case, you could replace automatic gradient with a stable version:</p>\n<pre><code>from tensorflow.python.framework import function\nimport numpy as np\nimport tensorflow as tf\n\n@function.Defun(tf.float32, tf.float32)\ndef norm_grad(x, dy):\n    return dy*(x/tf.norm(x))\n\n@function.Defun(tf.float32, grad_func=norm_grad)\ndef norm(x):\n    return tf.norm(x)\n\nsess = tf.InteractiveSession()\nX = tf.placeholder(tf.float32, shape=(4,None))\nX_ = np.array([\n    [1],  # Grad OK\n    [0],  # Grad NaN\n    [1e-16],  # Grad OK\n    [1e-19] #Grad Inf\n], dtype=np.float32)\nZ = norm(X)\nvar_grad = tf.gradients(Z, [X])\nprint(sess.run((Z, var_grad), feed_dict={X: X_}))\n\n#1.0, [array([[  1.00000000e+00],\n#      [  0.00000000e+00],\n#     [  1.00000002e-16],\n#    [  9.99999968e-20]], dtype=float32)])\n\n\n</code></pre>", "body_text": "Thinking philosophically, the general problem is that computational graph ends up with things likea/a where a is 0. Numerically it's undefined, but the limit exists. Similar issue exists with gradient of tf.select (#2540) and gradient of tf.exp(-tf.exp(x))\nYou have to do some algebraic massaging to get numerically defined result.\nIn your particular case, you could replace automatic gradient with a stable version:\nfrom tensorflow.python.framework import function\nimport numpy as np\nimport tensorflow as tf\n\n@function.Defun(tf.float32, tf.float32)\ndef norm_grad(x, dy):\n    return dy*(x/tf.norm(x))\n\n@function.Defun(tf.float32, grad_func=norm_grad)\ndef norm(x):\n    return tf.norm(x)\n\nsess = tf.InteractiveSession()\nX = tf.placeholder(tf.float32, shape=(4,None))\nX_ = np.array([\n    [1],  # Grad OK\n    [0],  # Grad NaN\n    [1e-16],  # Grad OK\n    [1e-19] #Grad Inf\n], dtype=np.float32)\nZ = norm(X)\nvar_grad = tf.gradients(Z, [X])\nprint(sess.run((Z, var_grad), feed_dict={X: X_}))\n\n#1.0, [array([[  1.00000000e+00],\n#      [  0.00000000e+00],\n#     [  1.00000002e-16],\n#    [  9.99999968e-20]], dtype=float32)])", "body": "Thinking philosophically, the general problem is that computational graph ends up with things like`a/a` where `a` is 0. Numerically it's undefined, but the limit exists. Similar issue exists with gradient of tf.select (https://github.com/tensorflow/tensorflow/issues/2540) and gradient of `tf.exp(-tf.exp(x))`\r\n\r\nYou have to do some algebraic massaging to get numerically defined result.\r\n\r\nIn your particular case, you could replace automatic gradient with a stable version:\r\n\r\n```\r\nfrom tensorflow.python.framework import function\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n@function.Defun(tf.float32, tf.float32)\r\ndef norm_grad(x, dy):\r\n    return dy*(x/tf.norm(x))\r\n\r\n@function.Defun(tf.float32, grad_func=norm_grad)\r\ndef norm(x):\r\n    return tf.norm(x)\r\n\r\nsess = tf.InteractiveSession()\r\nX = tf.placeholder(tf.float32, shape=(4,None))\r\nX_ = np.array([\r\n    [1],  # Grad OK\r\n    [0],  # Grad NaN\r\n    [1e-16],  # Grad OK\r\n    [1e-19] #Grad Inf\r\n], dtype=np.float32)\r\nZ = norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\nprint(sess.run((Z, var_grad), feed_dict={X: X_}))\r\n\r\n#1.0, [array([[  1.00000000e+00],\r\n#      [  0.00000000e+00],\r\n#     [  1.00000002e-16],\r\n#    [  9.99999968e-20]], dtype=float32)])\r\n\r\n\r\n```"}