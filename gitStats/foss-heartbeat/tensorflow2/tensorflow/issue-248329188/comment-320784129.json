{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320784129", "html_url": "https://github.com/tensorflow/tensorflow/issues/12071#issuecomment-320784129", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12071", "id": 320784129, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDc4NDEyOQ==", "user": {"login": "goodfeli", "id": 387866, "node_id": "MDQ6VXNlcjM4Nzg2Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/387866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goodfeli", "html_url": "https://github.com/goodfeli", "followers_url": "https://api.github.com/users/goodfeli/followers", "following_url": "https://api.github.com/users/goodfeli/following{/other_user}", "gists_url": "https://api.github.com/users/goodfeli/gists{/gist_id}", "starred_url": "https://api.github.com/users/goodfeli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goodfeli/subscriptions", "organizations_url": "https://api.github.com/users/goodfeli/orgs", "repos_url": "https://api.github.com/users/goodfeli/repos", "events_url": "https://api.github.com/users/goodfeli/events{/privacy}", "received_events_url": "https://api.github.com/users/goodfeli/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-07T21:20:54Z", "updated_at": "2017-08-07T21:20:54Z", "author_association": "NONE", "body_html": "<p>Theano gives NaN as the gradient of the norm of a vector with zero norm:</p>\n<pre><code>&gt;&gt;&gt; x = theano.tensor.vector()\n&gt;&gt;&gt; y = theano.tensor.square(x)\n&gt;&gt;&gt; z = y.sum()\n&gt;&gt;&gt; norm = theano.tensor.sqrt(z)\n&gt;&gt;&gt; d = theano.tensor.grad(norm, x)\n&gt;&gt;&gt; d.eval({x: [0., 0.]})\narray([ nan,  nan])\n</code></pre>\n<p>Theano does give 0 as the gradient of the norm of a <em>scalar</em>. I think for this it is probably using a patternsub to turn sqrt(square(x)) into abs(x). Note that in Theano's conventions, the derivative of abs(x) at 0 is treated as 0 rather than undefined.</p>", "body_text": "Theano gives NaN as the gradient of the norm of a vector with zero norm:\n>>> x = theano.tensor.vector()\n>>> y = theano.tensor.square(x)\n>>> z = y.sum()\n>>> norm = theano.tensor.sqrt(z)\n>>> d = theano.tensor.grad(norm, x)\n>>> d.eval({x: [0., 0.]})\narray([ nan,  nan])\n\nTheano does give 0 as the gradient of the norm of a scalar. I think for this it is probably using a patternsub to turn sqrt(square(x)) into abs(x). Note that in Theano's conventions, the derivative of abs(x) at 0 is treated as 0 rather than undefined.", "body": "Theano gives NaN as the gradient of the norm of a vector with zero norm:\r\n\r\n    >>> x = theano.tensor.vector()\r\n    >>> y = theano.tensor.square(x)\r\n    >>> z = y.sum()\r\n    >>> norm = theano.tensor.sqrt(z)\r\n    >>> d = theano.tensor.grad(norm, x)\r\n    >>> d.eval({x: [0., 0.]})\r\n    array([ nan,  nan])\r\n\r\nTheano does give 0 as the gradient of the norm of a *scalar*. I think for this it is probably using a patternsub to turn sqrt(square(x)) into abs(x). Note that in Theano's conventions, the derivative of abs(x) at 0 is treated as 0 rather than undefined."}