{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422788458", "html_url": "https://github.com/tensorflow/tensorflow/issues/12071#issuecomment-422788458", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12071", "id": 422788458, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjc4ODQ1OA==", "user": {"login": "dust0x", "id": 6918743, "node_id": "MDQ6VXNlcjY5MTg3NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/6918743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dust0x", "html_url": "https://github.com/dust0x", "followers_url": "https://api.github.com/users/dust0x/followers", "following_url": "https://api.github.com/users/dust0x/following{/other_user}", "gists_url": "https://api.github.com/users/dust0x/gists{/gist_id}", "starred_url": "https://api.github.com/users/dust0x/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dust0x/subscriptions", "organizations_url": "https://api.github.com/users/dust0x/orgs", "repos_url": "https://api.github.com/users/dust0x/repos", "events_url": "https://api.github.com/users/dust0x/events{/privacy}", "received_events_url": "https://api.github.com/users/dust0x/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-19T12:44:52Z", "updated_at": "2018-09-19T12:57:20Z", "author_association": "NONE", "body_html": "<p>Thanks, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7176092\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gsutanto\">@gsutanto</a> for your solution. I wanted to ask what the correct way to force calculate gradients is when there is a complicated interaction of variables, such as when we're using gradient descent to optimize a loss function term with the norm as penalty.</p>\n<p>Do I have to force the gradient of the entire loss function or should only the gradient of norm be computed separately?</p>\n<p>Drawing from your code, what I'm saying is something like in the case below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">...</span>\nX <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">1</span>))\nY <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\nW <span class=\"pl-k\">=</span> tf.Variable(tf.zeros(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>]))\n\nZ <span class=\"pl-k\">=</span>  norm(X)\nvar_grad <span class=\"pl-k\">=</span> tf.gradients(Z, [X])\n\nJ <span class=\"pl-k\">=</span> tf.reduce_mean((Y <span class=\"pl-k\">-</span> tf.matmul(W, X))) <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">.5</span> <span class=\"pl-k\">*</span> Z)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    X_ <span class=\"pl-k\">=</span> np.array([\n        [<span class=\"pl-c1\">0</span>],\n        [<span class=\"pl-c1\">0</span>],\n        [<span class=\"pl-c1\">0</span>],\n        [<span class=\"pl-c1\">0</span>]\n    ], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n    Y_ <span class=\"pl-k\">=</span> np.array([\n        [<span class=\"pl-c1\">1</span>]\n    ], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\n    \n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">1</span>).minimize(J)                                                   \n    sess.run([optimizer, J, var_grad], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{X: X_, Y: Y_})</pre></div>", "body_text": "Thanks, @gsutanto for your solution. I wanted to ask what the correct way to force calculate gradients is when there is a complicated interaction of variables, such as when we're using gradient descent to optimize a loss function term with the norm as penalty.\nDo I have to force the gradient of the entire loss function or should only the gradient of norm be computed separately?\nDrawing from your code, what I'm saying is something like in the case below:\n...\nX = tf.placeholder(tf.float32, shape=(4, 1))\nY = tf.placeholder(tf.float32, shape=(1, 1))\nW = tf.Variable(tf.zeros(shape=[1, 4]))\n\nZ =  norm(X)\nvar_grad = tf.gradients(Z, [X])\n\nJ = tf.reduce_mean((Y - tf.matmul(W, X))) + (.5 * Z)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    X_ = np.array([\n        [0],\n        [0],\n        [0],\n        [0]\n    ], dtype=np.float32)\n    Y_ = np.array([\n        [1]\n    ], dtype=np.float32)\n    \n    optimizer = tf.train.GradientDescentOptimizer(1).minimize(J)                                                   \n    sess.run([optimizer, J, var_grad], feed_dict={X: X_, Y: Y_})", "body": "Thanks, @gsutanto for your solution. I wanted to ask what the correct way to force calculate gradients is when there is a complicated interaction of variables, such as when we're using gradient descent to optimize a loss function term with the norm as penalty.\r\n\r\nDo I have to force the gradient of the entire loss function or should only the gradient of norm be computed separately?\r\n\r\nDrawing from your code, what I'm saying is something like in the case below:\r\n\r\n```python\r\n...\r\nX = tf.placeholder(tf.float32, shape=(4, 1))\r\nY = tf.placeholder(tf.float32, shape=(1, 1))\r\nW = tf.Variable(tf.zeros(shape=[1, 4]))\r\n\r\nZ =  norm(X)\r\nvar_grad = tf.gradients(Z, [X])\r\n\r\nJ = tf.reduce_mean((Y - tf.matmul(W, X))) + (.5 * Z)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    X_ = np.array([\r\n        [0],\r\n        [0],\r\n        [0],\r\n        [0]\r\n    ], dtype=np.float32)\r\n    Y_ = np.array([\r\n        [1]\r\n    ], dtype=np.float32)\r\n    \r\n    optimizer = tf.train.GradientDescentOptimizer(1).minimize(J)                                                   \r\n    sess.run([optimizer, J, var_grad], feed_dict={X: X_, Y: Y_})\r\n```"}