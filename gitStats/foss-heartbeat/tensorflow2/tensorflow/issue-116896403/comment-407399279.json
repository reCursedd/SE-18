{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407399279", "html_url": "https://github.com/tensorflow/tensorflow/issues/216#issuecomment-407399279", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/216", "id": 407399279, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzM5OTI3OQ==", "user": {"login": "GSPP", "id": 12032350, "node_id": "MDQ6VXNlcjEyMDMyMzUw", "avatar_url": "https://avatars0.githubusercontent.com/u/12032350?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GSPP", "html_url": "https://github.com/GSPP", "followers_url": "https://api.github.com/users/GSPP/followers", "following_url": "https://api.github.com/users/GSPP/following{/other_user}", "gists_url": "https://api.github.com/users/GSPP/gists{/gist_id}", "starred_url": "https://api.github.com/users/GSPP/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GSPP/subscriptions", "organizations_url": "https://api.github.com/users/GSPP/orgs", "repos_url": "https://api.github.com/users/GSPP/repos", "events_url": "https://api.github.com/users/GSPP/events{/privacy}", "received_events_url": "https://api.github.com/users/GSPP/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-24T13:05:39Z", "updated_at": "2018-07-24T13:09:59Z", "author_association": "NONE", "body_html": "<p>I understand that any fully connected NN layer requires matmul. Also, if you want performance then you <em>need</em> batching. So does this not mean that pretty much <em>any</em> neural network implementation has a need for batched matmul? What's the recommended way to do this? Many solutions are proposed here or on the web but they all seem to rely on copying lots of data (tile, concat, squeeze, arrange, reshape) causing loss of performance.</p>\n<p>Tiling causes another specific problem for me: With large batch sizes this created very high memory usage for intermediate tensors.</p>", "body_text": "I understand that any fully connected NN layer requires matmul. Also, if you want performance then you need batching. So does this not mean that pretty much any neural network implementation has a need for batched matmul? What's the recommended way to do this? Many solutions are proposed here or on the web but they all seem to rely on copying lots of data (tile, concat, squeeze, arrange, reshape) causing loss of performance.\nTiling causes another specific problem for me: With large batch sizes this created very high memory usage for intermediate tensors.", "body": "I understand that any fully connected NN layer requires matmul. Also, if you want performance then you *need* batching. So does this not mean that pretty much *any* neural network implementation has a need for batched matmul? What's the recommended way to do this? Many solutions are proposed here or on the web but they all seem to rely on copying lots of data (tile, concat, squeeze, arrange, reshape) causing loss of performance.\r\n\r\nTiling causes another specific problem for me: With large batch sizes this created very high memory usage for intermediate tensors."}