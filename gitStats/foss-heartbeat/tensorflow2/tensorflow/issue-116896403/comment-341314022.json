{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/341314022", "html_url": "https://github.com/tensorflow/tensorflow/issues/216#issuecomment-341314022", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/216", "id": 341314022, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MTMxNDAyMg==", "user": {"login": "TheButlah", "id": 6969415, "node_id": "MDQ6VXNlcjY5Njk0MTU=", "avatar_url": "https://avatars1.githubusercontent.com/u/6969415?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TheButlah", "html_url": "https://github.com/TheButlah", "followers_url": "https://api.github.com/users/TheButlah/followers", "following_url": "https://api.github.com/users/TheButlah/following{/other_user}", "gists_url": "https://api.github.com/users/TheButlah/gists{/gist_id}", "starred_url": "https://api.github.com/users/TheButlah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TheButlah/subscriptions", "organizations_url": "https://api.github.com/users/TheButlah/orgs", "repos_url": "https://api.github.com/users/TheButlah/repos", "events_url": "https://api.github.com/users/TheButlah/events{/privacy}", "received_events_url": "https://api.github.com/users/TheButlah/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-02T04:22:00Z", "updated_at": "2017-11-02T04:25:33Z", "author_association": "NONE", "body_html": "<p>I would also like <code>tf.matmul</code> to broadcast properly. It becomes a problem when trying to add a fully connected or softmax layer in an RNN:</p>\n<pre><code>outputs = tf.stack(outputs, axis=1, name='Outputs')  # Shape is [batch, sequence, rnn_cell.output_size]\nwith tf.name_scope('Softmax'):\n    w = tf.get_variable(name='Weight', initializer=tf.truncated_normal([rnn_cell.output_size, embedding_size], stddev=0.01))\n    b = tf.get_variable(name='Bias', initializer=tf.zeros(embedding_size))\n\n    scores = tf.matmul(outputs, w) + b  # Fails with ValueError: Shape must be rank 2 but is rank 3 for 'Softmax/MatMul' (op: 'MatMul') with input shapes: [?,50,256], [256,2569].\n\n    softmax = tf.nn.softmax(scores, name='Softmax')\n</code></pre>\n<p>Ideally it would broadcast and produce a tensor of shape [?,50,2569]</p>", "body_text": "I would also like tf.matmul to broadcast properly. It becomes a problem when trying to add a fully connected or softmax layer in an RNN:\noutputs = tf.stack(outputs, axis=1, name='Outputs')  # Shape is [batch, sequence, rnn_cell.output_size]\nwith tf.name_scope('Softmax'):\n    w = tf.get_variable(name='Weight', initializer=tf.truncated_normal([rnn_cell.output_size, embedding_size], stddev=0.01))\n    b = tf.get_variable(name='Bias', initializer=tf.zeros(embedding_size))\n\n    scores = tf.matmul(outputs, w) + b  # Fails with ValueError: Shape must be rank 2 but is rank 3 for 'Softmax/MatMul' (op: 'MatMul') with input shapes: [?,50,256], [256,2569].\n\n    softmax = tf.nn.softmax(scores, name='Softmax')\n\nIdeally it would broadcast and produce a tensor of shape [?,50,2569]", "body": "I would also like `tf.matmul` to broadcast properly. It becomes a problem when trying to add a fully connected or softmax layer in an RNN:\r\n\r\n```\r\noutputs = tf.stack(outputs, axis=1, name='Outputs')  # Shape is [batch, sequence, rnn_cell.output_size]\r\nwith tf.name_scope('Softmax'):\r\n    w = tf.get_variable(name='Weight', initializer=tf.truncated_normal([rnn_cell.output_size, embedding_size], stddev=0.01))\r\n    b = tf.get_variable(name='Bias', initializer=tf.zeros(embedding_size))\r\n\r\n    scores = tf.matmul(outputs, w) + b  # Fails with ValueError: Shape must be rank 2 but is rank 3 for 'Softmax/MatMul' (op: 'MatMul') with input shapes: [?,50,256], [256,2569].\r\n\r\n    softmax = tf.nn.softmax(scores, name='Softmax')\r\n```\r\n\r\nIdeally it would broadcast and produce a tensor of shape [?,50,2569]"}