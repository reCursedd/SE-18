{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/222538099", "html_url": "https://github.com/tensorflow/tensorflow/issues/216#issuecomment-222538099", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/216", "id": 222538099, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMjUzODA5OQ==", "user": {"login": "vladfi1", "id": 691536, "node_id": "MDQ6VXNlcjY5MTUzNg==", "avatar_url": "https://avatars0.githubusercontent.com/u/691536?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vladfi1", "html_url": "https://github.com/vladfi1", "followers_url": "https://api.github.com/users/vladfi1/followers", "following_url": "https://api.github.com/users/vladfi1/following{/other_user}", "gists_url": "https://api.github.com/users/vladfi1/gists{/gist_id}", "starred_url": "https://api.github.com/users/vladfi1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vladfi1/subscriptions", "organizations_url": "https://api.github.com/users/vladfi1/orgs", "repos_url": "https://api.github.com/users/vladfi1/repos", "events_url": "https://api.github.com/users/vladfi1/events{/privacy}", "received_events_url": "https://api.github.com/users/vladfi1/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-30T18:27:32Z", "updated_at": "2016-05-30T18:27:32Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I was going to open my own issue on this, but it seems that this one is being used for everything related to batch matrix-vector ops, <code>numpy.dot</code>, etc. For those looking for a workaround, I have two implementations of batch vector x matrix multiplication:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_vm</span>(<span class=\"pl-smi\">v</span>, <span class=\"pl-smi\">m</span>):\n  shape <span class=\"pl-k\">=</span> tf.shape(v)\n  rank <span class=\"pl-k\">=</span> shape.get_shape()[<span class=\"pl-c1\">0</span>].value\n  v <span class=\"pl-k\">=</span> tf.expand_dims(v, rank)\n\n  vm <span class=\"pl-k\">=</span> tf.mul(v, m)\n\n  <span class=\"pl-k\">return</span> tf.reduce_sum(vm, rank<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_vm2</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">m</span>):\n  [input_size, output_size] <span class=\"pl-k\">=</span> m.get_shape().as_list()\n\n  input_shape <span class=\"pl-k\">=</span> tf.shape(x)\n  batch_rank <span class=\"pl-k\">=</span> input_shape.get_shape()[<span class=\"pl-c1\">0</span>].value <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n  batch_shape <span class=\"pl-k\">=</span> input_shape[:batch_rank]\n  output_shape <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">0</span>, [batch_shape, [output_size]])\n\n  x <span class=\"pl-k\">=</span> tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, input_size])\n  y <span class=\"pl-k\">=</span> tf.matmul(x, m)\n\n  y <span class=\"pl-k\">=</span> tf.reshape(y, output_shape)\n\n  <span class=\"pl-k\">return</span> y</pre></div>\n<p>The first is based on the broadcasting behavior of <code>tf.mul</code>, while the second relies on reshaping all but the last dimension of the input to reduce the operation down to matrix-matrix multiplication. I suspect the second method is more efficient, but it might rely on the tensors being in the right memory order (although I think tf, unlike numpy, only uses C order).</p>", "body_text": "I was going to open my own issue on this, but it seems that this one is being used for everything related to batch matrix-vector ops, numpy.dot, etc. For those looking for a workaround, I have two implementations of batch vector x matrix multiplication:\ndef batch_vm(v, m):\n  shape = tf.shape(v)\n  rank = shape.get_shape()[0].value\n  v = tf.expand_dims(v, rank)\n\n  vm = tf.mul(v, m)\n\n  return tf.reduce_sum(vm, rank-1)\n\ndef batch_vm2(x, m):\n  [input_size, output_size] = m.get_shape().as_list()\n\n  input_shape = tf.shape(x)\n  batch_rank = input_shape.get_shape()[0].value - 1\n  batch_shape = input_shape[:batch_rank]\n  output_shape = tf.concat(0, [batch_shape, [output_size]])\n\n  x = tf.reshape(x, [-1, input_size])\n  y = tf.matmul(x, m)\n\n  y = tf.reshape(y, output_shape)\n\n  return y\nThe first is based on the broadcasting behavior of tf.mul, while the second relies on reshaping all but the last dimension of the input to reduce the operation down to matrix-matrix multiplication. I suspect the second method is more efficient, but it might rely on the tensors being in the right memory order (although I think tf, unlike numpy, only uses C order).", "body": "I was going to open my own issue on this, but it seems that this one is being used for everything related to batch matrix-vector ops, `numpy.dot`, etc. For those looking for a workaround, I have two implementations of batch vector x matrix multiplication:\n\n``` python\ndef batch_vm(v, m):\n  shape = tf.shape(v)\n  rank = shape.get_shape()[0].value\n  v = tf.expand_dims(v, rank)\n\n  vm = tf.mul(v, m)\n\n  return tf.reduce_sum(vm, rank-1)\n\ndef batch_vm2(x, m):\n  [input_size, output_size] = m.get_shape().as_list()\n\n  input_shape = tf.shape(x)\n  batch_rank = input_shape.get_shape()[0].value - 1\n  batch_shape = input_shape[:batch_rank]\n  output_shape = tf.concat(0, [batch_shape, [output_size]])\n\n  x = tf.reshape(x, [-1, input_size])\n  y = tf.matmul(x, m)\n\n  y = tf.reshape(y, output_shape)\n\n  return y\n```\n\nThe first is based on the broadcasting behavior of `tf.mul`, while the second relies on reshaping all but the last dimension of the input to reduce the operation down to matrix-matrix multiplication. I suspect the second method is more efficient, but it might rely on the tensors being in the right memory order (although I think tf, unlike numpy, only uses C order).\n"}