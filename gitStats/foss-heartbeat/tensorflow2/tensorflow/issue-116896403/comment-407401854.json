{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407401854", "html_url": "https://github.com/tensorflow/tensorflow/issues/216#issuecomment-407401854", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/216", "id": 407401854, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzQwMTg1NA==", "user": {"login": "rhaps0dy", "id": 4928242, "node_id": "MDQ6VXNlcjQ5MjgyNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4928242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rhaps0dy", "html_url": "https://github.com/rhaps0dy", "followers_url": "https://api.github.com/users/rhaps0dy/followers", "following_url": "https://api.github.com/users/rhaps0dy/following{/other_user}", "gists_url": "https://api.github.com/users/rhaps0dy/gists{/gist_id}", "starred_url": "https://api.github.com/users/rhaps0dy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rhaps0dy/subscriptions", "organizations_url": "https://api.github.com/users/rhaps0dy/orgs", "repos_url": "https://api.github.com/users/rhaps0dy/repos", "events_url": "https://api.github.com/users/rhaps0dy/events{/privacy}", "received_events_url": "https://api.github.com/users/rhaps0dy/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-24T13:14:33Z", "updated_at": "2018-07-24T13:14:33Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12032350\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/GSPP\">@GSPP</a> ,</p>\n<blockquote>\n<p>I understand that any fully connected NN layer requires matmul</p>\n</blockquote>\n<p>Yep!</p>\n<blockquote>\n<p>So does this not mean that pretty much any neural network implementation has a need for batched matmul?</p>\n</blockquote>\n<p>Actually, no. If you had a single example, x which is a vector, and you wanted to apply the weights and compute Wx, you need a matrix-vector multiplication. If you have a minibatch of inputs, then X is a matrix, and you compute X@W which is a matrix-matrix multiplication. At no point you need a batched matrix multiplication.</p>\n<p>Batched matmul would be needed if, for example, you need to do a matrix-matrix multiplication for every training example and then you need to minibatch them.</p>\n<p>Additionally, batched matmul <em>is</em> currently supported. What isn't supported is <em>broadcasted</em> batch matmul, that is, when you want to do W@X and then W is of shape [1, a, b] and X is of shape [c, b, d].</p>", "body_text": "Hi @GSPP ,\n\nI understand that any fully connected NN layer requires matmul\n\nYep!\n\nSo does this not mean that pretty much any neural network implementation has a need for batched matmul?\n\nActually, no. If you had a single example, x which is a vector, and you wanted to apply the weights and compute Wx, you need a matrix-vector multiplication. If you have a minibatch of inputs, then X is a matrix, and you compute X@W which is a matrix-matrix multiplication. At no point you need a batched matrix multiplication.\nBatched matmul would be needed if, for example, you need to do a matrix-matrix multiplication for every training example and then you need to minibatch them.\nAdditionally, batched matmul is currently supported. What isn't supported is broadcasted batch matmul, that is, when you want to do W@X and then W is of shape [1, a, b] and X is of shape [c, b, d].", "body": "Hi @GSPP , \r\n\r\n>I understand that any fully connected NN layer requires matmul\r\n\r\nYep!\r\n\r\n>So does this not mean that pretty much any neural network implementation has a need for batched matmul?\r\n\r\nActually, no. If you had a single example, x which is a vector, and you wanted to apply the weights and compute Wx, you need a matrix-vector multiplication. If you have a minibatch of inputs, then X is a matrix, and you compute X@W which is a matrix-matrix multiplication. At no point you need a batched matrix multiplication.\r\n\r\nBatched matmul would be needed if, for example, you need to do a matrix-matrix multiplication for every training example and then you need to minibatch them.\r\n\r\nAdditionally, batched matmul _is_ currently supported. What isn't supported is _broadcasted_ batch matmul, that is, when you want to do W@X and then W is of shape [1, a, b] and X is of shape [c, b, d].\r\n"}