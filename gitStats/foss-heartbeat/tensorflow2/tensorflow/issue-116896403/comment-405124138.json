{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405124138", "html_url": "https://github.com/tensorflow/tensorflow/issues/216#issuecomment-405124138", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/216", "id": 405124138, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTEyNDEzOA==", "user": {"login": "ModarTensai", "id": 7101743, "node_id": "MDQ6VXNlcjcxMDE3NDM=", "avatar_url": "https://avatars0.githubusercontent.com/u/7101743?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ModarTensai", "html_url": "https://github.com/ModarTensai", "followers_url": "https://api.github.com/users/ModarTensai/followers", "following_url": "https://api.github.com/users/ModarTensai/following{/other_user}", "gists_url": "https://api.github.com/users/ModarTensai/gists{/gist_id}", "starred_url": "https://api.github.com/users/ModarTensai/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ModarTensai/subscriptions", "organizations_url": "https://api.github.com/users/ModarTensai/orgs", "repos_url": "https://api.github.com/users/ModarTensai/repos", "events_url": "https://api.github.com/users/ModarTensai/events{/privacy}", "received_events_url": "https://api.github.com/users/ModarTensai/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-15T22:47:27Z", "updated_at": "2018-07-18T21:07:03Z", "author_association": "NONE", "body_html": "<p>I am using this until an official implementation comes around (inspired by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15991519\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/talhasaruhan\">@talhasaruhan</a>):</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_matmul</span>(<span class=\"pl-smi\">A</span>, <span class=\"pl-smi\">B</span>, <span class=\"pl-smi\">transpose_a</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-smi\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>Batch support for matrix matrix product.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">        A: General matrix of size (A_Batch, M, X).</span>\n<span class=\"pl-s\">        B: General matrix of size (B_Batch, X, N).</span>\n<span class=\"pl-s\">        transpose_a: Whether A is transposed (A_Batch, X, M).</span>\n<span class=\"pl-s\">        transpose_b: Whether B is transposed (B_Batch, N, X).</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">        The result of multiplying A with B (A_Batch, B_Batch, M, N).</span>\n<span class=\"pl-s\">        Works more efficiently if B_Batch is empty.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n    Andim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(A.shape)\n    Bndim <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(B.shape)\n    <span class=\"pl-k\">if</span> Andim <span class=\"pl-k\">==</span> Bndim:\n        <span class=\"pl-k\">return</span> tf.matmul(A, B, <span class=\"pl-v\">transpose_a</span><span class=\"pl-k\">=</span>transpose_a,\n                         <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span>transpose_b)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> faster than tensordot</span>\n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>matmul<span class=\"pl-pds\">'</span></span>):\n        a_index <span class=\"pl-k\">=</span> Andim <span class=\"pl-k\">-</span> (<span class=\"pl-c1\">2</span> <span class=\"pl-k\">if</span> transpose_a <span class=\"pl-k\">else</span> <span class=\"pl-c1\">1</span>)\n        b_index <span class=\"pl-k\">=</span> Bndim <span class=\"pl-k\">-</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">if</span> transpose_b <span class=\"pl-k\">else</span> <span class=\"pl-c1\">2</span>)\n        <span class=\"pl-c1\">AB</span> <span class=\"pl-k\">=</span> tf.tensordot(A, B, <span class=\"pl-v\">axes</span><span class=\"pl-k\">=</span>[a_index, b_index])\n        <span class=\"pl-k\">if</span> Bndim <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">2</span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> only if B is batched, rearrange the axes</span>\n            A_Batch <span class=\"pl-k\">=</span> np.arange(Andim <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>)\n            M <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(A_Batch)\n            B_Batch <span class=\"pl-k\">=</span> (M <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> np.arange(Bndim <span class=\"pl-k\">-</span> <span class=\"pl-c1\">2</span>)\n            N <span class=\"pl-k\">=</span> (M <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">len</span>(B_Batch)\n            perm <span class=\"pl-k\">=</span> np.concatenate((A_Batch, B_Batch, [M, N]))\n            <span class=\"pl-c1\">AB</span> <span class=\"pl-k\">=</span> tf.transpose(<span class=\"pl-c1\">AB</span>, perm)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">AB</span></pre></div>\n<p>This code would still work even if both A_batch and B_batch had multiple axes or none at all. The only caveat here is when B_Batch is not empty; the shape of the output tensor would be <code>(A_Batch, M, B_Batch, N)</code>, which requires calling <code>tf.transpose()</code> that <a href=\"https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/transpose\" rel=\"nofollow\">copies the full tensor</a> instead of creating an efficient view (on TensorFlow version 1.9 and older). I believe an efficient impelmentation of reshaping the inner-most axes of a tensor as a view would solve this issue but I am not aware of such function.</p>", "body_text": "I am using this until an official implementation comes around (inspired by @talhasaruhan):\nimport numpy as np\nimport tensorflow as tf\n\ndef batch_matmul(A, B, transpose_a=False, transpose_b=False):\n    '''Batch support for matrix matrix product.\n\n    Args:\n        A: General matrix of size (A_Batch, M, X).\n        B: General matrix of size (B_Batch, X, N).\n        transpose_a: Whether A is transposed (A_Batch, X, M).\n        transpose_b: Whether B is transposed (B_Batch, N, X).\n\n    Returns:\n        The result of multiplying A with B (A_Batch, B_Batch, M, N).\n        Works more efficiently if B_Batch is empty.\n    '''\n    Andim = len(A.shape)\n    Bndim = len(B.shape)\n    if Andim == Bndim:\n        return tf.matmul(A, B, transpose_a=transpose_a,\n                         transpose_b=transpose_b)  # faster than tensordot\n    with tf.name_scope('matmul'):\n        a_index = Andim - (2 if transpose_a else 1)\n        b_index = Bndim - (1 if transpose_b else 2)\n        AB = tf.tensordot(A, B, axes=[a_index, b_index])\n        if Bndim > 2:  # only if B is batched, rearrange the axes\n            A_Batch = np.arange(Andim - 2)\n            M = len(A_Batch)\n            B_Batch = (M + 1) + np.arange(Bndim - 2)\n            N = (M + 1) + len(B_Batch)\n            perm = np.concatenate((A_Batch, B_Batch, [M, N]))\n            AB = tf.transpose(AB, perm)\n    return AB\nThis code would still work even if both A_batch and B_batch had multiple axes or none at all. The only caveat here is when B_Batch is not empty; the shape of the output tensor would be (A_Batch, M, B_Batch, N), which requires calling tf.transpose() that copies the full tensor instead of creating an efficient view (on TensorFlow version 1.9 and older). I believe an efficient impelmentation of reshaping the inner-most axes of a tensor as a view would solve this issue but I am not aware of such function.", "body": "I am using this until an official implementation comes around (inspired by @talhasaruhan):\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef batch_matmul(A, B, transpose_a=False, transpose_b=False):\r\n    '''Batch support for matrix matrix product.\r\n\r\n    Args:\r\n        A: General matrix of size (A_Batch, M, X).\r\n        B: General matrix of size (B_Batch, X, N).\r\n        transpose_a: Whether A is transposed (A_Batch, X, M).\r\n        transpose_b: Whether B is transposed (B_Batch, N, X).\r\n\r\n    Returns:\r\n        The result of multiplying A with B (A_Batch, B_Batch, M, N).\r\n        Works more efficiently if B_Batch is empty.\r\n    '''\r\n    Andim = len(A.shape)\r\n    Bndim = len(B.shape)\r\n    if Andim == Bndim:\r\n        return tf.matmul(A, B, transpose_a=transpose_a,\r\n                         transpose_b=transpose_b)  # faster than tensordot\r\n    with tf.name_scope('matmul'):\r\n        a_index = Andim - (2 if transpose_a else 1)\r\n        b_index = Bndim - (1 if transpose_b else 2)\r\n        AB = tf.tensordot(A, B, axes=[a_index, b_index])\r\n        if Bndim > 2:  # only if B is batched, rearrange the axes\r\n            A_Batch = np.arange(Andim - 2)\r\n            M = len(A_Batch)\r\n            B_Batch = (M + 1) + np.arange(Bndim - 2)\r\n            N = (M + 1) + len(B_Batch)\r\n            perm = np.concatenate((A_Batch, B_Batch, [M, N]))\r\n            AB = tf.transpose(AB, perm)\r\n    return AB\r\n```\r\nThis code would still work even if both A_batch and B_batch had multiple axes or none at all. The only caveat here is when B_Batch is not empty; the shape of the output tensor would be `(A_Batch, M, B_Batch, N)`, which requires calling `tf.transpose()` that [copies the full tensor](https://www.tensorflow.org/versions/r1.9/api_docs/python/tf/transpose) instead of creating an efficient view (on TensorFlow version 1.9 and older). I believe an efficient impelmentation of reshaping the inner-most axes of a tensor as a view would solve this issue but I am not aware of such function."}