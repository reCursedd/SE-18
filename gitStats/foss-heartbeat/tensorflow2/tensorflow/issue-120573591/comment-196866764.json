{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/196866764", "html_url": "https://github.com/tensorflow/tensorflow/issues/418#issuecomment-196866764", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/418", "id": 196866764, "node_id": "MDEyOklzc3VlQ29tbWVudDE5Njg2Njc2NA==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-15T15:08:48Z", "updated_at": "2016-03-15T15:08:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I made a <code>ravel_multi_index</code> . From a multi-index, it produces an tensor suitable for indexing a flattened tensor. See the <a href=\"http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ravel_multi_index.html\" rel=\"nofollow\">numpy</a> documentation for instance.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">__cumprod</span>(<span class=\"pl-smi\">l</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Get the length and make a copy</span>\n    ll <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(l)\n    l <span class=\"pl-k\">=</span> [v <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> l]\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reverse cumulative product</span>\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(ll<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>):\n        l[ll<span class=\"pl-k\">-</span>i<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*=</span> l[ll<span class=\"pl-k\">-</span>i<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n    <span class=\"pl-k\">return</span> l\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">ravel_multi_index</span>(<span class=\"pl-smi\">tensor</span>, <span class=\"pl-smi\">multi_idx</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Returns a tensor suitable for use as the index</span>\n<span class=\"pl-s\">    on a gather operation on argument tensor.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(tensor, (tf.Variable, tf.Tensor)):\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tensor should be a tf.Variable<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(multi_idx, <span class=\"pl-c1\">list</span>):\n        multi_idx <span class=\"pl-k\">=</span> [multi_idx]\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Shape of the tensor in ints</span>\n    shape <span class=\"pl-k\">=</span> [i.value <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> tensor.get_shape()]\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(shape) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">len</span>(multi_idx):\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Tensor rank is different <span class=\"pl-pds\">\"</span></span>\n                        <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>from the multi_idx length.<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Work out the shape of each tensor in the multi_idx</span>\n    idx_shape <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">tuple</span>(j.value <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> i.get_shape()) <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> multi_idx]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ensure that each multi_idx tensor is length 1</span>\n    <span class=\"pl-k\">assert</span> <span class=\"pl-c1\">all</span>(<span class=\"pl-c1\">len</span>(i) <span class=\"pl-k\">==</span> <span class=\"pl-c1\">1</span> <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> idx_shape)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a list of reshaped indices. New shape will be</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [1, 1, dim[0], 1] for the 3rd index in multi_idx</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> for example.</span>\n    reshaped_idx <span class=\"pl-k\">=</span> [tf.reshape(idx, [<span class=\"pl-c1\">1</span> <span class=\"pl-k\">if</span> i <span class=\"pl-k\">!=</span>j <span class=\"pl-k\">else</span> dim[<span class=\"pl-c1\">0</span>]\n                    <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(shape))])\n                <span class=\"pl-k\">for</span> i, (idx, dim)\n                <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">zip</span>(multi_idx, idx_shape))]\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Figure out the base indices for each dimension</span>\n    base <span class=\"pl-k\">=</span> __cumprod(shape)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Now multiply base indices by each reshaped index</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> to produce the flat index</span>\n    <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">sum</span>(b<span class=\"pl-k\">*</span>s <span class=\"pl-k\">for</span> b, s <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(base[<span class=\"pl-c1\">1</span>:], reshaped_idx[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]))\n        <span class=\"pl-k\">+</span> reshaped_idx[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Shape and slice starts and sizes</span>\nshape <span class=\"pl-k\">=</span> (Z, Y, X) <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">6</span>\nZ0, Y0, X0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>\n<span class=\"pl-c1\">ZS</span>, <span class=\"pl-c1\">YS</span>, <span class=\"pl-c1\">XS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Numpy matrix and index</span>\nM <span class=\"pl-k\">=</span> np.random.random(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>shape)\nidx <span class=\"pl-k\">=</span> [\n    np.arange(Z0, Z0<span class=\"pl-k\">+</span><span class=\"pl-c1\">ZS</span>).reshape(<span class=\"pl-c1\">ZS</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>),\n    np.arange(Y0, Y0<span class=\"pl-k\">+</span><span class=\"pl-c1\">YS</span>).reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">YS</span>,<span class=\"pl-c1\">1</span>),\n    np.arange(X0, X0<span class=\"pl-k\">+</span><span class=\"pl-c1\">XS</span>).reshape(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">XS</span>),\n]\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensorflow matrix and indices</span>\n<span class=\"pl-c1\">TM</span> <span class=\"pl-k\">=</span> tf.Variable(M)\n<span class=\"pl-c1\">TF_flat_idx</span> <span class=\"pl-k\">=</span> ravel_multi_index(<span class=\"pl-c1\">TM</span>, [\n    tf.range(Z0, Z0<span class=\"pl-k\">+</span><span class=\"pl-c1\">ZS</span>),\n    tf.range(Y0, Y0<span class=\"pl-k\">+</span><span class=\"pl-c1\">YS</span>),\n    tf.range(X0, X0<span class=\"pl-k\">+</span><span class=\"pl-c1\">XS</span>)])\n<span class=\"pl-c1\">TF_data</span> <span class=\"pl-k\">=</span> tf.gather(tf.reshape(<span class=\"pl-c1\">TM</span>,[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]), <span class=\"pl-c1\">TF_flat_idx</span>)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> S:\n    S.run(tf.initialize_all_variables())\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Obtain data via flat indexing</span>\n    data <span class=\"pl-k\">=</span> S.run(<span class=\"pl-c1\">TF_data</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Check that it agrees with data obtained</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> by numpy smart indexing</span>\n    <span class=\"pl-k\">assert</span> np.all(data <span class=\"pl-k\">==</span> M[idx])</pre></div>", "body_text": "I made a ravel_multi_index . From a multi-index, it produces an tensor suitable for indexing a flattened tensor. See the numpy documentation for instance.\nimport tensorflow as tf\nimport numpy as np\n\ndef __cumprod(l):\n    # Get the length and make a copy\n    ll = len(l)\n    l = [v for v in l]\n\n    # Reverse cumulative product\n    for i in range(ll-1):\n        l[ll-i-2] *= l[ll-i-1]\n\n    return l\n\ndef ravel_multi_index(tensor, multi_idx):\n    \"\"\"\n    Returns a tensor suitable for use as the index\n    on a gather operation on argument tensor.\n    \"\"\"\n\n    if not isinstance(tensor, (tf.Variable, tf.Tensor)):\n        raise TypeError('tensor should be a tf.Variable')\n\n    if not isinstance(multi_idx, list):\n        multi_idx = [multi_idx]\n\n    # Shape of the tensor in ints\n    shape = [i.value for i in tensor.get_shape()]\n\n    if len(shape) != len(multi_idx):\n        raise ValueError(\"Tensor rank is different \"\n                        \"from the multi_idx length.\")\n\n    # Work out the shape of each tensor in the multi_idx\n    idx_shape = [tuple(j.value for j in i.get_shape()) for i in multi_idx]\n    # Ensure that each multi_idx tensor is length 1\n    assert all(len(i) == 1 for i in idx_shape)\n\n    # Create a list of reshaped indices. New shape will be\n    # [1, 1, dim[0], 1] for the 3rd index in multi_idx\n    # for example.\n    reshaped_idx = [tf.reshape(idx, [1 if i !=j else dim[0]\n                    for j in range(len(shape))])\n                for i, (idx, dim)\n                in enumerate(zip(multi_idx, idx_shape))]\n\n    # Figure out the base indices for each dimension\n    base = __cumprod(shape)\n\n    # Now multiply base indices by each reshaped index\n    # to produce the flat index\n    return (sum(b*s for b, s in zip(base[1:], reshaped_idx[:-1]))\n        + reshaped_idx[-1])\n\n# Shape and slice starts and sizes\nshape = (Z, Y, X) = 4, 5, 6\nZ0, Y0, X0 = 1, 1, 1\nZS, YS, XS = 3, 3, 4\n\n# Numpy matrix and index\nM = np.random.random(size=shape)\nidx = [\n    np.arange(Z0, Z0+ZS).reshape(ZS,1,1),\n    np.arange(Y0, Y0+YS).reshape(1,YS,1),\n    np.arange(X0, X0+XS).reshape(1,1,XS),\n]\n\n# Tensorflow matrix and indices\nTM = tf.Variable(M)\nTF_flat_idx = ravel_multi_index(TM, [\n    tf.range(Z0, Z0+ZS),\n    tf.range(Y0, Y0+YS),\n    tf.range(X0, X0+XS)])\nTF_data = tf.gather(tf.reshape(TM,[-1]), TF_flat_idx)\n\nwith tf.Session() as S:\n    S.run(tf.initialize_all_variables())\n\n    # Obtain data via flat indexing\n    data = S.run(TF_data)\n\n    # Check that it agrees with data obtained\n    # by numpy smart indexing\n    assert np.all(data == M[idx])", "body": "I made a `ravel_multi_index` . From a multi-index, it produces an tensor suitable for indexing a flattened tensor. See the [numpy](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.ravel_multi_index.html) documentation for instance.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\ndef __cumprod(l):\n    # Get the length and make a copy\n    ll = len(l)\n    l = [v for v in l]\n\n    # Reverse cumulative product\n    for i in range(ll-1):\n        l[ll-i-2] *= l[ll-i-1]\n\n    return l\n\ndef ravel_multi_index(tensor, multi_idx):\n    \"\"\"\n    Returns a tensor suitable for use as the index\n    on a gather operation on argument tensor.\n    \"\"\"\n\n    if not isinstance(tensor, (tf.Variable, tf.Tensor)):\n        raise TypeError('tensor should be a tf.Variable')\n\n    if not isinstance(multi_idx, list):\n        multi_idx = [multi_idx]\n\n    # Shape of the tensor in ints\n    shape = [i.value for i in tensor.get_shape()]\n\n    if len(shape) != len(multi_idx):\n        raise ValueError(\"Tensor rank is different \"\n                        \"from the multi_idx length.\")\n\n    # Work out the shape of each tensor in the multi_idx\n    idx_shape = [tuple(j.value for j in i.get_shape()) for i in multi_idx]\n    # Ensure that each multi_idx tensor is length 1\n    assert all(len(i) == 1 for i in idx_shape)\n\n    # Create a list of reshaped indices. New shape will be\n    # [1, 1, dim[0], 1] for the 3rd index in multi_idx\n    # for example.\n    reshaped_idx = [tf.reshape(idx, [1 if i !=j else dim[0]\n                    for j in range(len(shape))])\n                for i, (idx, dim)\n                in enumerate(zip(multi_idx, idx_shape))]\n\n    # Figure out the base indices for each dimension\n    base = __cumprod(shape)\n\n    # Now multiply base indices by each reshaped index\n    # to produce the flat index\n    return (sum(b*s for b, s in zip(base[1:], reshaped_idx[:-1]))\n        + reshaped_idx[-1])\n\n# Shape and slice starts and sizes\nshape = (Z, Y, X) = 4, 5, 6\nZ0, Y0, X0 = 1, 1, 1\nZS, YS, XS = 3, 3, 4\n\n# Numpy matrix and index\nM = np.random.random(size=shape)\nidx = [\n    np.arange(Z0, Z0+ZS).reshape(ZS,1,1),\n    np.arange(Y0, Y0+YS).reshape(1,YS,1),\n    np.arange(X0, X0+XS).reshape(1,1,XS),\n]\n\n# Tensorflow matrix and indices\nTM = tf.Variable(M)\nTF_flat_idx = ravel_multi_index(TM, [\n    tf.range(Z0, Z0+ZS),\n    tf.range(Y0, Y0+YS),\n    tf.range(X0, X0+XS)])\nTF_data = tf.gather(tf.reshape(TM,[-1]), TF_flat_idx)\n\nwith tf.Session() as S:\n    S.run(tf.initialize_all_variables())\n\n    # Obtain data via flat indexing\n    data = S.run(TF_data)\n\n    # Check that it agrees with data obtained\n    # by numpy smart indexing\n    assert np.all(data == M[idx])\n```\n"}