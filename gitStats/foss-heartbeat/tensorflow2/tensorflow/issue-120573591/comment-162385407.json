{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/162385407", "html_url": "https://github.com/tensorflow/tensorflow/issues/418#issuecomment-162385407", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/418", "id": 162385407, "node_id": "MDEyOklzc3VlQ29tbWVudDE2MjM4NTQwNw==", "user": {"login": "hugman", "id": 2958929, "node_id": "MDQ6VXNlcjI5NTg5Mjk=", "avatar_url": "https://avatars3.githubusercontent.com/u/2958929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hugman", "html_url": "https://github.com/hugman", "followers_url": "https://api.github.com/users/hugman/followers", "following_url": "https://api.github.com/users/hugman/following{/other_user}", "gists_url": "https://api.github.com/users/hugman/gists{/gist_id}", "starred_url": "https://api.github.com/users/hugman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hugman/subscriptions", "organizations_url": "https://api.github.com/users/hugman/orgs", "repos_url": "https://api.github.com/users/hugman/repos", "events_url": "https://api.github.com/users/hugman/events{/privacy}", "received_events_url": "https://api.github.com/users/hugman/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-07T01:23:53Z", "updated_at": "2015-12-07T01:23:53Z", "author_association": "NONE", "body_html": "<p>Here I put some toy codes to re-generate errors.<br>\n( The loss calculation doesn't make sense, It's just for re-generating errors)</p>\n<p>There are two problems.</p>\n<ul>\n<li>case I : using method_1() for slicing ( direct element slicing with tensor)\n<ul>\n<li>error when graph construction time</li>\n</ul>\n</li>\n<li>case II : using method_2() for slicing (gather approach)\n<ul>\n<li>ok when graph construction, but can't back-propagate</li>\n<li>also using gpu:0 and cpu:0 options show different error messages.</li>\n</ul>\n</li>\n<li>case III : using mehthod_3() for slicing ( using python INT for indexing)\n<ul>\n<li>OK.</li>\n</ul>\n</li>\n</ul>\n<p>Errors can be re-generated by switching<br>\n<code>scores = method_1()</code>, <code>scores = method_2()</code>, <code>scores = method_3()</code></p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport math\n\nwith tf.Graph().as_default():\n  with tf.device(\"/cpu:0\"):\n\n    # NxN matrix for weights\n    size = 3\n    w_init = tf.truncated_normal_initializer(stddev=1.0/ math.sqrt(float(size)) )\n    mat    = tf.get_variable(\"weight_matrix\", [size, size], initializer=w_init )\n\n    # Indices to retrieve weight\n    length = 3\n    x = tf.placeholder(tf.int32, shape=(length) ) \n    y = tf.placeholder(tf.int32, shape=(length) ) \n\n    # score\n    def method_1():   # &lt;-- graph construction error!\n      scores = []\n      for i in xrange(length):\n        v = mat[x[i], y[i]] # slice error \n        scores.append( v ) \n      return scores\n\n    def method_2():  # &lt;-- graph is ok. error when training time\n      scores = []\n      for i in xrange(length):\n        v = tf.gather( tf.gather(mat, x[i]), y[i] ) # ok. but can't minimize\n        scores.append( v ) \n      return scores\n\n    def method_3(): # &lt;-- graph is ok. train ok. \n      scores = []\n      scores.append( mat[0,0] )\n      scores.append( mat[1,1] )\n      scores.append( mat[2,2] )\n      return scores\n\n    # loss to minimize\n    # switch method_1, method_2 and method 3 to generate errors\n    scores = method_1()\n    loss = tf.reduce_sum( tf.pack(scores) )\n\n    # optmizer\n    optimizer   = tf.train.GradientDescentOptimizer(0.01)\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op    = optimizer.minimize(loss, global_step=global_step)\n\n  with tf.Session() as sess:\n    init  = tf.initialize_all_variables()\n    sess.run(init)\n\n    feed_data = {}\n    feed_data[x] = np.array([2,1,0])\n    feed_data[y] = np.array([0,1,1])\n\n    l = sess.run(loss, feed_dict=feed_data)\n    print(l)\n\n    print(\"!!! Training Time !!!\")\n    _ = sess.run(train_op, feed_dict=feed_data) # &lt;-- error!\n</code></pre>", "body_text": "Here I put some toy codes to re-generate errors.\n( The loss calculation doesn't make sense, It's just for re-generating errors)\nThere are two problems.\n\ncase I : using method_1() for slicing ( direct element slicing with tensor)\n\nerror when graph construction time\n\n\ncase II : using method_2() for slicing (gather approach)\n\nok when graph construction, but can't back-propagate\nalso using gpu:0 and cpu:0 options show different error messages.\n\n\ncase III : using mehthod_3() for slicing ( using python INT for indexing)\n\nOK.\n\n\n\nErrors can be re-generated by switching\nscores = method_1(), scores = method_2(), scores = method_3()\nimport tensorflow as tf\nimport numpy as np\nimport math\n\nwith tf.Graph().as_default():\n  with tf.device(\"/cpu:0\"):\n\n    # NxN matrix for weights\n    size = 3\n    w_init = tf.truncated_normal_initializer(stddev=1.0/ math.sqrt(float(size)) )\n    mat    = tf.get_variable(\"weight_matrix\", [size, size], initializer=w_init )\n\n    # Indices to retrieve weight\n    length = 3\n    x = tf.placeholder(tf.int32, shape=(length) ) \n    y = tf.placeholder(tf.int32, shape=(length) ) \n\n    # score\n    def method_1():   # <-- graph construction error!\n      scores = []\n      for i in xrange(length):\n        v = mat[x[i], y[i]] # slice error \n        scores.append( v ) \n      return scores\n\n    def method_2():  # <-- graph is ok. error when training time\n      scores = []\n      for i in xrange(length):\n        v = tf.gather( tf.gather(mat, x[i]), y[i] ) # ok. but can't minimize\n        scores.append( v ) \n      return scores\n\n    def method_3(): # <-- graph is ok. train ok. \n      scores = []\n      scores.append( mat[0,0] )\n      scores.append( mat[1,1] )\n      scores.append( mat[2,2] )\n      return scores\n\n    # loss to minimize\n    # switch method_1, method_2 and method 3 to generate errors\n    scores = method_1()\n    loss = tf.reduce_sum( tf.pack(scores) )\n\n    # optmizer\n    optimizer   = tf.train.GradientDescentOptimizer(0.01)\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op    = optimizer.minimize(loss, global_step=global_step)\n\n  with tf.Session() as sess:\n    init  = tf.initialize_all_variables()\n    sess.run(init)\n\n    feed_data = {}\n    feed_data[x] = np.array([2,1,0])\n    feed_data[y] = np.array([0,1,1])\n\n    l = sess.run(loss, feed_dict=feed_data)\n    print(l)\n\n    print(\"!!! Training Time !!!\")\n    _ = sess.run(train_op, feed_dict=feed_data) # <-- error!", "body": "Here I put some toy codes to re-generate errors. \n( The loss calculation doesn't make sense, It's just for re-generating errors)\n\nThere are two problems.\n- case I : using method_1() for slicing ( direct element slicing with tensor)\n  - error when graph construction time \n- case II : using method_2() for slicing (gather approach)\n  - ok when graph construction, but can't back-propagate \n  - also using gpu:0 and cpu:0 options show different error messages. \n- case III : using mehthod_3() for slicing ( using python INT for indexing)\n  - OK.\n\nErrors can be re-generated by switching \n<code>scores = method_1()</code>, <code>scores = method_2()</code>, <code>scores = method_3()</code>\n\n```\nimport tensorflow as tf\nimport numpy as np\nimport math\n\nwith tf.Graph().as_default():\n  with tf.device(\"/cpu:0\"):\n\n    # NxN matrix for weights\n    size = 3\n    w_init = tf.truncated_normal_initializer(stddev=1.0/ math.sqrt(float(size)) )\n    mat    = tf.get_variable(\"weight_matrix\", [size, size], initializer=w_init )\n\n    # Indices to retrieve weight\n    length = 3\n    x = tf.placeholder(tf.int32, shape=(length) ) \n    y = tf.placeholder(tf.int32, shape=(length) ) \n\n    # score\n    def method_1():   # <-- graph construction error!\n      scores = []\n      for i in xrange(length):\n        v = mat[x[i], y[i]] # slice error \n        scores.append( v ) \n      return scores\n\n    def method_2():  # <-- graph is ok. error when training time\n      scores = []\n      for i in xrange(length):\n        v = tf.gather( tf.gather(mat, x[i]), y[i] ) # ok. but can't minimize\n        scores.append( v ) \n      return scores\n\n    def method_3(): # <-- graph is ok. train ok. \n      scores = []\n      scores.append( mat[0,0] )\n      scores.append( mat[1,1] )\n      scores.append( mat[2,2] )\n      return scores\n\n    # loss to minimize\n    # switch method_1, method_2 and method 3 to generate errors\n    scores = method_1()\n    loss = tf.reduce_sum( tf.pack(scores) )\n\n    # optmizer\n    optimizer   = tf.train.GradientDescentOptimizer(0.01)\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    train_op    = optimizer.minimize(loss, global_step=global_step)\n\n  with tf.Session() as sess:\n    init  = tf.initialize_all_variables()\n    sess.run(init)\n\n    feed_data = {}\n    feed_data[x] = np.array([2,1,0])\n    feed_data[y] = np.array([0,1,1])\n\n    l = sess.run(loss, feed_dict=feed_data)\n    print(l)\n\n    print(\"!!! Training Time !!!\")\n    _ = sess.run(train_op, feed_dict=feed_data) # <-- error!\n```\n"}