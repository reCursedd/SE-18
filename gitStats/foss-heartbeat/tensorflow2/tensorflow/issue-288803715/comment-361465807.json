{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361465807", "html_url": "https://github.com/tensorflow/tensorflow/issues/16147#issuecomment-361465807", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16147", "id": 361465807, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQ2NTgwNw==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T03:29:00Z", "updated_at": "2018-01-30T03:29:00Z", "author_association": "MEMBER", "body_html": "<p>It would not be slower than a GTX950, something else is likely wrong.  If you are looking to utilize the TensorCores or just in general get the best inference on GPU you will want to look at TensorRT.  It can take a graph and optimize it.  Right now there is a script from NVIDIA that does it, but also a <a href=\"https://github.com/tensorflow/tensorflow/pull/16253\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/16253/hovercard\">PR</a> in progress to build it in to TensorFlow.  If I was trying to use it right now I would likely use the NVIDIA script that I have not seen or used but understand it exists.  I suspect it is not as easy to use as one would hope but some internal teams have tried it and the results were decent thus the desire to make it native to TF.  Progress will happen quickly as this is a priority and the initial focus is on FP16 in Q1 and then INT8.   But as with all things they take time.</p>\n<p>I am looking for a good example to benchmark for inference.  I would like it to use tf.data rather than feed_dict and prefer to have the model in code vs. loading from a saved graph.  I do not doubt your problems but I spent an entire afternoon looking for a cpu regression related to inference and the end result was I proved 1.5 was faster than 1.4 and the user realized they made a mistake.  Not a big deal and I was glad to gather the info but the example was not something I wanted to add to the benchmark so that made the work kind of throw away.</p>", "body_text": "It would not be slower than a GTX950, something else is likely wrong.  If you are looking to utilize the TensorCores or just in general get the best inference on GPU you will want to look at TensorRT.  It can take a graph and optimize it.  Right now there is a script from NVIDIA that does it, but also a PR in progress to build it in to TensorFlow.  If I was trying to use it right now I would likely use the NVIDIA script that I have not seen or used but understand it exists.  I suspect it is not as easy to use as one would hope but some internal teams have tried it and the results were decent thus the desire to make it native to TF.  Progress will happen quickly as this is a priority and the initial focus is on FP16 in Q1 and then INT8.   But as with all things they take time.\nI am looking for a good example to benchmark for inference.  I would like it to use tf.data rather than feed_dict and prefer to have the model in code vs. loading from a saved graph.  I do not doubt your problems but I spent an entire afternoon looking for a cpu regression related to inference and the end result was I proved 1.5 was faster than 1.4 and the user realized they made a mistake.  Not a big deal and I was glad to gather the info but the example was not something I wanted to add to the benchmark so that made the work kind of throw away.", "body": "It would not be slower than a GTX950, something else is likely wrong.  If you are looking to utilize the TensorCores or just in general get the best inference on GPU you will want to look at TensorRT.  It can take a graph and optimize it.  Right now there is a script from NVIDIA that does it, but also a [PR](https://github.com/tensorflow/tensorflow/pull/16253) in progress to build it in to TensorFlow.  If I was trying to use it right now I would likely use the NVIDIA script that I have not seen or used but understand it exists.  I suspect it is not as easy to use as one would hope but some internal teams have tried it and the results were decent thus the desire to make it native to TF.  Progress will happen quickly as this is a priority and the initial focus is on FP16 in Q1 and then INT8.   But as with all things they take time.  \r\n\r\nI am looking for a good example to benchmark for inference.  I would like it to use tf.data rather than feed_dict and prefer to have the model in code vs. loading from a saved graph.  I do not doubt your problems but I spent an entire afternoon looking for a cpu regression related to inference and the end result was I proved 1.5 was faster than 1.4 and the user realized they made a mistake.  Not a big deal and I was glad to gather the info but the example was not something I wanted to add to the benchmark so that made the work kind of throw away.  "}