{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11724", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11724/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11724/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11724/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11724", "id": 245186676, "node_id": "MDU6SXNzdWUyNDUxODY2NzY=", "number": 11724, "title": "Importing TensorFlow breaks numpy.linalg.matrix_power()", "user": {"login": "dburkhardt", "id": 8322751, "node_id": "MDQ6VXNlcjgzMjI3NTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/8322751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dburkhardt", "html_url": "https://github.com/dburkhardt", "followers_url": "https://api.github.com/users/dburkhardt/followers", "following_url": "https://api.github.com/users/dburkhardt/following{/other_user}", "gists_url": "https://api.github.com/users/dburkhardt/gists{/gist_id}", "starred_url": "https://api.github.com/users/dburkhardt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dburkhardt/subscriptions", "organizations_url": "https://api.github.com/users/dburkhardt/orgs", "repos_url": "https://api.github.com/users/dburkhardt/repos", "events_url": "https://api.github.com/users/dburkhardt/events{/privacy}", "received_events_url": "https://api.github.com/users/dburkhardt/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 29, "created_at": "2017-07-24T19:38:44Z", "updated_at": "2018-05-14T22:10:59Z", "closed_at": "2018-05-14T22:10:08Z", "author_association": "NONE", "body_html": "<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"236547816\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10771\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/10771/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/10771\">#10771</a></p>\n<h2>System information</h2>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Arch Linux</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2.1</li>\n<li><strong>Python version</strong>: 3.6.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: bazel release 0.5.1</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA V8.0.61 /</li>\n<li><strong>GPU model and memory</strong>: NVIDIA Titan X Fall 2016</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nnp.random.seed(<span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\nX <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">2000</span>, <span class=\"pl-c1\">2000</span>)\nY <span class=\"pl-k\">=</span> np.linalg.matrix_power(X, <span class=\"pl-c1\">30</span>)</pre></div>\n<h3>Describe the problem</h3>\n<p>This seems to be a bug where importing tensorflow breaks numpy.linalg.matrix_power()</p>\n<p>The two following code snippets are producing different results on my system when entered directly into the python interpreter. Note, I am restarting the python kernel between running each block.</p>\n<p><strong>This works:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\nnp.random.seed(<span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\nX <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">2000</span>, <span class=\"pl-c1\">2000</span>)\nY <span class=\"pl-k\">=</span> np.linalg.matrix_power(X, <span class=\"pl-c1\">30</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> X \narray([[ <span class=\"pl-c1\">1.76405235</span>,  <span class=\"pl-c1\">0.40015721</span>,  <span class=\"pl-c1\">0.97873798</span>, <span class=\"pl-c1\">...</span>,  <span class=\"pl-c1\">0.15843385</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.14190142</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.31097037</span>],\n       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.53292105</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.71197016</span>,  <span class=\"pl-c1\">0.04613506</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.03057244</span>,\n         <span class=\"pl-c1\">1.57708821</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.8128021</span> ],\n       [ <span class=\"pl-c1\">0.61334917</span>,  <span class=\"pl-c1\">1.84369998</span>,  <span class=\"pl-c1\">0.27109098</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.53788475</span>,\n         <span class=\"pl-c1\">0.39344443</span>,  <span class=\"pl-c1\">0.28651827</span>],\n       <span class=\"pl-c1\">...</span>, \n       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.17117027</span>,  <span class=\"pl-c1\">0.57332063</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.89516715</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.01409412</span>,\n         <span class=\"pl-c1\">1.28756456</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6953778</span> ],\n       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.53627571</span>,  <span class=\"pl-c1\">0.57441228</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.20564476</span>, <span class=\"pl-c1\">...</span>,  <span class=\"pl-c1\">0.90499929</span>,\n         <span class=\"pl-c1\">0.51428298</span>,  <span class=\"pl-c1\">0.72148202</span>],\n       [ <span class=\"pl-c1\">0.51262101</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.90758583</span>,  <span class=\"pl-c1\">1.78121159</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.12554283</span>,\n         <span class=\"pl-c1\">0.95170926</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.15237806</span>]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> Y\narray([[ <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.04752205e+48</span>,   <span class=\"pl-c1\">2.10841282e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.54826843e+47</span>, <span class=\"pl-c1\">...</span>,\n         <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.84526353e+46</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.45185369e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.96340973e+47</span>],\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.40802471e+46</span>,   <span class=\"pl-c1\">1.12546832e+48</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.88764494e+47</span>, <span class=\"pl-c1\">...</span>,\n         <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.72182046e+47</span>,   <span class=\"pl-c1\">7.97461852e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.04546546e+46</span>],\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.59835691e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.90559050e+46</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">8.78538707e+47</span>, <span class=\"pl-c1\">...</span>,\n          <span class=\"pl-c1\">7.67940928e+47</span>,   <span class=\"pl-c1\">2.10052546e+47</span>,   <span class=\"pl-c1\">1.75193723e+47</span>],\n       <span class=\"pl-c1\">...</span>,\n       [  <span class=\"pl-c1\">2.20970288e+46</span>,   <span class=\"pl-c1\">3.60679821e+47</span>,   <span class=\"pl-c1\">5.76631889e+47</span>, <span class=\"pl-c1\">...</span>,\n          <span class=\"pl-c1\">1.21938369e+48</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">8.61048462e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.93610572e+47</span>],\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.19116636e+48</span>,   <span class=\"pl-c1\">2.47318954e+48</span>,   <span class=\"pl-c1\">2.65693291e+46</span>, <span class=\"pl-c1\">...</span>,\n          <span class=\"pl-c1\">9.18513286e+47</span>,   <span class=\"pl-c1\">3.91490216e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.08113716e+47</span>],\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.25527724e+47</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.94088618e+46</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.69359430e+47</span>, <span class=\"pl-c1\">...</span>,\n         <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.07174632e+47</span>,   <span class=\"pl-c1\">7.38250907e+47</span>,   <span class=\"pl-c1\">5.86758288e+46</span>]])</pre></div>\n<p><strong>This produces the wrong result:</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nnp.random.seed(<span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\nX <span class=\"pl-k\">=</span> np.random.randn(<span class=\"pl-c1\">2000</span>, <span class=\"pl-c1\">2000</span>)\nY <span class=\"pl-k\">=</span> np.linalg.matrix_power(X, <span class=\"pl-c1\">30</span>)\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> X\narray([[ <span class=\"pl-c1\">1.76405235</span>,  <span class=\"pl-c1\">0.40015721</span>,  <span class=\"pl-c1\">0.97873798</span>, <span class=\"pl-c1\">...</span>,  <span class=\"pl-c1\">0.15843385</span>,\n        <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.14190142</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.31097037</span>],\n       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.53292105</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.71197016</span>,  <span class=\"pl-c1\">0.04613506</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.03057244</span>,\n         <span class=\"pl-c1\">1.57708821</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.8128021</span> ],\n       [ <span class=\"pl-c1\">0.61334917</span>,  <span class=\"pl-c1\">1.84369998</span>,  <span class=\"pl-c1\">0.27109098</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.53788475</span>,\n         <span class=\"pl-c1\">0.39344443</span>,  <span class=\"pl-c1\">0.28651827</span>],\n       <span class=\"pl-c1\">...</span>, \n       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">0.17117027</span>,  <span class=\"pl-c1\">0.57332063</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.89516715</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.01409412</span>,\n         <span class=\"pl-c1\">1.28756456</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.6953778</span> ],\n       [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1.53627571</span>,  <span class=\"pl-c1\">0.57441228</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.20564476</span>, <span class=\"pl-c1\">...</span>,  <span class=\"pl-c1\">0.90499929</span>,\n         <span class=\"pl-c1\">0.51428298</span>,  <span class=\"pl-c1\">0.72148202</span>],\n       [ <span class=\"pl-c1\">0.51262101</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.90758583</span>,  <span class=\"pl-c1\">1.78121159</span>, <span class=\"pl-c1\">...</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.12554283</span>,\n         <span class=\"pl-c1\">0.95170926</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.15237806</span>]])\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> Y\narray([[ <span class=\"pl-k\">-</span><span class=\"pl-c1\">3.40382764e+91</span>,   <span class=\"pl-c1\">2.85027458e+91</span>,   <span class=\"pl-c1\">1.14039870e+91</span>, <span class=\"pl-c1\">...</span>,\n          <span class=\"pl-c1\">3.32682992e+91</span>,   <span class=\"pl-c1\">6.00166234e+91</span>,   <span class=\"pl-c1\">2.33233825e+91</span>],\n       [  <span class=\"pl-c1\">3.86088264e+91</span>,   <span class=\"pl-c1\">1.15500453e+92</span>,   <span class=\"pl-c1\">2.83821815e+91</span>, <span class=\"pl-c1\">...</span>,\n         <span class=\"pl-k\">-</span><span class=\"pl-c1\">6.16959058e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.91501705e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.67672849e+91</span>],\n       [  <span class=\"pl-c1\">5.05026067e+91</span>,   <span class=\"pl-c1\">7.72796711e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.70112473e+91</span>, <span class=\"pl-c1\">...</span>,\n          <span class=\"pl-c1\">8.41553063e+90</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.66176140e+91</span>,   <span class=\"pl-c1\">8.50899233e+90</span>],\n       <span class=\"pl-c1\">...</span>,\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">2.08592878e+91</span>,   <span class=\"pl-c1\">2.28435173e+91</span>,   <span class=\"pl-c1\">9.15188619e+90</span>, <span class=\"pl-c1\">...</span>,\n         <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.25550051e+91</span>,   <span class=\"pl-c1\">1.85247259e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">8.73231986e+90</span>],\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">4.73923534e+91</span>,   <span class=\"pl-c1\">1.61385540e+92</span>,   <span class=\"pl-c1\">1.26364668e+92</span>, <span class=\"pl-c1\">...</span>,\n          <span class=\"pl-c1\">2.83667716e+91</span>,   <span class=\"pl-c1\">5.06236372e+90</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.18395025e+91</span>],\n       [ <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.52984791e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">5.57421948e+90</span>,   <span class=\"pl-c1\">3.27657918e+91</span>, <span class=\"pl-c1\">...</span>,\n         <span class=\"pl-k\">-</span><span class=\"pl-c1\">7.08972359e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.58912068e+91</span>,  <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.22216698e+91</span>]])</pre></div>\n<p>The values for X are exactly the same. Interestingly, if you run the first code block, then import tensorflow without restarting the kernel and recompute the matrix power, the correct result is returned.</p>\n<p>I haven't been able to find a reference to this anywhere online, and it's a big problem not being able to use tensorflow and numpy in the same script.</p>", "body_text": "#10771\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Arch Linux\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.2.1\nPython version: 3.6.1\nBazel version (if compiling from source): bazel release 0.5.1\nCUDA/cuDNN version: CUDA V8.0.61 /\nGPU model and memory: NVIDIA Titan X Fall 2016\nExact command to reproduce:\n\nimport numpy as np\nimport tensorflow as tf\nnp.random.seed(seed=0)\nX = np.random.randn(2000, 2000)\nY = np.linalg.matrix_power(X, 30)\nDescribe the problem\nThis seems to be a bug where importing tensorflow breaks numpy.linalg.matrix_power()\nThe two following code snippets are producing different results on my system when entered directly into the python interpreter. Note, I am restarting the python kernel between running each block.\nThis works:\nimport numpy as np\nnp.random.seed(seed=0)\nX = np.random.randn(2000, 2000)\nY = np.linalg.matrix_power(X, 30)\n>>> X \narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\n        -1.14190142, -1.31097037],\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\n         1.57708821, -0.8128021 ],\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\n         0.39344443,  0.28651827],\n       ..., \n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\n         1.28756456, -0.6953778 ],\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\n         0.51428298,  0.72148202],\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\n         0.95170926, -1.15237806]])\n>>> Y\narray([[ -1.04752205e+48,   2.10841282e+47,  -4.54826843e+47, ...,\n         -7.84526353e+46,  -4.45185369e+47,  -1.96340973e+47],\n       [ -5.40802471e+46,   1.12546832e+48,  -1.88764494e+47, ...,\n         -3.72182046e+47,   7.97461852e+47,  -5.04546546e+46],\n       [ -3.59835691e+47,  -6.90559050e+46,  -8.78538707e+47, ...,\n          7.67940928e+47,   2.10052546e+47,   1.75193723e+47],\n       ...,\n       [  2.20970288e+46,   3.60679821e+47,   5.76631889e+47, ...,\n          1.21938369e+48,  -8.61048462e+47,  -3.93610572e+47],\n       [ -1.19116636e+48,   2.47318954e+48,   2.65693291e+46, ...,\n          9.18513286e+47,   3.91490216e+47,  -7.08113716e+47],\n       [ -2.25527724e+47,  -4.94088618e+46,  -2.69359430e+47, ...,\n         -4.07174632e+47,   7.38250907e+47,   5.86758288e+46]])\nThis produces the wrong result:\nimport numpy as np\nimport tensorflow as tf\nnp.random.seed(seed=0)\nX = np.random.randn(2000, 2000)\nY = np.linalg.matrix_power(X, 30)\n>>> X\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\n        -1.14190142, -1.31097037],\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\n         1.57708821, -0.8128021 ],\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\n         0.39344443,  0.28651827],\n       ..., \n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\n         1.28756456, -0.6953778 ],\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\n         0.51428298,  0.72148202],\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\n         0.95170926, -1.15237806]])\n>>> Y\narray([[ -3.40382764e+91,   2.85027458e+91,   1.14039870e+91, ...,\n          3.32682992e+91,   6.00166234e+91,   2.33233825e+91],\n       [  3.86088264e+91,   1.15500453e+92,   2.83821815e+91, ...,\n         -6.16959058e+91,  -1.91501705e+91,  -4.67672849e+91],\n       [  5.05026067e+91,   7.72796711e+91,  -4.70112473e+91, ...,\n          8.41553063e+90,  -2.66176140e+91,   8.50899233e+90],\n       ...,\n       [ -2.08592878e+91,   2.28435173e+91,   9.15188619e+90, ...,\n         -1.25550051e+91,   1.85247259e+91,  -8.73231986e+90],\n       [ -4.73923534e+91,   1.61385540e+92,   1.26364668e+92, ...,\n          2.83667716e+91,   5.06236372e+90,  -5.18395025e+91],\n       [ -1.52984791e+91,  -5.57421948e+90,   3.27657918e+91, ...,\n         -7.08972359e+91,  -1.58912068e+91,  -1.22216698e+91]])\nThe values for X are exactly the same. Interestingly, if you run the first code block, then import tensorflow without restarting the kernel and recompute the matrix power, the correct result is returned.\nI haven't been able to find a reference to this anywhere online, and it's a big problem not being able to use tensorflow and numpy in the same script.", "body": "#10771 \r\n## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Arch Linux\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3.6.1\r\n- **Bazel version (if compiling from source)**: bazel release 0.5.1\r\n- **CUDA/cuDNN version**: CUDA V8.0.61 / \r\n- **GPU model and memory**: NVIDIA Titan X Fall 2016\r\n- **Exact command to reproduce**:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 30)\r\n```\r\n\r\n### Describe the problem\r\nThis seems to be a bug where importing tensorflow breaks numpy.linalg.matrix_power()\r\n\r\nThe two following code snippets are producing different results on my system when entered directly into the python interpreter. Note, I am restarting the python kernel between running each block.\r\n\r\n**This works:**\r\n```python\r\nimport numpy as np\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 30)\r\n>>> X \r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ..., \r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\n>>> Y\r\narray([[ -1.04752205e+48,   2.10841282e+47,  -4.54826843e+47, ...,\r\n         -7.84526353e+46,  -4.45185369e+47,  -1.96340973e+47],\r\n       [ -5.40802471e+46,   1.12546832e+48,  -1.88764494e+47, ...,\r\n         -3.72182046e+47,   7.97461852e+47,  -5.04546546e+46],\r\n       [ -3.59835691e+47,  -6.90559050e+46,  -8.78538707e+47, ...,\r\n          7.67940928e+47,   2.10052546e+47,   1.75193723e+47],\r\n       ...,\r\n       [  2.20970288e+46,   3.60679821e+47,   5.76631889e+47, ...,\r\n          1.21938369e+48,  -8.61048462e+47,  -3.93610572e+47],\r\n       [ -1.19116636e+48,   2.47318954e+48,   2.65693291e+46, ...,\r\n          9.18513286e+47,   3.91490216e+47,  -7.08113716e+47],\r\n       [ -2.25527724e+47,  -4.94088618e+46,  -2.69359430e+47, ...,\r\n         -4.07174632e+47,   7.38250907e+47,   5.86758288e+46]])\r\n```\r\n**This produces the wrong result:**\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nnp.random.seed(seed=0)\r\nX = np.random.randn(2000, 2000)\r\nY = np.linalg.matrix_power(X, 30)\r\n>>> X\r\narray([[ 1.76405235,  0.40015721,  0.97873798, ...,  0.15843385,\r\n        -1.14190142, -1.31097037],\r\n       [-1.53292105, -1.71197016,  0.04613506, ..., -0.03057244,\r\n         1.57708821, -0.8128021 ],\r\n       [ 0.61334917,  1.84369998,  0.27109098, ..., -0.53788475,\r\n         0.39344443,  0.28651827],\r\n       ..., \r\n       [-0.17117027,  0.57332063, -0.89516715, ..., -0.01409412,\r\n         1.28756456, -0.6953778 ],\r\n       [-1.53627571,  0.57441228, -0.20564476, ...,  0.90499929,\r\n         0.51428298,  0.72148202],\r\n       [ 0.51262101, -0.90758583,  1.78121159, ..., -1.12554283,\r\n         0.95170926, -1.15237806]])\r\n>>> Y\r\narray([[ -3.40382764e+91,   2.85027458e+91,   1.14039870e+91, ...,\r\n          3.32682992e+91,   6.00166234e+91,   2.33233825e+91],\r\n       [  3.86088264e+91,   1.15500453e+92,   2.83821815e+91, ...,\r\n         -6.16959058e+91,  -1.91501705e+91,  -4.67672849e+91],\r\n       [  5.05026067e+91,   7.72796711e+91,  -4.70112473e+91, ...,\r\n          8.41553063e+90,  -2.66176140e+91,   8.50899233e+90],\r\n       ...,\r\n       [ -2.08592878e+91,   2.28435173e+91,   9.15188619e+90, ...,\r\n         -1.25550051e+91,   1.85247259e+91,  -8.73231986e+90],\r\n       [ -4.73923534e+91,   1.61385540e+92,   1.26364668e+92, ...,\r\n          2.83667716e+91,   5.06236372e+90,  -5.18395025e+91],\r\n       [ -1.52984791e+91,  -5.57421948e+90,   3.27657918e+91, ...,\r\n         -7.08972359e+91,  -1.58912068e+91,  -1.22216698e+91]])\r\n```\r\n\r\nThe values for X are exactly the same. Interestingly, if you run the first code block, then import tensorflow without restarting the kernel and recompute the matrix power, the correct result is returned.\r\n\r\nI haven't been able to find a reference to this anywhere online, and it's a big problem not being able to use tensorflow and numpy in the same script.\r\n"}