{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20930", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20930/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20930/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20930/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20930", "id": 342396419, "node_id": "MDU6SXNzdWUzNDIzOTY0MTk=", "number": 20930, "title": "Reinitializable iterator doesn't use cached dataset upon reinitializing", "user": {"login": "saxenarohan97", "id": 17175695, "node_id": "MDQ6VXNlcjE3MTc1Njk1", "avatar_url": "https://avatars1.githubusercontent.com/u/17175695?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saxenarohan97", "html_url": "https://github.com/saxenarohan97", "followers_url": "https://api.github.com/users/saxenarohan97/followers", "following_url": "https://api.github.com/users/saxenarohan97/following{/other_user}", "gists_url": "https://api.github.com/users/saxenarohan97/gists{/gist_id}", "starred_url": "https://api.github.com/users/saxenarohan97/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saxenarohan97/subscriptions", "organizations_url": "https://api.github.com/users/saxenarohan97/orgs", "repos_url": "https://api.github.com/users/saxenarohan97/repos", "events_url": "https://api.github.com/users/saxenarohan97/events{/privacy}", "received_events_url": "https://api.github.com/users/saxenarohan97/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-18T16:09:59Z", "updated_at": "2018-11-20T07:54:18Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, written custom code</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 8.1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0, cuDNN v7.0</li>\n<li><strong>GPU model and memory</strong>: NVIDIA GeForce GTX 960 (faced the same problem with 1080 Ti)</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Found a likely bug when using a reinitialiable iterator with a cached dataset. Upon reinitializing the iterator, the iterator does not use the cached dataset. This can be verified using the sample code below.</p>\n<h3>Source code</h3>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\n\nLENGTH = 3\nCACHE = True\nVERBOSE = True\nSHAPE = (1,)\n\ndef data_generator():    \n    for n in range(LENGTH):\n        data = np.random.rand(*SHAPE)\n        yield data\n\ndef get_dataset():\n    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)\n    if CACHE:\n        train_data = train_data.cache()\n    return train_data\n\ndef get_dataset_repeat():\n    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)\n    if CACHE:\n        train_data = train_data.cache()\n    train_data = train_data.repeat()\n    return train_data\n\ndata = get_dataset()\ndata_repeat = get_dataset_repeat()\n\niterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes)\nnext_element = iterator.get_next()\n\ndata_init = iterator.make_initializer(data)\ndata_init_repeat = iterator.make_initializer(data_repeat)\n\nsess = tf.Session()\n\nprint('Reading values from iterator by reinitializing')\nepoch = 1\n\nfor _ in range(5):\n    start = time.time()\n    sess.run(data_init)\n    print('Epoch %d:' % epoch, end=' ')\n    for _ in range(LENGTH):\n        output = sess.run(next_element)\n        \n        if VERBOSE:\n            print(output, end=' ')\n        \n    end = time.time()\n    print(\"Time taken: %f\" % (end - start))\n\n    epoch += 1\n\nprint('Reading values from iterator using repeat')\nepoch = 1\nsess.run(data_init_repeat)\n\nfor _ in range(5):\n    start = time.time()\n    print('Epoch %d:' % epoch, end=' ')\n    for _ in range(LENGTH):\n        output = sess.run(next_element)\n        \n        if VERBOSE:\n            print(output, end=' ')\n\n    end = time.time()\n    print(\"Time taken: %f\" % (end - start))\n    \n    epoch += 1\n</code></pre>\n<p>This code gives the following output:</p>\n<blockquote>\n<p>Reading values from iterator by reinitializing<br>\nEpoch 1: [0.00544547] [0.19976228] [0.5371114] Time taken: 0.017011<br>\nEpoch 2: [0.97181207] [0.35276905] [0.69020385] Time taken: 0.003002<br>\nEpoch 3: [0.19892913] [0.2760849] [0.8980513] Time taken: 0.003003<br>\nEpoch 4: [0.92679894] [0.5854017] [0.6552748] Time taken: 0.003002<br>\nEpoch 5: [0.38050508] [0.7676437] [0.41214108] Time taken: 0.003002<br>\nReading values from iterator using repeat<br>\nEpoch 1: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003003<br>\nEpoch 2: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003002<br>\nEpoch 3: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.000000<br>\nEpoch 4: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001001<br>\nEpoch 5: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001000</p>\n</blockquote>\n<p>In the second case, the same values are printed at every epoch because the iterator uses the cached data. However, in the first case, different values are printed at every epoch - indicating that the iterator does not use the cached values. This can be further verified by looking at the time taken (which will be more prominent with larger data). So in the above code, if I set <code>LENGTH = 100</code>, <code>VERBOSE = False</code> and <code>SHAPE = (500, 1000, 3)</code>, I get the following output:</p>\n<blockquote>\n<p>Reading values from iterator by reinitializing<br>\nEpoch 1: Time taken: 2.228584<br>\nEpoch 2: Time taken: 2.182549<br>\nEpoch 3: Time taken: 2.175545<br>\nEpoch 4: Time taken: 2.185551<br>\nEpoch 5: Time taken: 2.161536<br>\nReading values from iterator using repeat<br>\nEpoch 1: Time taken: 2.147525<br>\nEpoch 2: Time taken: 0.263195<br>\nEpoch 3: Time taken: 0.259186<br>\nEpoch 4: Time taken: 0.266189<br>\nEpoch 5: Time taken: 0.268181</p>\n</blockquote>\n<p>In the second case, the first epoch takes considerably longer than the subsequent epochs - understandably, since the further epochs just reuse the cached data. However, in the first case, all the epochs take a long time.</p>\n<p>This seems like a bug to me because the reinitializable iterator has <a href=\"https://www.tensorflow.org/guide/datasets#creating_an_iterator\" rel=\"nofollow\">been advocated</a> as being useful in precisely such kind of situations - when you have 2 different datasets (say training and validation) and you simply reinitialize the iterator to use any one of the two at a time. However, it seems that if I reinitialize the iterator with a cached dataset, I lose the entire advantage of caching. If I set <code>CACHE = False</code> with the large shape mentioned above, I get the following output as expected:</p>\n<blockquote>\n<p>Reading values from iterator by reinitializing<br>\nEpoch 1: Time taken: 2.317660<br>\nEpoch 2: Time taken: 2.251585<br>\nEpoch 3: Time taken: 2.209581<br>\nEpoch 4: Time taken: 2.151515<br>\nEpoch 5: Time taken: 2.156546<br>\nReading values from iterator using repeat<br>\nEpoch 1: Time taken: 2.182563<br>\nEpoch 2: Time taken: 2.207575<br>\nEpoch 3: Time taken: 2.205560<br>\nEpoch 4: Time taken: 2.139524<br>\nEpoch 5: Time taken: 2.198561</p>\n</blockquote>\n<p>That is, each epoch takes the same time with either method since we don't cache the dataset.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, written custom code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 8.1\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8.0\nPython version: 3.6.5\nBazel version: N/A\nCUDA/cuDNN version: CUDA 9.0, cuDNN v7.0\nGPU model and memory: NVIDIA GeForce GTX 960 (faced the same problem with 1080 Ti)\nExact command to reproduce: see below\n\nDescribe the problem\nFound a likely bug when using a reinitialiable iterator with a cached dataset. Upon reinitializing the iterator, the iterator does not use the cached dataset. This can be verified using the sample code below.\nSource code\nimport tensorflow as tf\nimport numpy as np\nimport time\n\nLENGTH = 3\nCACHE = True\nVERBOSE = True\nSHAPE = (1,)\n\ndef data_generator():    \n    for n in range(LENGTH):\n        data = np.random.rand(*SHAPE)\n        yield data\n\ndef get_dataset():\n    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)\n    if CACHE:\n        train_data = train_data.cache()\n    return train_data\n\ndef get_dataset_repeat():\n    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)\n    if CACHE:\n        train_data = train_data.cache()\n    train_data = train_data.repeat()\n    return train_data\n\ndata = get_dataset()\ndata_repeat = get_dataset_repeat()\n\niterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes)\nnext_element = iterator.get_next()\n\ndata_init = iterator.make_initializer(data)\ndata_init_repeat = iterator.make_initializer(data_repeat)\n\nsess = tf.Session()\n\nprint('Reading values from iterator by reinitializing')\nepoch = 1\n\nfor _ in range(5):\n    start = time.time()\n    sess.run(data_init)\n    print('Epoch %d:' % epoch, end=' ')\n    for _ in range(LENGTH):\n        output = sess.run(next_element)\n        \n        if VERBOSE:\n            print(output, end=' ')\n        \n    end = time.time()\n    print(\"Time taken: %f\" % (end - start))\n\n    epoch += 1\n\nprint('Reading values from iterator using repeat')\nepoch = 1\nsess.run(data_init_repeat)\n\nfor _ in range(5):\n    start = time.time()\n    print('Epoch %d:' % epoch, end=' ')\n    for _ in range(LENGTH):\n        output = sess.run(next_element)\n        \n        if VERBOSE:\n            print(output, end=' ')\n\n    end = time.time()\n    print(\"Time taken: %f\" % (end - start))\n    \n    epoch += 1\n\nThis code gives the following output:\n\nReading values from iterator by reinitializing\nEpoch 1: [0.00544547] [0.19976228] [0.5371114] Time taken: 0.017011\nEpoch 2: [0.97181207] [0.35276905] [0.69020385] Time taken: 0.003002\nEpoch 3: [0.19892913] [0.2760849] [0.8980513] Time taken: 0.003003\nEpoch 4: [0.92679894] [0.5854017] [0.6552748] Time taken: 0.003002\nEpoch 5: [0.38050508] [0.7676437] [0.41214108] Time taken: 0.003002\nReading values from iterator using repeat\nEpoch 1: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003003\nEpoch 2: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003002\nEpoch 3: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.000000\nEpoch 4: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001001\nEpoch 5: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001000\n\nIn the second case, the same values are printed at every epoch because the iterator uses the cached data. However, in the first case, different values are printed at every epoch - indicating that the iterator does not use the cached values. This can be further verified by looking at the time taken (which will be more prominent with larger data). So in the above code, if I set LENGTH = 100, VERBOSE = False and SHAPE = (500, 1000, 3), I get the following output:\n\nReading values from iterator by reinitializing\nEpoch 1: Time taken: 2.228584\nEpoch 2: Time taken: 2.182549\nEpoch 3: Time taken: 2.175545\nEpoch 4: Time taken: 2.185551\nEpoch 5: Time taken: 2.161536\nReading values from iterator using repeat\nEpoch 1: Time taken: 2.147525\nEpoch 2: Time taken: 0.263195\nEpoch 3: Time taken: 0.259186\nEpoch 4: Time taken: 0.266189\nEpoch 5: Time taken: 0.268181\n\nIn the second case, the first epoch takes considerably longer than the subsequent epochs - understandably, since the further epochs just reuse the cached data. However, in the first case, all the epochs take a long time.\nThis seems like a bug to me because the reinitializable iterator has been advocated as being useful in precisely such kind of situations - when you have 2 different datasets (say training and validation) and you simply reinitialize the iterator to use any one of the two at a time. However, it seems that if I reinitialize the iterator with a cached dataset, I lose the entire advantage of caching. If I set CACHE = False with the large shape mentioned above, I get the following output as expected:\n\nReading values from iterator by reinitializing\nEpoch 1: Time taken: 2.317660\nEpoch 2: Time taken: 2.251585\nEpoch 3: Time taken: 2.209581\nEpoch 4: Time taken: 2.151515\nEpoch 5: Time taken: 2.156546\nReading values from iterator using repeat\nEpoch 1: Time taken: 2.182563\nEpoch 2: Time taken: 2.207575\nEpoch 3: Time taken: 2.205560\nEpoch 4: Time taken: 2.139524\nEpoch 5: Time taken: 2.198561\n\nThat is, each epoch takes the same time with either method since we don't cache the dataset.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, written custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 8.1\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: CUDA 9.0, cuDNN v7.0\r\n- **GPU model and memory**: NVIDIA GeForce GTX 960 (faced the same problem with 1080 Ti)\r\n- **Exact command to reproduce**: see below\r\n\r\n### Describe the problem\r\nFound a likely bug when using a reinitialiable iterator with a cached dataset. Upon reinitializing the iterator, the iterator does not use the cached dataset. This can be verified using the sample code below.\r\n\r\n### Source code\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\n\r\nLENGTH = 3\r\nCACHE = True\r\nVERBOSE = True\r\nSHAPE = (1,)\r\n\r\ndef data_generator():    \r\n    for n in range(LENGTH):\r\n        data = np.random.rand(*SHAPE)\r\n        yield data\r\n\r\ndef get_dataset():\r\n    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)\r\n    if CACHE:\r\n        train_data = train_data.cache()\r\n    return train_data\r\n\r\ndef get_dataset_repeat():\r\n    train_data = tf.data.Dataset.from_generator(data_generator, output_types=tf.float32)\r\n    if CACHE:\r\n        train_data = train_data.cache()\r\n    train_data = train_data.repeat()\r\n    return train_data\r\n\r\ndata = get_dataset()\r\ndata_repeat = get_dataset_repeat()\r\n\r\niterator = tf.data.Iterator.from_structure(data.output_types, data.output_shapes)\r\nnext_element = iterator.get_next()\r\n\r\ndata_init = iterator.make_initializer(data)\r\ndata_init_repeat = iterator.make_initializer(data_repeat)\r\n\r\nsess = tf.Session()\r\n\r\nprint('Reading values from iterator by reinitializing')\r\nepoch = 1\r\n\r\nfor _ in range(5):\r\n    start = time.time()\r\n    sess.run(data_init)\r\n    print('Epoch %d:' % epoch, end=' ')\r\n    for _ in range(LENGTH):\r\n        output = sess.run(next_element)\r\n        \r\n        if VERBOSE:\r\n            print(output, end=' ')\r\n        \r\n    end = time.time()\r\n    print(\"Time taken: %f\" % (end - start))\r\n\r\n    epoch += 1\r\n\r\nprint('Reading values from iterator using repeat')\r\nepoch = 1\r\nsess.run(data_init_repeat)\r\n\r\nfor _ in range(5):\r\n    start = time.time()\r\n    print('Epoch %d:' % epoch, end=' ')\r\n    for _ in range(LENGTH):\r\n        output = sess.run(next_element)\r\n        \r\n        if VERBOSE:\r\n            print(output, end=' ')\r\n\r\n    end = time.time()\r\n    print(\"Time taken: %f\" % (end - start))\r\n    \r\n    epoch += 1\r\n```\r\n\r\nThis code gives the following output:\r\n\r\n> Reading values from iterator by reinitializing\r\n> Epoch 1: [0.00544547] [0.19976228] [0.5371114] Time taken: 0.017011\r\n> Epoch 2: [0.97181207] [0.35276905] [0.69020385] Time taken: 0.003002\r\n> Epoch 3: [0.19892913] [0.2760849] [0.8980513] Time taken: 0.003003\r\n> Epoch 4: [0.92679894] [0.5854017] [0.6552748] Time taken: 0.003002\r\n> Epoch 5: [0.38050508] [0.7676437] [0.41214108] Time taken: 0.003002\r\n> Reading values from iterator using repeat\r\n> Epoch 1: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003003\r\n> Epoch 2: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.003002\r\n> Epoch 3: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.000000\r\n> Epoch 4: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001001\r\n> Epoch 5: [0.28107443] [0.8882484] [0.59744525] Time taken: 0.001000\r\n\r\nIn the second case, the same values are printed at every epoch because the iterator uses the cached data. However, in the first case, different values are printed at every epoch - indicating that the iterator does not use the cached values. This can be further verified by looking at the time taken (which will be more prominent with larger data). So in the above code, if I set `LENGTH = 100`, `VERBOSE = False` and `SHAPE = (500, 1000, 3)`, I get the following output:\r\n\r\n> Reading values from iterator by reinitializing\r\n> Epoch 1: Time taken: 2.228584\r\n> Epoch 2: Time taken: 2.182549\r\n> Epoch 3: Time taken: 2.175545\r\n> Epoch 4: Time taken: 2.185551\r\n> Epoch 5: Time taken: 2.161536\r\n> Reading values from iterator using repeat\r\n> Epoch 1: Time taken: 2.147525\r\n> Epoch 2: Time taken: 0.263195\r\n> Epoch 3: Time taken: 0.259186\r\n> Epoch 4: Time taken: 0.266189\r\n> Epoch 5: Time taken: 0.268181\r\n\r\nIn the second case, the first epoch takes considerably longer than the subsequent epochs - understandably, since the further epochs just reuse the cached data. However, in the first case, all the epochs take a long time.\r\n\r\nThis seems like a bug to me because the reinitializable iterator has [been advocated](https://www.tensorflow.org/guide/datasets#creating_an_iterator) as being useful in precisely such kind of situations - when you have 2 different datasets (say training and validation) and you simply reinitialize the iterator to use any one of the two at a time. However, it seems that if I reinitialize the iterator with a cached dataset, I lose the entire advantage of caching. If I set `CACHE = False` with the large shape mentioned above, I get the following output as expected:\r\n\r\n> Reading values from iterator by reinitializing\r\n> Epoch 1: Time taken: 2.317660\r\n> Epoch 2: Time taken: 2.251585\r\n> Epoch 3: Time taken: 2.209581\r\n> Epoch 4: Time taken: 2.151515\r\n> Epoch 5: Time taken: 2.156546\r\n> Reading values from iterator using repeat\r\n> Epoch 1: Time taken: 2.182563\r\n> Epoch 2: Time taken: 2.207575\r\n> Epoch 3: Time taken: 2.205560\r\n> Epoch 4: Time taken: 2.139524\r\n> Epoch 5: Time taken: 2.198561\r\n\r\nThat is, each epoch takes the same time with either method since we don't cache the dataset."}