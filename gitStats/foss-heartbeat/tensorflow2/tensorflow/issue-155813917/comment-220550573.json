{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/220550573", "html_url": "https://github.com/tensorflow/tensorflow/pull/2435#issuecomment-220550573", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2435", "id": 220550573, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMDU1MDU3Mw==", "user": {"login": "sjperkins", "id": 3530212, "node_id": "MDQ6VXNlcjM1MzAyMTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/3530212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sjperkins", "html_url": "https://github.com/sjperkins", "followers_url": "https://api.github.com/users/sjperkins/followers", "following_url": "https://api.github.com/users/sjperkins/following{/other_user}", "gists_url": "https://api.github.com/users/sjperkins/gists{/gist_id}", "starred_url": "https://api.github.com/users/sjperkins/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sjperkins/subscriptions", "organizations_url": "https://api.github.com/users/sjperkins/orgs", "repos_url": "https://api.github.com/users/sjperkins/repos", "events_url": "https://api.github.com/users/sjperkins/events{/privacy}", "received_events_url": "https://api.github.com/users/sjperkins/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-20T08:48:35Z", "updated_at": "2016-05-20T10:54:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi there! I was wondering if this pull request adds support for complex number types to <a href=\"https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#tensor-transformations\" rel=\"nofollow\">Tensor Transformation Ops</a> to the GPU.</p>\n<p>I was testing this <a href=\"https://github.com/ska-sa/montblanc/blob/867afab028e59c6675ef39d7e89b15fe2b250401/montblanc/tensorflow/rime_ops/test_b_sqrt.py#L114\">code</a> on last night's GPU build and ran into</p>\n<div class=\"highlight highlight-source-python\"><pre>tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node <span class=\"pl-s\"><span class=\"pl-pds\">'</span>pack_7<span class=\"pl-pds\">'</span></span>: Could <span class=\"pl-k\">not</span> satisfy explicit device specification <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:GPU:0<span class=\"pl-pds\">'</span></span> because no supported kernel <span class=\"pl-k\">for</span> <span class=\"pl-c1\">GPU</span> devices <span class=\"pl-k\">is</span> available\n     [[Node: pack_7 = Pack[N=<span class=\"pl-c1\">4</span>, T=<span class=\"pl-c1\">DT_COMPLEX64</span>, _device=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/device:GPU:0<span class=\"pl-pds\">\"</span></span>](Complex_4, Complex_5, Complex_6, Complex_7)]]</pre></div>\n<p>for complex64 and</p>\n<div class=\"highlight highlight-source-python\"><pre>tensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node <span class=\"pl-s\"><span class=\"pl-pds\">'</span>pack_7<span class=\"pl-pds\">'</span></span>: Could <span class=\"pl-k\">not</span> satisfy explicit device specification <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/device:GPU:0<span class=\"pl-pds\">'</span></span> because no supported kernel <span class=\"pl-k\">for</span> <span class=\"pl-c1\">GPU</span> devices <span class=\"pl-k\">is</span> available\n     [[Node: pack_7 = Pack[N=<span class=\"pl-c1\">4</span>, T=<span class=\"pl-c1\">DT_COMPLEX128</span>, _device=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/device:GPU:0<span class=\"pl-pds\">\"</span></span>](Complex_4, Complex_5, Complex_6, Complex_7)]]</pre></div>\n<p>for complex128.</p>\n<p>Let me know I should open a separate issue/PR for this.</p>\n<p><strong>EDIT:</strong> <em>On second thoughts, implementing generic packing and transposes on a GPU is not a trivial operation, I might be expecting too much from these ops on the GPU, for all types.</em></p>", "body_text": "Hi there! I was wondering if this pull request adds support for complex number types to Tensor Transformation Ops to the GPU.\nI was testing this code on last night's GPU build and ran into\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'pack_7': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: pack_7 = Pack[N=4, T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex_4, Complex_5, Complex_6, Complex_7)]]\nfor complex64 and\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'pack_7': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: pack_7 = Pack[N=4, T=DT_COMPLEX128, _device=\"/device:GPU:0\"](Complex_4, Complex_5, Complex_6, Complex_7)]]\nfor complex128.\nLet me know I should open a separate issue/PR for this.\nEDIT: On second thoughts, implementing generic packing and transposes on a GPU is not a trivial operation, I might be expecting too much from these ops on the GPU, for all types.", "body": "Hi there! I was wondering if this pull request adds support for complex number types to [Tensor Transformation Ops](https://www.tensorflow.org/versions/r0.8/api_docs/python/array_ops.html#tensor-transformations) to the GPU.\n\nI was testing this [code](https://github.com/ska-sa/montblanc/blob/867afab028e59c6675ef39d7e89b15fe2b250401/montblanc/tensorflow/rime_ops/test_b_sqrt.py#L114) on last night's GPU build and ran into\n\n``` python\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'pack_7': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: pack_7 = Pack[N=4, T=DT_COMPLEX64, _device=\"/device:GPU:0\"](Complex_4, Complex_5, Complex_6, Complex_7)]]\n```\n\nfor complex64 and\n\n``` python\ntensorflow.python.framework.errors.InvalidArgumentError: Cannot assign a device to node 'pack_7': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available\n     [[Node: pack_7 = Pack[N=4, T=DT_COMPLEX128, _device=\"/device:GPU:0\"](Complex_4, Complex_5, Complex_6, Complex_7)]]\n```\n\nfor complex128.\n\nLet me know I should open a separate issue/PR for this.\n\n**EDIT:** _On second thoughts, implementing generic packing and transposes on a GPU is not a trivial operation, I might be expecting too much from these ops on the GPU, for all types._\n"}