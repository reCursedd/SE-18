{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219467993", "html_url": "https://github.com/tensorflow/tensorflow/issues/2311#issuecomment-219467993", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2311", "id": 219467993, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTQ2Nzk5Mw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-16T16:11:21Z", "updated_at": "2016-05-16T16:11:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The assign op is not consuming memory, but the problem is caused by the fact that each instance of <code>new_weights</code> is converted to a constant op, and added to the graph. Each constant op owns a buffer containing the value that it produces, and a constant op on the GPU device will allocate that buffer in GPU memory.</p>\n<p>The fix is to rewrite your program somewhat. Instead of doing:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3000</span>):\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Assigning i:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(i)\n    sess.run(w1.assign(new_value_array))</pre></div>\n<p>... you should declare the assign op and a placeholder before the loop, and feed different values to the placeholder in each iteration:</p>\n<div class=\"highlight highlight-source-python\"><pre>assign_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>])\nassign_op <span class=\"pl-k\">=</span> w1.assign(assign_placeholder)\n\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">3000</span>):\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Assigning i:<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(i)\n    sess.run(assign_op, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{assign_placeholder: new_value_array})</pre></div>", "body_text": "The assign op is not consuming memory, but the problem is caused by the fact that each instance of new_weights is converted to a constant op, and added to the graph. Each constant op owns a buffer containing the value that it produces, and a constant op on the GPU device will allocate that buffer in GPU memory.\nThe fix is to rewrite your program somewhat. Instead of doing:\nfor i in range(3000):\n    print \"Assigning i:{}\".format(i)\n    sess.run(w1.assign(new_value_array))\n... you should declare the assign op and a placeholder before the loop, and feed different values to the placeholder in each iteration:\nassign_placeholder = tf.placeholder(tf.float32, shape=[1000, 1000])\nassign_op = w1.assign(assign_placeholder)\n\nfor i in range(3000):\n    print \"Assigning i:{}\".format(i)\n    sess.run(assign_op, feed_dict={assign_placeholder: new_value_array})", "body": "The assign op is not consuming memory, but the problem is caused by the fact that each instance of `new_weights` is converted to a constant op, and added to the graph. Each constant op owns a buffer containing the value that it produces, and a constant op on the GPU device will allocate that buffer in GPU memory.\n\nThe fix is to rewrite your program somewhat. Instead of doing:\n\n``` python\nfor i in range(3000):\n    print \"Assigning i:{}\".format(i)\n    sess.run(w1.assign(new_value_array))\n```\n\n... you should declare the assign op and a placeholder before the loop, and feed different values to the placeholder in each iteration:\n\n``` python\nassign_placeholder = tf.placeholder(tf.float32, shape=[1000, 1000])\nassign_op = w1.assign(assign_placeholder)\n\nfor i in range(3000):\n    print \"Assigning i:{}\".format(i)\n    sess.run(assign_op, feed_dict={assign_placeholder: new_value_array})\n```\n"}