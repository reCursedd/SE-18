{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2311", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2311/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2311/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2311/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2311", "id": 154079139, "node_id": "MDU6SXNzdWUxNTQwNzkxMzk=", "number": 2311, "title": "Calling variable.assign() too many times crashes on memory allocation.", "user": {"login": "jhollowayj", "id": 7515474, "node_id": "MDQ6VXNlcjc1MTU0NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7515474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhollowayj", "html_url": "https://github.com/jhollowayj", "followers_url": "https://api.github.com/users/jhollowayj/followers", "following_url": "https://api.github.com/users/jhollowayj/following{/other_user}", "gists_url": "https://api.github.com/users/jhollowayj/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhollowayj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhollowayj/subscriptions", "organizations_url": "https://api.github.com/users/jhollowayj/orgs", "repos_url": "https://api.github.com/users/jhollowayj/repos", "events_url": "https://api.github.com/users/jhollowayj/events{/privacy}", "received_events_url": "https://api.github.com/users/jhollowayj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-05-10T18:49:57Z", "updated_at": "2016-05-17T01:16:47Z", "closed_at": "2016-05-16T16:11:21Z", "author_association": "NONE", "body_html": "<p><strong>Background</strong>: I'm working on a set of networks that only share some layers, so I have a parameter server that sends new weights for the different clients to use.  These clients accept the new weights and bias for the layers they are using and assign the values to the TF.Variables via <code>sess.run(self.w1.assign(new_weights))</code>.  However, when I start it up and let it run, it crashes saying</p>\n<pre><code>W tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 16B.  See logs for memory state.\n</code></pre>\n<p>(Sometimes it's allocating 16B, other times its 3.9KiB)</p>\n<p>To give you an idea of the size of the weights, I have three layers of:<br>\nLayer 1(W,b): <code>(2, 1000), (1000, )</code><br>\nLayer 2(W,b): <code>(1000, 1000), (1000, )</code><br>\nLayer 3(W,b): <code>(1000, 4), (4, )</code><br>\nI'm running on a Titan X with 12G memory.</p>\n<p>With <code>per_process_gpu_memory_fraction = 0.01</code>, the program dies at ~190 assign commands.<br>\nWith <code>per_process_gpu_memory_fraction = 0.02</code>, the program dies at ~384 assign commands.<br>\nWith <code>per_process_gpu_memory_fraction = 0.03</code>, the program dies at ~780 assign commands.<br>\nWith <code>per_process_gpu_memory_fraction = 0.04</code>, the program dies at ~784 assign commands.<br>\nWith <code>per_process_gpu_memory_fraction = 0.05</code>, the program dies at ~1582 assign commands.<br>\nWith <code>per_process_gpu_memory_fraction = 0.06</code>, the program dies at ~1586 assign commands.</p>\n<p>I've tried to set <code>allow_growth=True</code>, and <code>deferred_deletion_bytes=1</code> in the session's GPUOptions after reading issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"142647609\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1578\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1578/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1578\">#1578</a>, but that didn't get me much further.  (I have no idea what <code>deferred_deletion_bytes</code> does...)  Looking at the numbers just above (GPU%vsAssignmentCommands), it seems to be fairly linear, so it seems to me that the assign operation takes some of the GPU ram and it's never freed up.  <strong>Is there any sense of GC on the GPU memory allocated durring the <code>var.assign()</code> op?</strong></p>\n<p>It seems that I could delete and create a new session, but that sounds expensive to me, and I'd have to maintain the weights outside of session to be able to restore them correctly.  The second idea I had would to use placeholders and ship the weights in every time with the feed_dict, but again, that seems less that ideal and I think it would struggle in the optimizer on knowing what to optimize if they are just placeholders.</p>\n<p>Let me know if you would like any other logs or reports from me.  I figure this is the first time someone has tried to use assign operations like this, so I want to be helpful in fixing it if it's a bug.</p>\n<p>Thanks</p>\n<h3>Environment info</h3>\n<p><strong>Operating System:</strong> Ubuntu 16.04<br>\n<strong>Installed version of CUDA and cuDNN:</strong><br>\n/usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.0<br>\n/usr/local/cuda/lib/libcudart.so.7.0 -&gt; libcudart.so.7.0.28<br>\n/usr/local/cuda/lib/libcudart.so.7.0.28<br>\n/usr/local/cuda/lib/libcudart_static.a<br>\n<strong>Built from source.  Commit hash:</strong>  <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/35cd6a30112abd4302a8b3a17dda277899f1ed40/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/35cd6a30112abd4302a8b3a17dda277899f1ed40\"><tt>35cd6a3</tt></a></p>", "body_text": "Background: I'm working on a set of networks that only share some layers, so I have a parameter server that sends new weights for the different clients to use.  These clients accept the new weights and bias for the layers they are using and assign the values to the TF.Variables via sess.run(self.w1.assign(new_weights)).  However, when I start it up and let it run, it crashes saying\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 16B.  See logs for memory state.\n\n(Sometimes it's allocating 16B, other times its 3.9KiB)\nTo give you an idea of the size of the weights, I have three layers of:\nLayer 1(W,b): (2, 1000), (1000, )\nLayer 2(W,b): (1000, 1000), (1000, )\nLayer 3(W,b): (1000, 4), (4, )\nI'm running on a Titan X with 12G memory.\nWith per_process_gpu_memory_fraction = 0.01, the program dies at ~190 assign commands.\nWith per_process_gpu_memory_fraction = 0.02, the program dies at ~384 assign commands.\nWith per_process_gpu_memory_fraction = 0.03, the program dies at ~780 assign commands.\nWith per_process_gpu_memory_fraction = 0.04, the program dies at ~784 assign commands.\nWith per_process_gpu_memory_fraction = 0.05, the program dies at ~1582 assign commands.\nWith per_process_gpu_memory_fraction = 0.06, the program dies at ~1586 assign commands.\nI've tried to set allow_growth=True, and deferred_deletion_bytes=1 in the session's GPUOptions after reading issue #1578, but that didn't get me much further.  (I have no idea what deferred_deletion_bytes does...)  Looking at the numbers just above (GPU%vsAssignmentCommands), it seems to be fairly linear, so it seems to me that the assign operation takes some of the GPU ram and it's never freed up.  Is there any sense of GC on the GPU memory allocated durring the var.assign() op?\nIt seems that I could delete and create a new session, but that sounds expensive to me, and I'd have to maintain the weights outside of session to be able to restore them correctly.  The second idea I had would to use placeholders and ship the weights in every time with the feed_dict, but again, that seems less that ideal and I think it would struggle in the optimizer on knowing what to optimize if they are just placeholders.\nLet me know if you would like any other logs or reports from me.  I figure this is the first time someone has tried to use assign operations like this, so I want to be helpful in fixing it if it's a bug.\nThanks\nEnvironment info\nOperating System: Ubuntu 16.04\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda/lib/libcudart.so -> libcudart.so.7.0\n/usr/local/cuda/lib/libcudart.so.7.0 -> libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart_static.a\nBuilt from source.  Commit hash:  35cd6a3", "body": "**Background**: I'm working on a set of networks that only share some layers, so I have a parameter server that sends new weights for the different clients to use.  These clients accept the new weights and bias for the layers they are using and assign the values to the TF.Variables via `sess.run(self.w1.assign(new_weights))`.  However, when I start it up and let it run, it crashes saying \n\n```\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 16B.  See logs for memory state.\n```\n\n(Sometimes it's allocating 16B, other times its 3.9KiB)\n\nTo give you an idea of the size of the weights, I have three layers of: \nLayer 1(W,b): `(2, 1000), (1000, )`\nLayer 2(W,b): `(1000, 1000), (1000, )`\nLayer 3(W,b): `(1000, 4), (4, )`\nI'm running on a Titan X with 12G memory.\n\nWith `per_process_gpu_memory_fraction = 0.01`, the program dies at ~190 assign commands.\nWith `per_process_gpu_memory_fraction = 0.02`, the program dies at ~384 assign commands.\nWith `per_process_gpu_memory_fraction = 0.03`, the program dies at ~780 assign commands.\nWith `per_process_gpu_memory_fraction = 0.04`, the program dies at ~784 assign commands.\nWith `per_process_gpu_memory_fraction = 0.05`, the program dies at ~1582 assign commands.\nWith `per_process_gpu_memory_fraction = 0.06`, the program dies at ~1586 assign commands.\n\nI've tried to set `allow_growth=True`, and `deferred_deletion_bytes=1` in the session's GPUOptions after reading issue #1578, but that didn't get me much further.  (I have no idea what `deferred_deletion_bytes` does...)  Looking at the numbers just above (GPU%vsAssignmentCommands), it seems to be fairly linear, so it seems to me that the assign operation takes some of the GPU ram and it's never freed up.  **Is there any sense of GC on the GPU memory allocated durring the `var.assign()` op?**\n\nIt seems that I could delete and create a new session, but that sounds expensive to me, and I'd have to maintain the weights outside of session to be able to restore them correctly.  The second idea I had would to use placeholders and ship the weights in every time with the feed_dict, but again, that seems less that ideal and I think it would struggle in the optimizer on knowing what to optimize if they are just placeholders.\n\nLet me know if you would like any other logs or reports from me.  I figure this is the first time someone has tried to use assign operations like this, so I want to be helpful in fixing it if it's a bug.\n\nThanks\n### Environment info\n\n**Operating System:** Ubuntu 16.04\n**Installed version of CUDA and cuDNN:**\n/usr/local/cuda/lib/libcudart.so -> libcudart.so.7.0\n/usr/local/cuda/lib/libcudart.so.7.0 -> libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart.so.7.0.28\n/usr/local/cuda/lib/libcudart_static.a\n**Built from source.  Commit hash:**  35cd6a30112abd4302a8b3a17dda277899f1ed40\n"}