{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275425702", "html_url": "https://github.com/tensorflow/tensorflow/issues/7091#issuecomment-275425702", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7091", "id": 275425702, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTQyNTcwMg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-26T15:56:19Z", "updated_at": "2017-01-26T15:56:19Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It seems useful, ie, there's <a href=\"https://arxiv.org/abs/1607.06450\" rel=\"nofollow\">Layer Normalization</a> which got good performance which is transpose of batch normalization, and <a href=\"https://arxiv.org/abs/1602.07868\" rel=\"nofollow\">Weight Normalization</a></p>\n<p>I wonder if it should be called something else though, since <code>batch</code> in <code>batch_norm</code> refers to batch dimension (0'th dimension).</p>\n<p>I don't see a way to specify axis in <code>tf.nb.batch_normalization</code></p>\n<p><code>tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)</code></p>", "body_text": "It seems useful, ie, there's Layer Normalization which got good performance which is transpose of batch normalization, and Weight Normalization\nI wonder if it should be called something else though, since batch in batch_norm refers to batch dimension (0'th dimension).\nI don't see a way to specify axis in tf.nb.batch_normalization\ntf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)", "body": "It seems useful, ie, there's [Layer Normalization](https://arxiv.org/abs/1607.06450) which got good performance which is transpose of batch normalization, and [Weight Normalization](https://arxiv.org/abs/1602.07868)\r\n\r\n I wonder if it should be called something else though, since `batch` in `batch_norm` refers to batch dimension (0'th dimension).\r\n\r\nI don't see a way to specify axis in `tf.nb.batch_normalization`\r\n\r\n`tf.nn.batch_normalization(x, mean, variance, offset, scale, variance_epsilon, name=None)`"}