{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239869299", "html_url": "https://github.com/tensorflow/tensorflow/issues/3350#issuecomment-239869299", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3350", "id": 239869299, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTg2OTI5OQ==", "user": {"login": "harpone", "id": 5112840, "node_id": "MDQ6VXNlcjUxMTI4NDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5112840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harpone", "html_url": "https://github.com/harpone", "followers_url": "https://api.github.com/users/harpone/followers", "following_url": "https://api.github.com/users/harpone/following{/other_user}", "gists_url": "https://api.github.com/users/harpone/gists{/gist_id}", "starred_url": "https://api.github.com/users/harpone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harpone/subscriptions", "organizations_url": "https://api.github.com/users/harpone/orgs", "repos_url": "https://api.github.com/users/harpone/repos", "events_url": "https://api.github.com/users/harpone/events{/privacy}", "received_events_url": "https://api.github.com/users/harpone/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-15T17:32:24Z", "updated_at": "2016-08-15T17:33:34Z", "author_association": "NONE", "body_html": "<p>Well basically LU decomposition would be used for computing the <em>value</em> of the logdet(M), while there's an explicit expression for the logdet gradient as inv(transpose(M))... so it would be enough if there was a fast GPU op for e.g. LU decomposition (there was some talk in some of the issues about SVD op coming soon, which would also do the trick). Then it would be simple to define a tf op for the logdet and register the gradient as the inverse transpose. So the LU/cholesky etc. wouldn't need to be differentiable (I actually don't know what's the point in having a differentiable Cholesky etc., but then again I haven't read the paper cited in the other issue).</p>\n<p>If an LU/SVD op comes up, I could probably work out the logdet...</p>\n<p>EDIT: also, another issue could be the numerical stability of the tf.inverse... :/ Haven't thought about that yet</p>", "body_text": "Well basically LU decomposition would be used for computing the value of the logdet(M), while there's an explicit expression for the logdet gradient as inv(transpose(M))... so it would be enough if there was a fast GPU op for e.g. LU decomposition (there was some talk in some of the issues about SVD op coming soon, which would also do the trick). Then it would be simple to define a tf op for the logdet and register the gradient as the inverse transpose. So the LU/cholesky etc. wouldn't need to be differentiable (I actually don't know what's the point in having a differentiable Cholesky etc., but then again I haven't read the paper cited in the other issue).\nIf an LU/SVD op comes up, I could probably work out the logdet...\nEDIT: also, another issue could be the numerical stability of the tf.inverse... :/ Haven't thought about that yet", "body": "Well basically LU decomposition would be used for computing the _value_ of the logdet(M), while there's an explicit expression for the logdet gradient as inv(transpose(M))... so it would be enough if there was a fast GPU op for e.g. LU decomposition (there was some talk in some of the issues about SVD op coming soon, which would also do the trick). Then it would be simple to define a tf op for the logdet and register the gradient as the inverse transpose. So the LU/cholesky etc. wouldn't need to be differentiable (I actually don't know what's the point in having a differentiable Cholesky etc., but then again I haven't read the paper cited in the other issue).\n\nIf an LU/SVD op comes up, I could probably work out the logdet...\n\nEDIT: also, another issue could be the numerical stability of the tf.inverse... :/ Haven't thought about that yet\n"}