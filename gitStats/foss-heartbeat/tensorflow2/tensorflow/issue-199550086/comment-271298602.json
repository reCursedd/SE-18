{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271298602", "html_url": "https://github.com/tensorflow/tensorflow/issues/6739#issuecomment-271298602", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6739", "id": 271298602, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTI5ODYwMg==", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-09T14:33:29Z", "updated_at": "2017-01-09T14:33:29Z", "author_association": "NONE", "body_html": "<p>Also find another error, using adagrad with latest master code which used to work with release 0.12</p>\n<p>File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 275, in optimize_loss<br>\ngradients, global_step=global_step, name=\"train\")<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 409, in apply_gradients<br>\nself._create_slots(var_list)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots<br>\nself._get_or_make_slot(v, val, \"accumulator\", self._name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 592, in _get_or_make_slot<br>\nnamed_slots[var] = slot_creator.create_slot(var, val, op_name)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot<br>\nreturn _create_slot_var(primary, val, '')<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var<br>\nslot = variable_scope.get_variable(scope, initializer=val, trainable=False)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 987, in get_variable<br>\ncustom_getter=custom_getter)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable<br>\ncustom_getter=custom_getter)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable<br>\nvalidate_shape=validate_shape)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter<br>\ncaching_device=caching_device, validate_shape=validate_shape)<br>\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 656, in _get_single_variable<br>\n\"VarScope?\" % name)<br>\nValueError: Variable bow/OptimizeLoss/bow/model_init/emb/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?</p>", "body_text": "Also find another error, using adagrad with latest master code which used to work with release 0.12\nFile \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 275, in optimize_loss\ngradients, global_step=global_step, name=\"train\")\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 409, in apply_gradients\nself._create_slots(var_list)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\nself._get_or_make_slot(v, val, \"accumulator\", self._name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 592, in _get_or_make_slot\nnamed_slots[var] = slot_creator.create_slot(var, val, op_name)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\nreturn _create_slot_var(primary, val, '')\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\nslot = variable_scope.get_variable(scope, initializer=val, trainable=False)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 987, in get_variable\ncustom_getter=custom_getter)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\ncustom_getter=custom_getter)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable\nvalidate_shape=validate_shape)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\ncaching_device=caching_device, validate_shape=validate_shape)\nFile \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 656, in _get_single_variable\n\"VarScope?\" % name)\nValueError: Variable bow/OptimizeLoss/bow/model_init/emb/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?", "body": "Also find another error, using adagrad with latest master code which used to work with release 0.12\r\n\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/contrib/layers/python/layers/optimizers.py\", line 275, in optimize_loss\r\n    gradients, global_step=global_step, name=\"train\")\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 409, in apply_gradients\r\n    self._create_slots(var_list)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/adagrad.py\", line 66, in _create_slots\r\n    self._get_or_make_slot(v, val, \"accumulator\", self._name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 592, in _get_or_make_slot\r\n    named_slots[var] = slot_creator.create_slot(var, val, op_name)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 101, in create_slot\r\n    return _create_slot_var(primary, val, '')\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/training/slot_creator.py\", line 55, in _create_slot_var\r\n    slot = variable_scope.get_variable(scope, initializer=val, trainable=False)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 987, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 889, in get_variable\r\n    custom_getter=custom_getter)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 347, in get_variable\r\n    validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 332, in _true_getter\r\n    caching_device=caching_device, validate_shape=validate_shape)\r\n  File \"/usr/lib/python2.7/site-packages/tensorflow/python/ops/variable_scope.py\", line 656, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable bow/OptimizeLoss/bow/model_init/emb/Adagrad/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n"}