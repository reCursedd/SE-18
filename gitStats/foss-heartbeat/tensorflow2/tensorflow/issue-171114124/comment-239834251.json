{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/239834251", "html_url": "https://github.com/tensorflow/tensorflow/issues/3812#issuecomment-239834251", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3812", "id": 239834251, "node_id": "MDEyOklzc3VlQ29tbWVudDIzOTgzNDI1MQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-15T15:33:00Z", "updated_at": "2016-08-15T15:33:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In general, the <code>tf.train.Saver</code> code assumes that the worker and parameter server jobs share the same filesystem. For example, you could use a shared NFS mount, or you could use the support for Google Cloud Storage; support for more filesystems - such as HDFS,, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"153008610\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2218\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2218/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2218\">#2218</a> - is in development.</p>\n<p>In the special case that you are using the same parameter server hosts when restoring, you can try passing <code>sharded=True</code> to the saver constructor, and each parameter server will write its shard of the parameters to its local filesystem. It should then be able to restore from that path as well.</p>", "body_text": "In general, the tf.train.Saver code assumes that the worker and parameter server jobs share the same filesystem. For example, you could use a shared NFS mount, or you could use the support for Google Cloud Storage; support for more filesystems - such as HDFS,, #2218 - is in development.\nIn the special case that you are using the same parameter server hosts when restoring, you can try passing sharded=True to the saver constructor, and each parameter server will write its shard of the parameters to its local filesystem. It should then be able to restore from that path as well.", "body": "In general, the `tf.train.Saver` code assumes that the worker and parameter server jobs share the same filesystem. For example, you could use a shared NFS mount, or you could use the support for Google Cloud Storage; support for more filesystems - such as HDFS,, #2218 - is in development.\n\nIn the special case that you are using the same parameter server hosts when restoring, you can try passing `sharded=True` to the saver constructor, and each parameter server will write its shard of the parameters to its local filesystem. It should then be able to restore from that path as well.\n"}