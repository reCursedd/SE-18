{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4193", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4193/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4193/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4193/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4193", "id": 174950040, "node_id": "MDU6SXNzdWUxNzQ5NTAwNDA=", "number": 4193, "title": "CTC loss is numerically unstable for long sequence lengths", "user": {"login": "andykitchen", "id": 29637, "node_id": "MDQ6VXNlcjI5NjM3", "avatar_url": "https://avatars3.githubusercontent.com/u/29637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andykitchen", "html_url": "https://github.com/andykitchen", "followers_url": "https://api.github.com/users/andykitchen/followers", "following_url": "https://api.github.com/users/andykitchen/following{/other_user}", "gists_url": "https://api.github.com/users/andykitchen/gists{/gist_id}", "starred_url": "https://api.github.com/users/andykitchen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andykitchen/subscriptions", "organizations_url": "https://api.github.com/users/andykitchen/orgs", "repos_url": "https://api.github.com/users/andykitchen/repos", "events_url": "https://api.github.com/users/andykitchen/events{/privacy}", "received_events_url": "https://api.github.com/users/andykitchen/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 15, "created_at": "2016-09-04T13:54:11Z", "updated_at": "2018-08-23T07:18:20Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>Hi All, great work TF team, thanks for implementing CTC loss ops recently!</p>\n<p>I've noticed that CTC loss becomes quite numerically unstable for long sequences, in the attached code that reproduces the plot from the CTC paper, sequences longer than 10,000 or so degrade the quality of the gradients quite severely.</p>\n<p>I'm not sure what the best fix is, trying to improve the numerical stability of CTC loss calculation could help, but I have no idea how difficult this would be.</p>\n<p>In the interim, I suppose it might be worth issuing a warning when the op is used with long sequences, or placing internal consistency checks that warn the user when CTC loss may be producing bogus outputs. Or even just improving the documentation for CTC loss so that users know that this is a pitfall.</p>\n<p>This is also sort of a public service announcement so nobody gets burned on long sequences.</p>\n<p>Again, great work, thanks for all your effort.</p>\n<p>:)</p>\n<h3>Environment info</h3>\n<p>Pip Package Version: 0.10.0rc0</p>\n<h3>Minimal Example</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">%</span>matplotlib inline\n<span class=\"pl-k\">import</span> matplotlib.pyplot <span class=\"pl-k\">as</span> plt\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> n = 100</span>\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20000</span>\nk <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    inputs <span class=\"pl-k\">=</span> tf.zeros((n, <span class=\"pl-c1\">1</span>, k<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    labels_indices <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">4</span>]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\n    labels_values  <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n    labels <span class=\"pl-k\">=</span> tf.SparseTensor(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>labels_indices, <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>labels_values, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, k<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>))\n    sequence_length <span class=\"pl-k\">=</span> np.array([n])\n\n    loss <span class=\"pl-k\">=</span> tf.nn.ctc_loss(inputs, labels, sequence_length)\n    g, <span class=\"pl-k\">=</span> tf.gradients(loss, inputs)\n\n    g_v <span class=\"pl-k\">=</span> sess.run(g)\n\nplt.plot(<span class=\"pl-k\">-</span>g_v[:,<span class=\"pl-c1\">0</span>,:])</pre></div>", "body_text": "Hi All, great work TF team, thanks for implementing CTC loss ops recently!\nI've noticed that CTC loss becomes quite numerically unstable for long sequences, in the attached code that reproduces the plot from the CTC paper, sequences longer than 10,000 or so degrade the quality of the gradients quite severely.\nI'm not sure what the best fix is, trying to improve the numerical stability of CTC loss calculation could help, but I have no idea how difficult this would be.\nIn the interim, I suppose it might be worth issuing a warning when the op is used with long sequences, or placing internal consistency checks that warn the user when CTC loss may be producing bogus outputs. Or even just improving the documentation for CTC loss so that users know that this is a pitfall.\nThis is also sort of a public service announcement so nobody gets burned on long sequences.\nAgain, great work, thanks for all your effort.\n:)\nEnvironment info\nPip Package Version: 0.10.0rc0\nMinimal Example\nimport numpy as np\nimport tensorflow as tf\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# n = 100\nn = 20000\nk = 4\n\nwith tf.Session() as sess:\n    inputs = tf.zeros((n, 1, k+1), dtype=tf.float32)\n    labels_indices = tf.constant([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]], dtype=tf.int64)\n    labels_values  = tf.constant([0, 1, 2, 3, 4], dtype=tf.int32)\n    labels = tf.SparseTensor(indices=labels_indices, values=labels_values, shape=(1, k+1))\n    sequence_length = np.array([n])\n\n    loss = tf.nn.ctc_loss(inputs, labels, sequence_length)\n    g, = tf.gradients(loss, inputs)\n\n    g_v = sess.run(g)\n\nplt.plot(-g_v[:,0,:])", "body": "Hi All, great work TF team, thanks for implementing CTC loss ops recently!\n\nI've noticed that CTC loss becomes quite numerically unstable for long sequences, in the attached code that reproduces the plot from the CTC paper, sequences longer than 10,000 or so degrade the quality of the gradients quite severely.\n\nI'm not sure what the best fix is, trying to improve the numerical stability of CTC loss calculation could help, but I have no idea how difficult this would be.\n\nIn the interim, I suppose it might be worth issuing a warning when the op is used with long sequences, or placing internal consistency checks that warn the user when CTC loss may be producing bogus outputs. Or even just improving the documentation for CTC loss so that users know that this is a pitfall.\n\nThis is also sort of a public service announcement so nobody gets burned on long sequences.\n\nAgain, great work, thanks for all your effort.\n\n:)\n### Environment info\n\nPip Package Version: 0.10.0rc0\n### Minimal Example\n\n``` python\nimport numpy as np\nimport tensorflow as tf\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\n# n = 100\nn = 20000\nk = 4\n\nwith tf.Session() as sess:\n    inputs = tf.zeros((n, 1, k+1), dtype=tf.float32)\n    labels_indices = tf.constant([[0, 0], [0, 1], [0, 2], [0, 3], [0, 4]], dtype=tf.int64)\n    labels_values  = tf.constant([0, 1, 2, 3, 4], dtype=tf.int32)\n    labels = tf.SparseTensor(indices=labels_indices, values=labels_values, shape=(1, k+1))\n    sequence_length = np.array([n])\n\n    loss = tf.nn.ctc_loss(inputs, labels, sequence_length)\n    g, = tf.gradients(loss, inputs)\n\n    g_v = sess.run(g)\n\nplt.plot(-g_v[:,0,:])\n```\n"}