{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6018", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6018/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6018/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6018/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6018", "id": 192905789, "node_id": "MDU6SXNzdWUxOTI5MDU3ODk=", "number": 6018, "title": "Unecessary messages in queue handling", "user": {"login": "MicaelCarvalho", "id": 17184992, "node_id": "MDQ6VXNlcjE3MTg0OTky", "avatar_url": "https://avatars3.githubusercontent.com/u/17184992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MicaelCarvalho", "html_url": "https://github.com/MicaelCarvalho", "followers_url": "https://api.github.com/users/MicaelCarvalho/followers", "following_url": "https://api.github.com/users/MicaelCarvalho/following{/other_user}", "gists_url": "https://api.github.com/users/MicaelCarvalho/gists{/gist_id}", "starred_url": "https://api.github.com/users/MicaelCarvalho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MicaelCarvalho/subscriptions", "organizations_url": "https://api.github.com/users/MicaelCarvalho/orgs", "repos_url": "https://api.github.com/users/MicaelCarvalho/repos", "events_url": "https://api.github.com/users/MicaelCarvalho/events{/privacy}", "received_events_url": "https://api.github.com/users/MicaelCarvalho/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-12-01T17:07:15Z", "updated_at": "2016-12-02T19:27:03Z", "closed_at": "2016-12-02T19:27:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Suppose you want to have one or multiple threads to preload/preprocess images and push them into an image queue that is going to be consumed by your main process, running on the GPU. A simple way of doing so would be:</p>\n<pre><code>def load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\n  index = 0\n  while not coord.should_stop():\n    idata = image_list[index]\n    session.run(enqueue_op, feed_dict={image_data_placeholder: idata})\n    index = (index + 1) % len(image_list)\n</code></pre>\n<p>Eventually, your optimization is finished and you want all threads to end, but if you have threads feeding your queue, they are most likely blocked at the enqueue_op, waiting for an image to be consumed, so they can finally insert theirs in the queue.</p>\n<p>Since closing the queue doesn't unblock them (surprisingly), your most evident choice is to use the timeout option to limit how long the enqueue_op can block your thread, with something like this:</p>\n<pre><code>def load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\n  index = 0\n  while not coord.should_stop():\n    idata = image_list[index]\n    try:\n      session.run(enqueue_op, feed_dict={image_data_placeholder: idata}, options=tf.RunOptions(timeout_in_ms=1000))\n      index = (index + 1) % len(name_list)\n    except:\n      pass\n</code></pre>\n<p>And it does work: the thread will continue trying to push the same image until either it succeeds (and moves on to the next one) or coordinator says it should stop everything and finish.</p>\n<p>The problem is that the timeout not only throws an exception saying that the time has been exceeded, but TF also \"manually\" prints a warning message. Bad news, you have 3 options: (1) manually edit TryAttemptLocked in queue_base.cc to remove the message from your tensorflow copy, (2) accept that this pesky message is going to flood your logs with useless information or (3) force TF_CPP_MIN_LOG_LEVEL to be at level 2, suppressing all information messages and warnings that could be important, only to get rid of this one message. There is also a 4th option: open an issue here in hope there is a better way of handling this, or that some dev changes this behavior \u2014 the exception should be enough for any situation, the forced message doesn't seem to do any good. :-)</p>\n<p>Disclaimer: I know it's far from optimal to pass a list of features (image_list) to be enqueued. This is just for the sake of simplicity in this example.</p>", "body_text": "Suppose you want to have one or multiple threads to preload/preprocess images and push them into an image queue that is going to be consumed by your main process, running on the GPU. A simple way of doing so would be:\ndef load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\n  index = 0\n  while not coord.should_stop():\n    idata = image_list[index]\n    session.run(enqueue_op, feed_dict={image_data_placeholder: idata})\n    index = (index + 1) % len(image_list)\n\nEventually, your optimization is finished and you want all threads to end, but if you have threads feeding your queue, they are most likely blocked at the enqueue_op, waiting for an image to be consumed, so they can finally insert theirs in the queue.\nSince closing the queue doesn't unblock them (surprisingly), your most evident choice is to use the timeout option to limit how long the enqueue_op can block your thread, with something like this:\ndef load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\n  index = 0\n  while not coord.should_stop():\n    idata = image_list[index]\n    try:\n      session.run(enqueue_op, feed_dict={image_data_placeholder: idata}, options=tf.RunOptions(timeout_in_ms=1000))\n      index = (index + 1) % len(name_list)\n    except:\n      pass\n\nAnd it does work: the thread will continue trying to push the same image until either it succeeds (and moves on to the next one) or coordinator says it should stop everything and finish.\nThe problem is that the timeout not only throws an exception saying that the time has been exceeded, but TF also \"manually\" prints a warning message. Bad news, you have 3 options: (1) manually edit TryAttemptLocked in queue_base.cc to remove the message from your tensorflow copy, (2) accept that this pesky message is going to flood your logs with useless information or (3) force TF_CPP_MIN_LOG_LEVEL to be at level 2, suppressing all information messages and warnings that could be important, only to get rid of this one message. There is also a 4th option: open an issue here in hope there is a better way of handling this, or that some dev changes this behavior \u2014 the exception should be enough for any situation, the forced message doesn't seem to do any good. :-)\nDisclaimer: I know it's far from optimal to pass a list of features (image_list) to be enqueued. This is just for the sake of simplicity in this example.", "body": "Suppose you want to have one or multiple threads to preload/preprocess images and push them into an image queue that is going to be consumed by your main process, running on the GPU. A simple way of doing so would be:\r\n\r\n```\r\ndef load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\r\n  index = 0\r\n  while not coord.should_stop():\r\n    idata = image_list[index]\r\n    session.run(enqueue_op, feed_dict={image_data_placeholder: idata})\r\n    index = (index + 1) % len(image_list)\r\n```\r\n\r\nEventually, your optimization is finished and you want all threads to end, but if you have threads feeding your queue, they are most likely blocked at the enqueue_op, waiting for an image to be consumed, so they can finally insert theirs in the queue.\r\n\r\nSince closing the queue doesn't unblock them (surprisingly), your most evident choice is to use the timeout option to limit how long the enqueue_op can block your thread, with something like this:\r\n\r\n```\r\ndef load_image(session, image_list, enqueue_op, image_data_placeholder, coord):\r\n  index = 0\r\n  while not coord.should_stop():\r\n    idata = image_list[index]\r\n    try:\r\n      session.run(enqueue_op, feed_dict={image_data_placeholder: idata}, options=tf.RunOptions(timeout_in_ms=1000))\r\n      index = (index + 1) % len(name_list)\r\n    except:\r\n      pass\r\n```\r\n\r\nAnd it does work: the thread will continue trying to push the same image until either it succeeds (and moves on to the next one) or coordinator says it should stop everything and finish.\r\n\r\nThe problem is that the timeout not only throws an exception saying that the time has been exceeded, but TF also \"manually\" prints a warning message. Bad news, you have 3 options: (1) manually edit TryAttemptLocked in queue_base.cc to remove the message from your tensorflow copy, (2) accept that this pesky message is going to flood your logs with useless information or (3) force TF_CPP_MIN_LOG_LEVEL to be at level 2, suppressing all information messages and warnings that could be important, only to get rid of this one message. There is also a 4th option: open an issue here in hope there is a better way of handling this, or that some dev changes this behavior \u2014 the exception should be enough for any situation, the forced message doesn't seem to do any good. :-)\r\n\r\nDisclaimer: I know it's far from optimal to pass a list of features (image_list) to be enqueued. This is just for the sake of simplicity in this example."}