{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361693047", "html_url": "https://github.com/tensorflow/tensorflow/issues/14084#issuecomment-361693047", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14084", "id": 361693047, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTY5MzA0Nw==", "user": {"login": "jimdowling", "id": 1904928, "node_id": "MDQ6VXNlcjE5MDQ5Mjg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1904928?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jimdowling", "html_url": "https://github.com/jimdowling", "followers_url": "https://api.github.com/users/jimdowling/followers", "following_url": "https://api.github.com/users/jimdowling/following{/other_user}", "gists_url": "https://api.github.com/users/jimdowling/gists{/gist_id}", "starred_url": "https://api.github.com/users/jimdowling/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jimdowling/subscriptions", "organizations_url": "https://api.github.com/users/jimdowling/orgs", "repos_url": "https://api.github.com/users/jimdowling/repos", "events_url": "https://api.github.com/users/jimdowling/events{/privacy}", "received_events_url": "https://api.github.com/users/jimdowling/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T18:43:37Z", "updated_at": "2018-01-30T18:43:37Z", "author_association": "NONE", "body_html": "<p>We have this problem as well. The problem is most acute when using the Estimator APIs, where we don't have an explicit handle on the hdfs writer object. The problem is that hdfs only updates the file size written either at the end of block being fully written or when an explicit hflush is called on the file. When the file sizes are much smaller than the block size and they aren't closed, they just appear to hang - not available for reading.</p>", "body_text": "We have this problem as well. The problem is most acute when using the Estimator APIs, where we don't have an explicit handle on the hdfs writer object. The problem is that hdfs only updates the file size written either at the end of block being fully written or when an explicit hflush is called on the file. When the file sizes are much smaller than the block size and they aren't closed, they just appear to hang - not available for reading.", "body": "We have this problem as well. The problem is most acute when using the Estimator APIs, where we don't have an explicit handle on the hdfs writer object. The problem is that hdfs only updates the file size written either at the end of block being fully written or when an explicit hflush is called on the file. When the file sizes are much smaller than the block size and they aren't closed, they just appear to hang - not available for reading. "}