{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/403179979", "html_url": "https://github.com/tensorflow/tensorflow/issues/20605#issuecomment-403179979", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20605", "id": 403179979, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMzE3OTk3OQ==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-07T01:37:49Z", "updated_at": "2018-07-07T01:37:49Z", "author_association": "MEMBER", "body_html": "<p>Could you be more specific on what operations you are talking about? Is this for tensorflow lite's quantization or the tensorflow operations?</p>\n<p>If this is for tensorflow, the relu6 operations have the quantization range fixed at 0 and 6 which helps quantize to good accuracy. Relu can also be quantized but needs the quantization range to be known. This can be accomplished with TensorFlow Lite's quantized models <a href=\"https://www.tensorflow.org/performance/quantization\" rel=\"nofollow\">https://www.tensorflow.org/performance/quantization</a></p>\n<p>TFLite supports quantization of ReLU and ReLU6 so you should consider using that.</p>", "body_text": "Could you be more specific on what operations you are talking about? Is this for tensorflow lite's quantization or the tensorflow operations?\nIf this is for tensorflow, the relu6 operations have the quantization range fixed at 0 and 6 which helps quantize to good accuracy. Relu can also be quantized but needs the quantization range to be known. This can be accomplished with TensorFlow Lite's quantized models https://www.tensorflow.org/performance/quantization\nTFLite supports quantization of ReLU and ReLU6 so you should consider using that.", "body": "Could you be more specific on what operations you are talking about? Is this for tensorflow lite's quantization or the tensorflow operations?\r\n\r\nIf this is for tensorflow, the relu6 operations have the quantization range fixed at 0 and 6 which helps quantize to good accuracy. Relu can also be quantized but needs the quantization range to be known. This can be accomplished with TensorFlow Lite's quantized models https://www.tensorflow.org/performance/quantization \r\n\r\nTFLite supports quantization of ReLU and ReLU6 so you should consider using that."}