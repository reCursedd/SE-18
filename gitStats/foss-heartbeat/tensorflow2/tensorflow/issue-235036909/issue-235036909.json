{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10619", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10619/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10619/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10619/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10619", "id": 235036909, "node_id": "MDU6SXNzdWUyMzUwMzY5MDk=", "number": 10619, "title": "Feature Request: Tensorboard visualization of what parts of a given training image were deemed as 'important' after a inception v3 has been retrained", "user": {"login": "John12341234", "id": 13044220, "node_id": "MDQ6VXNlcjEzMDQ0MjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/13044220?v=4", "gravatar_id": "", "url": "https://api.github.com/users/John12341234", "html_url": "https://github.com/John12341234", "followers_url": "https://api.github.com/users/John12341234/followers", "following_url": "https://api.github.com/users/John12341234/following{/other_user}", "gists_url": "https://api.github.com/users/John12341234/gists{/gist_id}", "starred_url": "https://api.github.com/users/John12341234/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/John12341234/subscriptions", "organizations_url": "https://api.github.com/users/John12341234/orgs", "repos_url": "https://api.github.com/users/John12341234/repos", "events_url": "https://api.github.com/users/John12341234/events{/privacy}", "received_events_url": "https://api.github.com/users/John12341234/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-11T01:33:42Z", "updated_at": "2017-06-13T16:23:02Z", "closed_at": "2017-06-13T16:23:00Z", "author_association": "NONE", "body_html": "<p>After retraining inception v3 on my own image set, I'm running into where any new image that I ask it to recognize, that <em>shouldn't</em> match any of the training categories, is confidently scored as one of the image categories.</p>\n<p>For example, if I train it on two types of steering wheels that are exactly the same except one category/type has two buttons on the right. After training has finished, if I show inception an image of some other random thing, like a horse, it confidently scores it as one of the steering wheel categories!</p>\n<p>The issue is that there is no way to get insight as to <em>why</em> this is happening. In other words, there is no way to tell that the model has correctly learned that the real difference between the two steering wheel categories is presence or absence of the the two buttons on the right. It would be extremely useful to be able to upload an image in tensorboard and have it show you a heat map, overlaid on the image, of what specific parts of the image the model deemed important when classifying it as a particular category.</p>\n<p>To elaborate further, if you have two categories that you retrained inception on, you would be able to upload an image, and you would see different regions of the image highlighted using two different colors (one color per category). Then you would be able to clearly see that the model has or has not correctly learned the real difference between the images. In addition, you could upload a image that should not be classified as one of the two categories and you would be able to see <em>why</em> it incorrectly thought it was one of the two categories.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13044220/27007360-baed3810-4e05-11e7-9e68-5e895fd5562b.jpg\"><img src=\"https://user-images.githubusercontent.com/13044220/27007360-baed3810-4e05-11e7-9e68-5e895fd5562b.jpg\" alt=\"00c0c_jygsfznatmv_600x450\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13044220/27007362-c6e05bc0-4e05-11e7-9280-4f40c4585545.jpg\"><img src=\"https://user-images.githubusercontent.com/13044220/27007362-c6e05bc0-4e05-11e7-9280-4f40c4585545.jpg\" alt=\"112_0905_04z-2010_toyota_prius-interior_view\" style=\"max-width:100%;\"></a></p>", "body_text": "After retraining inception v3 on my own image set, I'm running into where any new image that I ask it to recognize, that shouldn't match any of the training categories, is confidently scored as one of the image categories.\nFor example, if I train it on two types of steering wheels that are exactly the same except one category/type has two buttons on the right. After training has finished, if I show inception an image of some other random thing, like a horse, it confidently scores it as one of the steering wheel categories!\nThe issue is that there is no way to get insight as to why this is happening. In other words, there is no way to tell that the model has correctly learned that the real difference between the two steering wheel categories is presence or absence of the the two buttons on the right. It would be extremely useful to be able to upload an image in tensorboard and have it show you a heat map, overlaid on the image, of what specific parts of the image the model deemed important when classifying it as a particular category.\nTo elaborate further, if you have two categories that you retrained inception on, you would be able to upload an image, and you would see different regions of the image highlighted using two different colors (one color per category). Then you would be able to clearly see that the model has or has not correctly learned the real difference between the images. In addition, you could upload a image that should not be classified as one of the two categories and you would be able to see why it incorrectly thought it was one of the two categories.", "body": "After retraining inception v3 on my own image set, I'm running into where any new image that I ask it to recognize, that _shouldn't_ match any of the training categories, is confidently scored as one of the image categories. \r\n\r\nFor example, if I train it on two types of steering wheels that are exactly the same except one category/type has two buttons on the right. After training has finished, if I show inception an image of some other random thing, like a horse, it confidently scores it as one of the steering wheel categories! \r\n\r\nThe issue is that there is no way to get insight as to _why_ this is happening. In other words, there is no way to tell that the model has correctly learned that the real difference between the two steering wheel categories is presence or absence of the the two buttons on the right. It would be extremely useful to be able to upload an image in tensorboard and have it show you a heat map, overlaid on the image, of what specific parts of the image the model deemed important when classifying it as a particular category. \r\n\r\nTo elaborate further, if you have two categories that you retrained inception on, you would be able to upload an image, and you would see different regions of the image highlighted using two different colors (one color per category). Then you would be able to clearly see that the model has or has not correctly learned the real difference between the images. In addition, you could upload a image that should not be classified as one of the two categories and you would be able to see _why_ it incorrectly thought it was one of the two categories.\r\n![00c0c_jygsfznatmv_600x450](https://user-images.githubusercontent.com/13044220/27007360-baed3810-4e05-11e7-9e68-5e895fd5562b.jpg)\r\n![112_0905_04z-2010_toyota_prius-interior_view](https://user-images.githubusercontent.com/13044220/27007362-c6e05bc0-4e05-11e7-9280-4f40c4585545.jpg)\r\n"}