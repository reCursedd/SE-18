{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/357715340", "html_url": "https://github.com/tensorflow/tensorflow/issues/6035#issuecomment-357715340", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6035", "id": 357715340, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NzcxNTM0MA==", "user": {"login": "saeed68gm", "id": 6844416, "node_id": "MDQ6VXNlcjY4NDQ0MTY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6844416?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeed68gm", "html_url": "https://github.com/saeed68gm", "followers_url": "https://api.github.com/users/saeed68gm/followers", "following_url": "https://api.github.com/users/saeed68gm/following{/other_user}", "gists_url": "https://api.github.com/users/saeed68gm/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeed68gm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeed68gm/subscriptions", "organizations_url": "https://api.github.com/users/saeed68gm/orgs", "repos_url": "https://api.github.com/users/saeed68gm/repos", "events_url": "https://api.github.com/users/saeed68gm/events{/privacy}", "received_events_url": "https://api.github.com/users/saeed68gm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-15T15:32:55Z", "updated_at": "2018-01-15T15:37:26Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>SpatialMaxPoolWithArgMaxHelper used instead of LaunchMaxPoolingWithArgmax inside MaxPoolingWithArgmaxOp / Compute ?</p>\n</blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1001128\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mmpinso\">@mmpinso</a> No, if you look at the code, the LaunchMaxPoolingWithArgmax function will call the cuda code to compute maxPoolingWithArgmax on GPU. The problem is the unpooling operation that uses the scatter function and looks like everybody need the unpooling operation. Can somebody maybe implement this operation for CPU and share the code?</p>\n<p>Here is a sample in python, we need to create it in CPP:</p>\n<pre><code>def unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n    \n\twith tf.variable_scope(scope):\n\t\tinput_shape = pool.get_shape().as_list()\n\t\toutput_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n\t\tflat_input_size = np.prod(input_shape)\n\t\tflat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n\t\tpool_ = tf.reshape(pool, [flat_input_size])\n\t\tbatch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\n\t\tb = tf.ones_like(ind) * batch_range\n\t\tb = tf.reshape(b, [flat_input_size, 1])\n\t\tind_ = tf.reshape(ind, [flat_input_size, 1])\n\t\tind_ = tf.concat([b, ind_], 1)\n\n\t\tret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\n\t\tret = tf.reshape(ret, output_shape)\n\t\treturn ret\n</code></pre>", "body_text": "SpatialMaxPoolWithArgMaxHelper used instead of LaunchMaxPoolingWithArgmax inside MaxPoolingWithArgmaxOp / Compute ?\n\n@mmpinso No, if you look at the code, the LaunchMaxPoolingWithArgmax function will call the cuda code to compute maxPoolingWithArgmax on GPU. The problem is the unpooling operation that uses the scatter function and looks like everybody need the unpooling operation. Can somebody maybe implement this operation for CPU and share the code?\nHere is a sample in python, we need to create it in CPP:\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\n    \n\twith tf.variable_scope(scope):\n\t\tinput_shape = pool.get_shape().as_list()\n\t\toutput_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n\t\tflat_input_size = np.prod(input_shape)\n\t\tflat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\n\n\t\tpool_ = tf.reshape(pool, [flat_input_size])\n\t\tbatch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\n\t\tb = tf.ones_like(ind) * batch_range\n\t\tb = tf.reshape(b, [flat_input_size, 1])\n\t\tind_ = tf.reshape(ind, [flat_input_size, 1])\n\t\tind_ = tf.concat([b, ind_], 1)\n\n\t\tret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\n\t\tret = tf.reshape(ret, output_shape)\n\t\treturn ret", "body": "> SpatialMaxPoolWithArgMaxHelper used instead of LaunchMaxPoolingWithArgmax inside MaxPoolingWithArgmaxOp / Compute ?\r\n\r\n@mmpinso No, if you look at the code, the LaunchMaxPoolingWithArgmax function will call the cuda code to compute maxPoolingWithArgmax on GPU. The problem is the unpooling operation that uses the scatter function and looks like everybody need the unpooling operation. Can somebody maybe implement this operation for CPU and share the code?\r\n\r\nHere is a sample in python, we need to create it in CPP:\r\n```\r\ndef unpool(pool, ind, ksize=[1, 2, 2, 1], scope='unpool'):\r\n    \r\n\twith tf.variable_scope(scope):\r\n\t\tinput_shape = pool.get_shape().as_list()\r\n\t\toutput_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\r\n\r\n\t\tflat_input_size = np.prod(input_shape)\r\n\t\tflat_output_shape = [output_shape[0], output_shape[1] * output_shape[2] * output_shape[3]]\r\n\r\n\t\tpool_ = tf.reshape(pool, [flat_input_size])\r\n\t\tbatch_range = tf.reshape(tf.range(output_shape[0], dtype=ind.dtype), shape=[input_shape[0], 1, 1, 1])\r\n\t\tb = tf.ones_like(ind) * batch_range\r\n\t\tb = tf.reshape(b, [flat_input_size, 1])\r\n\t\tind_ = tf.reshape(ind, [flat_input_size, 1])\r\n\t\tind_ = tf.concat([b, ind_], 1)\r\n\r\n\t\tret = tf.scatter_nd(ind_, pool_, shape=flat_output_shape)\r\n\t\tret = tf.reshape(ret, output_shape)\r\n\t\treturn ret\r\n```\r\n"}