{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/352347415", "html_url": "https://github.com/tensorflow/tensorflow/issues/6035#issuecomment-352347415", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6035", "id": 352347415, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjM0NzQxNQ==", "user": {"login": "alecGraves", "id": 15484056, "node_id": "MDQ6VXNlcjE1NDg0MDU2", "avatar_url": "https://avatars2.githubusercontent.com/u/15484056?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alecGraves", "html_url": "https://github.com/alecGraves", "followers_url": "https://api.github.com/users/alecGraves/followers", "following_url": "https://api.github.com/users/alecGraves/following{/other_user}", "gists_url": "https://api.github.com/users/alecGraves/gists{/gist_id}", "starred_url": "https://api.github.com/users/alecGraves/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alecGraves/subscriptions", "organizations_url": "https://api.github.com/users/alecGraves/orgs", "repos_url": "https://api.github.com/users/alecGraves/repos", "events_url": "https://api.github.com/users/alecGraves/events{/privacy}", "received_events_url": "https://api.github.com/users/alecGraves/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-18T07:36:40Z", "updated_at": "2017-12-18T07:36:40Z", "author_association": "NONE", "body_html": "<p>I was able to make a workaround for CPU.<br>\nWarning: it is slow, bloated, and basically a last resort. But I can get ENet to run on my cpu at 5s/image...<br>\nRight now it is only for 2x2 maxpooling, but you could change the numbers and add things to expand upon this example.</p>\n<div class=\"highlight highlight-source-python\"><pre>net_main, pooling_indices <span class=\"pl-k\">=</span> tf.nn.max_pool_with_argmax(inputs,\n                                                                   <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],\n                                                                   <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],\n                                                                   <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>,\n                                                                   <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_main_max_pool<span class=\"pl-pds\">'</span></span>)</pre></div>\n<p>becomes</p>\n<div class=\"highlight highlight-source-python\"><pre>net_main <span class=\"pl-k\">=</span> tf.nn.max_pool(inputs,\n                        <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],\n                        <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],\n                        <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>,\n                        <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_main_max_pool<span class=\"pl-pds\">'</span></span>)\n\ninput_shape <span class=\"pl-k\">=</span> inputs.get_shape().as_list()\nmask_shape <span class=\"pl-k\">=</span> [input_shape[<span class=\"pl-c1\">0</span>], input_shape [<span class=\"pl-c1\">1</span>]<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span>,input_shape[<span class=\"pl-c1\">2</span>]<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span>, input_shape[<span class=\"pl-c1\">3</span>]]\npooling_indices <span class=\"pl-k\">=</span> tf.zeros(mask_shape, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int64)\n<span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(mask_shape[<span class=\"pl-c1\">0</span>]):\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(mask_shape[<span class=\"pl-c1\">1</span>]):\n        <span class=\"pl-k\">for</span> j <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(mask_shape[<span class=\"pl-c1\">2</span>]):\n            in_indices <span class=\"pl-k\">=</span> [ [n, w, h] <span class=\"pl-k\">for</span> w <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(i<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, i<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">2</span>) <span class=\"pl-k\">for</span> h <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(j<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span>, j<span class=\"pl-k\">*</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">2</span>)]\n            <span class=\"pl-c1\">slice</span> <span class=\"pl-k\">=</span> tf.gather_nd(inputs, in_indices)\n            argmax <span class=\"pl-k\">=</span> tf.argmax(<span class=\"pl-c1\">slice</span>, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n            indices_location <span class=\"pl-k\">=</span> [[n, i, j, d] <span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(input_shape[<span class=\"pl-c1\">3</span>])]\n            sparse_indices <span class=\"pl-k\">=</span> tf.SparseTensor(<span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>indices_location, <span class=\"pl-v\">values</span><span class=\"pl-k\">=</span>argmax, <span class=\"pl-v\">dense_shape</span><span class=\"pl-k\">=</span>mask_shape)\n            pooling_indices <span class=\"pl-k\">=</span> tf.sparse_add(pooling_indices, sparse_indices)</pre></div>\n<p>Also, that part of the gpu kernel is on <a href=\"https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/maxpooling_op_gpu.cu.cc#L88\">this line</a>. Simply copying that code from the gpu kernel to the cpu kernel may not be so bad for this operation. It is a nice operation to have, and even without optimization, it would probably still be much faster than what I ended up doing <g-emoji class=\"g-emoji\" alias=\"stuck_out_tongue\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f61b.png\">\ud83d\ude1b</g-emoji></p>", "body_text": "I was able to make a workaround for CPU.\nWarning: it is slow, bloated, and basically a last resort. But I can get ENet to run on my cpu at 5s/image...\nRight now it is only for 2x2 maxpooling, but you could change the numbers and add things to expand upon this example.\nnet_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\n                                                                   ksize=[1,2,2,1],\n                                                                   strides=[1,2,2,1],\n                                                                   padding='SAME',\n                                                                   name=scope+'_main_max_pool')\nbecomes\nnet_main = tf.nn.max_pool(inputs,\n                        ksize=[1,2,2,1],\n                        strides=[1,2,2,1],\n                        padding='SAME',\n                        name=scope+'_main_max_pool')\n\ninput_shape = inputs.get_shape().as_list()\nmask_shape = [input_shape[0], input_shape [1]/2,input_shape[2]/2, input_shape[3]]\npooling_indices = tf.zeros(mask_shape, dtype=tf.int64)\nfor n in range(mask_shape[0]):\n    for i in range(mask_shape[1]):\n        for j in range(mask_shape[2]):\n            in_indices = [ [n, w, h] for w in range(i*2, i*2+2) for h in range(j*2, j*2+2)]\n            slice = tf.gather_nd(inputs, in_indices)\n            argmax = tf.argmax(slice, axis=0)\n            indices_location = [[n, i, j, d] for d in range(input_shape[3])]\n            sparse_indices = tf.SparseTensor(indices=indices_location, values=argmax, dense_shape=mask_shape)\n            pooling_indices = tf.sparse_add(pooling_indices, sparse_indices)\nAlso, that part of the gpu kernel is on this line. Simply copying that code from the gpu kernel to the cpu kernel may not be so bad for this operation. It is a nice operation to have, and even without optimization, it would probably still be much faster than what I ended up doing \ud83d\ude1b", "body": "I was able to make a workaround for CPU.\r\nWarning: it is slow, bloated, and basically a last resort. But I can get ENet to run on my cpu at 5s/image...\r\nRight now it is only for 2x2 maxpooling, but you could change the numbers and add things to expand upon this example.\r\n```python\r\nnet_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\r\n                                                                   ksize=[1,2,2,1],\r\n                                                                   strides=[1,2,2,1],\r\n                                                                   padding='SAME',\r\n                                                                   name=scope+'_main_max_pool')\r\n```\r\nbecomes\r\n```python\r\nnet_main = tf.nn.max_pool(inputs,\r\n                        ksize=[1,2,2,1],\r\n                        strides=[1,2,2,1],\r\n                        padding='SAME',\r\n                        name=scope+'_main_max_pool')\r\n\r\ninput_shape = inputs.get_shape().as_list()\r\nmask_shape = [input_shape[0], input_shape [1]/2,input_shape[2]/2, input_shape[3]]\r\npooling_indices = tf.zeros(mask_shape, dtype=tf.int64)\r\nfor n in range(mask_shape[0]):\r\n    for i in range(mask_shape[1]):\r\n        for j in range(mask_shape[2]):\r\n            in_indices = [ [n, w, h] for w in range(i*2, i*2+2) for h in range(j*2, j*2+2)]\r\n            slice = tf.gather_nd(inputs, in_indices)\r\n            argmax = tf.argmax(slice, axis=0)\r\n            indices_location = [[n, i, j, d] for d in range(input_shape[3])]\r\n            sparse_indices = tf.SparseTensor(indices=indices_location, values=argmax, dense_shape=mask_shape)\r\n            pooling_indices = tf.sparse_add(pooling_indices, sparse_indices)\r\n```\r\n\r\nAlso, that part of the gpu kernel is on [this line](https://github.com/tensorflow/tensorflow/blob/40eef4473bda90442bb55fcc67842f097c024580/tensorflow/core/kernels/maxpooling_op_gpu.cu.cc#L88). Simply copying that code from the gpu kernel to the cpu kernel may not be so bad for this operation. It is a nice operation to have, and even without optimization, it would probably still be much faster than what I ended up doing :stuck_out_tongue: \r\n"}