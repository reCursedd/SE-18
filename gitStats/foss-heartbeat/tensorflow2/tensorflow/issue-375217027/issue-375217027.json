{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23357", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23357/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23357/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23357/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23357", "id": 375217027, "node_id": "MDU6SXNzdWUzNzUyMTcwMjc=", "number": 23357, "title": "FP16 Sparse Matrix multiply returns incorrect results on ARM", "user": {"login": "MattConley", "id": 10891704, "node_id": "MDQ6VXNlcjEwODkxNzA0", "avatar_url": "https://avatars0.githubusercontent.com/u/10891704?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MattConley", "html_url": "https://github.com/MattConley", "followers_url": "https://api.github.com/users/MattConley/followers", "following_url": "https://api.github.com/users/MattConley/following{/other_user}", "gists_url": "https://api.github.com/users/MattConley/gists{/gist_id}", "starred_url": "https://api.github.com/users/MattConley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MattConley/subscriptions", "organizations_url": "https://api.github.com/users/MattConley/orgs", "repos_url": "https://api.github.com/users/MattConley/repos", "events_url": "https://api.github.com/users/MattConley/events{/privacy}", "received_events_url": "https://api.github.com/users/MattConley/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-10-29T21:07:27Z", "updated_at": "2018-11-22T18:50:49Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): <strong>no</strong></li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): <strong>Linux for Tegra (Ubuntu 18.04)</strong></li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: <strong>Jetson AGX Xavier (ARM-aarch64)</strong></li>\n<li>TensorFlow installed from (source or binary): <strong>source</strong></li>\n<li>TensorFlow version (use command below): <strong>1.11</strong></li>\n<li>Python version: <strong>2.7</strong></li>\n<li>Bazel version (if compiling from source): <strong>0.15.0</strong></li>\n<li>GCC/Compiler version (if compiling from source): <strong>Linaro 7.3.0</strong></li>\n<li>CUDA/cuDNN version: <strong>CUDA 10 cuDNN 7.3</strong></li>\n<li>GPU model and memory: <strong>Jetson AGX Xavier, 16GB (shared with host)</strong></li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>When a matrix multiply is called such that one of the input tensors is of data type fp16 and declared sparse, the function returns incorrect results.  This error causes the failure of the sparse_matmul_op_test (found at tensorflow/python/kernel_tests/sparse_matmul_op_test.py)</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>The matmul function should return correct results, regardless of sparsity or data type.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<p>Running the sparse_matmul_op_test.py script on an ARM architecture device can reproduce the failure.  This python script is a reduced reproduction:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import math_ops\n\n\nwith tf.Session() as sess:\n  x=(np.clip(\n      np.random.uniform(\n          low=-256.0, high=256.0, size=3 * 3), -64,\n      64) / 128.0).reshape([3, 3]).astype(np.float32)\n  y=(np.clip(\n      np.random.uniform(\n          low=-256.0, high=256.0, size=3 * 3), -64,\n      64) / 128.0).reshape([3, 3]).astype(np.float32)\n\n\n\n  #Test matmul where a is sparse fp16, and b is not sparse fp32\n  tf_x = math_ops.cast(x, dtypes.bfloat16)\n  tf_y = math_ops.cast(y, dtypes.float32)\n  tf_ans = math_ops.matmul(\n      tf_x,\n      tf_y,\n      a_is_sparse=True,\n      b_is_sparse=False)\n  out = sess.run(tf_ans)\n  #Test the same matmul case as above, without sparsity\n  tf_ans_nosparse = math_ops.matmul(\n      tf_x,\n      tf_y,\n      a_is_sparse=False,\n      b_is_sparse=False)\n  out_nosparse = sess.run(tf_ans_nosparse)\n\n  #Numpy result where both types are np32, since the dot operation is not supported with fp16\n  np_x = sess.run(math_ops.cast(tf_x, dtypes.float32))\n  np_y = sess.run(math_ops.cast(tf_y, dtypes.float32))\n  np_ans = np.matrix(np_x) * np.matrix(np_y)\n\n  print(\"\\tNumpy answer: \")\n  print(np_ans)\n  print(\"\\tNon-Sparse TF: \")\n  print(out_nosparse)\n  print(\"\\tSparse fp16 a, non-sparse fp32 b\")\n  print(out)\n</code></pre>\n<p><strong>Other info / logs</strong></p>\n<p>Here is a sample output testing different configurations for the matmul inputs\u2019 sparsity and data types.  (This output was generating by adding additional test configurations to the provided reproduce script.)</p>\n<blockquote>\n<p>Numpy answer:<br>\n[[-0.24025428 -0.13065982 -0.25      ]<br>\n[-0.08487757 -0.36836362  0.74902344]<br>\n[ 0.16072105  0.00858951  0.3720703 ]]<br>\nNon-Sparse TF:<br>\n[[-0.24025428 -0.13065982 -0.25      ]<br>\n[-0.08487757 -0.36836362  0.74902344]<br>\n[ 0.16072105  0.00858951  0.3720703 ]]<br>\nSparse TF:<br>\nSparse fp16 a, non-sparse fp32 b<br>\n[[ 0.58551383  0.6306598  -0.25      ]<br>\n[ 0.58551383  0.6306598  -0.25      ]<br>\n[-0.33927894 -0.25273013 -0.12792969]]<br>\nSparse fp32 a, non-sparse fp16 b<br>\n[[-0.24023438 -0.13085938 -0.25      ]<br>\n[-0.08523875 -0.36806947  0.74892884]<br>\n[ 0.16089517  0.0092376   0.3716218 ]]<br>\nNon-sparse fp16 a, sparse fp32 b<br>\n[[-0.24025428 -0.13065982 -0.25      ]<br>\n[-0.08487757 -0.36836362  0.74902344]<br>\n[ 0.16072105  0.00858951  0.3720703 ]]<br>\nNon-sparse fp32 a, sparse fp16 b<br>\n[[-0.17285156 -0.08886719  0.25      ]<br>\n[-0.17211097 -0.17229089  0.24892886]<br>\n[ 0.08876151  0.02520579 -0.12837823]]<br>\nSparse fp16 a, sparse fp32 b<br>\n[[ 0.58551383  0.6306598  -0.25      ]<br>\n[ 0.58551383  0.6306598  -0.25      ]<br>\n[-0.33927894 -0.25273013 -0.12792969]]<br>\nSparse fp16 a, sparse fp16 b<br>\n[[bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]<br>\n[bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]<br>\n[bfloat16(-0.33984375) bfloat16(-0.25390625) bfloat16(-0.127929688)]]<br>\nSparse fp32 a, sparse fp32 b<br>\n[[-0.24025428 -0.13065982 -0.25      ]<br>\n[-0.08481594 -0.36826903  0.74892884]<br>\n[ 0.16101329  0.00903805  0.3716218 ]]</p>\n</blockquote>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux for Tegra (Ubuntu 18.04)\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Jetson AGX Xavier (ARM-aarch64)\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.11\nPython version: 2.7\nBazel version (if compiling from source): 0.15.0\nGCC/Compiler version (if compiling from source): Linaro 7.3.0\nCUDA/cuDNN version: CUDA 10 cuDNN 7.3\nGPU model and memory: Jetson AGX Xavier, 16GB (shared with host)\n\nDescribe the current behavior\nWhen a matrix multiply is called such that one of the input tensors is of data type fp16 and declared sparse, the function returns incorrect results.  This error causes the failure of the sparse_matmul_op_test (found at tensorflow/python/kernel_tests/sparse_matmul_op_test.py)\nDescribe the expected behavior\nThe matmul function should return correct results, regardless of sparsity or data type.\nCode to reproduce the issue\nRunning the sparse_matmul_op_test.py script on an ARM architecture device can reproduce the failure.  This python script is a reduced reproduction:\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.ops import math_ops\n\n\nwith tf.Session() as sess:\n  x=(np.clip(\n      np.random.uniform(\n          low=-256.0, high=256.0, size=3 * 3), -64,\n      64) / 128.0).reshape([3, 3]).astype(np.float32)\n  y=(np.clip(\n      np.random.uniform(\n          low=-256.0, high=256.0, size=3 * 3), -64,\n      64) / 128.0).reshape([3, 3]).astype(np.float32)\n\n\n\n  #Test matmul where a is sparse fp16, and b is not sparse fp32\n  tf_x = math_ops.cast(x, dtypes.bfloat16)\n  tf_y = math_ops.cast(y, dtypes.float32)\n  tf_ans = math_ops.matmul(\n      tf_x,\n      tf_y,\n      a_is_sparse=True,\n      b_is_sparse=False)\n  out = sess.run(tf_ans)\n  #Test the same matmul case as above, without sparsity\n  tf_ans_nosparse = math_ops.matmul(\n      tf_x,\n      tf_y,\n      a_is_sparse=False,\n      b_is_sparse=False)\n  out_nosparse = sess.run(tf_ans_nosparse)\n\n  #Numpy result where both types are np32, since the dot operation is not supported with fp16\n  np_x = sess.run(math_ops.cast(tf_x, dtypes.float32))\n  np_y = sess.run(math_ops.cast(tf_y, dtypes.float32))\n  np_ans = np.matrix(np_x) * np.matrix(np_y)\n\n  print(\"\\tNumpy answer: \")\n  print(np_ans)\n  print(\"\\tNon-Sparse TF: \")\n  print(out_nosparse)\n  print(\"\\tSparse fp16 a, non-sparse fp32 b\")\n  print(out)\n\nOther info / logs\nHere is a sample output testing different configurations for the matmul inputs\u2019 sparsity and data types.  (This output was generating by adding additional test configurations to the provided reproduce script.)\n\nNumpy answer:\n[[-0.24025428 -0.13065982 -0.25      ]\n[-0.08487757 -0.36836362  0.74902344]\n[ 0.16072105  0.00858951  0.3720703 ]]\nNon-Sparse TF:\n[[-0.24025428 -0.13065982 -0.25      ]\n[-0.08487757 -0.36836362  0.74902344]\n[ 0.16072105  0.00858951  0.3720703 ]]\nSparse TF:\nSparse fp16 a, non-sparse fp32 b\n[[ 0.58551383  0.6306598  -0.25      ]\n[ 0.58551383  0.6306598  -0.25      ]\n[-0.33927894 -0.25273013 -0.12792969]]\nSparse fp32 a, non-sparse fp16 b\n[[-0.24023438 -0.13085938 -0.25      ]\n[-0.08523875 -0.36806947  0.74892884]\n[ 0.16089517  0.0092376   0.3716218 ]]\nNon-sparse fp16 a, sparse fp32 b\n[[-0.24025428 -0.13065982 -0.25      ]\n[-0.08487757 -0.36836362  0.74902344]\n[ 0.16072105  0.00858951  0.3720703 ]]\nNon-sparse fp32 a, sparse fp16 b\n[[-0.17285156 -0.08886719  0.25      ]\n[-0.17211097 -0.17229089  0.24892886]\n[ 0.08876151  0.02520579 -0.12837823]]\nSparse fp16 a, sparse fp32 b\n[[ 0.58551383  0.6306598  -0.25      ]\n[ 0.58551383  0.6306598  -0.25      ]\n[-0.33927894 -0.25273013 -0.12792969]]\nSparse fp16 a, sparse fp16 b\n[[bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]\n[bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]\n[bfloat16(-0.33984375) bfloat16(-0.25390625) bfloat16(-0.127929688)]]\nSparse fp32 a, sparse fp32 b\n[[-0.24025428 -0.13065982 -0.25      ]\n[-0.08481594 -0.36826903  0.74892884]\n[ 0.16101329  0.00903805  0.3716218 ]]", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): **no**\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): **Linux for Tegra (Ubuntu 18.04)**\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: **Jetson AGX Xavier (ARM-aarch64)**\r\n- TensorFlow installed from (source or binary): **source**\r\n- TensorFlow version (use command below): **1.11**\r\n- Python version: **2.7**\r\n- Bazel version (if compiling from source): **0.15.0**\r\n- GCC/Compiler version (if compiling from source): **Linaro 7.3.0**\r\n- CUDA/cuDNN version: **CUDA 10 cuDNN 7.3**\r\n- GPU model and memory: **Jetson AGX Xavier, 16GB (shared with host)**\r\n\r\n\r\n**Describe the current behavior**\r\n\r\nWhen a matrix multiply is called such that one of the input tensors is of data type fp16 and declared sparse, the function returns incorrect results.  This error causes the failure of the sparse_matmul_op_test (found at tensorflow/python/kernel_tests/sparse_matmul_op_test.py)\r\n\r\n**Describe the expected behavior**\r\n\r\nThe matmul function should return correct results, regardless of sparsity or data type.\r\n\r\n**Code to reproduce the issue**\r\n\r\nRunning the sparse_matmul_op_test.py script on an ARM architecture device can reproduce the failure.  This python script is a reduced reproduction:\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.python.framework import dtypes\r\nfrom tensorflow.python.ops import math_ops\r\n\r\n\r\nwith tf.Session() as sess:\r\n  x=(np.clip(\r\n      np.random.uniform(\r\n          low=-256.0, high=256.0, size=3 * 3), -64,\r\n      64) / 128.0).reshape([3, 3]).astype(np.float32)\r\n  y=(np.clip(\r\n      np.random.uniform(\r\n          low=-256.0, high=256.0, size=3 * 3), -64,\r\n      64) / 128.0).reshape([3, 3]).astype(np.float32)\r\n\r\n\r\n\r\n  #Test matmul where a is sparse fp16, and b is not sparse fp32\r\n  tf_x = math_ops.cast(x, dtypes.bfloat16)\r\n  tf_y = math_ops.cast(y, dtypes.float32)\r\n  tf_ans = math_ops.matmul(\r\n      tf_x,\r\n      tf_y,\r\n      a_is_sparse=True,\r\n      b_is_sparse=False)\r\n  out = sess.run(tf_ans)\r\n  #Test the same matmul case as above, without sparsity\r\n  tf_ans_nosparse = math_ops.matmul(\r\n      tf_x,\r\n      tf_y,\r\n      a_is_sparse=False,\r\n      b_is_sparse=False)\r\n  out_nosparse = sess.run(tf_ans_nosparse)\r\n\r\n  #Numpy result where both types are np32, since the dot operation is not supported with fp16\r\n  np_x = sess.run(math_ops.cast(tf_x, dtypes.float32))\r\n  np_y = sess.run(math_ops.cast(tf_y, dtypes.float32))\r\n  np_ans = np.matrix(np_x) * np.matrix(np_y)\r\n\r\n  print(\"\\tNumpy answer: \")\r\n  print(np_ans)\r\n  print(\"\\tNon-Sparse TF: \")\r\n  print(out_nosparse)\r\n  print(\"\\tSparse fp16 a, non-sparse fp32 b\")\r\n  print(out)\r\n```\r\n\r\n\r\n**Other info / logs**\r\n\r\nHere is a sample output testing different configurations for the matmul inputs\u2019 sparsity and data types.  (This output was generating by adding additional test configurations to the provided reproduce script.)\r\n\r\n\t\r\n\r\n> Numpy answer: \r\n> [[-0.24025428 -0.13065982 -0.25      ]\r\n>  [-0.08487757 -0.36836362  0.74902344]\r\n>  [ 0.16072105  0.00858951  0.3720703 ]]\r\n> \tNon-Sparse TF: \r\n> [[-0.24025428 -0.13065982 -0.25      ]\r\n>  [-0.08487757 -0.36836362  0.74902344]\r\n>  [ 0.16072105  0.00858951  0.3720703 ]]\r\n> \tSparse TF: \r\n> Sparse fp16 a, non-sparse fp32 b\r\n> [[ 0.58551383  0.6306598  -0.25      ]\r\n>  [ 0.58551383  0.6306598  -0.25      ]\r\n>  [-0.33927894 -0.25273013 -0.12792969]]\r\n> Sparse fp32 a, non-sparse fp16 b\r\n> [[-0.24023438 -0.13085938 -0.25      ]\r\n>  [-0.08523875 -0.36806947  0.74892884]\r\n>  [ 0.16089517  0.0092376   0.3716218 ]]\r\n> Non-sparse fp16 a, sparse fp32 b\r\n> [[-0.24025428 -0.13065982 -0.25      ]\r\n>  [-0.08487757 -0.36836362  0.74902344]\r\n>  [ 0.16072105  0.00858951  0.3720703 ]]\r\n> Non-sparse fp32 a, sparse fp16 b\r\n> [[-0.17285156 -0.08886719  0.25      ]\r\n>  [-0.17211097 -0.17229089  0.24892886]\r\n>  [ 0.08876151  0.02520579 -0.12837823]]\r\n> Sparse fp16 a, sparse fp32 b\r\n> [[ 0.58551383  0.6306598  -0.25      ]\r\n>  [ 0.58551383  0.6306598  -0.25      ]\r\n>  [-0.33927894 -0.25273013 -0.12792969]]\r\n> Sparse fp16 a, sparse fp16 b\r\n> [[bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]\r\n>  [bfloat16(0.5859375) bfloat16(0.6328125) bfloat16(-0.25)]\r\n>  [bfloat16(-0.33984375) bfloat16(-0.25390625) bfloat16(-0.127929688)]]\r\n> Sparse fp32 a, sparse fp32 b\r\n> [[-0.24025428 -0.13065982 -0.25      ]\r\n>  [-0.08481594 -0.36826903  0.74892884]\r\n>  [ 0.16101329  0.00903805  0.3716218 ]]\r\n> "}