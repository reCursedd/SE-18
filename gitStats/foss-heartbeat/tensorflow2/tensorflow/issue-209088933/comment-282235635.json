{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282235635", "html_url": "https://github.com/tensorflow/tensorflow/pull/7730#issuecomment-282235635", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7730", "id": 282235635, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjIzNTYzNQ==", "user": {"login": "EronWright", "id": 1775518, "node_id": "MDQ6VXNlcjE3NzU1MTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/1775518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EronWright", "html_url": "https://github.com/EronWright", "followers_url": "https://api.github.com/users/EronWright/followers", "following_url": "https://api.github.com/users/EronWright/following{/other_user}", "gists_url": "https://api.github.com/users/EronWright/gists{/gist_id}", "starred_url": "https://api.github.com/users/EronWright/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EronWright/subscriptions", "organizations_url": "https://api.github.com/users/EronWright/orgs", "repos_url": "https://api.github.com/users/EronWright/repos", "events_url": "https://api.github.com/users/EronWright/events{/privacy}", "received_events_url": "https://api.github.com/users/EronWright/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-24T08:38:34Z", "updated_at": "2017-02-24T15:32:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I rely on a wrapper at this time, no worries you can close this.</p>\n<p>It can't hurt to have a finalizer as a safety net.   But as you know it's dangerous to leave the cleanup of large unmanaged blocks of memory to the GC.  Apparently direct ByteBuffers suffer a similar problem (and <a href=\"http://stackoverflow.com/questions/3496508/deallocating-direct-buffer-native-memory-in-java-for-jogl/26777380#26777380\" rel=\"nofollow\">hacks abound</a>).</p>\n<p>I do like how Netty solved this problem (<a href=\"http://netty.io/wiki/using-as-a-generic-library.html#buffer-api\" rel=\"nofollow\">ref</a>), and it inspired the approach in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"197757100\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6528\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/6528/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/6528\">#6528</a>.  Netty-style ref counting is the most powerful yet most convenient alternative that I know of.   Netty resembles a dataflow system in some ways, with its channel pipeline of buffers, further inspiring my earlier comments.</p>", "body_text": "I rely on a wrapper at this time, no worries you can close this.\nIt can't hurt to have a finalizer as a safety net.   But as you know it's dangerous to leave the cleanup of large unmanaged blocks of memory to the GC.  Apparently direct ByteBuffers suffer a similar problem (and hacks abound).\nI do like how Netty solved this problem (ref), and it inspired the approach in #6528.  Netty-style ref counting is the most powerful yet most convenient alternative that I know of.   Netty resembles a dataflow system in some ways, with its channel pipeline of buffers, further inspiring my earlier comments.", "body": "I rely on a wrapper at this time, no worries you can close this.\r\n\r\nIt can't hurt to have a finalizer as a safety net.   But as you know it's dangerous to leave the cleanup of large unmanaged blocks of memory to the GC.  Apparently direct ByteBuffers suffer a similar problem (and [hacks abound](http://stackoverflow.com/questions/3496508/deallocating-direct-buffer-native-memory-in-java-for-jogl/26777380#26777380)).\r\n\r\nI do like how Netty solved this problem ([ref](http://netty.io/wiki/using-as-a-generic-library.html#buffer-api)), and it inspired the approach in #6528.  Netty-style ref counting is the most powerful yet most convenient alternative that I know of.   Netty resembles a dataflow system in some ways, with its channel pipeline of buffers, further inspiring my earlier comments."}