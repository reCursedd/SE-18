{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21446", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21446/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21446/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21446/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21446", "id": 348335982, "node_id": "MDU6SXNzdWUzNDgzMzU5ODI=", "number": 21446, "title": "the error of freeze model", "user": {"login": "ztwe", "id": 7899459, "node_id": "MDQ6VXNlcjc4OTk0NTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/7899459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ztwe", "html_url": "https://github.com/ztwe", "followers_url": "https://api.github.com/users/ztwe/followers", "following_url": "https://api.github.com/users/ztwe/following{/other_user}", "gists_url": "https://api.github.com/users/ztwe/gists{/gist_id}", "starred_url": "https://api.github.com/users/ztwe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ztwe/subscriptions", "organizations_url": "https://api.github.com/users/ztwe/orgs", "repos_url": "https://api.github.com/users/ztwe/repos", "events_url": "https://api.github.com/users/ztwe/events{/privacy}", "received_events_url": "https://api.github.com/users/ztwe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-08-07T14:03:39Z", "updated_at": "2018-08-10T08:50:09Z", "closed_at": "2018-08-10T08:50:09Z", "author_association": "NONE", "body_html": "<ul>\n<li>OS Platform:  Ubuntu 16.04</li>\n<li>TensorFlow installed from:  anaconda</li>\n<li>TensorFlow version: GPU-1.8</li>\n<li>Python version: 3.6.6</li>\n<li>CUDA/cuDNN version: 9.0 / 7.1.2</li>\n<li>GPU model and memory:16G</li>\n</ul>\n<p>The model is mobilenetV2 + LSTM + attention.<br>\nMobilenetV2 is restored from the model zoo in <a href=\"https://github.com/tensorflow/models/tree/master/research/slim\">https://github.com/tensorflow/models/tree/master/research/slim</a>. It is not trained during the training of the whole model.<br>\nLSTM and attention is trained.</p>\n<p>The code about model is shown as following.</p>\n<pre><code>    def mobilenet_v2_rnn_attention(self, X, num_classes, dropout_keep_prob=0.8, is_train=False):\n        rnn_size = 4096\n        num_layers = 8\n        attention_size = 1024\n        arg_scope = training_scope()\n        with slim.arg_scope(arg_scope):\n            net_vis, end_points = mobilenet_base(X, num_classes=num_classes)\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n            with tf.variable_scope('Logits_out'):\n                orig_shape = net_vis.get_shape().as_list()\n                net = tf.reshape(net_vis, [-1, orig_shape[1] * orig_shape[2], orig_shape[3]])\n\n                def lstm_cell():\n                    return tf.contrib.rnn.LSTMCell(rnn_size)\n\n                stack = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(0, num_layers)], state_is_tuple=True)\n                net, _ = tf.nn.dynamic_rnn(stack, net, dtype=tf.float32)\n                # (B, T, D) =&gt; (T, B, D)\n                net = tf.transpose(net, (1, 0, 2))\n                net = attention(net, attention_size, True)\n                net = slim.fully_connected(net, num_classes, activation_fn=None, scope='Logits_out1')\n        return net, net_vis\n</code></pre>\n<p>At the first, I export the graph as following.</p>\n<pre><code>    X = tf.placeholder(tf.float32, [None, height, width, 3], name=\"inputs_placeholder\")\n    net, net_vis = build_model(X, num_classes, 1.0, False, arch_model)\n    net = tf.nn.sigmoid(net)\n    predict = tf.reshape(net, [-1, num_classes], name='predictions')\n\n    graph_def = graph.as_graph_def()\n\n    with gfile.GFile(FLAGS.output_file, 'wb') as f:\n      f.write(graph_def.SerializeToString())\n</code></pre>\n<p>Then, freeze the model using the api</p>\n<pre><code>python /root/anaconda3/pkgs/tensorflow-base-1.8.0-py36hc1a7637_0/lib/python3.6/site- \n    packages/tensorflow/python/tools/freeze_graph.py \\\n      --input_graph=${INFERENCE_GRAPH_PATH}/${TMP_GRAPH} \\\n      --input_checkpoint=${CHECKPOINT_PATH}/model.ckpt-3800 \\\n      --input_binary=true \\\n      --output_graph=${CHECKPOINT_PATH}/${EXPORT_INF_GRAPH} \\\n      --output_node_names=${OUTPUT_NODE_NAME}\n</code></pre>\n<p>The error is shown as following.</p>\n<p><code>[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output-&gt;ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?</code></p>", "body_text": "OS Platform:  Ubuntu 16.04\nTensorFlow installed from:  anaconda\nTensorFlow version: GPU-1.8\nPython version: 3.6.6\nCUDA/cuDNN version: 9.0 / 7.1.2\nGPU model and memory:16G\n\nThe model is mobilenetV2 + LSTM + attention.\nMobilenetV2 is restored from the model zoo in https://github.com/tensorflow/models/tree/master/research/slim. It is not trained during the training of the whole model.\nLSTM and attention is trained.\nThe code about model is shown as following.\n    def mobilenet_v2_rnn_attention(self, X, num_classes, dropout_keep_prob=0.8, is_train=False):\n        rnn_size = 4096\n        num_layers = 8\n        attention_size = 1024\n        arg_scope = training_scope()\n        with slim.arg_scope(arg_scope):\n            net_vis, end_points = mobilenet_base(X, num_classes=num_classes)\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\n            with tf.variable_scope('Logits_out'):\n                orig_shape = net_vis.get_shape().as_list()\n                net = tf.reshape(net_vis, [-1, orig_shape[1] * orig_shape[2], orig_shape[3]])\n\n                def lstm_cell():\n                    return tf.contrib.rnn.LSTMCell(rnn_size)\n\n                stack = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(0, num_layers)], state_is_tuple=True)\n                net, _ = tf.nn.dynamic_rnn(stack, net, dtype=tf.float32)\n                # (B, T, D) => (T, B, D)\n                net = tf.transpose(net, (1, 0, 2))\n                net = attention(net, attention_size, True)\n                net = slim.fully_connected(net, num_classes, activation_fn=None, scope='Logits_out1')\n        return net, net_vis\n\nAt the first, I export the graph as following.\n    X = tf.placeholder(tf.float32, [None, height, width, 3], name=\"inputs_placeholder\")\n    net, net_vis = build_model(X, num_classes, 1.0, False, arch_model)\n    net = tf.nn.sigmoid(net)\n    predict = tf.reshape(net, [-1, num_classes], name='predictions')\n\n    graph_def = graph.as_graph_def()\n\n    with gfile.GFile(FLAGS.output_file, 'wb') as f:\n      f.write(graph_def.SerializeToString())\n\nThen, freeze the model using the api\npython /root/anaconda3/pkgs/tensorflow-base-1.8.0-py36hc1a7637_0/lib/python3.6/site- \n    packages/tensorflow/python/tools/freeze_graph.py \\\n      --input_graph=${INFERENCE_GRAPH_PATH}/${TMP_GRAPH} \\\n      --input_checkpoint=${CHECKPOINT_PATH}/model.ckpt-3800 \\\n      --input_binary=true \\\n      --output_graph=${CHECKPOINT_PATH}/${EXPORT_INF_GRAPH} \\\n      --output_node_names=${OUTPUT_NODE_NAME}\n\nThe error is shown as following.\n[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?", "body": "- OS Platform:  Ubuntu 16.04\r\n- TensorFlow installed from:  anaconda\r\n- TensorFlow version: GPU-1.8\r\n- Python version: 3.6.6\r\n- CUDA/cuDNN version: 9.0 / 7.1.2\r\n- GPU model and memory:16G\r\n\r\nThe model is mobilenetV2 + LSTM + attention.\r\nMobilenetV2 is restored from the model zoo in [https://github.com/tensorflow/models/tree/master/research/slim](https://github.com/tensorflow/models/tree/master/research/slim). It is not trained during the training of the whole model.\r\nLSTM and attention is trained.\r\n\r\nThe code about model is shown as following.\r\n```\r\n    def mobilenet_v2_rnn_attention(self, X, num_classes, dropout_keep_prob=0.8, is_train=False):\r\n        rnn_size = 4096\r\n        num_layers = 8\r\n        attention_size = 1024\r\n        arg_scope = training_scope()\r\n        with slim.arg_scope(arg_scope):\r\n            net_vis, end_points = mobilenet_base(X, num_classes=num_classes)\r\n        with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d], stride=1, padding='SAME'):\r\n            with tf.variable_scope('Logits_out'):\r\n                orig_shape = net_vis.get_shape().as_list()\r\n                net = tf.reshape(net_vis, [-1, orig_shape[1] * orig_shape[2], orig_shape[3]])\r\n\r\n                def lstm_cell():\r\n                    return tf.contrib.rnn.LSTMCell(rnn_size)\r\n\r\n                stack = tf.contrib.rnn.MultiRNNCell([lstm_cell() for _ in range(0, num_layers)], state_is_tuple=True)\r\n                net, _ = tf.nn.dynamic_rnn(stack, net, dtype=tf.float32)\r\n                # (B, T, D) => (T, B, D)\r\n                net = tf.transpose(net, (1, 0, 2))\r\n                net = attention(net, attention_size, True)\r\n                net = slim.fully_connected(net, num_classes, activation_fn=None, scope='Logits_out1')\r\n        return net, net_vis\r\n```\r\n\r\n\r\n\r\n\r\nAt the first, I export the graph as following.\r\n```\r\n    X = tf.placeholder(tf.float32, [None, height, width, 3], name=\"inputs_placeholder\")\r\n    net, net_vis = build_model(X, num_classes, 1.0, False, arch_model)\r\n    net = tf.nn.sigmoid(net)\r\n    predict = tf.reshape(net, [-1, num_classes], name='predictions')\r\n\r\n    graph_def = graph.as_graph_def()\r\n\r\n    with gfile.GFile(FLAGS.output_file, 'wb') as f:\r\n      f.write(graph_def.SerializeToString())\r\n```\r\n\r\nThen, freeze the model using the api\r\n\r\n```\r\npython /root/anaconda3/pkgs/tensorflow-base-1.8.0-py36hc1a7637_0/lib/python3.6/site- \r\n    packages/tensorflow/python/tools/freeze_graph.py \\\r\n      --input_graph=${INFERENCE_GRAPH_PATH}/${TMP_GRAPH} \\\r\n      --input_checkpoint=${CHECKPOINT_PATH}/model.ckpt-3800 \\\r\n      --input_binary=true \\\r\n      --output_graph=${CHECKPOINT_PATH}/${EXPORT_INF_GRAPH} \\\r\n      --output_node_names=${OUTPUT_NODE_NAME}\r\n```\r\n\r\nThe error is shown as following.\r\n\r\n`[libprotobuf FATAL google/protobuf/wire_format.cc:830] CHECK failed: (output->ByteCount()) == (expected_endpoint): : Protocol message serialized to a size different from what was originally expected.  Perhaps it was modified by another thread during serialization?`"}