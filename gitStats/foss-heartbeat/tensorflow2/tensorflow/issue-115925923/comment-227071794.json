{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/227071794", "html_url": "https://github.com/tensorflow/tensorflow/issues/16#issuecomment-227071794", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16", "id": 227071794, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNzA3MTc5NA==", "user": {"login": "StephenOman", "id": 4146078, "node_id": "MDQ6VXNlcjQxNDYwNzg=", "avatar_url": "https://avatars1.githubusercontent.com/u/4146078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StephenOman", "html_url": "https://github.com/StephenOman", "followers_url": "https://api.github.com/users/StephenOman/followers", "following_url": "https://api.github.com/users/StephenOman/following{/other_user}", "gists_url": "https://api.github.com/users/StephenOman/gists{/gist_id}", "starred_url": "https://api.github.com/users/StephenOman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StephenOman/subscriptions", "organizations_url": "https://api.github.com/users/StephenOman/orgs", "repos_url": "https://api.github.com/users/StephenOman/repos", "events_url": "https://api.github.com/users/StephenOman/events{/privacy}", "received_events_url": "https://api.github.com/users/StephenOman/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-20T07:45:42Z", "updated_at": "2016-06-20T07:45:42Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2171885\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/karthiksekarnz\">@karthiksekarnz</a> I understand that the architecture for a photo recognition app on mobile client is that a pre-trained model is bundled with the app and downloaded by the user from an app store. The app then runs an inference on the neural network (i.e. graph execution) only. There is no learning capability as yet in the mobile app.</p>\n<p>I suspect that this approach is taken because a mobile phone doesn't have enough computing power to do learning efficiently, given that lots of models are built with multi-GPUs. But that remains to be seen because the TensorFlow public C++ API doesn't yet support the wide range of network training capabilities that the Python API does.</p>\n<p>It's also probably true that to transfer images from the camera's video stream to a server in real-time would be a bandwidth hog. If the user was on a cellular network, it may also be expensive, depending on their bundle.</p>\n<p>It's an interesting architecture problem though. Lots of interesting data that could be modelled will be captured by phones, but the learning has to be server based at the moment.</p>", "body_text": "@karthiksekarnz I understand that the architecture for a photo recognition app on mobile client is that a pre-trained model is bundled with the app and downloaded by the user from an app store. The app then runs an inference on the neural network (i.e. graph execution) only. There is no learning capability as yet in the mobile app.\nI suspect that this approach is taken because a mobile phone doesn't have enough computing power to do learning efficiently, given that lots of models are built with multi-GPUs. But that remains to be seen because the TensorFlow public C++ API doesn't yet support the wide range of network training capabilities that the Python API does.\nIt's also probably true that to transfer images from the camera's video stream to a server in real-time would be a bandwidth hog. If the user was on a cellular network, it may also be expensive, depending on their bundle.\nIt's an interesting architecture problem though. Lots of interesting data that could be modelled will be captured by phones, but the learning has to be server based at the moment.", "body": "@karthiksekarnz I understand that the architecture for a photo recognition app on mobile client is that a pre-trained model is bundled with the app and downloaded by the user from an app store. The app then runs an inference on the neural network (i.e. graph execution) only. There is no learning capability as yet in the mobile app.\n\nI suspect that this approach is taken because a mobile phone doesn't have enough computing power to do learning efficiently, given that lots of models are built with multi-GPUs. But that remains to be seen because the TensorFlow public C++ API doesn't yet support the wide range of network training capabilities that the Python API does.\n\nIt's also probably true that to transfer images from the camera's video stream to a server in real-time would be a bandwidth hog. If the user was on a cellular network, it may also be expensive, depending on their bundle.\n\nIt's an interesting architecture problem though. Lots of interesting data that could be modelled will be captured by phones, but the learning has to be server based at the moment.\n"}