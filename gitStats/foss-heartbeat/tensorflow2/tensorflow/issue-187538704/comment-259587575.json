{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259587575", "html_url": "https://github.com/tensorflow/tensorflow/issues/5422#issuecomment-259587575", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5422", "id": 259587575, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTU4NzU3NQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T02:42:07Z", "updated_at": "2016-11-10T02:43:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=21980268\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/QuantumLiu\">@QuantumLiu</a> generally TensorFlow has been optimized for large models/large data, and it takes some work to make it efficient for tiny models/tiny data. In the case of MNIST, the issue the common source of bottlenecks is the data transfer between Python runtime and TensorFlow runtime. There are at least 2 copies incurred -- copying from Python to TensorFlow CPU, and then internally, copying data from CPU to GPU. Since these copies are blocking, things are not efficient, and you can improve things by pre-loading data in parallel\u00a0using queues/input pipelines. So the thing to check is how <code>fit</code> is implemented in Keras and how efficient it is with data transfers. One caveat for pre-loading, it is inefficient for tiny datasets/models like MNIST because Python can not switch threads <a href=\"http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data/39842628#39842628\" rel=\"nofollow\">fast enough</a></p>", "body_text": "@QuantumLiu generally TensorFlow has been optimized for large models/large data, and it takes some work to make it efficient for tiny models/tiny data. In the case of MNIST, the issue the common source of bottlenecks is the data transfer between Python runtime and TensorFlow runtime. There are at least 2 copies incurred -- copying from Python to TensorFlow CPU, and then internally, copying data from CPU to GPU. Since these copies are blocking, things are not efficient, and you can improve things by pre-loading data in parallel\u00a0using queues/input pipelines. So the thing to check is how fit is implemented in Keras and how efficient it is with data transfers. One caveat for pre-loading, it is inefficient for tiny datasets/models like MNIST because Python can not switch threads fast enough", "body": "@QuantumLiu generally TensorFlow has been optimized for large models/large data, and it takes some work to make it efficient for tiny models/tiny data. In the case of MNIST, the issue the common source of bottlenecks is the data transfer between Python runtime and TensorFlow runtime. There are at least 2 copies incurred -- copying from Python to TensorFlow CPU, and then internally, copying data from CPU to GPU. Since these copies are blocking, things are not efficient, and you can improve things by pre-loading data in parallel\u00a0using queues/input pipelines. So the thing to check is how `fit` is implemented in Keras and how efficient it is with data transfers. One caveat for pre-loading, it is inefficient for tiny datasets/models like MNIST because Python can not switch threads [fast enough](http://stackoverflow.com/questions/39840323/benchmark-of-howto-reading-data/39842628#39842628)\n"}