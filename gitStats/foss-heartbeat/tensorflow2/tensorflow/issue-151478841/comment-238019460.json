{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/238019460", "html_url": "https://github.com/tensorflow/tensorflow/issues/2138#issuecomment-238019460", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2138", "id": 238019460, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODAxOTQ2MA==", "user": {"login": "fractalego", "id": 9697264, "node_id": "MDQ6VXNlcjk2OTcyNjQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/9697264?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fractalego", "html_url": "https://github.com/fractalego", "followers_url": "https://api.github.com/users/fractalego/followers", "following_url": "https://api.github.com/users/fractalego/following{/other_user}", "gists_url": "https://api.github.com/users/fractalego/gists{/gist_id}", "starred_url": "https://api.github.com/users/fractalego/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fractalego/subscriptions", "organizations_url": "https://api.github.com/users/fractalego/orgs", "repos_url": "https://api.github.com/users/fractalego/repos", "events_url": "https://api.github.com/users/fractalego/events{/privacy}", "received_events_url": "https://api.github.com/users/fractalego/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-06T11:44:29Z", "updated_at": "2016-08-07T18:12:55Z", "author_association": "NONE", "body_html": "<p>Thank you for your quick reply. I was using TF 0.9.0 when I posted the first comment. I upgraded to 0.10 and with the new version the error message is more clear.</p>\n<p>The issue is an out of memory problem: with embedding_attention_seq2seq an additional tensor is created with size [num_symbols x  (num_symbols+memory_dim)]. Since my vocabolary size is ~100000 I run out of memory.</p>\n<p>My guess is that this issue should be closed because it works as designed.</p>\n<p>I could ask you why the additional tensor is not shaped as [embedding_dim x (embedding_dim + memory_dim)], but this would be out of the current problem <g-emoji class=\"g-emoji\" alias=\"smiley\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f603.png\">\ud83d\ude03</g-emoji></p>", "body_text": "Thank you for your quick reply. I was using TF 0.9.0 when I posted the first comment. I upgraded to 0.10 and with the new version the error message is more clear.\nThe issue is an out of memory problem: with embedding_attention_seq2seq an additional tensor is created with size [num_symbols x  (num_symbols+memory_dim)]. Since my vocabolary size is ~100000 I run out of memory.\nMy guess is that this issue should be closed because it works as designed.\nI could ask you why the additional tensor is not shaped as [embedding_dim x (embedding_dim + memory_dim)], but this would be out of the current problem \ud83d\ude03", "body": "Thank you for your quick reply. I was using TF 0.9.0 when I posted the first comment. I upgraded to 0.10 and with the new version the error message is more clear.\n\nThe issue is an out of memory problem: with embedding_attention_seq2seq an additional tensor is created with size [num_symbols x  (num_symbols+memory_dim)]. Since my vocabolary size is ~100000 I run out of memory. \n\nMy guess is that this issue should be closed because it works as designed. \n\nI could ask you why the additional tensor is not shaped as [embedding_dim x (embedding_dim + memory_dim)], but this would be out of the current problem :smiley: \n"}