{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217037967", "html_url": "https://github.com/tensorflow/tensorflow/issues/2138#issuecomment-217037967", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2138", "id": 217037967, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzAzNzk2Nw==", "user": {"login": "harpribot", "id": 7632939, "node_id": "MDQ6VXNlcjc2MzI5Mzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7632939?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harpribot", "html_url": "https://github.com/harpribot", "followers_url": "https://api.github.com/users/harpribot/followers", "following_url": "https://api.github.com/users/harpribot/following{/other_user}", "gists_url": "https://api.github.com/users/harpribot/gists{/gist_id}", "starred_url": "https://api.github.com/users/harpribot/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harpribot/subscriptions", "organizations_url": "https://api.github.com/users/harpribot/orgs", "repos_url": "https://api.github.com/users/harpribot/repos", "events_url": "https://api.github.com/users/harpribot/events{/privacy}", "received_events_url": "https://api.github.com/users/harpribot/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-04T23:28:38Z", "updated_at": "2016-05-04T23:28:38Z", "author_association": "NONE", "body_html": "<p>yes it happens with that as well. Did it work for you? If yes, can you share your script that got it to work? I am not using separate decoder and encoder model. Just the wrapper code of run_seq2seq for both simple embedding and attention embedding.</p>", "body_text": "yes it happens with that as well. Did it work for you? If yes, can you share your script that got it to work? I am not using separate decoder and encoder model. Just the wrapper code of run_seq2seq for both simple embedding and attention embedding.", "body": "yes it happens with that as well. Did it work for you? If yes, can you share your script that got it to work? I am not using separate decoder and encoder model. Just the wrapper code of run_seq2seq for both simple embedding and attention embedding.\n"}