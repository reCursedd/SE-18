{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8630", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8630/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8630/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8630/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8630", "id": 216179160, "node_id": "MDU6SXNzdWUyMTYxNzkxNjA=", "number": 8630, "title": "NAN returning for cost and optimizer for tensorflow.train.GradientDescentOptimizer (updated code)", "user": {"login": "Ocean47", "id": 20768200, "node_id": "MDQ6VXNlcjIwNzY4MjAw", "avatar_url": "https://avatars0.githubusercontent.com/u/20768200?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ocean47", "html_url": "https://github.com/Ocean47", "followers_url": "https://api.github.com/users/Ocean47/followers", "following_url": "https://api.github.com/users/Ocean47/following{/other_user}", "gists_url": "https://api.github.com/users/Ocean47/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ocean47/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ocean47/subscriptions", "organizations_url": "https://api.github.com/users/Ocean47/orgs", "repos_url": "https://api.github.com/users/Ocean47/repos", "events_url": "https://api.github.com/users/Ocean47/events{/privacy}", "received_events_url": "https://api.github.com/users/Ocean47/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-22T19:17:38Z", "updated_at": "2017-03-24T16:49:02Z", "closed_at": "2017-03-24T16:48:57Z", "author_association": "NONE", "body_html": "<p>Been working on this all day now and totally at a loss...  I keep getting Nan values for my cost function right away and i cannot tell why.  Any help much appreciated at this point....</p>\n<p>I am running the following code with tensorflow GradientDescentOptmizer...</p>\n<pre><code>#!/usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom   os.path import isfile, join\nimport argparse\nimport sys\nimport glob\nimport pandas as pd\nimport csv\nimport re\nimport tempfile\nimport urllib\nimport matplotlib.pyplot as plt\n\n# Start basic regression\ntf.reset_default_graph()\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.0001\ntraining_epochs = 1000\ndisplay_step = 50\n\nlogs_path = '/tmp/tensorflow_logs/example'\n\n# Bad Training Data\ntrain_Y = np.asarray([  59.8000,   60.5000,   60.9000,   61.0000,   61.5000,   64.0000,   64.5000,\n                        64.8000,   67.8000,   71.2000,   72.0000,   78.9000,   79.2000,   81.0000,\n                        82.6000,   84.0000,   84.0000])\ntrain_X = np.asarray([600., 760., 802., 568., 679., 865., 1103., 865., 896., 1068.,\n                        769., 1062., 1123., 1081., 1137., 1137., 1137.])\n# Good Training Data\n#train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n#                     7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n#train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n#                     2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n\nprint(train_X.dtype)\nprint(train_Y.dtype)\nprint(str(train_X))\nprint(str(train_Y))\n\nn_samples = train_X.shape[0]\n\nprint(\"Samples = %d\" % n_samples)\n\n# tf Graph Input\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\n\n# Set model weights\nW = tf.Variable(1.0, name=\"weight\")\nb = tf.Variable(1.0, name=\"bias\")\n\n# Construct a linear model\nwith tf.name_scope('Model'):\n    pred = tf.add(tf.multiply(X, W), b)\n\n# Mean squared error\nwith tf.name_scope('Loss'):\n    cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n# Gradient descent\nwith tf.name_scope('SGD'):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# summaries\ntf.summary.scalar(\"loss\",cost)\nmerged_summary_op = tf.summary.merge_all()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    # op to write logs to Tensorboard\n    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n    # Fit all training data\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            #print(\"x=%f y=%f\" % (x,y))\n            op, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={X: x, Y: y})\n\n            summary_writer.add_summary(summary, epoch)\n\n            # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            c, summmary = sess.run([cost, merged_summary_op], feed_dict={X: train_X, Y:train_Y})\n            summary_writer.add_summary(summary, epoch)                \n              \n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n            \"W=\", sess.run(W), \"b=\", sess.run(b))\n\n    print(\"Optimization Finished!\")\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n\n    # Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()\n\n    # Bad Test Data\n    test_Y = np.asarray([ 48.2000,  56.5000,  57.0000,  59.5000,  15.0000,  17.8000,  43.5000,  50.2000])\n    test_X = np.asarray([ 549.,  710.,  568.,  825., 414.,  439.,  460.,  614. ])\n\n    # Good Test Data\n    #test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    #test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n\n\n    print(\"Testing... (Mean square loss Comparison)\")\n    testing_cost = sess.run(\n        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),\n        feed_dict={X: test_X, Y: test_Y})  # same function as cost above\n    print(\"Testing cost=\", testing_cost)\n    print(\"Absolute mean square loss difference:\", abs(\n        training_cost - testing_cost))\n\n    plt.plot(test_X, test_Y, 'bo', label='Testing data')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()`\n</code></pre>\n<p>And get the following result...</p>\n<blockquote>\n<p>float64<br>\nfloat64<br>\n[  600.   760.   802.   568.   679.   865.  1103.   865.   896.  1068.<br>\n769.  1062.  1123.  1081.  1137.  1137.  1137.]<br>\n[ 59.8  60.5  60.9  61.   61.5  64.   64.5  64.8  67.8  71.2  72.   78.9<br>\n79.2  81.   82.6  84.   84. ]<br>\nSamples = 17<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.<br>\nEpoch: 0050 cost= nan W= nan b= nan<br>\nEpoch: 0100 cost= nan W= nan b= nan<br>\nEpoch: 0150 cost= nan W= nan b= nan<br>\nEpoch: 0200 cost= nan W= nan b= nan<br>\nEpoch: 0250 cost= nan W= nan b= nan<br>\nEpoch: 0300 cost= nan W= nan b= nan<br>\nEpoch: 0350 cost= nan W= nan b= nan<br>\nEpoch: 0400 cost= nan W= nan b= nan<br>\nEpoch: 0450 cost= nan W= nan b= nan<br>\nEpoch: 0500 cost= nan W= nan b= nan<br>\nEpoch: 0550 cost= nan W= nan b= nan<br>\nEpoch: 0600 cost= nan W= nan b= nan<br>\nEpoch: 0650 cost= nan W= nan b= nan<br>\nEpoch: 0700 cost= nan W= nan b= nan<br>\nEpoch: 0750 cost= nan W= nan b= nan<br>\nEpoch: 0800 cost= nan W= nan b= nan<br>\nEpoch: 0850 cost= nan W= nan b= nan<br>\nEpoch: 0900 cost= nan W= nan b= nan<br>\nEpoch: 0950 cost= nan W= nan b= nan<br>\nEpoch: 1000 cost= nan W= nan b= nan<br>\nOptimization Finished!<br>\nTraining cost= nan W= nan b= nan</p>\n<p>Testing... (Mean square loss Comparison)<br>\nTesting cost= nan<br>\nAbsolute mean square loss difference: nan<br>\nAnd get the following output...</p>\n</blockquote>\n<p>if I uncomment out the alternate array values for X amd Y everything works fine...</p>\n<blockquote>\n<p>float64<br>\nfloat64<br>\n[  3.3     4.4     5.5     6.71    6.93    4.168   9.779   6.182   7.59<br>\n2.167   7.042  10.791   5.313   7.997   5.654   9.27    3.1  ]<br>\n[ 1.7    2.76   2.09   3.19   1.694  1.573  3.366  2.596  2.53   1.221<br>\n2.827  3.465  1.65   2.904  2.42   2.94   1.3  ]<br>\nSamples = 17<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.<br>\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.<br>\nEpoch: 0050 cost= 8.580235481 W= 0.846042 b= 0.978224<br>\nEpoch: 0100 cost= 5.489832401 W= 0.72321 b= 0.960839<br>\nEpoch: 0150 cost= 3.522622824 W= 0.625209 b= 0.946956<br>\nEpoch: 0200 cost= 2.270416975 W= 0.547022 b= 0.935869<br>\nEpoch: 0250 cost= 1.473346949 W= 0.484644 b= 0.927012<br>\nEpoch: 0300 cost= 0.965984821 W= 0.434878 b= 0.919934<br>\nEpoch: 0350 cost= 0.643029690 W= 0.395175 b= 0.914275<br>\nEpoch: 0400 cost= 0.437456042 W= 0.363499 b= 0.909749<br>\nEpoch: 0450 cost= 0.306603283 W= 0.338229 b= 0.906127<br>\nEpoch: 0500 cost= 0.223311335 W= 0.318069 b= 0.903225<br>\nEpoch: 0550 cost= 0.170292616 W= 0.301985 b= 0.900899<br>\nEpoch: 0600 cost= 0.136546493 W= 0.289155 b= 0.899031<br>\nEpoch: 0650 cost= 0.115067258 W= 0.278921 b= 0.89753<br>\nEpoch: 0700 cost= 0.101395160 W= 0.270757 b= 0.89632<br>\nEpoch: 0750 cost= 0.092692636 W= 0.264245 b= 0.895344<br>\nEpoch: 0800 cost= 0.087154001 W= 0.259051 b= 0.894554<br>\nEpoch: 0850 cost= 0.083628759 W= 0.254909 b= 0.893912<br>\nEpoch: 0900 cost= 0.081385054 W= 0.251606 b= 0.893389<br>\nEpoch: 0950 cost= 0.079956941 W= 0.248972 b= 0.892961<br>\nEpoch: 1000 cost= 0.079047829 W= 0.246872 b= 0.892606<br>\nOptimization Finished!<br>\nTraining cost= 0.0790478 W= 0.246872 b= 0.892606</p>\n<p>Testing... (Mean square loss Comparison)<br>\nTesting cost= 0.0796623<br>\nAbsolute mean square loss difference: 0.000614464</p>\n</blockquote>\n<p>I am using tensorflow 1.01 libraries, and running on a mac OS Sierra...</p>\n<p>Banging my head against the wall trying to figure this out. As you can see I tried to setup tensorboad to help, but seems I have limited success, get a graph but so far has not been any help debugging this...</p>", "body_text": "Been working on this all day now and totally at a loss...  I keep getting Nan values for my cost function right away and i cannot tell why.  Any help much appreciated at this point....\nI am running the following code with tensorflow GradientDescentOptmizer...\n#!/usr/bin/env python\n\nimport tensorflow as tf\nimport numpy as np\nimport os\nfrom   os.path import isfile, join\nimport argparse\nimport sys\nimport glob\nimport pandas as pd\nimport csv\nimport re\nimport tempfile\nimport urllib\nimport matplotlib.pyplot as plt\n\n# Start basic regression\ntf.reset_default_graph()\n\nrng = np.random\n\n# Parameters\nlearning_rate = 0.0001\ntraining_epochs = 1000\ndisplay_step = 50\n\nlogs_path = '/tmp/tensorflow_logs/example'\n\n# Bad Training Data\ntrain_Y = np.asarray([  59.8000,   60.5000,   60.9000,   61.0000,   61.5000,   64.0000,   64.5000,\n                        64.8000,   67.8000,   71.2000,   72.0000,   78.9000,   79.2000,   81.0000,\n                        82.6000,   84.0000,   84.0000])\ntrain_X = np.asarray([600., 760., 802., 568., 679., 865., 1103., 865., 896., 1068.,\n                        769., 1062., 1123., 1081., 1137., 1137., 1137.])\n# Good Training Data\n#train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\n#                     7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n#train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\n#                     2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n\nprint(train_X.dtype)\nprint(train_Y.dtype)\nprint(str(train_X))\nprint(str(train_Y))\n\nn_samples = train_X.shape[0]\n\nprint(\"Samples = %d\" % n_samples)\n\n# tf Graph Input\nX = tf.placeholder(\"float\")\nY = tf.placeholder(\"float\")\n\n# Set model weights\nW = tf.Variable(1.0, name=\"weight\")\nb = tf.Variable(1.0, name=\"bias\")\n\n# Construct a linear model\nwith tf.name_scope('Model'):\n    pred = tf.add(tf.multiply(X, W), b)\n\n# Mean squared error\nwith tf.name_scope('Loss'):\n    cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\n# Gradient descent\nwith tf.name_scope('SGD'):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n\n# Initializing the variables\ninit = tf.global_variables_initializer()\n\n# summaries\ntf.summary.scalar(\"loss\",cost)\nmerged_summary_op = tf.summary.merge_all()\n\n# Launch the graph\nwith tf.Session() as sess:\n    sess.run(init)\n    # op to write logs to Tensorboard\n    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\n\n    # Fit all training data\n    for epoch in range(training_epochs):\n        for (x, y) in zip(train_X, train_Y):\n            #print(\"x=%f y=%f\" % (x,y))\n            op, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={X: x, Y: y})\n\n            summary_writer.add_summary(summary, epoch)\n\n            # Display logs per epoch step\n        if (epoch+1) % display_step == 0:\n            c, summmary = sess.run([cost, merged_summary_op], feed_dict={X: train_X, Y:train_Y})\n            summary_writer.add_summary(summary, epoch)                \n              \n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\n            \"W=\", sess.run(W), \"b=\", sess.run(b))\n\n    print(\"Optimization Finished!\")\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\n\n    # Graphic display\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()\n\n    # Bad Test Data\n    test_Y = np.asarray([ 48.2000,  56.5000,  57.0000,  59.5000,  15.0000,  17.8000,  43.5000,  50.2000])\n    test_X = np.asarray([ 549.,  710.,  568.,  825., 414.,  439.,  460.,  614. ])\n\n    # Good Test Data\n    #test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\n    #test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\n\n\n    print(\"Testing... (Mean square loss Comparison)\")\n    testing_cost = sess.run(\n        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),\n        feed_dict={X: test_X, Y: test_Y})  # same function as cost above\n    print(\"Testing cost=\", testing_cost)\n    print(\"Absolute mean square loss difference:\", abs(\n        training_cost - testing_cost))\n\n    plt.plot(test_X, test_Y, 'bo', label='Testing data')\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\n    plt.legend()\n    plt.show()`\n\nAnd get the following result...\n\nfloat64\nfloat64\n[  600.   760.   802.   568.   679.   865.  1103.   865.   896.  1068.\n769.  1062.  1123.  1081.  1137.  1137.  1137.]\n[ 59.8  60.5  60.9  61.   61.5  64.   64.5  64.8  67.8  71.2  72.   78.9\n79.2  81.   82.6  84.   84. ]\nSamples = 17\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nEpoch: 0050 cost= nan W= nan b= nan\nEpoch: 0100 cost= nan W= nan b= nan\nEpoch: 0150 cost= nan W= nan b= nan\nEpoch: 0200 cost= nan W= nan b= nan\nEpoch: 0250 cost= nan W= nan b= nan\nEpoch: 0300 cost= nan W= nan b= nan\nEpoch: 0350 cost= nan W= nan b= nan\nEpoch: 0400 cost= nan W= nan b= nan\nEpoch: 0450 cost= nan W= nan b= nan\nEpoch: 0500 cost= nan W= nan b= nan\nEpoch: 0550 cost= nan W= nan b= nan\nEpoch: 0600 cost= nan W= nan b= nan\nEpoch: 0650 cost= nan W= nan b= nan\nEpoch: 0700 cost= nan W= nan b= nan\nEpoch: 0750 cost= nan W= nan b= nan\nEpoch: 0800 cost= nan W= nan b= nan\nEpoch: 0850 cost= nan W= nan b= nan\nEpoch: 0900 cost= nan W= nan b= nan\nEpoch: 0950 cost= nan W= nan b= nan\nEpoch: 1000 cost= nan W= nan b= nan\nOptimization Finished!\nTraining cost= nan W= nan b= nan\nTesting... (Mean square loss Comparison)\nTesting cost= nan\nAbsolute mean square loss difference: nan\nAnd get the following output...\n\nif I uncomment out the alternate array values for X amd Y everything works fine...\n\nfloat64\nfloat64\n[  3.3     4.4     5.5     6.71    6.93    4.168   9.779   6.182   7.59\n2.167   7.042  10.791   5.313   7.997   5.654   9.27    3.1  ]\n[ 1.7    2.76   2.09   3.19   1.694  1.573  3.366  2.596  2.53   1.221\n2.827  3.465  1.65   2.904  2.42   2.94   1.3  ]\nSamples = 17\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\nW tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nEpoch: 0050 cost= 8.580235481 W= 0.846042 b= 0.978224\nEpoch: 0100 cost= 5.489832401 W= 0.72321 b= 0.960839\nEpoch: 0150 cost= 3.522622824 W= 0.625209 b= 0.946956\nEpoch: 0200 cost= 2.270416975 W= 0.547022 b= 0.935869\nEpoch: 0250 cost= 1.473346949 W= 0.484644 b= 0.927012\nEpoch: 0300 cost= 0.965984821 W= 0.434878 b= 0.919934\nEpoch: 0350 cost= 0.643029690 W= 0.395175 b= 0.914275\nEpoch: 0400 cost= 0.437456042 W= 0.363499 b= 0.909749\nEpoch: 0450 cost= 0.306603283 W= 0.338229 b= 0.906127\nEpoch: 0500 cost= 0.223311335 W= 0.318069 b= 0.903225\nEpoch: 0550 cost= 0.170292616 W= 0.301985 b= 0.900899\nEpoch: 0600 cost= 0.136546493 W= 0.289155 b= 0.899031\nEpoch: 0650 cost= 0.115067258 W= 0.278921 b= 0.89753\nEpoch: 0700 cost= 0.101395160 W= 0.270757 b= 0.89632\nEpoch: 0750 cost= 0.092692636 W= 0.264245 b= 0.895344\nEpoch: 0800 cost= 0.087154001 W= 0.259051 b= 0.894554\nEpoch: 0850 cost= 0.083628759 W= 0.254909 b= 0.893912\nEpoch: 0900 cost= 0.081385054 W= 0.251606 b= 0.893389\nEpoch: 0950 cost= 0.079956941 W= 0.248972 b= 0.892961\nEpoch: 1000 cost= 0.079047829 W= 0.246872 b= 0.892606\nOptimization Finished!\nTraining cost= 0.0790478 W= 0.246872 b= 0.892606\nTesting... (Mean square loss Comparison)\nTesting cost= 0.0796623\nAbsolute mean square loss difference: 0.000614464\n\nI am using tensorflow 1.01 libraries, and running on a mac OS Sierra...\nBanging my head against the wall trying to figure this out. As you can see I tried to setup tensorboad to help, but seems I have limited success, get a graph but so far has not been any help debugging this...", "body": "Been working on this all day now and totally at a loss...  I keep getting Nan values for my cost function right away and i cannot tell why.  Any help much appreciated at this point....\r\n\r\nI am running the following code with tensorflow GradientDescentOptmizer...\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nfrom   os.path import isfile, join\r\nimport argparse\r\nimport sys\r\nimport glob\r\nimport pandas as pd\r\nimport csv\r\nimport re\r\nimport tempfile\r\nimport urllib\r\nimport matplotlib.pyplot as plt\r\n\r\n# Start basic regression\r\ntf.reset_default_graph()\r\n\r\nrng = np.random\r\n\r\n# Parameters\r\nlearning_rate = 0.0001\r\ntraining_epochs = 1000\r\ndisplay_step = 50\r\n\r\nlogs_path = '/tmp/tensorflow_logs/example'\r\n\r\n# Bad Training Data\r\ntrain_Y = np.asarray([  59.8000,   60.5000,   60.9000,   61.0000,   61.5000,   64.0000,   64.5000,\r\n                        64.8000,   67.8000,   71.2000,   72.0000,   78.9000,   79.2000,   81.0000,\r\n                        82.6000,   84.0000,   84.0000])\r\ntrain_X = np.asarray([600., 760., 802., 568., 679., 865., 1103., 865., 896., 1068.,\r\n                        769., 1062., 1123., 1081., 1137., 1137., 1137.])\r\n# Good Training Data\r\n#train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,\r\n#                     7.042,10.791,5.313,7.997,5.654,9.27,3.1])\r\n#train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,\r\n#                     2.827,3.465,1.65,2.904,2.42,2.94,1.3])\r\n\r\nprint(train_X.dtype)\r\nprint(train_Y.dtype)\r\nprint(str(train_X))\r\nprint(str(train_Y))\r\n\r\nn_samples = train_X.shape[0]\r\n\r\nprint(\"Samples = %d\" % n_samples)\r\n\r\n# tf Graph Input\r\nX = tf.placeholder(\"float\")\r\nY = tf.placeholder(\"float\")\r\n\r\n# Set model weights\r\nW = tf.Variable(1.0, name=\"weight\")\r\nb = tf.Variable(1.0, name=\"bias\")\r\n\r\n# Construct a linear model\r\nwith tf.name_scope('Model'):\r\n    pred = tf.add(tf.multiply(X, W), b)\r\n\r\n# Mean squared error\r\nwith tf.name_scope('Loss'):\r\n    cost = tf.reduce_sum(tf.pow(pred-Y, 2))/(2*n_samples)\r\n# Gradient descent\r\nwith tf.name_scope('SGD'):\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\r\n\r\n# Initializing the variables\r\ninit = tf.global_variables_initializer()\r\n\r\n# summaries\r\ntf.summary.scalar(\"loss\",cost)\r\nmerged_summary_op = tf.summary.merge_all()\r\n\r\n# Launch the graph\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    # op to write logs to Tensorboard\r\n    summary_writer = tf.summary.FileWriter(logs_path, graph=tf.get_default_graph())\r\n\r\n    # Fit all training data\r\n    for epoch in range(training_epochs):\r\n        for (x, y) in zip(train_X, train_Y):\r\n            #print(\"x=%f y=%f\" % (x,y))\r\n            op, c, summary = sess.run([optimizer, cost, merged_summary_op], feed_dict={X: x, Y: y})\r\n\r\n            summary_writer.add_summary(summary, epoch)\r\n\r\n            # Display logs per epoch step\r\n        if (epoch+1) % display_step == 0:\r\n            c, summmary = sess.run([cost, merged_summary_op], feed_dict={X: train_X, Y:train_Y})\r\n            summary_writer.add_summary(summary, epoch)                \r\n              \r\n            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c), \\\r\n            \"W=\", sess.run(W), \"b=\", sess.run(b))\r\n\r\n    print(\"Optimization Finished!\")\r\n    training_cost = sess.run(cost, feed_dict={X: train_X, Y: train_Y})\r\n    print(\"Training cost=\", training_cost, \"W=\", sess.run(W), \"b=\", sess.run(b), '\\n')\r\n\r\n    # Graphic display\r\n    plt.plot(train_X, train_Y, 'ro', label='Original data')\r\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\r\n    plt.legend()\r\n    plt.show()\r\n\r\n    # Bad Test Data\r\n    test_Y = np.asarray([ 48.2000,  56.5000,  57.0000,  59.5000,  15.0000,  17.8000,  43.5000,  50.2000])\r\n    test_X = np.asarray([ 549.,  710.,  568.,  825., 414.,  439.,  460.,  614. ])\r\n\r\n    # Good Test Data\r\n    #test_X = np.asarray([6.83, 4.668, 8.9, 7.91, 5.7, 8.7, 3.1, 2.1])\r\n    #test_Y = np.asarray([1.84, 2.273, 3.2, 2.831, 2.92, 3.24, 1.35, 1.03])\r\n\r\n\r\n    print(\"Testing... (Mean square loss Comparison)\")\r\n    testing_cost = sess.run(\r\n        tf.reduce_sum(tf.pow(pred - Y, 2)) / (2 * test_X.shape[0]),\r\n        feed_dict={X: test_X, Y: test_Y})  # same function as cost above\r\n    print(\"Testing cost=\", testing_cost)\r\n    print(\"Absolute mean square loss difference:\", abs(\r\n        training_cost - testing_cost))\r\n\r\n    plt.plot(test_X, test_Y, 'bo', label='Testing data')\r\n    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), label='Fitted line')\r\n    plt.legend()\r\n    plt.show()`\r\n```\r\n\r\nAnd get the following result...\r\n\r\n> float64\r\n> float64\r\n> [  600.   760.   802.   568.   679.   865.  1103.   865.   896.  1068.\r\n>    769.  1062.  1123.  1081.  1137.  1137.  1137.]\r\n> [ 59.8  60.5  60.9  61.   61.5  64.   64.5  64.8  67.8  71.2  72.   78.9\r\n>   79.2  81.   82.6  84.   84. ]\r\n> Samples = 17\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n> Epoch: 0050 cost= nan W= nan b= nan\r\n> Epoch: 0100 cost= nan W= nan b= nan\r\n> Epoch: 0150 cost= nan W= nan b= nan\r\n> Epoch: 0200 cost= nan W= nan b= nan\r\n> Epoch: 0250 cost= nan W= nan b= nan\r\n> Epoch: 0300 cost= nan W= nan b= nan\r\n> Epoch: 0350 cost= nan W= nan b= nan\r\n> Epoch: 0400 cost= nan W= nan b= nan\r\n> Epoch: 0450 cost= nan W= nan b= nan\r\n> Epoch: 0500 cost= nan W= nan b= nan\r\n> Epoch: 0550 cost= nan W= nan b= nan\r\n> Epoch: 0600 cost= nan W= nan b= nan\r\n> Epoch: 0650 cost= nan W= nan b= nan\r\n> Epoch: 0700 cost= nan W= nan b= nan\r\n> Epoch: 0750 cost= nan W= nan b= nan\r\n> Epoch: 0800 cost= nan W= nan b= nan\r\n> Epoch: 0850 cost= nan W= nan b= nan\r\n> Epoch: 0900 cost= nan W= nan b= nan\r\n> Epoch: 0950 cost= nan W= nan b= nan\r\n> Epoch: 1000 cost= nan W= nan b= nan\r\n> Optimization Finished!\r\n> Training cost= nan W= nan b= nan \r\n> \r\n> Testing... (Mean square loss Comparison)\r\n> Testing cost= nan\r\n> Absolute mean square loss difference: nan\r\n> And get the following output...\r\n> \r\n\r\nif I uncomment out the alternate array values for X amd Y everything works fine...\r\n\r\n> float64\r\n> float64\r\n> [  3.3     4.4     5.5     6.71    6.93    4.168   9.779   6.182   7.59\r\n>    2.167   7.042  10.791   5.313   7.997   5.654   9.27    3.1  ]\r\n> [ 1.7    2.76   2.09   3.19   1.694  1.573  3.366  2.596  2.53   1.221\r\n>   2.827  3.465  1.65   2.904  2.42   2.94   1.3  ]\r\n> Samples = 17\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n> Epoch: 0050 cost= 8.580235481 W= 0.846042 b= 0.978224\r\n> Epoch: 0100 cost= 5.489832401 W= 0.72321 b= 0.960839\r\n> Epoch: 0150 cost= 3.522622824 W= 0.625209 b= 0.946956\r\n> Epoch: 0200 cost= 2.270416975 W= 0.547022 b= 0.935869\r\n> Epoch: 0250 cost= 1.473346949 W= 0.484644 b= 0.927012\r\n> Epoch: 0300 cost= 0.965984821 W= 0.434878 b= 0.919934\r\n> Epoch: 0350 cost= 0.643029690 W= 0.395175 b= 0.914275\r\n> Epoch: 0400 cost= 0.437456042 W= 0.363499 b= 0.909749\r\n> Epoch: 0450 cost= 0.306603283 W= 0.338229 b= 0.906127\r\n> Epoch: 0500 cost= 0.223311335 W= 0.318069 b= 0.903225\r\n> Epoch: 0550 cost= 0.170292616 W= 0.301985 b= 0.900899\r\n> Epoch: 0600 cost= 0.136546493 W= 0.289155 b= 0.899031\r\n> Epoch: 0650 cost= 0.115067258 W= 0.278921 b= 0.89753\r\n> Epoch: 0700 cost= 0.101395160 W= 0.270757 b= 0.89632\r\n> Epoch: 0750 cost= 0.092692636 W= 0.264245 b= 0.895344\r\n> Epoch: 0800 cost= 0.087154001 W= 0.259051 b= 0.894554\r\n> Epoch: 0850 cost= 0.083628759 W= 0.254909 b= 0.893912\r\n> Epoch: 0900 cost= 0.081385054 W= 0.251606 b= 0.893389\r\n> Epoch: 0950 cost= 0.079956941 W= 0.248972 b= 0.892961\r\n> Epoch: 1000 cost= 0.079047829 W= 0.246872 b= 0.892606\r\n> Optimization Finished!\r\n> Training cost= 0.0790478 W= 0.246872 b= 0.892606 \r\n> \r\n> Testing... (Mean square loss Comparison)\r\n> Testing cost= 0.0796623\r\n> Absolute mean square loss difference: 0.000614464\r\n\r\n\r\nI am using tensorflow 1.01 libraries, and running on a mac OS Sierra...\r\n\r\nBanging my head against the wall trying to figure this out. As you can see I tried to setup tensorboad to help, but seems I have limited success, get a graph but so far has not been any help debugging this...\r\n"}