{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253433321", "html_url": "https://github.com/tensorflow/tensorflow/issues/4164#issuecomment-253433321", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4164", "id": 253433321, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzQzMzMyMQ==", "user": {"login": "wangyongliang", "id": 2075698, "node_id": "MDQ6VXNlcjIwNzU2OTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/2075698?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangyongliang", "html_url": "https://github.com/wangyongliang", "followers_url": "https://api.github.com/users/wangyongliang/followers", "following_url": "https://api.github.com/users/wangyongliang/following{/other_user}", "gists_url": "https://api.github.com/users/wangyongliang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangyongliang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangyongliang/subscriptions", "organizations_url": "https://api.github.com/users/wangyongliang/orgs", "repos_url": "https://api.github.com/users/wangyongliang/repos", "events_url": "https://api.github.com/users/wangyongliang/events{/privacy}", "received_events_url": "https://api.github.com/users/wangyongliang/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-13T07:13:35Z", "updated_at": "2016-10-13T07:13:35Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20574558\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AlvinChen13\">@AlvinChen13</a><br>\nHi Alvin~</p>\n<p>I have tried distributed inception model in <a href=\"https://github.com/tensorflow/models.git\">https://github.com/tensorflow/models.git</a> with 1 worker\uff0c2 workers\uff0c4 workers on GPU(K20C) server without infinityband.<br>\ncompared with single GPU version, distributed looks like can't reduce any training time to achieve the same error rate.</p>", "body_text": "@AlvinChen13\nHi Alvin~\nI have tried distributed inception model in https://github.com/tensorflow/models.git with 1 worker\uff0c2 workers\uff0c4 workers on GPU(K20C) server without infinityband.\ncompared with single GPU version, distributed looks like can't reduce any training time to achieve the same error rate.", "body": "@AlvinChen13 \nHi Alvin~\n\nI have tried distributed inception model in https://github.com/tensorflow/models.git with 1 worker\uff0c2 workers\uff0c4 workers on GPU(K20C) server without infinityband.\ncompared with single GPU version, distributed looks like can't reduce any training time to achieve the same error rate.\n"}