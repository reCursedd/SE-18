{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4164", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4164/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4164/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4164/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4164", "id": 174660735, "node_id": "MDU6SXNzdWUxNzQ2NjA3MzU=", "number": 4164, "title": "Performance issue for Distributed TF", "user": {"login": "AlvinChen13", "id": 20574558, "node_id": "MDQ6VXNlcjIwNTc0NTU4", "avatar_url": "https://avatars1.githubusercontent.com/u/20574558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AlvinChen13", "html_url": "https://github.com/AlvinChen13", "followers_url": "https://api.github.com/users/AlvinChen13/followers", "following_url": "https://api.github.com/users/AlvinChen13/following{/other_user}", "gists_url": "https://api.github.com/users/AlvinChen13/gists{/gist_id}", "starred_url": "https://api.github.com/users/AlvinChen13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AlvinChen13/subscriptions", "organizations_url": "https://api.github.com/users/AlvinChen13/orgs", "repos_url": "https://api.github.com/users/AlvinChen13/repos", "events_url": "https://api.github.com/users/AlvinChen13/events{/privacy}", "received_events_url": "https://api.github.com/users/AlvinChen13/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-09-02T01:16:36Z", "updated_at": "2016-11-08T01:23:08Z", "closed_at": "2016-11-08T01:23:08Z", "author_association": "NONE", "body_html": "<p>Hi All,</p>\n<p>We just enable distributed TF to train our model,  but we get very slow performance comparing to train on single machine. We just setup two machines one ps, one worker, and each machine is Xeon CPU with nvidia K40 GPU:<br>\nGPU 0: Quadro K420 (UUID: GPU-954fc4b7-6198-f5f9-1efb-69e73d001d5c)<br>\nGPU 1: Tesla K40c (UUID: GPU-ac243789-45bd-448f-87d9-db21c1c88c87)<br>\nGPU 2: Tesla K40c (UUID: GPU-d7b7783e-9041-e4e2-91f0-dc4188fd84e9)</p>\n<p>If we train on one machine we can meet 200+examples/sec, but on two machines, it is just 20+ examples/sec. 10 times slower....</p>\n<p>Our two machines are connected in 1GBits network, and we find the network bandwidth is just 40%.</p>\n<p>Some topic discussed this issue, but I haven't got any solutions...</p>\n<p>One machine:<br>\n2016-09-02 09:04:35.330931: step 240, lr = 0.0001, loss = 1.59115 (107.4 examples/sec; 0.14900 sec/batch)<br>\n2016-09-02 09:04:36.968806: step 250, lr = 0.0001, loss = 1.60836 (225.6 examples/sec; 0.07091 sec/batch)<br>\n2016-09-02 09:04:39.037962: step 260, lr = 0.0001, loss = 1.56989 (212.2 examples/sec; 0.07541 sec/batch)<br>\n2016-09-02 09:04:40.491946: step 270, lr = 0.0001, loss = 1.68502 (226.8 examples/sec; 0.07056 sec/batch)<br>\n2016-09-02 09:04:42.754090: step 280, lr = 0.0001, loss = 1.65222 (214.8 examples/sec; 0.07450 sec/batch)<br>\n2016-09-02 09:04:44.510914: step 290, lr = 0.0001, loss = 1.64218 (241.8 examples/sec; 0.06617 sec/batch)<br>\n2016-09-02 09:04:47.301761: step 300, lr = 0.0001, loss = 1.64549 (219.8 examples/sec; 0.07278 sec/batch)<br>\n2016-09-02 09:04:50.331641: step 310, lr = 0.0001, loss = 1.56824 (208.1 examples/sec; 0.07689 sec/batch)<br>\n2016-09-02 09:04:52.485210: step 320, lr = 0.0001, loss = 1.49187 (226.4 examples/sec; 0.07067 sec/batch)<br>\n2016-09-02 09:04:53.878589: step 330, lr = 0.0001, loss = 1.57825 (216.5 examples/sec; 0.07389 sec/batch)<br>\n2016-09-02 09:04:55.613968: step 340, lr = 0.0001, loss = 1.57355 (227.0 examples/sec; 0.07047 sec/batch)<br>\n2016-09-02 09:04:57.887825: step 350, lr = 0.0001, loss = 1.58644 (231.2 examples/sec; 0.06920 sec/batch)<br>\n2016-09-02 09:05:00.791740: step 360, lr = 0.0001, loss = 1.57324 (224.1 examples/sec; 0.07139 sec/batch)<br>\n2016-09-02 09:05:03.385293: step 370, lr = 0.0001, loss = 1.56881 (214.0 examples/sec; 0.07476 sec/batch)<br>\n2016-09-02 09:05:05.684695: step 380, lr = 0.0001, loss = 1.57652 (223.3 examples/sec; 0.07166 sec/batch)<br>\n2016-09-02 09:05:07.671139: step 390, lr = 0.0001, loss = 1.59098 (219.5 examples/sec; 0.07288 sec/batch)<br>\n2016-09-02 09:05:09.384184: step 400, lr = 0.0001, loss = 1.57292 (222.9 examples/sec; 0.07177 sec/batch)<br>\n2016-09-02 09:05:12.028866: step 410, lr = 0.0001, loss = 1.59291 (224.0 examples/sec; 0.07144 sec/batch)<br>\n2016-09-02 09:05:13.912951: step 420, lr = 0.0001, loss = 1.56065 (206.7 examples/sec; 0.07741 sec/batch)<br>\n2016-09-02 09:05:15.567746: step 430, lr = 0.0001, loss = 1.48168 (220.1 examples/sec; 0.07270 sec/batch)<br>\n2016-09-02 09:05:17.913313: step 440, lr = 0.0001, loss = 1.52358 (220.5 examples/sec; 0.07256 sec/batch)<br>\n2016-09-02 09:05:20.215508: step 450, lr = 0.0001, loss = 1.37813 (163.3 examples/sec; 0.09800 sec/batch)<br>\n2016-09-02 09:05:21.685596: step 460, lr = 0.0001, loss = 1.66846 (237.3 examples/sec; 0.06741 sec/batch)<br>\n2016-09-02 09:05:24.225214: step 470, lr = 0.0001, loss = 1.70520 (219.6 examples/sec; 0.07287 sec/batch)<br>\n2016-09-02 09:05:26.203767: step 480, lr = 0.0001, loss = 1.58498 (234.3 examples/sec; 0.06827 sec/batch)<br>\n2016-09-02 09:05:28.258299: step 490, lr = 0.0001, loss = 1.50158 (226.4 examples/sec; 0.07067 sec/batch)</p>\n<p>two machines:<br>\nNFO:tensorflow:Finished running Summary operation.<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:20.459047: step 12450, loss = 0.66(21.7 examples/sec; 0.737  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:43.294597: step 12480, loss = 0.50(20.7 examples/sec; 0.773  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:06.094232: step 12510, loss = 0.54(21.8 examples/sec; 0.733  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:29.405047: step 12540, loss = 0.51(20.5 examples/sec; 0.781  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:52.529421: step 12570, loss = 0.37(20.8 examples/sec; 0.768  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:15.177700: step 12600, loss = 0.49(21.3 examples/sec; 0.750  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:38.046958: step 12630, loss = 0.43(21.9 examples/sec; 0.731  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:00.978593: step 12660, loss = 0.48(20.7 examples/sec; 0.773  sec/batch)<br>\nINFO:tensorflow:Running Summary operation on the chief.<br>\nINFO:tensorflow:Finished running Summary operation.<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:24.747284: step 12690, loss = 0.47(20.7 examples/sec; 0.772  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:10.874324: step 12750, loss = 0.58(20.3 examples/sec; 0.788  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:33.881611: step 12780, loss = 0.49(21.0 examples/sec; 0.762  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:56.813524: step 12810, loss = 0.63(21.2 examples/sec; 0.755  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:19.631475: step 12840, loss = 0.36(21.0 examples/sec; 0.763  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:42.239483: step 12870, loss = 0.30(20.7 examples/sec; 0.773  sec/batch)<br>\nINFO:tensorflow:Worker 0: 2016-09-01 17:22:04.912129: step 12900, loss = 0.75(21.3 examples/sec; 0.750  sec/batch)</p>", "body_text": "Hi All,\nWe just enable distributed TF to train our model,  but we get very slow performance comparing to train on single machine. We just setup two machines one ps, one worker, and each machine is Xeon CPU with nvidia K40 GPU:\nGPU 0: Quadro K420 (UUID: GPU-954fc4b7-6198-f5f9-1efb-69e73d001d5c)\nGPU 1: Tesla K40c (UUID: GPU-ac243789-45bd-448f-87d9-db21c1c88c87)\nGPU 2: Tesla K40c (UUID: GPU-d7b7783e-9041-e4e2-91f0-dc4188fd84e9)\nIf we train on one machine we can meet 200+examples/sec, but on two machines, it is just 20+ examples/sec. 10 times slower....\nOur two machines are connected in 1GBits network, and we find the network bandwidth is just 40%.\nSome topic discussed this issue, but I haven't got any solutions...\nOne machine:\n2016-09-02 09:04:35.330931: step 240, lr = 0.0001, loss = 1.59115 (107.4 examples/sec; 0.14900 sec/batch)\n2016-09-02 09:04:36.968806: step 250, lr = 0.0001, loss = 1.60836 (225.6 examples/sec; 0.07091 sec/batch)\n2016-09-02 09:04:39.037962: step 260, lr = 0.0001, loss = 1.56989 (212.2 examples/sec; 0.07541 sec/batch)\n2016-09-02 09:04:40.491946: step 270, lr = 0.0001, loss = 1.68502 (226.8 examples/sec; 0.07056 sec/batch)\n2016-09-02 09:04:42.754090: step 280, lr = 0.0001, loss = 1.65222 (214.8 examples/sec; 0.07450 sec/batch)\n2016-09-02 09:04:44.510914: step 290, lr = 0.0001, loss = 1.64218 (241.8 examples/sec; 0.06617 sec/batch)\n2016-09-02 09:04:47.301761: step 300, lr = 0.0001, loss = 1.64549 (219.8 examples/sec; 0.07278 sec/batch)\n2016-09-02 09:04:50.331641: step 310, lr = 0.0001, loss = 1.56824 (208.1 examples/sec; 0.07689 sec/batch)\n2016-09-02 09:04:52.485210: step 320, lr = 0.0001, loss = 1.49187 (226.4 examples/sec; 0.07067 sec/batch)\n2016-09-02 09:04:53.878589: step 330, lr = 0.0001, loss = 1.57825 (216.5 examples/sec; 0.07389 sec/batch)\n2016-09-02 09:04:55.613968: step 340, lr = 0.0001, loss = 1.57355 (227.0 examples/sec; 0.07047 sec/batch)\n2016-09-02 09:04:57.887825: step 350, lr = 0.0001, loss = 1.58644 (231.2 examples/sec; 0.06920 sec/batch)\n2016-09-02 09:05:00.791740: step 360, lr = 0.0001, loss = 1.57324 (224.1 examples/sec; 0.07139 sec/batch)\n2016-09-02 09:05:03.385293: step 370, lr = 0.0001, loss = 1.56881 (214.0 examples/sec; 0.07476 sec/batch)\n2016-09-02 09:05:05.684695: step 380, lr = 0.0001, loss = 1.57652 (223.3 examples/sec; 0.07166 sec/batch)\n2016-09-02 09:05:07.671139: step 390, lr = 0.0001, loss = 1.59098 (219.5 examples/sec; 0.07288 sec/batch)\n2016-09-02 09:05:09.384184: step 400, lr = 0.0001, loss = 1.57292 (222.9 examples/sec; 0.07177 sec/batch)\n2016-09-02 09:05:12.028866: step 410, lr = 0.0001, loss = 1.59291 (224.0 examples/sec; 0.07144 sec/batch)\n2016-09-02 09:05:13.912951: step 420, lr = 0.0001, loss = 1.56065 (206.7 examples/sec; 0.07741 sec/batch)\n2016-09-02 09:05:15.567746: step 430, lr = 0.0001, loss = 1.48168 (220.1 examples/sec; 0.07270 sec/batch)\n2016-09-02 09:05:17.913313: step 440, lr = 0.0001, loss = 1.52358 (220.5 examples/sec; 0.07256 sec/batch)\n2016-09-02 09:05:20.215508: step 450, lr = 0.0001, loss = 1.37813 (163.3 examples/sec; 0.09800 sec/batch)\n2016-09-02 09:05:21.685596: step 460, lr = 0.0001, loss = 1.66846 (237.3 examples/sec; 0.06741 sec/batch)\n2016-09-02 09:05:24.225214: step 470, lr = 0.0001, loss = 1.70520 (219.6 examples/sec; 0.07287 sec/batch)\n2016-09-02 09:05:26.203767: step 480, lr = 0.0001, loss = 1.58498 (234.3 examples/sec; 0.06827 sec/batch)\n2016-09-02 09:05:28.258299: step 490, lr = 0.0001, loss = 1.50158 (226.4 examples/sec; 0.07067 sec/batch)\ntwo machines:\nNFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:20.459047: step 12450, loss = 0.66(21.7 examples/sec; 0.737  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:43.294597: step 12480, loss = 0.50(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:06.094232: step 12510, loss = 0.54(21.8 examples/sec; 0.733  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:29.405047: step 12540, loss = 0.51(20.5 examples/sec; 0.781  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:52.529421: step 12570, loss = 0.37(20.8 examples/sec; 0.768  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:15.177700: step 12600, loss = 0.49(21.3 examples/sec; 0.750  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:38.046958: step 12630, loss = 0.43(21.9 examples/sec; 0.731  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:00.978593: step 12660, loss = 0.48(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:24.747284: step 12690, loss = 0.47(20.7 examples/sec; 0.772  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:10.874324: step 12750, loss = 0.58(20.3 examples/sec; 0.788  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:33.881611: step 12780, loss = 0.49(21.0 examples/sec; 0.762  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:56.813524: step 12810, loss = 0.63(21.2 examples/sec; 0.755  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:19.631475: step 12840, loss = 0.36(21.0 examples/sec; 0.763  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:42.239483: step 12870, loss = 0.30(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:22:04.912129: step 12900, loss = 0.75(21.3 examples/sec; 0.750  sec/batch)", "body": "Hi All,\n\nWe just enable distributed TF to train our model,  but we get very slow performance comparing to train on single machine. We just setup two machines one ps, one worker, and each machine is Xeon CPU with nvidia K40 GPU:\nGPU 0: Quadro K420 (UUID: GPU-954fc4b7-6198-f5f9-1efb-69e73d001d5c)\nGPU 1: Tesla K40c (UUID: GPU-ac243789-45bd-448f-87d9-db21c1c88c87)\nGPU 2: Tesla K40c (UUID: GPU-d7b7783e-9041-e4e2-91f0-dc4188fd84e9)\n\nIf we train on one machine we can meet 200+examples/sec, but on two machines, it is just 20+ examples/sec. 10 times slower....\n\nOur two machines are connected in 1GBits network, and we find the network bandwidth is just 40%.\n\nSome topic discussed this issue, but I haven't got any solutions... \n\nOne machine:\n2016-09-02 09:04:35.330931: step 240, lr = 0.0001, loss = 1.59115 (107.4 examples/sec; 0.14900 sec/batch)\n2016-09-02 09:04:36.968806: step 250, lr = 0.0001, loss = 1.60836 (225.6 examples/sec; 0.07091 sec/batch)\n2016-09-02 09:04:39.037962: step 260, lr = 0.0001, loss = 1.56989 (212.2 examples/sec; 0.07541 sec/batch)\n2016-09-02 09:04:40.491946: step 270, lr = 0.0001, loss = 1.68502 (226.8 examples/sec; 0.07056 sec/batch)\n2016-09-02 09:04:42.754090: step 280, lr = 0.0001, loss = 1.65222 (214.8 examples/sec; 0.07450 sec/batch)\n2016-09-02 09:04:44.510914: step 290, lr = 0.0001, loss = 1.64218 (241.8 examples/sec; 0.06617 sec/batch)\n2016-09-02 09:04:47.301761: step 300, lr = 0.0001, loss = 1.64549 (219.8 examples/sec; 0.07278 sec/batch)\n2016-09-02 09:04:50.331641: step 310, lr = 0.0001, loss = 1.56824 (208.1 examples/sec; 0.07689 sec/batch)\n2016-09-02 09:04:52.485210: step 320, lr = 0.0001, loss = 1.49187 (226.4 examples/sec; 0.07067 sec/batch)\n2016-09-02 09:04:53.878589: step 330, lr = 0.0001, loss = 1.57825 (216.5 examples/sec; 0.07389 sec/batch)\n2016-09-02 09:04:55.613968: step 340, lr = 0.0001, loss = 1.57355 (227.0 examples/sec; 0.07047 sec/batch)\n2016-09-02 09:04:57.887825: step 350, lr = 0.0001, loss = 1.58644 (231.2 examples/sec; 0.06920 sec/batch)\n2016-09-02 09:05:00.791740: step 360, lr = 0.0001, loss = 1.57324 (224.1 examples/sec; 0.07139 sec/batch)\n2016-09-02 09:05:03.385293: step 370, lr = 0.0001, loss = 1.56881 (214.0 examples/sec; 0.07476 sec/batch)\n2016-09-02 09:05:05.684695: step 380, lr = 0.0001, loss = 1.57652 (223.3 examples/sec; 0.07166 sec/batch)\n2016-09-02 09:05:07.671139: step 390, lr = 0.0001, loss = 1.59098 (219.5 examples/sec; 0.07288 sec/batch)\n2016-09-02 09:05:09.384184: step 400, lr = 0.0001, loss = 1.57292 (222.9 examples/sec; 0.07177 sec/batch)\n2016-09-02 09:05:12.028866: step 410, lr = 0.0001, loss = 1.59291 (224.0 examples/sec; 0.07144 sec/batch)\n2016-09-02 09:05:13.912951: step 420, lr = 0.0001, loss = 1.56065 (206.7 examples/sec; 0.07741 sec/batch)\n2016-09-02 09:05:15.567746: step 430, lr = 0.0001, loss = 1.48168 (220.1 examples/sec; 0.07270 sec/batch)\n2016-09-02 09:05:17.913313: step 440, lr = 0.0001, loss = 1.52358 (220.5 examples/sec; 0.07256 sec/batch)\n2016-09-02 09:05:20.215508: step 450, lr = 0.0001, loss = 1.37813 (163.3 examples/sec; 0.09800 sec/batch)\n2016-09-02 09:05:21.685596: step 460, lr = 0.0001, loss = 1.66846 (237.3 examples/sec; 0.06741 sec/batch)\n2016-09-02 09:05:24.225214: step 470, lr = 0.0001, loss = 1.70520 (219.6 examples/sec; 0.07287 sec/batch)\n2016-09-02 09:05:26.203767: step 480, lr = 0.0001, loss = 1.58498 (234.3 examples/sec; 0.06827 sec/batch)\n2016-09-02 09:05:28.258299: step 490, lr = 0.0001, loss = 1.50158 (226.4 examples/sec; 0.07067 sec/batch)\n\ntwo machines:\nNFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:20.459047: step 12450, loss = 0.66(21.7 examples/sec; 0.737  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:16:43.294597: step 12480, loss = 0.50(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:06.094232: step 12510, loss = 0.54(21.8 examples/sec; 0.733  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:29.405047: step 12540, loss = 0.51(20.5 examples/sec; 0.781  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:17:52.529421: step 12570, loss = 0.37(20.8 examples/sec; 0.768  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:15.177700: step 12600, loss = 0.49(21.3 examples/sec; 0.750  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:18:38.046958: step 12630, loss = 0.43(21.9 examples/sec; 0.731  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:00.978593: step 12660, loss = 0.48(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Running Summary operation on the chief.\nINFO:tensorflow:Finished running Summary operation.\nINFO:tensorflow:Worker 0: 2016-09-01 17:19:24.747284: step 12690, loss = 0.47(20.7 examples/sec; 0.772  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:10.874324: step 12750, loss = 0.58(20.3 examples/sec; 0.788  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:33.881611: step 12780, loss = 0.49(21.0 examples/sec; 0.762  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:20:56.813524: step 12810, loss = 0.63(21.2 examples/sec; 0.755  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:19.631475: step 12840, loss = 0.36(21.0 examples/sec; 0.763  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:21:42.239483: step 12870, loss = 0.30(20.7 examples/sec; 0.773  sec/batch)\nINFO:tensorflow:Worker 0: 2016-09-01 17:22:04.912129: step 12900, loss = 0.75(21.3 examples/sec; 0.750  sec/batch)\n"}