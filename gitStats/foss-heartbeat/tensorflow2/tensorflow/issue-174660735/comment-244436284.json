{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/244436284", "html_url": "https://github.com/tensorflow/tensorflow/issues/4164#issuecomment-244436284", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4164", "id": 244436284, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NDQzNjI4NA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-02T17:21:26Z", "updated_at": "2016-09-02T17:21:26Z", "author_association": "MEMBER", "body_html": "<p>You haven't said anything specific about the structure of your model, or exactly what about this situation surprises you.</p>\n<p>Many or most models will run faster on a single GPU than they will when split across multiple GPUs, if it's possible to fit on a single GPU.</p>\n<p>Here's why: The latency per step is lower bounded by the most expensive path through a DAG where each node is a kernel execution, whose cost is the time that kernel takes, and each edge is a data dependency, whose cost is the time it takes to transfer the data from the computation that produces it to the computation where it is consumed.  If the model runs completely on one GPU, the data transfer times are all zero, and the overall execution time is just the sum of the kernel execution times.  GPU computations can be very fast, from a few microseconds to a few milliseconds.  By contrast, data transfers by RPC and even via PCIe bus are relatively slow.</p>\n<p>Consider the following example.  We'd like to do a dot product of two large vectors, of 10^9 float32 values, all of which reside on GPU0 to begin with.  We also have GPU1 available, on the same PCIe bus with a bandwidth of 6GB/s.  Naively it seems like partitioning the computation between devices might be a good idea.  Suppose both GPUs have an internal memory bandwidth of 200GB/s.  Based on that limitation, doing the dot product just inside GPU0 should take (10^9 * 4 / 200x10^9) secs = 0.02 sec.  Half of the computation would take 0.01 sec.  But copying 1/2 of the data between GPUs at 6GB/s would take 0.5 * 4 / 6 = 0.33 sec, much longer than the on-GPU computation.  If the data transfer time is even slower, say 10Gbit/sec over a typical RPC network link, then the disadvantage is even greater.</p>\n<p>Back to your specific case: It sounds like you're trying a separate ps/worker architecture.  For the reasons just cited, it should be clear that when using a single worker it's going to be slower; this sort of architecture is only advantageous if you use a lot of workers, so that the wall-clock training time is reduced.</p>\n<p>But maybe you're wondering why the time is 10x slower, and yet by your measurement the network is only 40% busy?  There could be lots of reasons.  1Gb/s is a slow network by contemporary standards.  It's usually hard to get more than 80% of the theoretical capacity of a network, and maybe there's other traffic going through it competing with your workload.  There may be other jobs timesharing with your TF jobs on both machines that are competing for kernel resources that interfere with RPC latency.  If you're getting 80% of the theoretical capacity going in each direction, when there's something to send, and measuring overall capacity as bidirectional, then you could see 40% bandwidth consumption when your job is completely network bound.</p>", "body_text": "You haven't said anything specific about the structure of your model, or exactly what about this situation surprises you.\nMany or most models will run faster on a single GPU than they will when split across multiple GPUs, if it's possible to fit on a single GPU.\nHere's why: The latency per step is lower bounded by the most expensive path through a DAG where each node is a kernel execution, whose cost is the time that kernel takes, and each edge is a data dependency, whose cost is the time it takes to transfer the data from the computation that produces it to the computation where it is consumed.  If the model runs completely on one GPU, the data transfer times are all zero, and the overall execution time is just the sum of the kernel execution times.  GPU computations can be very fast, from a few microseconds to a few milliseconds.  By contrast, data transfers by RPC and even via PCIe bus are relatively slow.\nConsider the following example.  We'd like to do a dot product of two large vectors, of 10^9 float32 values, all of which reside on GPU0 to begin with.  We also have GPU1 available, on the same PCIe bus with a bandwidth of 6GB/s.  Naively it seems like partitioning the computation between devices might be a good idea.  Suppose both GPUs have an internal memory bandwidth of 200GB/s.  Based on that limitation, doing the dot product just inside GPU0 should take (10^9 * 4 / 200x10^9) secs = 0.02 sec.  Half of the computation would take 0.01 sec.  But copying 1/2 of the data between GPUs at 6GB/s would take 0.5 * 4 / 6 = 0.33 sec, much longer than the on-GPU computation.  If the data transfer time is even slower, say 10Gbit/sec over a typical RPC network link, then the disadvantage is even greater.\nBack to your specific case: It sounds like you're trying a separate ps/worker architecture.  For the reasons just cited, it should be clear that when using a single worker it's going to be slower; this sort of architecture is only advantageous if you use a lot of workers, so that the wall-clock training time is reduced.\nBut maybe you're wondering why the time is 10x slower, and yet by your measurement the network is only 40% busy?  There could be lots of reasons.  1Gb/s is a slow network by contemporary standards.  It's usually hard to get more than 80% of the theoretical capacity of a network, and maybe there's other traffic going through it competing with your workload.  There may be other jobs timesharing with your TF jobs on both machines that are competing for kernel resources that interfere with RPC latency.  If you're getting 80% of the theoretical capacity going in each direction, when there's something to send, and measuring overall capacity as bidirectional, then you could see 40% bandwidth consumption when your job is completely network bound.", "body": "You haven't said anything specific about the structure of your model, or exactly what about this situation surprises you.  \n\nMany or most models will run faster on a single GPU than they will when split across multiple GPUs, if it's possible to fit on a single GPU.\n\nHere's why: The latency per step is lower bounded by the most expensive path through a DAG where each node is a kernel execution, whose cost is the time that kernel takes, and each edge is a data dependency, whose cost is the time it takes to transfer the data from the computation that produces it to the computation where it is consumed.  If the model runs completely on one GPU, the data transfer times are all zero, and the overall execution time is just the sum of the kernel execution times.  GPU computations can be very fast, from a few microseconds to a few milliseconds.  By contrast, data transfers by RPC and even via PCIe bus are relatively slow.  \n\nConsider the following example.  We'd like to do a dot product of two large vectors, of 10^9 float32 values, all of which reside on GPU0 to begin with.  We also have GPU1 available, on the same PCIe bus with a bandwidth of 6GB/s.  Naively it seems like partitioning the computation between devices might be a good idea.  Suppose both GPUs have an internal memory bandwidth of 200GB/s.  Based on that limitation, doing the dot product just inside GPU0 should take (10^9 \\* 4 / 200x10^9) secs = 0.02 sec.  Half of the computation would take 0.01 sec.  But copying 1/2 of the data between GPUs at 6GB/s would take 0.5 \\* 4 / 6 = 0.33 sec, much longer than the on-GPU computation.  If the data transfer time is even slower, say 10Gbit/sec over a typical RPC network link, then the disadvantage is even greater.\n\nBack to your specific case: It sounds like you're trying a separate ps/worker architecture.  For the reasons just cited, it should be clear that when using a single worker it's going to be slower; this sort of architecture is only advantageous if you use a lot of workers, so that the wall-clock training time is reduced.  \n\nBut maybe you're wondering why the time is 10x slower, and yet by your measurement the network is only 40% busy?  There could be lots of reasons.  1Gb/s is a slow network by contemporary standards.  It's usually hard to get more than 80% of the theoretical capacity of a network, and maybe there's other traffic going through it competing with your workload.  There may be other jobs timesharing with your TF jobs on both machines that are competing for kernel resources that interfere with RPC latency.  If you're getting 80% of the theoretical capacity going in each direction, when there's something to send, and measuring overall capacity as bidirectional, then you could see 40% bandwidth consumption when your job is completely network bound.\n"}