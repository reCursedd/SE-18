{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9892", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9892/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9892/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9892/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9892", "id": 228548083, "node_id": "MDU6SXNzdWUyMjg1NDgwODM=", "number": 9892, "title": "tf.while_loop runs very slow", "user": {"login": "thomasquintana", "id": 1891840, "node_id": "MDQ6VXNlcjE4OTE4NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1891840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thomasquintana", "html_url": "https://github.com/thomasquintana", "followers_url": "https://api.github.com/users/thomasquintana/followers", "following_url": "https://api.github.com/users/thomasquintana/following{/other_user}", "gists_url": "https://api.github.com/users/thomasquintana/gists{/gist_id}", "starred_url": "https://api.github.com/users/thomasquintana/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thomasquintana/subscriptions", "organizations_url": "https://api.github.com/users/thomasquintana/orgs", "repos_url": "https://api.github.com/users/thomasquintana/repos", "events_url": "https://api.github.com/users/thomasquintana/events{/privacy}", "received_events_url": "https://api.github.com/users/thomasquintana/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-14T15:14:37Z", "updated_at": "2017-05-15T15:17:22Z", "closed_at": "2017-05-15T15:17:22Z", "author_association": "NONE", "body_html": "<p>I'm writing a simple RNN implementation using tf.while_loop but it runs incredibly slow. Any insights would be incredibly helpful.</p>\n<p>OS: Ubuntu/Linux (16.04)<br>\nTensorFlow: Compiled from source<br>\nTensorFlow Version: r1.1<br>\nBazel Version: 0.4.5<br>\nCUDA/CuDNN Versions: 8.0/5.1<br>\nGPU Model/Memory: TitanX/12Gb</p>\n<p>Here's the implementation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">forward</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>):\n    inputs_shape <span class=\"pl-k\">=</span> tf.shape(inputs)\n    timesteps <span class=\"pl-k\">=</span> inputs_shape[<span class=\"pl-c1\">0</span>]\n    batch_size <span class=\"pl-k\">=</span> inputs_shape[<span class=\"pl-c1\">1</span>]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute the forward pass for every time step.</span>\n    output <span class=\"pl-k\">=</span> tf.reshape(inputs, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>._inputs_dim])\n    output <span class=\"pl-k\">=</span> tf.add(tf.matmul(output, <span class=\"pl-c1\">self</span>._weights), <span class=\"pl-c1\">self</span>._bias)\n    output <span class=\"pl-k\">=</span> tf.reshape(output, [timesteps, batch_size, <span class=\"pl-c1\">self</span>._units])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a tensor array to hold the output of our rnn layer.</span>\n    temp <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._dtype, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>timesteps)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step</span>(<span class=\"pl-smi\">c</span>, <span class=\"pl-smi\">t</span>, <span class=\"pl-smi\">h</span>, <span class=\"pl-smi\">s</span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">o</span>):\n      h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._activation(tf.add(tf.matmul(h, s), i[c]))\n      o <span class=\"pl-k\">=</span> o.write(c, h)\n      <span class=\"pl-k\">return</span> [c <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, t, h, s, i, o]\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">step_condition</span>(<span class=\"pl-smi\">c</span>, <span class=\"pl-smi\">t</span>, <span class=\"pl-smi\">h</span>, <span class=\"pl-smi\">s</span>, <span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">o</span>):\n      <span class=\"pl-k\">return</span> tf.less(c, t)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a counter to track the number of timesteps.</span>\n    count <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Define the initial hidden state.</span>\n    hidden <span class=\"pl-k\">=</span> tf.zeros([batch_size, <span class=\"pl-c1\">self</span>._units], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._dtype)\n\n    _, _, _, _, _, output <span class=\"pl-k\">=</span> tf.while_loop(\n      step_condition,\n      step,\n      [count, timesteps, hidden, <span class=\"pl-c1\">self</span>._state, output, temp]\n    )\n\n    <span class=\"pl-k\">return</span> output.stack()</pre></div>\n<p>It takes ~5 minutes to complete one epoch of the mnist dataset.</p>", "body_text": "I'm writing a simple RNN implementation using tf.while_loop but it runs incredibly slow. Any insights would be incredibly helpful.\nOS: Ubuntu/Linux (16.04)\nTensorFlow: Compiled from source\nTensorFlow Version: r1.1\nBazel Version: 0.4.5\nCUDA/CuDNN Versions: 8.0/5.1\nGPU Model/Memory: TitanX/12Gb\nHere's the implementation:\ndef forward(self, inputs):\n    inputs_shape = tf.shape(inputs)\n    timesteps = inputs_shape[0]\n    batch_size = inputs_shape[1]\n    # Compute the forward pass for every time step.\n    output = tf.reshape(inputs, [-1, self._inputs_dim])\n    output = tf.add(tf.matmul(output, self._weights), self._bias)\n    output = tf.reshape(output, [timesteps, batch_size, self._units])\n    # Create a tensor array to hold the output of our rnn layer.\n    temp = tf.TensorArray(dtype=self._dtype, size=timesteps)\n\n    def step(c, t, h, s, i, o):\n      h = self._activation(tf.add(tf.matmul(h, s), i[c]))\n      o = o.write(c, h)\n      return [c + 1, t, h, s, i, o]\n\n    def step_condition(c, t, h, s, i, o):\n      return tf.less(c, t)\n\n    # Create a counter to track the number of timesteps.\n    count = tf.constant(0)\n    # Define the initial hidden state.\n    hidden = tf.zeros([batch_size, self._units], dtype=self._dtype)\n\n    _, _, _, _, _, output = tf.while_loop(\n      step_condition,\n      step,\n      [count, timesteps, hidden, self._state, output, temp]\n    )\n\n    return output.stack()\nIt takes ~5 minutes to complete one epoch of the mnist dataset.", "body": "I'm writing a simple RNN implementation using tf.while_loop but it runs incredibly slow. Any insights would be incredibly helpful.\r\n\r\nOS: Ubuntu/Linux (16.04)\r\nTensorFlow: Compiled from source\r\nTensorFlow Version: r1.1\r\nBazel Version: 0.4.5\r\nCUDA/CuDNN Versions: 8.0/5.1\r\nGPU Model/Memory: TitanX/12Gb\r\n\r\nHere's the implementation:\r\n```python\r\ndef forward(self, inputs):\r\n    inputs_shape = tf.shape(inputs)\r\n    timesteps = inputs_shape[0]\r\n    batch_size = inputs_shape[1]\r\n    # Compute the forward pass for every time step.\r\n    output = tf.reshape(inputs, [-1, self._inputs_dim])\r\n    output = tf.add(tf.matmul(output, self._weights), self._bias)\r\n    output = tf.reshape(output, [timesteps, batch_size, self._units])\r\n    # Create a tensor array to hold the output of our rnn layer.\r\n    temp = tf.TensorArray(dtype=self._dtype, size=timesteps)\r\n\r\n    def step(c, t, h, s, i, o):\r\n      h = self._activation(tf.add(tf.matmul(h, s), i[c]))\r\n      o = o.write(c, h)\r\n      return [c + 1, t, h, s, i, o]\r\n\r\n    def step_condition(c, t, h, s, i, o):\r\n      return tf.less(c, t)\r\n\r\n    # Create a counter to track the number of timesteps.\r\n    count = tf.constant(0)\r\n    # Define the initial hidden state.\r\n    hidden = tf.zeros([batch_size, self._units], dtype=self._dtype)\r\n\r\n    _, _, _, _, _, output = tf.while_loop(\r\n      step_condition,\r\n      step,\r\n      [count, timesteps, hidden, self._state, output, temp]\r\n    )\r\n\r\n    return output.stack()\r\n```\r\n\r\nIt takes ~5 minutes to complete one epoch of the mnist dataset."}