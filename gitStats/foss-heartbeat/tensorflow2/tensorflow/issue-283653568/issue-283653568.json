{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15525", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15525/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15525/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15525/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15525", "id": 283653568, "node_id": "MDU6SXNzdWUyODM2NTM1Njg=", "number": 15525, "title": "Op type not registered HashTableV2 error while deploying model in cloud ml", "user": {"login": "KishoreKarunakaran", "id": 10724627, "node_id": "MDQ6VXNlcjEwNzI0NjI3", "avatar_url": "https://avatars3.githubusercontent.com/u/10724627?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KishoreKarunakaran", "html_url": "https://github.com/KishoreKarunakaran", "followers_url": "https://api.github.com/users/KishoreKarunakaran/followers", "following_url": "https://api.github.com/users/KishoreKarunakaran/following{/other_user}", "gists_url": "https://api.github.com/users/KishoreKarunakaran/gists{/gist_id}", "starred_url": "https://api.github.com/users/KishoreKarunakaran/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KishoreKarunakaran/subscriptions", "organizations_url": "https://api.github.com/users/KishoreKarunakaran/orgs", "repos_url": "https://api.github.com/users/KishoreKarunakaran/repos", "events_url": "https://api.github.com/users/KishoreKarunakaran/events{/privacy}", "received_events_url": "https://api.github.com/users/KishoreKarunakaran/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-12-20T18:22:04Z", "updated_at": "2017-12-30T17:47:40Z", "closed_at": "2017-12-20T23:04:58Z", "author_association": "NONE", "body_html": "<p>Problem while deploying model in Tensorflow 1.4.</p>\n<p><strong>Code :</strong></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">model_fn</span>(<span class=\"pl-smi\">features</span>, <span class=\"pl-smi\">labels</span>, <span class=\"pl-smi\">mode</span>):\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        tf.keras.backend.set_learning_phase(<span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">else</span>:\n        tf.keras.backend.set_learning_phase(<span class=\"pl-c1\">False</span>)\n\n    input_feature <span class=\"pl-k\">=</span> features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>]\n    table <span class=\"pl-k\">=</span> lookup.index_table_from_file(<span class=\"pl-v\">vocabulary_file</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>vocab.txt<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">num_oov_buckets</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    text <span class=\"pl-k\">=</span> tf.squeeze(input_feature, [<span class=\"pl-c1\">1</span>])\n    words <span class=\"pl-k\">=</span> tf.string_split(text)\n    dense_words <span class=\"pl-k\">=</span> tf.sparse_tensor_to_dense(words, <span class=\"pl-v\">default_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">PADWORD</span>)\n    numbers <span class=\"pl-k\">=</span> table.lookup(dense_words)\n    padding <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">MAX_LEN</span>]])\n    padded <span class=\"pl-k\">=</span> tf.pad(numbers, padding)\n    sliced <span class=\"pl-k\">=</span> tf.slice(padded, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">MAX_LEN</span>])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_sliced=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(words))\n\n    embeds <span class=\"pl-k\">=</span> tf.keras.layers.Embedding(<span class=\"pl-c1\">MAX_FEATURES</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-v\">input_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MAX_LEN</span>)(sliced)\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>words_embed=<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(embeds))\n    f1 <span class=\"pl-k\">=</span> tf.keras.layers.Dropout(<span class=\"pl-c1\">0.2</span>)(embeds)\n    f1 <span class=\"pl-k\">=</span> tf.keras.layers.Conv1D(filters, kernel_size, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>valid<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)(f1)\n    f1 <span class=\"pl-k\">=</span> tf.keras.layers.GlobalAveragePooling1D()(f1)\n    f1 <span class=\"pl-k\">=</span> tf.keras.layers.Dense(hidden_dims)(f1)\n    f1 <span class=\"pl-k\">=</span> tf.keras.layers.Dropout(<span class=\"pl-c1\">0.5</span>)(f1)\n    f1 <span class=\"pl-k\">=</span> tf.keras.layers.Activation(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu<span class=\"pl-pds\">'</span></span>)(f1)\n    logits <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-c1\">11</span>)(f1)\n\n    predictions_dict <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>: tf.argmax(logits, <span class=\"pl-c1\">1</span>),\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>prob<span class=\"pl-pds\">'</span></span>: tf.nn.softmax(logits)\n    }\n\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),</span>\n<span class=\"pl-s\">                                                           \"probabilities\": tf.nn.softmax(logits,</span>\n<span class=\"pl-s\">                                                                                          name=\"softmax_tensor\")})<span class=\"pl-pds\">'''</span></span>\n\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.estimator.ModeKeys.<span class=\"pl-c1\">PREDICT</span>:\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions_dict, <span class=\"pl-v\">export_outputs</span><span class=\"pl-k\">=</span>{\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>prediction<span class=\"pl-pds\">'</span></span>: tf.estimator.export.PredictOutput(predictions_dict)\n        })\n\n    loss <span class=\"pl-k\">=</span> tf.losses.sparse_softmax_cross_entropy(labels, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits)\n\n    <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> tf.contrib.learn.ModeKeys.<span class=\"pl-c1\">TRAIN</span>:\n        train_op <span class=\"pl-k\">=</span> tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adam<span class=\"pl-pds\">'</span></span>,\n                                                   <span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>)\n        <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss, <span class=\"pl-v\">train_op</span><span class=\"pl-k\">=</span>train_op)\n\n    eval_metrics_ops <span class=\"pl-k\">=</span> {\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>: tf.metrics.accuracy(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions_dict[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>]),\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>precision<span class=\"pl-pds\">'</span></span>: tf.metrics.precision(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions_dict[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>]),\n        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>recall<span class=\"pl-pds\">'</span></span>: tf.metrics.recall(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>labels, <span class=\"pl-v\">predictions</span><span class=\"pl-k\">=</span>predictions_dict[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>class<span class=\"pl-pds\">'</span></span>])\n    }\n    <span class=\"pl-k\">return</span> tf.estimator.EstimatorSpec(<span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>mode, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span>loss, <span class=\"pl-v\">eval_metric_ops</span><span class=\"pl-k\">=</span>eval_metrics_ops)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_train_record</span>(<span class=\"pl-smi\">record</span>):\n    vector <span class=\"pl-k\">=</span> tf.decode_csv(record, <span class=\"pl-c1\">DEFAULTS</span>, <span class=\"pl-v\">use_quote_delim</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    <span class=\"pl-k\">return</span> vector[<span class=\"pl-c1\">1</span>:], vector[<span class=\"pl-c1\">0</span>]\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">preprocess</span>(<span class=\"pl-smi\">text</span>):\n    text <span class=\"pl-k\">=</span> text.lower()\n    result <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span>.join([word <span class=\"pl-k\">for</span> word <span class=\"pl-k\">in</span> text.split() <span class=\"pl-k\">if</span> word <span class=\"pl-k\">not</span> <span class=\"pl-k\">in</span> (stop_words)])\n    <span class=\"pl-k\">return</span> result\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_vocab</span>(<span class=\"pl-smi\">file_name</span>, <span class=\"pl-smi\">vocab_file_name</span>):\n    df <span class=\"pl-k\">=</span> pd.read_csv(file_name, <span class=\"pl-v\">header</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">sep</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>,<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">skiprows</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">names</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>product<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>consumer_complaint_narrative<span class=\"pl-pds\">'</span></span>])\n    df[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>consumer_complaint_narrative<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> df[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>consumer_complaint_narrative<span class=\"pl-pds\">'</span></span>].apply(preprocess)\n    <span class=\"pl-c1\">print</span>(df[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>consumer_complaint_narrative<span class=\"pl-pds\">'</span></span>][<span class=\"pl-c1\">0</span>])\n    vocab_processor <span class=\"pl-k\">=</span> tflearn.preprocessing.VocabularyProcessor(<span class=\"pl-v\">max_document_length</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">MAX_FEATURES</span>, <span class=\"pl-v\">min_frequency</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>,\n                                                                <span class=\"pl-v\">tokenizer_fn</span><span class=\"pl-k\">=</span>tflearn.preprocessing.tokenizer)\n    vocab_processor.fit(df[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>consumer_complaint_narrative<span class=\"pl-pds\">'</span></span>])\n    <span class=\"pl-k\">with</span> gfile.Open(vocab_file_name, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>wb<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n        f.write(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{}</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>.format(<span class=\"pl-c1\">PADWORD</span>))\n        <span class=\"pl-k\">for</span> word, index <span class=\"pl-k\">in</span> vocab_processor.vocabulary_._mapping.items():\n            f.write(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">{}</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>.format(word))\n    nwords <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(vocab_processor.vocabulary_)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">{}</span> words into <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(nwords, vocab_file_name))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>(<span class=\"pl-smi\">file_name</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">repeat_count</span>, <span class=\"pl-smi\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">_input_fn</span>():\n        data_set <span class=\"pl-k\">=</span> tf.data.TextLineDataset(<span class=\"pl-v\">filenames</span><span class=\"pl-k\">=</span>file_name)\n        data_set <span class=\"pl-k\">=</span> data_set.map(get_train_record)\n        <span class=\"pl-k\">if</span> shuffle:\n            data_set <span class=\"pl-k\">=</span> data_set.shuffle(shuffle)\n        data_set <span class=\"pl-k\">=</span> data_set.repeat(repeat_count)\n        batch <span class=\"pl-k\">=</span> data_set.batch(batch_size)\n        iterator <span class=\"pl-k\">=</span> batch.make_one_shot_iterator()\n        features, labels <span class=\"pl-k\">=</span> iterator.get_next()\n        <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: features}, labels\n\n    <span class=\"pl-k\">return</span> _input_fn()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_train_spec</span>(<span class=\"pl-smi\">file_name</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">repeat_count</span>):\n    <span class=\"pl-k\">return</span> tf.estimator.TrainSpec(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: input_fn(file_name, batch_size, repeat_count, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), <span class=\"pl-v\">max_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1000</span>)\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_test_spec</span>(<span class=\"pl-smi\">file_name</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">repeat_count</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>):\n    <span class=\"pl-k\">return</span> tf.estimator.EvalSpec(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: input_fn(file_name, batch_size, repeat_count, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">serving_input_fn</span>():\n    feature_tensor <span class=\"pl-k\">=</span> tf.placeholder(tf.string, [<span class=\"pl-c1\">None</span>])\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> features = tf.py_func(preprocess, [feature_tensor], tf.string)</span>\n    features <span class=\"pl-k\">=</span> tf.expand_dims(feature_tensor, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">return</span> tf.estimator.export.ServingInputReceiver({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: features}, {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>: features})\n\nfinance_classifier <span class=\"pl-k\">=</span> tf.estimator.Estimator(<span class=\"pl-v\">model_fn</span><span class=\"pl-k\">=</span>model_fn, <span class=\"pl-v\">model_dir</span><span class=\"pl-k\">=</span>model_dir)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> Training .....<span class=\"pl-pds\">'</span></span>)\nfinance_classifier.train(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: input_fn(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dataset/train.csv<span class=\"pl-pds\">'</span></span>, batch_size, <span class=\"pl-v\">repeat_count</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> Evaluating.....<span class=\"pl-pds\">'</span></span>)\neval_results <span class=\"pl-k\">=</span> finance_classifier.evaluate(<span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: input_fn(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dataset/valid.csv<span class=\"pl-pds\">'</span></span>, batch_size, <span class=\"pl-v\">repeat_count</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                                                                  <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n<span class=\"pl-k\">for</span> key <span class=\"pl-k\">in</span> eval_results:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span> <span class=\"pl-c1\">{}</span> was <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(key, eval_results[key]))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span> Exporting<span class=\"pl-pds\">'</span></span>)\nexported_model_dir <span class=\"pl-k\">=</span> finance_classifier.export_savedmodel(model_dir, <span class=\"pl-v\">serving_input_receiver_fn</span><span class=\"pl-k\">=</span>serving_input_fn)\ndecoded_model_dir <span class=\"pl-k\">=</span> exported_model_dir.decode(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>utf-8<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p><a href=\"https://drive.google.com/open?id=1FAmoo9zCBJBAG2IFdySb_r4s1OV6YExn\" rel=\"nofollow\">Screenshot</a></p>\n<p>But when I tried with tf1.2 with some changes in the code in model_fn. Basically not using tf.keras but using tf.contrib.keras it was working. <strong>is this bug ?</strong></p>", "body_text": "Problem while deploying model in Tensorflow 1.4.\nCode :\ndef model_fn(features, labels, mode):\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        tf.keras.backend.set_learning_phase(True)\n    else:\n        tf.keras.backend.set_learning_phase(False)\n\n    input_feature = features['x']\n    table = lookup.index_table_from_file(vocabulary_file='vocab.txt', num_oov_buckets=1, default_value=-1)\n    text = tf.squeeze(input_feature, [1])\n    words = tf.string_split(text)\n    dense_words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\n    numbers = table.lookup(dense_words)\n    padding = tf.constant([[0, 0], [0, MAX_LEN]])\n    padded = tf.pad(numbers, padding)\n    sliced = tf.slice(padded, [0, 0], [-1, MAX_LEN])\n    print('words_sliced={}'.format(words))\n\n    embeds = tf.keras.layers.Embedding(MAX_FEATURES+1, 128, input_length=MAX_LEN)(sliced)\n\n    print('words_embed={}'.format(embeds))\n    f1 = tf.keras.layers.Dropout(0.2)(embeds)\n    f1 = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(f1)\n    f1 = tf.keras.layers.GlobalAveragePooling1D()(f1)\n    f1 = tf.keras.layers.Dense(hidden_dims)(f1)\n    f1 = tf.keras.layers.Dropout(0.5)(f1)\n    f1 = tf.keras.layers.Activation('relu')(f1)\n    logits = tf.keras.layers.Dense(11)(f1)\n\n    predictions_dict = {\n        'class': tf.argmax(logits, 1),\n        'prob': tf.nn.softmax(logits)\n    }\n\n    '''prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),\n                                                           \"probabilities\": tf.nn.softmax(logits,\n                                                                                          name=\"softmax_tensor\")})'''\n\n    if mode == tf.estimator.ModeKeys.PREDICT:\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions_dict, export_outputs={\n            'prediction': tf.estimator.export.PredictOutput(predictions_dict)\n        })\n\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits=logits)\n\n    if mode == tf.contrib.learn.ModeKeys.TRAIN:\n        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), optimizer='Adam',\n                                                   learning_rate=0.001)\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\n    eval_metrics_ops = {\n        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions_dict['class']),\n        'precision': tf.metrics.precision(labels=labels, predictions=predictions_dict['class']),\n        'recall': tf.metrics.recall(labels=labels, predictions=predictions_dict['class'])\n    }\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_ops)\n\ndef get_train_record(record):\n    vector = tf.decode_csv(record, DEFAULTS, use_quote_delim=True)\n    return vector[1:], vector[0]\n\ndef preprocess(text):\n    text = text.lower()\n    result = ' '.join([word for word in text.split() if word not in (stop_words)])\n    return result\n\n\ndef build_vocab(file_name, vocab_file_name):\n    df = pd.read_csv(file_name, header=None, sep=',', skiprows=[1], names=['product', 'consumer_complaint_narrative'])\n    df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].apply(preprocess)\n    print(df['consumer_complaint_narrative'][0])\n    vocab_processor = tflearn.preprocessing.VocabularyProcessor(max_document_length=MAX_FEATURES, min_frequency=10,\n                                                                tokenizer_fn=tflearn.preprocessing.tokenizer)\n    vocab_processor.fit(df['consumer_complaint_narrative'])\n    with gfile.Open(vocab_file_name, 'wb') as f:\n        f.write(\"{}\\n\".format(PADWORD))\n        for word, index in vocab_processor.vocabulary_._mapping.items():\n            f.write(\"{}\\n\".format(word))\n    nwords = len(vocab_processor.vocabulary_)\n    print('{} words into {}'.format(nwords, vocab_file_name))\n\n\ndef input_fn(file_name, batch_size, repeat_count, shuffle=False):\n    def _input_fn():\n        data_set = tf.data.TextLineDataset(filenames=file_name)\n        data_set = data_set.map(get_train_record)\n        if shuffle:\n            data_set = data_set.shuffle(shuffle)\n        data_set = data_set.repeat(repeat_count)\n        batch = data_set.batch(batch_size)\n        iterator = batch.make_one_shot_iterator()\n        features, labels = iterator.get_next()\n        return {'x': features}, labels\n\n    return _input_fn()\n\n\ndef get_train_spec(file_name, batch_size, repeat_count):\n    return tf.estimator.TrainSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True), max_steps=1000)\n\n\ndef get_test_spec(file_name, batch_size, repeat_count=1):\n    return tf.estimator.EvalSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True))\n\n\ndef serving_input_fn():\n    feature_tensor = tf.placeholder(tf.string, [None])\n    # features = tf.py_func(preprocess, [feature_tensor], tf.string)\n    features = tf.expand_dims(feature_tensor, -1)\n    return tf.estimator.export.ServingInputReceiver({'x': features}, {'x': features})\n\nfinance_classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir)\n\nprint('\\n Training .....')\nfinance_classifier.train(input_fn=lambda: input_fn('dataset/train.csv', batch_size, repeat_count=5, shuffle=True))\n\nprint('\\n Evaluating.....')\neval_results = finance_classifier.evaluate(input_fn=lambda: input_fn('dataset/valid.csv', batch_size, repeat_count=1,\n                                                                  shuffle=False))\nfor key in eval_results:\n    print(\" {} was {}\".format(key, eval_results[key]))\n\nprint('\\n Exporting')\nexported_model_dir = finance_classifier.export_savedmodel(model_dir, serving_input_receiver_fn=serving_input_fn)\ndecoded_model_dir = exported_model_dir.decode(\"utf-8\")\nScreenshot\nBut when I tried with tf1.2 with some changes in the code in model_fn. Basically not using tf.keras but using tf.contrib.keras it was working. is this bug ?", "body": "Problem while deploying model in Tensorflow 1.4.\r\n\r\n**Code :**\r\n\r\n```python\r\ndef model_fn(features, labels, mode):\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        tf.keras.backend.set_learning_phase(True)\r\n    else:\r\n        tf.keras.backend.set_learning_phase(False)\r\n\r\n    input_feature = features['x']\r\n    table = lookup.index_table_from_file(vocabulary_file='vocab.txt', num_oov_buckets=1, default_value=-1)\r\n    text = tf.squeeze(input_feature, [1])\r\n    words = tf.string_split(text)\r\n    dense_words = tf.sparse_tensor_to_dense(words, default_value=PADWORD)\r\n    numbers = table.lookup(dense_words)\r\n    padding = tf.constant([[0, 0], [0, MAX_LEN]])\r\n    padded = tf.pad(numbers, padding)\r\n    sliced = tf.slice(padded, [0, 0], [-1, MAX_LEN])\r\n    print('words_sliced={}'.format(words))\r\n\r\n    embeds = tf.keras.layers.Embedding(MAX_FEATURES+1, 128, input_length=MAX_LEN)(sliced)\r\n\r\n    print('words_embed={}'.format(embeds))\r\n    f1 = tf.keras.layers.Dropout(0.2)(embeds)\r\n    f1 = tf.keras.layers.Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1)(f1)\r\n    f1 = tf.keras.layers.GlobalAveragePooling1D()(f1)\r\n    f1 = tf.keras.layers.Dense(hidden_dims)(f1)\r\n    f1 = tf.keras.layers.Dropout(0.5)(f1)\r\n    f1 = tf.keras.layers.Activation('relu')(f1)\r\n    logits = tf.keras.layers.Dense(11)(f1)\r\n\r\n    predictions_dict = {\r\n        'class': tf.argmax(logits, 1),\r\n        'prob': tf.nn.softmax(logits)\r\n    }\r\n\r\n    '''prediction_output = tf.estimator.export.PredictOutput({\"classes\": tf.argmax(input=logits, axis=1),\r\n                                                           \"probabilities\": tf.nn.softmax(logits,\r\n                                                                                          name=\"softmax_tensor\")})'''\r\n\r\n    if mode == tf.estimator.ModeKeys.PREDICT:\r\n        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions_dict, export_outputs={\r\n            'prediction': tf.estimator.export.PredictOutput(predictions_dict)\r\n        })\r\n\r\n    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits=logits)\r\n\r\n    if mode == tf.contrib.learn.ModeKeys.TRAIN:\r\n        train_op = tf.contrib.layers.optimize_loss(loss, tf.contrib.framework.get_global_step(), optimizer='Adam',\r\n                                                   learning_rate=0.001)\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\n    eval_metrics_ops = {\r\n        'accuracy': tf.metrics.accuracy(labels=labels, predictions=predictions_dict['class']),\r\n        'precision': tf.metrics.precision(labels=labels, predictions=predictions_dict['class']),\r\n        'recall': tf.metrics.recall(labels=labels, predictions=predictions_dict['class'])\r\n    }\r\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metrics_ops)\r\n\r\ndef get_train_record(record):\r\n    vector = tf.decode_csv(record, DEFAULTS, use_quote_delim=True)\r\n    return vector[1:], vector[0]\r\n\r\ndef preprocess(text):\r\n    text = text.lower()\r\n    result = ' '.join([word for word in text.split() if word not in (stop_words)])\r\n    return result\r\n\r\n\r\ndef build_vocab(file_name, vocab_file_name):\r\n    df = pd.read_csv(file_name, header=None, sep=',', skiprows=[1], names=['product', 'consumer_complaint_narrative'])\r\n    df['consumer_complaint_narrative'] = df['consumer_complaint_narrative'].apply(preprocess)\r\n    print(df['consumer_complaint_narrative'][0])\r\n    vocab_processor = tflearn.preprocessing.VocabularyProcessor(max_document_length=MAX_FEATURES, min_frequency=10,\r\n                                                                tokenizer_fn=tflearn.preprocessing.tokenizer)\r\n    vocab_processor.fit(df['consumer_complaint_narrative'])\r\n    with gfile.Open(vocab_file_name, 'wb') as f:\r\n        f.write(\"{}\\n\".format(PADWORD))\r\n        for word, index in vocab_processor.vocabulary_._mapping.items():\r\n            f.write(\"{}\\n\".format(word))\r\n    nwords = len(vocab_processor.vocabulary_)\r\n    print('{} words into {}'.format(nwords, vocab_file_name))\r\n\r\n\r\ndef input_fn(file_name, batch_size, repeat_count, shuffle=False):\r\n    def _input_fn():\r\n        data_set = tf.data.TextLineDataset(filenames=file_name)\r\n        data_set = data_set.map(get_train_record)\r\n        if shuffle:\r\n            data_set = data_set.shuffle(shuffle)\r\n        data_set = data_set.repeat(repeat_count)\r\n        batch = data_set.batch(batch_size)\r\n        iterator = batch.make_one_shot_iterator()\r\n        features, labels = iterator.get_next()\r\n        return {'x': features}, labels\r\n\r\n    return _input_fn()\r\n\r\n\r\ndef get_train_spec(file_name, batch_size, repeat_count):\r\n    return tf.estimator.TrainSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True), max_steps=1000)\r\n\r\n\r\ndef get_test_spec(file_name, batch_size, repeat_count=1):\r\n    return tf.estimator.EvalSpec(input_fn=lambda: input_fn(file_name, batch_size, repeat_count, shuffle=True))\r\n\r\n\r\ndef serving_input_fn():\r\n    feature_tensor = tf.placeholder(tf.string, [None])\r\n    # features = tf.py_func(preprocess, [feature_tensor], tf.string)\r\n    features = tf.expand_dims(feature_tensor, -1)\r\n    return tf.estimator.export.ServingInputReceiver({'x': features}, {'x': features})\r\n\r\nfinance_classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir)\r\n\r\nprint('\\n Training .....')\r\nfinance_classifier.train(input_fn=lambda: input_fn('dataset/train.csv', batch_size, repeat_count=5, shuffle=True))\r\n\r\nprint('\\n Evaluating.....')\r\neval_results = finance_classifier.evaluate(input_fn=lambda: input_fn('dataset/valid.csv', batch_size, repeat_count=1,\r\n                                                                  shuffle=False))\r\nfor key in eval_results:\r\n    print(\" {} was {}\".format(key, eval_results[key]))\r\n\r\nprint('\\n Exporting')\r\nexported_model_dir = finance_classifier.export_savedmodel(model_dir, serving_input_receiver_fn=serving_input_fn)\r\ndecoded_model_dir = exported_model_dir.decode(\"utf-8\")\r\n```\r\n\r\n[Screenshot](https://drive.google.com/open?id=1FAmoo9zCBJBAG2IFdySb_r4s1OV6YExn)\r\n\r\nBut when I tried with tf1.2 with some changes in the code in model_fn. Basically not using tf.keras but using tf.contrib.keras it was working. **is this bug ?**"}