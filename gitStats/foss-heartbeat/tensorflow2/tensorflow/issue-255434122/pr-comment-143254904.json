{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143254904", "pull_request_review_id": 67745250, "id": 143254904, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MzI1NDkwNA==", "diff_hunk": "@@ -633,6 +633,7 @@ def batch_norm(inputs,\n           renorm_clipping=renorm_clipping,\n           renorm_momentum=renorm_decay,\n           name=sc.name,\n+          dtype=inputs.dtype.base_dtype,", "path": "tensorflow/contrib/layers/python/layers/layers.py", "position": null, "original_position": 4, "commit_id": "7bcf8a127c5c9141535b126ac6d32b7a2dbea841", "original_commit_id": "e8ee3e4fa73fa004053f0df9567c5286ffa56088", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "body": "We should not support fp16 and fp64 without tests. Also, for fused_batch_norm, the only fp16 case we support is where the input is fp16 and the scale and offset are in fp32. So if we support fp16 for non-fused batch norm, we should support that case.\r\n\r\nIn short, I'd remove the line. I plan on adding support for fp16 in layers shortly. If you want to do so, tell me so I don't repeat the work.", "created_at": "2017-10-06T17:47:01Z", "updated_at": "2017-10-25T22:56:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12832#discussion_r143254904", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12832", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/143254904"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12832#discussion_r143254904"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12832"}}, "body_html": "<p>We should not support fp16 and fp64 without tests. Also, for fused_batch_norm, the only fp16 case we support is where the input is fp16 and the scale and offset are in fp32. So if we support fp16 for non-fused batch norm, we should support that case.</p>\n<p>In short, I'd remove the line. I plan on adding support for fp16 in layers shortly. If you want to do so, tell me so I don't repeat the work.</p>", "body_text": "We should not support fp16 and fp64 without tests. Also, for fused_batch_norm, the only fp16 case we support is where the input is fp16 and the scale and offset are in fp32. So if we support fp16 for non-fused batch norm, we should support that case.\nIn short, I'd remove the line. I plan on adding support for fp16 in layers shortly. If you want to do so, tell me so I don't repeat the work.", "in_reply_to_id": 143250841}