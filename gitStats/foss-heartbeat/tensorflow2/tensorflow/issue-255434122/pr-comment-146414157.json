{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146414157", "pull_request_review_id": 71352742, "id": 146414157, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjQxNDE1Nw==", "diff_hunk": "@@ -45,22 +47,34 @@ def GetTestConfigs():\n \n class Conv3DTest(test.TestCase):\n \n+  def _DtypesToTest(self, use_gpu):\n+    if use_gpu:\n+      if not test_util.CudaSupportsHalfMatMulAndConv():\n+        return [dtypes.float32]\n+      else:\n+        # It is important that float32 comes before float16 here,\n+        # as we will be using its gradients as reference for fp16 gradients.\n+        return [dtypes.float32, dtypes.float16]\n+    else:\n+      return [dtypes.float64, dtypes.float32, dtypes.float16]\n+\n   def _SetupValuesForDevice(self, tensor_in_sizes, filter_in_sizes, stride,\n-                            padding, data_format, use_gpu):\n+                            padding, data_format, dtype, use_gpu):\n     total_size_1 = 1\n     total_size_2 = 1\n     for s in tensor_in_sizes:\n       total_size_1 *= s\n     for s in filter_in_sizes:\n       total_size_2 *= s\n \n-    # Initializes the input tensor with array containing incrementing\n-    # numbers from 1.\n-    x1 = [f * 1.0 for f in range(1, total_size_1 + 1)]\n-    x2 = [f * 1.0 for f in range(1, total_size_2 + 1)]\n+    # Initializes the input tensor with array containing numbers from 0 to 1.\n+    # We keep the input tensor values fairly small to avoid overflowing a float16 \n+    # tensor during the conv3d and to keep absolute errors small", "path": "tensorflow/python/kernel_tests/conv_ops_3d_test.py", "position": null, "original_position": 40, "commit_id": "7bcf8a127c5c9141535b126ac6d32b7a2dbea841", "original_commit_id": "f5466099da8bcecf91a60cafc6f607c6f56f140d", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "body": "Sorry for late response. I see your point about making 'tolerances' smaller, so you can leave atol as is.\r\n\r\nI think you should remove the \"and to keep absolute errors small\" comment. You're right that having a large exponent with fp16 means large absolute errors, but I don't see that as a problem. We could have just raised 'atol' in that case. Differences greater than 10 seem reasonable given how imprecise fp16 is. However, currently, some expected_outputs are above the maximum fp16 value of 65504, which would cause them to overflow. This seems like the only reason to reduce the numbers in the input tensor IMO.", "created_at": "2017-10-23T22:46:34Z", "updated_at": "2017-10-25T22:56:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12832#discussion_r146414157", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12832", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146414157"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12832#discussion_r146414157"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12832"}}, "body_html": "<p>Sorry for late response. I see your point about making 'tolerances' smaller, so you can leave atol as is.</p>\n<p>I think you should remove the \"and to keep absolute errors small\" comment. You're right that having a large exponent with fp16 means large absolute errors, but I don't see that as a problem. We could have just raised 'atol' in that case. Differences greater than 10 seem reasonable given how imprecise fp16 is. However, currently, some expected_outputs are above the maximum fp16 value of 65504, which would cause them to overflow. This seems like the only reason to reduce the numbers in the input tensor IMO.</p>", "body_text": "Sorry for late response. I see your point about making 'tolerances' smaller, so you can leave atol as is.\nI think you should remove the \"and to keep absolute errors small\" comment. You're right that having a large exponent with fp16 means large absolute errors, but I don't see that as a problem. We could have just raised 'atol' in that case. Differences greater than 10 seem reasonable given how imprecise fp16 is. However, currently, some expected_outputs are above the maximum fp16 value of 65504, which would cause them to overflow. This seems like the only reason to reduce the numbers in the input tensor IMO.", "in_reply_to_id": 145851069}