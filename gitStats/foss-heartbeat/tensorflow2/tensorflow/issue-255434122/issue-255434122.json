{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12832", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12832/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12832/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12832/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/12832", "id": 255434122, "node_id": "MDExOlB1bGxSZXF1ZXN0MTM5NDQwMTE0", "number": 12832, "title": "Float16 (half or Eigen::half) for conv3d ops", "user": {"login": "opensourcemattress", "id": 31660642, "node_id": "MDQ6VXNlcjMxNjYwNjQy", "avatar_url": "https://avatars1.githubusercontent.com/u/31660642?v=4", "gravatar_id": "", "url": "https://api.github.com/users/opensourcemattress", "html_url": "https://github.com/opensourcemattress", "followers_url": "https://api.github.com/users/opensourcemattress/followers", "following_url": "https://api.github.com/users/opensourcemattress/following{/other_user}", "gists_url": "https://api.github.com/users/opensourcemattress/gists{/gist_id}", "starred_url": "https://api.github.com/users/opensourcemattress/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/opensourcemattress/subscriptions", "organizations_url": "https://api.github.com/users/opensourcemattress/orgs", "repos_url": "https://api.github.com/users/opensourcemattress/repos", "events_url": "https://api.github.com/users/opensourcemattress/events{/privacy}", "received_events_url": "https://api.github.com/users/opensourcemattress/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 419840263, "node_id": "MDU6TGFiZWw0MTk4NDAyNjM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20testing%20(then%20merge)", "name": "awaiting testing (then merge)", "color": "c2e0c6", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2017-09-05T23:07:16Z", "updated_at": "2017-11-05T07:23:37Z", "closed_at": "2017-11-05T07:23:37Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12832", "html_url": "https://github.com/tensorflow/tensorflow/pull/12832", "diff_url": "https://github.com/tensorflow/tensorflow/pull/12832.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/12832.patch"}, "body_html": "<p>Registrations of conv3d operations with fp16,  fp16 for batch_norms in tf.layers and tf.contrib.layers.  Related issue: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"241149812\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11341\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/11341/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/11341\">#11341</a></p>\n<p>I don't understand how fp16 operations must be implemented in low-level with CUDA and I didn't make any additional optimizations.<br>\nThere is no implementation for fused batch_norm yet.</p>\n<p>I copied part of code for dtypes from conv2d test to conv3d test and seems like all works.<br>\nWith fp16 I get <code>inf</code> and large absolute errors with large numbers. I suppose that it's fine. At least CPU and GPU  implementations return almost similar values. I skip such cases in test.<br>\nExample of such case:</p>\n<blockquote>\n<p>use_gpu: False<br>\ndtype: &lt;dtype: 'float32'&gt;<br>\ndata_format: NDHWC<br>\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]<br>\nactual =  [[[[[  36564.   38022.   39480.]<br>\n[  37824.   39354.   40884.]<br>\n[  39084.   40686.   42288.]]</p>\n<p>[[  46644.   48678.   50712.]<br>\n[  47904.   50010.   52116.]<br>\n[  49164.   51342.   53520.]]]</p>\n<p>[[[ 107124.  112614.  118104.]<br>\n[ 108384.  113946.  119508.]<br>\n[ 109644.  115278.  120912.]]</p>\n<p>[[ 117204.  123270.  129336.]<br>\n[ 118464.  124602.  130740.]<br>\n[ 119724.  125934.  132144.]]]]]</p>\n<p>use_gpu: False<br>\ndtype: &lt;dtype: 'float16'&gt;<br>\ndata_format: NDHWC<br>\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]<br>\nactual =  [[[[[ 36544.  38016.  39488.]<br>\n[ 37824.  39360.  40896.]<br>\n[ 39072.  40704.  42304.]]</p>\n<p>[[ 46656.  48672.  50688.]<br>\n[ 47936.  50016.  52128.]<br>\n[ 49184.  51328.  53536.]]]</p>\n<p>[[[    inf     inf     inf]<br>\n[    inf     inf     inf]<br>\n[    inf     inf     inf]]</p>\n<p>[[    inf     inf     inf]<br>\n[    inf     inf     inf]<br>\n[    inf     inf     inf]]]]]<br>\nfp16 using may result in inf values and large absolute errors when used with large numbers, skipping</p>\n<p>use_gpu: True<br>\ndtype: &lt;dtype: 'float32'&gt;<br>\ndata_format: NDHWC<br>\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]<br>\nactual =  [[[[[  36564.   38022.   39480.]<br>\n[  37824.   39354.   40884.]<br>\n[  39084.   40686.   42288.]]</p>\n<p>[[  46644.   48678.   50712.]<br>\n[  47904.   50010.   52116.]<br>\n[  49164.   51342.   53520.]]]</p>\n<p>[[[ 107124.  112614.  118104.]<br>\n[ 108384.  113946.  119508.]<br>\n[ 109644.  115278.  120912.]]</p>\n<p>[[ 117204.  123270.  129336.]<br>\n[ 118464.  124602.  130740.]<br>\n[ 119724.  125934.  132144.]]]]]</p>\n<p>use_gpu: True<br>\ndtype: &lt;dtype: 'float16'&gt;<br>\ndata_format: NDHWC<br>\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]<br>\nactual =  [[[[[ 36576.  38016.  39488.]<br>\n[ 37824.  39360.  40896.]<br>\n[ 39072.  40672.  42304.]]</p>\n<p>[[ 46656.  48672.  50720.]<br>\n[ 47904.  50016.  52128.]<br>\n[ 49152.  51328.  53504.]]]</p>\n<p>[[[    inf     inf     inf]<br>\n[    inf     inf     inf]<br>\n[    inf     inf     inf]]</p>\n<p>[[    inf     inf     inf]<br>\n[    inf     inf     inf]<br>\n[    inf     inf     inf]]]]]<br>\nfp16 using may result in inf values and large absolute errors when used with large numbers, skipping</p>\n</blockquote>\n<p>fp16 is not fully covered by tests because I not sure how to do it.</p>", "body_text": "Registrations of conv3d operations with fp16,  fp16 for batch_norms in tf.layers and tf.contrib.layers.  Related issue: #11341\nI don't understand how fp16 operations must be implemented in low-level with CUDA and I didn't make any additional optimizations.\nThere is no implementation for fused batch_norm yet.\nI copied part of code for dtypes from conv2d test to conv3d test and seems like all works.\nWith fp16 I get inf and large absolute errors with large numbers. I suppose that it's fine. At least CPU and GPU  implementations return almost similar values. I skip such cases in test.\nExample of such case:\n\nuse_gpu: False\ndtype: <dtype: 'float32'>\ndata_format: NDHWC\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\nactual =  [[[[[  36564.   38022.   39480.]\n[  37824.   39354.   40884.]\n[  39084.   40686.   42288.]]\n[[  46644.   48678.   50712.]\n[  47904.   50010.   52116.]\n[  49164.   51342.   53520.]]]\n[[[ 107124.  112614.  118104.]\n[ 108384.  113946.  119508.]\n[ 109644.  115278.  120912.]]\n[[ 117204.  123270.  129336.]\n[ 118464.  124602.  130740.]\n[ 119724.  125934.  132144.]]]]]\nuse_gpu: False\ndtype: <dtype: 'float16'>\ndata_format: NDHWC\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\nactual =  [[[[[ 36544.  38016.  39488.]\n[ 37824.  39360.  40896.]\n[ 39072.  40704.  42304.]]\n[[ 46656.  48672.  50688.]\n[ 47936.  50016.  52128.]\n[ 49184.  51328.  53536.]]]\n[[[    inf     inf     inf]\n[    inf     inf     inf]\n[    inf     inf     inf]]\n[[    inf     inf     inf]\n[    inf     inf     inf]\n[    inf     inf     inf]]]]]\nfp16 using may result in inf values and large absolute errors when used with large numbers, skipping\nuse_gpu: True\ndtype: <dtype: 'float32'>\ndata_format: NDHWC\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\nactual =  [[[[[  36564.   38022.   39480.]\n[  37824.   39354.   40884.]\n[  39084.   40686.   42288.]]\n[[  46644.   48678.   50712.]\n[  47904.   50010.   52116.]\n[  49164.   51342.   53520.]]]\n[[[ 107124.  112614.  118104.]\n[ 108384.  113946.  119508.]\n[ 109644.  115278.  120912.]]\n[[ 117204.  123270.  129336.]\n[ 118464.  124602.  130740.]\n[ 119724.  125934.  132144.]]]]]\nuse_gpu: True\ndtype: <dtype: 'float16'>\ndata_format: NDHWC\nexpected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\nactual =  [[[[[ 36576.  38016.  39488.]\n[ 37824.  39360.  40896.]\n[ 39072.  40672.  42304.]]\n[[ 46656.  48672.  50720.]\n[ 47904.  50016.  52128.]\n[ 49152.  51328.  53504.]]]\n[[[    inf     inf     inf]\n[    inf     inf     inf]\n[    inf     inf     inf]]\n[[    inf     inf     inf]\n[    inf     inf     inf]\n[    inf     inf     inf]]]]]\nfp16 using may result in inf values and large absolute errors when used with large numbers, skipping\n\nfp16 is not fully covered by tests because I not sure how to do it.", "body": "Registrations of conv3d operations with fp16,  fp16 for batch_norms in tf.layers and tf.contrib.layers.  Related issue: https://github.com/tensorflow/tensorflow/issues/11341\r\n\r\nI don't understand how fp16 operations must be implemented in low-level with CUDA and I didn't make any additional optimizations.\r\nThere is no implementation for fused batch_norm yet.\r\n\r\nI copied part of code for dtypes from conv2d test to conv3d test and seems like all works. \r\nWith fp16 I get `inf` and large absolute errors with large numbers. I suppose that it's fine. At least CPU and GPU  implementations return almost similar values. I skip such cases in test.\r\nExample of such case:\r\n\r\n> \r\n> use_gpu: False\r\n> dtype: <dtype: 'float32'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[  36564.   38022.   39480.]\r\n>     [  37824.   39354.   40884.]\r\n>     [  39084.   40686.   42288.]]\r\n> \r\n>    [[  46644.   48678.   50712.]\r\n>     [  47904.   50010.   52116.]\r\n>     [  49164.   51342.   53520.]]]\r\n> \r\n> \r\n>   [[[ 107124.  112614.  118104.]\r\n>     [ 108384.  113946.  119508.]\r\n>     [ 109644.  115278.  120912.]]\r\n> \r\n>    [[ 117204.  123270.  129336.]\r\n>     [ 118464.  124602.  130740.]\r\n>     [ 119724.  125934.  132144.]]]]]\r\n> \r\n> use_gpu: False\r\n> dtype: <dtype: 'float16'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[ 36544.  38016.  39488.]\r\n>     [ 37824.  39360.  40896.]\r\n>     [ 39072.  40704.  42304.]]\r\n> \r\n>    [[ 46656.  48672.  50688.]\r\n>     [ 47936.  50016.  52128.]\r\n>     [ 49184.  51328.  53536.]]]\r\n> \r\n> \r\n>   [[[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]\r\n> \r\n>    [[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]]]]\r\n> fp16 using may result in inf values and large absolute errors when used with large numbers, skipping\r\n> \r\n> use_gpu: True\r\n> dtype: <dtype: 'float32'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[  36564.   38022.   39480.]\r\n>     [  37824.   39354.   40884.]\r\n>     [  39084.   40686.   42288.]]\r\n> \r\n>    [[  46644.   48678.   50712.]\r\n>     [  47904.   50010.   52116.]\r\n>     [  49164.   51342.   53520.]]]\r\n> \r\n> \r\n>   [[[ 107124.  112614.  118104.]\r\n>     [ 108384.  113946.  119508.]\r\n>     [ 109644.  115278.  120912.]]\r\n> \r\n>    [[ 117204.  123270.  129336.]\r\n>     [ 118464.  124602.  130740.]\r\n>     [ 119724.  125934.  132144.]]]]]\r\n> \r\n> use_gpu: True\r\n> dtype: <dtype: 'float16'>\r\n> data_format: NDHWC\r\n> expected =  [36564.0, 38022.0, 39480.0, 37824.0, 39354.0, 40884.0, 39084.0, 40686.0, 42288.0, 46644.0, 48678.0, 50712.0, 47904.0, 50010.0, 52116.0, 49164.0, 51342.0, 53520.0, 107124.0, 112614.0, 118104.0, 108384.0, 113946.0, 119508.0, 109644.0, 115278.0, 120912.0, 117204.0, 123270.0, 129336.0, 118464.0, 124602.0, 130740.0, 119724.0, 125934.0, 132144.0]\r\n> actual =  [[[[[ 36576.  38016.  39488.]\r\n>     [ 37824.  39360.  40896.]\r\n>     [ 39072.  40672.  42304.]]\r\n> \r\n>    [[ 46656.  48672.  50720.]\r\n>     [ 47904.  50016.  52128.]\r\n>     [ 49152.  51328.  53504.]]]\r\n> \r\n> \r\n>   [[[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]\r\n> \r\n>    [[    inf     inf     inf]\r\n>     [    inf     inf     inf]\r\n>     [    inf     inf     inf]]]]]\r\n> fp16 using may result in inf values and large absolute errors when used with large numbers, skipping\r\n\r\n\r\nfp16 is not fully covered by tests because I not sure how to do it."}