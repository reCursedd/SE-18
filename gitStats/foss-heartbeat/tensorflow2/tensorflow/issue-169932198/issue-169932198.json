{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3696", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3696/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3696/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3696/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3696", "id": 169932198, "node_id": "MDU6SXNzdWUxNjk5MzIxOTg=", "number": 3696, "title": "conv3d_transpose not freeing memory", "user": {"login": "KendallWeihe", "id": 3602993, "node_id": "MDQ6VXNlcjM2MDI5OTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3602993?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KendallWeihe", "html_url": "https://github.com/KendallWeihe", "followers_url": "https://api.github.com/users/KendallWeihe/followers", "following_url": "https://api.github.com/users/KendallWeihe/following{/other_user}", "gists_url": "https://api.github.com/users/KendallWeihe/gists{/gist_id}", "starred_url": "https://api.github.com/users/KendallWeihe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KendallWeihe/subscriptions", "organizations_url": "https://api.github.com/users/KendallWeihe/orgs", "repos_url": "https://api.github.com/users/KendallWeihe/repos", "events_url": "https://api.github.com/users/KendallWeihe/events{/privacy}", "received_events_url": "https://api.github.com/users/KendallWeihe/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2016-08-08T14:07:38Z", "updated_at": "2016-08-08T15:39:30Z", "closed_at": "2016-08-08T15:39:30Z", "author_association": "NONE", "body_html": "<p>This issue might be the same as <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"163229319\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3128\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3128/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3128\">#3128</a> (which is still awaiting Googler), but it is occurring in a different way. The above issue was run on a CPU. I have since upgrade to a GTX 1070 with 8GB RAM.</p>\n<p>Instead of a Segmentation Fault, the program now runs until it runs out of memory -- in my case it runs for roughly 2000 iterations. Running on a CPU was spitting our a <code>free()</code> error, which leads me to believe this is the same underlying issue.</p>\n<p>The error log looks like this...</p>\n<pre><code>I tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 14.36MiB was 8.00MiB, Chunk State: \nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00200 of size 256\n\n\n... a ton of other chunks\n\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a1686e00 of size 100362240\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a763d600 of size 15054336\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a8498c00 of size 199075840\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005c7cd00 of size 64000\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005d66600 of size 1638400\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x100063fcc00 of size 196608\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10193467b00 of size 6272768\nI tensorflow/core/common_runtime/bfc_allocator.cc:689]      Summary of in-use Chunks by size: \nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 30 Chunks of size 256 totalling 7.5KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 512 totalling 2.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 3072 totalling 12.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 16128 totalling 63.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 65536 totalling 256.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 262144 totalling 1.00MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 614400 totalling 2.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 1638400 totalling 6.25MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 3136512 totalling 2.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1061 Chunks of size 6272768 totalling 6.20GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 15054336 totalling 43.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 100362240 totalling 287.14MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 199075840 totalling 189.85MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 6.72GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                  7223063348\nInUse:                  7214891776\nMaxInUse:               7214891776\nNumAllocs:                  312188\nMaxAllocSize:            199075840\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ***************************************************************************************************x\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 14.36MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[1,32,3,198,198]\nTraceback (most recent call last):\n  File \"CNN-seg-3D.py\", line 208, in &lt;module&gt;\n    keep_prob: dropout})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[1,32,3,198,198]\n     [[Node: opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 2, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1/Relu, conv1/MaxPool3D, opt/gradients/conv2/Conv3D_grad/tuple/control_dependency)]]\nCaused by op u'opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad', defined at:\n  File \"CNN-seg-3D.py\", line 181, in &lt;module&gt;\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 476, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\n    padding=op.get_attr(\"padding\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1182, in max_pool3d_grad\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'conv1/MaxPool3D', defined at:\n  File \"CNN-seg-3D.py\", line 168, in &lt;module&gt;\n    pred = conv_net(x, weights, biases, keep_prob)\n  File \"CNN-seg-3D.py\", line 97, in conv_net\n    conv1 = maxpool3d(conv1, k=2)\n  File \"CNN-seg-3D.py\", line 78, in maxpool3d\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1150, in max_pool3d\n    strides=strides, padding=padding, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n</code></pre>", "body_text": "This issue might be the same as #3128 (which is still awaiting Googler), but it is occurring in a different way. The above issue was run on a CPU. I have since upgrade to a GTX 1070 with 8GB RAM.\nInstead of a Segmentation Fault, the program now runs until it runs out of memory -- in my case it runs for roughly 2000 iterations. Running on a CPU was spitting our a free() error, which leads me to believe this is the same underlying issue.\nThe error log looks like this...\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 14.36MiB was 8.00MiB, Chunk State: \nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00200 of size 256\n\n\n... a ton of other chunks\n\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a1686e00 of size 100362240\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a763d600 of size 15054336\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a8498c00 of size 199075840\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005c7cd00 of size 64000\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005d66600 of size 1638400\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x100063fcc00 of size 196608\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10193467b00 of size 6272768\nI tensorflow/core/common_runtime/bfc_allocator.cc:689]      Summary of in-use Chunks by size: \nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 30 Chunks of size 256 totalling 7.5KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 512 totalling 2.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 3072 totalling 12.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 16128 totalling 63.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 65536 totalling 256.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 262144 totalling 1.00MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 614400 totalling 2.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 1638400 totalling 6.25MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 3136512 totalling 2.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1061 Chunks of size 6272768 totalling 6.20GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 15054336 totalling 43.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 100362240 totalling 287.14MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 199075840 totalling 189.85MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 6.72GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                  7223063348\nInUse:                  7214891776\nMaxInUse:               7214891776\nNumAllocs:                  312188\nMaxAllocSize:            199075840\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ***************************************************************************************************x\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 14.36MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[1,32,3,198,198]\nTraceback (most recent call last):\n  File \"CNN-seg-3D.py\", line 208, in <module>\n    keep_prob: dropout})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[1,32,3,198,198]\n     [[Node: opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 2, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1/Relu, conv1/MaxPool3D, opt/gradients/conv2/Conv3D_grad/tuple/control_dependency)]]\nCaused by op u'opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad', defined at:\n  File \"CNN-seg-3D.py\", line 181, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 476, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\n    padding=op.get_attr(\"padding\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1182, in max_pool3d_grad\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'conv1/MaxPool3D', defined at:\n  File \"CNN-seg-3D.py\", line 168, in <module>\n    pred = conv_net(x, weights, biases, keep_prob)\n  File \"CNN-seg-3D.py\", line 97, in conv_net\n    conv1 = maxpool3d(conv1, k=2)\n  File \"CNN-seg-3D.py\", line 78, in maxpool3d\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1150, in max_pool3d\n    strides=strides, padding=padding, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()", "body": "This issue might be the same as #3128 (which is still awaiting Googler), but it is occurring in a different way. The above issue was run on a CPU. I have since upgrade to a GTX 1070 with 8GB RAM. \n\nInstead of a Segmentation Fault, the program now runs until it runs out of memory -- in my case it runs for roughly 2000 iterations. Running on a CPU was spitting our a `free()` error, which leads me to believe this is the same underlying issue. \n\nThe error log looks like this...\n\n```\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (134217728):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:639] Bin (268435456):     Total Chunks: 0, Chunks in use: 0 0B allocated for chunks. 0B client-requested for chunks. 0B in use in bin. 0B client-requested in use in bin.\nI tensorflow/core/common_runtime/bfc_allocator.cc:656] Bin for 14.36MiB was 8.00MiB, Chunk State: \nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00000 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00100 of size 256\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x10005a00200 of size 256\n\n\n... a ton of other chunks\n\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a1686e00 of size 100362240\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a763d600 of size 15054336\nI tensorflow/core/common_runtime/bfc_allocator.cc:674] Chunk at 0x101a8498c00 of size 199075840\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005c7cd00 of size 64000\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10005d66600 of size 1638400\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x100063fcc00 of size 196608\nI tensorflow/core/common_runtime/bfc_allocator.cc:683] Free at 0x10193467b00 of size 6272768\nI tensorflow/core/common_runtime/bfc_allocator.cc:689]      Summary of in-use Chunks by size: \nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 30 Chunks of size 256 totalling 7.5KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 512 totalling 2.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 3072 totalling 12.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 16128 totalling 63.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 65536 totalling 256.0KiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 262144 totalling 1.00MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 5 Chunks of size 614400 totalling 2.93MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 4 Chunks of size 1638400 totalling 6.25MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 3136512 totalling 2.99MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1061 Chunks of size 6272768 totalling 6.20GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 15054336 totalling 43.07MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 3 Chunks of size 100362240 totalling 287.14MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:692] 1 Chunks of size 199075840 totalling 189.85MiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] Sum Total of in-use chunks: 6.72GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:698] Stats: \nLimit:                  7223063348\nInUse:                  7214891776\nMaxInUse:               7214891776\nNumAllocs:                  312188\nMaxAllocSize:            199075840\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:270] ***************************************************************************************************x\nW tensorflow/core/common_runtime/bfc_allocator.cc:271] Ran out of memory trying to allocate 14.36MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:940] Resource exhausted: OOM when allocating tensor with shape[1,32,3,198,198]\nTraceback (most recent call last):\n  File \"CNN-seg-3D.py\", line 208, in <module>\n    keep_prob: dropout})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.ResourceExhaustedError: OOM when allocating tensor with shape[1,32,3,198,198]\n     [[Node: opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 2, 2, 2, 1], padding=\"SAME\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](conv1/Relu, conv1/MaxPool3D, opt/gradients/conv2/Conv3D_grad/tuple/control_dependency)]]\nCaused by op u'opt/gradients/conv1/MaxPool3D_grad/MaxPool3DGrad', defined at:\n  File \"CNN-seg-3D.py\", line 181, in <module>\n    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gradients.py\", line 476, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\n    padding=op.get_attr(\"padding\"))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1182, in max_pool3d_grad\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'conv1/MaxPool3D', defined at:\n  File \"CNN-seg-3D.py\", line 168, in <module>\n    pred = conv_net(x, weights, biases, keep_prob)\n  File \"CNN-seg-3D.py\", line 97, in conv_net\n    conv1 = maxpool3d(conv1, k=2)\n  File \"CNN-seg-3D.py\", line 78, in maxpool3d\n    padding='SAME')\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1150, in max_pool3d\n    strides=strides, padding=padding, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n"}