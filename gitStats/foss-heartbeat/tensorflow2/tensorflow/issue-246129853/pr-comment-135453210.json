{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/135453210", "pull_request_review_id": 58865346, "id": 135453210, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzNTQ1MzIxMA==", "diff_hunk": "@@ -0,0 +1,111 @@\n+# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# =============================================================================\n+\n+# pylint: disable=unused-import,g-bad-import-order\n+\"\"\"Contains the maxout layer\n+\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import gen_array_ops\n+\n+from tensorflow.python.layers import base\n+\n+\n+def maxout(inputs, num_units, axis=-1, name=None):\n+    \"\"\"Adds a maxout op from https://arxiv.org/abs/1302.4389\n+\n+      \"Maxout Networks\"\n+\n+      Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua\n+      Bengio\n+\n+    Usually the operation is performed in the filter/channel dimension. This can also be\n+    used after fully-connected layers to reduce number of features.\n+\n+    Args:\n+      inputs: Tensor input\n+      num_units: Specifies how many features will remain after maxout in the `axis` dimension (usually channel).\n+      This must be multiple of number of `axis`.\n+      axis: The dimension where max pooling will be performed. Default is the\n+        last dimension.\n+      name: Optional scope for name_scope.\n+    Returns:\n+      A `Tensor` representing the results of the pooling operation.\n+    Raises:\n+      ValueError: if num_units is not multiple of number of features.\n+    \"\"\"\n+\n+    return MaxOut(num_units=num_units, axis=axis, name=name)(inputs)\n+\n+\n+class MaxOut(base.Layer):\n+    \"\"\"Adds a maxout op from https://arxiv.org/abs/1302.4389\n+\n+      \"Maxout Networks\"\n+\n+      Ian J. Goodfellow, David Warde-Farley, Mehdi Mirza, Aaron Courville, Yoshua\n+      Bengio\n+\n+    Usually the operation is performed in the filter/channel dimension. This can also be\n+    used after fully-connected layers to reduce number of features.\n+\n+    Args:\n+      inputs: Tensor input\n+      num_units: Specifies how many features will remain after maxout in the `axis` dimension (usually channel).\n+      This must be multiple of number of `axis`.\n+      axis: The dimension where max pooling will be performed. Default is the\n+        last dimension.\n+      name: Optional scope for name_scope.\n+    Returns:\n+      A `Tensor` representing the results of the pooling operation.\n+    Raises:\n+      ValueError: if num_units is not multiple of number of features.\n+    \"\"\"\n+\n+    def __init__(self,\n+                 num_units,\n+                 axis=-1,\n+                 name=None,\n+                 **kwargs):\n+        super(MaxOut, self).__init__(\n+            name=name, trainable=False, **kwargs)\n+        self.axis = axis\n+        self.num_units = num_units\n+\n+    def call(self, inputs, training=False):\n+        inputs = ops.convert_to_tensor(inputs)\n+        shape = inputs.get_shape().as_list()\n+        if self.axis is None:", "path": "tensorflow/python/layers/maxout.py", "position": null, "original_position": 94, "commit_id": "487ad4b071dfb996879519716e34ce82391e447d", "original_commit_id": "d0193c1f616f1d17e41133d39b1066c5b769342e", "user": {"login": "tiagofrepereira2012", "id": 1748842, "node_id": "MDQ6VXNlcjE3NDg4NDI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1748842?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tiagofrepereira2012", "html_url": "https://github.com/tiagofrepereira2012", "followers_url": "https://api.github.com/users/tiagofrepereira2012/followers", "following_url": "https://api.github.com/users/tiagofrepereira2012/following{/other_user}", "gists_url": "https://api.github.com/users/tiagofrepereira2012/gists{/gist_id}", "starred_url": "https://api.github.com/users/tiagofrepereira2012/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tiagofrepereira2012/subscriptions", "organizations_url": "https://api.github.com/users/tiagofrepereira2012/orgs", "repos_url": "https://api.github.com/users/tiagofrepereira2012/repos", "events_url": "https://api.github.com/users/tiagofrepereira2012/events{/privacy}", "received_events_url": "https://api.github.com/users/tiagofrepereira2012/received_events", "type": "User", "site_admin": false}, "body": "I'm back.\r\n\r\nYes, you are right. I'll patch that.", "created_at": "2017-08-28T06:58:27Z", "updated_at": "2017-08-30T20:08:20Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11824#discussion_r135453210", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11824", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/135453210"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11824#discussion_r135453210"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11824"}}, "body_html": "<p>I'm back.</p>\n<p>Yes, you are right. I'll patch that.</p>", "body_text": "I'm back.\nYes, you are right. I'll patch that.", "in_reply_to_id": 131455000}