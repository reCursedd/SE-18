{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22528", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22528/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22528/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22528/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22528", "id": 363953463, "node_id": "MDU6SXNzdWUzNjM5NTM0NjM=", "number": 22528, "title": "TfLite Quantization Failure: Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data,", "user": {"login": "abladdha", "id": 41565525, "node_id": "MDQ6VXNlcjQxNTY1NTI1", "avatar_url": "https://avatars1.githubusercontent.com/u/41565525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abladdha", "html_url": "https://github.com/abladdha", "followers_url": "https://api.github.com/users/abladdha/followers", "following_url": "https://api.github.com/users/abladdha/following{/other_user}", "gists_url": "https://api.github.com/users/abladdha/gists{/gist_id}", "starred_url": "https://api.github.com/users/abladdha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abladdha/subscriptions", "organizations_url": "https://api.github.com/users/abladdha/orgs", "repos_url": "https://api.github.com/users/abladdha/repos", "events_url": "https://api.github.com/users/abladdha/events{/privacy}", "received_events_url": "https://api.github.com/users/abladdha/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-09-26T10:14:07Z", "updated_at": "2018-10-17T03:01:13Z", "closed_at": "2018-10-16T20:44:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.11.0-rc1</li>\n<li><strong>Python version</strong>: 2.7</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to save Quantized version of graph in tflite format but getting the following error</p>\n<p>F tensorflow/contrib/lite/toco/tooling_util.cc:1633] Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.</p>\n<p>I am not able to understand why isn't Min/Max information stored for dense_prev operator if for dense_typed it's not giving any error.</p>\n<p>The code to reproduce the problem</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.lite.toco import types_pb2 as _types_pb2\nQUANTIZED_UINT8 = _types_pb2.QUANTIZED_UINT8\nimport numpy as np\n\ndef train_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\n        label = tf.placeholder(tf.float32, shape=(vocab_output_size), name='labels')\n\n        data_x_typed = data_x[:vocab_typed_size]\n        data_x_prev = data_x[vocab_typed_size:]\n        \n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\n\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\n\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\n\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\n\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\n\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\n\n        # Loss function and optimizer\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_message, labels=label), name=\"loss\")\n\n        # Call the training rewrite which rewrites the graph in-place with\n        # FakeQuantization nodes and folds batchnorm for training. It is\n        # often needed to fine tune a floating point model for quantization\n        # with this training tool. When training from scratch, quant_delay\n        # can be used to activate quantization after training to converge\n        # with the float graph, effectively fine-tuning the model.\n\n        tf.contrib.quantize.create_training_graph(quant_delay=0)\n\n        optimizer = tf.train.AdamOptimizer().minimize(loss)\n\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        print \"Dumping model\"\n        saver.save(sess, './test/oink.ckpt')\n\n        writer = tf.summary.FileWriter(\"./test/\", sess.graph)\n        writer.close()\n        \n\ndef eval_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\n        data_x_typed = data_x[:vocab_typed_size]\n        data_x_prev = data_x[vocab_typed_size:]\n\n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\n\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\n        \n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\n\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\n\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\n\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\n\n        tf.contrib.quantize.create_eval_graph()\n\n        saver = tf.train.Saver()\n        saver.restore(sess, \"./test/oink.ckpt\")\n\n        converter = tf.contrib.lite.TocoConverter.from_session(sess, [data_x], [probs_message])\n        converter.quantize_weights = True\n        converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\n        input_arrays = converter.get_input_arrays()\n        print (input_arrays)\n        converter.quantized_input_stats = {input_arrays[0] : (0, 2)}\n        #converter.default_ranges_stats = (-1.0,1.0)\n        converter.change_concat_input_ranges = True\n        tflite_model = converter.convert()\n        open(\"sr.tflite\", \"wb\").write(tflite_model)\n\nif __name__ == \"__main__\":  \n    train_model(50000, 50000, 100, 10000)\n    eval_model(50000, 50000, 100, 10000)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.11.0-rc1\nPython version: 2.7\n\nDescribe the problem\nI am trying to save Quantized version of graph in tflite format but getting the following error\nF tensorflow/contrib/lite/toco/tooling_util.cc:1633] Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\nI am not able to understand why isn't Min/Max information stored for dense_prev operator if for dense_typed it's not giving any error.\nThe code to reproduce the problem\nimport tensorflow as tf\nfrom tensorflow.contrib.lite.toco import types_pb2 as _types_pb2\nQUANTIZED_UINT8 = _types_pb2.QUANTIZED_UINT8\nimport numpy as np\n\ndef train_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\n        label = tf.placeholder(tf.float32, shape=(vocab_output_size), name='labels')\n\n        data_x_typed = data_x[:vocab_typed_size]\n        data_x_prev = data_x[vocab_typed_size:]\n        \n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\n\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\n\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\n\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\n\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\n\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\n\n        # Loss function and optimizer\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_message, labels=label), name=\"loss\")\n\n        # Call the training rewrite which rewrites the graph in-place with\n        # FakeQuantization nodes and folds batchnorm for training. It is\n        # often needed to fine tune a floating point model for quantization\n        # with this training tool. When training from scratch, quant_delay\n        # can be used to activate quantization after training to converge\n        # with the float graph, effectively fine-tuning the model.\n\n        tf.contrib.quantize.create_training_graph(quant_delay=0)\n\n        optimizer = tf.train.AdamOptimizer().minimize(loss)\n\n        sess.run(tf.global_variables_initializer())\n        saver = tf.train.Saver()\n        print \"Dumping model\"\n        saver.save(sess, './test/oink.ckpt')\n\n        writer = tf.summary.FileWriter(\"./test/\", sess.graph)\n        writer.close()\n        \n\ndef eval_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\n    g = tf.Graph()\n    with tf.Session(graph=g) as sess:\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\n        data_x_typed = data_x[:vocab_typed_size]\n        data_x_prev = data_x[vocab_typed_size:]\n\n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\n\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\n        \n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\n\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\n\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\n\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\n\n        tf.contrib.quantize.create_eval_graph()\n\n        saver = tf.train.Saver()\n        saver.restore(sess, \"./test/oink.ckpt\")\n\n        converter = tf.contrib.lite.TocoConverter.from_session(sess, [data_x], [probs_message])\n        converter.quantize_weights = True\n        converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\n        input_arrays = converter.get_input_arrays()\n        print (input_arrays)\n        converter.quantized_input_stats = {input_arrays[0] : (0, 2)}\n        #converter.default_ranges_stats = (-1.0,1.0)\n        converter.change_concat_input_ranges = True\n        tflite_model = converter.convert()\n        open(\"sr.tflite\", \"wb\").write(tflite_model)\n\nif __name__ == \"__main__\":  \n    train_model(50000, 50000, 100, 10000)\n    eval_model(50000, 50000, 100, 10000)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.11.0-rc1\r\n- **Python version**: 2.7\r\n\r\n### Describe the problem\r\nI am trying to save Quantized version of graph in tflite format but getting the following error\r\n\r\nF tensorflow/contrib/lite/toco/tooling_util.cc:1633] Array dense_prev, which is an input to the Concatenation operator producing the output array dense_input, is lacking min/max data, which is necessary for quantization. If accuracy matters, either target a non-quantized output format, or run quantized training with your model from a floating point checkpoint to change the input graph to contain min/max information. If you don't care about accuracy, you can pass --default_ranges_min= and --default_ranges_max= for easy experimentation.\r\n\r\nI am not able to understand why isn't Min/Max information stored for dense_prev operator if for dense_typed it's not giving any error.  \r\n\r\nThe code to reproduce the problem  \r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.lite.toco import types_pb2 as _types_pb2\r\nQUANTIZED_UINT8 = _types_pb2.QUANTIZED_UINT8\r\nimport numpy as np\r\n\r\ndef train_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\r\n    g = tf.Graph()\r\n    with tf.Session(graph=g) as sess:\r\n\r\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\r\n        label = tf.placeholder(tf.float32, shape=(vocab_output_size), name='labels')\r\n\r\n        data_x_typed = data_x[:vocab_typed_size]\r\n        data_x_prev = data_x[vocab_typed_size:]\r\n        \r\n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\r\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\r\n\r\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\r\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\r\n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\r\n\r\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\r\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\r\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\r\n\r\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \r\n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\r\n\r\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\r\n\r\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\r\n\r\n        # Loss function and optimizer\r\n        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_message, labels=label), name=\"loss\")\r\n\r\n        # Call the training rewrite which rewrites the graph in-place with\r\n        # FakeQuantization nodes and folds batchnorm for training. It is\r\n        # often needed to fine tune a floating point model for quantization\r\n        # with this training tool. When training from scratch, quant_delay\r\n        # can be used to activate quantization after training to converge\r\n        # with the float graph, effectively fine-tuning the model.\r\n\r\n        tf.contrib.quantize.create_training_graph(quant_delay=0)\r\n\r\n        optimizer = tf.train.AdamOptimizer().minimize(loss)\r\n\r\n        sess.run(tf.global_variables_initializer())\r\n        saver = tf.train.Saver()\r\n        print \"Dumping model\"\r\n        saver.save(sess, './test/oink.ckpt')\r\n\r\n        writer = tf.summary.FileWriter(\"./test/\", sess.graph)\r\n        writer.close()\r\n        \r\n\r\ndef eval_model(vocab_typed_size,vocab_prev_size, decoder_size, vocab_output_size):\r\n    g = tf.Graph()\r\n    with tf.Session(graph=g) as sess:\r\n        data_x = tf.placeholder(tf.float32, shape=(vocab_typed_size + vocab_prev_size), name=\"data_x\" )\r\n        data_x_typed = data_x[:vocab_typed_size]\r\n        data_x_prev = data_x[vocab_typed_size:]\r\n\r\n        embedding_typed = tf.get_variable(name=\"embedding_typed\",shape=(vocab_typed_size, 100))\r\n        embedding_prev = tf.get_variable(name=\"embedding_prev\",shape=(vocab_prev_size, 100))\r\n\r\n        dense_typed = tf.matmul(tf.reshape(data_x_typed, (1,vocab_typed_size)),embedding_typed, name=\"dense_typed\")\r\n        dense_prev = tf.matmul(tf.reshape(data_x_prev, (1,vocab_prev_size)),embedding_prev,name=\"dense_prev\")\r\n        \r\n        dense_input = tf.concat([dense_prev,dense_typed], axis = 1,name=\"dense_input\")\r\n\r\n        h2_w = tf.get_variable(shape=(200 , decoder_size),name=\"h2_w\")\r\n        h2_b = tf.get_variable(shape=(1,decoder_size),name=\"h2_b\")\r\n        h2_out = tf.add(tf.matmul(dense_input, h2_w), h2_b, name =\"h2_out\")\r\n\r\n        decoder_w = tf.get_variable(shape=[decoder_size,vocab_output_size],name=\"decoder_w\")     \r\n        decoder_b = tf.get_variable(shape=[1,vocab_output_size],name=\"decoder_b\")\r\n\r\n        logits_message = tf.add(tf.matmul(tf.nn.relu(h2_out), decoder_w),decoder_b, name='logits_message')\r\n        probs_message = tf.nn.sigmoid(logits_message, name = 'probs_sticker')\r\n\r\n        tf.contrib.quantize.create_eval_graph()\r\n\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, \"./test/oink.ckpt\")\r\n\r\n        converter = tf.contrib.lite.TocoConverter.from_session(sess, [data_x], [probs_message])\r\n        converter.quantize_weights = True\r\n        converter.inference_type = tf.contrib.lite.constants.QUANTIZED_UINT8\r\n        input_arrays = converter.get_input_arrays()\r\n        print (input_arrays)\r\n        converter.quantized_input_stats = {input_arrays[0] : (0, 2)}\r\n        #converter.default_ranges_stats = (-1.0,1.0)\r\n        converter.change_concat_input_ranges = True\r\n        tflite_model = converter.convert()\r\n        open(\"sr.tflite\", \"wb\").write(tflite_model)\r\n\r\nif __name__ == \"__main__\":  \r\n    train_model(50000, 50000, 100, 10000)\r\n    eval_model(50000, 50000, 100, 10000)\r\n ```\r\n"}