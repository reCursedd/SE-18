{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260675873", "html_url": "https://github.com/tensorflow/tensorflow/issues/5562#issuecomment-260675873", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5562", "id": 260675873, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDY3NTg3Mw==", "user": {"login": "hholst80", "id": 6200749, "node_id": "MDQ6VXNlcjYyMDA3NDk=", "avatar_url": "https://avatars0.githubusercontent.com/u/6200749?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hholst80", "html_url": "https://github.com/hholst80", "followers_url": "https://api.github.com/users/hholst80/followers", "following_url": "https://api.github.com/users/hholst80/following{/other_user}", "gists_url": "https://api.github.com/users/hholst80/gists{/gist_id}", "starred_url": "https://api.github.com/users/hholst80/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hholst80/subscriptions", "organizations_url": "https://api.github.com/users/hholst80/orgs", "repos_url": "https://api.github.com/users/hholst80/repos", "events_url": "https://api.github.com/users/hholst80/events{/privacy}", "received_events_url": "https://api.github.com/users/hholst80/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-15T15:41:19Z", "updated_at": "2016-11-15T15:42:50Z", "author_association": "CONTRIBUTOR", "body_html": "<blockquote>\n<p>This gives nan. The gradient of sqrt(x) with respect to x is inf at 0 which is likely causing the problem.</p>\n</blockquote>\n<p>I think your second example is just a more complicated instance of the problem you are describing in your first example, cf. the sqrt here <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.global_norm.md\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.global_norm.md</a></p>\n<p>One way to attack this problem is that you explicitly define the desired behavior in the limit case. I think you would have to resort to defining your operator own gradient for that op because <code>tf.select</code> are not to be trusted. At least I have had bad experiences with trying to do that. Now I often I tend to use <code>tf.clip_by_value</code> to handle these problems. Ugly, but often does the trick.</p>\n<p>Side note: We can understand why we are getting a NaN in the gradient backprop computation by applying the chain rule:</p>\n<blockquote>\n<h3>Define</h3>\n<p>loss(x) = sqr(sqrt(x))<br>\nloss(y) = sqr(y)<br>\ny(x) = sqrt(x)<br>\nd{loss}/d{x} = d{loss}/d{y} * d{y}/d{x} = 2 y (1/2) 1/sqrt(x).</p>\n<h3>Plug x=0, y(0)=0 into the expression above,</h3>\n<p>2 (0) (1/2) 1/sqrt(0) = 0 (-Inf) = NaN.</p>\n<h3>we could symbolically, for x&gt;0, simplify this to</h3>\n<p>sqrt(x) 1/sqrt(x) = 1</p>\n<h3>and extend that definition to x=0, why not</h3>\n</blockquote>", "body_text": "This gives nan. The gradient of sqrt(x) with respect to x is inf at 0 which is likely causing the problem.\n\nI think your second example is just a more complicated instance of the problem you are describing in your first example, cf. the sqrt here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.global_norm.md\nOne way to attack this problem is that you explicitly define the desired behavior in the limit case. I think you would have to resort to defining your operator own gradient for that op because tf.select are not to be trusted. At least I have had bad experiences with trying to do that. Now I often I tend to use tf.clip_by_value to handle these problems. Ugly, but often does the trick.\nSide note: We can understand why we are getting a NaN in the gradient backprop computation by applying the chain rule:\n\nDefine\nloss(x) = sqr(sqrt(x))\nloss(y) = sqr(y)\ny(x) = sqrt(x)\nd{loss}/d{x} = d{loss}/d{y} * d{y}/d{x} = 2 y (1/2) 1/sqrt(x).\nPlug x=0, y(0)=0 into the expression above,\n2 (0) (1/2) 1/sqrt(0) = 0 (-Inf) = NaN.\nwe could symbolically, for x>0, simplify this to\nsqrt(x) 1/sqrt(x) = 1\nand extend that definition to x=0, why not", "body": "> This gives nan. The gradient of sqrt(x) with respect to x is inf at 0 which is likely causing the problem.\n\nI think your second example is just a more complicated instance of the problem you are describing in your first example, cf. the sqrt here https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard0/tf.global_norm.md\n\nOne way to attack this problem is that you explicitly define the desired behavior in the limit case. I think you would have to resort to defining your operator own gradient for that op because `tf.select` are not to be trusted. At least I have had bad experiences with trying to do that. Now I often I tend to use `tf.clip_by_value` to handle these problems. Ugly, but often does the trick.\n\nSide note: We can understand why we are getting a NaN in the gradient backprop computation by applying the chain rule:\n\n> ### Define\n> \n> loss(x) = sqr(sqrt(x))\n> loss(y) = sqr(y)\n> y(x) = sqrt(x)\n> d{loss}/d{x} = d{loss}/d{y} \\* d{y}/d{x} = 2 y (1/2) 1/sqrt(x).\n> \n> ### Plug x=0, y(0)=0 into the expression above,\n> \n> 2 (0) (1/2) 1/sqrt(0) = 0 (-Inf) = NaN.\n> \n> ### we could symbolically, for x>0, simplify this to\n> \n> sqrt(x) 1/sqrt(x) = 1\n> \n> ### and extend that definition to x=0, why not\n"}