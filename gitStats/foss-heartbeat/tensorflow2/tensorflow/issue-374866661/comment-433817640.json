{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433817640", "html_url": "https://github.com/tensorflow/tensorflow/issues/23340#issuecomment-433817640", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23340", "id": 433817640, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzgxNzY0MA==", "user": {"login": "PINTO0309", "id": 33194443, "node_id": "MDQ6VXNlcjMzMTk0NDQz", "avatar_url": "https://avatars3.githubusercontent.com/u/33194443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PINTO0309", "html_url": "https://github.com/PINTO0309", "followers_url": "https://api.github.com/users/PINTO0309/followers", "following_url": "https://api.github.com/users/PINTO0309/following{/other_user}", "gists_url": "https://api.github.com/users/PINTO0309/gists{/gist_id}", "starred_url": "https://api.github.com/users/PINTO0309/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PINTO0309/subscriptions", "organizations_url": "https://api.github.com/users/PINTO0309/orgs", "repos_url": "https://api.github.com/users/PINTO0309/repos", "events_url": "https://api.github.com/users/PINTO0309/events{/privacy}", "received_events_url": "https://api.github.com/users/PINTO0309/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T07:54:25Z", "updated_at": "2018-10-29T08:13:40Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=21351944\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/seongkyun\">@seongkyun</a></p>\n<p>Notable errors are <strong>\"Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/ MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which If you do not care about accuracy. If you do not care for accuracy. If you do not care about your accuracy , you can pass --default_ranges_min = and --default_ranges_max = for easy experimentation. \"</strong></p>\n<p>According to the official tutorial,<br>\n<strong><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/cmdline_reference.md#transformation-flags\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/cmdline_reference.md#transformation-flags</a></strong></p>\n<p>Although not stated explicitly, I think that it is necessary to specify it when quantizing.<br>\nIn the case of <strong><code>--inference_input_type QUANTIZED_UINT8</code> + <code>Relu6</code></strong></p>\n<p>==============================================================================</p>\n<p><strong><code>--default_ranges_min</code></strong>, <strong><code>--default_ranges_max</code></strong>. Type: floating-point. Default value for the (min, max) range values used for all arrays without a specified range. Allows user to proceed with quantization of non-quantized or incorrectly-quantized input files. These flags produce models with low accuracy. They are intended for easy experimentation with quantization via \"dummy quantization\".</p>\n<p>==============================================================================</p>\n<p>By the way, the quantization script example of UNet model I created is as follows.<br>\nThere seems to be many patterns.<br>\nMy \"Toco\" is self-built. (Tensorflow v1.11.0)</p>\n<pre><code>sudo bazel-bin/tensorflow/contrib/lite/toco/toco \\\n--input_file=semanticsegmentation_frozen_person_32.pb  \\\n--input_format=TENSORFLOW_GRAPHDEF \\\n--output_format=TFLITE \\\n--output_file=output/semanticsegmentation_frozen_person_quantized_32.tflite \\\n--input_shapes=1,128,128,3 \\\n--inference_type=FLOAT \\\n--input_type=FLOAT \\\n--input_arrays=input \\\n--output_arrays=output/BiasAdd \\\n--post_training_quantize\n</code></pre>", "body_text": "@seongkyun\nNotable errors are \"Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/ MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which If you do not care about accuracy. If you do not care for accuracy. If you do not care about your accuracy , you can pass --default_ranges_min = and --default_ranges_max = for easy experimentation. \"\nAccording to the official tutorial,\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/cmdline_reference.md#transformation-flags\nAlthough not stated explicitly, I think that it is necessary to specify it when quantizing.\nIn the case of --inference_input_type QUANTIZED_UINT8 + Relu6\n==============================================================================\n--default_ranges_min, --default_ranges_max. Type: floating-point. Default value for the (min, max) range values used for all arrays without a specified range. Allows user to proceed with quantization of non-quantized or incorrectly-quantized input files. These flags produce models with low accuracy. They are intended for easy experimentation with quantization via \"dummy quantization\".\n==============================================================================\nBy the way, the quantization script example of UNet model I created is as follows.\nThere seems to be many patterns.\nMy \"Toco\" is self-built. (Tensorflow v1.11.0)\nsudo bazel-bin/tensorflow/contrib/lite/toco/toco \\\n--input_file=semanticsegmentation_frozen_person_32.pb  \\\n--input_format=TENSORFLOW_GRAPHDEF \\\n--output_format=TFLITE \\\n--output_file=output/semanticsegmentation_frozen_person_quantized_32.tflite \\\n--input_shapes=1,128,128,3 \\\n--inference_type=FLOAT \\\n--input_type=FLOAT \\\n--input_arrays=input \\\n--output_arrays=output/BiasAdd \\\n--post_training_quantize", "body": "@seongkyun \r\n\r\nNotable errors are **\"Array FeatureExtractor/MobilenetV1/MobilenetV1/Conv2d_0/Relu6, which is an input to the DepthwiseConv operator producing the output array FeatureExtractor/MobilenetV1/ MobilenetV1/Conv2d_1_depthwise/Relu6, is lacking min/max data, which If you do not care about accuracy. If you do not care for accuracy. If you do not care about your accuracy , you can pass --default_ranges_min = and --default_ranges_max = for easy experimentation. \"**\r\n\r\nAccording to the official tutorial,\r\n**https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/convert/cmdline_reference.md#transformation-flags**\r\n\r\nAlthough not stated explicitly, I think that it is necessary to specify it when quantizing.\r\nIn the case of **`--inference_input_type QUANTIZED_UINT8` + `Relu6`**\r\n\r\n==============================================================================\r\n\r\n**`--default_ranges_min`**, **`--default_ranges_max`**. Type: floating-point. Default value for the (min, max) range values used for all arrays without a specified range. Allows user to proceed with quantization of non-quantized or incorrectly-quantized input files. These flags produce models with low accuracy. They are intended for easy experimentation with quantization via \"dummy quantization\".\r\n\r\n==============================================================================\r\n\r\nBy the way, the quantization script example of UNet model I created is as follows.\r\nThere seems to be many patterns.\r\nMy \"Toco\" is self-built. (Tensorflow v1.11.0)\r\n```\r\nsudo bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n--input_file=semanticsegmentation_frozen_person_32.pb  \\\r\n--input_format=TENSORFLOW_GRAPHDEF \\\r\n--output_format=TFLITE \\\r\n--output_file=output/semanticsegmentation_frozen_person_quantized_32.tflite \\\r\n--input_shapes=1,128,128,3 \\\r\n--inference_type=FLOAT \\\r\n--input_type=FLOAT \\\r\n--input_arrays=input \\\r\n--output_arrays=output/BiasAdd \\\r\n--post_training_quantize\r\n```"}