{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413102548", "html_url": "https://github.com/tensorflow/tensorflow/issues/20833#issuecomment-413102548", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20833", "id": 413102548, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzEwMjU0OA==", "user": {"login": "ashahba", "id": 12436063, "node_id": "MDQ6VXNlcjEyNDM2MDYz", "avatar_url": "https://avatars3.githubusercontent.com/u/12436063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashahba", "html_url": "https://github.com/ashahba", "followers_url": "https://api.github.com/users/ashahba/followers", "following_url": "https://api.github.com/users/ashahba/following{/other_user}", "gists_url": "https://api.github.com/users/ashahba/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashahba/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashahba/subscriptions", "organizations_url": "https://api.github.com/users/ashahba/orgs", "repos_url": "https://api.github.com/users/ashahba/repos", "events_url": "https://api.github.com/users/ashahba/events{/privacy}", "received_events_url": "https://api.github.com/users/ashahba/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-15T06:04:04Z", "updated_at": "2018-08-15T06:04:04Z", "author_association": "NONE", "body_html": "<p>Hi, I'm also experiencing this behavior.<br>\nI'm using <code>TensorFlow</code> version <code>1.9.0</code> and I'm running a <code>distributed mnist</code> job using <a href=\"https://github.com/kubeflow/tf-operator\">tfoperator</a> and <a href=\"https://github.com/IntelAI/mlt\">mlt</a> on a <code>GKE</code> cluster.</p>\n<p>After my <a href=\"https://github.com/IntelAI/mlt/blob/master/mlt-templates/tf-dist-mnist/main.py\">main.py</a> finishes near line 221, I get:</p>\n<pre><code>[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 254 of 256: loss = 1.478 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 254 of 256: loss = 1.481 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 255 of 256: loss = 1.499 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 255 of 256: loss = 1.47 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Finished on task 1 in 98.08173823356628 seconds \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Session from worker 1 closed cleanly \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\n</code></pre>\n<p>and for that reason my <code>Pods</code> remain running indefinitely and the TfJob is never marked as <code>Succeeded</code>.</p>\n<p>Have I written custom code: No<br>\nOS Platform and Distribution: Ubuntu 16.04<br>\nTensorFlow installed from: <a href=\"https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/\" rel=\"nofollow\">https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/</a><br>\nTensorFlow version: 1.9.0<br>\nBazel version: N/A<br>\nCUDA/cuDNN version: N/A<br>\nGPU model and memory: N/A<br>\nExact command to reproduce:</p>\n<pre><code>mlt init my-tf-dist --template=tf-dist-mnist --template-repo=.\nmlt deploy -l\n</code></pre>\n<p>first command instantiates a distributed <code>TensorFlow mnist</code> template to be used for deploying as a TfJob on a <code>GKE</code> cluster and then the second command deploys the job and uses <a href=\"https://github.com/johanhaleby/kubetail\">kubetail</a> to tail the aggregated logs for all the Pods.</p>\n<p>This is currently blocking one of our PRs on <code>mlt</code> here: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"348989930\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/IntelAI/mlt/issues/387\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/IntelAI/mlt/pull/387/hovercard\" href=\"https://github.com/IntelAI/mlt/pull/387\">IntelAI/mlt#387</a></p>", "body_text": "Hi, I'm also experiencing this behavior.\nI'm using TensorFlow version 1.9.0 and I'm running a distributed mnist job using tfoperator and mlt on a GKE cluster.\nAfter my main.py finishes near line 221, I get:\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 254 of 256: loss = 1.478 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 254 of 256: loss = 1.481 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 255 of 256: loss = 1.499 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 255 of 256: loss = 1.47 \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Finished on task 1 in 98.08173823356628 seconds \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Session from worker 1 closed cleanly \n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\n\nand for that reason my Pods remain running indefinitely and the TfJob is never marked as Succeeded.\nHave I written custom code: No\nOS Platform and Distribution: Ubuntu 16.04\nTensorFlow installed from: https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/\nTensorFlow version: 1.9.0\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:\nmlt init my-tf-dist --template=tf-dist-mnist --template-repo=.\nmlt deploy -l\n\nfirst command instantiates a distributed TensorFlow mnist template to be used for deploying as a TfJob on a GKE cluster and then the second command deploys the job and uses kubetail to tail the aggregated logs for all the Pods.\nThis is currently blocking one of our PRs on mlt here: IntelAI/mlt#387", "body": "Hi, I'm also experiencing this behavior.\r\nI'm using `TensorFlow` version `1.9.0` and I'm running a `distributed mnist` job using [tfoperator](https://github.com/kubeflow/tf-operator) and [mlt](https://github.com/IntelAI/mlt) on a `GKE` cluster.\r\n\r\nAfter my [main.py](https://github.com/IntelAI/mlt/blob/master/mlt-templates/tf-dist-mnist/main.py) finishes near line 221, I get:\r\n```\r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 254 of 256: loss = 1.478 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 254 of 256: loss = 1.481 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:root:worker 0, step 255 of 256: loss = 1.499 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:worker 1, step 255 of 256: loss = 1.47 \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Finished on task 1 in 98.08173823356628 seconds \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-1-bmbvb] INFO:root:Session from worker 1 closed cleanly \r\n[my-tf-dist-a5dfa2c6-a28b-4f86-8b4c-251ee-worker-6mdt-0-zlea4] INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\n```\r\nand for that reason my `Pods` remain running indefinitely and the TfJob is never marked as `Succeeded`.\r\n\r\nHave I written custom code: No\r\nOS Platform and Distribution: Ubuntu 16.04\r\nTensorFlow installed from: https://hub.docker.com/r/intelaipg/intel-optimized-tensorflow/\r\nTensorFlow version: 1.9.0\r\nBazel version: N/A\r\nCUDA/cuDNN version: N/A\r\nGPU model and memory: N/A\r\nExact command to reproduce: \r\n```\r\nmlt init my-tf-dist --template=tf-dist-mnist --template-repo=.\r\nmlt deploy -l\r\n```\r\nfirst command instantiates a distributed `TensorFlow mnist` template to be used for deploying as a TfJob on a `GKE` cluster and then the second command deploys the job and uses [kubetail](https://github.com/johanhaleby/kubetail) to tail the aggregated logs for all the Pods.\r\n\r\nThis is currently blocking one of our PRs on `mlt` here: https://github.com/IntelAI/mlt/pull/387"}