{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20833", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20833/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20833/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20833/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20833", "id": 341443721, "node_id": "MDU6SXNzdWUzNDE0NDM3MjE=", "number": 20833, "title": "Coordinator stopped with threads still running", "user": {"login": "dcmrlee", "id": 11792282, "node_id": "MDQ6VXNlcjExNzkyMjgy", "avatar_url": "https://avatars0.githubusercontent.com/u/11792282?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dcmrlee", "html_url": "https://github.com/dcmrlee", "followers_url": "https://api.github.com/users/dcmrlee/followers", "following_url": "https://api.github.com/users/dcmrlee/following{/other_user}", "gists_url": "https://api.github.com/users/dcmrlee/gists{/gist_id}", "starred_url": "https://api.github.com/users/dcmrlee/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dcmrlee/subscriptions", "organizations_url": "https://api.github.com/users/dcmrlee/orgs", "repos_url": "https://api.github.com/users/dcmrlee/repos", "events_url": "https://api.github.com/users/dcmrlee/events{/privacy}", "received_events_url": "https://api.github.com/users/dcmrlee/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-07-16T09:19:01Z", "updated_at": "2018-10-24T11:45:54Z", "closed_at": "2018-09-28T20:44:28Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:  Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: NV-P40</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Running distributed tensorflow using estimator in sync mode, there is always a exception after the last training step, as followed:</p>\n<pre><code>INFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\n    coord.request_stop(e)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\n    six.reraise(*sys.exc_info())\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n    enqueue_callable()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\n</code></pre>\n<p>It seems there is a closing problem when using tf.train.SyncReplicasOptimizer.<br>\nAny one know how to fix this?</p>\n<h3>Source code / logs</h3>\n<p>Main Code Fragment(model_fn) as followed:</p>\n<pre><code>with tf.device('/job:worker/task:%d' % task_index):\n    with slim.arg_scope([slim.variables.variable, slim.variables.global_step],\n                 device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n             global_step = slim.variables.global_step()\n             # Calculate the learning rate schedule.\n             num_batches_per_epoch = (num_examples_per_epoch / FLAGS.batch_size)\n             decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\n\n             # Decay the learning rate exponentially based on the number of steps.\n             lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                             global_step,\n                                             decay_steps,\n                                             FLAGS.learning_rate_decay_factor,\n                                             staircase=True)\n             opt = tf.train.RMSPropOptimizer(learning_rate=lr,\n                                                 decay=RMSPROP_DECAY,\n                                                 momentum=RMSPROP_MOMENTUM,\n                                                 epsilon=RMSPROP_EPSILON)\n            # forward\n             logits = inception.inference(features, num_classes, for_training=True)\n\n             # loss\n             inception.loss(logits, labels)\n             losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n             losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n             total_loss = tf.add_n(losses, name='total_loss')\n\n              # Create synchronous replica optimizer.\n             opt = tf.train.SyncReplicasOptimizer(\n                     opt,\n                     replicas_to_aggregate=num_replicas_to_aggregate,\n                     total_num_replicas=num_workers)\n\n             # Compute gradients with respect to the loss.\n             grads = opt.compute_gradients(total_loss)\n             apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n             sync_replicas_hook = opt.make_session_run_hook(is_chief, num_tokens=num_workers)\n             \n              return tf.estimator.EstimatorSpec(\n                 mode=mode,\n                 loss=total_loss,\n                 train_op=train_op,\n                 training_chief_hooks=[],\n                 training_hooks=[sync_replicas_hook])\n</code></pre>\n<hr>\n<p>And some  logs:</p>\n<pre><code>INFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:tokens_needed = 0\nINFO:tensorflow:Graph was finalized.\n2018-07-16 09:13:43.014523: I tensorflow/core/distributed_runtime/master_session.cc:1142] Start master session a13a7f78ee358c84 with config: allow_soft_placement: true\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:loss = 13.082163, step = 0\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.082163\nINFO:tensorflow:loss = 13.115122 (15.557 sec)\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.103788 (7.314 sec)\nINFO:tensorflow:global_step/sec: 0.043722\nINFO:tensorflow:Saving checkpoints for 3 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.137239 (6.563 sec)\nINFO:tensorflow:global_step/sec: 0.152378\nINFO:tensorflow:Saving checkpoints for 4 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.200961 (6.763 sec)\nINFO:tensorflow:global_step/sec: 0.147865\nINFO:tensorflow:Saving checkpoints for 5 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.265466 (6.318 sec)\nINFO:tensorflow:global_step/sec: 0.158291\nINFO:tensorflow:Saving checkpoints for 6 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.165977 (6.270 sec)\nINFO:tensorflow:global_step/sec: 0.159476\nINFO:tensorflow:Saving checkpoints for 7 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.478054 (7.017 sec)\nINFO:tensorflow:global_step/sec: 0.14252\nINFO:tensorflow:Saving checkpoints for 8 into /tmp/aves/train_dir/model.ckpt.\nWARNING:tensorflow:Ignoring: /tmp/aves/train_dir/model.ckpt-3.meta; No such file or directory\nINFO:tensorflow:loss = 13.231509 (6.544 sec)\nINFO:tensorflow:global_step/sec: 0.152785\nINFO:tensorflow:Saving checkpoints for 9 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.367797 (6.686 sec)\nINFO:tensorflow:global_step/sec: 0.149593\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.461123 (6.659 sec)\nINFO:tensorflow:global_step/sec: 0.150159\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\n    coord.request_stop(e)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\n    six.reraise(*sys.exc_info())\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n    enqueue_callable()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\n\nINFO:tensorflow:Loss for final step: 13.461123.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):  Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8\nPython version: 3.5\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 8.0\nGPU model and memory: NV-P40\nExact command to reproduce: N/A\n\nDescribe the problem\nRunning distributed tensorflow using estimator in sync mode, there is always a exception after the last training step, as followed:\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\n    coord.request_stop(e)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\n    six.reraise(*sys.exc_info())\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n    enqueue_callable()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\n\nIt seems there is a closing problem when using tf.train.SyncReplicasOptimizer.\nAny one know how to fix this?\nSource code / logs\nMain Code Fragment(model_fn) as followed:\nwith tf.device('/job:worker/task:%d' % task_index):\n    with slim.arg_scope([slim.variables.variable, slim.variables.global_step],\n                 device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n             global_step = slim.variables.global_step()\n             # Calculate the learning rate schedule.\n             num_batches_per_epoch = (num_examples_per_epoch / FLAGS.batch_size)\n             decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\n\n             # Decay the learning rate exponentially based on the number of steps.\n             lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\n                                             global_step,\n                                             decay_steps,\n                                             FLAGS.learning_rate_decay_factor,\n                                             staircase=True)\n             opt = tf.train.RMSPropOptimizer(learning_rate=lr,\n                                                 decay=RMSPROP_DECAY,\n                                                 momentum=RMSPROP_MOMENTUM,\n                                                 epsilon=RMSPROP_EPSILON)\n            # forward\n             logits = inception.inference(features, num_classes, for_training=True)\n\n             # loss\n             inception.loss(logits, labels)\n             losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\n             losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n\n             total_loss = tf.add_n(losses, name='total_loss')\n\n              # Create synchronous replica optimizer.\n             opt = tf.train.SyncReplicasOptimizer(\n                     opt,\n                     replicas_to_aggregate=num_replicas_to_aggregate,\n                     total_num_replicas=num_workers)\n\n             # Compute gradients with respect to the loss.\n             grads = opt.compute_gradients(total_loss)\n             apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\n             sync_replicas_hook = opt.make_session_run_hook(is_chief, num_tokens=num_workers)\n             \n              return tf.estimator.EstimatorSpec(\n                 mode=mode,\n                 loss=total_loss,\n                 train_op=train_op,\n                 training_chief_hooks=[],\n                 training_hooks=[sync_replicas_hook])\n\n\nAnd some  logs:\nINFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Done calling model_fn.\nINFO:tensorflow:tokens_needed = 0\nINFO:tensorflow:Graph was finalized.\n2018-07-16 09:13:43.014523: I tensorflow/core/distributed_runtime/master_session.cc:1142] Start master session a13a7f78ee358c84 with config: allow_soft_placement: true\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:loss = 13.082163, step = 0\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.082163\nINFO:tensorflow:loss = 13.115122 (15.557 sec)\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.103788 (7.314 sec)\nINFO:tensorflow:global_step/sec: 0.043722\nINFO:tensorflow:Saving checkpoints for 3 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.137239 (6.563 sec)\nINFO:tensorflow:global_step/sec: 0.152378\nINFO:tensorflow:Saving checkpoints for 4 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.200961 (6.763 sec)\nINFO:tensorflow:global_step/sec: 0.147865\nINFO:tensorflow:Saving checkpoints for 5 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.265466 (6.318 sec)\nINFO:tensorflow:global_step/sec: 0.158291\nINFO:tensorflow:Saving checkpoints for 6 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.165977 (6.270 sec)\nINFO:tensorflow:global_step/sec: 0.159476\nINFO:tensorflow:Saving checkpoints for 7 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.478054 (7.017 sec)\nINFO:tensorflow:global_step/sec: 0.14252\nINFO:tensorflow:Saving checkpoints for 8 into /tmp/aves/train_dir/model.ckpt.\nWARNING:tensorflow:Ignoring: /tmp/aves/train_dir/model.ckpt-3.meta; No such file or directory\nINFO:tensorflow:loss = 13.231509 (6.544 sec)\nINFO:tensorflow:global_step/sec: 0.152785\nINFO:tensorflow:Saving checkpoints for 9 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.367797 (6.686 sec)\nINFO:tensorflow:global_step/sec: 0.149593\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/aves/train_dir/model.ckpt.\nINFO:tensorflow:loss = 13.461123 (6.659 sec)\nINFO:tensorflow:global_step/sec: 0.150159\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\nTraceback (most recent call last):\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\n    coord.request_stop(e)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\n    six.reraise(*sys.exc_info())\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\n    raise value\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\n    enqueue_callable()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\n\nINFO:tensorflow:Loss for final step: 13.461123.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:  Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: NV-P40\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nRunning distributed tensorflow using estimator in sync mode, there is always a exception after the last training step, as followed:\r\n```\r\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\r\n``` \r\n\r\nIt seems there is a closing problem when using tf.train.SyncReplicasOptimizer.\r\nAny one know how to fix this?  \r\n\r\n### Source code / logs\r\nMain Code Fragment(model_fn) as followed:\r\n```\r\nwith tf.device('/job:worker/task:%d' % task_index):\r\n    with slim.arg_scope([slim.variables.variable, slim.variables.global_step],\r\n                 device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\r\n             global_step = slim.variables.global_step()\r\n             # Calculate the learning rate schedule.\r\n             num_batches_per_epoch = (num_examples_per_epoch / FLAGS.batch_size)\r\n             decay_steps = int(num_batches_per_epoch * FLAGS.num_epochs_per_decay)\r\n\r\n             # Decay the learning rate exponentially based on the number of steps.\r\n             lr = tf.train.exponential_decay(FLAGS.initial_learning_rate,\r\n                                             global_step,\r\n                                             decay_steps,\r\n                                             FLAGS.learning_rate_decay_factor,\r\n                                             staircase=True)\r\n             opt = tf.train.RMSPropOptimizer(learning_rate=lr,\r\n                                                 decay=RMSPROP_DECAY,\r\n                                                 momentum=RMSPROP_MOMENTUM,\r\n                                                 epsilon=RMSPROP_EPSILON)\r\n            # forward\r\n             logits = inception.inference(features, num_classes, for_training=True)\r\n\r\n             # loss\r\n             inception.loss(logits, labels)\r\n             losses = tf.get_collection(slim.losses.LOSSES_COLLECTION)\r\n             losses += tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n\r\n             total_loss = tf.add_n(losses, name='total_loss')\r\n\r\n              # Create synchronous replica optimizer.\r\n             opt = tf.train.SyncReplicasOptimizer(\r\n                     opt,\r\n                     replicas_to_aggregate=num_replicas_to_aggregate,\r\n                     total_num_replicas=num_workers)\r\n\r\n             # Compute gradients with respect to the loss.\r\n             grads = opt.compute_gradients(total_loss)\r\n             apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)\r\n             sync_replicas_hook = opt.make_session_run_hook(is_chief, num_tokens=num_workers)\r\n             \r\n              return tf.estimator.EstimatorSpec(\r\n                 mode=mode,\r\n                 loss=total_loss,\r\n                 train_op=train_op,\r\n                 training_chief_hooks=[],\r\n                 training_hooks=[sync_replicas_hook])\r\n```\r\n--------------------\r\nAnd some  logs:\r\n```\r\nINFO:tensorflow:SyncReplicasV2: replicas_to_aggregate=2; total_num_replicas=2\r\nINFO:tensorflow:Create CheckpointSaverHook.\r\nINFO:tensorflow:Done calling model_fn.\r\nINFO:tensorflow:tokens_needed = 0\r\nINFO:tensorflow:Graph was finalized.\r\n2018-07-16 09:13:43.014523: I tensorflow/core/distributed_runtime/master_session.cc:1142] Start master session a13a7f78ee358c84 with config: allow_soft_placement: true\r\nINFO:tensorflow:Running local_init_op.\r\nINFO:tensorflow:Done running local_init_op.\r\nINFO:tensorflow:loss = 13.082163, step = 0\r\nINFO:tensorflow:Saving checkpoints for 1 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.082163\r\nINFO:tensorflow:loss = 13.115122 (15.557 sec)\r\nINFO:tensorflow:Saving checkpoints for 2 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.103788 (7.314 sec)\r\nINFO:tensorflow:global_step/sec: 0.043722\r\nINFO:tensorflow:Saving checkpoints for 3 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.137239 (6.563 sec)\r\nINFO:tensorflow:global_step/sec: 0.152378\r\nINFO:tensorflow:Saving checkpoints for 4 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.200961 (6.763 sec)\r\nINFO:tensorflow:global_step/sec: 0.147865\r\nINFO:tensorflow:Saving checkpoints for 5 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.265466 (6.318 sec)\r\nINFO:tensorflow:global_step/sec: 0.158291\r\nINFO:tensorflow:Saving checkpoints for 6 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.165977 (6.270 sec)\r\nINFO:tensorflow:global_step/sec: 0.159476\r\nINFO:tensorflow:Saving checkpoints for 7 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.478054 (7.017 sec)\r\nINFO:tensorflow:global_step/sec: 0.14252\r\nINFO:tensorflow:Saving checkpoints for 8 into /tmp/aves/train_dir/model.ckpt.\r\nWARNING:tensorflow:Ignoring: /tmp/aves/train_dir/model.ckpt-3.meta; No such file or directory\r\nINFO:tensorflow:loss = 13.231509 (6.544 sec)\r\nINFO:tensorflow:global_step/sec: 0.152785\r\nINFO:tensorflow:Saving checkpoints for 9 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.367797 (6.686 sec)\r\nINFO:tensorflow:global_step/sec: 0.149593\r\nINFO:tensorflow:Saving checkpoints for 10 into /tmp/aves/train_dir/model.ckpt.\r\nINFO:tensorflow:loss = 13.461123 (6.659 sec)\r\nINFO:tensorflow:global_step/sec: 0.150159\r\nINFO:tensorflow:Coordinator stopped with threads still running: QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany\r\nException in thread QueueRunnerThread-dummy_queue-sync_token_q_EnqueueMany:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\r\n    self.run()\r\n  File \"/usr/lib/python3.5/threading.py\", line 862, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 268, in _run\r\n    coord.request_stop(e)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/coordinator.py\", line 213, in request_stop\r\n    six.reraise(*sys.exc_info())\r\n  File \"/usr/local/lib/python3.5/dist-packages/six.py\", line 693, in reraise\r\n    raise value\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 252, in _run\r\n    enqueue_callable()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1244, in _single_operation_run\r\n    self._call_tf_sessionrun(None, {}, [], target_list, None)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.CancelledError: Step was cancelled by an explicit call to `Session::Close()`.\r\n\r\nINFO:tensorflow:Loss for final step: 13.461123.\r\n```"}