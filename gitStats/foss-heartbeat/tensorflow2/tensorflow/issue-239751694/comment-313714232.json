{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313714232", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313714232", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313714232, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzcxNDIzMg==", "user": {"login": "MarvinTeichmann", "id": 2729159, "node_id": "MDQ6VXNlcjI3MjkxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/2729159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarvinTeichmann", "html_url": "https://github.com/MarvinTeichmann", "followers_url": "https://api.github.com/users/MarvinTeichmann/followers", "following_url": "https://api.github.com/users/MarvinTeichmann/following{/other_user}", "gists_url": "https://api.github.com/users/MarvinTeichmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarvinTeichmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarvinTeichmann/subscriptions", "organizations_url": "https://api.github.com/users/MarvinTeichmann/orgs", "repos_url": "https://api.github.com/users/MarvinTeichmann/repos", "events_url": "https://api.github.com/users/MarvinTeichmann/events{/privacy}", "received_events_url": "https://api.github.com/users/MarvinTeichmann/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T15:28:35Z", "updated_at": "2017-07-07T15:35:33Z", "author_association": "NONE", "body_html": "<p>Thanks for the very fast reply, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>. I am very happy to learn about existing ways to recompute gradients instead of keeping them. This will allow me to use Tensorflow for my next CRF based project. I will take a closer look at defun and how it might be used for gradient recomputation after I return from my conference. I am very happy about additional suggestions in the meantime.</p>\n<p>Regarding the [feature-request]: In any model it is cheaper in terms of computational time to keep all intermediate results. However it is always cheaper in terms of memory requirement to recompute instead. GPU memory usually is a hard constrained. Having an easy way of doing recomputation in tf is therefore a very useful feature.</p>", "body_text": "Thanks for the very fast reply, @prb12. I am very happy to learn about existing ways to recompute gradients instead of keeping them. This will allow me to use Tensorflow for my next CRF based project. I will take a closer look at defun and how it might be used for gradient recomputation after I return from my conference. I am very happy about additional suggestions in the meantime.\nRegarding the [feature-request]: In any model it is cheaper in terms of computational time to keep all intermediate results. However it is always cheaper in terms of memory requirement to recompute instead. GPU memory usually is a hard constrained. Having an easy way of doing recomputation in tf is therefore a very useful feature.", "body": "Thanks for the very fast reply, @prb12. I am very happy to learn about existing ways to recompute gradients instead of keeping them. This will allow me to use Tensorflow for my next CRF based project. I will take a closer look at defun and how it might be used for gradient recomputation after I return from my conference. I am very happy about additional suggestions in the meantime.\r\n\r\nRegarding the [feature-request]: In any model it is cheaper in terms of computational time to keep all intermediate results. However it is always cheaper in terms of memory requirement to recompute instead. GPU memory usually is a hard constrained. Having an easy way of doing recomputation in tf is therefore a very useful feature."}