{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313171027", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313171027", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313171027, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzE3MTAyNw==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-05T17:26:58Z", "updated_at": "2017-07-05T17:27:29Z", "author_association": "MEMBER", "body_html": "<p>I'm not really the best person to answer this, but yes we are aware of this technique and similar things crop up in various places in TensorFlow:</p>\n<ul>\n<li>XLA has an optimization pass that does selective rematerialization of HLO expressions when necessary to fit in a memory budget <code>tensorflow/tensorflow/compiler/xla/service/hlo_rematerialization.h</code></li>\n<li>Grappler has a rewriting pass that does this: <code>tensorflow/tensorflow/core/grappler/optimizers/memory_optimizer.cc</code></li>\n<li>Not strictly speaking recomputation, but the dynamic RNN classes support a <code>swap_memory</code> flag that temporarily copies intermediate values required for backprop from the GPU back to host memory to allow very long sequence lengths without running out of GPU memory.</li>\n</ul>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a> is the best person to answer Grappler questions, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> for the RNN code, and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1130906\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/eliben\">@eliben</a> or <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8258238\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/meheffernan\">@meheffernan</a> for the XLA work.</p>", "body_text": "I'm not really the best person to answer this, but yes we are aware of this technique and similar things crop up in various places in TensorFlow:\n\nXLA has an optimization pass that does selective rematerialization of HLO expressions when necessary to fit in a memory budget tensorflow/tensorflow/compiler/xla/service/hlo_rematerialization.h\nGrappler has a rewriting pass that does this: tensorflow/tensorflow/core/grappler/optimizers/memory_optimizer.cc\nNot strictly speaking recomputation, but the dynamic RNN classes support a swap_memory flag that temporarily copies intermediate values required for backprop from the GPU back to host memory to allow very long sequence lengths without running out of GPU memory.\n\n@benoitsteiner is the best person to answer Grappler questions, @ebrevdo for the RNN code, and @eliben or @meheffernan for the XLA work.", "body": "I'm not really the best person to answer this, but yes we are aware of this technique and similar things crop up in various places in TensorFlow:\r\n\r\n- XLA has an optimization pass that does selective rematerialization of HLO expressions when necessary to fit in a memory budget `tensorflow/tensorflow/compiler/xla/service/hlo_rematerialization.h`\r\n- Grappler has a rewriting pass that does this: `tensorflow/tensorflow/core/grappler/optimizers/memory_optimizer.cc`\r\n- Not strictly speaking recomputation, but the dynamic RNN classes support a `swap_memory` flag that temporarily copies intermediate values required for backprop from the GPU back to host memory to allow very long sequence lengths without running out of GPU memory.\r\n\r\n@benoitsteiner is the best person to answer Grappler questions, @ebrevdo for the RNN code, and @eliben or @meheffernan for the XLA work.\r\n"}