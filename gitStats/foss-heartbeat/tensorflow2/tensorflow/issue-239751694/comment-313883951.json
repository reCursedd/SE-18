{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313883951", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313883951", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313883951, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzg4Mzk1MQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-08T22:10:35Z", "updated_at": "2017-07-08T22:10:35Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Not sure I fully understand your checkpoints model, Yaroslav.  How is it\ndifferent from just using the Defun every sqrt(n) nodes - and the rest of\nthe time using the standard calls?</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sat, Jul 8, 2017 at 11:01 AM, Yaroslav Bulatov ***@***.***&gt; wrote:\n Getting Chen's performance is too complicated for gradient decorator --\n for a uniform chain of length n, you want to insert \"checkpoints\" every\n sqrt(n) nodes, which doesn't necessarily correspond to number of gradient\n calls. This kind of mirroring is indeed possible by using\n contrib.graph_editor to clone nodes. This enables a node execution order\n which uses less memory, and you can use control dependencies to force\n TensorFlow to use this schedule. In other words, if you have a node like\n \"conv-forward\", you could duplicate it and then disconnect one of them from\n the gradient. Since the original node is not connected to gradient\n computation, its value can be forgotten at the stage of forward prop.\n\n TensorFlow default strategy will execute nodes as soon as possible, which\n is the opposite of what you want to do you in your scenario. Here's a\n utility &lt;<a href=\"https://github.com/yaroslavvb/stuff/tree/master/linearize\">https://github.com/yaroslavvb/stuff/tree/master/linearize</a>&gt; that\n forces TensorFlow to execute nodes as late as possible, breaking ties by\n using estimate of memory usage.\n\n Forgetting intermediate results to fit in memory budget is actually a\n pretty interesting general problem, studied in compiler literature as\n \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can\n also be solved exactly for typical neural networks, since they tend to have\n small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes\n with 2^k states such that resulting \"merged\" graph is a chain)).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"239751694\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11171\" href=\"https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313871515\">#11171 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimz_9lLTEe_QOPxaMVE8n7glMfIWBks5sL8PggaJpZM4OKdxb\">https://github.com/notifications/unsubscribe-auth/ABtimz_9lLTEe_QOPxaMVE8n7glMfIWBks5sL8PggaJpZM4OKdxb</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Not sure I fully understand your checkpoints model, Yaroslav.  How is it\ndifferent from just using the Defun every sqrt(n) nodes - and the rest of\nthe time using the standard calls?\n\u2026\nOn Sat, Jul 8, 2017 at 11:01 AM, Yaroslav Bulatov ***@***.***> wrote:\n Getting Chen's performance is too complicated for gradient decorator --\n for a uniform chain of length n, you want to insert \"checkpoints\" every\n sqrt(n) nodes, which doesn't necessarily correspond to number of gradient\n calls. This kind of mirroring is indeed possible by using\n contrib.graph_editor to clone nodes. This enables a node execution order\n which uses less memory, and you can use control dependencies to force\n TensorFlow to use this schedule. In other words, if you have a node like\n \"conv-forward\", you could duplicate it and then disconnect one of them from\n the gradient. Since the original node is not connected to gradient\n computation, its value can be forgotten at the stage of forward prop.\n\n TensorFlow default strategy will execute nodes as soon as possible, which\n is the opposite of what you want to do you in your scenario. Here's a\n utility <https://github.com/yaroslavvb/stuff/tree/master/linearize> that\n forces TensorFlow to execute nodes as late as possible, breaking ties by\n using estimate of memory usage.\n\n Forgetting intermediate results to fit in memory budget is actually a\n pretty interesting general problem, studied in compiler literature as\n \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can\n also be solved exactly for typical neural networks, since they tend to have\n small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes\n with 2^k states such that resulting \"merged\" graph is a chain)).\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#11171 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimz_9lLTEe_QOPxaMVE8n7glMfIWBks5sL8PggaJpZM4OKdxb>\n .", "body": "Not sure I fully understand your checkpoints model, Yaroslav.  How is it\ndifferent from just using the Defun every sqrt(n) nodes - and the rest of\nthe time using the standard calls?\n\nOn Sat, Jul 8, 2017 at 11:01 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> Getting Chen's performance is too complicated for gradient decorator --\n> for a uniform chain of length n, you want to insert \"checkpoints\" every\n> sqrt(n) nodes, which doesn't necessarily correspond to number of gradient\n> calls. This kind of mirroring is indeed possible by using\n> contrib.graph_editor to clone nodes. This enables a node execution order\n> which uses less memory, and you can use control dependencies to force\n> TensorFlow to use this schedule. In other words, if you have a node like\n> \"conv-forward\", you could duplicate it and then disconnect one of them from\n> the gradient. Since the original node is not connected to gradient\n> computation, its value can be forgotten at the stage of forward prop.\n>\n> TensorFlow default strategy will execute nodes as soon as possible, which\n> is the opposite of what you want to do you in your scenario. Here's a\n> utility <https://github.com/yaroslavvb/stuff/tree/master/linearize> that\n> forces TensorFlow to execute nodes as late as possible, breaking ties by\n> using estimate of memory usage.\n>\n> Forgetting intermediate results to fit in memory budget is actually a\n> pretty interesting general problem, studied in compiler literature as\n> \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can\n> also be solved exactly for typical neural networks, since they tend to have\n> small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes\n> with 2^k states such that resulting \"merged\" graph is a chain)).\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313871515>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz_9lLTEe_QOPxaMVE8n7glMfIWBks5sL8PggaJpZM4OKdxb>\n> .\n>\n"}