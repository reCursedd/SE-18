{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313871515", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313871515", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313871515, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzg3MTUxNQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-08T18:00:21Z", "updated_at": "2017-07-08T18:00:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Getting Chen's performance is too complicated for gradient decorator -- for a uniform chain of length n, you want to insert \"checkpoints\" every sqrt(n) nodes, which doesn't necessarily correspond to number of gradient calls. This kind of mirroring is indeed possible by using contrib.graph_editor to clone nodes. This enables a node execution order which uses less memory, and you can use control dependencies to force TensorFlow to use this schedule. In other words, if you have a node like \"conv-forward\", you could duplicate it and then disconnect one of them from the gradient. Since the original node is not connected to gradient computation, its value can be forgotten at the stage of forward prop.</p>\n<p>TensorFlow default strategy will execute nodes as soon as possible, which is the opposite of what you want to do you in your scenario. Here's a <a href=\"https://github.com/yaroslavvb/stuff/tree/master/linearize\">utility</a> that forces TensorFlow to execute nodes as late as possible, breaking ties by using estimate of memory usage.</p>\n<p>Forgetting intermediate results to fit in memory budget is actually a pretty interesting general problem, studied in compiler literature as \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can also be solved exactly for typical neural networks, since they tend to have small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes with 2^k states such that resulting \"merged\" graph is a chain)).</p>", "body_text": "Getting Chen's performance is too complicated for gradient decorator -- for a uniform chain of length n, you want to insert \"checkpoints\" every sqrt(n) nodes, which doesn't necessarily correspond to number of gradient calls. This kind of mirroring is indeed possible by using contrib.graph_editor to clone nodes. This enables a node execution order which uses less memory, and you can use control dependencies to force TensorFlow to use this schedule. In other words, if you have a node like \"conv-forward\", you could duplicate it and then disconnect one of them from the gradient. Since the original node is not connected to gradient computation, its value can be forgotten at the stage of forward prop.\nTensorFlow default strategy will execute nodes as soon as possible, which is the opposite of what you want to do you in your scenario. Here's a utility that forces TensorFlow to execute nodes as late as possible, breaking ties by using estimate of memory usage.\nForgetting intermediate results to fit in memory budget is actually a pretty interesting general problem, studied in compiler literature as \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can also be solved exactly for typical neural networks, since they tend to have small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes with 2^k states such that resulting \"merged\" graph is a chain)).", "body": "Getting Chen's performance is too complicated for gradient decorator -- for a uniform chain of length n, you want to insert \"checkpoints\" every sqrt(n) nodes, which doesn't necessarily correspond to number of gradient calls. This kind of mirroring is indeed possible by using contrib.graph_editor to clone nodes. This enables a node execution order which uses less memory, and you can use control dependencies to force TensorFlow to use this schedule. In other words, if you have a node like \"conv-forward\", you could duplicate it and then disconnect one of them from the gradient. Since the original node is not connected to gradient computation, its value can be forgotten at the stage of forward prop.\r\n\r\nTensorFlow default strategy will execute nodes as soon as possible, which is the opposite of what you want to do you in your scenario. Here's a [utility](https://github.com/yaroslavvb/stuff/tree/master/linearize) that forces TensorFlow to execute nodes as late as possible, breaking ties by using estimate of memory usage.\r\n\r\nForgetting intermediate results to fit in memory budget is actually a pretty interesting general problem, studied in compiler literature as \"one-shot pebbling\". It can be solved exactly for chains, and I suspect can also be solved exactly for typical neural networks, since they tend to have small treewidth (ie, you can pick size-k subgraphs, and treat them as nodes with 2^k states such that resulting \"merged\" graph is a chain))."}