{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313968463", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313968463", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313968463, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzk2ODQ2Mw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-09T22:02:18Z", "updated_at": "2017-07-09T22:02:18Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">To do fw recomputation in the gradient with a defun you would do something\nlike:\n\ndef fw(x1, x2):  # x1 and x2 are tensors\n  ...\n  return y1, y2  # y1 and y2 are tensors\n\ndef recalc_fw_grad(fw_op, dy1, dy2):\n  x1, x2 = fw_op.inputs\n  # DON'T USE THESE: # y1, y2 = fw_op.outputs\n  y1, y2 = fw(x1, x2)\n  return tf.gradients(ys=[y1, y2], xs=[x1, x2], grad_ys=[dy1, dy2])\n\n<a class=\"user-mention\" href=\"https://github.com/DeFun\">@DeFun</a>(python_grad_func=recalc_fw_grad, shape_func=...)\ndef fw_with_recalc_fw_grad(x1, x2):\n  return fw(x1, x2)</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sun, Jul 9, 2017 at 5:34 AM, Yaroslav Bulatov ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; manually wrapping sqrt(n) nodes\n into defun would do the trick, I just didn't get how the decorator would do\n this.\n\n There's some ambiguity in docs on whether Defun recomputes the values.\n There's noinline=True option to Defun, although when I tried on 1.1, it\n worked without having to set that attribute.\n\n PS, swapping stuff to main memory is not that great for O(n) operations. I\n benchmarked it on TitanX, and recomputing large Mul from inputs on GPU\n was 7x faster than fetching it from main memory. Recomputing Concat was\n 10x faster\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"239751694\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11171\" href=\"https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313917326\">#11171 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimz2q0GkE8o8g-scWStoufZ5lT4V5ks5sMMjpgaJpZM4OKdxb\">https://github.com/notifications/unsubscribe-auth/ABtimz2q0GkE8o8g-scWStoufZ5lT4V5ks5sMMjpgaJpZM4OKdxb</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "To do fw recomputation in the gradient with a defun you would do something\nlike:\n\ndef fw(x1, x2):  # x1 and x2 are tensors\n  ...\n  return y1, y2  # y1 and y2 are tensors\n\ndef recalc_fw_grad(fw_op, dy1, dy2):\n  x1, x2 = fw_op.inputs\n  # DON'T USE THESE: # y1, y2 = fw_op.outputs\n  y1, y2 = fw(x1, x2)\n  return tf.gradients(ys=[y1, y2], xs=[x1, x2], grad_ys=[dy1, dy2])\n\n@DeFun(python_grad_func=recalc_fw_grad, shape_func=...)\ndef fw_with_recalc_fw_grad(x1, x2):\n  return fw(x1, x2)\n\u2026\nOn Sun, Jul 9, 2017 at 5:34 AM, Yaroslav Bulatov ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> manually wrapping sqrt(n) nodes\n into defun would do the trick, I just didn't get how the decorator would do\n this.\n\n There's some ambiguity in docs on whether Defun recomputes the values.\n There's noinline=True option to Defun, although when I tried on 1.1, it\n worked without having to set that attribute.\n\n PS, swapping stuff to main memory is not that great for O(n) operations. I\n benchmarked it on TitanX, and recomputing large Mul from inputs on GPU\n was 7x faster than fetching it from main memory. Recomputing Concat was\n 10x faster\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#11171 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimz2q0GkE8o8g-scWStoufZ5lT4V5ks5sMMjpgaJpZM4OKdxb>\n .", "body": "To do fw recomputation in the gradient with a defun you would do something\nlike:\n\ndef fw(x1, x2):  # x1 and x2 are tensors\n  ...\n  return y1, y2  # y1 and y2 are tensors\n\ndef recalc_fw_grad(fw_op, dy1, dy2):\n  x1, x2 = fw_op.inputs\n  # DON'T USE THESE: # y1, y2 = fw_op.outputs\n  y1, y2 = fw(x1, x2)\n  return tf.gradients(ys=[y1, y2], xs=[x1, x2], grad_ys=[dy1, dy2])\n\n@Defun(python_grad_func=recalc_fw_grad, shape_func=...)\ndef fw_with_recalc_fw_grad(x1, x2):\n  return fw(x1, x2)\n\nOn Sun, Jul 9, 2017 at 5:34 AM, Yaroslav Bulatov <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo> manually wrapping sqrt(n) nodes\n> into defun would do the trick, I just didn't get how the decorator would do\n> this.\n>\n> There's some ambiguity in docs on whether Defun recomputes the values.\n> There's noinline=True option to Defun, although when I tried on 1.1, it\n> worked without having to set that attribute.\n>\n> PS, swapping stuff to main memory is not that great for O(n) operations. I\n> benchmarked it on TitanX, and recomputing large Mul from inputs on GPU\n> was 7x faster than fetching it from main memory. Recomputing Concat was\n> 10x faster\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313917326>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimz2q0GkE8o8g-scWStoufZ5lT4V5ks5sMMjpgaJpZM4OKdxb>\n> .\n>\n"}