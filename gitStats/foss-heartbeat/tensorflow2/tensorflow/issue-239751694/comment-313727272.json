{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313727272", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313727272", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313727272, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzcyNzI3Mg==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T16:18:24Z", "updated_at": "2017-07-07T16:18:24Z", "author_association": "MEMBER", "body_html": "<p>One other way to temporarily override gradient code is to use <code>gradient_override_map</code><br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/python/framework/ops.py#L3773\">tensorflow/tensorflow/python/framework/ops.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 3773\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/2cfbd3347b943a389bb688125c2a90095b7735b5\">2cfbd33</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L3773\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"3773\"></td>\n          <td id=\"LC3773\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-s\">      with g.gradient_override_map({\"Square\": \"CustomSquare\"}):</span> </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<div class=\"highlight highlight-source-python\"><pre> <span class=\"pl-en\">@tf.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CustomSquare<span class=\"pl-pds\">\"</span></span>)\n\u00a0  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_custom_square_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n\u00a0  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ...</span>\n\u00a0  \u00a0\n\u00a0<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> g:\n\u00a0  c <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">5.0</span>)\n\u00a0  s_1 <span class=\"pl-k\">=</span> tf.square(c)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Uses the default gradient for tf.square.</span>\n\u00a0  <span class=\"pl-k\">with</span> g.gradient_override_map({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Square<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CustomSquare<span class=\"pl-pds\">\"</span></span>}):\n\u00a0    s_2 <span class=\"pl-k\">=</span> tf.square(s_2)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Uses _custom_square_grad to compute the gradient of s_2.</span></pre></div>\n<p>Since the custom gradient function is passed the original <code>Operation</code>, it can recompute the op from the same inputs.</p>\n<p>The main reason this doesn't get used for recomputation is that this mechanism is done at an op granularity, whereas most people probably want to do recomputation at a layer granularity.</p>\n<p>The <code>Defun</code> trick is basically the same thing but applied to a larger subgraph.</p>", "body_text": "One other way to temporarily override gradient code is to use gradient_override_map\n\n  \n    \n      tensorflow/tensorflow/python/framework/ops.py\n    \n    \n         Line 3773\n      in\n      2cfbd33\n    \n    \n    \n    \n\n        \n          \n                 with g.gradient_override_map({\"Square\": \"CustomSquare\"}): \n        \n    \n  \n\n\n @tf.RegisterGradient(\"CustomSquare\")\n\u00a0  def _custom_square_grad(op, grad):\n\u00a0  # ...\n\u00a0  \u00a0\n\u00a0with tf.Graph().as_default() as g:\n\u00a0  c = tf.constant(5.0)\n\u00a0  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n\u00a0  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n\u00a0    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\nSince the custom gradient function is passed the original Operation, it can recompute the op from the same inputs.\nThe main reason this doesn't get used for recomputation is that this mechanism is done at an op granularity, whereas most people probably want to do recomputation at a layer granularity.\nThe Defun trick is basically the same thing but applied to a larger subgraph.", "body": "One other way to temporarily override gradient code is to use `gradient_override_map`  \r\nhttps://github.com/tensorflow/tensorflow/blob/2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/python/framework/ops.py#L3773\r\n\r\n```python\r\n @tf.RegisterGradient(\"CustomSquare\")\r\n\u00a0  def _custom_square_grad(op, grad):\r\n\u00a0  # ...\r\n\u00a0  \u00a0\r\n\u00a0with tf.Graph().as_default() as g:\r\n\u00a0  c = tf.constant(5.0)\r\n\u00a0  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\r\n\u00a0  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\r\n\u00a0    s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\r\n```\r\nSince the custom gradient function is passed the original `Operation`, it can recompute the op from the same inputs.\r\n\r\nThe main reason this doesn't get used for recomputation is that this mechanism is done at an op granularity, whereas most people probably want to do recomputation at a layer granularity.\r\n\r\nThe `Defun` trick is basically the same thing but applied to a larger subgraph.    "}