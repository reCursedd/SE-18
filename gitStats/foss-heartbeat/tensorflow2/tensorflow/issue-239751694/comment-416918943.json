{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416918943", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-416918943", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 416918943, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjkxODk0Mw==", "user": {"login": "joeyearsley", "id": 1836025, "node_id": "MDQ6VXNlcjE4MzYwMjU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1836025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joeyearsley", "html_url": "https://github.com/joeyearsley", "followers_url": "https://api.github.com/users/joeyearsley/followers", "following_url": "https://api.github.com/users/joeyearsley/following{/other_user}", "gists_url": "https://api.github.com/users/joeyearsley/gists{/gist_id}", "starred_url": "https://api.github.com/users/joeyearsley/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joeyearsley/subscriptions", "organizations_url": "https://api.github.com/users/joeyearsley/orgs", "repos_url": "https://api.github.com/users/joeyearsley/repos", "events_url": "https://api.github.com/users/joeyearsley/events{/privacy}", "received_events_url": "https://api.github.com/users/joeyearsley/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-29T11:26:27Z", "updated_at": "2018-08-29T12:19:40Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Maybe I'm doing something wrong, but the recompute grad decorator isn't seemingly saving myself any memory. I'm using it naively on every <code>tf.layers.conv2d</code> like so:</p>\n<pre><code>@recompute_grad\ndef _x(x):\n    return tf.layers.conv2d(x, ...)\n</code></pre>\n<p>However, it uses the same amount of memory as standard TF 1.10 (i.e. not using the decorator).</p>\n<p>Also, I used to have gradient_checkpointing allowing me to have larger batch sizes (back in april) however it now doesn't seem to work either.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3731025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/allenlavoie\">@allenlavoie</a> Any ideas why?</p>", "body_text": "Maybe I'm doing something wrong, but the recompute grad decorator isn't seemingly saving myself any memory. I'm using it naively on every tf.layers.conv2d like so:\n@recompute_grad\ndef _x(x):\n    return tf.layers.conv2d(x, ...)\n\nHowever, it uses the same amount of memory as standard TF 1.10 (i.e. not using the decorator).\nAlso, I used to have gradient_checkpointing allowing me to have larger batch sizes (back in april) however it now doesn't seem to work either.\n@allenlavoie Any ideas why?", "body": "Maybe I'm doing something wrong, but the recompute grad decorator isn't seemingly saving myself any memory. I'm using it naively on every `tf.layers.conv2d` like so:\r\n\r\n```\r\n@recompute_grad\r\ndef _x(x):\r\n    return tf.layers.conv2d(x, ...)\r\n```\r\nHowever, it uses the same amount of memory as standard TF 1.10 (i.e. not using the decorator). \r\n\r\nAlso, I used to have gradient_checkpointing allowing me to have larger batch sizes (back in april) however it now doesn't seem to work either.\r\n\r\n@allenlavoie Any ideas why? "}