{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313708027", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313708027", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313708027, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzcwODAyNw==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T15:05:41Z", "updated_at": "2017-07-07T15:05:41Z", "author_association": "MEMBER", "body_html": "<p>You are right that the default automatic gradient code assumes that it's always cheaper to hang on to intermediate values than recompute.  This is usually true.  <em>But</em> there's nothing forcing you to use the automatic gradient code, and there are multiple ways to override gradient behavior.</p>\n<p>One approach I've seen used for things like this is to write parts of your model using TensorFlow functions  (see <code>function.Defun</code>) <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function.py#L42\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function.py#L42</a></p>\n<p>This allows you to define a reusable subgraph as a python function, and optionally specify a second function for the gradient.  The gradient function is at liberty to recompute if it likes, or reuse the output(s) of the forward function.</p>\n<p>I'm not the expert on this, but <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7943790\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zffchen78\">@zffchen78</a> could probably suggest interesting ways to use these mechanisms.</p>", "body_text": "You are right that the default automatic gradient code assumes that it's always cheaper to hang on to intermediate values than recompute.  This is usually true.  But there's nothing forcing you to use the automatic gradient code, and there are multiple ways to override gradient behavior.\nOne approach I've seen used for things like this is to write parts of your model using TensorFlow functions  (see function.Defun) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function.py#L42\nThis allows you to define a reusable subgraph as a python function, and optionally specify a second function for the gradient.  The gradient function is at liberty to recompute if it likes, or reuse the output(s) of the forward function.\nI'm not the expert on this, but @zffchen78 could probably suggest interesting ways to use these mechanisms.", "body": "You are right that the default automatic gradient code assumes that it's always cheaper to hang on to intermediate values than recompute.  This is usually true.  *But* there's nothing forcing you to use the automatic gradient code, and there are multiple ways to override gradient behavior.\r\n\r\nOne approach I've seen used for things like this is to write parts of your model using TensorFlow functions  (see `function.Defun`) https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/framework/function.py#L42\r\n\r\nThis allows you to define a reusable subgraph as a python function, and optionally specify a second function for the gradient.  The gradient function is at liberty to recompute if it likes, or reuse the output(s) of the forward function.\r\n\r\nI'm not the expert on this, but @zffchen78 could probably suggest interesting ways to use these mechanisms. \r\n"}