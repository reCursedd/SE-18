{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/343767935", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-343767935", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 343767935, "node_id": "MDEyOklzc3VlQ29tbWVudDM0Mzc2NzkzNQ==", "user": {"login": "alexlee-gk", "id": 839426, "node_id": "MDQ6VXNlcjgzOTQyNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/839426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexlee-gk", "html_url": "https://github.com/alexlee-gk", "followers_url": "https://api.github.com/users/alexlee-gk/followers", "following_url": "https://api.github.com/users/alexlee-gk/following{/other_user}", "gists_url": "https://api.github.com/users/alexlee-gk/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexlee-gk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexlee-gk/subscriptions", "organizations_url": "https://api.github.com/users/alexlee-gk/orgs", "repos_url": "https://api.github.com/users/alexlee-gk/repos", "events_url": "https://api.github.com/users/alexlee-gk/events{/privacy}", "received_events_url": "https://api.github.com/users/alexlee-gk/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-12T21:09:18Z", "updated_at": "2017-11-12T21:09:18Z", "author_association": "NONE", "body_html": "<p>Hi, I'd like to reopen this discussion on whether there is a better way to do this. I followed <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> suggestion and implemented a <code>recompute_on_gradient</code> decorator that uses <code>Defun</code>. I have included the decorator and sample usage below. It works as desired, but it has some major limitations:</p>\n<ol>\n<li>I couldn't find a good way to pass in function parameters, e.g. <code>reduction_dim</code> in the example below.</li>\n<li>The tensors being passed to the grad function are of unknown shape, so you can't figure out certain properties of the inputs, e.g. <code>broadcast_dim_a</code> and <code>broadcast_dim_b</code> in the example below.</li>\n</ol>\n<p>For 1., I tried using a closure to pass in the parameters, but that significantly slows down the execution. I also tried using <code>tf.name_scope</code>, but that is not that useful since the grad function doesn't get the <code>op</code>.</p>\n<p>In general, it would be really useful to have a way of defining a subgraph of ops as a single op so that you can define a gradient for that subgraph as a whole, and thus preventing tensorflow from storing the intermediate tensors (which can be really huge) of that subgraph for the backward pass.</p>\n<p>I have looked into <code>gradient_override_map</code> and <code>tf.py_func</code>, but they don't seem to be applicable here. The former only seems to work for individual ops that have been defined in C++. The latter does group a subgraph of ops into a single one, however this is for numpy functions. What's needed is something like <code>tf.tf_func</code> for a subgraph of tensorflow ops.</p>\n<pre><code>def recompute_on_gradient(func_name, grad_func=None):\n    \"\"\"\n    Assumes func takes in multiple inputs and returns a single output.\n    \"\"\"\n    def _recompute_on_gradient(func):\n        def _shape_func(op):\n            try:\n                return [output.shape for output in func(*op.inputs)]\n            except TypeError:\n                return [func(*op.inputs).shape]\n\n        if grad_func is None:\n            def _grad_func(*args):\n                inputs = list(args[:-1])\n                grad = args[-1]\n                output = func(*inputs)\n                return tf.gradients(ys=[output], xs=inputs, grad_ys=[grad])\n        else:\n            _grad_func = grad_func\n\n        _grad_func = function.Defun(func_name=func_name + \"_grad\",\n                                    noinline=True)(_grad_func)\n\n        return function.Defun(grad_func=_grad_func,\n                              shape_func=_shape_func,\n                              func_name=func_name,\n                              noinline=True)(func)\n    return _recompute_on_gradient\n\n\ndef _multiply_sum_grad(a, b, grad_c):\n    reduction_dim = -2\n    broadcast_dim_a = -1\n    broadcast_dim_b = 0\n\n    grad_c = tf.expand_dims(grad_c, axis=reduction_dim)\n    grad_a = grad_c * b\n    grad_b = grad_c * a\n    # sum over broadcast dimensions\n    grad_a = tf.reduce_sum(grad_a, axis=broadcast_dim_a, keep_dims=True)\n    grad_b = tf.reduce_sum(grad_b, axis=broadcast_dim_b, keep_dims=True)\n    return grad_a, grad_b\n\n\n@recompute_on_gradient(\"multiply_sum\", _multiply_sum_grad)\ndef multiply_sum(a, b):\n    reduction_dim = -2\n    return tf.reduce_sum(a * b, axis=reduction_dim)\n\n\n@recompute_on_gradient(\"multiply_sum_no_grad\")\ndef multiply_sum_no_grad(a, b):\n    reduction_dim = -2\n    return tf.reduce_sum(a * b, axis=reduction_dim)\n\n\ntime = 3\nbatch = 16\nin_height = 64\nin_width = 64\nin_channels = 1\nkernel_size = [17, 17]\nfilters = 4\npatches = tf.get_variable('a', [time, batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels])\nkernel = tf.get_variable('b', [batch, in_height, in_width, kernel_size[0], kernel_size[1], in_channels, filters])\nkernel_reshaped = tf.reshape(kernel, [batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels, filters])\n\noutput = tf.reduce_sum(patches[..., None] * kernel_reshaped[None, ...], axis=-2)  # uses more than 3GB to compute the gradient\noutput_mulsum = multiply_sum(patches[..., None], kernel_reshaped[None, ...])\noutput_mulsum_no_grad = multiply_sum_no_grad(patches[..., None], kernel_reshaped[None, ...])\n</code></pre>", "body_text": "Hi, I'd like to reopen this discussion on whether there is a better way to do this. I followed @ebrevdo suggestion and implemented a recompute_on_gradient decorator that uses Defun. I have included the decorator and sample usage below. It works as desired, but it has some major limitations:\n\nI couldn't find a good way to pass in function parameters, e.g. reduction_dim in the example below.\nThe tensors being passed to the grad function are of unknown shape, so you can't figure out certain properties of the inputs, e.g. broadcast_dim_a and broadcast_dim_b in the example below.\n\nFor 1., I tried using a closure to pass in the parameters, but that significantly slows down the execution. I also tried using tf.name_scope, but that is not that useful since the grad function doesn't get the op.\nIn general, it would be really useful to have a way of defining a subgraph of ops as a single op so that you can define a gradient for that subgraph as a whole, and thus preventing tensorflow from storing the intermediate tensors (which can be really huge) of that subgraph for the backward pass.\nI have looked into gradient_override_map and tf.py_func, but they don't seem to be applicable here. The former only seems to work for individual ops that have been defined in C++. The latter does group a subgraph of ops into a single one, however this is for numpy functions. What's needed is something like tf.tf_func for a subgraph of tensorflow ops.\ndef recompute_on_gradient(func_name, grad_func=None):\n    \"\"\"\n    Assumes func takes in multiple inputs and returns a single output.\n    \"\"\"\n    def _recompute_on_gradient(func):\n        def _shape_func(op):\n            try:\n                return [output.shape for output in func(*op.inputs)]\n            except TypeError:\n                return [func(*op.inputs).shape]\n\n        if grad_func is None:\n            def _grad_func(*args):\n                inputs = list(args[:-1])\n                grad = args[-1]\n                output = func(*inputs)\n                return tf.gradients(ys=[output], xs=inputs, grad_ys=[grad])\n        else:\n            _grad_func = grad_func\n\n        _grad_func = function.Defun(func_name=func_name + \"_grad\",\n                                    noinline=True)(_grad_func)\n\n        return function.Defun(grad_func=_grad_func,\n                              shape_func=_shape_func,\n                              func_name=func_name,\n                              noinline=True)(func)\n    return _recompute_on_gradient\n\n\ndef _multiply_sum_grad(a, b, grad_c):\n    reduction_dim = -2\n    broadcast_dim_a = -1\n    broadcast_dim_b = 0\n\n    grad_c = tf.expand_dims(grad_c, axis=reduction_dim)\n    grad_a = grad_c * b\n    grad_b = grad_c * a\n    # sum over broadcast dimensions\n    grad_a = tf.reduce_sum(grad_a, axis=broadcast_dim_a, keep_dims=True)\n    grad_b = tf.reduce_sum(grad_b, axis=broadcast_dim_b, keep_dims=True)\n    return grad_a, grad_b\n\n\n@recompute_on_gradient(\"multiply_sum\", _multiply_sum_grad)\ndef multiply_sum(a, b):\n    reduction_dim = -2\n    return tf.reduce_sum(a * b, axis=reduction_dim)\n\n\n@recompute_on_gradient(\"multiply_sum_no_grad\")\ndef multiply_sum_no_grad(a, b):\n    reduction_dim = -2\n    return tf.reduce_sum(a * b, axis=reduction_dim)\n\n\ntime = 3\nbatch = 16\nin_height = 64\nin_width = 64\nin_channels = 1\nkernel_size = [17, 17]\nfilters = 4\npatches = tf.get_variable('a', [time, batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels])\nkernel = tf.get_variable('b', [batch, in_height, in_width, kernel_size[0], kernel_size[1], in_channels, filters])\nkernel_reshaped = tf.reshape(kernel, [batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels, filters])\n\noutput = tf.reduce_sum(patches[..., None] * kernel_reshaped[None, ...], axis=-2)  # uses more than 3GB to compute the gradient\noutput_mulsum = multiply_sum(patches[..., None], kernel_reshaped[None, ...])\noutput_mulsum_no_grad = multiply_sum_no_grad(patches[..., None], kernel_reshaped[None, ...])", "body": "Hi, I'd like to reopen this discussion on whether there is a better way to do this. I followed @ebrevdo suggestion and implemented a `recompute_on_gradient` decorator that uses `Defun`. I have included the decorator and sample usage below. It works as desired, but it has some major limitations:\r\n1. I couldn't find a good way to pass in function parameters, e.g. `reduction_dim` in the example below.\r\n2. The tensors being passed to the grad function are of unknown shape, so you can't figure out certain properties of the inputs, e.g. `broadcast_dim_a` and `broadcast_dim_b` in the example below.\r\n\r\nFor 1., I tried using a closure to pass in the parameters, but that significantly slows down the execution. I also tried using `tf.name_scope`, but that is not that useful since the grad function doesn't get the `op`.\r\n\r\nIn general, it would be really useful to have a way of defining a subgraph of ops as a single op so that you can define a gradient for that subgraph as a whole, and thus preventing tensorflow from storing the intermediate tensors (which can be really huge) of that subgraph for the backward pass.\r\n\r\nI have looked into `gradient_override_map` and `tf.py_func`, but they don't seem to be applicable here. The former only seems to work for individual ops that have been defined in C++. The latter does group a subgraph of ops into a single one, however this is for numpy functions. What's needed is something like `tf.tf_func` for a subgraph of tensorflow ops.\r\n\r\n```\r\ndef recompute_on_gradient(func_name, grad_func=None):\r\n    \"\"\"\r\n    Assumes func takes in multiple inputs and returns a single output.\r\n    \"\"\"\r\n    def _recompute_on_gradient(func):\r\n        def _shape_func(op):\r\n            try:\r\n                return [output.shape for output in func(*op.inputs)]\r\n            except TypeError:\r\n                return [func(*op.inputs).shape]\r\n\r\n        if grad_func is None:\r\n            def _grad_func(*args):\r\n                inputs = list(args[:-1])\r\n                grad = args[-1]\r\n                output = func(*inputs)\r\n                return tf.gradients(ys=[output], xs=inputs, grad_ys=[grad])\r\n        else:\r\n            _grad_func = grad_func\r\n\r\n        _grad_func = function.Defun(func_name=func_name + \"_grad\",\r\n                                    noinline=True)(_grad_func)\r\n\r\n        return function.Defun(grad_func=_grad_func,\r\n                              shape_func=_shape_func,\r\n                              func_name=func_name,\r\n                              noinline=True)(func)\r\n    return _recompute_on_gradient\r\n\r\n\r\ndef _multiply_sum_grad(a, b, grad_c):\r\n    reduction_dim = -2\r\n    broadcast_dim_a = -1\r\n    broadcast_dim_b = 0\r\n\r\n    grad_c = tf.expand_dims(grad_c, axis=reduction_dim)\r\n    grad_a = grad_c * b\r\n    grad_b = grad_c * a\r\n    # sum over broadcast dimensions\r\n    grad_a = tf.reduce_sum(grad_a, axis=broadcast_dim_a, keep_dims=True)\r\n    grad_b = tf.reduce_sum(grad_b, axis=broadcast_dim_b, keep_dims=True)\r\n    return grad_a, grad_b\r\n\r\n\r\n@recompute_on_gradient(\"multiply_sum\", _multiply_sum_grad)\r\ndef multiply_sum(a, b):\r\n    reduction_dim = -2\r\n    return tf.reduce_sum(a * b, axis=reduction_dim)\r\n\r\n\r\n@recompute_on_gradient(\"multiply_sum_no_grad\")\r\ndef multiply_sum_no_grad(a, b):\r\n    reduction_dim = -2\r\n    return tf.reduce_sum(a * b, axis=reduction_dim)\r\n\r\n\r\ntime = 3\r\nbatch = 16\r\nin_height = 64\r\nin_width = 64\r\nin_channels = 1\r\nkernel_size = [17, 17]\r\nfilters = 4\r\npatches = tf.get_variable('a', [time, batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels])\r\nkernel = tf.get_variable('b', [batch, in_height, in_width, kernel_size[0], kernel_size[1], in_channels, filters])\r\nkernel_reshaped = tf.reshape(kernel, [batch, in_height, in_width, kernel_size[0] * kernel_size[1] * in_channels, filters])\r\n\r\noutput = tf.reduce_sum(patches[..., None] * kernel_reshaped[None, ...], axis=-2)  # uses more than 3GB to compute the gradient\r\noutput_mulsum = multiply_sum(patches[..., None], kernel_reshaped[None, ...])\r\noutput_mulsum_no_grad = multiply_sum_no_grad(patches[..., None], kernel_reshaped[None, ...])\r\n```"}