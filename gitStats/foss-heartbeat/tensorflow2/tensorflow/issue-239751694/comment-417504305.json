{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417504305", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-417504305", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 417504305, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzUwNDMwNQ==", "user": {"login": "allenlavoie", "id": 3731025, "node_id": "MDQ6VXNlcjM3MzEwMjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/3731025?v=4", "gravatar_id": "", "url": "https://api.github.com/users/allenlavoie", "html_url": "https://github.com/allenlavoie", "followers_url": "https://api.github.com/users/allenlavoie/followers", "following_url": "https://api.github.com/users/allenlavoie/following{/other_user}", "gists_url": "https://api.github.com/users/allenlavoie/gists{/gist_id}", "starred_url": "https://api.github.com/users/allenlavoie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/allenlavoie/subscriptions", "organizations_url": "https://api.github.com/users/allenlavoie/orgs", "repos_url": "https://api.github.com/users/allenlavoie/repos", "events_url": "https://api.github.com/users/allenlavoie/events{/privacy}", "received_events_url": "https://api.github.com/users/allenlavoie/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-30T23:51:31Z", "updated_at": "2018-08-30T23:51:31Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1836025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/joeyearsley\">@joeyearsley</a> recompute_grad needs to catch an intermediate activation which would otherwise feed into a gradient op in order to save memory. <code>tf.layers.conv2d</code> has a conv op and maybe a ReLU, and ReLU automatically recomputes, so if the model is just a sequence of <code>_x</code> calls then it's already saving only one activation per <code>_x</code>. To save memory, you could wrap a sequence of <code>_x</code> calls in <code>recompute_grad</code> (but include too many and the recomputation of the gradient will take as much memory as the model would have otherwise). If there's a batch norm layer I'd include that in the decorated function, since it isn't automatically recomputed like ReLU.</p>", "body_text": "@joeyearsley recompute_grad needs to catch an intermediate activation which would otherwise feed into a gradient op in order to save memory. tf.layers.conv2d has a conv op and maybe a ReLU, and ReLU automatically recomputes, so if the model is just a sequence of _x calls then it's already saving only one activation per _x. To save memory, you could wrap a sequence of _x calls in recompute_grad (but include too many and the recomputation of the gradient will take as much memory as the model would have otherwise). If there's a batch norm layer I'd include that in the decorated function, since it isn't automatically recomputed like ReLU.", "body": "@joeyearsley recompute_grad needs to catch an intermediate activation which would otherwise feed into a gradient op in order to save memory. `tf.layers.conv2d` has a conv op and maybe a ReLU, and ReLU automatically recomputes, so if the model is just a sequence of `_x` calls then it's already saving only one activation per `_x`. To save memory, you could wrap a sequence of `_x` calls in `recompute_grad` (but include too many and the recomputation of the gradient will take as much memory as the model would have otherwise). If there's a batch norm layer I'd include that in the decorated function, since it isn't automatically recomputed like ReLU."}