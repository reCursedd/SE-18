{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313706276", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313706276", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313706276, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzcwNjI3Ng==", "user": {"login": "MarvinTeichmann", "id": 2729159, "node_id": "MDQ6VXNlcjI3MjkxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/2729159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarvinTeichmann", "html_url": "https://github.com/MarvinTeichmann", "followers_url": "https://api.github.com/users/MarvinTeichmann/followers", "following_url": "https://api.github.com/users/MarvinTeichmann/following{/other_user}", "gists_url": "https://api.github.com/users/MarvinTeichmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarvinTeichmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarvinTeichmann/subscriptions", "organizations_url": "https://api.github.com/users/MarvinTeichmann/orgs", "repos_url": "https://api.github.com/users/MarvinTeichmann/repos", "events_url": "https://api.github.com/users/MarvinTeichmann/events{/privacy}", "received_events_url": "https://api.github.com/users/MarvinTeichmann/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T14:59:23Z", "updated_at": "2017-07-07T15:02:15Z", "author_association": "NONE", "body_html": "<p>Also, I had a short look at the other memory optimization projects mentioned. XLA and grapper implement good ideas. However those projects aim at performing memory optimization fully automated. I feel for advanced users it will be much more valuable to give them options for manual memory optimization. I think many users know the bottleneck of their model and manual optimization can archive much better results then automated memory optimization projects. [See the crf example].</p>\n<p>The swap memory used for RNNs is a pretty cool idea. This can be used as an alternative to the recomputation approach. I think it would be great to have a function which could perform <code>swap_memory</code> on arbitrary (i.e. non-rnn) tensors.  There are long chain models even outside the rnn world and this can be very useful for those as well. [<em>Btw. could someone point me to the line where swap_memory is implemented? I am not familiar with the rnn code. What I have seen so far most functions just pass this parameter on. What would it take to have the output of an arbitrary tensor <code>t</code> swapped to CPU memory?</em>]</p>\n<p>Overall I think it would be cool, if this could be pinged to the right person at the tensorflow team. I feel that the large memory usage is the <a href=\"https://github.com/tensorflow/tensorflow/issues/492\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/492/hovercard\">biggest down-side of tensorflow</a>. We know two solutions for this. Namely <code>recomputation</code> of some tensors during back-prop (i.e. via node mirroring) or <code>memory swapping</code>. Both can be implemented in pure python. So I don't see much reason for not tackling this.</p>", "body_text": "Also, I had a short look at the other memory optimization projects mentioned. XLA and grapper implement good ideas. However those projects aim at performing memory optimization fully automated. I feel for advanced users it will be much more valuable to give them options for manual memory optimization. I think many users know the bottleneck of their model and manual optimization can archive much better results then automated memory optimization projects. [See the crf example].\nThe swap memory used for RNNs is a pretty cool idea. This can be used as an alternative to the recomputation approach. I think it would be great to have a function which could perform swap_memory on arbitrary (i.e. non-rnn) tensors.  There are long chain models even outside the rnn world and this can be very useful for those as well. [Btw. could someone point me to the line where swap_memory is implemented? I am not familiar with the rnn code. What I have seen so far most functions just pass this parameter on. What would it take to have the output of an arbitrary tensor t swapped to CPU memory?]\nOverall I think it would be cool, if this could be pinged to the right person at the tensorflow team. I feel that the large memory usage is the biggest down-side of tensorflow. We know two solutions for this. Namely recomputation of some tensors during back-prop (i.e. via node mirroring) or memory swapping. Both can be implemented in pure python. So I don't see much reason for not tackling this.", "body": "Also, I had a short look at the other memory optimization projects mentioned. XLA and grapper implement good ideas. However those projects aim at performing memory optimization fully automated. I feel for advanced users it will be much more valuable to give them options for manual memory optimization. I think many users know the bottleneck of their model and manual optimization can archive much better results then automated memory optimization projects. [See the crf example].\r\n\r\nThe swap memory used for RNNs is a pretty cool idea. This can be used as an alternative to the recomputation approach. I think it would be great to have a function which could perform `swap_memory` on arbitrary (i.e. non-rnn) tensors.  There are long chain models even outside the rnn world and this can be very useful for those as well. [*Btw. could someone point me to the line where swap_memory is implemented? I am not familiar with the rnn code. What I have seen so far most functions just pass this parameter on. What would it take to have the output of an arbitrary tensor `t` swapped to CPU memory?*]\r\n\r\nOverall I think it would be cool, if this could be pinged to the right person at the tensorflow team. I feel that the large memory usage is the [biggest down-side of tensorflow](https://github.com/tensorflow/tensorflow/issues/492). We know two solutions for this. Namely `recomputation` of some tensors during back-prop (i.e. via node mirroring) or `memory swapping`. Both can be implemented in pure python. So I don't see much reason for not tackling this."}