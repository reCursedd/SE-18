{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313730975", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313730975", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "id": 313730975, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzczMDk3NQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-07T16:33:48Z", "updated_at": "2017-07-07T16:33:48Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">We may be able to add a decorator/wrapper that just does this Defun trick.\nCall it \"@recompute_on_gradient\" or something.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Jul 7, 2017 at 9:18 AM, Paul Barham ***@***.***&gt; wrote:\n One other way to temporarily override gradient code is to use\n gradient_override_map\n <a href=\"https://github.com/tensorflow/tensorflow/blob/\">https://github.com/tensorflow/tensorflow/blob/</a>\n <a href=\"https://github.com/tensorflow/tensorflow/commit/2cfbd3347b943a389bb688125c2a90095b7735b5\" class=\"commit-link\"><tt>2cfbd33</tt></a>/tensorflow/python/\n framework/ops.py#L3773\n\n  @tf.RegisterGradient(\"CustomSquare\")\n    def _custom_square_grad(op, grad):\n    # ...\n\n  with tf.Graph().as_default() as g:\n    c = tf.constant(5.0)\n    s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n    with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n      s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\n\n Since the custom gradient function is passed the original Operation, it\n can recompute the op from the same inputs.\n\n The main reason this doesn't get used for recomputation is that this\n mechanism is done at an op granularity, whereas most people probably want\n to do recomputation at a layer granularity.\n\n The Defun trick is basically the same thing but applied to a larger\n subgraph.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"239751694\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/11171\" href=\"https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313727272\">#11171 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim1TG9PmZIC7cGgBeK-kmWYy09-YRks5sLlpqgaJpZM4OKdxb\">https://github.com/notifications/unsubscribe-auth/ABtim1TG9PmZIC7cGgBeK-kmWYy09-YRks5sLlpqgaJpZM4OKdxb</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "We may be able to add a decorator/wrapper that just does this Defun trick.\nCall it \"@recompute_on_gradient\" or something.\n\u2026\nOn Fri, Jul 7, 2017 at 9:18 AM, Paul Barham ***@***.***> wrote:\n One other way to temporarily override gradient code is to use\n gradient_override_map\n https://github.com/tensorflow/tensorflow/blob/\n 2cfbd33/tensorflow/python/\n framework/ops.py#L3773\n\n  @tf.RegisterGradient(\"CustomSquare\")\n    def _custom_square_grad(op, grad):\n    # ...\n\n  with tf.Graph().as_default() as g:\n    c = tf.constant(5.0)\n    s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n    with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n      s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\n\n Since the custom gradient function is passed the original Operation, it\n can recompute the op from the same inputs.\n\n The main reason this doesn't get used for recomputation is that this\n mechanism is done at an op granularity, whereas most people probably want\n to do recomputation at a layer granularity.\n\n The Defun trick is basically the same thing but applied to a larger\n subgraph.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#11171 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim1TG9PmZIC7cGgBeK-kmWYy09-YRks5sLlpqgaJpZM4OKdxb>\n .", "body": "We may be able to add a decorator/wrapper that just does this Defun trick.\nCall it \"@recompute_on_gradient\" or something.\n\nOn Fri, Jul 7, 2017 at 9:18 AM, Paul Barham <notifications@github.com>\nwrote:\n\n> One other way to temporarily override gradient code is to use\n> gradient_override_map\n> https://github.com/tensorflow/tensorflow/blob/\n> 2cfbd3347b943a389bb688125c2a90095b7735b5/tensorflow/python/\n> framework/ops.py#L3773\n>\n>  @tf.RegisterGradient(\"CustomSquare\")\n>    def _custom_square_grad(op, grad):\n>    # ...\n>\n>  with tf.Graph().as_default() as g:\n>    c = tf.constant(5.0)\n>    s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n>    with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n>      s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the gradient of s_2.\n>\n> Since the custom gradient function is passed the original Operation, it\n> can recompute the op from the same inputs.\n>\n> The main reason this doesn't get used for recomputation is that this\n> mechanism is done at an op granularity, whereas most people probably want\n> to do recomputation at a layer granularity.\n>\n> The Defun trick is basically the same thing but applied to a larger\n> subgraph.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/11171#issuecomment-313727272>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim1TG9PmZIC7cGgBeK-kmWYy09-YRks5sLlpqgaJpZM4OKdxb>\n> .\n>\n"}