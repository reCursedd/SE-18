{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11171/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11171", "id": 239751694, "node_id": "MDU6SXNzdWUyMzk3NTE2OTQ=", "number": 11171, "title": " [Feature] Node mirroring for GPU-memory reduction", "user": {"login": "MarvinTeichmann", "id": 2729159, "node_id": "MDQ6VXNlcjI3MjkxNTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/2729159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MarvinTeichmann", "html_url": "https://github.com/MarvinTeichmann", "followers_url": "https://api.github.com/users/MarvinTeichmann/followers", "following_url": "https://api.github.com/users/MarvinTeichmann/following{/other_user}", "gists_url": "https://api.github.com/users/MarvinTeichmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/MarvinTeichmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MarvinTeichmann/subscriptions", "organizations_url": "https://api.github.com/users/MarvinTeichmann/orgs", "repos_url": "https://api.github.com/users/MarvinTeichmann/repos", "events_url": "https://api.github.com/users/MarvinTeichmann/events{/privacy}", "received_events_url": "https://api.github.com/users/MarvinTeichmann/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 33, "created_at": "2017-06-30T11:24:18Z", "updated_at": "2018-08-30T23:51:31Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>In the paper <a href=\"https://arxiv.org/abs/1604.06174\" rel=\"nofollow\">Training Deep Nets with Sublinear Memory Cost</a> Chen et al. introduced a very good idea to greatly reduce GPU memory requirements.</p>\n<p>The idea bowls down to discarding the output of some nodes during the forward pass and recompute those values when they are needed again in the backward pass. Only the output of some key ops is kept in memory. During back-prop all forward computation from those key nodes are redone. They also describe how this can be implemented in a graph based computation model by mirroring non-key ops. This is depicted in figure 3 (see below).</p>\n<p>Performing this kind of graph manipulation in <a href=\"https://github.com/dmlc/mxnet\">MxNet</a> is quite easy and I have played around with this myself. I am able to reduce the memory cost of a SotA segmentation model from <code>13504 MB</code> to <code>3382 MB</code> for the cost of about ~40% increase in computational time. (Given that we have plenty 1080 TI and view P100 GPUs, I am very happy to pay this cost).</p>\n<p>For me as deep learning researcher this is a totally awesome killer feature. In most of my experiments I am limited by the amount of available GPU memory. Doing node mirroring allows me to try a whole bunch of new stuff, I was always wanting to do.</p>\n<ol>\n<li>Is anything like this planned to be implemented in Tensorflow any time soon?</li>\n<li>In the current API, is there already a way to build and / or manipulate the computational graph to perform node mirroring (like in figure 3)?</li>\n</ol>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://camo.githubusercontent.com/a5beeb5cbba949aed6fb5e2f512ecac2c2ab2946/68747470733a2f2f692e696d6775722e636f6d2f436e6b557a774a2e706e67\"><img src=\"https://camo.githubusercontent.com/a5beeb5cbba949aed6fb5e2f512ecac2c2ab2946/68747470733a2f2f692e696d6775722e636f6d2f436e6b557a774a2e706e67\" alt=\"\" data-canonical-src=\"https://i.imgur.com/CnkUzwJ.png\" style=\"max-width:100%;\"></a></p>\n<p>Regarding question two, I don't mind if it gets messy. Copying some nodes inside the graph is possible in tensorflow. Gradient flow can also be stopped for the first copy. What is missing is to utilize the second node for gradient computation. I don't know how I can archive this using the python API in tensorflow. Any ideas with this?</p>", "body_text": "In the paper Training Deep Nets with Sublinear Memory Cost Chen et al. introduced a very good idea to greatly reduce GPU memory requirements.\nThe idea bowls down to discarding the output of some nodes during the forward pass and recompute those values when they are needed again in the backward pass. Only the output of some key ops is kept in memory. During back-prop all forward computation from those key nodes are redone. They also describe how this can be implemented in a graph based computation model by mirroring non-key ops. This is depicted in figure 3 (see below).\nPerforming this kind of graph manipulation in MxNet is quite easy and I have played around with this myself. I am able to reduce the memory cost of a SotA segmentation model from 13504 MB to 3382 MB for the cost of about ~40% increase in computational time. (Given that we have plenty 1080 TI and view P100 GPUs, I am very happy to pay this cost).\nFor me as deep learning researcher this is a totally awesome killer feature. In most of my experiments I am limited by the amount of available GPU memory. Doing node mirroring allows me to try a whole bunch of new stuff, I was always wanting to do.\n\nIs anything like this planned to be implemented in Tensorflow any time soon?\nIn the current API, is there already a way to build and / or manipulate the computational graph to perform node mirroring (like in figure 3)?\n\n\nRegarding question two, I don't mind if it gets messy. Copying some nodes inside the graph is possible in tensorflow. Gradient flow can also be stopped for the first copy. What is missing is to utilize the second node for gradient computation. I don't know how I can archive this using the python API in tensorflow. Any ideas with this?", "body": "In the paper [Training Deep Nets with Sublinear Memory Cost](https://arxiv.org/abs/1604.06174) Chen et al. introduced a very good idea to greatly reduce GPU memory requirements. \r\n\r\nThe idea bowls down to discarding the output of some nodes during the forward pass and recompute those values when they are needed again in the backward pass. Only the output of some key ops is kept in memory. During back-prop all forward computation from those key nodes are redone. They also describe how this can be implemented in a graph based computation model by mirroring non-key ops. This is depicted in figure 3 (see below).\r\n\r\nPerforming this kind of graph manipulation in [MxNet](https://github.com/dmlc/mxnet) is quite easy and I have played around with this myself. I am able to reduce the memory cost of a SotA segmentation model from `13504 MB` to `3382 MB` for the cost of about ~40% increase in computational time. (Given that we have plenty 1080 TI and view P100 GPUs, I am very happy to pay this cost).\r\n\r\nFor me as deep learning researcher this is a totally awesome killer feature. In most of my experiments I am limited by the amount of available GPU memory. Doing node mirroring allows me to try a whole bunch of new stuff, I was always wanting to do. \r\n\r\n1. Is anything like this planned to be implemented in Tensorflow any time soon?\r\n2. In the current API, is there already a way to build and / or manipulate the computational graph to perform node mirroring (like in figure 3)?\r\n\r\n![](https://i.imgur.com/CnkUzwJ.png)\r\n\r\nRegarding question two, I don't mind if it gets messy. Copying some nodes inside the graph is possible in tensorflow. Gradient flow can also be stopped for the first copy. What is missing is to utilize the second node for gradient computation. I don't know how I can archive this using the python API in tensorflow. Any ideas with this?\r\n\r\n"}