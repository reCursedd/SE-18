{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319855835", "html_url": "https://github.com/tensorflow/tensorflow/issues/11965#issuecomment-319855835", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11965", "id": 319855835, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTg1NTgzNQ==", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-03T03:09:26Z", "updated_at": "2017-08-03T03:09:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>You probably forgot this which is written in the document of batch_norm:</p>\n<pre><code>  Note: when training, the moving_mean and moving_variance need to be updated.\n  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n  need to be added as a dependency to the `train_op`. For example:\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss)\n\n  One can set updates_collections=None to force the updates in place, but that\n  can have a speed penalty, especially in distributed settings.\n</code></pre>", "body_text": "You probably forgot this which is written in the document of batch_norm:\n  Note: when training, the moving_mean and moving_variance need to be updated.\n  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\n  need to be added as a dependency to the `train_op`. For example:\n\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    with tf.control_dependencies(update_ops):\n      train_op = optimizer.minimize(loss)\n\n  One can set updates_collections=None to force the updates in place, but that\n  can have a speed penalty, especially in distributed settings.", "body": "You probably forgot this which is written in the document of batch_norm:\r\n```\r\n  Note: when training, the moving_mean and moving_variance need to be updated.\r\n  By default the update ops are placed in `tf.GraphKeys.UPDATE_OPS`, so they\r\n  need to be added as a dependency to the `train_op`. For example:\r\n\r\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    with tf.control_dependencies(update_ops):\r\n      train_op = optimizer.minimize(loss)\r\n\r\n  One can set updates_collections=None to force the updates in place, but that\r\n  can have a speed penalty, especially in distributed settings.\r\n```"}