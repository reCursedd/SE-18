{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428462358", "html_url": "https://github.com/tensorflow/tensorflow/issues/22824#issuecomment-428462358", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22824", "id": 428462358, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODQ2MjM1OA==", "user": {"login": "xinyu-Naturali", "id": 43464505, "node_id": "MDQ6VXNlcjQzNDY0NTA1", "avatar_url": "https://avatars3.githubusercontent.com/u/43464505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xinyu-Naturali", "html_url": "https://github.com/xinyu-Naturali", "followers_url": "https://api.github.com/users/xinyu-Naturali/followers", "following_url": "https://api.github.com/users/xinyu-Naturali/following{/other_user}", "gists_url": "https://api.github.com/users/xinyu-Naturali/gists{/gist_id}", "starred_url": "https://api.github.com/users/xinyu-Naturali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xinyu-Naturali/subscriptions", "organizations_url": "https://api.github.com/users/xinyu-Naturali/orgs", "repos_url": "https://api.github.com/users/xinyu-Naturali/repos", "events_url": "https://api.github.com/users/xinyu-Naturali/events{/privacy}", "received_events_url": "https://api.github.com/users/xinyu-Naturali/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T07:09:32Z", "updated_at": "2018-10-10T07:09:32Z", "author_association": "NONE", "body_html": "<p>I think it is a feature request.  Being able to do sampled softmax loss should be a standard feature.  When someone makes a custom model and wants to train with sampling, the loss function should share the weights with the last dense layer.  But for now there's no easy way to do it because by the time of compiling the model with the loss function, the loss function can't be defined as all the layers haven't been hooked up into a model (only happens in model call) and thus dense won't have its dimensions ready to initialize the weights.  This conflict should really be addressed</p>", "body_text": "I think it is a feature request.  Being able to do sampled softmax loss should be a standard feature.  When someone makes a custom model and wants to train with sampling, the loss function should share the weights with the last dense layer.  But for now there's no easy way to do it because by the time of compiling the model with the loss function, the loss function can't be defined as all the layers haven't been hooked up into a model (only happens in model call) and thus dense won't have its dimensions ready to initialize the weights.  This conflict should really be addressed", "body": "I think it is a feature request.  Being able to do sampled softmax loss should be a standard feature.  When someone makes a custom model and wants to train with sampling, the loss function should share the weights with the last dense layer.  But for now there's no easy way to do it because by the time of compiling the model with the loss function, the loss function can't be defined as all the layers haven't been hooked up into a model (only happens in model call) and thus dense won't have its dimensions ready to initialize the weights.  This conflict should really be addressed  "}