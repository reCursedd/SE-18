{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22824", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22824/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22824/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22824/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22824", "id": 368022833, "node_id": "MDU6SXNzdWUzNjgwMjI4MzM=", "number": 22824, "title": "Sampled softmax in tf keras", "user": {"login": "xinyu-Naturali", "id": 43464505, "node_id": "MDQ6VXNlcjQzNDY0NTA1", "avatar_url": "https://avatars3.githubusercontent.com/u/43464505?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xinyu-Naturali", "html_url": "https://github.com/xinyu-Naturali", "followers_url": "https://api.github.com/users/xinyu-Naturali/followers", "following_url": "https://api.github.com/users/xinyu-Naturali/following{/other_user}", "gists_url": "https://api.github.com/users/xinyu-Naturali/gists{/gist_id}", "starred_url": "https://api.github.com/users/xinyu-Naturali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xinyu-Naturali/subscriptions", "organizations_url": "https://api.github.com/users/xinyu-Naturali/orgs", "repos_url": "https://api.github.com/users/xinyu-Naturali/repos", "events_url": "https://api.github.com/users/xinyu-Naturali/events{/privacy}", "received_events_url": "https://api.github.com/users/xinyu-Naturali/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-10-09T03:02:46Z", "updated_at": "2018-11-13T23:20:36Z", "closed_at": "2018-11-13T23:20:36Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nno</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nno</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.11</li>\n<li><strong>Python version</strong>:<br>\n3.5.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I want to do sampled softmax loss in tf keras.  I defined my own model by subclassing keras Model.  In <strong>init</strong>, I specify the layers I need including the last Dense projection layer.  But this Dense layer shouldn't be called in training as I want to do sampled softmax and only to use it's weights and biases.  Then I define the loss function like this:</p>\n<pre><code>class SampledSoftmax(Layer):\n    def __init__(self,\n                       num_sampled,\n                       num_classes,\n                       projection,\n                       bias,\n                       hidden_size):\n        self.weights = tf.transpose(projection)\n        self.bias = bias\n        self.num_classes = num_classes\n        self.num_sampled = num_sampled\n        self.hidden_size = hidden_size\n\n    def __call__(self, y_true, input):\n        \"\"\" reshaping of y_true and input to make them fit each other \"\"\"\n        input = tf.reshape(input, (-1,self.hidden_size))\n        y_true = tf.reshape(y_true, (-1,1))\n\n        return tf.nn.sampled_softmax_loss(\n            weights=self.weights,\n            biases=self.bias,\n            labels=y_true,\n            inputs=input,\n            num_sampled=self.num_sampled,\n            num_classes=self.num_classes,\n            partition_strategy='div')\n</code></pre>\n<p>It takes in the necessary parameters to initialize and the class call will be the needed sampled softmax loss function.  The catch is that to add loss to model compile I need the weights etc of the last Dense.  But 1) in training Dense is not included in the model, and 2) even if it does, the Dense layer would only be hooked up with input and thus get its input dimensions etc in <strong>call</strong> of my custom model.  In short, the weights etc won't be available before compiling model.  Can anyone offer some help to point me to the right direction?</p>\n<p>BTW, the loss defined above would work in small test cases like the following.</p>\n<pre><code>x = Input(shape=(10,), name='input_x')\nemb_out = Embedding(10000,200,input_length=10)(x)\nlstm_out = LSTM(200, return_sequences=True)(emb_out)\n\ndense = Dense(10000, activation='sigmoid')\noutput = dense(lstm_out)\n\nsl = SampledSoftmax(10, 10000, dense.kernel, dense.bias)\n\nmodel = Model(inputs=x, outputs=lstm_out)\nmodel.compile(optimizer='adam', loss=sl)\nmodel.summary()\nmodel.fit(dataset, epochs=20, steps_per_epoch=5)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nno\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nno\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\n1.11\nPython version:\n3.5.3\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI want to do sampled softmax loss in tf keras.  I defined my own model by subclassing keras Model.  In init, I specify the layers I need including the last Dense projection layer.  But this Dense layer shouldn't be called in training as I want to do sampled softmax and only to use it's weights and biases.  Then I define the loss function like this:\nclass SampledSoftmax(Layer):\n    def __init__(self,\n                       num_sampled,\n                       num_classes,\n                       projection,\n                       bias,\n                       hidden_size):\n        self.weights = tf.transpose(projection)\n        self.bias = bias\n        self.num_classes = num_classes\n        self.num_sampled = num_sampled\n        self.hidden_size = hidden_size\n\n    def __call__(self, y_true, input):\n        \"\"\" reshaping of y_true and input to make them fit each other \"\"\"\n        input = tf.reshape(input, (-1,self.hidden_size))\n        y_true = tf.reshape(y_true, (-1,1))\n\n        return tf.nn.sampled_softmax_loss(\n            weights=self.weights,\n            biases=self.bias,\n            labels=y_true,\n            inputs=input,\n            num_sampled=self.num_sampled,\n            num_classes=self.num_classes,\n            partition_strategy='div')\n\nIt takes in the necessary parameters to initialize and the class call will be the needed sampled softmax loss function.  The catch is that to add loss to model compile I need the weights etc of the last Dense.  But 1) in training Dense is not included in the model, and 2) even if it does, the Dense layer would only be hooked up with input and thus get its input dimensions etc in call of my custom model.  In short, the weights etc won't be available before compiling model.  Can anyone offer some help to point me to the right direction?\nBTW, the loss defined above would work in small test cases like the following.\nx = Input(shape=(10,), name='input_x')\nemb_out = Embedding(10000,200,input_length=10)(x)\nlstm_out = LSTM(200, return_sequences=True)(emb_out)\n\ndense = Dense(10000, activation='sigmoid')\noutput = dense(lstm_out)\n\nsl = SampledSoftmax(10, 10000, dense.kernel, dense.bias)\n\nmodel = Model(inputs=x, outputs=lstm_out)\nmodel.compile(optimizer='adam', loss=sl)\nmodel.summary()\nmodel.fit(dataset, epochs=20, steps_per_epoch=5)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nno\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nno\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.11\r\n- **Python version**:\r\n3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI want to do sampled softmax loss in tf keras.  I defined my own model by subclassing keras Model.  In __init__, I specify the layers I need including the last Dense projection layer.  But this Dense layer shouldn't be called in training as I want to do sampled softmax and only to use it's weights and biases.  Then I define the loss function like this:\r\n\r\n    class SampledSoftmax(Layer):\r\n        def __init__(self,\r\n                           num_sampled,\r\n                           num_classes,\r\n                           projection,\r\n                           bias,\r\n                           hidden_size):\r\n            self.weights = tf.transpose(projection)\r\n            self.bias = bias\r\n            self.num_classes = num_classes\r\n            self.num_sampled = num_sampled\r\n            self.hidden_size = hidden_size\r\n\r\n        def __call__(self, y_true, input):\r\n            \"\"\" reshaping of y_true and input to make them fit each other \"\"\"\r\n            input = tf.reshape(input, (-1,self.hidden_size))\r\n            y_true = tf.reshape(y_true, (-1,1))\r\n\r\n            return tf.nn.sampled_softmax_loss(\r\n                weights=self.weights,\r\n                biases=self.bias,\r\n                labels=y_true,\r\n                inputs=input,\r\n                num_sampled=self.num_sampled,\r\n                num_classes=self.num_classes,\r\n                partition_strategy='div')\r\n\r\nIt takes in the necessary parameters to initialize and the class call will be the needed sampled softmax loss function.  The catch is that to add loss to model compile I need the weights etc of the last Dense.  But 1) in training Dense is not included in the model, and 2) even if it does, the Dense layer would only be hooked up with input and thus get its input dimensions etc in __call__ of my custom model.  In short, the weights etc won't be available before compiling model.  Can anyone offer some help to point me to the right direction? \r\n\r\nBTW, the loss defined above would work in small test cases like the following.\r\n\r\n    x = Input(shape=(10,), name='input_x')\r\n    emb_out = Embedding(10000,200,input_length=10)(x)\r\n    lstm_out = LSTM(200, return_sequences=True)(emb_out)\r\n\r\n    dense = Dense(10000, activation='sigmoid')\r\n    output = dense(lstm_out)\r\n\r\n    sl = SampledSoftmax(10, 10000, dense.kernel, dense.bias)\r\n\r\n    model = Model(inputs=x, outputs=lstm_out)\r\n    model.compile(optimizer='adam', loss=sl)\r\n    model.summary()\r\n    model.fit(dataset, epochs=20, steps_per_epoch=5)"}