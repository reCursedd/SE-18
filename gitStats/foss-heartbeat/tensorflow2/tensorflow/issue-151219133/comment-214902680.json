{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214902680", "html_url": "https://github.com/tensorflow/tensorflow/issues/2117#issuecomment-214902680", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2117", "id": 214902680, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDkwMjY4MA==", "user": {"login": "hamidb", "id": 4418982, "node_id": "MDQ6VXNlcjQ0MTg5ODI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4418982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hamidb", "html_url": "https://github.com/hamidb", "followers_url": "https://api.github.com/users/hamidb/followers", "following_url": "https://api.github.com/users/hamidb/following{/other_user}", "gists_url": "https://api.github.com/users/hamidb/gists{/gist_id}", "starred_url": "https://api.github.com/users/hamidb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hamidb/subscriptions", "organizations_url": "https://api.github.com/users/hamidb/orgs", "repos_url": "https://api.github.com/users/hamidb/repos", "events_url": "https://api.github.com/users/hamidb/events{/privacy}", "received_events_url": "https://api.github.com/users/hamidb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-26T22:07:40Z", "updated_at": "2016-04-26T22:07:40Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15676913\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/poxvoculi\">@poxvoculi</a><br>\nIt is hard to reproduce it as it takes really long to reach 113K steps. I had it running overnight several times and sometimes my system reboots after core dumps. So I lost the step at which it failed.<br>\nI am trying to keep track of it in a log file to see if there is a random failure or not.<br>\nI did not notice any sign of memory exhaustion. Indeed, it was my initial guess. But what is the best way to ensure there is no memory bandwidth limit?<br>\nMy current GPU memory usage at step=70K is 11501MiB with GPU utilization around %30-%45</p>", "body_text": "@poxvoculi\nIt is hard to reproduce it as it takes really long to reach 113K steps. I had it running overnight several times and sometimes my system reboots after core dumps. So I lost the step at which it failed.\nI am trying to keep track of it in a log file to see if there is a random failure or not.\nI did not notice any sign of memory exhaustion. Indeed, it was my initial guess. But what is the best way to ensure there is no memory bandwidth limit?\nMy current GPU memory usage at step=70K is 11501MiB with GPU utilization around %30-%45", "body": "@poxvoculi \nIt is hard to reproduce it as it takes really long to reach 113K steps. I had it running overnight several times and sometimes my system reboots after core dumps. So I lost the step at which it failed.\nI am trying to keep track of it in a log file to see if there is a random failure or not.\nI did not notice any sign of memory exhaustion. Indeed, it was my initial guess. But what is the best way to ensure there is no memory bandwidth limit?\nMy current GPU memory usage at step=70K is 11501MiB with GPU utilization around %30-%45\n"}