{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/214909519", "html_url": "https://github.com/tensorflow/tensorflow/issues/2117#issuecomment-214909519", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2117", "id": 214909519, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNDkwOTUxOQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-26T22:42:34Z", "updated_at": "2016-04-26T22:42:34Z", "author_association": "MEMBER", "body_html": "<p>I have not personally seen this CUDA error previously.  The error likely is not directly related to the memcpy, but rather the memcpy is failing because the GPU is in a bad state.  Many CUDA driver commands are asynchronous: the error from a prior action is only fielded when trying to launch a subsequent action.  If it's a bad error (e.g. one we never expect to see), then a bad status will propagate back up to the top of TensorFlow. That's what's happening here.  NVIDIA documentation indicates that this error can result when a kernel takes too long to execute.  If you're not using any modified kernels and  are just running the provided model with the usual data, then no kernel should take an excessive amount of time unless something causes the GPU to not be able to make progress.  It's hard to guess what that might be.  TensorFlow uses its own external memory manager for GPU memory, so memory exhaustion <em>should</em> trigger a TF bad status exception on the CPU side, and never trigger a GPU error.  Since this error only showed up after a long period, I don't think it's impossible that it could be a hardware issue, e.g. an un-corrected memory error (IIRC, most consumer NVIDIA GPUs don't use ECC memory).</p>", "body_text": "I have not personally seen this CUDA error previously.  The error likely is not directly related to the memcpy, but rather the memcpy is failing because the GPU is in a bad state.  Many CUDA driver commands are asynchronous: the error from a prior action is only fielded when trying to launch a subsequent action.  If it's a bad error (e.g. one we never expect to see), then a bad status will propagate back up to the top of TensorFlow. That's what's happening here.  NVIDIA documentation indicates that this error can result when a kernel takes too long to execute.  If you're not using any modified kernels and  are just running the provided model with the usual data, then no kernel should take an excessive amount of time unless something causes the GPU to not be able to make progress.  It's hard to guess what that might be.  TensorFlow uses its own external memory manager for GPU memory, so memory exhaustion should trigger a TF bad status exception on the CPU side, and never trigger a GPU error.  Since this error only showed up after a long period, I don't think it's impossible that it could be a hardware issue, e.g. an un-corrected memory error (IIRC, most consumer NVIDIA GPUs don't use ECC memory).", "body": "I have not personally seen this CUDA error previously.  The error likely is not directly related to the memcpy, but rather the memcpy is failing because the GPU is in a bad state.  Many CUDA driver commands are asynchronous: the error from a prior action is only fielded when trying to launch a subsequent action.  If it's a bad error (e.g. one we never expect to see), then a bad status will propagate back up to the top of TensorFlow. That's what's happening here.  NVIDIA documentation indicates that this error can result when a kernel takes too long to execute.  If you're not using any modified kernels and  are just running the provided model with the usual data, then no kernel should take an excessive amount of time unless something causes the GPU to not be able to make progress.  It's hard to guess what that might be.  TensorFlow uses its own external memory manager for GPU memory, so memory exhaustion _should_ trigger a TF bad status exception on the CPU side, and never trigger a GPU error.  Since this error only showed up after a long period, I don't think it's impossible that it could be a hardware issue, e.g. an un-corrected memory error (IIRC, most consumer NVIDIA GPUs don't use ECC memory).  \n"}