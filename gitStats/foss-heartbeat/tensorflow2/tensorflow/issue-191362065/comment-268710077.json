{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/268710077", "html_url": "https://github.com/tensorflow/tensorflow/issues/5816#issuecomment-268710077", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5816", "id": 268710077, "node_id": "MDEyOklzc3VlQ29tbWVudDI2ODcxMDA3Nw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-22T03:32:27Z", "updated_at": "2016-12-22T17:53:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's possible to have non-deterministic \"out of memory\" because you are unlucky. TensorFlow has non-deterministic order of execution, so depending on timing, you may have things scheduling in different order and needing different amounts of memory.</p>\n<p>To verify this is indeed a bug, you could use memory profiling as detailed in <a href=\"https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb\">https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb</a></p>\n<p>That notebook shows how to examine timeline with Tensor allocation + de-allocation events and confirm that tensors are deallocated quickly after they are no longer needed.</p>\n<p>For an example of unlucky scheduling, consider computational graph below<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/21414260/a605697e-c7b2-11e6-87f9-e7b1682ba705.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/21414260/a605697e-c7b2-11e6-87f9-e7b1682ba705.png\" alt=\"snake-graph\" style=\"max-width:100%;\"></a></p>\n<p>TensorFlow schedules ops asynchronously as soon as they are ready, so if \"circle\" execute faster than any \"square\" ops then TF could schedule them first and hence allocate memory for 5 circle ops.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/21414406/fe679a46-c7b3-11e6-9e7b-3d38403f5ea3.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/21414406/fe679a46-c7b3-11e6-9e7b-3d38403f5ea3.png\" alt=\"snake2\" style=\"max-width:100%;\"></a></p>\n<p>But if circle ops take longer to execute, you may end up going in sequence left to right, and hence requiring less memory</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/21414408/06033508-c7b4-11e6-939b-f0607d8e2a3a.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/21414408/06033508-c7b4-11e6-939b-f0607d8e2a3a.png\" alt=\"snake3\" style=\"max-width:100%;\"></a></p>\n<p>You could use control dependencies to force a particular execution order. IE, with <code>tf.control_dependencies</code> or if your graph has already been constructed such as when you use <code>tf.gradients</code>, you could use <code>graph_editor</code>. IE, something like this</p>\n<pre><code>import tensorflow.contrib.graph_editor as ge\ndef run_after(a_tensor, b_tensor):\n    \"\"\"Force a to run after b\"\"\"\n    ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\n\n ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\n\n\n\n</code></pre>", "body_text": "It's possible to have non-deterministic \"out of memory\" because you are unlucky. TensorFlow has non-deterministic order of execution, so depending on timing, you may have things scheduling in different order and needing different amounts of memory.\nTo verify this is indeed a bug, you could use memory profiling as detailed in https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb\nThat notebook shows how to examine timeline with Tensor allocation + de-allocation events and confirm that tensors are deallocated quickly after they are no longer needed.\nFor an example of unlucky scheduling, consider computational graph below\n\nTensorFlow schedules ops asynchronously as soon as they are ready, so if \"circle\" execute faster than any \"square\" ops then TF could schedule them first and hence allocate memory for 5 circle ops.\n\nBut if circle ops take longer to execute, you may end up going in sequence left to right, and hence requiring less memory\n\nYou could use control dependencies to force a particular execution order. IE, with tf.control_dependencies or if your graph has already been constructed such as when you use tf.gradients, you could use graph_editor. IE, something like this\nimport tensorflow.contrib.graph_editor as ge\ndef run_after(a_tensor, b_tensor):\n    \"\"\"Force a to run after b\"\"\"\n    ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\n\n ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])", "body": "It's possible to have non-deterministic \"out of memory\" because you are unlucky. TensorFlow has non-deterministic order of execution, so depending on timing, you may have things scheduling in different order and needing different amounts of memory.\r\n\r\nTo verify this is indeed a bug, you could use memory profiling as detailed in https://github.com/yaroslavvb/notebooks/blob/master/mnist-memory.ipynb\r\n\r\nThat notebook shows how to examine timeline with Tensor allocation + de-allocation events and confirm that tensors are deallocated quickly after they are no longer needed.\r\n\r\nFor an example of unlucky scheduling, consider computational graph below\r\n![snake-graph](https://cloud.githubusercontent.com/assets/23068/21414260/a605697e-c7b2-11e6-87f9-e7b1682ba705.png)\r\n\r\nTensorFlow schedules ops asynchronously as soon as they are ready, so if \"circle\" execute faster than any \"square\" ops then TF could schedule them first and hence allocate memory for 5 circle ops.\r\n\r\n![snake2](https://cloud.githubusercontent.com/assets/23068/21414406/fe679a46-c7b3-11e6-9e7b-3d38403f5ea3.png)\r\n\r\nBut if circle ops take longer to execute, you may end up going in sequence left to right, and hence requiring less memory\r\n\r\n![snake3](https://cloud.githubusercontent.com/assets/23068/21414408/06033508-c7b4-11e6-939b-f0607d8e2a3a.png)\r\n\r\n\r\nYou could use control dependencies to force a particular execution order. IE, with `tf.control_dependencies` or if your graph has already been constructed such as when you use `tf.gradients`, you could use `graph_editor`. IE, something like this\r\n\r\n```\r\nimport tensorflow.contrib.graph_editor as ge\r\ndef run_after(a_tensor, b_tensor):\r\n    \"\"\"Force a to run after b\"\"\"\r\n    ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\r\n\r\n ge.reroute.add_control_inputs(a_tensor.op, [b_tensor.op])\r\n\r\n\r\n\r\n```"}