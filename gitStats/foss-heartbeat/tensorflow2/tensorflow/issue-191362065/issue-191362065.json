{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5816", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5816/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5816/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5816/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5816", "id": 191362065, "node_id": "MDU6SXNzdWUxOTEzNjIwNjU=", "number": 5816, "title": "MaxPool3DGrad - Out of Memory Issue", "user": {"login": "tomrunia", "id": 5536129, "node_id": "MDQ6VXNlcjU1MzYxMjk=", "avatar_url": "https://avatars1.githubusercontent.com/u/5536129?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tomrunia", "html_url": "https://github.com/tomrunia", "followers_url": "https://api.github.com/users/tomrunia/followers", "following_url": "https://api.github.com/users/tomrunia/following{/other_user}", "gists_url": "https://api.github.com/users/tomrunia/gists{/gist_id}", "starred_url": "https://api.github.com/users/tomrunia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tomrunia/subscriptions", "organizations_url": "https://api.github.com/users/tomrunia/orgs", "repos_url": "https://api.github.com/users/tomrunia/repos", "events_url": "https://api.github.com/users/tomrunia/events{/privacy}", "received_events_url": "https://api.github.com/users/tomrunia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-11-23T19:33:17Z", "updated_at": "2017-06-16T18:13:04Z", "closed_at": "2017-06-16T18:13:04Z", "author_association": "NONE", "body_html": "<p>I am training a fairly big network with many 3D convolutions that almost fills up all the GPU memory (Titan X). When settings the batch size to a small amount, e.g. 32 examples the training process crashes after a number of steps with an out of memory issue caused by <code>MaxPool3DGrad</code>. I lowered the batch size to 20, which makes the training run fine for &gt;2000 training steps but then at some point the model crashes again with the same error. It seems like some operations are not freeing memory, maybe the <code>MaxPool3DGrad</code> kernel? Full error message is given below.</p>\n<p><strong>Configuration</strong>: Linux Mint, checkout of the TensorFlow master 5 days ago (<code>dfc5cd48a095b133ece9caff663e3cc512e8a268</code>) with CUDA 8.0 and CuDNN 5.1.</p>\n<p>This might be relevant: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"169932198\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3696\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3696/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3696\">#3696</a></p>\n<pre><code>I tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 1722368000 totalling 4.81GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2674491392 totalling 2.49GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3444734720 totalling 3.21GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.21GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\nLimit:                 12051264308\nInUse:                 12032303616\nMaxInUse:              12032303616\nNumAllocs:                 2860074\nMaxAllocSize:           3444734976\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ******************************xxxxxxxxxxxxx**************************************************xxxxxxx\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 205.32MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[20,64,50,29,29]\nTraceback (most recent call last):\n  File \"train_c3d.py\", line 324, in &lt;module&gt;\n    is_training: True\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[20,64,50,29,29]\n         [[Node: gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/repl\nica:0/task:0/gpu:0\"](3D_CNN/conv-relu-0/Relu, 3D_CNN/conv-relu-0/MaxPool3D, gradients/3D_CNN/conv-relu-1/conv-1_grad/tuple/control_dependency)]]\n\nCaused by op u'gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad', defined at:\n  File \"train_c3d.py\", line 145, in &lt;module&gt;\n    grads_and_vars = optimizer.compute_gradients(total_loss)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 335, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\n    in_grads = grad_fn(op, *out_grads)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\n    padding=op.get_attr(\"padding\"))\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1657, in max_pool3d_grad\n    name=name)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2259, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1130, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'3D_CNN/conv-relu-0/MaxPool3D', defined at:\n  File \"train_c3d.py\", line 128, in &lt;module&gt;\n</code></pre>", "body_text": "I am training a fairly big network with many 3D convolutions that almost fills up all the GPU memory (Titan X). When settings the batch size to a small amount, e.g. 32 examples the training process crashes after a number of steps with an out of memory issue caused by MaxPool3DGrad. I lowered the batch size to 20, which makes the training run fine for >2000 training steps but then at some point the model crashes again with the same error. It seems like some operations are not freeing memory, maybe the MaxPool3DGrad kernel? Full error message is given below.\nConfiguration: Linux Mint, checkout of the TensorFlow master 5 days ago (dfc5cd48a095b133ece9caff663e3cc512e8a268) with CUDA 8.0 and CuDNN 5.1.\nThis might be relevant: #3696\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 1722368000 totalling 4.81GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2674491392 totalling 2.49GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3444734720 totalling 3.21GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.21GiB\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\nLimit:                 12051264308\nInUse:                 12032303616\nMaxInUse:              12032303616\nNumAllocs:                 2860074\nMaxAllocSize:           3444734976\n\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ******************************xxxxxxxxxxxxx**************************************************xxxxxxx\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 205.32MiB.  See logs for memory state.\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[20,64,50,29,29]\nTraceback (most recent call last):\n  File \"train_c3d.py\", line 324, in <module>\n    is_training: True\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\n    run_metadata_ptr)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\n    target_list, options, run_metadata)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[20,64,50,29,29]\n         [[Node: gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/repl\nica:0/task:0/gpu:0\"](3D_CNN/conv-relu-0/Relu, 3D_CNN/conv-relu-0/MaxPool3D, gradients/3D_CNN/conv-relu-1/conv-1_grad/tuple/control_dependency)]]\n\nCaused by op u'gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad', defined at:\n  File \"train_c3d.py\", line 145, in <module>\n    grads_and_vars = optimizer.compute_gradients(total_loss)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 335, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\n    in_grads = grad_fn(op, *out_grads)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\n    padding=op.get_attr(\"padding\"))\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1657, in max_pool3d_grad\n    name=name)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\n    op_def=op_def)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2259, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1130, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'3D_CNN/conv-relu-0/MaxPool3D', defined at:\n  File \"train_c3d.py\", line 128, in <module>", "body": "I am training a fairly big network with many 3D convolutions that almost fills up all the GPU memory (Titan X). When settings the batch size to a small amount, e.g. 32 examples the training process crashes after a number of steps with an out of memory issue caused by `MaxPool3DGrad`. I lowered the batch size to 20, which makes the training run fine for >2000 training steps but then at some point the model crashes again with the same error. It seems like some operations are not freeing memory, maybe the `MaxPool3DGrad` kernel? Full error message is given below. \r\n\r\n**Configuration**: Linux Mint, checkout of the TensorFlow master 5 days ago (`dfc5cd48a095b133ece9caff663e3cc512e8a268`) with CUDA 8.0 and CuDNN 5.1.  \r\n\r\nThis might be relevant: https://github.com/tensorflow/tensorflow/issues/3696\r\n\r\n```\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 3 Chunks of size 1722368000 totalling 4.81GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 2674491392 totalling 2.49GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:696] 1 Chunks of size 3444734720 totalling 3.21GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:700] Sum Total of in-use chunks: 11.21GiB\r\nI tensorflow/core/common_runtime/bfc_allocator.cc:702] Stats:\r\nLimit:                 12051264308\r\nInUse:                 12032303616\r\nMaxInUse:              12032303616\r\nNumAllocs:                 2860074\r\nMaxAllocSize:           3444734976\r\n\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:274] ******************************xxxxxxxxxxxxx**************************************************xxxxxxx\r\nW tensorflow/core/common_runtime/bfc_allocator.cc:275] Ran out of memory trying to allocate 205.32MiB.  See logs for memory state.\r\nW tensorflow/core/framework/op_kernel.cc:975] Resource exhausted: OOM when allocating tensor with shape[20,64,50,29,29]\r\nTraceback (most recent call last):\r\n  File \"train_c3d.py\", line 324, in <module>\r\n    is_training: True\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 766, in run\r\n    run_metadata_ptr)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 964, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1014, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1034, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.ResourceExhaustedError: OOM when allocating tensor with shape[20,64,50,29,29]\r\n         [[Node: gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad = MaxPool3DGrad[T=DT_FLOAT, ksize=[1, 1, 2, 2, 1], padding=\"VALID\", strides=[1, 2, 2, 2, 1], _device=\"/job:localhost/repl\r\nica:0/task:0/gpu:0\"](3D_CNN/conv-relu-0/Relu, 3D_CNN/conv-relu-0/MaxPool3D, gradients/3D_CNN/conv-relu-1/conv-1_grad/tuple/control_dependency)]]\r\n\r\nCaused by op u'gradients/3D_CNN/conv-relu-0/MaxPool3D_grad/MaxPool3DGrad', defined at:\r\n  File \"train_c3d.py\", line 145, in <module>\r\n    grads_and_vars = optimizer.compute_gradients(total_loss)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 335, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py\", line 482, in gradients\r\n    in_grads = grad_fn(op, *out_grads)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 130, in _MaxPool3DGrad\r\n    padding=op.get_attr(\"padding\"))\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 1657, in max_pool3d_grad\r\n    name=name)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 759, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2259, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/trunia/virtualenv/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1130, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\n...which was originally created as op u'3D_CNN/conv-relu-0/MaxPool3D', defined at:\r\n  File \"train_c3d.py\", line 128, in <module>\r\n```"}