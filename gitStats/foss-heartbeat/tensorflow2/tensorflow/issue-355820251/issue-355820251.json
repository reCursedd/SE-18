{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21985", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21985/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21985/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21985/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21985", "id": 355820251, "node_id": "MDU6SXNzdWUzNTU4MjAyNTE=", "number": 21985, "title": "While loop no gradients provided for any variable", "user": {"login": "lizaigaoge550", "id": 12975526, "node_id": "MDQ6VXNlcjEyOTc1NTI2", "avatar_url": "https://avatars1.githubusercontent.com/u/12975526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lizaigaoge550", "html_url": "https://github.com/lizaigaoge550", "followers_url": "https://api.github.com/users/lizaigaoge550/followers", "following_url": "https://api.github.com/users/lizaigaoge550/following{/other_user}", "gists_url": "https://api.github.com/users/lizaigaoge550/gists{/gist_id}", "starred_url": "https://api.github.com/users/lizaigaoge550/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lizaigaoge550/subscriptions", "organizations_url": "https://api.github.com/users/lizaigaoge550/orgs", "repos_url": "https://api.github.com/users/lizaigaoge550/repos", "events_url": "https://api.github.com/users/lizaigaoge550/events{/privacy}", "received_events_url": "https://api.github.com/users/lizaigaoge550/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "cy89", "id": 29663194, "node_id": "MDQ6VXNlcjI5NjYzMTk0", "avatar_url": "https://avatars0.githubusercontent.com/u/29663194?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cy89", "html_url": "https://github.com/cy89", "followers_url": "https://api.github.com/users/cy89/followers", "following_url": "https://api.github.com/users/cy89/following{/other_user}", "gists_url": "https://api.github.com/users/cy89/gists{/gist_id}", "starred_url": "https://api.github.com/users/cy89/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cy89/subscriptions", "organizations_url": "https://api.github.com/users/cy89/orgs", "repos_url": "https://api.github.com/users/cy89/repos", "events_url": "https://api.github.com/users/cy89/events{/privacy}", "received_events_url": "https://api.github.com/users/cy89/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-31T02:46:10Z", "updated_at": "2018-08-31T22:40:02Z", "closed_at": "2018-08-31T22:40:02Z", "author_association": "NONE", "body_html": "<p>After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG\"><img src=\"https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG\" alt=\"default\" style=\"max-width:100%;\"></a></p>\n<h1>while_loop</h1>\n<p>def dynamic_pointing_decoder(self, U, mask):<br>\ndef _HMN(ut, h, us, ue):<br>\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d<br>\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())<br>\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d<br>\nut_r = tf.concat([ut, r], axis=1) #batch,3d<br>\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())<br>\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt1 = tf.einsum('bt,top-&gt;bop', ut_r, W1) + b1<br>\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d<br>\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt2 = tf.einsum('bi,ijp-&gt;bjp', mt1, W2) + b2<br>\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d<br>\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d<br>\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())<br>\nhmn = tf.einsum('bi,ijp-&gt;bjp', mt12, W3) + b3<br>\nhmn = tf.reduce_max(hmn, axis=2) #batch 1<br>\nhmn = tf.reshape(hmn, [-1]) #batch<br>\nreturn hmn</p>\n<pre><code>    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\n        us_ue = tf.concat([us, ue], axis=1)  # batch 4d\n        h, state = cell(inputs=us_ue, state=state)  # batch * d\n\n        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\n            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_start = tf.argmax(alpha, axis=1)  # batch\n        i_start = tf.cast(i_start, tf.int32)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        us = tf.gather_nd(U, s_idx)  # batch 2d\n\n        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\n            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_end = tf.argmax(beta, axis=1)  # batch\n        i_end = tf.cast(i_end, tf.int32)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        ue = tf.gather_nd(U, e_idx)  # batch 2d\n\n        p1s.write(time_step, i_start)\n        p2s.write(time_step, i_end)\n        alphas.write(time_step, alpha)\n        betas.write(time_step, beta)\n        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\n\n    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\n        return tf.less(time_step, 4)\n\n\n    with tf.variable_scope('dynamic_pointing_decoder'):\n        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\n        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        idx = tf.range(0, tf.shape(U)[0], 1)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        us = tf.gather_nd(U, s_idx) #batch 2d\n        ue = tf.gather_nd(U, e_idx) #batch 2d\n        p1s = tf.TensorArray(tf.int32, size=4)\n        p2s = tf.TensorArray(tf.int32, size=4)\n        alphas = tf.TensorArray(tf.float32, size=4)\n        betas = tf.TensorArray(tf.float32, size=4)\n        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\n                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\n        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\n        #time_step, p1s, p2s, us, ue, state\n        time_step = 0\n        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\n                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\n                                                                          maximum_iterations=4)\n        self.p1s = tf.transpose(p1s.stack()) #batch*4\n        self.p2s = tf.transpose(p2s.stack())\n        print(\n            \"p1s  shape : {0}\".format(np.shape(self.p1s))\n        )\n        self.p1 = tf.unstack(self.p1s,axis=0)[-1]\n        print(\"p1 shape : {0}\".format(np.shape(self.p1)))\n        self.p2 = tf.unstack(self.p2s, axis=0)[-1]\n        alphas = tf.unstack(alphas.stack(), axis=0)\n        betas = tf.unstack(betas.stack(), axis=0)\n        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\n        return alphas, betas\n</code></pre>\n<p>#no while loop<br>\ndef dynamic_pointing_decoder(self, U, mask):<br>\ndef _HMN(ut, h, us, ue):<br>\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d<br>\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())<br>\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d<br>\nut_r = tf.concat([ut, r], axis=1) #batch,3d<br>\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())<br>\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt1 = tf.einsum('bt,top-&gt;bop', ut_r, W1) + b1<br>\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d<br>\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())<br>\nmt2 = tf.einsum('bi,ijp-&gt;bjp', mt1, W2) + b2<br>\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d<br>\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d<br>\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())<br>\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())<br>\nhmn = tf.einsum('bi,ijp-&gt;bjp', mt12, W3) + b3<br>\nhmn = tf.reduce_max(hmn, axis=2) #batch 1<br>\nhmn = tf.reshape(hmn, [-1]) #batch<br>\nreturn hmn<br>\nwith tf.variable_scope('dynamic_pointing_decoder'):<br>\n#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)<br>\n#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])<br>\n#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)<br>\ncell = CudnnCompatibleLSTMCell(self._config.hidden_dim)<br>\ni_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\ni_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)<br>\nidx = tf.range(0, tf.shape(U)[0], 1)<br>\ns_idx = tf.stack([idx, i_start], axis=1)<br>\ne_idx = tf.stack([idx, i_end], axis=1)<br>\nus = tf.gather_nd(U, s_idx) #batch 2d<br>\nue = tf.gather_nd(U, e_idx) #batch 2d<br>\nalphas, betas = [], []<br>\nstate = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),<br>\ntf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN<br>\nU_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d<br>\nfor time_step in range(4):<br>\nif time_step &gt;= 1:<br>\ntf.get_variable_scope().reuse_variables()<br>\nus_ue = tf.concat([us,ue], axis=1) #batch 4d<br>\nh, state = cell(inputs=us_ue, state=state) #batch * d</p>\n<pre><code>            with tf.variable_scope('alpha_HMN'):\n                if time_step &gt;= 1:\n                    tf.get_variable_scope().reuse_variables()\n                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_start = tf.argmax(alpha, axis=1) #batch\n            i_start = tf.cast(i_start, tf.int32)\n            s_idx = tf.stack([idx, i_start], axis=1)\n            us = tf.gather_nd(U, s_idx) #batch 2d\n\n            with tf.variable_scope('betas_HMN'):\n                if time_step &gt;= 1:\n                    tf.get_variable_scope().reuse_variables()\n                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_end = tf.argmax(alpha, axis=1) #batch\n            i_end = tf.cast(i_end, tf.int32)\n            e_idx = tf.stack([idx, i_end], axis=1)\n            ue = tf.gather_nd(U, e_idx) #batch 2d\n\n            alphas.append(alpha)\n            betas.append(beta)\n            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\n            #    break\n            #else:\n            #    pre_start = i_start\n            #    pre_end = i_end\n        return alpha, beta\n</code></pre>\n<p>anyone can help me? Thank you very much</p>", "body_text": "After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.\n\nwhile_loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\n    def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\n        us_ue = tf.concat([us, ue], axis=1)  # batch 4d\n        h, state = cell(inputs=us_ue, state=state)  # batch * d\n\n        with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\n            alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_start = tf.argmax(alpha, axis=1)  # batch\n        i_start = tf.cast(i_start, tf.int32)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        us = tf.gather_nd(U, s_idx)  # batch 2d\n\n        with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\n            beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n            beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\n        i_end = tf.argmax(beta, axis=1)  # batch\n        i_end = tf.cast(i_end, tf.int32)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        ue = tf.gather_nd(U, e_idx)  # batch 2d\n\n        p1s.write(time_step, i_start)\n        p2s.write(time_step, i_end)\n        alphas.write(time_step, alpha)\n        betas.write(time_step, beta)\n        return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\n\n    def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\n        return tf.less(time_step, 4)\n\n\n    with tf.variable_scope('dynamic_pointing_decoder'):\n        cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\n        i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n        idx = tf.range(0, tf.shape(U)[0], 1)\n        s_idx = tf.stack([idx, i_start], axis=1)\n        e_idx = tf.stack([idx, i_end], axis=1)\n        us = tf.gather_nd(U, s_idx) #batch 2d\n        ue = tf.gather_nd(U, e_idx) #batch 2d\n        p1s = tf.TensorArray(tf.int32, size=4)\n        p2s = tf.TensorArray(tf.int32, size=4)\n        alphas = tf.TensorArray(tf.float32, size=4)\n        betas = tf.TensorArray(tf.float32, size=4)\n        state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\n                                          tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\n        U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\n        #time_step, p1s, p2s, us, ue, state\n        time_step = 0\n        time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\n                                                                          loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\n                                                                          maximum_iterations=4)\n        self.p1s = tf.transpose(p1s.stack()) #batch*4\n        self.p2s = tf.transpose(p2s.stack())\n        print(\n            \"p1s  shape : {0}\".format(np.shape(self.p1s))\n        )\n        self.p1 = tf.unstack(self.p1s,axis=0)[-1]\n        print(\"p1 shape : {0}\".format(np.shape(self.p1)))\n        self.p2 = tf.unstack(self.p2s, axis=0)[-1]\n        alphas = tf.unstack(alphas.stack(), axis=0)\n        betas = tf.unstack(betas.stack(), axis=0)\n        print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\n        return alphas, betas\n\n#no while loop\ndef dynamic_pointing_decoder(self, U, mask):\ndef _HMN(ut, h, us, ue):\nh_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\nWD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\nr = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\nut_r = tf.concat([ut, r], axis=1) #batch,3d\nW1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\nb1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\nmt1 = tf.reduce_max(mt1, axis=2) #batch * d\nW2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\nmt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\nmt2 = tf.reduce_max(mt2, axis=2) #batch * d\nmt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\nW3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\nb3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\nhmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\nhmn = tf.reduce_max(hmn, axis=2) #batch 1\nhmn = tf.reshape(hmn, [-1]) #batch\nreturn hmn\nwith tf.variable_scope('dynamic_pointing_decoder'):\n#single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)\n#cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])\n#cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)\ncell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\ni_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\ni_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\n#pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\nidx = tf.range(0, tf.shape(U)[0], 1)\ns_idx = tf.stack([idx, i_start], axis=1)\ne_idx = tf.stack([idx, i_end], axis=1)\nus = tf.gather_nd(U, s_idx) #batch 2d\nue = tf.gather_nd(U, e_idx) #batch 2d\nalphas, betas = [], []\nstate = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\ntf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\nU_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\nfor time_step in range(4):\nif time_step >= 1:\ntf.get_variable_scope().reuse_variables()\nus_ue = tf.concat([us,ue], axis=1) #batch 4d\nh, state = cell(inputs=us_ue, state=state) #batch * d\n            with tf.variable_scope('alpha_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_start = tf.argmax(alpha, axis=1) #batch\n            i_start = tf.cast(i_start, tf.int32)\n            s_idx = tf.stack([idx, i_start], axis=1)\n            us = tf.gather_nd(U, s_idx) #batch 2d\n\n            with tf.variable_scope('betas_HMN'):\n                if time_step >= 1:\n                    tf.get_variable_scope().reuse_variables()\n                beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\n                beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\n            i_end = tf.argmax(alpha, axis=1) #batch\n            i_end = tf.cast(i_end, tf.int32)\n            e_idx = tf.stack([idx, i_end], axis=1)\n            ue = tf.gather_nd(U, e_idx) #batch 2d\n\n            alphas.append(alpha)\n            betas.append(beta)\n            #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\n            #    break\n            #else:\n            #    pre_start = i_start\n            #    pre_end = i_end\n        return alpha, beta\n\nanyone can help me? Thank you very much", "body": "After i used while_loop, ValueError: no gradients provided for any variable , it seems like RefvariableProcessor wraps tf.variable.\r\n\r\n![default](https://user-images.githubusercontent.com/12975526/44889979-d9ca6b80-ad0a-11e8-967b-0f31a2467c68.PNG)\r\n\r\n# while_loop\r\ndef dynamic_pointing_decoder(self, U, mask):\r\n        def _HMN(ut, h, us, ue):\r\n            h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\r\n            WD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\r\n            r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\r\n            ut_r = tf.concat([ut, r], axis=1) #batch,3d\r\n            W1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\r\n            b1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\r\n            mt1 = tf.reduce_max(mt1, axis=2) #batch * d\r\n            W2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\r\n            mt2 = tf.reduce_max(mt2, axis=2) #batch * d\r\n            mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\r\n            W3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\r\n            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\r\n            hmn = tf.reduce_max(hmn, axis=2) #batch 1\r\n            hmn = tf.reshape(hmn, [-1]) #batch\r\n            return hmn\r\n\r\n        def body(time_step, p1s, p2s, alphas, betas, us, ue, state):\r\n            us_ue = tf.concat([us, ue], axis=1)  # batch 4d\r\n            h, state = cell(inputs=us_ue, state=state)  # batch * d\r\n\r\n            with tf.variable_scope('alpha_HMN', reuse=tf.AUTO_REUSE):\r\n                alpha = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                alpha = tf.transpose(alpha, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\r\n            i_start = tf.argmax(alpha, axis=1)  # batch\r\n            i_start = tf.cast(i_start, tf.int32)\r\n            s_idx = tf.stack([idx, i_start], axis=1)\r\n            us = tf.gather_nd(U, s_idx)  # batch 2d\r\n\r\n            with tf.variable_scope('betas_HMN', reuse=tf.AUTO_REUSE):\r\n                beta = tf.map_fn(lambda ut: _HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                beta = tf.transpose(beta, [1, 0]) * tf.cast(mask, dtype=tf.float32)  # batch article_len\r\n            i_end = tf.argmax(beta, axis=1)  # batch\r\n            i_end = tf.cast(i_end, tf.int32)\r\n            e_idx = tf.stack([idx, i_end], axis=1)\r\n            ue = tf.gather_nd(U, e_idx)  # batch 2d\r\n\r\n            p1s.write(time_step, i_start)\r\n            p2s.write(time_step, i_end)\r\n            alphas.write(time_step, alpha)\r\n            betas.write(time_step, beta)\r\n            return (time_step+1, p1s, p2s, alphas, betas, us, ue, state)\r\n\r\n        def condition(time_step, p1s, p2s,alphas, betas, us, ue, state):\r\n            return tf.less(time_step, 4)\r\n\r\n\r\n        with tf.variable_scope('dynamic_pointing_decoder'):\r\n            cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\r\n            i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            idx = tf.range(0, tf.shape(U)[0], 1)\r\n            s_idx = tf.stack([idx, i_start], axis=1)\r\n            e_idx = tf.stack([idx, i_end], axis=1)\r\n            us = tf.gather_nd(U, s_idx) #batch 2d\r\n            ue = tf.gather_nd(U, e_idx) #batch 2d\r\n            p1s = tf.TensorArray(tf.int32, size=4)\r\n            p2s = tf.TensorArray(tf.int32, size=4)\r\n            alphas = tf.TensorArray(tf.float32, size=4)\r\n            betas = tf.TensorArray(tf.float32, size=4)\r\n            state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\r\n                                              tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\r\n            U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\r\n            #time_step, p1s, p2s, us, ue, state\r\n            time_step = 0\r\n            time_step, p1s, p2s, alphas, betas, us, ue, state = tf.while_loop(cond=condition, body=body,\r\n                                                                              loop_vars=(time_step, p1s, p2s, alphas, betas, us, ue, state),\r\n                                                                              maximum_iterations=4)\r\n            self.p1s = tf.transpose(p1s.stack()) #batch*4\r\n            self.p2s = tf.transpose(p2s.stack())\r\n            print(\r\n                \"p1s  shape : {0}\".format(np.shape(self.p1s))\r\n            )\r\n            self.p1 = tf.unstack(self.p1s,axis=0)[-1]\r\n            print(\"p1 shape : {0}\".format(np.shape(self.p1)))\r\n            self.p2 = tf.unstack(self.p2s, axis=0)[-1]\r\n            alphas = tf.unstack(alphas.stack(), axis=0)\r\n            betas = tf.unstack(betas.stack(), axis=0)\r\n            print('alphas 0 shape :{0}'.format(np.shape(alphas[0])))\r\n            return alphas, betas\r\n\r\n#no while loop\r\ndef dynamic_pointing_decoder(self, U, mask):\r\n        def _HMN(ut, h, us, ue):\r\n            h_us_ue = tf.concat([h, us, ue], axis=1) #batch,5*d\r\n            WD = tf.get_variable(name=\"WD\", shape=(5 * self._config.hidden_dim, self._config.hidden_dim), dtype=tf.float32,initializer=xavier_initializer())\r\n            r = tf.nn.tanh(tf.matmul(h_us_ue, WD)) #batch, d\r\n            ut_r = tf.concat([ut, r], axis=1) #batch,3d\r\n            W1 = tf.get_variable(name=\"W1\", shape=(3 * self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype='float32', initializer=xavier_initializer())\r\n            b1 = tf.get_variable(name=\"b1_Bias\", shape=(self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt1 = tf.einsum('bt,top->bop', ut_r, W1) + b1\r\n            mt1 = tf.reduce_max(mt1, axis=2) #batch * d\r\n            W2 = tf.get_variable(name=\"W2\", shape=(self._config.hidden_dim, self._config.hidden_dim, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b2 = tf.get_variable(name=\"b2_Bias\", shape=(self._config.hidden_dim,self._config.pool_size), dtype=tf.float32, initializer=tf.zeros_initializer())\r\n            mt2 = tf.einsum('bi,ijp->bjp', mt1, W2) + b2\r\n            mt2 = tf.reduce_max(mt2, axis=2) #batch * d\r\n            mt12 = tf.concat([mt1, mt2], axis=1) # batch * 2d\r\n            W3 = tf.get_variable(name=\"W3\", shape=(2 * self._config.hidden_dim, 1, self._config.pool_size), dtype=tf.float32, initializer=xavier_initializer())\r\n            b3 = tf.get_variable(name=\"b3_Bias\", shape=(1, self._config.pool_size), dtype='float32', initializer=tf.zeros_initializer())\r\n            hmn = tf.einsum('bi,ijp->bjp', mt12, W3) + b3\r\n            hmn = tf.reduce_max(hmn, axis=2) #batch 1\r\n            hmn = tf.reshape(hmn, [-1]) #batch\r\n            return hmn\r\n        with tf.variable_scope('dynamic_pointing_decoder'):\r\n            #single_cell = lambda: CudnnCompatibleLSTMCell(self._config.hidden_dim)\r\n            #cell = tf.nn.rnn_cell.MultiRNNCell([single_cell() for _ in range(1)])\r\n            #cell = tf.nn.rnn_cell.LSTMCell(self._config.hidden_dim)\r\n            cell = CudnnCompatibleLSTMCell(self._config.hidden_dim)\r\n            i_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            i_end = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            #pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            #pre_start = tf.zeros(shape=(tf.shape(U)[0],), dtype=tf.int32)\r\n            idx = tf.range(0, tf.shape(U)[0], 1)\r\n            s_idx = tf.stack([idx, i_start], axis=1)\r\n            e_idx = tf.stack([idx, i_end], axis=1)\r\n            us = tf.gather_nd(U, s_idx) #batch 2d\r\n            ue = tf.gather_nd(U, e_idx) #batch 2d\r\n            alphas, betas = [], []\r\n            state = tf.nn.rnn_cell.LSTMStateTuple(tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32),\r\n                                              tf.zeros(shape=(tf.shape(U)[0], self._config.hidden_dim),dtype=tf.float32))  # initial hidden state of RNN\r\n            U_transpose = tf.transpose(U, [1, 0, 2]) #a batch 2d\r\n            for time_step in range(4):\r\n                if time_step >= 1:\r\n                    tf.get_variable_scope().reuse_variables()\r\n                us_ue = tf.concat([us,ue], axis=1) #batch 4d\r\n                h, state = cell(inputs=us_ue, state=state) #batch * d\r\n\r\n                with tf.variable_scope('alpha_HMN'):\r\n                    if time_step >= 1:\r\n                        tf.get_variable_scope().reuse_variables()\r\n                    alpha = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                    alpha = tf.transpose(alpha, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\r\n                i_start = tf.argmax(alpha, axis=1) #batch\r\n                i_start = tf.cast(i_start, tf.int32)\r\n                s_idx = tf.stack([idx, i_start], axis=1)\r\n                us = tf.gather_nd(U, s_idx) #batch 2d\r\n\r\n                with tf.variable_scope('betas_HMN'):\r\n                    if time_step >= 1:\r\n                        tf.get_variable_scope().reuse_variables()\r\n                    beta = tf.map_fn(lambda ut:_HMN(ut, h, us, ue), U_transpose, dtype=tf.float32)\r\n                    beta = tf.transpose(beta, [1,0]) * tf.cast(mask,dtype=tf.float32) # batch article_len\r\n                i_end = tf.argmax(alpha, axis=1) #batch\r\n                i_end = tf.cast(i_end, tf.int32)\r\n                e_idx = tf.stack([idx, i_end], axis=1)\r\n                ue = tf.gather_nd(U, e_idx) #batch 2d\r\n\r\n                alphas.append(alpha)\r\n                betas.append(beta)\r\n                #if tf.equal(pre_start, i_end) and tf.equal(pre_end, i_end):\r\n                #    break\r\n                #else:\r\n                #    pre_start = i_start\r\n                #    pre_end = i_end\r\n            return alpha, beta\r\n\r\nanyone can help me? Thank you very much\r\n"}