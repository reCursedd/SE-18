{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/359490686", "html_url": "https://github.com/tensorflow/tensorflow/issues/16226#issuecomment-359490686", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16226", "id": 359490686, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTQ5MDY4Ng==", "user": {"login": "jswelch", "id": 35576504, "node_id": "MDQ6VXNlcjM1NTc2NTA0", "avatar_url": "https://avatars3.githubusercontent.com/u/35576504?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jswelch", "html_url": "https://github.com/jswelch", "followers_url": "https://api.github.com/users/jswelch/followers", "following_url": "https://api.github.com/users/jswelch/following{/other_user}", "gists_url": "https://api.github.com/users/jswelch/gists{/gist_id}", "starred_url": "https://api.github.com/users/jswelch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jswelch/subscriptions", "organizations_url": "https://api.github.com/users/jswelch/orgs", "repos_url": "https://api.github.com/users/jswelch/repos", "events_url": "https://api.github.com/users/jswelch/events{/privacy}", "received_events_url": "https://api.github.com/users/jswelch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-22T16:55:35Z", "updated_at": "2018-01-22T16:55:35Z", "author_association": "NONE", "body_html": "<p>Kubeflow can alleviate some of this - if the job is running in a k8s environment. My use case is in HPC - where distributed tensorflow is running under an HPC workload management system such as Slurm, PBSPRo or LSF. When someone runs an MPI application, they don't have to worry about multiple MPI jobs clashing with each other as MPI sets up its communications automatically. Forcing the end user to select (hopefully) free ports on every node really hampers usability.</p>\n<p>I can see where kubeflow/docker's own port mapping can help with a single instance of tensorflow - the app uses the same fixed port inside the container and mapped to some chosen port outside. For distributed tensorflow, conceptually every communication could be to the same port number, and the port mapper handles it -- but we're back to the same problem - the port mapper needs to map unused ports on every host, and the user needs to manually select them. Port mapping would also not work when multiple jobs are sharing the same host - they can't all send to the same port.</p>", "body_text": "Kubeflow can alleviate some of this - if the job is running in a k8s environment. My use case is in HPC - where distributed tensorflow is running under an HPC workload management system such as Slurm, PBSPRo or LSF. When someone runs an MPI application, they don't have to worry about multiple MPI jobs clashing with each other as MPI sets up its communications automatically. Forcing the end user to select (hopefully) free ports on every node really hampers usability.\nI can see where kubeflow/docker's own port mapping can help with a single instance of tensorflow - the app uses the same fixed port inside the container and mapped to some chosen port outside. For distributed tensorflow, conceptually every communication could be to the same port number, and the port mapper handles it -- but we're back to the same problem - the port mapper needs to map unused ports on every host, and the user needs to manually select them. Port mapping would also not work when multiple jobs are sharing the same host - they can't all send to the same port.", "body": "Kubeflow can alleviate some of this - if the job is running in a k8s environment. My use case is in HPC - where distributed tensorflow is running under an HPC workload management system such as Slurm, PBSPRo or LSF. When someone runs an MPI application, they don't have to worry about multiple MPI jobs clashing with each other as MPI sets up its communications automatically. Forcing the end user to select (hopefully) free ports on every node really hampers usability.\r\n\r\nI can see where kubeflow/docker's own port mapping can help with a single instance of tensorflow - the app uses the same fixed port inside the container and mapped to some chosen port outside. For distributed tensorflow, conceptually every communication could be to the same port number, and the port mapper handles it -- but we're back to the same problem - the port mapper needs to map unused ports on every host, and the user needs to manually select them. Port mapping would also not work when multiple jobs are sharing the same host - they can't all send to the same port.\r\n"}