{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433562028", "html_url": "https://github.com/tensorflow/tensorflow/issues/22216#issuecomment-433562028", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22216", "id": 433562028, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzU2MjAyOA==", "user": {"login": "vishalsubbiah", "id": 3079888, "node_id": "MDQ6VXNlcjMwNzk4ODg=", "avatar_url": "https://avatars3.githubusercontent.com/u/3079888?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vishalsubbiah", "html_url": "https://github.com/vishalsubbiah", "followers_url": "https://api.github.com/users/vishalsubbiah/followers", "following_url": "https://api.github.com/users/vishalsubbiah/following{/other_user}", "gists_url": "https://api.github.com/users/vishalsubbiah/gists{/gist_id}", "starred_url": "https://api.github.com/users/vishalsubbiah/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vishalsubbiah/subscriptions", "organizations_url": "https://api.github.com/users/vishalsubbiah/orgs", "repos_url": "https://api.github.com/users/vishalsubbiah/repos", "events_url": "https://api.github.com/users/vishalsubbiah/events{/privacy}", "received_events_url": "https://api.github.com/users/vishalsubbiah/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-26T22:41:40Z", "updated_at": "2018-10-26T22:42:15Z", "author_association": "NONE", "body_html": "<p>I used the xla.compile as seen [here] (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"357442068\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/22102\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/22102/hovercard?comment_id=428684652&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/22102#issuecomment-428684652\">#22102 (comment)</a>) (that is the stack trace when I run it)</p>\n<p>The code I ran is :</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport os\nimport argparse\nfrom tensorflow.python.estimator import run_config\nfrom tensorflow.python.ops.rnn import _transpose_batch_time\nfrom tensorflow.python.training import training\n\nfrom tensorflow.python import debug as tf_debug\nhooks = [tf_debug.LocalCLIDebugHook()]\n\n\n\nBATCH_SIZE = 64#64\nSEQ_LENGTH = 32 #32\nNUM_BATCHES = 1 #for testing on really small datasets\n\n\ndef data_from_file(file, seq_length):\n    \"\"\"Takes params, file, converts to one hot numpy arrays\n    and returns x, y, and vocab_size\"\"\"\n    text = open(file).read()[:10000] # for debugging\n    print(\"Corpus len:\", len(text))\n    chars = sorted(list(set(text)))\n    vocab_size = len(chars)\n    print('total chars:', len(chars))\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n    sentences = []\n    next_chars = []\n    for i in range(0, len(text)-seq_length, 2):\n        sentences.append(text[i:i+seq_length]) #\"the cat in the ha\"\n        next_chars.append(text[i+seq_length])     #\"t\"\n    print(\"Vectorization...\")\n    x = np.zeros((len(sentences), seq_length, vocab_size), dtype=np.float32)\n    y = np.zeros((len(sentences), vocab_size), dtype=np.float32)\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            x[i, t, char_indices[char]] = 1\n        y[i, char_indices[next_chars[i]]] = 1\n    return {'x':x, 'y':y, 'vocab_size':vocab_size,'indices_char':indices_char,'char_indices':char_indices,'text':text}\n\n\ndef input_fn(params, mode):\n    batch_size = params.get('batch_size', BATCH_SIZE)\n    seq_length = params.get('seq_length', SEQ_LENGTH)\n    char_indices = params.get('char_indices', None)\n    num_classes = params.get('num_classes', None)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        x = params.get('x', None)#[:32000]  #[:batch_size*NUM_BATCHES]\n        #print(\"X SHAPE\", x.shape)\n        y = params.get('y', None) #size: (4984, 57) for 10,000 char sample\n        repeat_count = None\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        data_source = params.get('val_data', None)\n        repeat_count = 1\n    elif mode == tf.estimator.ModeKeys.PREDICT:\n        sentence = params.get('seed_text')\n        char_indices = params.get('char_indices')\n\n\n        x = np.zeros((len(sentence)//seq_length, seq_length, num_classes), dtype=np.float32)\n        for t, char in enumerate(sentence):\n            x[0, t, char_indices[char]] = 1\n\n\n        #print(\"X SHAPE: \", x.shape)\n        ds = tf.data.Dataset.from_tensor_slices(x)\n        ds=ds.batch(1)\n        features,labels=ds.make_one_shot_iterator().get_next()\n        #ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n        return features,labels\n\n    assert x is not None, \"Must supply the data\"\n    #print(\"test1\")\n    ds = tf.data.Dataset.from_tensor_slices((x, y))\n    ds = ds.repeat(repeat_count)\n    #print(\"test2\")\n    ds=ds.batch(batch_size)\n    #ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n    features,labels=ds.make_one_shot_iterator().get_next()\n    return features,labels\n\n\ndef lstm_model_fn(features, labels, mode, params):\n    \"\"\" Basic RNN that uses LSTM Cells\n    features = input_tensors: one-hot tensors encoding the chars\n    mode: instance of tf.estimator.ModeKeys (e.g. ModeKeys.TRAIN)\n    rnn_type = GRU, LSTM, BasicRNN\n    params: dict specifying [learning_rate, seq_length, hidden_units]\"\"\"\n    if True:\n    #with tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\n        with tf.variable_scope(\"test_conv\", use_resource=True):\n            num_classes = params.get(\"num_classes\")\n            batch_size = params.get('batch_size', BATCH_SIZE)\n            state_size = params.get(\"state_size\", 128)\n            seq_length = params.get(\"seq_length\", SEQ_LENGTH)\n\n        #     labels = tf.Print(labels, [labels, tf.shape(labels), \"LABELS SHAPE\"])\n\n            # input tensors need to be [batch_size x sentence_size x vocab_size]\n            #lstm_cell = tf.nn.rnn_cell.LSTMCell(state_size,name=\"lstm_cell\",reuse=tf.AUTO_REUSE)#tf.nn.rnn_cell.BasicLSTMCell(state_size)\n            lstm_cell = tf.nn.rnn_cell.BasicRNNCell(state_size,name=\"lstm_cell\",reuse=tf.AUTO_REUSE)\n            initial_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n            logits = None\n            predictions = None\n\n            if mode != tf.estimator.ModeKeys.PREDICT:\n                inputs=features\n                outputs,final_state=tf.contrib.recurrent.functional_rnn(cell=lstm_cell,\n                            inputs=inputs,initial_state=initial_state,scope=\"func\",\n                            sequence_length=seq_length,\n                            dtype= tf.float32,use_tpu=True)\n                #outputs = outputs_ta#.stack()\n\n                #outputs = _transpose_batch_time(outputs)\n        #         outputs = tf.Print(outputs, [outputs, tf.shape(outputs), \"outputs SHAPE\"])\n                with tf.variable_scope(\"rnn_final\",use_resource=True): #to allow prediction to call same function\n                    logits = tf.layers.dense(\n                        outputs, num_classes, name=\"final_layer\") #tf.reshape(outputs, [-1 , state_size])\n\n                logit_loss = logits[:,-1,:] #(64, 57) gets the prediction from the whole sequence\n                logit_loss = tf.reshape(logit_loss, (batch_size, num_classes))\n\n\n                predictions = tf.argmax(logits, axis=1,name=\"predictions\")\n\n            elif mode == tf.estimator.ModeKeys.PREDICT:\n                seed_length = len(params.get('seed_text'))\n                target_length = params.get('target_length')\n                inputs = features\n\n                outputs_ta,final_state=tf.contrib.recurrent.functional_rnn(lstm_cell,inputs,initial_state=initial_state,scope=\"func\",use_tpu=True)\n                predictions = outputs_ta.stack() #outputs of this are partly one-hot, partly log probs?\n\n\n                predictions = tf.Print(predictions, [predictions[90], \"PREDICTIONS\"])\n                return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n        #     acc_labels = tf.reshape(labels, [-1, 1]) #disabling accuracy since shapes mismatched\n        #     accuracy = tf.metrics.accuracy(labels=acc_labels, predictions=predictions, name=\"acc_op\")\n\n            loss = tf.reduce_mean(\n                    tf.nn.softmax_cross_entropy_with_logits(\n                    logits=logit_loss, labels=labels)) #tf.reshape(labels, [-1]))\n\n            optimizer = tf.train.MomentumOptimizer(learning_rate=0.15, momentum=0.97)\n            #     optimizer = tf.train.AdamOptimizer(learning_rate=0.15)\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n        #     metrics = {\"accuracy\": accuracy}\n        #     tf.summary.scalar('accuracy', accuracy[1])\n\n            #specify what to do with each mode\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                return tf.estimator.EstimatorSpec(\n                    mode=mode, loss=loss, train_op=train_op)\n            elif mode == tf.estimator.ModeKeys.EVAL:\n                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=metrics)\n\n\n\nif __name__ == \"__main__\":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--train_file',\n        type=str,\n        default=os.path.expanduser(\"./input.txt\"), #currently only works with 'input.txt'?\n        help='where pre-processed data is stored')\n    parser.add_argument(\n        '--mode',\n        type=str,\n        default='train',\n        help='train, eval, or predict')\n    parser.add_argument(\n        '--model_dir',\n        type=str,\n        default=\"/tmp/char_rnn\",\n        help='where to store model results')\n    parser.add_argument(\n        '--num_states', type=int, default=128, help='size of each rnn layer')\n    parser.add_argument(\n        '--seed_text', type=str, default=\"The Volsces have much corn; take\", help='seed for prediction mode')\n    parser.add_argument(\n        '--target_length', type=int, default=100, help='the output length of your given prediction')\n    parser.add_argument(\n        '--seq_length', type=int, default=SEQ_LENGTH, help='number of unrolled time steps')\n    parser.add_argument(\n        '--num_steps', type=int, default=10, help='number of training steps')\n    parser.add_argument(\n        '--batch_size', type=int, default=BATCH_SIZE, help=\"samples per batch\")\n    args = parser.parse_args()\n\n    data = data_from_file(args.train_file, seq_length=args.seq_length)\n    #{'x':x, 'y':y, 'vocab_size':vocab_size,'indices_char':indices_char,'char_indices':char_indices,'text':text}\n\n    classifier = tf.estimator.Estimator(\n        model_fn=lstm_model_fn,\n        params={\n            'num_classes': data[\"vocab_size\"],\n            'state_size': args.num_states,\n            'batch_size': args.batch_size,\n            'seq_length': args.seq_length,\n            'x': data[\"x\"],\n            'y': data['y'],\n            'char_indices': data[\"char_indices\"],\n            'seed_text': args.seed_text,\n            'target_length': args.target_length,\n        },\n        config=run_config.RunConfig(\n            save_summary_steps=10,\n            save_checkpoints_steps=100,\n            model_dir=\"./model\"))\n\n\n\nfrom tensorflow.contrib.compiler.xla import compile\n\ndef run(features,labels):\n    mode=tf.estimator.ModeKeys.TRAIN\n    params=classifier.params\n    net=lstm_model_fn(features, labels, mode, params)\n    graph = tf.get_default_graph()\n    list_tensors=[]\n    for op in graph.get_operations():\n        if(len(op.values())&gt;0):\n            list_tensors.append(op.values()[0])\n\n    return list_tensors+tf.get_default_graph().get_operations()\n\nfeatures,labels=input_fn(classifier.params,tf.estimator.ModeKeys.TRAIN)\n#features,labels=csv_input_fn(train_path, train_batch_size)\nresult=compile(run,inputs=[features,labels])\n\nwith tf.Session() as sess:\n   sess.run(tf.global_variables_initializer())\n   sess.run(result)\n</code></pre>\n<p>Hope this helps</p>", "body_text": "I used the xla.compile as seen [here] (#22102 (comment)) (that is the stack trace when I run it)\nThe code I ran is :\nimport tensorflow as tf\nimport numpy as np\nimport os\nimport argparse\nfrom tensorflow.python.estimator import run_config\nfrom tensorflow.python.ops.rnn import _transpose_batch_time\nfrom tensorflow.python.training import training\n\nfrom tensorflow.python import debug as tf_debug\nhooks = [tf_debug.LocalCLIDebugHook()]\n\n\n\nBATCH_SIZE = 64#64\nSEQ_LENGTH = 32 #32\nNUM_BATCHES = 1 #for testing on really small datasets\n\n\ndef data_from_file(file, seq_length):\n    \"\"\"Takes params, file, converts to one hot numpy arrays\n    and returns x, y, and vocab_size\"\"\"\n    text = open(file).read()[:10000] # for debugging\n    print(\"Corpus len:\", len(text))\n    chars = sorted(list(set(text)))\n    vocab_size = len(chars)\n    print('total chars:', len(chars))\n    char_indices = dict((c, i) for i, c in enumerate(chars))\n    indices_char = dict((i, c) for i, c in enumerate(chars))\n    sentences = []\n    next_chars = []\n    for i in range(0, len(text)-seq_length, 2):\n        sentences.append(text[i:i+seq_length]) #\"the cat in the ha\"\n        next_chars.append(text[i+seq_length])     #\"t\"\n    print(\"Vectorization...\")\n    x = np.zeros((len(sentences), seq_length, vocab_size), dtype=np.float32)\n    y = np.zeros((len(sentences), vocab_size), dtype=np.float32)\n    for i, sentence in enumerate(sentences):\n        for t, char in enumerate(sentence):\n            x[i, t, char_indices[char]] = 1\n        y[i, char_indices[next_chars[i]]] = 1\n    return {'x':x, 'y':y, 'vocab_size':vocab_size,'indices_char':indices_char,'char_indices':char_indices,'text':text}\n\n\ndef input_fn(params, mode):\n    batch_size = params.get('batch_size', BATCH_SIZE)\n    seq_length = params.get('seq_length', SEQ_LENGTH)\n    char_indices = params.get('char_indices', None)\n    num_classes = params.get('num_classes', None)\n    if mode == tf.estimator.ModeKeys.TRAIN:\n        x = params.get('x', None)#[:32000]  #[:batch_size*NUM_BATCHES]\n        #print(\"X SHAPE\", x.shape)\n        y = params.get('y', None) #size: (4984, 57) for 10,000 char sample\n        repeat_count = None\n    elif mode == tf.estimator.ModeKeys.EVAL:\n        data_source = params.get('val_data', None)\n        repeat_count = 1\n    elif mode == tf.estimator.ModeKeys.PREDICT:\n        sentence = params.get('seed_text')\n        char_indices = params.get('char_indices')\n\n\n        x = np.zeros((len(sentence)//seq_length, seq_length, num_classes), dtype=np.float32)\n        for t, char in enumerate(sentence):\n            x[0, t, char_indices[char]] = 1\n\n\n        #print(\"X SHAPE: \", x.shape)\n        ds = tf.data.Dataset.from_tensor_slices(x)\n        ds=ds.batch(1)\n        features,labels=ds.make_one_shot_iterator().get_next()\n        #ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n        return features,labels\n\n    assert x is not None, \"Must supply the data\"\n    #print(\"test1\")\n    ds = tf.data.Dataset.from_tensor_slices((x, y))\n    ds = ds.repeat(repeat_count)\n    #print(\"test2\")\n    ds=ds.batch(batch_size)\n    #ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\n    features,labels=ds.make_one_shot_iterator().get_next()\n    return features,labels\n\n\ndef lstm_model_fn(features, labels, mode, params):\n    \"\"\" Basic RNN that uses LSTM Cells\n    features = input_tensors: one-hot tensors encoding the chars\n    mode: instance of tf.estimator.ModeKeys (e.g. ModeKeys.TRAIN)\n    rnn_type = GRU, LSTM, BasicRNN\n    params: dict specifying [learning_rate, seq_length, hidden_units]\"\"\"\n    if True:\n    #with tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\n        with tf.variable_scope(\"test_conv\", use_resource=True):\n            num_classes = params.get(\"num_classes\")\n            batch_size = params.get('batch_size', BATCH_SIZE)\n            state_size = params.get(\"state_size\", 128)\n            seq_length = params.get(\"seq_length\", SEQ_LENGTH)\n\n        #     labels = tf.Print(labels, [labels, tf.shape(labels), \"LABELS SHAPE\"])\n\n            # input tensors need to be [batch_size x sentence_size x vocab_size]\n            #lstm_cell = tf.nn.rnn_cell.LSTMCell(state_size,name=\"lstm_cell\",reuse=tf.AUTO_REUSE)#tf.nn.rnn_cell.BasicLSTMCell(state_size)\n            lstm_cell = tf.nn.rnn_cell.BasicRNNCell(state_size,name=\"lstm_cell\",reuse=tf.AUTO_REUSE)\n            initial_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n            logits = None\n            predictions = None\n\n            if mode != tf.estimator.ModeKeys.PREDICT:\n                inputs=features\n                outputs,final_state=tf.contrib.recurrent.functional_rnn(cell=lstm_cell,\n                            inputs=inputs,initial_state=initial_state,scope=\"func\",\n                            sequence_length=seq_length,\n                            dtype= tf.float32,use_tpu=True)\n                #outputs = outputs_ta#.stack()\n\n                #outputs = _transpose_batch_time(outputs)\n        #         outputs = tf.Print(outputs, [outputs, tf.shape(outputs), \"outputs SHAPE\"])\n                with tf.variable_scope(\"rnn_final\",use_resource=True): #to allow prediction to call same function\n                    logits = tf.layers.dense(\n                        outputs, num_classes, name=\"final_layer\") #tf.reshape(outputs, [-1 , state_size])\n\n                logit_loss = logits[:,-1,:] #(64, 57) gets the prediction from the whole sequence\n                logit_loss = tf.reshape(logit_loss, (batch_size, num_classes))\n\n\n                predictions = tf.argmax(logits, axis=1,name=\"predictions\")\n\n            elif mode == tf.estimator.ModeKeys.PREDICT:\n                seed_length = len(params.get('seed_text'))\n                target_length = params.get('target_length')\n                inputs = features\n\n                outputs_ta,final_state=tf.contrib.recurrent.functional_rnn(lstm_cell,inputs,initial_state=initial_state,scope=\"func\",use_tpu=True)\n                predictions = outputs_ta.stack() #outputs of this are partly one-hot, partly log probs?\n\n\n                predictions = tf.Print(predictions, [predictions[90], \"PREDICTIONS\"])\n                return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n        #     acc_labels = tf.reshape(labels, [-1, 1]) #disabling accuracy since shapes mismatched\n        #     accuracy = tf.metrics.accuracy(labels=acc_labels, predictions=predictions, name=\"acc_op\")\n\n            loss = tf.reduce_mean(\n                    tf.nn.softmax_cross_entropy_with_logits(\n                    logits=logit_loss, labels=labels)) #tf.reshape(labels, [-1]))\n\n            optimizer = tf.train.MomentumOptimizer(learning_rate=0.15, momentum=0.97)\n            #     optimizer = tf.train.AdamOptimizer(learning_rate=0.15)\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n\n        #     metrics = {\"accuracy\": accuracy}\n        #     tf.summary.scalar('accuracy', accuracy[1])\n\n            #specify what to do with each mode\n            if mode == tf.estimator.ModeKeys.TRAIN:\n                return tf.estimator.EstimatorSpec(\n                    mode=mode, loss=loss, train_op=train_op)\n            elif mode == tf.estimator.ModeKeys.EVAL:\n                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=metrics)\n\n\n\nif __name__ == \"__main__\":\n    tf.logging.set_verbosity(tf.logging.INFO)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--train_file',\n        type=str,\n        default=os.path.expanduser(\"./input.txt\"), #currently only works with 'input.txt'?\n        help='where pre-processed data is stored')\n    parser.add_argument(\n        '--mode',\n        type=str,\n        default='train',\n        help='train, eval, or predict')\n    parser.add_argument(\n        '--model_dir',\n        type=str,\n        default=\"/tmp/char_rnn\",\n        help='where to store model results')\n    parser.add_argument(\n        '--num_states', type=int, default=128, help='size of each rnn layer')\n    parser.add_argument(\n        '--seed_text', type=str, default=\"The Volsces have much corn; take\", help='seed for prediction mode')\n    parser.add_argument(\n        '--target_length', type=int, default=100, help='the output length of your given prediction')\n    parser.add_argument(\n        '--seq_length', type=int, default=SEQ_LENGTH, help='number of unrolled time steps')\n    parser.add_argument(\n        '--num_steps', type=int, default=10, help='number of training steps')\n    parser.add_argument(\n        '--batch_size', type=int, default=BATCH_SIZE, help=\"samples per batch\")\n    args = parser.parse_args()\n\n    data = data_from_file(args.train_file, seq_length=args.seq_length)\n    #{'x':x, 'y':y, 'vocab_size':vocab_size,'indices_char':indices_char,'char_indices':char_indices,'text':text}\n\n    classifier = tf.estimator.Estimator(\n        model_fn=lstm_model_fn,\n        params={\n            'num_classes': data[\"vocab_size\"],\n            'state_size': args.num_states,\n            'batch_size': args.batch_size,\n            'seq_length': args.seq_length,\n            'x': data[\"x\"],\n            'y': data['y'],\n            'char_indices': data[\"char_indices\"],\n            'seed_text': args.seed_text,\n            'target_length': args.target_length,\n        },\n        config=run_config.RunConfig(\n            save_summary_steps=10,\n            save_checkpoints_steps=100,\n            model_dir=\"./model\"))\n\n\n\nfrom tensorflow.contrib.compiler.xla import compile\n\ndef run(features,labels):\n    mode=tf.estimator.ModeKeys.TRAIN\n    params=classifier.params\n    net=lstm_model_fn(features, labels, mode, params)\n    graph = tf.get_default_graph()\n    list_tensors=[]\n    for op in graph.get_operations():\n        if(len(op.values())>0):\n            list_tensors.append(op.values()[0])\n\n    return list_tensors+tf.get_default_graph().get_operations()\n\nfeatures,labels=input_fn(classifier.params,tf.estimator.ModeKeys.TRAIN)\n#features,labels=csv_input_fn(train_path, train_batch_size)\nresult=compile(run,inputs=[features,labels])\n\nwith tf.Session() as sess:\n   sess.run(tf.global_variables_initializer())\n   sess.run(result)\n\nHope this helps", "body": "I used the xla.compile as seen [here] (https://github.com/tensorflow/tensorflow/issues/22102#issuecomment-428684652) (that is the stack trace when I run it)\r\n\r\nThe code I ran is : \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport os\r\nimport argparse\r\nfrom tensorflow.python.estimator import run_config\r\nfrom tensorflow.python.ops.rnn import _transpose_batch_time\r\nfrom tensorflow.python.training import training\r\n\r\nfrom tensorflow.python import debug as tf_debug\r\nhooks = [tf_debug.LocalCLIDebugHook()]\r\n\r\n\r\n\r\nBATCH_SIZE = 64#64\r\nSEQ_LENGTH = 32 #32\r\nNUM_BATCHES = 1 #for testing on really small datasets\r\n\r\n\r\ndef data_from_file(file, seq_length):\r\n    \"\"\"Takes params, file, converts to one hot numpy arrays\r\n    and returns x, y, and vocab_size\"\"\"\r\n    text = open(file).read()[:10000] # for debugging\r\n    print(\"Corpus len:\", len(text))\r\n    chars = sorted(list(set(text)))\r\n    vocab_size = len(chars)\r\n    print('total chars:', len(chars))\r\n    char_indices = dict((c, i) for i, c in enumerate(chars))\r\n    indices_char = dict((i, c) for i, c in enumerate(chars))\r\n    sentences = []\r\n    next_chars = []\r\n    for i in range(0, len(text)-seq_length, 2):\r\n        sentences.append(text[i:i+seq_length]) #\"the cat in the ha\"\r\n        next_chars.append(text[i+seq_length])     #\"t\"\r\n    print(\"Vectorization...\")\r\n    x = np.zeros((len(sentences), seq_length, vocab_size), dtype=np.float32)\r\n    y = np.zeros((len(sentences), vocab_size), dtype=np.float32)\r\n    for i, sentence in enumerate(sentences):\r\n        for t, char in enumerate(sentence):\r\n            x[i, t, char_indices[char]] = 1\r\n        y[i, char_indices[next_chars[i]]] = 1\r\n    return {'x':x, 'y':y, 'vocab_size':vocab_size,'indices_char':indices_char,'char_indices':char_indices,'text':text}\r\n\r\n\r\ndef input_fn(params, mode):\r\n    batch_size = params.get('batch_size', BATCH_SIZE)\r\n    seq_length = params.get('seq_length', SEQ_LENGTH)\r\n    char_indices = params.get('char_indices', None)\r\n    num_classes = params.get('num_classes', None)\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        x = params.get('x', None)#[:32000]  #[:batch_size*NUM_BATCHES]\r\n        #print(\"X SHAPE\", x.shape)\r\n        y = params.get('y', None) #size: (4984, 57) for 10,000 char sample\r\n        repeat_count = None\r\n    elif mode == tf.estimator.ModeKeys.EVAL:\r\n        data_source = params.get('val_data', None)\r\n        repeat_count = 1\r\n    elif mode == tf.estimator.ModeKeys.PREDICT:\r\n        sentence = params.get('seed_text')\r\n        char_indices = params.get('char_indices')\r\n\r\n\r\n        x = np.zeros((len(sentence)//seq_length, seq_length, num_classes), dtype=np.float32)\r\n        for t, char in enumerate(sentence):\r\n            x[0, t, char_indices[char]] = 1\r\n\r\n\r\n        #print(\"X SHAPE: \", x.shape)\r\n        ds = tf.data.Dataset.from_tensor_slices(x)\r\n        ds=ds.batch(1)\r\n        features,labels=ds.make_one_shot_iterator().get_next()\r\n        #ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n        return features,labels\r\n\r\n    assert x is not None, \"Must supply the data\"\r\n    #print(\"test1\")\r\n    ds = tf.data.Dataset.from_tensor_slices((x, y))\r\n    ds = ds.repeat(repeat_count)\r\n    #print(\"test2\")\r\n    ds=ds.batch(batch_size)\r\n    #ds = ds.apply(tf.contrib.data.batch_and_drop_remainder(batch_size))\r\n    features,labels=ds.make_one_shot_iterator().get_next()\r\n    return features,labels\r\n\r\n\r\ndef lstm_model_fn(features, labels, mode, params):\r\n    \"\"\" Basic RNN that uses LSTM Cells\r\n    features = input_tensors: one-hot tensors encoding the chars\r\n    mode: instance of tf.estimator.ModeKeys (e.g. ModeKeys.TRAIN)\r\n    rnn_type = GRU, LSTM, BasicRNN\r\n    params: dict specifying [learning_rate, seq_length, hidden_units]\"\"\"\r\n    if True:\r\n    #with tf.device(\"/job:localhost/replica:0/task:0/device:XLA_CPU:0\"):\r\n        with tf.variable_scope(\"test_conv\", use_resource=True):\r\n            num_classes = params.get(\"num_classes\")\r\n            batch_size = params.get('batch_size', BATCH_SIZE)\r\n            state_size = params.get(\"state_size\", 128)\r\n            seq_length = params.get(\"seq_length\", SEQ_LENGTH)\r\n\r\n        #     labels = tf.Print(labels, [labels, tf.shape(labels), \"LABELS SHAPE\"])\r\n\r\n            # input tensors need to be [batch_size x sentence_size x vocab_size]\r\n            #lstm_cell = tf.nn.rnn_cell.LSTMCell(state_size,name=\"lstm_cell\",reuse=tf.AUTO_REUSE)#tf.nn.rnn_cell.BasicLSTMCell(state_size)\r\n            lstm_cell = tf.nn.rnn_cell.BasicRNNCell(state_size,name=\"lstm_cell\",reuse=tf.AUTO_REUSE)\r\n            initial_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\r\n            logits = None\r\n            predictions = None\r\n\r\n            if mode != tf.estimator.ModeKeys.PREDICT:\r\n                inputs=features\r\n                outputs,final_state=tf.contrib.recurrent.functional_rnn(cell=lstm_cell,\r\n                            inputs=inputs,initial_state=initial_state,scope=\"func\",\r\n                            sequence_length=seq_length,\r\n                            dtype= tf.float32,use_tpu=True)\r\n                #outputs = outputs_ta#.stack()\r\n\r\n                #outputs = _transpose_batch_time(outputs)\r\n        #         outputs = tf.Print(outputs, [outputs, tf.shape(outputs), \"outputs SHAPE\"])\r\n                with tf.variable_scope(\"rnn_final\",use_resource=True): #to allow prediction to call same function\r\n                    logits = tf.layers.dense(\r\n                        outputs, num_classes, name=\"final_layer\") #tf.reshape(outputs, [-1 , state_size])\r\n\r\n                logit_loss = logits[:,-1,:] #(64, 57) gets the prediction from the whole sequence\r\n                logit_loss = tf.reshape(logit_loss, (batch_size, num_classes))\r\n\r\n\r\n                predictions = tf.argmax(logits, axis=1,name=\"predictions\")\r\n\r\n            elif mode == tf.estimator.ModeKeys.PREDICT:\r\n                seed_length = len(params.get('seed_text'))\r\n                target_length = params.get('target_length')\r\n                inputs = features\r\n\r\n                outputs_ta,final_state=tf.contrib.recurrent.functional_rnn(lstm_cell,inputs,initial_state=initial_state,scope=\"func\",use_tpu=True)\r\n                predictions = outputs_ta.stack() #outputs of this are partly one-hot, partly log probs?\r\n\r\n\r\n                predictions = tf.Print(predictions, [predictions[90], \"PREDICTIONS\"])\r\n                return tf.estimator.EstimatorSpec(mode, predictions=predictions)\r\n\r\n        #     acc_labels = tf.reshape(labels, [-1, 1]) #disabling accuracy since shapes mismatched\r\n        #     accuracy = tf.metrics.accuracy(labels=acc_labels, predictions=predictions, name=\"acc_op\")\r\n\r\n            loss = tf.reduce_mean(\r\n                    tf.nn.softmax_cross_entropy_with_logits(\r\n                    logits=logit_loss, labels=labels)) #tf.reshape(labels, [-1]))\r\n\r\n            optimizer = tf.train.MomentumOptimizer(learning_rate=0.15, momentum=0.97)\r\n            #     optimizer = tf.train.AdamOptimizer(learning_rate=0.15)\r\n            train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\r\n\r\n        #     metrics = {\"accuracy\": accuracy}\r\n        #     tf.summary.scalar('accuracy', accuracy[1])\r\n\r\n            #specify what to do with each mode\r\n            if mode == tf.estimator.ModeKeys.TRAIN:\r\n                return tf.estimator.EstimatorSpec(\r\n                    mode=mode, loss=loss, train_op=train_op)\r\n            elif mode == tf.estimator.ModeKeys.EVAL:\r\n                return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=metrics)\r\n\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        '--train_file',\r\n        type=str,\r\n        default=os.path.expanduser(\"./input.txt\"), #currently only works with 'input.txt'?\r\n        help='where pre-processed data is stored')\r\n    parser.add_argument(\r\n        '--mode',\r\n        type=str,\r\n        default='train',\r\n        help='train, eval, or predict')\r\n    parser.add_argument(\r\n        '--model_dir',\r\n        type=str,\r\n        default=\"/tmp/char_rnn\",\r\n        help='where to store model results')\r\n    parser.add_argument(\r\n        '--num_states', type=int, default=128, help='size of each rnn layer')\r\n    parser.add_argument(\r\n        '--seed_text', type=str, default=\"The Volsces have much corn; take\", help='seed for prediction mode')\r\n    parser.add_argument(\r\n        '--target_length', type=int, default=100, help='the output length of your given prediction')\r\n    parser.add_argument(\r\n        '--seq_length', type=int, default=SEQ_LENGTH, help='number of unrolled time steps')\r\n    parser.add_argument(\r\n        '--num_steps', type=int, default=10, help='number of training steps')\r\n    parser.add_argument(\r\n        '--batch_size', type=int, default=BATCH_SIZE, help=\"samples per batch\")\r\n    args = parser.parse_args()\r\n\r\n    data = data_from_file(args.train_file, seq_length=args.seq_length)\r\n    #{'x':x, 'y':y, 'vocab_size':vocab_size,'indices_char':indices_char,'char_indices':char_indices,'text':text}\r\n\r\n    classifier = tf.estimator.Estimator(\r\n        model_fn=lstm_model_fn,\r\n        params={\r\n            'num_classes': data[\"vocab_size\"],\r\n            'state_size': args.num_states,\r\n            'batch_size': args.batch_size,\r\n            'seq_length': args.seq_length,\r\n            'x': data[\"x\"],\r\n            'y': data['y'],\r\n            'char_indices': data[\"char_indices\"],\r\n            'seed_text': args.seed_text,\r\n            'target_length': args.target_length,\r\n        },\r\n        config=run_config.RunConfig(\r\n            save_summary_steps=10,\r\n            save_checkpoints_steps=100,\r\n            model_dir=\"./model\"))\r\n\r\n\r\n\r\nfrom tensorflow.contrib.compiler.xla import compile\r\n\r\ndef run(features,labels):\r\n    mode=tf.estimator.ModeKeys.TRAIN\r\n    params=classifier.params\r\n    net=lstm_model_fn(features, labels, mode, params)\r\n    graph = tf.get_default_graph()\r\n    list_tensors=[]\r\n    for op in graph.get_operations():\r\n        if(len(op.values())>0):\r\n            list_tensors.append(op.values()[0])\r\n\r\n    return list_tensors+tf.get_default_graph().get_operations()\r\n\r\nfeatures,labels=input_fn(classifier.params,tf.estimator.ModeKeys.TRAIN)\r\n#features,labels=csv_input_fn(train_path, train_batch_size)\r\nresult=compile(run,inputs=[features,labels])\r\n\r\nwith tf.Session() as sess:\r\n   sess.run(tf.global_variables_initializer())\r\n   sess.run(result)\r\n```\r\n\r\nHope this helps \r\n\r\n"}