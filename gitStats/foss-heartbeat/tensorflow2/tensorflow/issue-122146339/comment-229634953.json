{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/229634953", "html_url": "https://github.com/tensorflow/tensorflow/issues/511#issuecomment-229634953", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511", "id": 229634953, "node_id": "MDEyOklzc3VlQ29tbWVudDIyOTYzNDk1Mw==", "user": {"login": "qingzew", "id": 3603171, "node_id": "MDQ6VXNlcjM2MDMxNzE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3603171?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qingzew", "html_url": "https://github.com/qingzew", "followers_url": "https://api.github.com/users/qingzew/followers", "following_url": "https://api.github.com/users/qingzew/following{/other_user}", "gists_url": "https://api.github.com/users/qingzew/gists{/gist_id}", "starred_url": "https://api.github.com/users/qingzew/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qingzew/subscriptions", "organizations_url": "https://api.github.com/users/qingzew/orgs", "repos_url": "https://api.github.com/users/qingzew/repos", "events_url": "https://api.github.com/users/qingzew/events{/privacy}", "received_events_url": "https://api.github.com/users/qingzew/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-30T11:42:22Z", "updated_at": "2016-06-30T11:42:57Z", "author_association": "NONE", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>create encoder...<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lstm_encoder<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope:\n                initializer <span class=\"pl-k\">=</span> tf.random_uniform_initializer(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.)\n                lstm_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.LSTMCell(<span class=\"pl-c1\">self</span>.hidden_units, <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> initializer, <span class=\"pl-v\">state_is_tuple</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.is_training <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.keep_prob <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1</span>:\n                    lstm_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.DropoutWrapper(lstm_cell, <span class=\"pl-v\">output_keep_prob</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.keep_prob)\n\n                cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell([lstm_cell] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.num_layers, <span class=\"pl-v\">state_is_tuple</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n\n                initial_state <span class=\"pl-k\">=</span> cell.zero_state(batch_size, tf.float32)\n\n                encoder_outputs, _ <span class=\"pl-k\">=</span> tf.nn.rnn(cell, inputs, <span class=\"pl-v\">initial_state</span> <span class=\"pl-k\">=</span> initial_state, <span class=\"pl-v\">sequence_length</span> <span class=\"pl-k\">=</span> early_stops)\n\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>create decoder...<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lstm_decoder<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> scope:\n                initializer <span class=\"pl-k\">=</span> tf.random_uniform_initializer(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>.)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer)</span>\n                lstm_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.LSTMCell(<span class=\"pl-c1\">self</span>.hidden_units, <span class=\"pl-v\">initializer</span> <span class=\"pl-k\">=</span> initializer, <span class=\"pl-v\">state_is_tuple</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n                <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.is_training <span class=\"pl-k\">and</span> <span class=\"pl-c1\">self</span>.keep_prob <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">1</span>:\n                    lstm_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.DropoutWrapper(lstm_cell, <span class=\"pl-v\">output_keep_prob</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.keep_prob)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers)</span>\n                cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell([lstm_cell] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.num_layers, <span class=\"pl-v\">state_is_tuple</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n\n                state <span class=\"pl-k\">=</span> cell.zero_state(batch_size, tf.float32)\n\n                weight_input <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_with_weight_decay(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight_context<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">self</span>.hidden_units, <span class=\"pl-c1\">1</span>])\n                weight_context <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_with_weight_decay(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight_input<span class=\"pl-pds\">'</span></span>, [cell.state_size[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">1</span>])\n\n                <span class=\"pl-k\">for</span> x <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_steps):\n                    <span class=\"pl-k\">if</span> x <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n                        tf.get_variable_scope().reuse_variables()\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> tanh</span>\n                    w <span class=\"pl-k\">=</span> []\n                    <span class=\"pl-k\">for</span> y <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_steps):\n                        w.append(tf.tanh(tf.matmul(encoder_outputs[y], weight_input) <span class=\"pl-k\">+</span> tf.matmul(state[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">0</span>], weight_context)))\n\n                    <span class=\"pl-c\"><span class=\"pl-c\">#</span> softmax</span>\n                    w <span class=\"pl-k\">=</span> tf.reshape(tf.pack(w), [num_steps])\n                    w <span class=\"pl-k\">=</span> tf.exp(w <span class=\"pl-k\">-</span> tf.reduce_max(w))\n                    w <span class=\"pl-k\">/=</span> tf.reduce_sum(w)\n\n                    input_x <span class=\"pl-k\">=</span> tf.zeros_like(encoder_outputs[<span class=\"pl-c1\">0</span>])\n                    <span class=\"pl-k\">for</span> y <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_steps):\n                        input_x <span class=\"pl-k\">+=</span> encoder_outputs[y] <span class=\"pl-k\">*</span> w[y]\n\n                    _, state  <span class=\"pl-k\">=</span> cell(input_x, state)\n</pre></div>\n<p>I write a endocer-decoder, the decoder cost too much time,  still can't run, and there's no mistakes, anyway to modify this decoder?</p>", "body_text": "with tf.device('/cpu:0'):\n            print('create encoder...')\n            with tf.variable_scope(\"lstm_encoder\") as scope:\n                initializer = tf.random_uniform_initializer(-1., 1.)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers, state_is_tuple = True)\n\n                initial_state = cell.zero_state(batch_size, tf.float32)\n\n                encoder_outputs, _ = tf.nn.rnn(cell, inputs, initial_state = initial_state, sequence_length = early_stops)\n\n            print('create decoder...')\n            with tf.variable_scope(\"lstm_decoder\") as scope:\n                initializer = tf.random_uniform_initializer(-1., 1.)\n                # lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n                # cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers)\n                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers, state_is_tuple = True)\n\n                state = cell.zero_state(batch_size, tf.float32)\n\n                weight_input = self._variable_with_weight_decay('weight_context', [self.hidden_units, 1])\n                weight_context = self._variable_with_weight_decay('weight_input', [cell.state_size[0][0], 1])\n\n                for x in range(num_steps):\n                    if x > 0:\n                        tf.get_variable_scope().reuse_variables()\n                    # tanh\n                    w = []\n                    for y in range(num_steps):\n                        w.append(tf.tanh(tf.matmul(encoder_outputs[y], weight_input) + tf.matmul(state[0][0], weight_context)))\n\n                    # softmax\n                    w = tf.reshape(tf.pack(w), [num_steps])\n                    w = tf.exp(w - tf.reduce_max(w))\n                    w /= tf.reduce_sum(w)\n\n                    input_x = tf.zeros_like(encoder_outputs[0])\n                    for y in range(num_steps):\n                        input_x += encoder_outputs[y] * w[y]\n\n                    _, state  = cell(input_x, state)\n\nI write a endocer-decoder, the decoder cost too much time,  still can't run, and there's no mistakes, anyway to modify this decoder?", "body": "``` python\nwith tf.device('/cpu:0'):\n            print('create encoder...')\n            with tf.variable_scope(\"lstm_encoder\") as scope:\n                initializer = tf.random_uniform_initializer(-1., 1.)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers, state_is_tuple = True)\n\n                initial_state = cell.zero_state(batch_size, tf.float32)\n\n                encoder_outputs, _ = tf.nn.rnn(cell, inputs, initial_state = initial_state, sequence_length = early_stops)\n\n            print('create decoder...')\n            with tf.variable_scope(\"lstm_decoder\") as scope:\n                initializer = tf.random_uniform_initializer(-1., 1.)\n                # lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer)\n                lstm_cell = tf.nn.rnn_cell.LSTMCell(self.hidden_units, initializer = initializer, state_is_tuple = True)\n                if self.is_training and self.keep_prob < 1:\n                    lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob = self.keep_prob)\n\n                # cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers)\n                cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * self.num_layers, state_is_tuple = True)\n\n                state = cell.zero_state(batch_size, tf.float32)\n\n                weight_input = self._variable_with_weight_decay('weight_context', [self.hidden_units, 1])\n                weight_context = self._variable_with_weight_decay('weight_input', [cell.state_size[0][0], 1])\n\n                for x in range(num_steps):\n                    if x > 0:\n                        tf.get_variable_scope().reuse_variables()\n                    # tanh\n                    w = []\n                    for y in range(num_steps):\n                        w.append(tf.tanh(tf.matmul(encoder_outputs[y], weight_input) + tf.matmul(state[0][0], weight_context)))\n\n                    # softmax\n                    w = tf.reshape(tf.pack(w), [num_steps])\n                    w = tf.exp(w - tf.reduce_max(w))\n                    w /= tf.reduce_sum(w)\n\n                    input_x = tf.zeros_like(encoder_outputs[0])\n                    for y in range(num_steps):\n                        input_x += encoder_outputs[y] * w[y]\n\n                    _, state  = cell(input_x, state)\n\n```\n\nI write a endocer-decoder, the decoder cost too much time,  still can't run, and there's no mistakes, anyway to modify this decoder?\n"}