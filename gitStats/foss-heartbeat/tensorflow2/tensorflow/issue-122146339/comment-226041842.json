{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226041842", "html_url": "https://github.com/tensorflow/tensorflow/issues/511#issuecomment-226041842", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511", "id": 226041842, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjA0MTg0Mg==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T23:01:32Z", "updated_at": "2016-06-14T23:01:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13108255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lightingghost\">@lightingghost</a> just to comment on this as I have heavily worked on seq2seq in tensorflow. I have found that I need at minimum 24gb of ram to have a working model of the timesteps you're describing. The demands go up even further to 32 or 64gb for large sized models (2048 sizes). Your 16gb limitation seems normal to me.</p>", "body_text": "@lightingghost just to comment on this as I have heavily worked on seq2seq in tensorflow. I have found that I need at minimum 24gb of ram to have a working model of the timesteps you're describing. The demands go up even further to 32 or 64gb for large sized models (2048 sizes). Your 16gb limitation seems normal to me.", "body": "@lightingghost just to comment on this as I have heavily worked on seq2seq in tensorflow. I have found that I need at minimum 24gb of ram to have a working model of the timesteps you're describing. The demands go up even further to 32 or 64gb for large sized models (2048 sizes). Your 16gb limitation seems normal to me. \n"}