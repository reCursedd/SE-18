{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/164909268", "html_url": "https://github.com/tensorflow/tensorflow/issues/511#issuecomment-164909268", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511", "id": 164909268, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NDkwOTI2OA==", "user": {"login": "ludimagister", "id": 15710643, "node_id": "MDQ6VXNlcjE1NzEwNjQz", "avatar_url": "https://avatars3.githubusercontent.com/u/15710643?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ludimagister", "html_url": "https://github.com/ludimagister", "followers_url": "https://api.github.com/users/ludimagister/followers", "following_url": "https://api.github.com/users/ludimagister/following{/other_user}", "gists_url": "https://api.github.com/users/ludimagister/gists{/gist_id}", "starred_url": "https://api.github.com/users/ludimagister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ludimagister/subscriptions", "organizations_url": "https://api.github.com/users/ludimagister/orgs", "repos_url": "https://api.github.com/users/ludimagister/repos", "events_url": "https://api.github.com/users/ludimagister/events{/privacy}", "received_events_url": "https://api.github.com/users/ludimagister/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-15T21:55:31Z", "updated_at": "2015-12-15T21:55:31Z", "author_association": "NONE", "body_html": "<p>Can you try to use one of those two (currently not on by default) gradient accumulation methods and see if this improves steptime:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L204\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L204</a><br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L231\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L231</a></p>\n<p>Also, for experimentation I would recommend to use a small batch_size=1 and make it bigger when everything works to keep memory down.</p>\n<p>Overall, we are fully aware that long unrolled RNNs/LSTMs are still problematic in TensorFlow, we are working on making it better.</p>", "body_text": "Can you try to use one of those two (currently not on by default) gradient accumulation methods and see if this improves steptime:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L204\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L231\nAlso, for experimentation I would recommend to use a small batch_size=1 and make it bigger when everything works to keep memory down.\nOverall, we are fully aware that long unrolled RNNs/LSTMs are still problematic in TensorFlow, we are working on making it better.", "body": "Can you try to use one of those two (currently not on by default) gradient accumulation methods and see if this improves steptime:\n\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L204\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/gradients_test.py#L231\n\nAlso, for experimentation I would recommend to use a small batch_size=1 and make it bigger when everything works to keep memory down.\n\nOverall, we are fully aware that long unrolled RNNs/LSTMs are still problematic in TensorFlow, we are working on making it better.\n"}