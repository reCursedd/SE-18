{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165182596", "html_url": "https://github.com/tensorflow/tensorflow/issues/511#issuecomment-165182596", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511", "id": 165182596, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTE4MjU5Ng==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-16T17:20:59Z", "updated_at": "2015-12-16T17:20:59Z", "author_association": "NONE", "body_html": "<p>Bit late to this discussion, but I too am doing seq2seq in tf <a href=\"https://github.com/LeavesBreathe/Project_RNN_Enhancement\">here</a>.</p>\n<p>I don't want to beat a dead horse, but I have slow step times, though my compilation isn't too bad. I currently run 2 layers of 1536 units (GRU). However, the biggest hurdle by far is the memory.</p>\n<p>When I run these two layers, each of them on a 980 TI, I get 99% usage, and I'm overclocking my GPUs. So I feel that the GPU is being used. However the usage percent is sinusoidal with a average of 50%, but I also had the same experience in Theano.</p>\n<p>But because of memory usage, I can only use a batch size of 16 whereas in theano I could use a batch size of 256 easily. And it is the batch size that makes tf very slow for me.</p>\n<p>I know you guys are working hard on improving the memory allocator. I'm very appreciative of it. Just wanted to give my 2 cents.</p>", "body_text": "Bit late to this discussion, but I too am doing seq2seq in tf here.\nI don't want to beat a dead horse, but I have slow step times, though my compilation isn't too bad. I currently run 2 layers of 1536 units (GRU). However, the biggest hurdle by far is the memory.\nWhen I run these two layers, each of them on a 980 TI, I get 99% usage, and I'm overclocking my GPUs. So I feel that the GPU is being used. However the usage percent is sinusoidal with a average of 50%, but I also had the same experience in Theano.\nBut because of memory usage, I can only use a batch size of 16 whereas in theano I could use a batch size of 256 easily. And it is the batch size that makes tf very slow for me.\nI know you guys are working hard on improving the memory allocator. I'm very appreciative of it. Just wanted to give my 2 cents.", "body": "Bit late to this discussion, but I too am doing seq2seq in tf [here](https://github.com/LeavesBreathe/Project_RNN_Enhancement).\n\nI don't want to beat a dead horse, but I have slow step times, though my compilation isn't too bad. I currently run 2 layers of 1536 units (GRU). However, the biggest hurdle by far is the memory.\n\nWhen I run these two layers, each of them on a 980 TI, I get 99% usage, and I'm overclocking my GPUs. So I feel that the GPU is being used. However the usage percent is sinusoidal with a average of 50%, but I also had the same experience in Theano.\n\nBut because of memory usage, I can only use a batch size of 16 whereas in theano I could use a batch size of 256 easily. And it is the batch size that makes tf very slow for me. \n\nI know you guys are working hard on improving the memory allocator. I'm very appreciative of it. Just wanted to give my 2 cents. \n"}