{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/511/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/511", "id": 122146339, "node_id": "MDU6SXNzdWUxMjIxNDYzMzk=", "number": 511, "title": "Large RNN graphs, incredibly slow graph creation and step time...", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 48, "created_at": "2015-12-14T22:28:03Z", "updated_at": "2017-02-09T22:37:46Z", "closed_at": "2016-06-08T06:13:22Z", "author_association": "NONE", "body_html": "<p>When I build a large seq2seq RNN graph (i.e., 3000 timesteps encoder, and 100 time steps decoder), both the graph construction and step time is insanely slow.</p>\n<p>For example, using approx 3000 timesteps of stacked 4 layers of GRUs (in a seq2seq model w/ attention), we get something like this:</p>\n<p>*** graph creation time ***<br>\ncreate_encoder graph time 77.442740<br>\ncreate_decoder graph time 7.298214<br>\ncreate_loss graph time 0.934293<br>\ncreate_optimizer graph time 349.426908<br>\ncreate_model graph time 489.119399</p>\n<p>create_optimizer is the part where i call something like this (pulled from the tutorial):<br>\ndef create_optimizer(self):<br>\nstart_time = time.time()</p>\n<pre><code>params = tf.trainable_variables()\n\nself.gradient_norms = []\nself.updates = []\nopt = tf.train.GradientDescentOptimizer(0.001)\n\ngradients = tf.gradients(self.losses, params)\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n\nself.gradient_norms.append(norm)\nself.updates.append(opt.apply_gradients(\n    zip(clipped_gradients, params), global_step=self.global_step))\n\nprint('create_optimizer graph time %f' % (time.time() - start_time))\n</code></pre>\n<p>*** step time ***<br>\nstep_time: 142.356251001</p>\n<p>When I run nvidia-smi while the graph is computing , the utilization is something like 12% ... definitely something wrong...</p>\n<p>In my personal framework (not TF), creating a graph of equal size takes approx 2 secs (as opposed to TF almost 8 mins) and the step time is less 4 seconds (as opposed to TF 2 mins).... something really weird is going on.</p>\n<p>I wasn't able to profile / debug it too much (are there public tools for that? i cant find anything on tensorboard). also, is there a command i can run to confirm how many weights i have (and the dimensions and the device location of the variables).</p>\n<p>I've confirmed with the tensorboard and log_device_placement=True, and seems like most of the graph is in the GPU (except for embedding layers).</p>\n<p>If you want a link to my graph, send me an email.</p>\n<p>Thanks All!</p>", "body_text": "When I build a large seq2seq RNN graph (i.e., 3000 timesteps encoder, and 100 time steps decoder), both the graph construction and step time is insanely slow.\nFor example, using approx 3000 timesteps of stacked 4 layers of GRUs (in a seq2seq model w/ attention), we get something like this:\n*** graph creation time ***\ncreate_encoder graph time 77.442740\ncreate_decoder graph time 7.298214\ncreate_loss graph time 0.934293\ncreate_optimizer graph time 349.426908\ncreate_model graph time 489.119399\ncreate_optimizer is the part where i call something like this (pulled from the tutorial):\ndef create_optimizer(self):\nstart_time = time.time()\nparams = tf.trainable_variables()\n\nself.gradient_norms = []\nself.updates = []\nopt = tf.train.GradientDescentOptimizer(0.001)\n\ngradients = tf.gradients(self.losses, params)\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n\nself.gradient_norms.append(norm)\nself.updates.append(opt.apply_gradients(\n    zip(clipped_gradients, params), global_step=self.global_step))\n\nprint('create_optimizer graph time %f' % (time.time() - start_time))\n\n*** step time ***\nstep_time: 142.356251001\nWhen I run nvidia-smi while the graph is computing , the utilization is something like 12% ... definitely something wrong...\nIn my personal framework (not TF), creating a graph of equal size takes approx 2 secs (as opposed to TF almost 8 mins) and the step time is less 4 seconds (as opposed to TF 2 mins).... something really weird is going on.\nI wasn't able to profile / debug it too much (are there public tools for that? i cant find anything on tensorboard). also, is there a command i can run to confirm how many weights i have (and the dimensions and the device location of the variables).\nI've confirmed with the tensorboard and log_device_placement=True, and seems like most of the graph is in the GPU (except for embedding layers).\nIf you want a link to my graph, send me an email.\nThanks All!", "body": "When I build a large seq2seq RNN graph (i.e., 3000 timesteps encoder, and 100 time steps decoder), both the graph construction and step time is insanely slow.\n\nFor example, using approx 3000 timesteps of stacked 4 layers of GRUs (in a seq2seq model w/ attention), we get something like this:\n\n**\\* graph creation time ***\ncreate_encoder graph time 77.442740\ncreate_decoder graph time 7.298214\ncreate_loss graph time 0.934293\ncreate_optimizer graph time 349.426908\ncreate_model graph time 489.119399\n\ncreate_optimizer is the part where i call something like this (pulled from the tutorial):\n  def create_optimizer(self):\n    start_time = time.time()\n\n```\nparams = tf.trainable_variables()\n\nself.gradient_norms = []\nself.updates = []\nopt = tf.train.GradientDescentOptimizer(0.001)\n\ngradients = tf.gradients(self.losses, params)\nclipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n\nself.gradient_norms.append(norm)\nself.updates.append(opt.apply_gradients(\n    zip(clipped_gradients, params), global_step=self.global_step))\n\nprint('create_optimizer graph time %f' % (time.time() - start_time))\n```\n\n**\\* step time ***\nstep_time: 142.356251001\n\nWhen I run nvidia-smi while the graph is computing , the utilization is something like 12% ... definitely something wrong...\n\nIn my personal framework (not TF), creating a graph of equal size takes approx 2 secs (as opposed to TF almost 8 mins) and the step time is less 4 seconds (as opposed to TF 2 mins).... something really weird is going on.\n\nI wasn't able to profile / debug it too much (are there public tools for that? i cant find anything on tensorboard). also, is there a command i can run to confirm how many weights i have (and the dimensions and the device location of the variables).\n\nI've confirmed with the tensorboard and log_device_placement=True, and seems like most of the graph is in the GPU (except for embedding layers).\n\nIf you want a link to my graph, send me an email.\n\nThanks All!\n"}