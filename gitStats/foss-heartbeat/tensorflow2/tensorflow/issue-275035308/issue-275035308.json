{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14676", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14676/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14676/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14676/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14676", "id": 275035308, "node_id": "MDU6SXNzdWUyNzUwMzUzMDg=", "number": 14676, "title": "XLA operation semantics documentation BatchNormTrain error", "user": {"login": "mateoespinosa", "id": 6935411, "node_id": "MDQ6VXNlcjY5MzU0MTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/6935411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mateoespinosa", "html_url": "https://github.com/mateoespinosa", "followers_url": "https://api.github.com/users/mateoespinosa/followers", "following_url": "https://api.github.com/users/mateoespinosa/following{/other_user}", "gists_url": "https://api.github.com/users/mateoespinosa/gists{/gist_id}", "starred_url": "https://api.github.com/users/mateoespinosa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mateoespinosa/subscriptions", "organizations_url": "https://api.github.com/users/mateoespinosa/orgs", "repos_url": "https://api.github.com/users/mateoespinosa/repos", "events_url": "https://api.github.com/users/mateoespinosa/events{/privacy}", "received_events_url": "https://api.github.com/users/mateoespinosa/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yunxing", "id": 1017689, "node_id": "MDQ6VXNlcjEwMTc2ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1017689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yunxing", "html_url": "https://github.com/yunxing", "followers_url": "https://api.github.com/users/yunxing/followers", "following_url": "https://api.github.com/users/yunxing/following{/other_user}", "gists_url": "https://api.github.com/users/yunxing/gists{/gist_id}", "starred_url": "https://api.github.com/users/yunxing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yunxing/subscriptions", "organizations_url": "https://api.github.com/users/yunxing/orgs", "repos_url": "https://api.github.com/users/yunxing/repos", "events_url": "https://api.github.com/users/yunxing/events{/privacy}", "received_events_url": "https://api.github.com/users/yunxing/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yunxing", "id": 1017689, "node_id": "MDQ6VXNlcjEwMTc2ODk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1017689?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yunxing", "html_url": "https://github.com/yunxing", "followers_url": "https://api.github.com/users/yunxing/followers", "following_url": "https://api.github.com/users/yunxing/following{/other_user}", "gists_url": "https://api.github.com/users/yunxing/gists{/gist_id}", "starred_url": "https://api.github.com/users/yunxing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yunxing/subscriptions", "organizations_url": "https://api.github.com/users/yunxing/orgs", "repos_url": "https://api.github.com/users/yunxing/repos", "events_url": "https://api.github.com/users/yunxing/events{/privacy}", "received_events_url": "https://api.github.com/users/yunxing/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2017-11-18T00:30:18Z", "updated_at": "2018-04-12T14:05:25Z", "closed_at": "2018-04-12T14:05:25Z", "author_association": "NONE", "body_html": "<p>Hey! I think there are a few errors in the documentation for the XLA BatchNormTrain operation in <a href=\"https://www.tensorflow.org/performance/xla/operation_semantics#batchnormgrad\" rel=\"nofollow\">https://www.tensorflow.org/performance/xla/operation_semantics#batchnormgrad</a>.</p>\n<p>The gradient of the scaling factor <code>gamma</code> should be<br>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\nabla&amp;space;\\gamma&amp;space;=&amp;space;\\text{sum}\\Big(&amp;space;\\nabla&amp;space;y&amp;space;*&amp;space;\\frac{(&amp;space;x&amp;space;-&amp;space;\\mu&amp;space;)}{&amp;space;\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}}&amp;space;\\Big)\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7836f030f587a3d85f5e1cf73ad3277f1df627d0/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c6e61626c612673706163653b5c67616d6d612673706163653b3d2673706163653b5c746578747b73756d7d5c426967282673706163653b5c6e61626c612673706163653b792673706163653b2a2673706163653b5c667261637b282673706163653b782673706163653b2d2673706163653b5c6d752673706163653b297d7b2673706163653b5c737172747b5c7369676d615e322673706163653b2b2673706163653b5c657073696c6f6e7d7d2673706163653b5c42696729\" title=\"\\nabla \\gamma = \\text{sum}\\Big( \\nabla y * \\frac{( x - \\mu )}{ \\sqrt{\\sigma^2 + \\epsilon}} \\Big)\" data-canonical-src=\"https://latex.codecogs.com/gif.latex?\\nabla&amp;space;\\gamma&amp;space;=&amp;space;\\text{sum}\\Big(&amp;space;\\nabla&amp;space;y&amp;space;*&amp;space;\\frac{(&amp;space;x&amp;space;-&amp;space;\\mu&amp;space;)}{&amp;space;\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}}&amp;space;\\Big)\" style=\"max-width:100%;\"></a><br>\ninstead of<br>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\nabla&amp;space;\\gamma&amp;space;=&amp;space;\\text{sum}\\Big(&amp;space;\\nabla&amp;space;y&amp;space;*&amp;space;(&amp;space;x&amp;space;-&amp;space;\\mu&amp;space;)&amp;space;*&amp;space;\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}&amp;space;\\Big)\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b1c11b71d3b42ef5497d5a8b67b2607d2ee3914a/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c6e61626c612673706163653b5c67616d6d612673706163653b3d2673706163653b5c746578747b73756d7d5c426967282673706163653b5c6e61626c612673706163653b792673706163653b2a2673706163653b282673706163653b782673706163653b2d2673706163653b5c6d752673706163653b292673706163653b2a2673706163653b5c737172747b5c7369676d615e322673706163653b2b2673706163653b5c657073696c6f6e7d2673706163653b5c42696729\" title=\"\\nabla \\gamma = \\text{sum}\\Big( \\nabla y * ( x - \\mu ) * \\sqrt{\\sigma^2 + \\epsilon} \\Big)\" data-canonical-src=\"https://latex.codecogs.com/gif.latex?\\nabla&amp;space;\\gamma&amp;space;=&amp;space;\\text{sum}\\Big(&amp;space;\\nabla&amp;space;y&amp;space;*&amp;space;(&amp;space;x&amp;space;-&amp;space;\\mu&amp;space;)&amp;space;*&amp;space;\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}&amp;space;\\Big)\" style=\"max-width:100%;\"></a></p>\n<p>Moreover, the gradient for the input tensor is also not correct. It should instead read something like this:<br>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\Big(&amp;space;\\nabla&amp;space;y&amp;space;-&amp;space;\\frac{\\hat{x}&amp;space;\\times&amp;space;\\nabla&amp;space;\\gamma&amp;space;+&amp;space;\\nabla&amp;space;\\beta}{m&amp;space;w&amp;space;h}\\Big)&amp;space;\\times&amp;space;\\frac{\\gamma}{\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}}\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/18a13b3502d0e407486ab5e2fc42d507689a05af/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c426967282673706163653b5c6e61626c612673706163653b792673706163653b2d2673706163653b5c667261637b5c6861747b787d2673706163653b5c74696d65732673706163653b5c6e61626c612673706163653b5c67616d6d612673706163653b2b2673706163653b5c6e61626c612673706163653b5c626574617d7b6d2673706163653b772673706163653b687d5c426967292673706163653b5c74696d65732673706163653b5c667261637b5c67616d6d617d7b5c737172747b5c7369676d615e322673706163653b2b2673706163653b5c657073696c6f6e7d7d\" title=\"\\Big( \\nabla y - \\frac{\\hat{x} \\times \\nabla \\gamma + \\nabla \\beta}{m w h}\\Big) \\times \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}\" data-canonical-src=\"https://latex.codecogs.com/gif.latex?\\Big(&amp;space;\\nabla&amp;space;y&amp;space;-&amp;space;\\frac{\\hat{x}&amp;space;\\times&amp;space;\\nabla&amp;space;\\gamma&amp;space;+&amp;space;\\nabla&amp;space;\\beta}{m&amp;space;w&amp;space;h}\\Big)&amp;space;\\times&amp;space;\\frac{\\gamma}{\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}}\" style=\"max-width:100%;\"></a></p>\n<p>Where the products and summations are done in a \"broadcasted\" manner when shapes don't match and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\hat{x}\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/23c8f932dcd1c0d97a863da90427b5bd176e95cc/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c6861747b787d\" title=\"\\hat{x}\" data-canonical-src=\"https://latex.codecogs.com/gif.latex?\\hat{x}\" style=\"max-width:100%;\"></a> is the whitened input tensor. That is:<br>\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\hat{x}&amp;space;=&amp;space;\\frac{x&amp;space;-&amp;space;\\mu}{\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}}\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/16dacc977b6160d53028501f6513b0176ec7698d/68747470733a2f2f6c617465782e636f6465636f67732e636f6d2f6769662e6c617465783f5c6861747b787d2673706163653b3d2673706163653b5c667261637b782673706163653b2d2673706163653b5c6d757d7b5c737172747b5c7369676d615e322673706163653b2b2673706163653b5c657073696c6f6e7d7d\" title=\"\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\" data-canonical-src=\"https://latex.codecogs.com/gif.latex?\\hat{x}&amp;space;=&amp;space;\\frac{x&amp;space;-&amp;space;\\mu}{\\sqrt{\\sigma^2&amp;space;+&amp;space;\\epsilon}}\" style=\"max-width:100%;\"></a></p>\n<p>There also seem to be a typo in the table explaining the output of this XLA node. The second row should say <code>grad_offset</code> in the first column, <code>ComputationalDataHandle</code> in the second column, and the semantics column should say something like <code>gradient with respect to input offset</code>.</p>\n<p>Finally, it may be good if it could be specified that the summation used for the gradients of the scaling and offset tensors is performed over all dimensions that were used to compute the different statistics during normalization.</p>", "body_text": "Hey! I think there are a few errors in the documentation for the XLA BatchNormTrain operation in https://www.tensorflow.org/performance/xla/operation_semantics#batchnormgrad.\nThe gradient of the scaling factor gamma should be\n\ninstead of\n\nMoreover, the gradient for the input tensor is also not correct. It should instead read something like this:\n\nWhere the products and summations are done in a \"broadcasted\" manner when shapes don't match and  is the whitened input tensor. That is:\n\nThere also seem to be a typo in the table explaining the output of this XLA node. The second row should say grad_offset in the first column, ComputationalDataHandle in the second column, and the semantics column should say something like gradient with respect to input offset.\nFinally, it may be good if it could be specified that the summation used for the gradients of the scaling and offset tensors is performed over all dimensions that were used to compute the different statistics during normalization.", "body": "Hey! I think there are a few errors in the documentation for the XLA BatchNormTrain operation in https://www.tensorflow.org/performance/xla/operation_semantics#batchnormgrad.\r\n\r\nThe gradient of the scaling factor `gamma` should be\r\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\nabla&space;\\gamma&space;=&space;\\text{sum}\\Big(&space;\\nabla&space;y&space;*&space;\\frac{(&space;x&space;-&space;\\mu&space;)}{&space;\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}}&space;\\Big)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\nabla&space;\\gamma&space;=&space;\\text{sum}\\Big(&space;\\nabla&space;y&space;*&space;\\frac{(&space;x&space;-&space;\\mu&space;)}{&space;\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}}&space;\\Big)\" title=\"\\nabla \\gamma = \\text{sum}\\Big( \\nabla y * \\frac{( x - \\mu )}{ \\sqrt{\\sigma^2 + \\epsilon}} \\Big)\" /></a>\r\ninstead of \r\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\nabla&space;\\gamma&space;=&space;\\text{sum}\\Big(&space;\\nabla&space;y&space;*&space;(&space;x&space;-&space;\\mu&space;)&space;*&space;\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}&space;\\Big)\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\nabla&space;\\gamma&space;=&space;\\text{sum}\\Big(&space;\\nabla&space;y&space;*&space;(&space;x&space;-&space;\\mu&space;)&space;*&space;\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}&space;\\Big)\" title=\"\\nabla \\gamma = \\text{sum}\\Big( \\nabla y * ( x - \\mu ) * \\sqrt{\\sigma^2 + \\epsilon} \\Big)\" /></a>\r\n\r\nMoreover, the gradient for the input tensor is also not correct. It should instead read something like this:\r\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\Big(&space;\\nabla&space;y&space;-&space;\\frac{\\hat{x}&space;\\times&space;\\nabla&space;\\gamma&space;&plus;&space;\\nabla&space;\\beta}{m&space;w&space;h}\\Big)&space;\\times&space;\\frac{\\gamma}{\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\Big(&space;\\nabla&space;y&space;-&space;\\frac{\\hat{x}&space;\\times&space;\\nabla&space;\\gamma&space;&plus;&space;\\nabla&space;\\beta}{m&space;w&space;h}\\Big)&space;\\times&space;\\frac{\\gamma}{\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}}\" title=\"\\Big( \\nabla y - \\frac{\\hat{x} \\times \\nabla \\gamma + \\nabla \\beta}{m w h}\\Big) \\times \\frac{\\gamma}{\\sqrt{\\sigma^2 + \\epsilon}}\" /></a>\r\n\r\nWhere the products and summations are done in a \"broadcasted\" manner when shapes don't match and <a href=\"https://www.codecogs.com/eqnedit.php?latex=\\hat{x}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\hat{x}\" title=\"\\hat{x}\" /></a> is the whitened input tensor. That is:\r\n<a href=\"https://www.codecogs.com/eqnedit.php?latex=\\hat{x}&space;=&space;\\frac{x&space;-&space;\\mu}{\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}}\" target=\"_blank\"><img src=\"https://latex.codecogs.com/gif.latex?\\hat{x}&space;=&space;\\frac{x&space;-&space;\\mu}{\\sqrt{\\sigma^2&space;&plus;&space;\\epsilon}}\" title=\"\\hat{x} = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\" /></a>\r\n\r\n\r\nThere also seem to be a typo in the table explaining the output of this XLA node. The second row should say `grad_offset` in the first column, `ComputationalDataHandle` in the second column, and the semantics column should say something like `gradient with respect to input offset`.\r\n\r\n \r\nFinally, it may be good if it could be specified that the summation used for the gradients of the scaling and offset tensors is performed over all dimensions that were used to compute the different statistics during normalization."}