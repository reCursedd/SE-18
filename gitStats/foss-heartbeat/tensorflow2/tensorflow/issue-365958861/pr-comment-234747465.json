{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/234747465", "pull_request_review_id": 176455617, "id": 234747465, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzNDc0NzQ2NQ==", "diff_hunk": "@@ -0,0 +1,400 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// ROCM userspace driver library wrapper functionality.\n+\n+#ifndef TENSORFLOW_STREAM_EXECUTOR_ROCM_ROCM_DRIVER_H_\n+#define TENSORFLOW_STREAM_EXECUTOR_ROCM_ROCM_DRIVER_H_\n+\n+#include <stddef.h>\n+#include \"tensorflow/stream_executor/platform/port.h\"\n+\n+#include \"rocm/include/hip/hip_runtime.h\"\n+#include \"tensorflow/stream_executor/device_options.h\"\n+#include \"tensorflow/stream_executor/lib/status.h\"\n+#include \"tensorflow/stream_executor/lib/statusor.h\"\n+#include \"tensorflow/stream_executor/platform/port.h\"\n+\n+namespace stream_executor {\n+namespace rocm {\n+\n+// Identifies the memory space where an allocation resides. See\n+// ROCMDriver::GetPointerMemorySpace().\n+enum class MemorySpace { kHost, kDevice };\n+\n+// Returns a casual string, such as \"host\" for the provided memory space.\n+string MemorySpaceString(MemorySpace memory_space);\n+\n+// ROCMDriver contains wrappers for calls to the userspace library driver. It's\n+// useful to isolate these calls and put basic wrappers around them to separate\n+// userspace library driver behaviors from the rest of the program.\n+//\n+// At the moment it's simply used as a namespace.\n+//\n+// The calls log any specific errors internally and return whether the operation\n+// was successful to the caller.\n+//\n+// Thread safety: these functions should not be used from signal handlers.\n+class ROCMDriver {", "path": "tensorflow/stream_executor/rocm/rocm_driver.h", "position": 50, "original_position": 50, "commit_id": "9226372134ab2e6b58c8f933391fb693a0f11f1f", "original_commit_id": "9226372134ab2e6b58c8f933391fb693a0f11f1f", "user": {"login": "timshen91", "id": 1157432, "node_id": "MDQ6VXNlcjExNTc0MzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1157432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/timshen91", "html_url": "https://github.com/timshen91", "followers_url": "https://api.github.com/users/timshen91/followers", "following_url": "https://api.github.com/users/timshen91/following{/other_user}", "gists_url": "https://api.github.com/users/timshen91/gists{/gist_id}", "starred_url": "https://api.github.com/users/timshen91/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/timshen91/subscriptions", "organizations_url": "https://api.github.com/users/timshen91/orgs", "repos_url": "https://api.github.com/users/timshen91/repos", "events_url": "https://api.github.com/users/timshen91/events{/privacy}", "received_events_url": "https://api.github.com/users/timshen91/received_events", "type": "User", "site_admin": false}, "body": "> hi @timshen91,\r\n> \r\n> I think that ROCMDriver / CUDADriver can potentially be merged into a single GPUDriver class. However I am not entirely convinced whether that is proper choice here.\r\n> \r\n> Althogh it will remove some code duplication, but it also add some complexity to the code organization.\r\n> \r\n> * Currently all the ROCM specific code is in the \"rocm\" dir, CUDA specific code is in the \"cuda\" dir.  We will need to add a new \"gpu\" dir to accomodate this change (and create \"gpu_driver.[h,cc]\" within it\r\n> * Some more but _not all_ code within \"rocm\" and \"cuda\" dirs can be de-duped, which means all three dirs (rocm / cuda / gpu) will need to co-exist.\r\n\r\nYes. I don't worry too much about the three directories co-exist per se. I do worry, as you noted, that code is scattered into the three directories too much, reducing readability. Below is my attempt to resolve the worries.\r\n\r\n> * Code within the \"gpu\" dir will look like\r\n> \r\n> ```\r\n> void some_func(...) {\r\n> #if ROCM_USE_TENSORFLOW\r\n>    // ROCm specific implementation here\r\n> #else\r\n>   // CUDA specific implementation here\r\n> #endif\r\n> }\r\n> ```\r\n\r\nHow about: anything under ifdef should go into the rocm or cuda dir, not the gpu dir, as the gpu dir means to keep portable/common code. For example, I'd expect:\r\n```\r\n// gpu/x.h\r\nvoid some_func(...);\r\n\r\n// rocm/x.cc\r\nvoid some_func(...) { ... }\r\n\r\n// cuda/x.cc\r\nvoid some_func(...) { ... }\r\n```\r\nOf course, one has to ensure that only one of the x.cc files gets picked and compiled. That can be done in the BUILD file, or, use ifdefs in rocm/x.cc and cuda/x.cc.\r\n\r\nThis file organization pattern is not new. tensorflow/core/platform uses this pattern too. portable code lives in platform/*.{h,cc}, while non-portable code lives in platform/{default,cloud,hadoop,d3,windows,...}.\r\n\r\nComing back to our use case, for example, ROCMEvent and CUDAEvent can be merged and live in gpu/, as they don't have any ifdefs in their implementations.\r\n\r\nAlso, It'd be great if gpu/gpu_driver.h can be written without ifdefs (that's why it's in the gpu/ dir), and rocm/gpu_driver.cc and cuda/gpu_driver.cc implement them separately.\r\n\r\n> * Developers will now need to keep track of what parts of the CUDA/ROCm specific implementations are shared within \"gpu\" dir and what parts are specific to \"cuda\"/\"rocm\" dirs.\r\n\r\nI'd like to avoid that too. Echoing with the previous comment, if gpu/ only contains \"real\" portable code, that is free from ifdefs, then developers don't have to track which ROCm/CUDA specific implementations are in gpu dir. Platform specific code is always in the rocm/cuda dirs.\r\n\r\n> \r\n> For these reasons, it would be preferable to keep the the current code organization. Please let me know whether you agree.\r\n> \r\n> Thanks\r\n> \r\n> deven\r\n\r\n", "created_at": "2018-11-19T19:17:22Z", "updated_at": "2018-11-19T19:17:22Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22669#discussion_r234747465", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22669", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/234747465"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22669#discussion_r234747465"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22669"}}, "body_html": "<blockquote>\n<p>hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1157432\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/timshen91\">@timshen91</a>,</p>\n<p>I think that ROCMDriver / CUDADriver can potentially be merged into a single GPUDriver class. However I am not entirely convinced whether that is proper choice here.</p>\n<p>Althogh it will remove some code duplication, but it also add some complexity to the code organization.</p>\n<ul>\n<li>Currently all the ROCM specific code is in the \"rocm\" dir, CUDA specific code is in the \"cuda\" dir.  We will need to add a new \"gpu\" dir to accomodate this change (and create \"gpu_driver.[h,cc]\" within it</li>\n<li>Some more but <em>not all</em> code within \"rocm\" and \"cuda\" dirs can be de-duped, which means all three dirs (rocm / cuda / gpu) will need to co-exist.</li>\n</ul>\n</blockquote>\n<p>Yes. I don't worry too much about the three directories co-exist per se. I do worry, as you noted, that code is scattered into the three directories too much, reducing readability. Below is my attempt to resolve the worries.</p>\n<blockquote>\n<ul>\n<li>Code within the \"gpu\" dir will look like</li>\n</ul>\n<pre><code>void some_func(...) {\n#if ROCM_USE_TENSORFLOW\n   // ROCm specific implementation here\n#else\n  // CUDA specific implementation here\n#endif\n}\n</code></pre>\n</blockquote>\n<p>How about: anything under ifdef should go into the rocm or cuda dir, not the gpu dir, as the gpu dir means to keep portable/common code. For example, I'd expect:</p>\n<pre><code>// gpu/x.h\nvoid some_func(...);\n\n// rocm/x.cc\nvoid some_func(...) { ... }\n\n// cuda/x.cc\nvoid some_func(...) { ... }\n</code></pre>\n<p>Of course, one has to ensure that only one of the x.cc files gets picked and compiled. That can be done in the BUILD file, or, use ifdefs in rocm/x.cc and cuda/x.cc.</p>\n<p>This file organization pattern is not new. tensorflow/core/platform uses this pattern too. portable code lives in platform/*.{h,cc}, while non-portable code lives in platform/{default,cloud,hadoop,d3,windows,...}.</p>\n<p>Coming back to our use case, for example, ROCMEvent and CUDAEvent can be merged and live in gpu/, as they don't have any ifdefs in their implementations.</p>\n<p>Also, It'd be great if gpu/gpu_driver.h can be written without ifdefs (that's why it's in the gpu/ dir), and rocm/gpu_driver.cc and cuda/gpu_driver.cc implement them separately.</p>\n<blockquote>\n<ul>\n<li>Developers will now need to keep track of what parts of the CUDA/ROCm specific implementations are shared within \"gpu\" dir and what parts are specific to \"cuda\"/\"rocm\" dirs.</li>\n</ul>\n</blockquote>\n<p>I'd like to avoid that too. Echoing with the previous comment, if gpu/ only contains \"real\" portable code, that is free from ifdefs, then developers don't have to track which ROCm/CUDA specific implementations are in gpu dir. Platform specific code is always in the rocm/cuda dirs.</p>\n<blockquote>\n<p>For these reasons, it would be preferable to keep the the current code organization. Please let me know whether you agree.</p>\n<p>Thanks</p>\n<p>deven</p>\n</blockquote>", "body_text": "hi @timshen91,\nI think that ROCMDriver / CUDADriver can potentially be merged into a single GPUDriver class. However I am not entirely convinced whether that is proper choice here.\nAlthogh it will remove some code duplication, but it also add some complexity to the code organization.\n\nCurrently all the ROCM specific code is in the \"rocm\" dir, CUDA specific code is in the \"cuda\" dir.  We will need to add a new \"gpu\" dir to accomodate this change (and create \"gpu_driver.[h,cc]\" within it\nSome more but not all code within \"rocm\" and \"cuda\" dirs can be de-duped, which means all three dirs (rocm / cuda / gpu) will need to co-exist.\n\n\nYes. I don't worry too much about the three directories co-exist per se. I do worry, as you noted, that code is scattered into the three directories too much, reducing readability. Below is my attempt to resolve the worries.\n\n\nCode within the \"gpu\" dir will look like\n\nvoid some_func(...) {\n#if ROCM_USE_TENSORFLOW\n   // ROCm specific implementation here\n#else\n  // CUDA specific implementation here\n#endif\n}\n\n\nHow about: anything under ifdef should go into the rocm or cuda dir, not the gpu dir, as the gpu dir means to keep portable/common code. For example, I'd expect:\n// gpu/x.h\nvoid some_func(...);\n\n// rocm/x.cc\nvoid some_func(...) { ... }\n\n// cuda/x.cc\nvoid some_func(...) { ... }\n\nOf course, one has to ensure that only one of the x.cc files gets picked and compiled. That can be done in the BUILD file, or, use ifdefs in rocm/x.cc and cuda/x.cc.\nThis file organization pattern is not new. tensorflow/core/platform uses this pattern too. portable code lives in platform/*.{h,cc}, while non-portable code lives in platform/{default,cloud,hadoop,d3,windows,...}.\nComing back to our use case, for example, ROCMEvent and CUDAEvent can be merged and live in gpu/, as they don't have any ifdefs in their implementations.\nAlso, It'd be great if gpu/gpu_driver.h can be written without ifdefs (that's why it's in the gpu/ dir), and rocm/gpu_driver.cc and cuda/gpu_driver.cc implement them separately.\n\n\nDevelopers will now need to keep track of what parts of the CUDA/ROCm specific implementations are shared within \"gpu\" dir and what parts are specific to \"cuda\"/\"rocm\" dirs.\n\n\nI'd like to avoid that too. Echoing with the previous comment, if gpu/ only contains \"real\" portable code, that is free from ifdefs, then developers don't have to track which ROCm/CUDA specific implementations are in gpu dir. Platform specific code is always in the rocm/cuda dirs.\n\nFor these reasons, it would be preferable to keep the the current code organization. Please let me know whether you agree.\nThanks\ndeven", "in_reply_to_id": 230946354}