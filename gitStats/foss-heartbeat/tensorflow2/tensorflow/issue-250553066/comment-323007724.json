{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323007724", "html_url": "https://github.com/tensorflow/tensorflow/issues/12319#issuecomment-323007724", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12319", "id": 323007724, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzAwNzcyNA==", "user": {"login": "wangyaobupt", "id": 2189852, "node_id": "MDQ6VXNlcjIxODk4NTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2189852?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangyaobupt", "html_url": "https://github.com/wangyaobupt", "followers_url": "https://api.github.com/users/wangyaobupt/followers", "following_url": "https://api.github.com/users/wangyaobupt/following{/other_user}", "gists_url": "https://api.github.com/users/wangyaobupt/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangyaobupt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangyaobupt/subscriptions", "organizations_url": "https://api.github.com/users/wangyaobupt/orgs", "repos_url": "https://api.github.com/users/wangyaobupt/repos", "events_url": "https://api.github.com/users/wangyaobupt/events{/privacy}", "received_events_url": "https://api.github.com/users/wangyaobupt/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-17T08:44:33Z", "updated_at": "2017-08-17T09:15:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=161459\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/petewarden\">@petewarden</a>, thanks for your book.</p>\n<p>jakajacky and I worked in same team, and today we further debugged this issue with help from your guide book, but error still exist.</p>\n<p><em><strong>Current status is</strong></em></p>\n<ol>\n<li>Freezing model is successful.</li>\n</ol>\n<blockquote>\n<p>$ bazel-bin/tensorflow/python/tools/freeze_graph \\<br>\n--input_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/graph_dropout.pb \\<br>\n--input_checkpoint=/Users/xiaoqiang/Desktop/Volcano_Offline_1/models/model.ckpt-0 <br>\n--input_node_names=models_simple/x --output_node_names=models_simple/y_conv <br>\n--input_binary <br>\n--output_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/frozen.pb</p>\n</blockquote>\n<blockquote>\n<p>console log:</p>\n<p>2017-08-17 16:20:37.775619: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.<br>\n2017-08-17 16:20:37.775643: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.<br>\n2017-08-17 16:20:37.775650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.<br>\n2017-08-17 16:20:37.775655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.<br>\nConverted 8 variables to const ops.<br>\n47 ops in the final graph.</p>\n<p>result:<br>\nfrozen.pb</p>\n</blockquote>\n<ol start=\"2\">\n<li>Print summary of model, there is a \"RandomUniform\" op, which will cause error when create session in IOS</li>\n</ol>\n<blockquote>\n<p>$ bazel run tensorflow/tools/graph_transforms:summarize_graph -- \\<br>\n--in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb\"</p>\n</blockquote>\n<blockquote>\n<p>console log:</p>\n<p>INFO: Found 1 target...<br>\nTarget //tensorflow/tools/graph_transforms:summarize_graph up-to-date:<br>\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph<br>\nINFO: Elapsed time: 0.173s, Critical Path: 0.00s</p>\n<p>INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb'<br>\nFound 2 possible inputs: (name=models_simple/x, type=float(1), shape=[]) (name=models_simple/Placeholder, type=float(1), shape=[])<br>\nNo variables spotted.<br>\nFound 1 possible outputs: (name=models_simple/y_conv, op=Softmax)<br>\nFound 70140 (70.14k) const parameters, 0 (0) variable parameters, and 0 control_edges<br>\nOp types used: 12 Const, 6 Add, 3 Relu, 2 Placeholder, 2 MatMul, 2 MaxPool, 2 Mul, 2 Reshape, 2 Conv2D, 1 RealDiv, 1 RandomUniform, 1 Floor, 1 Shape, 1 Softmax, 1 Sub</p>\n</blockquote>\n<ol start=\"3\">\n<li>Try \"Removing training-only nodes\" (bazel run tensorflow/tools/graph_transforms:transform_graph). The operation is successfully completed.</li>\n</ol>\n<blockquote>\n<p>$ bazel run tensorflow/tools/graph_transforms:transform_graph -- \\<br>\n--in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb\"  <br>\n    --out_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb\"  <br>\n    --inputs=\"models_simple/x\" <br>\n    --outputs=\"models_simple/y_conv\" <br>\n    --transforms='strip_unused_nodes(type=float,shape=\"1,256\")'</p>\n</blockquote>\n<blockquote>\n<p>console log:</p>\n<p>INFO: Found 1 target...<br>\nTarget //tensorflow/tools/graph_transforms:transform_graph up-to-date:<br>\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph<br>\nINFO: Elapsed time: 0.191s, Critical Path: 0.00s</p>\n<p>INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/transform_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb' '--out_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb' '--inputs=models_simple/x' '--outputs=models_simple/y_conv' '--transforms=strip_unused_nodes(type=float,shape=\"1,256\")'<br>\n2017-08-17 16:27:20.074270: I tensorflow/tools/graph_transforms/transform_graph.cc:262] Applying strip_unused_nodes</p>\n<p>result:<br>\noptimized_inception_graph.pb</p>\n</blockquote>\n<ol start=\"4\">\n<li>Since  \"dropout\" layers is left in graph, error persist when create session.</li>\n</ol>\n<blockquote>\n<p>$ bazel run tensorflow/tools/graph_transforms:summarize_graph -- \\<br>\n--in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb\"</p>\n</blockquote>\n<blockquote>\n<p>console log:</p>\n<p>INFO: Found 1 target...<br>\nTarget //tensorflow/tools/graph_transforms:summarize_graph up-to-date:<br>\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph<br>\nINFO: Elapsed time: 0.179s, Critical Path: 0.00s</p>\n<p>INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb'<br>\nFound 2 possible inputs: (name=models_simple/x, type=float(1), shape=[]) (name=models_simple/Placeholder, type=float(1), shape=[])<br>\nNo variables spotted.<br>\nFound 1 possible outputs: (name=models_simple/y_conv, op=Softmax)<br>\nFound 70140 (70.14k) const parameters, 0 (0) variable parameters, and 0 control_edges<br>\nOp types used: 12 Const, 6 Add, 3 Relu, 2 Placeholder, 2 MatMul, 2 MaxPool, 2 Mul, 2 Reshape, 2 Conv2D, 1 RealDiv, 1 RandomUniform, 1 Floor, 1 Shape, 1 Softmax, 1 Sub</p>\n</blockquote>\n<p><em><strong>Oher Experiments</strong></em><br>\nWe also tried to remove dropout layer from training code on Ubuntu Server, then re-train model, export model to IOS, the session_t-&gt;Create() operation passed. However, we cannot use this approach as solution, because removing dropout layer before training will lead to changes of weights and other parameters in neural networks.</p>\n<p>Our objectives are as follow</p>\n<ol>\n<li>Given a model (Checkpoint + GraphDef) trained and verified on server</li>\n<li>process the model with tools to remove all unnecessary nodes/ops for inference.</li>\n<li>inference on IOS</li>\n</ol>\n<p><em><strong>Question</strong></em><br>\nHow to remove dropout layer from model, since \"dropout\" is unnecessary in inference procedure.</p>\n<p><em><strong>Code for model generation (Python)</strong></em><br>\n`           self.x = tf.placeholder(tf.float32, [None, feature_num], name = 'x')<br>\nx_image = tf.reshape(self.x, [-1, 1, feature_num, 1])<br>\nself.y_ = tf.placeholder(tf.float32, [None, label_dim],name='y_')</p>\n<pre><code>        self.W_conv1 = weight_variable([1, self.conv_len, 1, self.num_conv1])\n        self.b_conv1 = bias_variable([self.num_conv1])\n        self.h_conv1 = tf.nn.relu(conv2d(x_image, self.W_conv1) + self.b_conv1)\n\n        self.h_pool1,pooling_factor = max_pool_1x4(self.h_conv1,pooling_factor)\n        self.W_conv2 = weight_variable([1, self.conv_len, self.num_conv1, self.num_conv2])\n        self.b_conv2 = bias_variable([self.num_conv2])\n        self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)\n\n        self.h_pool2,pooling_factor = max_pool_1x2(self.h_conv2,pooling_factor)\n        pooled_size = int(feature_num / pooling_factor * self.num_conv2)\n        print (\"pooling: (%d,%d) -&gt; %d\" % (feature_num, pooling_factor, pooled_size))\n        self.W_fc1 = weight_variable([pooled_size, self.num_fullc])\n        self.b_fc1 = bias_variable([self.num_fullc])\n        self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, pooled_size])\n        self.h_fc1 = tf.nn.relu(tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)\n\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)\n        self.W_fc2 = weight_variable([self.num_fullc, label_dim])\n        self.b_fc2 = bias_variable([label_dim])\n\n        self.y_conv = tf.nn.softmax(tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2, name='y_conv')\n\n        classes_weights = tf.constant([ 1.0,  0.8,  1.0, 1.0])\n        cross = tf.nn.weighted_cross_entropy_with_logits(self.y_,self.y_conv, pos_weight=classes_weights)\n        self.cross_entropy = tf.reduce_mean(cross)\n\n        # self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_conv, labels=self.y_))\n        self.train_step = tf.train.AdamOptimizer(lr).minimize(self.cross_entropy)`\n</code></pre>", "body_text": "@petewarden, thanks for your book.\njakajacky and I worked in same team, and today we further debugged this issue with help from your guide book, but error still exist.\nCurrent status is\n\nFreezing model is successful.\n\n\n$ bazel-bin/tensorflow/python/tools/freeze_graph \\\n--input_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/graph_dropout.pb \\\n--input_checkpoint=/Users/xiaoqiang/Desktop/Volcano_Offline_1/models/model.ckpt-0 \n--input_node_names=models_simple/x --output_node_names=models_simple/y_conv \n--input_binary \n--output_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/frozen.pb\n\n\nconsole log:\n2017-08-17 16:20:37.775619: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-17 16:20:37.775643: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-17 16:20:37.775650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-08-17 16:20:37.775655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\nConverted 8 variables to const ops.\n47 ops in the final graph.\nresult:\nfrozen.pb\n\n\nPrint summary of model, there is a \"RandomUniform\" op, which will cause error when create session in IOS\n\n\n$ bazel run tensorflow/tools/graph_transforms:summarize_graph -- \\\n--in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb\"\n\n\nconsole log:\nINFO: Found 1 target...\nTarget //tensorflow/tools/graph_transforms:summarize_graph up-to-date:\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph\nINFO: Elapsed time: 0.173s, Critical Path: 0.00s\nINFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb'\nFound 2 possible inputs: (name=models_simple/x, type=float(1), shape=[]) (name=models_simple/Placeholder, type=float(1), shape=[])\nNo variables spotted.\nFound 1 possible outputs: (name=models_simple/y_conv, op=Softmax)\nFound 70140 (70.14k) const parameters, 0 (0) variable parameters, and 0 control_edges\nOp types used: 12 Const, 6 Add, 3 Relu, 2 Placeholder, 2 MatMul, 2 MaxPool, 2 Mul, 2 Reshape, 2 Conv2D, 1 RealDiv, 1 RandomUniform, 1 Floor, 1 Shape, 1 Softmax, 1 Sub\n\n\nTry \"Removing training-only nodes\" (bazel run tensorflow/tools/graph_transforms:transform_graph). The operation is successfully completed.\n\n\n$ bazel run tensorflow/tools/graph_transforms:transform_graph -- \\\n--in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb\"  \n    --out_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb\"  \n    --inputs=\"models_simple/x\" \n    --outputs=\"models_simple/y_conv\" \n    --transforms='strip_unused_nodes(type=float,shape=\"1,256\")'\n\n\nconsole log:\nINFO: Found 1 target...\nTarget //tensorflow/tools/graph_transforms:transform_graph up-to-date:\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph\nINFO: Elapsed time: 0.191s, Critical Path: 0.00s\nINFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/transform_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb' '--out_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb' '--inputs=models_simple/x' '--outputs=models_simple/y_conv' '--transforms=strip_unused_nodes(type=float,shape=\"1,256\")'\n2017-08-17 16:27:20.074270: I tensorflow/tools/graph_transforms/transform_graph.cc:262] Applying strip_unused_nodes\nresult:\noptimized_inception_graph.pb\n\n\nSince  \"dropout\" layers is left in graph, error persist when create session.\n\n\n$ bazel run tensorflow/tools/graph_transforms:summarize_graph -- \\\n--in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb\"\n\n\nconsole log:\nINFO: Found 1 target...\nTarget //tensorflow/tools/graph_transforms:summarize_graph up-to-date:\nbazel-bin/tensorflow/tools/graph_transforms/summarize_graph\nINFO: Elapsed time: 0.179s, Critical Path: 0.00s\nINFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb'\nFound 2 possible inputs: (name=models_simple/x, type=float(1), shape=[]) (name=models_simple/Placeholder, type=float(1), shape=[])\nNo variables spotted.\nFound 1 possible outputs: (name=models_simple/y_conv, op=Softmax)\nFound 70140 (70.14k) const parameters, 0 (0) variable parameters, and 0 control_edges\nOp types used: 12 Const, 6 Add, 3 Relu, 2 Placeholder, 2 MatMul, 2 MaxPool, 2 Mul, 2 Reshape, 2 Conv2D, 1 RealDiv, 1 RandomUniform, 1 Floor, 1 Shape, 1 Softmax, 1 Sub\n\nOher Experiments\nWe also tried to remove dropout layer from training code on Ubuntu Server, then re-train model, export model to IOS, the session_t->Create() operation passed. However, we cannot use this approach as solution, because removing dropout layer before training will lead to changes of weights and other parameters in neural networks.\nOur objectives are as follow\n\nGiven a model (Checkpoint + GraphDef) trained and verified on server\nprocess the model with tools to remove all unnecessary nodes/ops for inference.\ninference on IOS\n\nQuestion\nHow to remove dropout layer from model, since \"dropout\" is unnecessary in inference procedure.\nCode for model generation (Python)\n`           self.x = tf.placeholder(tf.float32, [None, feature_num], name = 'x')\nx_image = tf.reshape(self.x, [-1, 1, feature_num, 1])\nself.y_ = tf.placeholder(tf.float32, [None, label_dim],name='y_')\n        self.W_conv1 = weight_variable([1, self.conv_len, 1, self.num_conv1])\n        self.b_conv1 = bias_variable([self.num_conv1])\n        self.h_conv1 = tf.nn.relu(conv2d(x_image, self.W_conv1) + self.b_conv1)\n\n        self.h_pool1,pooling_factor = max_pool_1x4(self.h_conv1,pooling_factor)\n        self.W_conv2 = weight_variable([1, self.conv_len, self.num_conv1, self.num_conv2])\n        self.b_conv2 = bias_variable([self.num_conv2])\n        self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)\n\n        self.h_pool2,pooling_factor = max_pool_1x2(self.h_conv2,pooling_factor)\n        pooled_size = int(feature_num / pooling_factor * self.num_conv2)\n        print (\"pooling: (%d,%d) -> %d\" % (feature_num, pooling_factor, pooled_size))\n        self.W_fc1 = weight_variable([pooled_size, self.num_fullc])\n        self.b_fc1 = bias_variable([self.num_fullc])\n        self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, pooled_size])\n        self.h_fc1 = tf.nn.relu(tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)\n\n        self.keep_prob = tf.placeholder(tf.float32)\n        self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)\n        self.W_fc2 = weight_variable([self.num_fullc, label_dim])\n        self.b_fc2 = bias_variable([label_dim])\n\n        self.y_conv = tf.nn.softmax(tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2, name='y_conv')\n\n        classes_weights = tf.constant([ 1.0,  0.8,  1.0, 1.0])\n        cross = tf.nn.weighted_cross_entropy_with_logits(self.y_,self.y_conv, pos_weight=classes_weights)\n        self.cross_entropy = tf.reduce_mean(cross)\n\n        # self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_conv, labels=self.y_))\n        self.train_step = tf.train.AdamOptimizer(lr).minimize(self.cross_entropy)`", "body": "@petewarden, thanks for your book. \r\n\r\njakajacky and I worked in same team, and today we further debugged this issue with help from your guide book, but error still exist.\r\n\r\n***Current status is***\r\n1. Freezing model is successful. \r\n\r\n> $ bazel-bin/tensorflow/python/tools/freeze_graph \\                       \r\n--input_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/graph_dropout.pb \\        \r\n--input_checkpoint=/Users/xiaoqiang/Desktop/Volcano_Offline_1/models/model.ckpt-0 \\\r\n--input_node_names=models_simple/x --output_node_names=models_simple/y_conv \\\r\n--input_binary \\\r\n--output_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/frozen.pb\r\n\r\n> console log:\r\n> \r\n> 2017-08-17 16:20:37.775619: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-08-17 16:20:37.775643: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-08-17 16:20:37.775650: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n> 2017-08-17 16:20:37.775655: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n> Converted 8 variables to const ops.\r\n> 47 ops in the final graph.\r\n> \r\n> result: \r\n> frozen.pb\r\n\r\n2. Print summary of model, there is a \"RandomUniform\" op, which will cause error when create session in IOS\r\n\r\n> $ bazel run tensorflow/tools/graph_transforms:summarize_graph -- \\       \r\n     --in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb\"\r\n\r\n> console log:\r\n> \r\n> INFO: Found 1 target...\r\n> Target //tensorflow/tools/graph_transforms:summarize_graph up-to-date:\r\n>   bazel-bin/tensorflow/tools/graph_transforms/summarize_graph\r\n> INFO: Elapsed time: 0.173s, Critical Path: 0.00s\r\n> \r\n> INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb'\r\n> Found 2 possible inputs: (name=models_simple/x, type=float(1), shape=[]) (name=models_simple/Placeholder, type=float(1), shape=[]) \r\n> No variables spotted.\r\n> Found 1 possible outputs: (name=models_simple/y_conv, op=Softmax) \r\n> Found 70140 (70.14k) const parameters, 0 (0) variable parameters, and 0 control_edges\r\n> Op types used: 12 Const, 6 Add, 3 Relu, 2 Placeholder, 2 MatMul, 2 MaxPool, 2 Mul, 2 Reshape, 2 Conv2D, 1 RealDiv, 1 RandomUniform, 1 Floor, 1 Shape, 1 Softmax, 1 Sub\r\n> \r\n \r\n3. Try \"Removing training-only nodes\" (bazel run tensorflow/tools/graph_transforms:transform_graph). The operation is successfully completed.\r\n\r\n> $ bazel run tensorflow/tools/graph_transforms:transform_graph -- \\       \r\n    --in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb\"  \\\r\n    --out_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb\"  \\\r\n    --inputs=\"models_simple/x\" \\\r\n    --outputs=\"models_simple/y_conv\" \\\r\n    --transforms='strip_unused_nodes(type=float,shape=\"1,256\")'\r\n\r\n> console log:\r\n> \r\n> INFO: Found 1 target...\r\n> Target //tensorflow/tools/graph_transforms:transform_graph up-to-date:\r\n>   bazel-bin/tensorflow/tools/graph_transforms/transform_graph\r\n> INFO: Elapsed time: 0.191s, Critical Path: 0.00s\r\n> \r\n> INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/transform_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/inference.pb' '--out_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb' '--inputs=models_simple/x' '--outputs=models_simple/y_conv' '--transforms=strip_unused_nodes(type=float,shape=\"1,256\")'\r\n> 2017-08-17 16:27:20.074270: I tensorflow/tools/graph_transforms/transform_graph.cc:262] Applying strip_unused_nodes\r\n> \r\n> result: \r\n> optimized_inception_graph.pb\r\n\r\n4. Since  \"dropout\" layers is left in graph, error persist when create session.\r\n\r\n> $ bazel run tensorflow/tools/graph_transforms:summarize_graph -- \\       \r\n     --in_graph=\"/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb\"\r\n\r\n> console log: \r\n> \r\n> INFO: Found 1 target...\r\n> Target //tensorflow/tools/graph_transforms:summarize_graph up-to-date:\r\n>   bazel-bin/tensorflow/tools/graph_transforms/summarize_graph\r\n> INFO: Elapsed time: 0.179s, Critical Path: 0.00s\r\n> \r\n> INFO: Running command line: bazel-bin/tensorflow/tools/graph_transforms/summarize_graph '--in_graph=/Users/xiaoqiang/Desktop/Volcano_Offline_1/optimized_inception_graph.pb'\r\n> Found 2 possible inputs: (name=models_simple/x, type=float(1), shape=[]) (name=models_simple/Placeholder, type=float(1), shape=[]) \r\n> No variables spotted.\r\n> Found 1 possible outputs: (name=models_simple/y_conv, op=Softmax) \r\n> Found 70140 (70.14k) const parameters, 0 (0) variable parameters, and 0 control_edges\r\n> Op types used: 12 Const, 6 Add, 3 Relu, 2 Placeholder, 2 MatMul, 2 MaxPool, 2 Mul, 2 Reshape, 2 Conv2D, 1 RealDiv, 1 RandomUniform, 1 Floor, 1 Shape, 1 Softmax, 1 Sub\r\n\r\n***Oher Experiments***\r\nWe also tried to remove dropout layer from training code on Ubuntu Server, then re-train model, export model to IOS, the session_t->Create() operation passed. However, we cannot use this approach as solution, because removing dropout layer before training will lead to changes of weights and other parameters in neural networks.\r\n\r\nOur objectives are as follow\r\n1. Given a model (Checkpoint + GraphDef) trained and verified on server\r\n2. process the model with tools to remove all unnecessary nodes/ops for inference.\r\n3. inference on IOS\r\n\r\n***Question***\r\nHow to remove dropout layer from model, since \"dropout\" is unnecessary in inference procedure.\r\n\r\n***Code for model generation (Python)***\r\n`           self.x = tf.placeholder(tf.float32, [None, feature_num], name = 'x')\r\n            x_image = tf.reshape(self.x, [-1, 1, feature_num, 1])\r\n            self.y_ = tf.placeholder(tf.float32, [None, label_dim],name='y_')\r\n\r\n            self.W_conv1 = weight_variable([1, self.conv_len, 1, self.num_conv1])\r\n            self.b_conv1 = bias_variable([self.num_conv1])\r\n            self.h_conv1 = tf.nn.relu(conv2d(x_image, self.W_conv1) + self.b_conv1)\r\n\r\n            self.h_pool1,pooling_factor = max_pool_1x4(self.h_conv1,pooling_factor)\r\n            self.W_conv2 = weight_variable([1, self.conv_len, self.num_conv1, self.num_conv2])\r\n            self.b_conv2 = bias_variable([self.num_conv2])\r\n            self.h_conv2 = tf.nn.relu(conv2d(self.h_pool1, self.W_conv2) + self.b_conv2)\r\n\r\n            self.h_pool2,pooling_factor = max_pool_1x2(self.h_conv2,pooling_factor)\r\n            pooled_size = int(feature_num / pooling_factor * self.num_conv2)\r\n            print (\"pooling: (%d,%d) -> %d\" % (feature_num, pooling_factor, pooled_size))\r\n            self.W_fc1 = weight_variable([pooled_size, self.num_fullc])\r\n            self.b_fc1 = bias_variable([self.num_fullc])\r\n            self.h_pool2_flat = tf.reshape(self.h_pool2, [-1, pooled_size])\r\n            self.h_fc1 = tf.nn.relu(tf.matmul(self.h_pool2_flat, self.W_fc1) + self.b_fc1)\r\n\r\n            self.keep_prob = tf.placeholder(tf.float32)\r\n            self.h_fc1_drop = tf.nn.dropout(self.h_fc1, self.keep_prob)\r\n            self.W_fc2 = weight_variable([self.num_fullc, label_dim])\r\n            self.b_fc2 = bias_variable([label_dim])\r\n\r\n            self.y_conv = tf.nn.softmax(tf.matmul(self.h_fc1_drop, self.W_fc2) + self.b_fc2, name='y_conv')\r\n\r\n            classes_weights = tf.constant([ 1.0,  0.8,  1.0, 1.0])\r\n            cross = tf.nn.weighted_cross_entropy_with_logits(self.y_,self.y_conv, pos_weight=classes_weights)\r\n            self.cross_entropy = tf.reduce_mean(cross)\r\n\r\n            # self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y_conv, labels=self.y_))\r\n            self.train_step = tf.train.AdamOptimizer(lr).minimize(self.cross_entropy)`\r\n\r\n"}