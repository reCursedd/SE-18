{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/157153298", "html_url": "https://github.com/tensorflow/tensorflow/issues/7#issuecomment-157153298", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7", "id": 157153298, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NzE1MzI5OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-16T20:00:35Z", "updated_at": "2015-11-16T20:00:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3055719\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zer0n\">@zer0n</a> It depends on your task.</p>\n<p>Returning zeros is fine if you only care about outputs (i.e., you're not hooking up to a decoder); and your loss function knows to ignore outputs past the sequence_length.</p>\n<p>Returning the state from the end of the last time step might also be considered \"wrong\", but will generally always happen if you have inputs of different lengths (and aren't performing dynamic computation).  This is a typical approach to performing RNN with minibatches.  For this reason when performing encoding/decoding, people usually right-align with left-side padding instead, so the last input of any example always corresponds to the very last state.  This seems like the cleanest solution for now.</p>\n<p>Anyway, this part of the API may change; not sure yet the best approach.</p>", "body_text": "@zer0n It depends on your task.\nReturning zeros is fine if you only care about outputs (i.e., you're not hooking up to a decoder); and your loss function knows to ignore outputs past the sequence_length.\nReturning the state from the end of the last time step might also be considered \"wrong\", but will generally always happen if you have inputs of different lengths (and aren't performing dynamic computation).  This is a typical approach to performing RNN with minibatches.  For this reason when performing encoding/decoding, people usually right-align with left-side padding instead, so the last input of any example always corresponds to the very last state.  This seems like the cleanest solution for now.\nAnyway, this part of the API may change; not sure yet the best approach.", "body": "@zer0n It depends on your task.\n\nReturning zeros is fine if you only care about outputs (i.e., you're not hooking up to a decoder); and your loss function knows to ignore outputs past the sequence_length.\n\nReturning the state from the end of the last time step might also be considered \"wrong\", but will generally always happen if you have inputs of different lengths (and aren't performing dynamic computation).  This is a typical approach to performing RNN with minibatches.  For this reason when performing encoding/decoding, people usually right-align with left-side padding instead, so the last input of any example always corresponds to the very last state.  This seems like the cleanest solution for now.\n\nAnyway, this part of the API may change; not sure yet the best approach.\n"}