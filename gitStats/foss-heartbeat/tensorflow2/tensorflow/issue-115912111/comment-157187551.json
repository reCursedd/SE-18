{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/157187551", "html_url": "https://github.com/tensorflow/tensorflow/issues/7#issuecomment-157187551", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7", "id": 157187551, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NzE4NzU1MQ==", "user": {"login": "zer0n", "id": 3055719, "node_id": "MDQ6VXNlcjMwNTU3MTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/3055719?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zer0n", "html_url": "https://github.com/zer0n", "followers_url": "https://api.github.com/users/zer0n/followers", "following_url": "https://api.github.com/users/zer0n/following{/other_user}", "gists_url": "https://api.github.com/users/zer0n/gists{/gist_id}", "starred_url": "https://api.github.com/users/zer0n/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zer0n/subscriptions", "organizations_url": "https://api.github.com/users/zer0n/orgs", "repos_url": "https://api.github.com/users/zer0n/repos", "events_url": "https://api.github.com/users/zer0n/events{/privacy}", "received_events_url": "https://api.github.com/users/zer0n/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-16T22:11:17Z", "updated_at": "2015-11-16T22:11:30Z", "author_association": "NONE", "body_html": "<p>OK, I did miss the line <code>outputs.append(output)</code>. I originally thought that it returned the final state, not a sequence of states.</p>\n<p>Anyway, this implementation still looks weird (I'm aware it's changing so I'm only discussing the current state). Usually, for truncated BPTT implementation, people pad <code>eos</code> for short sentences and truncate the sentences if the lengths are larger than <code>max_length</code>. This enables static unrolling and efficient mini-batching.</p>\n<p>The RNN example seems doing the reverse. What I see is that it's doing <em>dynamic</em> unrolling (i.e. with dynamic output size), but padding zeros to the outputs past <code>max_length</code>.</p>", "body_text": "OK, I did miss the line outputs.append(output). I originally thought that it returned the final state, not a sequence of states.\nAnyway, this implementation still looks weird (I'm aware it's changing so I'm only discussing the current state). Usually, for truncated BPTT implementation, people pad eos for short sentences and truncate the sentences if the lengths are larger than max_length. This enables static unrolling and efficient mini-batching.\nThe RNN example seems doing the reverse. What I see is that it's doing dynamic unrolling (i.e. with dynamic output size), but padding zeros to the outputs past max_length.", "body": "OK, I did miss the line `outputs.append(output)`. I originally thought that it returned the final state, not a sequence of states.\n\nAnyway, this implementation still looks weird (I'm aware it's changing so I'm only discussing the current state). Usually, for truncated BPTT implementation, people pad `eos` for short sentences and truncate the sentences if the lengths are larger than `max_length`. This enables static unrolling and efficient mini-batching.\n\nThe RNN example seems doing the reverse. What I see is that it's doing _dynamic_ unrolling (i.e. with dynamic output size), but padding zeros to the outputs past `max_length`.\n"}