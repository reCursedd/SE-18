{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/167439054", "html_url": "https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167439054", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7", "id": 167439054, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NzQzOTA1NA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-27T20:09:27Z", "updated_at": "2015-12-27T20:09:27Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The main consideration is in how calculations common across time are<br>\ncached.  Simpler caching is traded off against having RNNCell be a pure<br>\nlogic object (non-stateful).</p>\n<p>For example, if you access two Variables at each time step and then concat<br>\nthem, using the result in your <strong>call</strong> calculation, then this is something<br>\nthat should be cached beforehand because it creates redundant computation.<br>\nThe two approaches to caching are:</p>\n<ol>\n<li>RNNCell is stateful: create and cache this Tensor inside the RNNCell<br>\nobject</li>\n<li>RNNCell is non-stateful: <strong>call</strong> looks for the cached Tensor inside a<br>\ngraph collection; if it doesn't exist, it creates it (similar to using<br>\nget_variable).</li>\n</ol>\n<p>With a stateful RNNCell, Variables are created when the RNNCell is created;<br>\nand so that variable scope is used.  With a non-stateful RNNCell, Variables<br>\nare created / accessed during <strong>call</strong> and the variable scope used is<br>\nwhatever it was when you ran rnn() or bidirectional_rnn() or whatever.</p>\n<p>Because of this, moving from non-stateful RNNCell to a stateful one (and<br>\nmodifying the associated implementations of LSTM, GRU, etc cells) would be<br>\na breaking change.</p>\n<p>I personally prefer stateful objects, because it's easier to understand and<br>\ndebug them.  But there are arguments in both directions that have to be<br>\nconsidered.</p>\n<p>On Sat, Dec 26, 2015 at 2:50 PM, Michael R. Bernstein &lt;<br>\n<a href=\"mailto:notifications@github.com\">notifications@github.com</a>&gt; wrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a> what are the considerations<br>\nsurrounding this decision between stateful rnncell implementations vs.<br>\nstoring the state in the graph collections?</p>\n<p>\u2014<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115912111\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7/hovercard?comment_id=167369971&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167369971\">#7 (comment)</a><br>\n.</p>\n</blockquote>", "body_text": "The main consideration is in how calculations common across time are\ncached.  Simpler caching is traded off against having RNNCell be a pure\nlogic object (non-stateful).\nFor example, if you access two Variables at each time step and then concat\nthem, using the result in your call calculation, then this is something\nthat should be cached beforehand because it creates redundant computation.\nThe two approaches to caching are:\n\nRNNCell is stateful: create and cache this Tensor inside the RNNCell\nobject\nRNNCell is non-stateful: call looks for the cached Tensor inside a\ngraph collection; if it doesn't exist, it creates it (similar to using\nget_variable).\n\nWith a stateful RNNCell, Variables are created when the RNNCell is created;\nand so that variable scope is used.  With a non-stateful RNNCell, Variables\nare created / accessed during call and the variable scope used is\nwhatever it was when you ran rnn() or bidirectional_rnn() or whatever.\nBecause of this, moving from non-stateful RNNCell to a stateful one (and\nmodifying the associated implementations of LSTM, GRU, etc cells) would be\na breaking change.\nI personally prefer stateful objects, because it's easier to understand and\ndebug them.  But there are arguments in both directions that have to be\nconsidered.\nOn Sat, Dec 26, 2015 at 2:50 PM, Michael R. Bernstein <\nnotifications@github.com> wrote:\n\n@ebrevdo https://github.com/ebrevdo what are the considerations\nsurrounding this decision between stateful rnncell implementations vs.\nstoring the state in the graph collections?\n\u2014\nReply to this email directly or view it on GitHub\n#7 (comment)\n.", "body": "The main consideration is in how calculations common across time are\ncached.  Simpler caching is traded off against having RNNCell be a pure\nlogic object (non-stateful).\n\nFor example, if you access two Variables at each time step and then concat\nthem, using the result in your **call** calculation, then this is something\nthat should be cached beforehand because it creates redundant computation.\nThe two approaches to caching are:\n1. RNNCell is stateful: create and cache this Tensor inside the RNNCell\n   object\n2. RNNCell is non-stateful: **call** looks for the cached Tensor inside a\n   graph collection; if it doesn't exist, it creates it (similar to using\n   get_variable).\n\nWith a stateful RNNCell, Variables are created when the RNNCell is created;\nand so that variable scope is used.  With a non-stateful RNNCell, Variables\nare created / accessed during **call** and the variable scope used is\nwhatever it was when you ran rnn() or bidirectional_rnn() or whatever.\n\nBecause of this, moving from non-stateful RNNCell to a stateful one (and\nmodifying the associated implementations of LSTM, GRU, etc cells) would be\na breaking change.\n\nI personally prefer stateful objects, because it's easier to understand and\ndebug them.  But there are arguments in both directions that have to be\nconsidered.\n\nOn Sat, Dec 26, 2015 at 2:50 PM, Michael R. Bernstein <\nnotifications@github.com> wrote:\n\n> @ebrevdo https://github.com/ebrevdo what are the considerations\n> surrounding this decision between stateful rnncell implementations vs.\n> storing the state in the graph collections?\n> \n> \u2014\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/7#issuecomment-167369971\n> .\n"}