{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/418371446", "html_url": "https://github.com/tensorflow/tensorflow/issues/14746#issuecomment-418371446", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14746", "id": 418371446, "node_id": "MDEyOklzc3VlQ29tbWVudDQxODM3MTQ0Ng==", "user": {"login": "simtony", "id": 17560198, "node_id": "MDQ6VXNlcjE3NTYwMTk4", "avatar_url": "https://avatars3.githubusercontent.com/u/17560198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/simtony", "html_url": "https://github.com/simtony", "followers_url": "https://api.github.com/users/simtony/followers", "following_url": "https://api.github.com/users/simtony/following{/other_user}", "gists_url": "https://api.github.com/users/simtony/gists{/gist_id}", "starred_url": "https://api.github.com/users/simtony/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/simtony/subscriptions", "organizations_url": "https://api.github.com/users/simtony/orgs", "repos_url": "https://api.github.com/users/simtony/repos", "events_url": "https://api.github.com/users/simtony/events{/privacy}", "received_events_url": "https://api.github.com/users/simtony/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-04T13:42:02Z", "updated_at": "2018-09-04T13:42:49Z", "author_association": "NONE", "body_html": "<p>Directly drops the whole embedding matrix is inefficient for large vocabulary, since it requires <code>vocab_size*batch_size</code> random variable evaluation.<br>\nThe implementation below cuts number of random variable evaluation to <code>num_uniq_words_in_batch * batch_size</code></p>\n<pre><code># preparing inputs\nkeep_prob = 0.8\nids = tf.convert_to_tensor([[1,2,3], [3,2,1]])\nbatch_size = tf.shape(ids)[0]\nmaxlen = tf.shape(ids)[1]\nembed_matrix = tf.ones([10, 20])\nembed_ids = tf.nn.embedding_lookup(embed_matrix, ids)\n\nuniq_ids, indices = tf.unique(tf.reshape(ids, [-1]))\n# generate random mask for each uniq_id\n# independent sample for each instance\nrand_mask = tf.random_uniform([batch_size, tf.size(uniq_ids)], dtype=embeddings.dtype)\n\n# prepare indices for tf.gather_nd\nbatch_wise = tf.broadcast_to(tf.expand_dims(tf.range(batch_size), axis=-1), [batch_size, maxlen])\nuniq_ids_wise = tf.reshape(indices, [batch_size, maxlen])\n\n# gather mask and convert it to binary mask\nmask_indices = tf.stack([batch_wise, uniq_ids_wise], axis=-1)\nbinary_mask = tf.floor(tf.gather_nd(rand_mask, mask_indices) + keep_prob)\n\n# apply mask and scale\ndropped_embeddings = embed_ids * tf.expand_dims(binary_mask, axis=-1) / keep_prob\n</code></pre>\n<p>And here is the result:</p>\n<pre><code>In [52]: dropped_embeddings.eval()\nOut[52]: \narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\n      dtype=float32)\n\nIn [53]: dropped_embeddings.eval()\nOut[53]: \narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]],\n\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\n      dtype=float32)\n</code></pre>", "body_text": "Directly drops the whole embedding matrix is inefficient for large vocabulary, since it requires vocab_size*batch_size random variable evaluation.\nThe implementation below cuts number of random variable evaluation to num_uniq_words_in_batch * batch_size\n# preparing inputs\nkeep_prob = 0.8\nids = tf.convert_to_tensor([[1,2,3], [3,2,1]])\nbatch_size = tf.shape(ids)[0]\nmaxlen = tf.shape(ids)[1]\nembed_matrix = tf.ones([10, 20])\nembed_ids = tf.nn.embedding_lookup(embed_matrix, ids)\n\nuniq_ids, indices = tf.unique(tf.reshape(ids, [-1]))\n# generate random mask for each uniq_id\n# independent sample for each instance\nrand_mask = tf.random_uniform([batch_size, tf.size(uniq_ids)], dtype=embeddings.dtype)\n\n# prepare indices for tf.gather_nd\nbatch_wise = tf.broadcast_to(tf.expand_dims(tf.range(batch_size), axis=-1), [batch_size, maxlen])\nuniq_ids_wise = tf.reshape(indices, [batch_size, maxlen])\n\n# gather mask and convert it to binary mask\nmask_indices = tf.stack([batch_wise, uniq_ids_wise], axis=-1)\nbinary_mask = tf.floor(tf.gather_nd(rand_mask, mask_indices) + keep_prob)\n\n# apply mask and scale\ndropped_embeddings = embed_ids * tf.expand_dims(binary_mask, axis=-1) / keep_prob\n\nAnd here is the result:\nIn [52]: dropped_embeddings.eval()\nOut[52]: \narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\n      dtype=float32)\n\nIn [53]: dropped_embeddings.eval()\nOut[53]: \narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]],\n\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\n      dtype=float32)", "body": "Directly drops the whole embedding matrix is inefficient for large vocabulary, since it requires `vocab_size*batch_size` random variable evaluation.\r\nThe implementation below cuts number of random variable evaluation to `num_uniq_words_in_batch * batch_size`\r\n```\r\n# preparing inputs\r\nkeep_prob = 0.8\r\nids = tf.convert_to_tensor([[1,2,3], [3,2,1]])\r\nbatch_size = tf.shape(ids)[0]\r\nmaxlen = tf.shape(ids)[1]\r\nembed_matrix = tf.ones([10, 20])\r\nembed_ids = tf.nn.embedding_lookup(embed_matrix, ids)\r\n\r\nuniq_ids, indices = tf.unique(tf.reshape(ids, [-1]))\r\n# generate random mask for each uniq_id\r\n# independent sample for each instance\r\nrand_mask = tf.random_uniform([batch_size, tf.size(uniq_ids)], dtype=embeddings.dtype)\r\n\r\n# prepare indices for tf.gather_nd\r\nbatch_wise = tf.broadcast_to(tf.expand_dims(tf.range(batch_size), axis=-1), [batch_size, maxlen])\r\nuniq_ids_wise = tf.reshape(indices, [batch_size, maxlen])\r\n\r\n# gather mask and convert it to binary mask\r\nmask_indices = tf.stack([batch_wise, uniq_ids_wise], axis=-1)\r\nbinary_mask = tf.floor(tf.gather_nd(rand_mask, mask_indices) + keep_prob)\r\n\r\n# apply mask and scale\r\ndropped_embeddings = embed_ids * tf.expand_dims(binary_mask, axis=-1) / keep_prob\r\n```\r\n\r\nAnd here is the result:\r\n```\r\nIn [52]: dropped_embeddings.eval()\r\nOut[52]: \r\narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\r\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\r\n\r\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\r\n         0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\r\n      dtype=float32)\r\n\r\nIn [53]: dropped_embeddings.eval()\r\nOut[53]: \r\narray([[[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]],\r\n\r\n       [[1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25],\r\n        [1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25,\r\n         1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25, 1.25]]],\r\n      dtype=float32)\r\n```\r\n\r\n"}