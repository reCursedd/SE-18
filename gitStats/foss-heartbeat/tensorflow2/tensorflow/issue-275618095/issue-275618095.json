{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14746", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14746/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14746/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14746/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14746", "id": 275618095, "node_id": "MDU6SXNzdWUyNzU2MTgwOTU=", "number": 14746, "title": "How to use `tf.nn.dropout` to implement embedding dropout", "user": {"login": "zotroneneis", "id": 15320635, "node_id": "MDQ6VXNlcjE1MzIwNjM1", "avatar_url": "https://avatars0.githubusercontent.com/u/15320635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zotroneneis", "html_url": "https://github.com/zotroneneis", "followers_url": "https://api.github.com/users/zotroneneis/followers", "following_url": "https://api.github.com/users/zotroneneis/following{/other_user}", "gists_url": "https://api.github.com/users/zotroneneis/gists{/gist_id}", "starred_url": "https://api.github.com/users/zotroneneis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zotroneneis/subscriptions", "organizations_url": "https://api.github.com/users/zotroneneis/orgs", "repos_url": "https://api.github.com/users/zotroneneis/repos", "events_url": "https://api.github.com/users/zotroneneis/events{/privacy}", "received_events_url": "https://api.github.com/users/zotroneneis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-11-21T08:01:09Z", "updated_at": "2018-09-04T13:42:49Z", "closed_at": "2017-11-23T11:54:12Z", "author_association": "NONE", "body_html": "<p>Recent papers in language modeling use a specific form of embedding dropout that was proposed in <a href=\"https://arxiv.org/pdf/1512.05287.pdf\" rel=\"nofollow\">this paper</a>. The paper also proposed variational recurrent dropout which was discussed already in <a href=\"https://github.com/tensorflow/tensorflow/issues/7927\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7927/hovercard\">this issue</a>.</p>\n<p>In embedding dropout, the same dropout mask is used at each timestep and entire words are dropped (i.e. the whole word vector of a word is set to zero). This behavior can be achieved by providing a <code>noise_shape</code> to <code>tf.nn.dropout</code>.  In addition, the same words are dropped throughout a sequence:</p>\n<p>\"Since we repeat the same mask at each time step, we drop the same words throughout the sequence \u2013 i.e. we drop word types at random rather than word tokens (as an example, the sentence \u201cthe dog and the cat\u201d might become \u201c\u2014 dog and \u2014 cat\u201d or \u201cthe \u2014 and the cat\u201d, but never \u201c\u2014 dog and the cat\u201d). \"</p>\n<p>I couldn't find a way to implement this functionality of embedding dropout efficiently. Are there any plans to incorporate these advances?</p>", "body_text": "Recent papers in language modeling use a specific form of embedding dropout that was proposed in this paper. The paper also proposed variational recurrent dropout which was discussed already in this issue.\nIn embedding dropout, the same dropout mask is used at each timestep and entire words are dropped (i.e. the whole word vector of a word is set to zero). This behavior can be achieved by providing a noise_shape to tf.nn.dropout.  In addition, the same words are dropped throughout a sequence:\n\"Since we repeat the same mask at each time step, we drop the same words throughout the sequence \u2013 i.e. we drop word types at random rather than word tokens (as an example, the sentence \u201cthe dog and the cat\u201d might become \u201c\u2014 dog and \u2014 cat\u201d or \u201cthe \u2014 and the cat\u201d, but never \u201c\u2014 dog and the cat\u201d). \"\nI couldn't find a way to implement this functionality of embedding dropout efficiently. Are there any plans to incorporate these advances?", "body": "Recent papers in language modeling use a specific form of embedding dropout that was proposed in [this paper](https://arxiv.org/pdf/1512.05287.pdf). The paper also proposed variational recurrent dropout which was discussed already in [this issue](https://github.com/tensorflow/tensorflow/issues/7927).\r\n\r\nIn embedding dropout, the same dropout mask is used at each timestep and entire words are dropped (i.e. the whole word vector of a word is set to zero). This behavior can be achieved by providing a `noise_shape` to `tf.nn.dropout`.  In addition, the same words are dropped throughout a sequence: \r\n\r\n\"Since we repeat the same mask at each time step, we drop the same words throughout the sequence \u2013 i.e. we drop word types at random rather than word tokens (as an example, the sentence \u201cthe dog and the cat\u201d might become \u201c\u2014 dog and \u2014 cat\u201d or \u201cthe \u2014 and the cat\u201d, but never \u201c\u2014 dog and the cat\u201d). \"\r\n\r\nI couldn't find a way to implement this functionality of embedding dropout efficiently. Are there any plans to incorporate these advances?"}