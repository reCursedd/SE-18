{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/271717765", "html_url": "https://github.com/tensorflow/tensorflow/issues/6752#issuecomment-271717765", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6752", "id": 271717765, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MTcxNzc2NQ==", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-10T22:25:17Z", "updated_at": "2017-01-10T22:25:17Z", "author_association": "MEMBER", "body_html": "<p>It is correct that the Eigen implementation deliberately falls back to a sequential implementation for matrix-vector multiplication. The reason is that there are specialized sequential matvec kernels in core Eigen that are approximately 4x faster than the matmul kernel called by the parallelized tensor contraction code used for matmul in TensorFlow.</p>\n<p>In other words, if using the matmul kernel, you'd typically have to burn &gt;4x CPU cycles to match the performance of the sequential kernels. Based on some real-world use cases we decided to stick with this and rely on model-level parallelism.</p>\n<p>I agree that there is room for improvement and that it would be awesome if XLA could contribute better code for this.</p>\n<p>Top-level logic in tensorflow (batch_matmul is the new matmul):<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/batch_matmul_op_impl.h#L212\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/batch_matmul_op_impl.h#L212</a></p>\n<p>matvec kernels: <a href=\"https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralMatrixVector.h?at=default&amp;fileviewer=file-view-default#GeneralMatrixVector.h-31\" rel=\"nofollow\">https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralMatrixVector.h?at=default&amp;fileviewer=file-view-default#GeneralMatrixVector.h-31</a></p>\n<p>matmul block panel kernels:<br>\n<a href=\"https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralBlockPanelKernel.h?at=default&amp;fileviewer=file-view-default\" rel=\"nofollow\">https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralBlockPanelKernel.h?at=default&amp;fileviewer=file-view-default</a></p>", "body_text": "It is correct that the Eigen implementation deliberately falls back to a sequential implementation for matrix-vector multiplication. The reason is that there are specialized sequential matvec kernels in core Eigen that are approximately 4x faster than the matmul kernel called by the parallelized tensor contraction code used for matmul in TensorFlow.\nIn other words, if using the matmul kernel, you'd typically have to burn >4x CPU cycles to match the performance of the sequential kernels. Based on some real-world use cases we decided to stick with this and rely on model-level parallelism.\nI agree that there is room for improvement and that it would be awesome if XLA could contribute better code for this.\nTop-level logic in tensorflow (batch_matmul is the new matmul):\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/batch_matmul_op_impl.h#L212\nmatvec kernels: https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralMatrixVector.h?at=default&fileviewer=file-view-default#GeneralMatrixVector.h-31\nmatmul block panel kernels:\nhttps://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralBlockPanelKernel.h?at=default&fileviewer=file-view-default", "body": "It is correct that the Eigen implementation deliberately falls back to a sequential implementation for matrix-vector multiplication. The reason is that there are specialized sequential matvec kernels in core Eigen that are approximately 4x faster than the matmul kernel called by the parallelized tensor contraction code used for matmul in TensorFlow.\r\n\r\nIn other words, if using the matmul kernel, you'd typically have to burn >4x CPU cycles to match the performance of the sequential kernels. Based on some real-world use cases we decided to stick with this and rely on model-level parallelism. \r\n\r\nI agree that there is room for improvement and that it would be awesome if XLA could contribute better code for this.\r\n\r\nTop-level logic in tensorflow (batch_matmul is the new matmul):\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/batch_matmul_op_impl.h#L212\r\n\r\nmatvec kernels: https://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralMatrixVector.h?at=default&fileviewer=file-view-default#GeneralMatrixVector.h-31\r\n\r\nmatmul block panel kernels:\r\nhttps://bitbucket.org/eigen/eigen/src/e6d6c87e1e27ba39a8f4484366ead7d31e6e97a4/Eigen/src/Core/products/GeneralBlockPanelKernel.h?at=default&fileviewer=file-view-default\r\n"}