{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/323415245", "html_url": "https://github.com/tensorflow/tensorflow/issues/12387#issuecomment-323415245", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12387", "id": 323415245, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMzQxNTI0NQ==", "user": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-18T17:35:42Z", "updated_at": "2017-08-18T17:44:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is on CPU?</p>\n<p>There are likely two issues:</p>\n<ol>\n<li>\n<p>The CPU reductions use a numerically terrible algorithm for doing sums.  Summing into one or few accumulators - this leads to inaccurate sums for large N.  Breaking the sum in 3 parts ameliorates this problem.  This problem has come up multiple times before.</p>\n</li>\n<li>\n<p>The mean reductions are also done by dividing the inputs by N and then summing which losing some accuracy in the mantissa (likely not the big issue here).</p>\n</li>\n</ol>\n<p>The GPU uses a more numerically stable sum algorithm, so (1) is not a problem there.  And the GPU mean calculation will soon also fix (2).  Using float64 is less of a cost on CPU than GPU, so if accurate reductions are important, the best workaround for now is probably to change the type before the reduction to float64 and after back to float32.  Breaking along multiple dimensions also works if you have a multi-dimensional tensor.</p>", "body_text": "This is on CPU?\nThere are likely two issues:\n\n\nThe CPU reductions use a numerically terrible algorithm for doing sums.  Summing into one or few accumulators - this leads to inaccurate sums for large N.  Breaking the sum in 3 parts ameliorates this problem.  This problem has come up multiple times before.\n\n\nThe mean reductions are also done by dividing the inputs by N and then summing which losing some accuracy in the mantissa (likely not the big issue here).\n\n\nThe GPU uses a more numerically stable sum algorithm, so (1) is not a problem there.  And the GPU mean calculation will soon also fix (2).  Using float64 is less of a cost on CPU than GPU, so if accurate reductions are important, the best workaround for now is probably to change the type before the reduction to float64 and after back to float32.  Breaking along multiple dimensions also works if you have a multi-dimensional tensor.", "body": "This is on CPU?  \r\n\r\nThere are likely two issues:\r\n\r\n1) The CPU reductions use a numerically terrible algorithm for doing sums.  Summing into one or few accumulators - this leads to inaccurate sums for large N.  Breaking the sum in 3 parts ameliorates this problem.  This problem has come up multiple times before.\r\n\r\n2) The mean reductions are also done by dividing the inputs by N and then summing which losing some accuracy in the mantissa (likely not the big issue here).\r\n\r\nThe GPU uses a more numerically stable sum algorithm, so (1) is not a problem there.  And the GPU mean calculation will soon also fix (2).  Using float64 is less of a cost on CPU than GPU, so if accurate reductions are important, the best workaround for now is probably to change the type before the reduction to float64 and after back to float32.  Breaking along multiple dimensions also works if you have a multi-dimensional tensor."}