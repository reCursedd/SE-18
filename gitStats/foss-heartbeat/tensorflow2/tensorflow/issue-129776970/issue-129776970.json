{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/932", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/932/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/932/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/932/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/932", "id": 129776970, "node_id": "MDU6SXNzdWUxMjk3NzY5NzA=", "number": 932, "title": "thread pool deadlocks on shutdown", "user": {"login": "dvyukov", "id": 1095328, "node_id": "MDQ6VXNlcjEwOTUzMjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1095328?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dvyukov", "html_url": "https://github.com/dvyukov", "followers_url": "https://api.github.com/users/dvyukov/followers", "following_url": "https://api.github.com/users/dvyukov/following{/other_user}", "gists_url": "https://api.github.com/users/dvyukov/gists{/gist_id}", "starred_url": "https://api.github.com/users/dvyukov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dvyukov/subscriptions", "organizations_url": "https://api.github.com/users/dvyukov/orgs", "repos_url": "https://api.github.com/users/dvyukov/repos", "events_url": "https://api.github.com/users/dvyukov/events{/privacy}", "received_events_url": "https://api.github.com/users/dvyukov/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-01-29T14:13:14Z", "updated_at": "2017-02-09T22:02:22Z", "closed_at": "2016-06-07T07:56:18Z", "author_association": "NONE", "body_html": "<p>ThreadPool dtor does not pop waiters from waiters_ list. As the result dead waiters are left on the list. If remaining tasks submit new tasks, thread pool deadlocks because some notifications are consumed by the leftover dead waiters instead of alive threads that should receive the notifications.</p>\n<p>Here is a simple test that does classical parallel decomposition and reliably deadlocks:</p>\n<div class=\"highlight highlight-source-c\"><pre><span class=\"pl-k\">static</span> <span class=\"pl-k\">void</span> <span class=\"pl-en\">BM_ParallelDivide</span>(<span class=\"pl-k\">int</span> iters, <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* impl) {\n  THREAD_POOL_IMPL_NAME = impl;\n  <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i = <span class=\"pl-c1\">0</span>; i &lt; iters; i++) {\n    <span class=\"pl-k\">const</span> <span class=\"pl-k\">int</span> <span class=\"pl-c1\">kTasks</span> = <span class=\"pl-c1\">10</span>;\n    <span class=\"pl-k\">const</span> <span class=\"pl-k\">int</span> <span class=\"pl-c1\">kLevels</span> = <span class=\"pl-c1\">22</span>;\n    std::atomic&lt;<span class=\"pl-k\">unsigned</span>&gt; <span class=\"pl-c1\">count</span>(<span class=\"pl-c1\">kTasks</span> * (<span class=\"pl-c1\">1</span> &lt;&lt; <span class=\"pl-c1\">kLevels</span>));\n    mutex done_lock;\n    condition_variable done;\n    <span class=\"pl-k\">bool</span> done_flag = <span class=\"pl-c1\">false</span>;\n    std::function&lt;<span class=\"pl-c1\">void</span>(<span class=\"pl-k\">int</span>)&gt; work;\n    ThreadPool <span class=\"pl-smi\">pool</span>(<span class=\"pl-c1\">Env::Default</span>(), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">kNumThreads</span>);\n    work = [&amp;pool, &amp;work, &amp;count, &amp;done_lock, &amp;done, &amp;done_flag](<span class=\"pl-k\">int</span> level) {\n      <span class=\"pl-k\">if</span> (level-- &gt; <span class=\"pl-c1\">0</span>) {\n        pool.<span class=\"pl-c1\">Schedule</span>([&amp;work, level]() { <span class=\"pl-c1\">work</span>(level); });\n        pool.<span class=\"pl-c1\">Schedule</span>([&amp;work, level]() { <span class=\"pl-c1\">work</span>(level); });\n        <span class=\"pl-k\">return</span>;\n      }\n      <span class=\"pl-c1\">delay</span>();\n    };\n    <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> t = <span class=\"pl-c1\">0</span>; t &lt; <span class=\"pl-c1\">kTasks</span>; ++t) {\n      pool.<span class=\"pl-c1\">Schedule</span>([&amp;work]() {\n        <span class=\"pl-c1\">work</span>(<span class=\"pl-c1\">kLevels</span>);\n      });\n    }\n  }\n}</pre></div>", "body_text": "ThreadPool dtor does not pop waiters from waiters_ list. As the result dead waiters are left on the list. If remaining tasks submit new tasks, thread pool deadlocks because some notifications are consumed by the leftover dead waiters instead of alive threads that should receive the notifications.\nHere is a simple test that does classical parallel decomposition and reliably deadlocks:\nstatic void BM_ParallelDivide(int iters, const char* impl) {\n  THREAD_POOL_IMPL_NAME = impl;\n  for (int i = 0; i < iters; i++) {\n    const int kTasks = 10;\n    const int kLevels = 22;\n    std::atomic<unsigned> count(kTasks * (1 << kLevels));\n    mutex done_lock;\n    condition_variable done;\n    bool done_flag = false;\n    std::function<void(int)> work;\n    ThreadPool pool(Env::Default(), \"test\", kNumThreads);\n    work = [&pool, &work, &count, &done_lock, &done, &done_flag](int level) {\n      if (level-- > 0) {\n        pool.Schedule([&work, level]() { work(level); });\n        pool.Schedule([&work, level]() { work(level); });\n        return;\n      }\n      delay();\n    };\n    for (int t = 0; t < kTasks; ++t) {\n      pool.Schedule([&work]() {\n        work(kLevels);\n      });\n    }\n  }\n}", "body": "ThreadPool dtor does not pop waiters from waiters_ list. As the result dead waiters are left on the list. If remaining tasks submit new tasks, thread pool deadlocks because some notifications are consumed by the leftover dead waiters instead of alive threads that should receive the notifications.\n\nHere is a simple test that does classical parallel decomposition and reliably deadlocks:\n\n``` c\nstatic void BM_ParallelDivide(int iters, const char* impl) {\n  THREAD_POOL_IMPL_NAME = impl;\n  for (int i = 0; i < iters; i++) {\n    const int kTasks = 10;\n    const int kLevels = 22;\n    std::atomic<unsigned> count(kTasks * (1 << kLevels));\n    mutex done_lock;\n    condition_variable done;\n    bool done_flag = false;\n    std::function<void(int)> work;\n    ThreadPool pool(Env::Default(), \"test\", kNumThreads);\n    work = [&pool, &work, &count, &done_lock, &done, &done_flag](int level) {\n      if (level-- > 0) {\n        pool.Schedule([&work, level]() { work(level); });\n        pool.Schedule([&work, level]() { work(level); });\n        return;\n      }\n      delay();\n    };\n    for (int t = 0; t < kTasks; ++t) {\n      pool.Schedule([&work]() {\n        work(kLevels);\n      });\n    }\n  }\n}\n```\n"}