{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12371", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12371/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12371/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12371/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12371", "id": 251063918, "node_id": "MDU6SXNzdWUyNTEwNjM5MTg=", "number": 12371, "title": "tf.contrib.distributions.NegativeBinomial numerical stability (inf)", "user": {"login": "EdeMeijer", "id": 5758565, "node_id": "MDQ6VXNlcjU3NTg1NjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/5758565?v=4", "gravatar_id": "", "url": "https://api.github.com/users/EdeMeijer", "html_url": "https://github.com/EdeMeijer", "followers_url": "https://api.github.com/users/EdeMeijer/followers", "following_url": "https://api.github.com/users/EdeMeijer/following{/other_user}", "gists_url": "https://api.github.com/users/EdeMeijer/gists{/gist_id}", "starred_url": "https://api.github.com/users/EdeMeijer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/EdeMeijer/subscriptions", "organizations_url": "https://api.github.com/users/EdeMeijer/orgs", "repos_url": "https://api.github.com/users/EdeMeijer/repos", "events_url": "https://api.github.com/users/EdeMeijer/events{/privacy}", "received_events_url": "https://api.github.com/users/EdeMeijer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-08-17T20:33:23Z", "updated_at": "2017-10-27T09:29:51Z", "closed_at": "2017-10-25T22:35:32Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2.1</li>\n<li><strong>Python version</strong>: 3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I'm working on a negative binomial regression model and ran into <code>inf</code>/<code>nan</code> values during training. I was able to trace it back to the NegativeBinomial implementation, and specificially when dealing with relatively large probability logits.</p>\n<p>Example to demonstrate the issue:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nlogits <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">5.0</span>, <span class=\"pl-c1\">10.0</span>, <span class=\"pl-c1\">15.0</span>, <span class=\"pl-c1\">16.0</span>, <span class=\"pl-c1\">17.0</span>, <span class=\"pl-c1\">20.0</span>, <span class=\"pl-c1\">50.0</span>])\nnb <span class=\"pl-k\">=</span> tf.contrib.distributions.NegativeBinomial(<span class=\"pl-c1\">10.0</span>, logits)\n\nsess <span class=\"pl-k\">=</span> tf.Session()\nx <span class=\"pl-k\">=</span> sess.run(nb.log_prob(<span class=\"pl-c1\">10.0</span>))\n<span class=\"pl-c1\">print</span>(x)</pre></div>\n<p>Results in</p>\n<pre><code>[  -4.83159161  -38.70070267  -88.56268311 -137.00408936 -147.99020386\n          -inf          -inf          -inf]\n</code></pre>\n<p>Looking at the values before going to <code>-inf</code> (somewhere between 16.0 and 17.0) it doesn't seem to approaching any sort of limit, so I figured something else was going on.</p>\n<p>Inside the <code>NegativeBinomial</code> class, the provides logits are first transformed to real probabilities by passing them through a sigmoid, which is <code>1.0 / (exp(-x) + 1.0)</code>. Later, in <code>_log_unnormalized_prob</code>, these probabilities go through a <code>log1p(-x)</code> function. The problem is that for big values and limited floating point precision, sigmoid will output exactly 1.0, which results in taking the log of -1.0 + 1.0, and the log of 0 yields negative infinity.</p>\n<p>I first tried to fix it by simplifying the combination of functions and getting rid of the sigmoid.</p>\n<p>I replaced <code> math_ops.log1p(-self.probs)</code> with <code>tf.log(1.0 / (tf.exp(self.logits) + 1.0))</code>, which is equivalent (<code>log((1 / (1 + exp(-x)) + 1) = log(1 / (exp(x) + 1))</code> but gives better numerical stability. For the same script but with higher values (1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0, 70.0, 80.0, 90.0), the output now is:</p>\n<pre><code>[  -4.8315897   -38.70066452  -88.567276   -138.56636047 -148.56636047\n -158.56636047 -188.56636047 -488.56634521 -688.56634521 -788.56634521\n          -inf]\n</code></pre>\n<p>This is better, but it still goes to -inf. Finally, I inspected the raw values of the function I changed, and noticed that it converges to <code>-x</code> somewhere after 15.0 as input, so my final solution looks like this:</p>\n<div class=\"highlight highlight-source-python\"><pre>tf.where(<span class=\"pl-c1\">self</span>.logits <span class=\"pl-k\">&lt;=</span> <span class=\"pl-c1\">20.0</span>, <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>tf.log(<span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">/</span> (tf.exp(<span class=\"pl-c1\">self</span>.logits) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1.0</span>)), <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">self</span>.logits)</pre></div>\n<p>So ultimately I changed from</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_log_unnormalized_prob</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.validate_args:\n        x <span class=\"pl-k\">=</span> distribution_util.embed_check_nonnegative_integer_form(x)\n    <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">self</span>.total_count <span class=\"pl-k\">*</span> math_ops.log1p(<span class=\"pl-k\">-</span><span class=\"pl-c1\">self</span>.probs)\n            <span class=\"pl-k\">+</span> x <span class=\"pl-k\">*</span> math_ops.log(<span class=\"pl-c1\">self</span>.probs))</pre></div>\n<p>to</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">_log_unnormalized_prob</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.validate_args:\n        x <span class=\"pl-k\">=</span> distribution_util.embed_check_nonnegative_integer_form(x)\n    <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">self</span>.total_count\n            <span class=\"pl-k\">*</span> tf.where(<span class=\"pl-c1\">self</span>.logits <span class=\"pl-k\">&lt;=</span> <span class=\"pl-c1\">20.0</span>, <span class=\"pl-v\">x</span><span class=\"pl-k\">=</span>tf.log(<span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">/</span> (tf.exp(<span class=\"pl-c1\">self</span>.logits) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1.0</span>)), <span class=\"pl-v\">y</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">self</span>.logits)\n            <span class=\"pl-k\">+</span> x <span class=\"pl-k\">*</span> math_ops.log(<span class=\"pl-c1\">self</span>.probs))</pre></div>\n<p>which gives the correct probabilities for arbitrarily large probability logits.</p>\n<p>I don't feel like creating a PR since I'm not convinced this is the most elegant solution, and not sure whether this problem might exist at other placed in the code base, for example other distribution types. For now, I made my own copy of BinomialDistribution with this fix included, and I'd like to invite you to verify the issue, check the math and perhaps come up with a better solution.</p>\n<p>Thanks.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.2.1\nPython version: 3\nBazel version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nI'm working on a negative binomial regression model and ran into inf/nan values during training. I was able to trace it back to the NegativeBinomial implementation, and specificially when dealing with relatively large probability logits.\nExample to demonstrate the issue:\nimport tensorflow as tf\n\nlogits = tf.constant([1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0])\nnb = tf.contrib.distributions.NegativeBinomial(10.0, logits)\n\nsess = tf.Session()\nx = sess.run(nb.log_prob(10.0))\nprint(x)\nResults in\n[  -4.83159161  -38.70070267  -88.56268311 -137.00408936 -147.99020386\n          -inf          -inf          -inf]\n\nLooking at the values before going to -inf (somewhere between 16.0 and 17.0) it doesn't seem to approaching any sort of limit, so I figured something else was going on.\nInside the NegativeBinomial class, the provides logits are first transformed to real probabilities by passing them through a sigmoid, which is 1.0 / (exp(-x) + 1.0). Later, in _log_unnormalized_prob, these probabilities go through a log1p(-x) function. The problem is that for big values and limited floating point precision, sigmoid will output exactly 1.0, which results in taking the log of -1.0 + 1.0, and the log of 0 yields negative infinity.\nI first tried to fix it by simplifying the combination of functions and getting rid of the sigmoid.\nI replaced  math_ops.log1p(-self.probs) with tf.log(1.0 / (tf.exp(self.logits) + 1.0)), which is equivalent (log((1 / (1 + exp(-x)) + 1) = log(1 / (exp(x) + 1)) but gives better numerical stability. For the same script but with higher values (1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0, 70.0, 80.0, 90.0), the output now is:\n[  -4.8315897   -38.70066452  -88.567276   -138.56636047 -148.56636047\n -158.56636047 -188.56636047 -488.56634521 -688.56634521 -788.56634521\n          -inf]\n\nThis is better, but it still goes to -inf. Finally, I inspected the raw values of the function I changed, and noticed that it converges to -x somewhere after 15.0 as input, so my final solution looks like this:\ntf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\nSo ultimately I changed from\ndef _log_unnormalized_prob(self, x):\n    if self.validate_args:\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\n    return (self.total_count * math_ops.log1p(-self.probs)\n            + x * math_ops.log(self.probs))\nto\ndef _log_unnormalized_prob(self, x):\n    if self.validate_args:\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\n    return (self.total_count\n            * tf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\n            + x * math_ops.log(self.probs))\nwhich gives the correct probabilities for arbitrarily large probability logits.\nI don't feel like creating a PR since I'm not convinced this is the most elegant solution, and not sure whether this problem might exist at other placed in the code base, for example other distribution types. For now, I made my own copy of BinomialDistribution with this fix included, and I'd like to invite you to verify the issue, check the math and perhaps come up with a better solution.\nThanks.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**: 3\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\n\r\nI'm working on a negative binomial regression model and ran into `inf`/`nan` values during training. I was able to trace it back to the NegativeBinomial implementation, and specificially when dealing with relatively large probability logits.\r\n\r\nExample to demonstrate the issue:\r\n```python\r\nimport tensorflow as tf\r\n\r\nlogits = tf.constant([1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0])\r\nnb = tf.contrib.distributions.NegativeBinomial(10.0, logits)\r\n\r\nsess = tf.Session()\r\nx = sess.run(nb.log_prob(10.0))\r\nprint(x)\r\n```\r\nResults in\r\n```\r\n[  -4.83159161  -38.70070267  -88.56268311 -137.00408936 -147.99020386\r\n          -inf          -inf          -inf]\r\n```\r\n\r\nLooking at the values before going to `-inf` (somewhere between 16.0 and 17.0) it doesn't seem to approaching any sort of limit, so I figured something else was going on.\r\n\r\nInside the `NegativeBinomial` class, the provides logits are first transformed to real probabilities by passing them through a sigmoid, which is `1.0 / (exp(-x) + 1.0)`. Later, in `_log_unnormalized_prob`, these probabilities go through a `log1p(-x)` function. The problem is that for big values and limited floating point precision, sigmoid will output exactly 1.0, which results in taking the log of -1.0 + 1.0, and the log of 0 yields negative infinity. \r\n\r\nI first tried to fix it by simplifying the combination of functions and getting rid of the sigmoid.\r\n\r\nI replaced ` math_ops.log1p(-self.probs)` with `tf.log(1.0 / (tf.exp(self.logits) + 1.0))`, which is equivalent (`log((1 / (1 + exp(-x)) + 1) = log(1 / (exp(x) + 1))` but gives better numerical stability. For the same script but with higher values (1.0, 5.0, 10.0, 15.0, 16.0, 17.0, 20.0, 50.0, 70.0, 80.0, 90.0), the output now is:\r\n```\r\n[  -4.8315897   -38.70066452  -88.567276   -138.56636047 -148.56636047\r\n -158.56636047 -188.56636047 -488.56634521 -688.56634521 -788.56634521\r\n          -inf]\r\n```\r\n\r\nThis is better, but it still goes to -inf. Finally, I inspected the raw values of the function I changed, and noticed that it converges to `-x` somewhere after 15.0 as input, so my final solution looks like this:\r\n```python\r\ntf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\r\n```\r\n\r\nSo ultimately I changed from\r\n```python\r\ndef _log_unnormalized_prob(self, x):\r\n    if self.validate_args:\r\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\r\n    return (self.total_count * math_ops.log1p(-self.probs)\r\n            + x * math_ops.log(self.probs))\r\n```\r\nto\r\n```python\r\ndef _log_unnormalized_prob(self, x):\r\n    if self.validate_args:\r\n        x = distribution_util.embed_check_nonnegative_integer_form(x)\r\n    return (self.total_count\r\n            * tf.where(self.logits <= 20.0, x=tf.log(1.0 / (tf.exp(self.logits) + 1.0)), y=-self.logits)\r\n            + x * math_ops.log(self.probs))\r\n```\r\n\r\nwhich gives the correct probabilities for arbitrarily large probability logits.\r\n\r\nI don't feel like creating a PR since I'm not convinced this is the most elegant solution, and not sure whether this problem might exist at other placed in the code base, for example other distribution types. For now, I made my own copy of BinomialDistribution with this fix included, and I'd like to invite you to verify the issue, check the math and perhaps come up with a better solution.\r\n\r\nThanks."}