{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9797", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9797/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9797/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9797/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9797", "id": 227376485, "node_id": "MDU6SXNzdWUyMjczNzY0ODU=", "number": 9797, "title": "Bijector caching breaks when used with TransformedDistribution", "user": {"login": "admcrae", "id": 11247551, "node_id": "MDQ6VXNlcjExMjQ3NTUx", "avatar_url": "https://avatars3.githubusercontent.com/u/11247551?v=4", "gravatar_id": "", "url": "https://api.github.com/users/admcrae", "html_url": "https://github.com/admcrae", "followers_url": "https://api.github.com/users/admcrae/followers", "following_url": "https://api.github.com/users/admcrae/following{/other_user}", "gists_url": "https://api.github.com/users/admcrae/gists{/gist_id}", "starred_url": "https://api.github.com/users/admcrae/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/admcrae/subscriptions", "organizations_url": "https://api.github.com/users/admcrae/orgs", "repos_url": "https://api.github.com/users/admcrae/repos", "events_url": "https://api.github.com/users/admcrae/events{/privacy}", "received_events_url": "https://api.github.com/users/admcrae/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-05-09T13:58:47Z", "updated_at": "2017-10-21T21:22:44Z", "closed_at": "2017-10-21T21:22:44Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Currently, the caching that <a href=\"https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/bijector_impl.py#L114\">Bijector</a> objects do to avoid unnecessary calculations does not work when the bijector is used in a <a href=\"https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/transformed_distribution.py#L124\">TransformedDistribution</a> object. I believe the culprit is the reshaping that the distribution object does <a href=\"https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/distribution.py#L640\">here</a>; when we call Bijector.inverse on the output, the Bijector object cannot tell that this is merely a reshaped version of what it calculated previously.</p>\n<p>My use case is to sample from a TransformedDistribution and then later calculate the log probability of that sample.</p>\n<p>This issue is particularly a problem when using a bijector whose inverse is numerically delicate (in my case, I'm chaining together softplus bijectors and my own custom affine bijector).</p>\n<p>I'm willing to work on a fix for this problem, but I'm not sure what the best way to do it is (adding caching to the TransformedDistribution code might work, but that seems like code duplication).</p>\n<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>: Source</li>\n<li><strong>TensorFlow version</strong>: v1.1.0-rc2-773-g7fa0cf3 (commit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64\"><tt>7fa0cf3</tt></a>)</li>\n<li><strong>Bazel version</strong>: 0.4.5</li>\n</ul>", "body_text": "Currently, the caching that Bijector objects do to avoid unnecessary calculations does not work when the bijector is used in a TransformedDistribution object. I believe the culprit is the reshaping that the distribution object does here; when we call Bijector.inverse on the output, the Bijector object cannot tell that this is merely a reshaped version of what it calculated previously.\nMy use case is to sample from a TransformedDistribution and then later calculate the log probability of that sample.\nThis issue is particularly a problem when using a bijector whose inverse is numerically delicate (in my case, I'm chaining together softplus bijectors and my own custom affine bijector).\nI'm willing to work on a fix for this problem, but I'm not sure what the best way to do it is (adding caching to the TransformedDistribution code might work, but that seems like code duplication).\nSystem information\n\nOS Platform and Distribution: Ubuntu 16.04\nTensorFlow installed from: Source\nTensorFlow version: v1.1.0-rc2-773-g7fa0cf3 (commit 7fa0cf3)\nBazel version: 0.4.5", "body": "Currently, the caching that [Bijector](https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/bijector_impl.py#L114) objects do to avoid unnecessary calculations does not work when the bijector is used in a [TransformedDistribution](https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/transformed_distribution.py#L124) object. I believe the culprit is the reshaping that the distribution object does [here](https://github.com/tensorflow/tensorflow/blob/7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64/tensorflow/python/ops/distributions/distribution.py#L640); when we call Bijector.inverse on the output, the Bijector object cannot tell that this is merely a reshaped version of what it calculated previously.\r\n\r\nMy use case is to sample from a TransformedDistribution and then later calculate the log probability of that sample.\r\n\r\nThis issue is particularly a problem when using a bijector whose inverse is numerically delicate (in my case, I'm chaining together softplus bijectors and my own custom affine bijector).\r\n\r\nI'm willing to work on a fix for this problem, but I'm not sure what the best way to do it is (adding caching to the TransformedDistribution code might work, but that seems like code duplication).\r\n\r\n### System information\r\n- **OS Platform and Distribution**: Ubuntu 16.04\r\n- **TensorFlow installed from**: Source\r\n- **TensorFlow version**: v1.1.0-rc2-773-g7fa0cf3 (commit 7fa0cf39f854d5fdaaa19ad6425dfed02f5fea64)\r\n- **Bazel version**: 0.4.5"}