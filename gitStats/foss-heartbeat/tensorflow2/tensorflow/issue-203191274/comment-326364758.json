{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/326364758", "html_url": "https://github.com/tensorflow/tensorflow/issues/7068#issuecomment-326364758", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7068", "id": 326364758, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNjM2NDc1OA==", "user": {"login": "shoeffner", "id": 1836815, "node_id": "MDQ6VXNlcjE4MzY4MTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/1836815?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shoeffner", "html_url": "https://github.com/shoeffner", "followers_url": "https://api.github.com/users/shoeffner/followers", "following_url": "https://api.github.com/users/shoeffner/following{/other_user}", "gists_url": "https://api.github.com/users/shoeffner/gists{/gist_id}", "starred_url": "https://api.github.com/users/shoeffner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shoeffner/subscriptions", "organizations_url": "https://api.github.com/users/shoeffner/orgs", "repos_url": "https://api.github.com/users/shoeffner/repos", "events_url": "https://api.github.com/users/shoeffner/events{/privacy}", "received_events_url": "https://api.github.com/users/shoeffner/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-31T17:22:04Z", "updated_at": "2017-08-31T17:23:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7604948\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/stevendavis\">@stevendavis</a> We managed to run some models on the SGE, but it's tricky because you always run into walltimes and have to somehow handle them properly. Additionally the creation of the ClusterSpecs is not really trivial, as you submit to queues and not to hosts. But by making some tweaks you can actually do it.</p>\n<p>I didn't have too much time to make some proper write up, but you can skim <a href=\"https://github.com/shoeffner/ann3depth\">https://github.com/shoeffner/ann3depth</a> for how we made it. Since it was a project for a university seminar, there's also a small <a href=\"https://github.com/shoeffner/ann3depth/tree/master/docs/documentation.md\">project report</a> which explains some of the code we used in <a href=\"https://github.com/shoeffner/ann3depth/tree/master/tools/grid\">tools/grid</a>. Most of it is not really well designed, some scripts take input via stdin, others via arguments etc., but it should give you some ideas.</p>\n<p>In short these are the important ideas:</p>\n<ul>\n<li>Open ports in your grid system. We had 5001 - 5007 available, you potentially just need one. Ask your admins about it.</li>\n<li>One \"keepalive\" job which first checks what machines are available and distributes the workload to submit to hosts (This can be heavily improved but was enough for our proof of concept). The keepalive job is submitted by you, it will submit all jobs for worker and ps nodes.</li>\n<li>The keepalive job checks if the jobs are still running and if not, resubmits the whole lot (this turned out to be easier than submitting individual jobs). It also restarts the whole thing before it hits its own walltime.</li>\n<li>With the <code>-notify</code> flag you can let the SGE send USR1/USR2 signals to your jobs before they get killed. This gives you a window to store a checkpoint right before the walltime is reached. Unfortunately, as in our setup, the SGE can be configured to not send those signals. Then your best bet is to use ALRM signals which erupt before the walltimes are hit. We used a hook (<a href=\"https://github.com/shoeffner/ann3depth/blob/1789366bbc7cd97bd70839bc5cff2db1e13e2a5d/src/tfhelper.py#L160-L189\">src/tfhelper.py#L160-L189</a>) to handle this.</li>\n<li>Save checkpoints every now and then, your jobs can fail to various reasons (sometimes shells were not found, sometimes some higher priority jobs can kill your job, even people running processes locally on machines without using the SGE resource manage can occur quite often and are troublesome).</li>\n</ul>\n<p>What you have to keep in mind is that the SGE is there to distribute computation time on specialized systems among several users for a fair sharing of resources. TensorFlow is not really designed for this use case, it suits designs using containerization where you can assume (almost) infinite computation power and resources. This makes things very tricky.</p>", "body_text": "@stevendavis We managed to run some models on the SGE, but it's tricky because you always run into walltimes and have to somehow handle them properly. Additionally the creation of the ClusterSpecs is not really trivial, as you submit to queues and not to hosts. But by making some tweaks you can actually do it.\nI didn't have too much time to make some proper write up, but you can skim https://github.com/shoeffner/ann3depth for how we made it. Since it was a project for a university seminar, there's also a small project report which explains some of the code we used in tools/grid. Most of it is not really well designed, some scripts take input via stdin, others via arguments etc., but it should give you some ideas.\nIn short these are the important ideas:\n\nOpen ports in your grid system. We had 5001 - 5007 available, you potentially just need one. Ask your admins about it.\nOne \"keepalive\" job which first checks what machines are available and distributes the workload to submit to hosts (This can be heavily improved but was enough for our proof of concept). The keepalive job is submitted by you, it will submit all jobs for worker and ps nodes.\nThe keepalive job checks if the jobs are still running and if not, resubmits the whole lot (this turned out to be easier than submitting individual jobs). It also restarts the whole thing before it hits its own walltime.\nWith the -notify flag you can let the SGE send USR1/USR2 signals to your jobs before they get killed. This gives you a window to store a checkpoint right before the walltime is reached. Unfortunately, as in our setup, the SGE can be configured to not send those signals. Then your best bet is to use ALRM signals which erupt before the walltimes are hit. We used a hook (src/tfhelper.py#L160-L189) to handle this.\nSave checkpoints every now and then, your jobs can fail to various reasons (sometimes shells were not found, sometimes some higher priority jobs can kill your job, even people running processes locally on machines without using the SGE resource manage can occur quite often and are troublesome).\n\nWhat you have to keep in mind is that the SGE is there to distribute computation time on specialized systems among several users for a fair sharing of resources. TensorFlow is not really designed for this use case, it suits designs using containerization where you can assume (almost) infinite computation power and resources. This makes things very tricky.", "body": "@stevendavis We managed to run some models on the SGE, but it's tricky because you always run into walltimes and have to somehow handle them properly. Additionally the creation of the ClusterSpecs is not really trivial, as you submit to queues and not to hosts. But by making some tweaks you can actually do it.\r\n\r\nI didn't have too much time to make some proper write up, but you can skim [https://github.com/shoeffner/ann3depth](https://github.com/shoeffner/ann3depth) for how we made it. Since it was a project for a university seminar, there's also a small [project report](https://github.com/shoeffner/ann3depth/tree/master/docs/documentation.md) which explains some of the code we used in [tools/grid](https://github.com/shoeffner/ann3depth/tree/master/tools/grid). Most of it is not really well designed, some scripts take input via stdin, others via arguments etc., but it should give you some ideas.\r\n\r\nIn short these are the important ideas:\r\n\r\n- Open ports in your grid system. We had 5001 - 5007 available, you potentially just need one. Ask your admins about it.\r\n- One \"keepalive\" job which first checks what machines are available and distributes the workload to submit to hosts (This can be heavily improved but was enough for our proof of concept). The keepalive job is submitted by you, it will submit all jobs for worker and ps nodes.\r\n- The keepalive job checks if the jobs are still running and if not, resubmits the whole lot (this turned out to be easier than submitting individual jobs). It also restarts the whole thing before it hits its own walltime.\r\n- With the `-notify` flag you can let the SGE send USR1/USR2 signals to your jobs before they get killed. This gives you a window to store a checkpoint right before the walltime is reached. Unfortunately, as in our setup, the SGE can be configured to not send those signals. Then your best bet is to use ALRM signals which erupt before the walltimes are hit. We used a hook ([src/tfhelper.py#L160-L189](https://github.com/shoeffner/ann3depth/blob/1789366bbc7cd97bd70839bc5cff2db1e13e2a5d/src/tfhelper.py#L160-L189)) to handle this.\r\n- Save checkpoints every now and then, your jobs can fail to various reasons (sometimes shells were not found, sometimes some higher priority jobs can kill your job, even people running processes locally on machines without using the SGE resource manage can occur quite often and are troublesome).\r\n\r\nWhat you have to keep in mind is that the SGE is there to distribute computation time on specialized systems among several users for a fair sharing of resources. TensorFlow is not really designed for this use case, it suits designs using containerization where you can assume (almost) infinite computation power and resources. This makes things very tricky."}