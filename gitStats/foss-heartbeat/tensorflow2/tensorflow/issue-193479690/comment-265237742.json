{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/265237742", "html_url": "https://github.com/tensorflow/tensorflow/issues/6085#issuecomment-265237742", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6085", "id": 265237742, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NTIzNzc0Mg==", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-06T18:53:14Z", "updated_at": "2016-12-06T18:53:14Z", "author_association": "MEMBER", "body_html": "<p>From your example it looks like numpy produces vectors that are orthogonal to 1 part in 10^8 (off-diagonal elements are   approx. -9e-09) and TensorFlow produces vectors that are orthogonal to 1 part in 10^7 (off-diagonal elements are approx. -1.2e-07). While differering by a factor of 10, both are <em>numerically</em> very close to the identity, and certainly no cause for alarm. This is as good as one can expect in 32 bit floating point arithmetic. You can convert your tensors double (float64) if you want higher accuracy.</p>\n<p>BTW, a QR factorization op was recently added to TensorFlow, see this commit: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/715f951eb9ca20fdcef20bb544b74dbe576734da/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/715f951eb9ca20fdcef20bb544b74dbe576734da\"><tt>715f951</tt></a></p>", "body_text": "From your example it looks like numpy produces vectors that are orthogonal to 1 part in 10^8 (off-diagonal elements are   approx. -9e-09) and TensorFlow produces vectors that are orthogonal to 1 part in 10^7 (off-diagonal elements are approx. -1.2e-07). While differering by a factor of 10, both are numerically very close to the identity, and certainly no cause for alarm. This is as good as one can expect in 32 bit floating point arithmetic. You can convert your tensors double (float64) if you want higher accuracy.\nBTW, a QR factorization op was recently added to TensorFlow, see this commit: 715f951", "body": "From your example it looks like numpy produces vectors that are orthogonal to 1 part in 10^8 (off-diagonal elements are   approx. -9e-09) and TensorFlow produces vectors that are orthogonal to 1 part in 10^7 (off-diagonal elements are approx. -1.2e-07). While differering by a factor of 10, both are _numerically_ very close to the identity, and certainly no cause for alarm. This is as good as one can expect in 32 bit floating point arithmetic. You can convert your tensors double (float64) if you want higher accuracy.\r\n\r\nBTW, a QR factorization op was recently added to TensorFlow, see this commit: https://github.com/tensorflow/tensorflow/commit/715f951eb9ca20fdcef20bb544b74dbe576734da"}