{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3114", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3114/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3114/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3114/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3114", "id": 163042317, "node_id": "MDU6SXNzdWUxNjMwNDIzMTc=", "number": 3114, "title": "Backpropagation through the while-loop doesn't work if external tensors are used inside", "user": {"login": "rizar", "id": 654434, "node_id": "MDQ6VXNlcjY1NDQzNA==", "avatar_url": "https://avatars0.githubusercontent.com/u/654434?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rizar", "html_url": "https://github.com/rizar", "followers_url": "https://api.github.com/users/rizar/followers", "following_url": "https://api.github.com/users/rizar/following{/other_user}", "gists_url": "https://api.github.com/users/rizar/gists{/gist_id}", "starred_url": "https://api.github.com/users/rizar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rizar/subscriptions", "organizations_url": "https://api.github.com/users/rizar/orgs", "repos_url": "https://api.github.com/users/rizar/repos", "events_url": "https://api.github.com/users/rizar/events{/privacy}", "received_events_url": "https://api.github.com/users/rizar/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2016-06-29T22:32:29Z", "updated_at": "2016-10-28T06:20:07Z", "closed_at": "2016-07-21T17:13:39Z", "author_association": "NONE", "body_html": "<p>In the simple example below I copy <code>inputs</code> to <code>outputs</code> in a while loop. When I try to take gradient of the <code>outputs</code> w.r. to <code>inputs</code>, I get an error. It's worth noting that 1) the forward pass works 2) the gradient computation works if I made <code>inputs</code> a <code>TensorArray</code>, like it is done in the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py#L289\">scan implementation</a></p>\n<p>The code:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Graph().as_default(), tf.Session() <span class=\"pl-k\">as</span> sess:\n  num_steps <span class=\"pl-k\">=</span> <span class=\"pl-c1\">9</span>\n\n  inputs <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(num_steps))\n  initial_outputs <span class=\"pl-k\">=</span> tf.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>num_steps)\n  initial_i <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>int32<span class=\"pl-pds\">'</span></span>)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">should_continue</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span>):\n    <span class=\"pl-k\">return</span> i <span class=\"pl-k\">&lt;</span> num_steps\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">iteration</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">outputs_</span>):\n    outputs_ <span class=\"pl-k\">=</span> outputs_.write(i, tf.gather(inputs, i))\n    <span class=\"pl-k\">return</span> i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, outputs_\n\n  i, outputs <span class=\"pl-k\">=</span> tf.while_loop(\n    should_continue, iteration,\n    [initial_i, initial_outputs])\n\n  outputs <span class=\"pl-k\">=</span> outputs.pack()\n  grad_wr_inputs <span class=\"pl-k\">=</span> tf.convert_to_tensor(tf.gradients([tf.reduce_sum(outputs)], [inputs])[<span class=\"pl-c1\">0</span>])\n  <span class=\"pl-c1\">print</span> sess.run([outputs, grad_wr_inputs],\n                 <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{inputs: [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">7</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">0</span>]})</pre></div>\n<p>The stack trace:</p>\n<pre><code>---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n&lt;ipython-input-354-e1d117e742f9&gt; in &lt;module&gt;()\n     20   grad_wr_inputs = tf.convert_to_tensor(tf.gradients([tf.reduce_sum(outputs)], [inputs])[0])\n     21   print sess.run([outputs, grad_wr_inputs],\n---&gt; 22                  feed_dict={inputs: [4, 6, 0, 7, 0, 0, 1, 2, 0]})\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    366     try:\n    367       result = self._run(None, fetches, feed_dict, options_ptr,\n--&gt; 368                          run_metadata_ptr)\n    369       if run_metadata:\n    370         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    639     movers = self._update_with_movers(feed_dict_string, feed_map)\n    640     results = self._do_run(handle, target_list, unique_fetches,\n--&gt; 641                            feed_dict_string, options, run_metadata)\n    642 \n    643     # User may have fetched the same tensor multiple times, but we\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n    707     if handle is None:\n    708       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--&gt; 709                            target_list, options, run_metadata)\n    710     else:\n    711       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n    727         except KeyError:\n    728           pass\n--&gt; 729       raise type(e)(node_def, op, message)\n    730 \n    731   def _extend_graph(self):\n\nInvalidArgumentError: All inputs to node Slice must be from the same frame.\n</code></pre>", "body_text": "In the simple example below I copy inputs to outputs in a while loop. When I try to take gradient of the outputs w.r. to inputs, I get an error. It's worth noting that 1) the forward pass works 2) the gradient computation works if I made inputs a TensorArray, like it is done in the scan implementation\nThe code:\nwith tf.Graph().as_default(), tf.Session() as sess:\n  num_steps = 9\n\n  inputs = tf.placeholder(dtype='float32', shape=(num_steps))\n  initial_outputs = tf.TensorArray(dtype=tf.float32, size=num_steps)\n  initial_i = tf.constant(0, dtype='int32')\n\n  def should_continue(i, *args):\n    return i < num_steps\n\n  def iteration(i, outputs_):\n    outputs_ = outputs_.write(i, tf.gather(inputs, i))\n    return i + 1, outputs_\n\n  i, outputs = tf.while_loop(\n    should_continue, iteration,\n    [initial_i, initial_outputs])\n\n  outputs = outputs.pack()\n  grad_wr_inputs = tf.convert_to_tensor(tf.gradients([tf.reduce_sum(outputs)], [inputs])[0])\n  print sess.run([outputs, grad_wr_inputs],\n                 feed_dict={inputs: [4, 6, 0, 7, 0, 0, 1, 2, 0]})\nThe stack trace:\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n<ipython-input-354-e1d117e742f9> in <module>()\n     20   grad_wr_inputs = tf.convert_to_tensor(tf.gradients([tf.reduce_sum(outputs)], [inputs])[0])\n     21   print sess.run([outputs, grad_wr_inputs],\n---> 22                  feed_dict={inputs: [4, 6, 0, 7, 0, 0, 1, 2, 0]})\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    366     try:\n    367       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 368                          run_metadata_ptr)\n    369       if run_metadata:\n    370         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    639     movers = self._update_with_movers(feed_dict_string, feed_map)\n    640     results = self._do_run(handle, target_list, unique_fetches,\n--> 641                            feed_dict_string, options, run_metadata)\n    642 \n    643     # User may have fetched the same tensor multiple times, but we\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n    707     if handle is None:\n    708       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--> 709                            target_list, options, run_metadata)\n    710     else:\n    711       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n    727         except KeyError:\n    728           pass\n--> 729       raise type(e)(node_def, op, message)\n    730 \n    731   def _extend_graph(self):\n\nInvalidArgumentError: All inputs to node Slice must be from the same frame.", "body": "In the simple example below I copy `inputs` to `outputs` in a while loop. When I try to take gradient of the `outputs` w.r. to `inputs`, I get an error. It's worth noting that 1) the forward pass works 2) the gradient computation works if I made `inputs` a `TensorArray`, like it is done in the [scan implementation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/functional_ops.py#L289)\n\nThe code:\n\n``` python\nwith tf.Graph().as_default(), tf.Session() as sess:\n  num_steps = 9\n\n  inputs = tf.placeholder(dtype='float32', shape=(num_steps))\n  initial_outputs = tf.TensorArray(dtype=tf.float32, size=num_steps)\n  initial_i = tf.constant(0, dtype='int32')\n\n  def should_continue(i, *args):\n    return i < num_steps\n\n  def iteration(i, outputs_):\n    outputs_ = outputs_.write(i, tf.gather(inputs, i))\n    return i + 1, outputs_\n\n  i, outputs = tf.while_loop(\n    should_continue, iteration,\n    [initial_i, initial_outputs])\n\n  outputs = outputs.pack()\n  grad_wr_inputs = tf.convert_to_tensor(tf.gradients([tf.reduce_sum(outputs)], [inputs])[0])\n  print sess.run([outputs, grad_wr_inputs],\n                 feed_dict={inputs: [4, 6, 0, 7, 0, 0, 1, 2, 0]})\n```\n\nThe stack trace:\n\n```\n---------------------------------------------------------------------------\nInvalidArgumentError                      Traceback (most recent call last)\n<ipython-input-354-e1d117e742f9> in <module>()\n     20   grad_wr_inputs = tf.convert_to_tensor(tf.gradients([tf.reduce_sum(outputs)], [inputs])[0])\n     21   print sess.run([outputs, grad_wr_inputs],\n---> 22                  feed_dict={inputs: [4, 6, 0, 7, 0, 0, 1, 2, 0]})\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in run(self, fetches, feed_dict, options, run_metadata)\n    366     try:\n    367       result = self._run(None, fetches, feed_dict, options_ptr,\n--> 368                          run_metadata_ptr)\n    369       if run_metadata:\n    370         proto_data = tf_session.TF_GetBuffer(run_metadata_ptr)\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _run(self, handle, fetches, feed_dict, options, run_metadata)\n    639     movers = self._update_with_movers(feed_dict_string, feed_map)\n    640     results = self._do_run(handle, target_list, unique_fetches,\n--> 641                            feed_dict_string, options, run_metadata)\n    642 \n    643     # User may have fetched the same tensor multiple times, but we\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _do_run(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\n    707     if handle is None:\n    708       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n--> 709                            target_list, options, run_metadata)\n    710     else:\n    711       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n\n/google/data/ro/teams/colab/tensorflow/google3/third_party/tensorflow/python/client/session.py in _do_call(self, fn, *args)\n    727         except KeyError:\n    728           pass\n--> 729       raise type(e)(node_def, op, message)\n    730 \n    731   def _extend_graph(self):\n\nInvalidArgumentError: All inputs to node Slice must be from the same frame.\n```\n"}