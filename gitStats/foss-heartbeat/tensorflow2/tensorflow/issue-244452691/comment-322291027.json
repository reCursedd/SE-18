{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322291027", "html_url": "https://github.com/tensorflow/tensorflow/issues/11650#issuecomment-322291027", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650", "id": 322291027, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjI5MTAyNw==", "user": {"login": "davidhstern", "id": 31016454, "node_id": "MDQ6VXNlcjMxMDE2NDU0", "avatar_url": "https://avatars0.githubusercontent.com/u/31016454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidhstern", "html_url": "https://github.com/davidhstern", "followers_url": "https://api.github.com/users/davidhstern/followers", "following_url": "https://api.github.com/users/davidhstern/following{/other_user}", "gists_url": "https://api.github.com/users/davidhstern/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidhstern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidhstern/subscriptions", "organizations_url": "https://api.github.com/users/davidhstern/orgs", "repos_url": "https://api.github.com/users/davidhstern/repos", "events_url": "https://api.github.com/users/davidhstern/events{/privacy}", "received_events_url": "https://api.github.com/users/davidhstern/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-14T19:51:36Z", "updated_at": "2017-08-14T19:51:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26974149\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/santon834\">@santon834</a> You might want to check out <a href=\"https://arxiv.org/pdf/1603.05118.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1603.05118.pdf</a></p>\n<p>It looks like not dropping the LSTM memory state might actually be the key to getting good performance from dropout in LSTMs.  Using one mask per sequence might be redundant as long as you do this.</p>", "body_text": "@santon834 You might want to check out https://arxiv.org/pdf/1603.05118.pdf\nIt looks like not dropping the LSTM memory state might actually be the key to getting good performance from dropout in LSTMs.  Using one mask per sequence might be redundant as long as you do this.", "body": "@santon834 You might want to check out https://arxiv.org/pdf/1603.05118.pdf\r\n\r\nIt looks like not dropping the LSTM memory state might actually be the key to getting good performance from dropout in LSTMs.  Using one mask per sequence might be redundant as long as you do this."}