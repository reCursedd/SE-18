{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322286998", "html_url": "https://github.com/tensorflow/tensorflow/issues/11650#issuecomment-322286998", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650", "id": 322286998, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjI4Njk5OA==", "user": {"login": "davidhstern", "id": 31016454, "node_id": "MDQ6VXNlcjMxMDE2NDU0", "avatar_url": "https://avatars0.githubusercontent.com/u/31016454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidhstern", "html_url": "https://github.com/davidhstern", "followers_url": "https://api.github.com/users/davidhstern/followers", "following_url": "https://api.github.com/users/davidhstern/following{/other_user}", "gists_url": "https://api.github.com/users/davidhstern/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidhstern/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidhstern/subscriptions", "organizations_url": "https://api.github.com/users/davidhstern/orgs", "repos_url": "https://api.github.com/users/davidhstern/repos", "events_url": "https://api.github.com/users/davidhstern/events{/privacy}", "received_events_url": "https://api.github.com/users/davidhstern/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-14T19:34:43Z", "updated_at": "2017-08-14T19:38:09Z", "author_association": "NONE", "body_html": "<p>Has anyone had a chance to look at this?  I agree with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26974149\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/santon834\">@santon834</a> that it looks like variational dropout  has been wrongly implemented for the LSTM and GRU if state_keep_prob &lt; 1.  I think that the LSTM could be fixed by excluding the c state from dropout.   However, fixing the GRU would require more changes as dropout should only be applied after the weights (U) are applied but not after the multiplication by z.   I am not sure the best way to achieve this but perhaps a new pair of cells with dropout built in might be the way forward?</p>\n<p>To be clear: the bug applies not just to the division by p, but also to the application of the dropout mask: it should only be applied to the results of multiplication by weights, not to the propagation of cell state (z*h in the case of the GRU and c in the case of LSTM).</p>\n<p>I also agree that it the implemetation seems to give poor results even when variational_recurrent=False and it might work better if it did not drop the internal LSTM state.   However, I guess the best implementation of non-variational dropout for LSTM is more open to debate so I would not call that part wrong necessary.</p>", "body_text": "Has anyone had a chance to look at this?  I agree with @santon834 that it looks like variational dropout  has been wrongly implemented for the LSTM and GRU if state_keep_prob < 1.  I think that the LSTM could be fixed by excluding the c state from dropout.   However, fixing the GRU would require more changes as dropout should only be applied after the weights (U) are applied but not after the multiplication by z.   I am not sure the best way to achieve this but perhaps a new pair of cells with dropout built in might be the way forward?\nTo be clear: the bug applies not just to the division by p, but also to the application of the dropout mask: it should only be applied to the results of multiplication by weights, not to the propagation of cell state (z*h in the case of the GRU and c in the case of LSTM).\nI also agree that it the implemetation seems to give poor results even when variational_recurrent=False and it might work better if it did not drop the internal LSTM state.   However, I guess the best implementation of non-variational dropout for LSTM is more open to debate so I would not call that part wrong necessary.", "body": "Has anyone had a chance to look at this?  I agree with @santon834 that it looks like variational dropout  has been wrongly implemented for the LSTM and GRU if state_keep_prob < 1.  I think that the LSTM could be fixed by excluding the c state from dropout.   However, fixing the GRU would require more changes as dropout should only be applied after the weights (U) are applied but not after the multiplication by z.   I am not sure the best way to achieve this but perhaps a new pair of cells with dropout built in might be the way forward?\r\n\r\nTo be clear: the bug applies not just to the division by p, but also to the application of the dropout mask: it should only be applied to the results of multiplication by weights, not to the propagation of cell state (z*h in the case of the GRU and c in the case of LSTM).\r\n\r\nI also agree that it the implemetation seems to give poor results even when variational_recurrent=False and it might work better if it did not drop the internal LSTM state.   However, I guess the best implementation of non-variational dropout for LSTM is more open to debate so I would not call that part wrong necessary."}