{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11650", "id": 244452691, "node_id": "MDU6SXNzdWUyNDQ0NTI2OTE=", "number": 11650, "title": "BUG: DropoutWrapper incorrectly updates memory state", "user": {"login": "santon834", "id": 26974149, "node_id": "MDQ6VXNlcjI2OTc0MTQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/26974149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/santon834", "html_url": "https://github.com/santon834", "followers_url": "https://api.github.com/users/santon834/followers", "following_url": "https://api.github.com/users/santon834/following{/other_user}", "gists_url": "https://api.github.com/users/santon834/gists{/gist_id}", "starred_url": "https://api.github.com/users/santon834/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/santon834/subscriptions", "organizations_url": "https://api.github.com/users/santon834/orgs", "repos_url": "https://api.github.com/users/santon834/repos", "events_url": "https://api.github.com/users/santon834/events{/privacy}", "received_events_url": "https://api.github.com/users/santon834/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-07-20T18:17:53Z", "updated_at": "2017-08-23T23:14:20Z", "closed_at": "2017-08-23T23:14:20Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>GRU units (wikipedia):<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/26974149/28431669-fb89275a-6d39-11e7-929a-43be917d2602.png\"><img src=\"https://user-images.githubusercontent.com/26974149/28431669-fb89275a-6d39-11e7-929a-43be917d2602.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>tf.contrib.rnn.DropoutWrapper documentation:<br>\nstate_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the output states of the cell.</p>\n<p>In GRU units, the output state is the memory state. When variational_recurrent=True, the same temporal dropout mask is applied to the output state, in each time step, with the remaining outputs divided by the dropout probability. This leads to exponential growth of the memory state and exploding outputs (given long enough time series):<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/26974149/28432360-7644d7b2-6d3c-11e7-8999-80ff55344f61.png\"><img src=\"https://user-images.githubusercontent.com/26974149/28432360-7644d7b2-6d3c-11e7-8999-80ff55344f61.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>The correct way is probably to divide U_z, U_r and U_h and not the output state.</p>\n<h3>Source code / logs</h3>\n<pre><code>################################### \ndef length(sequence):\n\tused = tf.sign(tf.reduce_max(tf.abs(sequence), axis=2))\n\tlength = tf.reduce_sum(used, axis=1)\n\treturn length\n################################### \ndef GRU(x, units, act, in_dp, out_dp, tmp_dp):\n\tgru_cell = tf.contrib.rnn.GRUCell(units, activation=act, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True))\n\twrapped_gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell, input_keep_prob=in_dp, output_keep_prob=out_dp, state_keep_prob=tmp_dp, variational_recurrent=True, dtype=x.dtype, input_size=x.get_shape()[2])\n\toutputs, state = tf.nn.dynamic_rnn(wrapped_gru_cell, x, dtype=x.dtype, sequence_length=length(x))\n\treturn [outputs, state]\n################################### \nRunning with parameters (all exploded):\nact=tf.nn.tanh, in_dp=1., out_dp=1., tmp_dp=0.5/0.9, n_hidden = 32/64/128\n</code></pre>\n<h3>System information</h3>\n<p>== cat /etc/issue ===============================================<br>\nLinux mvdslab 4.4.0-83-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116224545\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/106\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/106/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/106\">#106</a>-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux<br>\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"<br>\nVERSION_ID=\"16.04\"<br>\nVERSION_CODENAME=xenial</p>\n<p>== are we in docker =============================================<br>\nNo</p>\n<p>== compiler =====================================================<br>\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609<br>\nCopyright (C) 2015 Free Software Foundation, Inc.<br>\nThis is free software; see the source for copying conditions.  There is NO<br>\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>\n<p>== uname -a =====================================================<br>\nLinux mvdslab 4.4.0-83-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116224545\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/106\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/106/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/106\">#106</a>-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p>== check pips ===================================================<br>\nnumpy (1.12.1)<br>\nnumpydoc (0.6.0)<br>\nprotobuf (3.3.0)<br>\ntensorflow (1.2.0)<br>\ntensorflow-gpu (1.2.0)</p>\n<p>== check for virtualenv =========================================<br>\nFalse</p>\n<p>== tensorflow import ============================================<br>\ntf.VERSION = 1.2.0<br>\ntf.GIT_VERSION = v1.2.0-1131-gbc691dd<br>\ntf.COMPILER_VERSION = v1.2.0-1131-gbc691dd<br>\nSanity check: array([1], dtype=int32)</p>\n<p>== env ==========================================================<br>\nLD_LIBRARY_PATH /home/anton/torch/install/lib:/usr/local/cuda-8.0/lib64<br>\nDYLD_LIBRARY_PATH /home/anton/torch/install/lib:/home/anton/torch/install/lib:</p>\n<p>== nvidia-smi ===================================================<br>\nThu Jul 20 10:36:06 2017<br>\n+-----------------------------------------------------------------------------+<br>\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |<br>\n|-------------------------------+----------------------+----------------------+<br>\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |<br>\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |<br>\n|===============================+======================+======================|<br>\n|   0  Quadro K1200        Off  | 0000:01:00.0      On |                  N/A |<br>\n| 39%   38C    P0     1W /  35W |    459MiB /  4034MiB |      3%      Default |<br>\n+-------------------------------+----------------------+----------------------+</p>\n<p>+-----------------------------------------------------------------------------+<br>\n| Processes:                                                       GPU Memory |<br>\n|  GPU       PID  Type  Process name                               Usage      |<br>\n|=============================================================================|<br>\n|    0      1183    G   /usr/lib/xorg/Xorg                             110MiB |<br>\n|    0      6218    G   ...el-token=544D2517E333A364F4E0C0630D2C9DD1   347MiB |<br>\n+-----------------------------------------------------------------------------+</p>\n<p>== cuda libs  ===================================================<br>\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61<br>\n/usr/local/cuda-8.0/lib64/libcudart_static.a<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7<br>\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7</p>", "body_text": "Describe the problem\nGRU units (wikipedia):\n\ntf.contrib.rnn.DropoutWrapper documentation:\nstate_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the output states of the cell.\nIn GRU units, the output state is the memory state. When variational_recurrent=True, the same temporal dropout mask is applied to the output state, in each time step, with the remaining outputs divided by the dropout probability. This leads to exponential growth of the memory state and exploding outputs (given long enough time series):\n\nThe correct way is probably to divide U_z, U_r and U_h and not the output state.\nSource code / logs\n################################### \ndef length(sequence):\n\tused = tf.sign(tf.reduce_max(tf.abs(sequence), axis=2))\n\tlength = tf.reduce_sum(used, axis=1)\n\treturn length\n################################### \ndef GRU(x, units, act, in_dp, out_dp, tmp_dp):\n\tgru_cell = tf.contrib.rnn.GRUCell(units, activation=act, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True))\n\twrapped_gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell, input_keep_prob=in_dp, output_keep_prob=out_dp, state_keep_prob=tmp_dp, variational_recurrent=True, dtype=x.dtype, input_size=x.get_shape()[2])\n\toutputs, state = tf.nn.dynamic_rnn(wrapped_gru_cell, x, dtype=x.dtype, sequence_length=length(x))\n\treturn [outputs, state]\n################################### \nRunning with parameters (all exploded):\nact=tf.nn.tanh, in_dp=1., out_dp=1., tmp_dp=0.5/0.9, n_hidden = 32/64/128\n\nSystem information\n== cat /etc/issue ===============================================\nLinux mvdslab 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n== are we in docker =============================================\nNo\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n== uname -a =====================================================\nLinux mvdslab 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n== check pips ===================================================\nnumpy (1.12.1)\nnumpydoc (0.6.0)\nprotobuf (3.3.0)\ntensorflow (1.2.0)\ntensorflow-gpu (1.2.0)\n== check for virtualenv =========================================\nFalse\n== tensorflow import ============================================\ntf.VERSION = 1.2.0\ntf.GIT_VERSION = v1.2.0-1131-gbc691dd\ntf.COMPILER_VERSION = v1.2.0-1131-gbc691dd\nSanity check: array([1], dtype=int32)\n== env ==========================================================\nLD_LIBRARY_PATH /home/anton/torch/install/lib:/usr/local/cuda-8.0/lib64\nDYLD_LIBRARY_PATH /home/anton/torch/install/lib:/home/anton/torch/install/lib:\n== nvidia-smi ===================================================\nThu Jul 20 10:36:06 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Quadro K1200        Off  | 0000:01:00.0      On |                  N/A |\n| 39%   38C    P0     1W /  35W |    459MiB /  4034MiB |      3%      Default |\n+-------------------------------+----------------------+----------------------+\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|=============================================================================|\n|    0      1183    G   /usr/lib/xorg/Xorg                             110MiB |\n|    0      6218    G   ...el-token=544D2517E333A364F4E0C0630D2C9DD1   347MiB |\n+-----------------------------------------------------------------------------+\n== cuda libs  ===================================================\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\n/usr/local/cuda-8.0/lib64/libcudart_static.a\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7", "body": "### Describe the problem\r\nGRU units (wikipedia):\r\n![image](https://user-images.githubusercontent.com/26974149/28431669-fb89275a-6d39-11e7-929a-43be917d2602.png)\r\n\r\ntf.contrib.rnn.DropoutWrapper documentation:\r\nstate_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added. State dropout is performed on the output states of the cell.\r\n\r\nIn GRU units, the output state is the memory state. When variational_recurrent=True, the same temporal dropout mask is applied to the output state, in each time step, with the remaining outputs divided by the dropout probability. This leads to exponential growth of the memory state and exploding outputs (given long enough time series):\r\n![image](https://user-images.githubusercontent.com/26974149/28432360-7644d7b2-6d3c-11e7-8999-80ff55344f61.png)\r\n\r\nThe correct way is probably to divide U_z, U_r and U_h and not the output state.\r\n\r\n### Source code / logs\r\n```\r\n################################### \r\ndef length(sequence):\r\n\tused = tf.sign(tf.reduce_max(tf.abs(sequence), axis=2))\r\n\tlength = tf.reduce_sum(used, axis=1)\r\n\treturn length\r\n################################### \r\ndef GRU(x, units, act, in_dp, out_dp, tmp_dp):\r\n\tgru_cell = tf.contrib.rnn.GRUCell(units, activation=act, kernel_initializer=tf.contrib.layers.xavier_initializer(uniform=True))\r\n\twrapped_gru_cell = tf.contrib.rnn.DropoutWrapper(gru_cell, input_keep_prob=in_dp, output_keep_prob=out_dp, state_keep_prob=tmp_dp, variational_recurrent=True, dtype=x.dtype, input_size=x.get_shape()[2])\r\n\toutputs, state = tf.nn.dynamic_rnn(wrapped_gru_cell, x, dtype=x.dtype, sequence_length=length(x))\r\n\treturn [outputs, state]\r\n################################### \r\nRunning with parameters (all exploded):\r\nact=tf.nn.tanh, in_dp=1., out_dp=1., tmp_dp=0.5/0.9, n_hidden = 32/64/128\r\n```\r\n\r\n### System information\r\n== cat /etc/issue ===============================================\r\nLinux mvdslab 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.2 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux mvdslab 4.4.0-83-generic #106-Ubuntu SMP Mon Jun 26 17:54:43 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.12.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.0)\r\ntensorflow-gpu (1.2.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.0\r\ntf.GIT_VERSION = v1.2.0-1131-gbc691dd\r\ntf.COMPILER_VERSION = v1.2.0-1131-gbc691dd\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /home/anton/torch/install/lib:/usr/local/cuda-8.0/lib64\r\nDYLD_LIBRARY_PATH /home/anton/torch/install/lib:/home/anton/torch/install/lib:\r\n\r\n== nvidia-smi ===================================================\r\nThu Jul 20 10:36:06 2017       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.66                 Driver Version: 375.66                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Quadro K1200        Off  | 0000:01:00.0      On |                  N/A |\r\n| 39%   38C    P0     1W /  35W |    459MiB /  4034MiB |      3%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|=============================================================================|\r\n|    0      1183    G   /usr/lib/xorg/Xorg                             110MiB |\r\n|    0      6218    G   ...el-token=544D2517E333A364F4E0C0630D2C9DD1   347MiB |\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-8.0/lib64/libcudart.so.8.0.61\r\n/usr/local/cuda-8.0/lib64/libcudart_static.a\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.7\r\n/usr/local/cuda-8.0/doc/man/man7/libcudart.so.7"}