{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317053130", "html_url": "https://github.com/tensorflow/tensorflow/issues/11650#issuecomment-317053130", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11650", "id": 317053130, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzA1MzEzMA==", "user": {"login": "santon834", "id": 26974149, "node_id": "MDQ6VXNlcjI2OTc0MTQ5", "avatar_url": "https://avatars2.githubusercontent.com/u/26974149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/santon834", "html_url": "https://github.com/santon834", "followers_url": "https://api.github.com/users/santon834/followers", "following_url": "https://api.github.com/users/santon834/following{/other_user}", "gists_url": "https://api.github.com/users/santon834/gists{/gist_id}", "starred_url": "https://api.github.com/users/santon834/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/santon834/subscriptions", "organizations_url": "https://api.github.com/users/santon834/orgs", "repos_url": "https://api.github.com/users/santon834/repos", "events_url": "https://api.github.com/users/santon834/events{/privacy}", "received_events_url": "https://api.github.com/users/santon834/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-21T16:50:30Z", "updated_at": "2017-07-21T16:56:09Z", "author_association": "NONE", "body_html": "<p>Update:<br>\nThe same problem occurs with variational_recurrent=false.<br>\nLine 754 in rnn_cell_impl.py updates the memory state with the dropout mask and divides the state by the probability, which is wrong. The division in dropout is performed to adjust for a constant expected activation arguments. However, it should not modify the internal memory state of the unit.<br>\nWith variational_recurrent=false, we have a random recurrent dropout mask at each step and we don't get the same \"accumulating effect\" as with variational_recurrent=true. As a result, the memory state remains in the same magnitude as the candidate update (right hand element in the memory state update equation) and is practically impossible to debug. I wonder if this is the cause for the general consensus that recurrent dropout doesn't work/cause RNN to not remember.</p>", "body_text": "Update:\nThe same problem occurs with variational_recurrent=false.\nLine 754 in rnn_cell_impl.py updates the memory state with the dropout mask and divides the state by the probability, which is wrong. The division in dropout is performed to adjust for a constant expected activation arguments. However, it should not modify the internal memory state of the unit.\nWith variational_recurrent=false, we have a random recurrent dropout mask at each step and we don't get the same \"accumulating effect\" as with variational_recurrent=true. As a result, the memory state remains in the same magnitude as the candidate update (right hand element in the memory state update equation) and is practically impossible to debug. I wonder if this is the cause for the general consensus that recurrent dropout doesn't work/cause RNN to not remember.", "body": "Update:\r\nThe same problem occurs with variational_recurrent=false.\r\nLine 754 in rnn_cell_impl.py updates the memory state with the dropout mask and divides the state by the probability, which is wrong. The division in dropout is performed to adjust for a constant expected activation arguments. However, it should not modify the internal memory state of the unit. \r\nWith variational_recurrent=false, we have a random recurrent dropout mask at each step and we don't get the same \"accumulating effect\" as with variational_recurrent=true. As a result, the memory state remains in the same magnitude as the candidate update (right hand element in the memory state update equation) and is practically impossible to debug. I wonder if this is the cause for the general consensus that recurrent dropout doesn't work/cause RNN to not remember.\r\n\r\n"}