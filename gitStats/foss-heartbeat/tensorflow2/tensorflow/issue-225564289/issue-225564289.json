{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9581", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9581/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9581/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9581/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9581", "id": 225564289, "node_id": "MDU6SXNzdWUyMjU1NjQyODk=", "number": 9581, "title": "Enhancement: Better Model Saving & Loading", "user": {"login": "drcrook1", "id": 6099287, "node_id": "MDQ6VXNlcjYwOTkyODc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6099287?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drcrook1", "html_url": "https://github.com/drcrook1", "followers_url": "https://api.github.com/users/drcrook1/followers", "following_url": "https://api.github.com/users/drcrook1/following{/other_user}", "gists_url": "https://api.github.com/users/drcrook1/gists{/gist_id}", "starred_url": "https://api.github.com/users/drcrook1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drcrook1/subscriptions", "organizations_url": "https://api.github.com/users/drcrook1/orgs", "repos_url": "https://api.github.com/users/drcrook1/repos", "events_url": "https://api.github.com/users/drcrook1/events{/privacy}", "received_events_url": "https://api.github.com/users/drcrook1/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-05-02T00:53:53Z", "updated_at": "2017-06-16T21:43:33Z", "closed_at": "2017-06-16T21:43:33Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"http://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">http://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug or a feature request.</li>\n<li>The form below must be filled out.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10 Pro</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.something</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: Feature Request; not bug</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<p>Much of it is documented here in depth; especially the pain part: <a href=\"http://stackoverflow.com/questions/43708616/tensorflow-inference\" rel=\"nofollow\">http://stackoverflow.com/questions/43708616/tensorflow-inference</a></p>\n<p>I would really really appreciate a way to save JUST the model and load JUST the model as a graph.  I might even be able to help in coding this feature.</p>\n<p>You can do it however you want; but the easiest might be to use a scoping prefix such as 'model/stuff'  I would very much like to do this:</p>\n<p>def inference(X):<br>\ncreate a model<br>\nreturn prediction_op</p>\n<p>do your training however; with TFRecords; with feed dicts; with your custom queue runners etc...There are a million options...</p>\n<p>tf.train.ModelSaver(SCOPE_PARENT, PATH)</p>\n<p>where SCOPE_PARENT is the top level scope to save; so if I structure my scoping as such:</p>\n<p>'inputs/inputstuffs'<br>\n'model/layer1' , 'model/layer2' etc etc<br>\n'adams etc'</p>\n<p>it would save just 'model/layer1' (but not any of the gradient stuff...)</p>\n<p>then at inference time I can do...</p>\n<p>model = tf.train.ModelLoader(PATH)<br>\nthat model takes the initial input of the first layer of that graph; whatever that is.  Most commonly for inference I see feed_dict.</p>\n<p>so I can set up whatever kind of processing I want; either in memory based with feed_dicts or perhaps batch processing via queue runners; but the point is that the input is seperate from the model.</p>\n<p>As it stands right now; I'm looking at TensorFlow for our use case as a non starter if the training input system is really this tightly bound to the inference system.  This type of thing really needs to be built...</p>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>", "body_text": "Please go to Stack Overflow for help and support:\nhttp://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10 Pro\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.something\nBazel version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: Feature Request; not bug\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nMuch of it is documented here in depth; especially the pain part: http://stackoverflow.com/questions/43708616/tensorflow-inference\nI would really really appreciate a way to save JUST the model and load JUST the model as a graph.  I might even be able to help in coding this feature.\nYou can do it however you want; but the easiest might be to use a scoping prefix such as 'model/stuff'  I would very much like to do this:\ndef inference(X):\ncreate a model\nreturn prediction_op\ndo your training however; with TFRecords; with feed dicts; with your custom queue runners etc...There are a million options...\ntf.train.ModelSaver(SCOPE_PARENT, PATH)\nwhere SCOPE_PARENT is the top level scope to save; so if I structure my scoping as such:\n'inputs/inputstuffs'\n'model/layer1' , 'model/layer2' etc etc\n'adams etc'\nit would save just 'model/layer1' (but not any of the gradient stuff...)\nthen at inference time I can do...\nmodel = tf.train.ModelLoader(PATH)\nthat model takes the initial input of the first layer of that graph; whatever that is.  Most commonly for inference I see feed_dict.\nso I can set up whatever kind of processing I want; either in memory based with feed_dicts or perhaps batch processing via queue runners; but the point is that the input is seperate from the model.\nAs it stands right now; I'm looking at TensorFlow for our use case as a non starter if the training input system is really this tightly bound to the inference system.  This type of thing really needs to be built...\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttp://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10 Pro\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.something\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: Feature Request; not bug\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nMuch of it is documented here in depth; especially the pain part: http://stackoverflow.com/questions/43708616/tensorflow-inference \r\n\r\nI would really really appreciate a way to save JUST the model and load JUST the model as a graph.  I might even be able to help in coding this feature.\r\n\r\nYou can do it however you want; but the easiest might be to use a scoping prefix such as 'model/stuff'  I would very much like to do this:\r\n\r\ndef inference(X):\r\n   create a model\r\n   return prediction_op\r\n\r\ndo your training however; with TFRecords; with feed dicts; with your custom queue runners etc...There are a million options...\r\n\r\ntf.train.ModelSaver(SCOPE_PARENT, PATH)\r\n\r\nwhere SCOPE_PARENT is the top level scope to save; so if I structure my scoping as such:\r\n\r\n'inputs/inputstuffs'\r\n'model/layer1' , 'model/layer2' etc etc \r\n'adams etc'\r\n\r\nit would save just 'model/layer1' (but not any of the gradient stuff...)\r\n\r\nthen at inference time I can do...\r\n\r\nmodel = tf.train.ModelLoader(PATH)\r\nthat model takes the initial input of the first layer of that graph; whatever that is.  Most commonly for inference I see feed_dict.\r\n\r\nso I can set up whatever kind of processing I want; either in memory based with feed_dicts or perhaps batch processing via queue runners; but the point is that the input is seperate from the model.\r\n\r\nAs it stands right now; I'm looking at TensorFlow for our use case as a non starter if the training input system is really this tightly bound to the inference system.  This type of thing really needs to be built...\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n"}