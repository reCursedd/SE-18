{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/372158980", "html_url": "https://github.com/tensorflow/tensorflow/issues/17390#issuecomment-372158980", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17390", "id": 372158980, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjE1ODk4MA==", "user": {"login": "Nicholas-Schaub", "id": 15925882, "node_id": "MDQ6VXNlcjE1OTI1ODgy", "avatar_url": "https://avatars3.githubusercontent.com/u/15925882?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Nicholas-Schaub", "html_url": "https://github.com/Nicholas-Schaub", "followers_url": "https://api.github.com/users/Nicholas-Schaub/followers", "following_url": "https://api.github.com/users/Nicholas-Schaub/following{/other_user}", "gists_url": "https://api.github.com/users/Nicholas-Schaub/gists{/gist_id}", "starred_url": "https://api.github.com/users/Nicholas-Schaub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Nicholas-Schaub/subscriptions", "organizations_url": "https://api.github.com/users/Nicholas-Schaub/orgs", "repos_url": "https://api.github.com/users/Nicholas-Schaub/repos", "events_url": "https://api.github.com/users/Nicholas-Schaub/events{/privacy}", "received_events_url": "https://api.github.com/users/Nicholas-Schaub/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-11T23:20:07Z", "updated_at": "2018-03-11T23:20:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10109534\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/karllessard\">@karllessard</a> If the issue is just figuring out how to expose the logic, isn't it just a matter of parsing the `registeredOpList' to get a list of functions and their inputs/outputs and writing a rudimentary translator to make sure that the appropriate inputs and outputs are being sent/received? The way it was discussed above, I thought that we would have to implement the gradient descent optimizer. My approach was just to parse the op list and write a generic graph building class that shows which operations are available and what inputs/attributes are needed. If the gradient descent functions are implemented in the core library, then it seems like it's just a matter of then building a simple class to cycle through layers for backpropagation. The hardest part for me seemed like it was going to be parsing the operations list, and then I could build a couple simple classes to build and train.</p>", "body_text": "@karllessard If the issue is just figuring out how to expose the logic, isn't it just a matter of parsing the `registeredOpList' to get a list of functions and their inputs/outputs and writing a rudimentary translator to make sure that the appropriate inputs and outputs are being sent/received? The way it was discussed above, I thought that we would have to implement the gradient descent optimizer. My approach was just to parse the op list and write a generic graph building class that shows which operations are available and what inputs/attributes are needed. If the gradient descent functions are implemented in the core library, then it seems like it's just a matter of then building a simple class to cycle through layers for backpropagation. The hardest part for me seemed like it was going to be parsing the operations list, and then I could build a couple simple classes to build and train.", "body": "@karllessard If the issue is just figuring out how to expose the logic, isn't it just a matter of parsing the `registeredOpList' to get a list of functions and their inputs/outputs and writing a rudimentary translator to make sure that the appropriate inputs and outputs are being sent/received? The way it was discussed above, I thought that we would have to implement the gradient descent optimizer. My approach was just to parse the op list and write a generic graph building class that shows which operations are available and what inputs/attributes are needed. If the gradient descent functions are implemented in the core library, then it seems like it's just a matter of then building a simple class to cycle through layers for backpropagation. The hardest part for me seemed like it was going to be parsing the operations list, and then I could build a couple simple classes to build and train."}