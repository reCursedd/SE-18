{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428409506", "html_url": "https://github.com/tensorflow/tensorflow/issues/22106#issuecomment-428409506", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22106", "id": 428409506, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODQwOTUwNg==", "user": {"login": "BryanRansil", "id": 1517013, "node_id": "MDQ6VXNlcjE1MTcwMTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1517013?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BryanRansil", "html_url": "https://github.com/BryanRansil", "followers_url": "https://api.github.com/users/BryanRansil/followers", "following_url": "https://api.github.com/users/BryanRansil/following{/other_user}", "gists_url": "https://api.github.com/users/BryanRansil/gists{/gist_id}", "starred_url": "https://api.github.com/users/BryanRansil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BryanRansil/subscriptions", "organizations_url": "https://api.github.com/users/BryanRansil/orgs", "repos_url": "https://api.github.com/users/BryanRansil/repos", "events_url": "https://api.github.com/users/BryanRansil/events{/privacy}", "received_events_url": "https://api.github.com/users/BryanRansil/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-10T01:44:04Z", "updated_at": "2018-10-10T01:44:24Z", "author_association": "NONE", "body_html": "<p>So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:</p>\n<ol>\n<li>I checked out r1.10 for both the models and tensorflow repos</li>\n<li>Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. <strong>This by far is most likely what solved my problem</strong>, since I was trying different versions of tensorflow as I was dealing with this issue.</li>\n<li>I trained using a command similar to this:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>python <span class=\"pl-k\">~</span>/tensorflow/models/research/object_detection/model_main.py \\\n       --pipeline_config_path=<span class=\"pl-smi\">${PIPELINE_CONFIG_PATH}</span> \\\n       --model_dir=<span class=\"pl-smi\">${MODEL_DIR}</span> \\\n       --num_train_steps=<span class=\"pl-smi\">${NUM_TRAIN_STEPS}</span> \\\n       --num_eval_steps=<span class=\"pl-smi\">${NUM_EVAL_STEPS}</span> \\\n       --alsologtostderr</pre></div>\n<ol start=\"4\">\n<li>For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>python <span class=\"pl-k\">~</span>/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\n--pipeline_config_path=<span class=\"pl-smi\">$CONFIG_FILE</span> \\\n--trained_checkpoint_prefix=<span class=\"pl-smi\">$CHECKPOINT_PATH</span> \\\n--output_directory=<span class=\"pl-smi\">$EXPORT_OUTPUT_DIR</span> \\\n--add_postprocessing_op=true</pre></div>\n<ol start=\"5\">\n<li>Then we have the toco command. Similar to the blog, but I needed to add a few parameters:</li>\n</ol>\n<div class=\"highlight highlight-source-shell\"><pre>./bazel-bin/tensorflow/contrib/lite/toco/toco \\\n  --input_file=<span class=\"pl-smi\">$INPUT_PB_GRAPH</span> \\\n  --output_file=<span class=\"pl-smi\">$OUTPUT_TFLITE_FILE</span> \\\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\n  --inference_type=QUANTIZED_UINT8 \\\n  --input_shapes=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>1,300, 300,3<span class=\"pl-pds\">\"</span></span> \\\n  --input_arrays=normalized_input_image_tensor \\\n--output_arrays=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TFLite_Detection_PostProcess<span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TFLite_Detection_PostProcess:1<span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TFLite_Detection_PostProcess:2<span class=\"pl-pds\">'</span></span>,<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TFLite_Detection_PostProcess:3<span class=\"pl-pds\">'</span></span> \\\n  --std_values=128.0 --mean_values=128.0 \\\n  --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6</pre></div>\n<ol start=\"6\">\n<li>Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:</li>\n</ol>\n<pre><code>git diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\ndiff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\nindex 9eb21de..2cfa7e0 100644\n--- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\n+++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\n@@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\n       // in label file and class labels start from 1 to number_of_classes+1,\n       // while outputClasses correspond to class index from 0 to number_of_classes\n       int labelOffset = 1;\n-      recognitions.add(\n-          new Recognition(\n-              \"\" + i,\n-              labels.get((int) outputClasses[0][i] + labelOffset),\n-              outputScores[0][i],\n-              detection));\n+        final int classLabel = (int) outputClasses[0][i] + labelOffset;\n+        if (inRange(classLabel, labels.size(), 0) &amp;&amp; inRange(outputScores[0][i], 1, 0)) {\n+            recognitions.add(\n+                    new Recognition(\n+                            \"\" + i,\n+                            labels.get(classLabel),\n+                            outputScores[0][i],\n+                            detection));\n+        }\n     }\n     Trace.endSection(); // \"recognizeImage\"\n     return recognitions;\n   }\n \n+  private boolean inRange(float number, float max, float min) {\n+    return number &lt; max &amp;&amp; number &gt;= min;\n+  }\n+\n</code></pre>\n<p>And then I was able to run the tflite example on my phone! Thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4723042\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/achowdhery\">@achowdhery</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=479117\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jdduke\">@jdduke</a> for responding and the help!</p>", "body_text": "So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\n\nI checked out r1.10 for both the models and tensorflow repos\nUsed bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. This by far is most likely what solved my problem, since I was trying different versions of tensorflow as I was dealing with this issue.\nI trained using a command similar to this:\n\npython ~/tensorflow/models/research/object_detection/model_main.py \\\n       --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\n       --model_dir=${MODEL_DIR} \\\n       --num_train_steps=${NUM_TRAIN_STEPS} \\\n       --num_eval_steps=${NUM_EVAL_STEPS} \\\n       --alsologtostderr\n\nFor specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\n\npython ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\n--pipeline_config_path=$CONFIG_FILE \\\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\n--output_directory=$EXPORT_OUTPUT_DIR \\\n--add_postprocessing_op=true\n\nThen we have the toco command. Similar to the blog, but I needed to add a few parameters:\n\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\n  --input_file=$INPUT_PB_GRAPH \\\n  --output_file=$OUTPUT_TFLITE_FILE \\\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\n  --inference_type=QUANTIZED_UINT8 \\\n  --input_shapes=\"1,300, 300,3\" \\\n  --input_arrays=normalized_input_image_tensor \\\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\n  --std_values=128.0 --mean_values=128.0 \\\n  --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\n\nThen when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\n\ngit diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\ndiff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\nindex 9eb21de..2cfa7e0 100644\n--- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\n+++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\n@@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\n       // in label file and class labels start from 1 to number_of_classes+1,\n       // while outputClasses correspond to class index from 0 to number_of_classes\n       int labelOffset = 1;\n-      recognitions.add(\n-          new Recognition(\n-              \"\" + i,\n-              labels.get((int) outputClasses[0][i] + labelOffset),\n-              outputScores[0][i],\n-              detection));\n+        final int classLabel = (int) outputClasses[0][i] + labelOffset;\n+        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\n+            recognitions.add(\n+                    new Recognition(\n+                            \"\" + i,\n+                            labels.get(classLabel),\n+                            outputScores[0][i],\n+                            detection));\n+        }\n     }\n     Trace.endSection(); // \"recognizeImage\"\n     return recognitions;\n   }\n \n+  private boolean inRange(float number, float max, float min) {\n+    return number < max && number >= min;\n+  }\n+\n\nAnd then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!", "body": "So as a follow up I was able to deal with this issue. For anyone wondering here's what I did:\r\n1. I checked out r1.10 for both the models and tensorflow repos\r\n2. Used bazel to clean the tensorflow repo, that way whenever I use bazel commands we'll use the r1.10 binaries. **This by far is most likely what solved my problem**, since I was trying different versions of tensorflow as I was dealing with this issue.\r\n3. I trained using a command similar to this:\r\n```bash\r\npython ~/tensorflow/models/research/object_detection/model_main.py \\\r\n       --pipeline_config_path=${PIPELINE_CONFIG_PATH} \\\r\n       --model_dir=${MODEL_DIR} \\\r\n       --num_train_steps=${NUM_TRAIN_STEPS} \\\r\n       --num_eval_steps=${NUM_EVAL_STEPS} \\\r\n       --alsologtostderr\r\n```\r\n\r\n4. For specifically tflite I needed to use export_tflite_ssd_graph.py, not export_inference_graph. So the next command was something like:\r\n```bash\r\npython ~/tensorflow/models/research/object_detection/export_tflite_ssd_graph.py \\\r\n--pipeline_config_path=$CONFIG_FILE \\\r\n--trained_checkpoint_prefix=$CHECKPOINT_PATH \\\r\n--output_directory=$EXPORT_OUTPUT_DIR \\\r\n--add_postprocessing_op=true\r\n```\r\n\r\n5. Then we have the toco command. Similar to the blog, but I needed to add a few parameters:\r\n```bash\r\n./bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=$INPUT_PB_GRAPH \\\r\n  --output_file=$OUTPUT_TFLITE_FILE \\\r\n  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --input_shapes=\"1,300, 300,3\" \\\r\n  --input_arrays=normalized_input_image_tensor \\\r\n--output_arrays='TFLite_Detection_PostProcess','TFLite_Detection_PostProcess:1','TFLite_Detection_PostProcess:2','TFLite_Detection_PostProcess:3' \\\r\n  --std_values=128.0 --mean_values=128.0 \\\r\n  --allow_custom_ops --default_ranges_min=0 --default_ranges_max=6\r\n```\r\n\r\n6. Then when loading into the tflite example (so tensorflow/tensorflow/contrib/lite/examples/android) I needed some changes to compile or get past runtime errors and other behaviour:\r\n```\r\ngit diff tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\ndiff --git a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\nindex 9eb21de..2cfa7e0 100644\r\n--- a/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n+++ b/tensorflow/contrib/lite/examples/android/app/src/main/java/org/tensorflow/demo/TFLiteObjectDetectionAPIModel.java\r\n@@ -208,17 +208,24 @@ public class TFLiteObjectDetectionAPIModel implements Classifier {\r\n       // in label file and class labels start from 1 to number_of_classes+1,\r\n       // while outputClasses correspond to class index from 0 to number_of_classes\r\n       int labelOffset = 1;\r\n-      recognitions.add(\r\n-          new Recognition(\r\n-              \"\" + i,\r\n-              labels.get((int) outputClasses[0][i] + labelOffset),\r\n-              outputScores[0][i],\r\n-              detection));\r\n+        final int classLabel = (int) outputClasses[0][i] + labelOffset;\r\n+        if (inRange(classLabel, labels.size(), 0) && inRange(outputScores[0][i], 1, 0)) {\r\n+            recognitions.add(\r\n+                    new Recognition(\r\n+                            \"\" + i,\r\n+                            labels.get(classLabel),\r\n+                            outputScores[0][i],\r\n+                            detection));\r\n+        }\r\n     }\r\n     Trace.endSection(); // \"recognizeImage\"\r\n     return recognitions;\r\n   }\r\n \r\n+  private boolean inRange(float number, float max, float min) {\r\n+    return number < max && number >= min;\r\n+  }\r\n+\r\n```\r\n\r\nAnd then I was able to run the tflite example on my phone! Thanks to @achowdhery and @jdduke for responding and the help!"}