{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22106", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22106/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22106/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22106/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22106", "id": 357524843, "node_id": "MDU6SXNzdWUzNTc1MjQ4NDM=", "number": 22106, "title": "Toco/TFLite_Convert for TFLite Problem", "user": {"login": "BryanRansil", "id": 1517013, "node_id": "MDQ6VXNlcjE1MTcwMTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1517013?v=4", "gravatar_id": "", "url": "https://api.github.com/users/BryanRansil", "html_url": "https://github.com/BryanRansil", "followers_url": "https://api.github.com/users/BryanRansil/followers", "following_url": "https://api.github.com/users/BryanRansil/following{/other_user}", "gists_url": "https://api.github.com/users/BryanRansil/gists{/gist_id}", "starred_url": "https://api.github.com/users/BryanRansil/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/BryanRansil/subscriptions", "organizations_url": "https://api.github.com/users/BryanRansil/orgs", "repos_url": "https://api.github.com/users/BryanRansil/repos", "events_url": "https://api.github.com/users/BryanRansil/events{/privacy}", "received_events_url": "https://api.github.com/users/BryanRansil/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "achowdhery", "id": 4723042, "node_id": "MDQ6VXNlcjQ3MjMwNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4723042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achowdhery", "html_url": "https://github.com/achowdhery", "followers_url": "https://api.github.com/users/achowdhery/followers", "following_url": "https://api.github.com/users/achowdhery/following{/other_user}", "gists_url": "https://api.github.com/users/achowdhery/gists{/gist_id}", "starred_url": "https://api.github.com/users/achowdhery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achowdhery/subscriptions", "organizations_url": "https://api.github.com/users/achowdhery/orgs", "repos_url": "https://api.github.com/users/achowdhery/repos", "events_url": "https://api.github.com/users/achowdhery/events{/privacy}", "received_events_url": "https://api.github.com/users/achowdhery/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "achowdhery", "id": 4723042, "node_id": "MDQ6VXNlcjQ3MjMwNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4723042?v=4", "gravatar_id": "", "url": "https://api.github.com/users/achowdhery", "html_url": "https://github.com/achowdhery", "followers_url": "https://api.github.com/users/achowdhery/followers", "following_url": "https://api.github.com/users/achowdhery/following{/other_user}", "gists_url": "https://api.github.com/users/achowdhery/gists{/gist_id}", "starred_url": "https://api.github.com/users/achowdhery/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/achowdhery/subscriptions", "organizations_url": "https://api.github.com/users/achowdhery/orgs", "repos_url": "https://api.github.com/users/achowdhery/repos", "events_url": "https://api.github.com/users/achowdhery/events{/privacy}", "received_events_url": "https://api.github.com/users/achowdhery/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-09-06T07:15:07Z", "updated_at": "2018-10-10T01:44:24Z", "closed_at": "2018-10-10T01:44:04Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: Pixel 1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0 (commit r1.9)</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.16.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<ol>\n<li></li>\n</ol>\n<p><code>bazel run -c opt tensorflow/python/tools/optimize_for_inference -- --input=$ORIGINAL_PB  --output=$STRIPPED_PB --frozen_graph=True --input_names=Preprocessor/sub --output_names=concat,concat_1 --alsologtostderr</code></p>\n<ol start=\"2\">\n<li></li>\n</ol>\n<p><code>bazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/new_model.tflite  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=QUANTIZED_UINT8 --logtostderr --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_values=127 --allow_custom_opps</code></p>\n<p>or</p>\n<p><code>bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps</code><br>\n(which fails)</p>\n<p>or</p>\n<p><code>bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --input_shapes=1,300,300,3</code><br>\n(which succeeds)</p>\n<ol start=\"3\">\n<li>\n<p>Change TF_OD_API_MODEL_FILE and append new file to the assets list in BUILD</p>\n</li>\n<li></li>\n</ol>\n<p><code>bazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo</code></p>\n<ol start=\"5\">\n<li></li>\n</ol>\n<p><code>adb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk</code></p>\n<ol start=\"6\">\n<li>Open TFL Detect</li>\n</ol>\n<h3>Describe the problem</h3>\n<p>I'm attempting to import <a href=\"http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz\" rel=\"nofollow\">ssd_mobilenet_v1</a>, <a href=\"http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz\" rel=\"nofollow\">ssd_mobilenet_v2</a> and <a href=\"http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz\" rel=\"nofollow\">ssdlite</a> from the (model zoo)[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md] into the TFLite Android example. Ultimately I'm aiming to retrain either the ssdlite or ssd_mobilenet_v2 models, but for right now all models I use trigger runtime errors. All of the errors imply that the models are changed by the <code>optimize_for_inference</code> and <code>toco</code>/<code>tflite_convert</code> commands in a way that makes them incompatible with r1.9.</p>\n<p>Now, it's most likely that my command for <code>toco</code>/<code>tflite_convert</code> are to blame, but since these commands seem to be well formed I'm elevating this to github.</p>\n<h3>Source code / logs</h3>\n<p>Firstly, according to (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md] we're only supposed to use <code>tflite_convert</code> once we're in r1.9. When I try to actually specify all of the fields that the command has in the help (aka the tflite_convert command I put in above) I get the following log and no file is produced:</p>\n<pre lang=\"bazel\" data-meta=\"run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps\"><code> WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\n INFO: Analysed target //tensorflow/contrib/lite/python:tflite_convert (0 packages loaded).\n INFO: Found 1 target...\n Target //tensorflow/contrib/lite/python:tflite_convert up-to-date:\n   bazel-bin/tensorflow/contrib/lite/python/tflite_convert\n INFO: Elapsed time: 0.254s, Critical Path: 0.00s\n INFO: 0 processes.\n INFO: Build completed successfully, 1 total action\n INFO: Running command line: bazel-bin/tensorflow/contrib/lite/python/tflite_convert '--graph_def_file=/home/bryan/Downloads/ssd_mobilenet_v1_coco_2018_01_28/stripped' '--output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite' '--input_arrays=Preprocessor/sub' '--output_arrays=concat,concat_1' '--output_format=TFLITE' '--input_shapes=1,300,3INFO: Build completed successfully, 1 total action\n /home/bryan/.local/lib/python2.7/site-packages/scipy/__init__.py:114: UserWarning: Numpy 1.8.2 or above is recommended for this version of scipy (detected version 1.8.0)\n   UserWarning)\n usage: tflite_convert.py [-h] --output_file OUTPUT_FILE\n                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)\n                          [--output_format {TFLITE,GRAPHVIZ_DOT}]\n                          [--inference_type {FLOAT,QUANTIZED_UINT8}]\n                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\n                          [--input_arrays INPUT_ARRAYS]\n                          [--input_shapes INPUT_SHAPES]\n                          [--output_arrays OUTPUT_ARRAYS]\n                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]\n                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\n                          [--std_dev_values STD_DEV_VALUES]\n                          [--mean_values MEAN_VALUES]\n                          [--default_ranges_min DEFAULT_RANGES_MIN]\n                          [--default_ranges_max DEFAULT_RANGES_MAX]\n                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]\n                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]\n                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]\n                          [--allow_custom_ops ALLOW_CUSTOM_OPS]\n tflite_convert.py: error:\n</code></pre>\n<p>When I strip the tflite_convert params to just include the bare minimum (graph_def_file, output_file, input_arrays, output_arrays, input_shapes) it does create an unquantized tflite model. When I load an unquantized tflite model generated with either command, TFL Detect exits with the following log:</p>\n<pre><code>09-06 00:00:51.046 25024 25041 E AndroidRuntime: FATAL EXCEPTION: inference\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 25024\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: java.lang.IllegalArgumentException: Output error: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4].\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Tensor.copyTo(Tensor.java:44)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:156)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:873)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:99)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:193)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:65)\n09-06 00:00:51.049   914  2995 W ActivityManager:   Force finishing activity org.tensorflow.lite.demo/org.tensorflow.demo.DetectorActivity\n</code></pre>\n<p>The closest I've found as a solution is in (this stackoverflow page)[https://stackoverflow.com/questions/50388330/java-lang-illegalargumentexception-output-error-shape-of-output-target-1-191] which suggests modifying the <code>TFLiteObjectDetectionAPIModel</code> itself (which runs into problems when you get similar errors on the outputClassification array).</p>\n<p>If we use a quantized model, it crashes with this error:</p>\n<pre><code>09-05 22:54:27.413 21650 21667 E AndroidRuntime: java.lang.IllegalArgumentException: Input error: DataType (1) of input data does not match with the DataType (3) of model inputs.\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:123)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:144)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\n</code></pre>\n<p>Given similar error messages in (this test)[https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/contrib/lite/java/src/test/java/org/tensorflow/lite/NativeInterpreterWrapperTest.java#L228] and how commits after r1.9 give the <code>TFLiteObjectDetectionAPIModel</code> class an <code>isQuantized</code> flag this makes me think that r1.9 may not support quantization. If so, is there a definitive source for this? There are several sources that are imperfect in different ways (for the most official sources (fixed point quantization)[https://www.tensorflow.org/performance/quantization] page seems geared towards classification instead of the object detection, the required output arrays in the (medium page)[https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193] are not found when we run toco, and both of them are supposedly out of date because of (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md]).</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: Pixel 1\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.9.0 (commit r1.9)\nPython version: N/A\nBazel version (if compiling from source): 0.16.1\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce:\n\n\n\n\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- --input=$ORIGINAL_PB  --output=$STRIPPED_PB --frozen_graph=True --input_names=Preprocessor/sub --output_names=concat,concat_1 --alsologtostderr\n\n\n\nbazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/new_model.tflite  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=QUANTIZED_UINT8 --logtostderr --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_values=127 --allow_custom_opps\nor\nbazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps\n(which fails)\nor\nbazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --input_shapes=1,300,300,3\n(which succeeds)\n\n\nChange TF_OD_API_MODEL_FILE and append new file to the assets list in BUILD\n\n\n\nbazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo\n\n\n\nadb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk\n\nOpen TFL Detect\n\nDescribe the problem\nI'm attempting to import ssd_mobilenet_v1, ssd_mobilenet_v2 and ssdlite from the (model zoo)[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md] into the TFLite Android example. Ultimately I'm aiming to retrain either the ssdlite or ssd_mobilenet_v2 models, but for right now all models I use trigger runtime errors. All of the errors imply that the models are changed by the optimize_for_inference and toco/tflite_convert commands in a way that makes them incompatible with r1.9.\nNow, it's most likely that my command for toco/tflite_convert are to blame, but since these commands seem to be well formed I'm elevating this to github.\nSource code / logs\nFirstly, according to (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md] we're only supposed to use tflite_convert once we're in r1.9. When I try to actually specify all of the fields that the command has in the help (aka the tflite_convert command I put in above) I get the following log and no file is produced:\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\n INFO: Analysed target //tensorflow/contrib/lite/python:tflite_convert (0 packages loaded).\n INFO: Found 1 target...\n Target //tensorflow/contrib/lite/python:tflite_convert up-to-date:\n   bazel-bin/tensorflow/contrib/lite/python/tflite_convert\n INFO: Elapsed time: 0.254s, Critical Path: 0.00s\n INFO: 0 processes.\n INFO: Build completed successfully, 1 total action\n INFO: Running command line: bazel-bin/tensorflow/contrib/lite/python/tflite_convert '--graph_def_file=/home/bryan/Downloads/ssd_mobilenet_v1_coco_2018_01_28/stripped' '--output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite' '--input_arrays=Preprocessor/sub' '--output_arrays=concat,concat_1' '--output_format=TFLITE' '--input_shapes=1,300,3INFO: Build completed successfully, 1 total action\n /home/bryan/.local/lib/python2.7/site-packages/scipy/__init__.py:114: UserWarning: Numpy 1.8.2 or above is recommended for this version of scipy (detected version 1.8.0)\n   UserWarning)\n usage: tflite_convert.py [-h] --output_file OUTPUT_FILE\n                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)\n                          [--output_format {TFLITE,GRAPHVIZ_DOT}]\n                          [--inference_type {FLOAT,QUANTIZED_UINT8}]\n                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\n                          [--input_arrays INPUT_ARRAYS]\n                          [--input_shapes INPUT_SHAPES]\n                          [--output_arrays OUTPUT_ARRAYS]\n                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]\n                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\n                          [--std_dev_values STD_DEV_VALUES]\n                          [--mean_values MEAN_VALUES]\n                          [--default_ranges_min DEFAULT_RANGES_MIN]\n                          [--default_ranges_max DEFAULT_RANGES_MAX]\n                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]\n                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]\n                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]\n                          [--allow_custom_ops ALLOW_CUSTOM_OPS]\n tflite_convert.py: error:\n\nWhen I strip the tflite_convert params to just include the bare minimum (graph_def_file, output_file, input_arrays, output_arrays, input_shapes) it does create an unquantized tflite model. When I load an unquantized tflite model generated with either command, TFL Detect exits with the following log:\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: FATAL EXCEPTION: inference\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 25024\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: java.lang.IllegalArgumentException: Output error: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4].\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Tensor.copyTo(Tensor.java:44)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:156)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:873)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:99)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:193)\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:65)\n09-06 00:00:51.049   914  2995 W ActivityManager:   Force finishing activity org.tensorflow.lite.demo/org.tensorflow.demo.DetectorActivity\n\nThe closest I've found as a solution is in (this stackoverflow page)[https://stackoverflow.com/questions/50388330/java-lang-illegalargumentexception-output-error-shape-of-output-target-1-191] which suggests modifying the TFLiteObjectDetectionAPIModel itself (which runs into problems when you get similar errors on the outputClassification array).\nIf we use a quantized model, it crashes with this error:\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: java.lang.IllegalArgumentException: Input error: DataType (1) of input data does not match with the DataType (3) of model inputs.\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:123)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:144)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\n\nGiven similar error messages in (this test)[https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/contrib/lite/java/src/test/java/org/tensorflow/lite/NativeInterpreterWrapperTest.java#L228] and how commits after r1.9 give the TFLiteObjectDetectionAPIModel class an isQuantized flag this makes me think that r1.9 may not support quantization. If so, is there a definitive source for this? There are several sources that are imperfect in different ways (for the most official sources (fixed point quantization)[https://www.tensorflow.org/performance/quantization] page seems geared towards classification instead of the object detection, the required output arrays in the (medium page)[https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193] are not found when we run toco, and both of them are supposedly out of date because of (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md]).", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: Pixel 1\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0 (commit r1.9)\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**:\r\n\r\n1.\r\n```bazel run -c opt tensorflow/python/tools/optimize_for_inference -- --input=$ORIGINAL_PB  --output=$STRIPPED_PB --frozen_graph=True --input_names=Preprocessor/sub --output_names=concat,concat_1 --alsologtostderr```\r\n\r\n2.\r\n```bazel run tensorflow/contrib/lite/toco:toco -- --input_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/new_model.tflite  --input_format=TENSORFLOW_GRAPHDEF --output_format=TFLITE --input_shapes=1,300,300,3 --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --inference_type=QUANTIZED_UINT8 --logtostderr --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_values=127 --allow_custom_opps```\r\n\r\nor\r\n\r\n```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps```\r\n(which fails)\r\n\r\nor\r\n\r\n```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/absolute/path/to/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --input_shapes=1,300,300,3```\r\n(which succeeds)\r\n\r\n3. Change TF_OD_API_MODEL_FILE and append new file to the assets list in BUILD\r\n\r\n4.\r\n```bazel build -c opt --cxxopt='--std=c++11' //tensorflow/contrib/lite/examples/android:tflite_demo```\r\n\r\n5.\r\n```adb install -r -f bazel-bin/tensorflow/contrib/lite/examples/android/tflite_demo.apk```\r\n\r\n6. Open TFL Detect\r\n\r\n### Describe the problem\r\nI'm attempting to import [ssd_mobilenet_v1](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_coco_2018_01_28.tar.gz), [ssd_mobilenet_v2](http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_coco_2018_03_29.tar.gz) and [ssdlite](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) from the (model zoo)[https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md] into the TFLite Android example. Ultimately I'm aiming to retrain either the ssdlite or ssd_mobilenet_v2 models, but for right now all models I use trigger runtime errors. All of the errors imply that the models are changed by the `optimize_for_inference` and `toco`/`tflite_convert` commands in a way that makes them incompatible with r1.9.\r\n\r\nNow, it's most likely that my command for `toco`/`tflite_convert` are to blame, but since these commands seem to be well formed I'm elevating this to github.\r\n\r\n### Source code / logs\r\nFirstly, according to (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md] we're only supposed to use `tflite_convert` once we're in r1.9. When I try to actually specify all of the fields that the command has in the help (aka the tflite_convert command I put in above) I get the following log and no file is produced:\r\n\r\n```bazel run //tensorflow/contrib/lite/python:tflite_convert -- --graph_def_file=$STRIPPED_PB --output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite --input_arrays=Preprocessor/sub --output_arrays=concat,concat_1 --output_format=TFLITE --input_shapes=1,300,300,3  --inference_type=QUANTIZED_UINT8 --default_ranges_min=0 --default_ranges_max=5 --mean_values=128 --std_dev_values=127 --allow_custom_opps\r\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_common.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_decode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n WARNING: /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/BUILD:1960:1: in srcs attribute of cc_library rule @grpc//:grpc_nanopb: please do not import '@grpc//third_party/nanopb:pb_encode.c' directly. You should either move the file to this package or depend on an appropriate rule there. Since this rule was created by the macro 'grpc_generate_one_off_targets', the error might have been caused by the macro implementation in /home/bryan/.cache/bazel/_bazel_bryan/36e77cb69a80a4f75d1ba8b192f69b6d/external/grpc/bazel/grpc_build_system.bzl:172:12\r\n INFO: Analysed target //tensorflow/contrib/lite/python:tflite_convert (0 packages loaded).\r\n INFO: Found 1 target...\r\n Target //tensorflow/contrib/lite/python:tflite_convert up-to-date:\r\n   bazel-bin/tensorflow/contrib/lite/python/tflite_convert\r\n INFO: Elapsed time: 0.254s, Critical Path: 0.00s\r\n INFO: 0 processes.\r\n INFO: Build completed successfully, 1 total action\r\n INFO: Running command line: bazel-bin/tensorflow/contrib/lite/python/tflite_convert '--graph_def_file=/home/bryan/Downloads/ssd_mobilenet_v1_coco_2018_01_28/stripped' '--output_file=/home/bryan/Support/tensorflow/tensorflow/contrib/lite/examples/android/assets/tflite_convert_example.tflite' '--input_arrays=Preprocessor/sub' '--output_arrays=concat,concat_1' '--output_format=TFLITE' '--input_shapes=1,300,3INFO: Build completed successfully, 1 total action\r\n /home/bryan/.local/lib/python2.7/site-packages/scipy/__init__.py:114: UserWarning: Numpy 1.8.2 or above is recommended for this version of scipy (detected version 1.8.0)\r\n   UserWarning)\r\n usage: tflite_convert.py [-h] --output_file OUTPUT_FILE\r\n                          (--graph_def_file GRAPH_DEF_FILE | --saved_model_dir SAVED_MODEL_DIR)\r\n                          [--output_format {TFLITE,GRAPHVIZ_DOT}]\r\n                          [--inference_type {FLOAT,QUANTIZED_UINT8}]\r\n                          [--inference_input_type {FLOAT,QUANTIZED_UINT8}]\r\n                          [--input_arrays INPUT_ARRAYS]\r\n                          [--input_shapes INPUT_SHAPES]\r\n                          [--output_arrays OUTPUT_ARRAYS]\r\n                          [--saved_model_tag_set SAVED_MODEL_TAG_SET]\r\n                          [--saved_model_signature_key SAVED_MODEL_SIGNATURE_KEY]\r\n                          [--std_dev_values STD_DEV_VALUES]\r\n                          [--mean_values MEAN_VALUES]\r\n                          [--default_ranges_min DEFAULT_RANGES_MIN]\r\n                          [--default_ranges_max DEFAULT_RANGES_MAX]\r\n                          [--drop_control_dependency DROP_CONTROL_DEPENDENCY]\r\n                          [--reorder_across_fake_quant REORDER_ACROSS_FAKE_QUANT]\r\n                          [--change_concat_input_ranges CHANGE_CONCAT_INPUT_RANGES]\r\n                          [--allow_custom_ops ALLOW_CUSTOM_OPS]\r\n tflite_convert.py: error:\r\n```\r\n\r\nWhen I strip the tflite_convert params to just include the bare minimum (graph_def_file, output_file, input_arrays, output_arrays, input_shapes) it does create an unquantized tflite model. When I load an unquantized tflite model generated with either command, TFL Detect exits with the following log:\r\n\r\n```\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: FATAL EXCEPTION: inference\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: Process: org.tensorflow.lite.demo, PID: 25024\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: java.lang.IllegalArgumentException: Output error: Shape of output target [1, 1917, 4] does not match with the shape of the Tensor [1, 1917, 1, 4].\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Tensor.copyTo(Tensor.java:44)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:156)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.handleCallback(Handler.java:873)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Handler.dispatchMessage(Handler.java:99)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.Looper.loop(Looper.java:193)\r\n09-06 00:00:51.046 25024 25041 E AndroidRuntime: \tat android.os.HandlerThread.run(HandlerThread.java:65)\r\n09-06 00:00:51.049   914  2995 W ActivityManager:   Force finishing activity org.tensorflow.lite.demo/org.tensorflow.demo.DetectorActivity\r\n```\r\n\r\nThe closest I've found as a solution is in (this stackoverflow page)[https://stackoverflow.com/questions/50388330/java-lang-illegalargumentexception-output-error-shape-of-output-target-1-191] which suggests modifying the `TFLiteObjectDetectionAPIModel` itself (which runs into problems when you get similar errors on the outputClassification array).\r\n\r\nIf we use a quantized model, it crashes with this error:\r\n```\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: java.lang.IllegalArgumentException: Input error: DataType (1) of input data does not match with the DataType (3) of model inputs.\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(Native Method)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.NativeInterpreterWrapper.run(NativeInterpreterWrapper.java:123)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.lite.Interpreter.runForMultipleInputsOutputs(Interpreter.java:144)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.TFLiteObjectDetectionAPIModel.recognizeImage(TFLiteObjectDetectionAPIModel.java:222)\r\n09-05 22:54:27.413 21650 21667 E AndroidRuntime: \tat org.tensorflow.demo.DetectorActivity$3.run(DetectorActivity.java:242)\r\n```\r\n\r\nGiven similar error messages in (this test)[https://github.com/OAID/TensorFlow-HRT/blob/master/tensorflow/contrib/lite/java/src/test/java/org/tensorflow/lite/NativeInterpreterWrapperTest.java#L228] and how commits after r1.9 give the `TFLiteObjectDetectionAPIModel` class an `isQuantized` flag this makes me think that r1.9 may not support quantization. If so, is there a definitive source for this? There are several sources that are imperfect in different ways (for the most official sources (fixed point quantization)[https://www.tensorflow.org/performance/quantization] page seems geared towards classification instead of the object detection, the required output arrays in the (medium page)[https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193] are not found when we run toco, and both of them are supposedly out of date because of (the toco documentation)[https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/toco/g3doc/cmdline_examples.md])."}