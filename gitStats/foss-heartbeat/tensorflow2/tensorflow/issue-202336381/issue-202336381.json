{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6998", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6998/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6998/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6998/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6998", "id": 202336381, "node_id": "MDU6SXNzdWUyMDIzMzYzODE=", "number": 6998, "title": "Back-propagating gradients through a sparse tensor?", "user": {"login": "zergylord", "id": 2257125, "node_id": "MDQ6VXNlcjIyNTcxMjU=", "avatar_url": "https://avatars2.githubusercontent.com/u/2257125?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zergylord", "html_url": "https://github.com/zergylord", "followers_url": "https://api.github.com/users/zergylord/followers", "following_url": "https://api.github.com/users/zergylord/following{/other_user}", "gists_url": "https://api.github.com/users/zergylord/gists{/gist_id}", "starred_url": "https://api.github.com/users/zergylord/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zergylord/subscriptions", "organizations_url": "https://api.github.com/users/zergylord/orgs", "repos_url": "https://api.github.com/users/zergylord/repos", "events_url": "https://api.github.com/users/zergylord/events{/privacy}", "received_events_url": "https://api.github.com/users/zergylord/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-01-21T19:57:39Z", "updated_at": "2017-06-16T16:40:49Z", "closed_at": "2017-06-16T16:40:49Z", "author_association": "NONE", "body_html": "<p>I have a normal feed-forward network that produces a vector v. The elements of v are then used as the non-zero entries of a sparse matrix M (assume the coordinates are predefined). The sparse matrix is then multiplied by a dense vector and a loss is defined on the resulting scalar. I want to back-propagate the loss w.r.t. the weights of the network, which entails going through the sparse matrix.</p>\n<p>This seems like a perfectly reasonable use-case for a sparse matrix, but it appears that such functionality is not supported. Indeed, even calling tf.gradients(M,[v]) produces an error:</p>\n<blockquote>\n<p>AttributeError: 'SparseTensor' object has no attribute 'value_index'</p>\n</blockquote>\n<p>Am I doing something wrong or am I correct in presuming that this functionality doesn't (yet?) exist? If the latter, then is there a work-around for this particular use-case short of rewriting all of the sparse tensor operations with gradients defined?</p>", "body_text": "I have a normal feed-forward network that produces a vector v. The elements of v are then used as the non-zero entries of a sparse matrix M (assume the coordinates are predefined). The sparse matrix is then multiplied by a dense vector and a loss is defined on the resulting scalar. I want to back-propagate the loss w.r.t. the weights of the network, which entails going through the sparse matrix.\nThis seems like a perfectly reasonable use-case for a sparse matrix, but it appears that such functionality is not supported. Indeed, even calling tf.gradients(M,[v]) produces an error:\n\nAttributeError: 'SparseTensor' object has no attribute 'value_index'\n\nAm I doing something wrong or am I correct in presuming that this functionality doesn't (yet?) exist? If the latter, then is there a work-around for this particular use-case short of rewriting all of the sparse tensor operations with gradients defined?", "body": "I have a normal feed-forward network that produces a vector v. The elements of v are then used as the non-zero entries of a sparse matrix M (assume the coordinates are predefined). The sparse matrix is then multiplied by a dense vector and a loss is defined on the resulting scalar. I want to back-propagate the loss w.r.t. the weights of the network, which entails going through the sparse matrix. \r\n\r\nThis seems like a perfectly reasonable use-case for a sparse matrix, but it appears that such functionality is not supported. Indeed, even calling tf.gradients(M,[v]) produces an error:\r\n\r\n> AttributeError: 'SparseTensor' object has no attribute 'value_index'\r\n\r\nAm I doing something wrong or am I correct in presuming that this functionality doesn't (yet?) exist? If the latter, then is there a work-around for this particular use-case short of rewriting all of the sparse tensor operations with gradients defined? "}