{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/370475392", "html_url": "https://github.com/tensorflow/tensorflow/issues/17427#issuecomment-370475392", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17427", "id": 370475392, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDQ3NTM5Mg==", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-05T16:24:08Z", "updated_at": "2018-03-05T16:24:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's actually super easy to get data parallel, single-machine, multi-GPU training going with a <code>tf.estimator</code> by using the <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/estimator/replicate_model_fn\" rel=\"nofollow\">replicate_model_fn</a> and <a href=\"https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/estimator/TowerOptimizer\" rel=\"nofollow\">TowerOptimizer</a> decorators. Just make sure your batch size is divisible by the number of GPUs.</p>", "body_text": "It's actually super easy to get data parallel, single-machine, multi-GPU training going with a tf.estimator by using the replicate_model_fn and TowerOptimizer decorators. Just make sure your batch size is divisible by the number of GPUs.", "body": "It's actually super easy to get data parallel, single-machine, multi-GPU training going with a `tf.estimator` by using the [replicate_model_fn](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/estimator/replicate_model_fn) and [TowerOptimizer](https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/estimator/TowerOptimizer) decorators. Just make sure your batch size is divisible by the number of GPUs."}