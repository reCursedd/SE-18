{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20764", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20764/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20764/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20764/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20764", "id": 340856237, "node_id": "MDU6SXNzdWUzNDA4NTYyMzc=", "number": 20764, "title": "Can't build custom ops in tensorflow-gpu 1.9.0 if include \"cuda_kernel_helper.h\" with C++14", "user": {"login": "traveller59", "id": 28866047, "node_id": "MDQ6VXNlcjI4ODY2MDQ3", "avatar_url": "https://avatars1.githubusercontent.com/u/28866047?v=4", "gravatar_id": "", "url": "https://api.github.com/users/traveller59", "html_url": "https://github.com/traveller59", "followers_url": "https://api.github.com/users/traveller59/followers", "following_url": "https://api.github.com/users/traveller59/following{/other_user}", "gists_url": "https://api.github.com/users/traveller59/gists{/gist_id}", "starred_url": "https://api.github.com/users/traveller59/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/traveller59/subscriptions", "organizations_url": "https://api.github.com/users/traveller59/orgs", "repos_url": "https://api.github.com/users/traveller59/repos", "events_url": "https://api.github.com/users/traveller59/events{/privacy}", "received_events_url": "https://api.github.com/users/traveller59/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ymodak", "id": 42785357, "node_id": "MDQ6VXNlcjQyNzg1MzU3", "avatar_url": "https://avatars1.githubusercontent.com/u/42785357?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ymodak", "html_url": "https://github.com/ymodak", "followers_url": "https://api.github.com/users/ymodak/followers", "following_url": "https://api.github.com/users/ymodak/following{/other_user}", "gists_url": "https://api.github.com/users/ymodak/gists{/gist_id}", "starred_url": "https://api.github.com/users/ymodak/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ymodak/subscriptions", "organizations_url": "https://api.github.com/users/ymodak/orgs", "repos_url": "https://api.github.com/users/ymodak/repos", "events_url": "https://api.github.com/users/ymodak/events{/privacy}", "received_events_url": "https://api.github.com/users/ymodak/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-13T02:07:09Z", "updated_at": "2018-10-15T19:09:35Z", "closed_at": "2018-10-15T19:09:35Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:v1.9.0-0-g25c197e023 1.9.0</li>\n<li><strong>Python version</strong>: 3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:5.5.0</li>\n<li><strong>CUDA/cuDNN version</strong>:9.0/7.0</li>\n<li><strong>GPU model and memory</strong>:GTX1060</li>\n<li><strong>Exact command to reproduce</strong>:N?A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>If I build a custom ops with C++ 14 and \"cuda_kernel_helper.h\", following errors are produced:</p>\n<div class=\"highlight highlight-source-shell\"><pre><span class=\"pl-k\">&gt;&gt;</span> nvcc -std=c++14 -c -o fill_functor.cu.o fill_functor.cu.cc    -arch=sm_61 <span class=\"pl-smi\">${TF_CFLAGS[@]}</span> -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -I/usr/local --expt-relaxed-constexpr\n\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool <span class=\"pl-en\">stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::ok</span>() const [with T <span class=\"pl-k\">=</span> stream_executor::dnn::VersionInfo]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from <span class=\"pl-en\">\u2018stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::~StatusOrData</span>() [with T <span class=\"pl-k\">=</span> stream_executor::dnn::VersionInfo]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from <span class=\"pl-c1\">type</span> \u2018const Status {aka const tensorflow::Status}\u2019 to <span class=\"pl-c1\">type</span> \u2018stream_executor::port::Status<span class=\"pl-k\">&amp;</span> {aka tensorflow::Status<span class=\"pl-k\">&amp;</span>}\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool <span class=\"pl-en\">stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::ok</span>() const [with T <span class=\"pl-k\">=</span> std::unique_ptr<span class=\"pl-k\">&lt;</span>stream_executor::dnn::RnnDescriptor<span class=\"pl-k\">&gt;</span>]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from <span class=\"pl-en\">\u2018stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::~StatusOrData</span>() [with T <span class=\"pl-k\">=</span> std::unique_ptr<span class=\"pl-k\">&lt;</span>stream_executor::dnn::RnnDescriptor<span class=\"pl-k\">&gt;</span>]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from <span class=\"pl-c1\">type</span> \u2018const Status {aka const tensorflow::Status}\u2019 to <span class=\"pl-c1\">type</span> \u2018stream_executor::port::Status<span class=\"pl-k\">&amp;</span> {aka tensorflow::Status<span class=\"pl-k\">&amp;</span>}\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool <span class=\"pl-en\">stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::ok</span>() const [with T <span class=\"pl-k\">=</span> std::unique_ptr<span class=\"pl-k\">&lt;</span>stream_executor::dnn::RnnSequenceTensorDescriptor<span class=\"pl-k\">&gt;</span>]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from <span class=\"pl-en\">\u2018stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::~StatusOrData</span>() [with T <span class=\"pl-k\">=</span> std::unique_ptr<span class=\"pl-k\">&lt;</span>stream_executor::dnn::RnnSequenceTensorDescriptor<span class=\"pl-k\">&gt;</span>]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from <span class=\"pl-c1\">type</span> \u2018const Status {aka const tensorflow::Status}\u2019 to <span class=\"pl-c1\">type</span> \u2018stream_executor::port::Status<span class=\"pl-k\">&amp;</span> {aka tensorflow::Status<span class=\"pl-k\">&amp;</span>}\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool <span class=\"pl-en\">stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::ok</span>() const [with T <span class=\"pl-k\">=</span> std::unique_ptr<span class=\"pl-k\">&lt;</span>stream_executor::dnn::RnnStateTensorDescriptor<span class=\"pl-k\">&gt;</span>]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from <span class=\"pl-en\">\u2018stream_executor::port::internal_statusor::StatusOrData&lt;T&gt;::~StatusOrData</span>() [with T <span class=\"pl-k\">=</span> std::unique_ptr<span class=\"pl-k\">&lt;</span>stream_executor::dnn::RnnStateTensorDescriptor<span class=\"pl-k\">&gt;</span>]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from <span class=\"pl-c1\">type</span> \u2018const Status {aka const tensorflow::Status}\u2019 to <span class=\"pl-c1\">type</span> \u2018stream_executor::port::Status<span class=\"pl-k\">&amp;</span> {aka tensorflow::Status<span class=\"pl-k\">&amp;</span>}\u2019</pre></div>\n<p>If I build it with <code>std=c++11</code> or tensorflow 1.8.0, there is no problem. But I need auto lambda to use static loop in my host code which only available in c++14.</p>\n<h3>Source code / logs</h3>\n<p>The following code is a minimal example to reproduce that error.</p>\n<div class=\"highlight highlight-source-c++\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> fill_functor.h</span>\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">EIGEN_USE_THREADS</span>\n\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>third_party/eigen3/unsupported/Eigen/CXX11/Tensor<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/tensor_types.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/types.h<span class=\"pl-pds\">\"</span></span>\n\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">tensorflow</span> {\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">functor</span> {\n\n<span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> Device, <span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">FillFunctor</span> {\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> Computes on device \"d\": out = out.constant(in(0)),</span>\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> Device&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::Flat out,\n                  <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::ConstScalar in);\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> Device&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::Flat out,\n                  T in);\n};\n\n<span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> Device, <span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">SetZeroFunctor</span> {\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> Computes on device \"d\": out = out.setZero(),</span>\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> Device&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::Flat out);\n};\n\n\n}  <span class=\"pl-c\"><span class=\"pl-c\">//</span> namespace functor</span>\n}  <span class=\"pl-c\"><span class=\"pl-c\">//</span> namespace tensorflow</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> fill_functor.cu.cc</span>\n#<span class=\"pl-k\">if</span> GOOGLE_CUDA\n\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">EIGEN_USE_GPU</span>\n\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/register_types.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/framework/tensor_types.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>fill_functor.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/platform/types.h<span class=\"pl-pds\">\"</span></span>\n#<span class=\"pl-k\">include</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>tensorflow/core/util/cuda_kernel_helper.h<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">Eigen</span> {\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">internal</span> {\n\n<span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">scalar_const_op</span> {\n  <span class=\"pl-k\">typedef</span> <span class=\"pl-k\">typename</span> packet_traits&lt;T&gt;::type Packet;\n\n  <span class=\"pl-k\">const</span> T* val;\n\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\n  <span class=\"pl-en\">scalar_const_op</span>(<span class=\"pl-k\">const</span> scalar_const_op&amp; x)\n      : val(x.val) {}\n\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE <span class=\"pl-en\">scalar_const_op</span>(<span class=\"pl-k\">const</span> T* v) : val(v) {}\n\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE <span class=\"pl-k\">const</span> T <span class=\"pl-en\">operator</span>()() <span class=\"pl-k\">const</span> {\n    <span class=\"pl-k\">return</span> *val;\n  }\n\n  <span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> PacketType = Packet&gt;\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE <span class=\"pl-k\">const</span> PacketType <span class=\"pl-en\">packetOp</span>() <span class=\"pl-k\">const</span> {\n    <span class=\"pl-k\">return</span> internal::pset1&lt;PacketType&gt;(*val);\n  }\n};\n\n<span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">functor_traits</span>&lt;scalar_const_op&lt;T&gt; &gt; {\n  <span class=\"pl-k\">enum</span> {\n    Cost = <span class=\"pl-c1\">1</span>,\n    PacketAccess = packet_traits&lt;T&gt;::Vectorizable,\n    IsRepeatable = <span class=\"pl-c1\">true</span>\n  };\n};\n\n}  <span class=\"pl-c\"><span class=\"pl-c\">//</span> end namespace internal</span>\n}  <span class=\"pl-c\"><span class=\"pl-c\">//</span> end namespace Eigen</span>\n\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">tensorflow</span> {\n\n<span class=\"pl-k\">namespace</span> <span class=\"pl-en\">functor</span> {\n\n<span class=\"pl-k\">typedef</span> Eigen::GpuDevice GPUDevice;\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Partial specialization FillFunctor&lt;Device=GPUDevice, T&gt;</span>\n<span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">FillFunctor</span>&lt;GPUDevice, T&gt; {\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> GPUDevice&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::Flat out,\n                  <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::ConstScalar in) {\n    Eigen::internal::scalar_const_op&lt;T&gt; <span class=\"pl-c1\">f</span>(in.<span class=\"pl-c1\">data</span>());\n    <span class=\"pl-c1\">To32Bit</span>(out).<span class=\"pl-c1\">device</span>(d) = <span class=\"pl-c1\">To32Bit</span>(out).<span class=\"pl-c1\">nullaryExpr</span>(f);\n  }\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> GPUDevice&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::Flat out,\n                  T in) {\n    <span class=\"pl-c1\">To32Bit</span>(out).<span class=\"pl-c1\">device</span>(d) = <span class=\"pl-c1\">To32Bit</span>(out).<span class=\"pl-c1\">constant</span>(in);\n  }\n};\n\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">DEFINE_FILL_GPU</span>(<span class=\"pl-v\">T</span>) <span class=\"pl-k\">template </span><span class=\"pl-k\">struct</span> <span class=\"pl-en\">FillFunctor</span>&lt;GPUDevice, T&gt;;\n<span class=\"pl-en\">TF_CALL_NUMBER_TYPES</span>(DEFINE_FILL_GPU);\n<span class=\"pl-en\">TF_CALL_bool</span>(DEFINE_FILL_GPU);\n#<span class=\"pl-k\">undef</span> DEFINE_FILL_GPU\n\n<span class=\"pl-c\"><span class=\"pl-c\">//</span> Partial specialization of FillFunctor&lt;Device=GPUDevice, T&gt;.</span>\n<span class=\"pl-k\">template </span>&lt;<span class=\"pl-k\">typename</span> T&gt;\n<span class=\"pl-k\">struct</span> <span class=\"pl-en\">SetZeroFunctor</span>&lt;GPUDevice, T&gt; {\n  <span class=\"pl-k\">void</span> <span class=\"pl-en\">operator</span>()(<span class=\"pl-k\">const</span> GPUDevice&amp; d, <span class=\"pl-k\">typename</span> TTypes&lt;T&gt;::Flat out) {\n    <span class=\"pl-c1\">To32Bit</span>(out).<span class=\"pl-c1\">device</span>(d) = <span class=\"pl-c1\">To32Bit</span>(out).<span class=\"pl-c1\">constant</span>(<span class=\"pl-c1\">T</span>(<span class=\"pl-c1\">0</span>));\n  }\n};\n\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">DEFINE_SETZERO_GPU</span>(<span class=\"pl-v\">T</span>) <span class=\"pl-k\">template </span><span class=\"pl-k\">struct</span> <span class=\"pl-en\">SetZeroFunctor</span>&lt;GPUDevice, T&gt;;\n<span class=\"pl-en\">TF_CALL_NUMBER_TYPES</span>(DEFINE_SETZERO_GPU);\n<span class=\"pl-en\">TF_CALL_bool</span>(DEFINE_SETZERO_GPU);\n#<span class=\"pl-k\">undef</span> DEFINE_SETZERO_GPU\n\n\n}  <span class=\"pl-c\"><span class=\"pl-c\">//</span> end namespace functor</span>\n}  <span class=\"pl-c\"><span class=\"pl-c\">//</span> end namespace tensorflow</span>\n\n#<span class=\"pl-k\">endif</span>  <span class=\"pl-c\"><span class=\"pl-c\">//</span> GOOGLE_CUDA</span></pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):v1.9.0-0-g25c197e023 1.9.0\nPython version: 3.6.5\nBazel version (if compiling from source):N/A\nGCC/Compiler version (if compiling from source):5.5.0\nCUDA/cuDNN version:9.0/7.0\nGPU model and memory:GTX1060\nExact command to reproduce:N?A\n\nDescribe the problem\nIf I build a custom ops with C++ 14 and \"cuda_kernel_helper.h\", following errors are produced:\n>> nvcc -std=c++14 -c -o fill_functor.cu.o fill_functor.cu.cc    -arch=sm_61 ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -I/usr/local --expt-relaxed-constexpr\n\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = stream_executor::dnn::VersionInfo]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = stream_executor::dnn::VersionInfo]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]\u2019:\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]\u2019\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\nIf I build it with std=c++11 or tensorflow 1.8.0, there is no problem. But I need auto lambda to use static loop in my host code which only available in c++14.\nSource code / logs\nThe following code is a minimal example to reproduce that error.\n// fill_functor.h\n#define EIGEN_USE_THREADS\n\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"tensorflow/core/framework/types.h\"\n\nnamespace tensorflow {\nnamespace functor {\n\ntemplate <typename Device, typename T>\nstruct FillFunctor {\n  // Computes on device \"d\": out = out.constant(in(0)),\n  void operator()(const Device& d, typename TTypes<T>::Flat out,\n                  typename TTypes<T>::ConstScalar in);\n  void operator()(const Device& d, typename TTypes<T>::Flat out,\n                  T in);\n};\n\ntemplate <typename Device, typename T>\nstruct SetZeroFunctor {\n  // Computes on device \"d\": out = out.setZero(),\n  void operator()(const Device& d, typename TTypes<T>::Flat out);\n};\n\n\n}  // namespace functor\n}  // namespace tensorflow\n\n// fill_functor.cu.cc\n#if GOOGLE_CUDA\n\n#define EIGEN_USE_GPU\n\n#include \"tensorflow/core/framework/register_types.h\"\n#include \"tensorflow/core/framework/tensor_types.h\"\n#include \"fill_functor.h\"\n#include \"tensorflow/core/platform/types.h\"\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\nnamespace Eigen {\nnamespace internal {\n\ntemplate <typename T>\nstruct scalar_const_op {\n  typedef typename packet_traits<T>::type Packet;\n\n  const T* val;\n\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\n  scalar_const_op(const scalar_const_op& x)\n      : val(x.val) {}\n\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_const_op(const T* v) : val(v) {}\n\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T operator()() const {\n    return *val;\n  }\n\n  template <typename PacketType = Packet>\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const PacketType packetOp() const {\n    return internal::pset1<PacketType>(*val);\n  }\n};\n\ntemplate <typename T>\nstruct functor_traits<scalar_const_op<T> > {\n  enum {\n    Cost = 1,\n    PacketAccess = packet_traits<T>::Vectorizable,\n    IsRepeatable = true\n  };\n};\n\n}  // end namespace internal\n}  // end namespace Eigen\n\nnamespace tensorflow {\n\nnamespace functor {\n\ntypedef Eigen::GpuDevice GPUDevice;\n\n// Partial specialization FillFunctor<Device=GPUDevice, T>\ntemplate <typename T>\nstruct FillFunctor<GPUDevice, T> {\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,\n                  typename TTypes<T>::ConstScalar in) {\n    Eigen::internal::scalar_const_op<T> f(in.data());\n    To32Bit(out).device(d) = To32Bit(out).nullaryExpr(f);\n  }\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,\n                  T in) {\n    To32Bit(out).device(d) = To32Bit(out).constant(in);\n  }\n};\n\n#define DEFINE_FILL_GPU(T) template struct FillFunctor<GPUDevice, T>;\nTF_CALL_NUMBER_TYPES(DEFINE_FILL_GPU);\nTF_CALL_bool(DEFINE_FILL_GPU);\n#undef DEFINE_FILL_GPU\n\n// Partial specialization of FillFunctor<Device=GPUDevice, T>.\ntemplate <typename T>\nstruct SetZeroFunctor<GPUDevice, T> {\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out) {\n    To32Bit(out).device(d) = To32Bit(out).constant(T(0));\n  }\n};\n\n#define DEFINE_SETZERO_GPU(T) template struct SetZeroFunctor<GPUDevice, T>;\nTF_CALL_NUMBER_TYPES(DEFINE_SETZERO_GPU);\nTF_CALL_bool(DEFINE_SETZERO_GPU);\n#undef DEFINE_SETZERO_GPU\n\n\n}  // end namespace functor\n}  // end namespace tensorflow\n\n#endif  // GOOGLE_CUDA", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6.5\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:5.5.0\r\n- **CUDA/cuDNN version**:9.0/7.0\r\n- **GPU model and memory**:GTX1060\r\n- **Exact command to reproduce**:N?A\r\n\r\n### Describe the problem\r\nIf I build a custom ops with C++ 14 and \"cuda_kernel_helper.h\", following errors are produced:\r\n```bash\r\n>> nvcc -std=c++14 -c -o fill_functor.cu.o fill_functor.cu.cc    -arch=sm_61 ${TF_CFLAGS[@]} -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC -I/usr/local --expt-relaxed-constexpr\r\n\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = stream_executor::dnn::VersionInfo]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = stream_executor::dnn::VersionInfo]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnDescriptor>]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnSequenceTensorDescriptor>]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h: In instantiation of \u2018bool stream_executor::port::internal_statusor::StatusOrData<T>::ok() const [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]\u2019:\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:124:7:   required from \u2018stream_executor::port::internal_statusor::StatusOrData<T>::~StatusOrData() [with T = std::unique_ptr<stream_executor::dnn::RnnStateTensorDescriptor>]\u2019\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor.h:85:7:   required from here\r\n/home/xxx/anaconda3/lib/python3.6/site-packages/tensorflow/include/tensorflow/stream_executor/lib/statusor_internals.h:164:76: error: invalid static_cast from type \u2018const Status {aka const tensorflow::Status}\u2019 to type \u2018stream_executor::port::Status& {aka tensorflow::Status&}\u2019\r\n```\r\nIf I build it with `std=c++11` or tensorflow 1.8.0, there is no problem. But I need auto lambda to use static loop in my host code which only available in c++14.\r\n### Source code / logs\r\nThe following code is a minimal example to reproduce that error.\r\n```C++\r\n// fill_functor.h\r\n#define EIGEN_USE_THREADS\r\n\r\n#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\r\n#include \"tensorflow/core/framework/tensor_types.h\"\r\n#include \"tensorflow/core/framework/types.h\"\r\n\r\nnamespace tensorflow {\r\nnamespace functor {\r\n\r\ntemplate <typename Device, typename T>\r\nstruct FillFunctor {\r\n  // Computes on device \"d\": out = out.constant(in(0)),\r\n  void operator()(const Device& d, typename TTypes<T>::Flat out,\r\n                  typename TTypes<T>::ConstScalar in);\r\n  void operator()(const Device& d, typename TTypes<T>::Flat out,\r\n                  T in);\r\n};\r\n\r\ntemplate <typename Device, typename T>\r\nstruct SetZeroFunctor {\r\n  // Computes on device \"d\": out = out.setZero(),\r\n  void operator()(const Device& d, typename TTypes<T>::Flat out);\r\n};\r\n\r\n\r\n}  // namespace functor\r\n}  // namespace tensorflow\r\n\r\n// fill_functor.cu.cc\r\n#if GOOGLE_CUDA\r\n\r\n#define EIGEN_USE_GPU\r\n\r\n#include \"tensorflow/core/framework/register_types.h\"\r\n#include \"tensorflow/core/framework/tensor_types.h\"\r\n#include \"fill_functor.h\"\r\n#include \"tensorflow/core/platform/types.h\"\r\n#include \"tensorflow/core/util/cuda_kernel_helper.h\"\r\nnamespace Eigen {\r\nnamespace internal {\r\n\r\ntemplate <typename T>\r\nstruct scalar_const_op {\r\n  typedef typename packet_traits<T>::type Packet;\r\n\r\n  const T* val;\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE\r\n  scalar_const_op(const scalar_const_op& x)\r\n      : val(x.val) {}\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE scalar_const_op(const T* v) : val(v) {}\r\n\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const T operator()() const {\r\n    return *val;\r\n  }\r\n\r\n  template <typename PacketType = Packet>\r\n  EIGEN_DEVICE_FUNC EIGEN_STRONG_INLINE const PacketType packetOp() const {\r\n    return internal::pset1<PacketType>(*val);\r\n  }\r\n};\r\n\r\ntemplate <typename T>\r\nstruct functor_traits<scalar_const_op<T> > {\r\n  enum {\r\n    Cost = 1,\r\n    PacketAccess = packet_traits<T>::Vectorizable,\r\n    IsRepeatable = true\r\n  };\r\n};\r\n\r\n}  // end namespace internal\r\n}  // end namespace Eigen\r\n\r\nnamespace tensorflow {\r\n\r\nnamespace functor {\r\n\r\ntypedef Eigen::GpuDevice GPUDevice;\r\n\r\n// Partial specialization FillFunctor<Device=GPUDevice, T>\r\ntemplate <typename T>\r\nstruct FillFunctor<GPUDevice, T> {\r\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,\r\n                  typename TTypes<T>::ConstScalar in) {\r\n    Eigen::internal::scalar_const_op<T> f(in.data());\r\n    To32Bit(out).device(d) = To32Bit(out).nullaryExpr(f);\r\n  }\r\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out,\r\n                  T in) {\r\n    To32Bit(out).device(d) = To32Bit(out).constant(in);\r\n  }\r\n};\r\n\r\n#define DEFINE_FILL_GPU(T) template struct FillFunctor<GPUDevice, T>;\r\nTF_CALL_NUMBER_TYPES(DEFINE_FILL_GPU);\r\nTF_CALL_bool(DEFINE_FILL_GPU);\r\n#undef DEFINE_FILL_GPU\r\n\r\n// Partial specialization of FillFunctor<Device=GPUDevice, T>.\r\ntemplate <typename T>\r\nstruct SetZeroFunctor<GPUDevice, T> {\r\n  void operator()(const GPUDevice& d, typename TTypes<T>::Flat out) {\r\n    To32Bit(out).device(d) = To32Bit(out).constant(T(0));\r\n  }\r\n};\r\n\r\n#define DEFINE_SETZERO_GPU(T) template struct SetZeroFunctor<GPUDevice, T>;\r\nTF_CALL_NUMBER_TYPES(DEFINE_SETZERO_GPU);\r\nTF_CALL_bool(DEFINE_SETZERO_GPU);\r\n#undef DEFINE_SETZERO_GPU\r\n\r\n\r\n}  // end namespace functor\r\n}  // end namespace tensorflow\r\n\r\n#endif  // GOOGLE_CUDA\r\n```\r\n"}