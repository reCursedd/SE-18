{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335335232", "html_url": "https://github.com/tensorflow/tensorflow/issues/13589#issuecomment-335335232", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13589", "id": 335335232, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTMzNTIzMg==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-10T01:45:18Z", "updated_at": "2017-10-10T01:45:18Z", "author_association": "MEMBER", "body_html": "<p>You pasted a slightly older version of the chunk of code. The code is currently</p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Only use _fused_batch_norm if all of the following three</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> conditions are true:</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (1) fused is set True;</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (2) it is possible to use (currently it doesn't support batch weights,</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   renorm, and the case when rank is neither 2 nor 4);</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (3) it is used with zero_debias_moving_mean, or an input shape of rank 2,</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   or non-default updates_collections (not implemented in</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   normalization_layers.BatchNormalization yet); otherwise use the fused</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>   implementation in normalization_layers.BatchNormalization.</span>\n  inputs <span class=\"pl-k\">=</span> ops.convert_to_tensor(inputs)\n  rank <span class=\"pl-k\">=</span> inputs.get_shape().ndims\n  possible_to_fuse <span class=\"pl-k\">=</span> batch_weights <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> renorm <span class=\"pl-k\">and</span> rank <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>]\n  <span class=\"pl-k\">if</span> fused <span class=\"pl-k\">and</span> possible_to_fuse <span class=\"pl-k\">and</span> (\n      zero_debias_moving_mean <span class=\"pl-k\">or</span> rank <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">or</span>\n      updates_collections <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> ops.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>):</pre></div>\n<p>Now, the comment and the code match. Note the rank can be 4, as long as <code>zero_debias_moving_mean</code> is True or if non-default update collections are used.</p>", "body_text": "You pasted a slightly older version of the chunk of code. The code is currently\n  # Only use _fused_batch_norm if all of the following three\n  # conditions are true:\n  # (1) fused is set True;\n  # (2) it is possible to use (currently it doesn't support batch weights,\n  #   renorm, and the case when rank is neither 2 nor 4);\n  # (3) it is used with zero_debias_moving_mean, or an input shape of rank 2,\n  #   or non-default updates_collections (not implemented in\n  #   normalization_layers.BatchNormalization yet); otherwise use the fused\n  #   implementation in normalization_layers.BatchNormalization.\n  inputs = ops.convert_to_tensor(inputs)\n  rank = inputs.get_shape().ndims\n  possible_to_fuse = batch_weights is None and not renorm and rank in [2, 4]\n  if fused and possible_to_fuse and (\n      zero_debias_moving_mean or rank == 2 or\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\nNow, the comment and the code match. Note the rank can be 4, as long as zero_debias_moving_mean is True or if non-default update collections are used.", "body": "You pasted a slightly older version of the chunk of code. The code is currently\r\n\r\n```python\r\n  # Only use _fused_batch_norm if all of the following three\r\n  # conditions are true:\r\n  # (1) fused is set True;\r\n  # (2) it is possible to use (currently it doesn't support batch weights,\r\n  #   renorm, and the case when rank is neither 2 nor 4);\r\n  # (3) it is used with zero_debias_moving_mean, or an input shape of rank 2,\r\n  #   or non-default updates_collections (not implemented in\r\n  #   normalization_layers.BatchNormalization yet); otherwise use the fused\r\n  #   implementation in normalization_layers.BatchNormalization.\r\n  inputs = ops.convert_to_tensor(inputs)\r\n  rank = inputs.get_shape().ndims\r\n  possible_to_fuse = batch_weights is None and not renorm and rank in [2, 4]\r\n  if fused and possible_to_fuse and (\r\n      zero_debias_moving_mean or rank == 2 or\r\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\r\n```\r\n\r\nNow, the comment and the code match. Note the rank can be 4, as long as `zero_debias_moving_mean` is True or if non-default update collections are used."}