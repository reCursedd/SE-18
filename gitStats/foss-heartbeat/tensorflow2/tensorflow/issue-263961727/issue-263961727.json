{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13589", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13589/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13589/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13589/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13589", "id": 263961727, "node_id": "MDU6SXNzdWUyNjM5NjE3Mjc=", "number": 13589, "title": "tensorflow batch norm used when rank = 4?", "user": {"login": "boulderZ", "id": 23661381, "node_id": "MDQ6VXNlcjIzNjYxMzgx", "avatar_url": "https://avatars1.githubusercontent.com/u/23661381?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boulderZ", "html_url": "https://github.com/boulderZ", "followers_url": "https://api.github.com/users/boulderZ/followers", "following_url": "https://api.github.com/users/boulderZ/following{/other_user}", "gists_url": "https://api.github.com/users/boulderZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/boulderZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boulderZ/subscriptions", "organizations_url": "https://api.github.com/users/boulderZ/orgs", "repos_url": "https://api.github.com/users/boulderZ/repos", "events_url": "https://api.github.com/users/boulderZ/events{/privacy}", "received_events_url": "https://api.github.com/users/boulderZ/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-10-09T16:55:09Z", "updated_at": "2017-10-10T01:45:18Z", "closed_at": "2017-10-10T01:45:18Z", "author_association": "NONE", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug or a feature request.</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No custom code</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: pip install</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8</li>\n<li><strong>GPU model and memory</strong>: Titan X Pascal</li>\n<li><strong>Exact command to reproduce</strong>: See code snippet</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>I am using slim.batch_norm from layers and trying to understand the code flow in my use case. It looks to me like the logic that decides whether to use _fused_batch_norm() or the base class will only use the _fused_batch_norm() in my case if the input rank is 2. The code description sounds like it should also be used if rank is 4 and the function itself (_fused_batch_norm()) supports rank of 4, but the logic seems to prevent calling it.</p>\n<p>If my input is rank 4, it looks like the code will use the fused implementation in normalization_layers.BatchNormalization Is my understanding of the logic correct?</p>\n<p>Is this the expected and proper behavior? I am wondering if the the condition rank==2 should actually be rank in [2,4]? If the latter is correct, then this would be a potential bug. If the original is correct, then why have rank in [2,4] for determining feature_supported ?</p>\n<p>Issue posted to stack overflow and cited as a bug to report: <a href=\"https://stackoverflow.com/questions/44809342/which-tensorflow-batch-norm-code-is-used-when-input-rank-is-4/46620919#46620919\" rel=\"nofollow\">which tensorflow batch norm..</a></p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Only use _fused_batch_norm (1) if fused is set True or if it is</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> possible to use (currently it doesn't support batch weights,</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> renorm, and the case when rank is neither 2 nor 4),</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> and (2) if used with zero_debias_moving_mean, or an input shape of rank 2,</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> or non-default updates_collections (not implemented in</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> normalization_layers.BatchNormalization yet); otherwise use the fused</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> implementation in normalization_layers.BatchNormalization.</span>\n  inputs <span class=\"pl-k\">=</span> ops.convert_to_tensor(inputs)\n  rank <span class=\"pl-k\">=</span> inputs.get_shape().ndims\n  feature_supported <span class=\"pl-k\">=</span> batch_weights <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> renorm <span class=\"pl-k\">and</span> rank <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>]\n  possible_to_fuse <span class=\"pl-k\">=</span> fused <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> feature_supported\n  <span class=\"pl-k\">if</span> (fused <span class=\"pl-k\">or</span> possible_to_fuse) <span class=\"pl-k\">and</span> (\n      zero_debias_moving_mean <span class=\"pl-k\">or</span> rank <span class=\"pl-k\">==</span> <span class=\"pl-c1\">2</span> <span class=\"pl-k\">or</span>\n      updates_collections <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> ops.GraphKeys.<span class=\"pl-c1\">UPDATE_OPS</span>):\n      <span class=\"pl-k\">return</span> _fused_batch_norm(<span class=\"pl-c1\">...</span>)</pre></div>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug or a feature request.\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No custom code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\nTensorFlow installed from (source or binary): pip install\nTensorFlow version (use command below): 1.2\nPython version: 3.5\nBazel version (if compiling from source):\nCUDA/cuDNN version: 8\nGPU model and memory: Titan X Pascal\nExact command to reproduce: See code snippet\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nI am using slim.batch_norm from layers and trying to understand the code flow in my use case. It looks to me like the logic that decides whether to use _fused_batch_norm() or the base class will only use the _fused_batch_norm() in my case if the input rank is 2. The code description sounds like it should also be used if rank is 4 and the function itself (_fused_batch_norm()) supports rank of 4, but the logic seems to prevent calling it.\nIf my input is rank 4, it looks like the code will use the fused implementation in normalization_layers.BatchNormalization Is my understanding of the logic correct?\nIs this the expected and proper behavior? I am wondering if the the condition rank==2 should actually be rank in [2,4]? If the latter is correct, then this would be a potential bug. If the original is correct, then why have rank in [2,4] for determining feature_supported ?\nIssue posted to stack overflow and cited as a bug to report: which tensorflow batch norm..\nSource code / logs\n  # Only use _fused_batch_norm (1) if fused is set True or if it is\n  # possible to use (currently it doesn't support batch weights,\n  # renorm, and the case when rank is neither 2 nor 4),\n  # and (2) if used with zero_debias_moving_mean, or an input shape of rank 2,\n  # or non-default updates_collections (not implemented in\n  # normalization_layers.BatchNormalization yet); otherwise use the fused\n  # implementation in normalization_layers.BatchNormalization.\n  inputs = ops.convert_to_tensor(inputs)\n  rank = inputs.get_shape().ndims\n  feature_supported = batch_weights is None and not renorm and rank in [2, 4]\n  possible_to_fuse = fused is None and feature_supported\n  if (fused or possible_to_fuse) and (\n      zero_debias_moving_mean or rank == 2 or\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\n      return _fused_batch_norm(...)", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug or a feature request.\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: pip install \r\n- **TensorFlow version (use command below)**: 1.2\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8\r\n- **GPU model and memory**: Titan X Pascal\r\n- **Exact command to reproduce**: See code snippet\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\nI am using slim.batch_norm from layers and trying to understand the code flow in my use case. It looks to me like the logic that decides whether to use _fused_batch_norm() or the base class will only use the _fused_batch_norm() in my case if the input rank is 2. The code description sounds like it should also be used if rank is 4 and the function itself (_fused_batch_norm()) supports rank of 4, but the logic seems to prevent calling it. \r\n\r\nIf my input is rank 4, it looks like the code will use the fused implementation in normalization_layers.BatchNormalization Is my understanding of the logic correct?\r\n\r\nIs this the expected and proper behavior? I am wondering if the the condition rank==2 should actually be rank in [2,4]? If the latter is correct, then this would be a potential bug. If the original is correct, then why have rank in [2,4] for determining feature_supported ?\r\n\r\nIssue posted to stack overflow and cited as a bug to report: [which tensorflow batch norm..](https://stackoverflow.com/questions/44809342/which-tensorflow-batch-norm-code-is-used-when-input-rank-is-4/46620919#46620919)\r\n\r\n### Source code / logs\r\n```python\r\n  # Only use _fused_batch_norm (1) if fused is set True or if it is\r\n  # possible to use (currently it doesn't support batch weights,\r\n  # renorm, and the case when rank is neither 2 nor 4),\r\n  # and (2) if used with zero_debias_moving_mean, or an input shape of rank 2,\r\n  # or non-default updates_collections (not implemented in\r\n  # normalization_layers.BatchNormalization yet); otherwise use the fused\r\n  # implementation in normalization_layers.BatchNormalization.\r\n  inputs = ops.convert_to_tensor(inputs)\r\n  rank = inputs.get_shape().ndims\r\n  feature_supported = batch_weights is None and not renorm and rank in [2, 4]\r\n  possible_to_fuse = fused is None and feature_supported\r\n  if (fused or possible_to_fuse) and (\r\n      zero_debias_moving_mean or rank == 2 or\r\n      updates_collections is not ops.GraphKeys.UPDATE_OPS):\r\n      return _fused_batch_norm(...)\r\n```"}