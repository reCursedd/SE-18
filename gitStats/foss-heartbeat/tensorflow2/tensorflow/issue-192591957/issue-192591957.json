{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5982", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5982/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5982/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5982/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5982", "id": 192591957, "node_id": "MDU6SXNzdWUxOTI1OTE5NTc=", "number": 5982, "title": "tf.dynamic_rnn causes rnn state vector to have an undefined batch size", "user": {"login": "al626", "id": 19927736, "node_id": "MDQ6VXNlcjE5OTI3NzM2", "avatar_url": "https://avatars3.githubusercontent.com/u/19927736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/al626", "html_url": "https://github.com/al626", "followers_url": "https://api.github.com/users/al626/followers", "following_url": "https://api.github.com/users/al626/following{/other_user}", "gists_url": "https://api.github.com/users/al626/gists{/gist_id}", "starred_url": "https://api.github.com/users/al626/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/al626/subscriptions", "organizations_url": "https://api.github.com/users/al626/orgs", "repos_url": "https://api.github.com/users/al626/repos", "events_url": "https://api.github.com/users/al626/events{/privacy}", "received_events_url": "https://api.github.com/users/al626/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-11-30T15:16:33Z", "updated_at": "2017-06-16T20:07:55Z", "closed_at": "2017-06-16T17:28:28Z", "author_association": "NONE", "body_html": "<p>Minimal example:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nsess <span class=\"pl-k\">=</span> tf.Session()\ninputs <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">12</span>], tf.float32)\ncell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.BasicLSTMCell(<span class=\"pl-c1\">20</span>)\nmulticell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell([cell] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>)\noutput, state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(cell, inputs, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n<span class=\"pl-c1\">print</span> state, <span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>, output\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> LSTMStateTuple(c=&lt;tf.Tensor 'RNN/while/Exit_2:0' shape=(?, 20) dtype=float32&gt;, h=&lt;tf.Tensor 'RNN/while/Exit_3:0' shape=(?, 20) dtype=float32&gt;) </span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Tensor(\"RNN/transpose:0\", shape=(4, 10, 20), dtype=float32)</span></pre></div>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/5657d0dee8d87f4594b3e5902ed3e3ca8d6dfc0a/tensorflow/python/ops/rnn.py#L809\">This code</a> seems to be the the where the problem is being caused:</p>\n<div class=\"highlight highlight-source-python\"><pre>    input_shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(array_ops.shape(input_) <span class=\"pl-k\">for</span> input_ <span class=\"pl-k\">in</span> flat_input)\n    batch_size <span class=\"pl-k\">=</span> input_shape[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">1</span>]\n\n    <span class=\"pl-k\">for</span> input_ <span class=\"pl-k\">in</span> input_shape:\n      <span class=\"pl-k\">if</span> input_[<span class=\"pl-c1\">1</span>].get_shape() <span class=\"pl-k\">!=</span> batch_size.get_shape():\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>All inputs should have the same batch size<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>From what I understand:</p>\n<ol>\n<li><code>flat_input</code> is a tuple of tensors</li>\n<li>thus <code>input_shape</code> is a tuple of 1D tensors</li>\n<li><code>batch_size</code> gets the 2nd element of the 1st <code>input_shape</code> tensor, which is a 0D tensor</li>\n<li><code>input_[1]</code> is the shape of the 1st element of the 1D <code>input_shape</code> tensor which is a 0D tensor</li>\n<li>=&gt; <code>input_[1].get_shape() == batch_size.get_shape() == TensorShape([]) </code> for all <code>flat_input</code></li>\n</ol>\n<p>Thus regardless of the content of <code>flat_input</code>, the <code>ValueError</code> will never be raised and batch_size will always be a 0D tensor, unknown until it is evaluated, as opposed to a number.</p>\n<p>Rewriting the above code to something that makes more sense to me (below) causes the tests to fail when <code>dynamic_rnn</code> is instantiated with variable batch size.</p>\n<div class=\"highlight highlight-source-python\"><pre>    input_shape <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(input_.get_shape() <span class=\"pl-k\">for</span> input_ <span class=\"pl-k\">in</span> flat_input)\n    batch_size <span class=\"pl-k\">=</span> input_shape[<span class=\"pl-c1\">0</span>][<span class=\"pl-c1\">1</span>]\n \n    <span class=\"pl-k\">for</span> shape <span class=\"pl-k\">in</span> input_shape:\n      <span class=\"pl-k\">if</span> shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">!=</span> batch_size:\n        <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>All inputs should have the same batch size<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>I suppose the question I'm asking is whether it is intended behaviour to throw away the batch size information when it is fixed? If it isn't, I'm happy to write a fix.</p>\n<p>This was brought to my attention by a subtle bug found when implementing a custom LSTM cell:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">VariationalLSTMCell</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">nn</span>.<span class=\"pl-e\">rnn_cell</span>.<span class=\"pl-e\">BasicLSTMCell</span>):\n\t<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">\tLong short-term memory unit (LSTM) recurrent network cell based on:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">\thttp://arxiv.org/pdf/1512.05287v3.pdf</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">\tYarin Gal</span>\n<span class=\"pl-s\">\t\"A theoretically grounded application of dropout in recurrent neural networks\"</span>\n<span class=\"pl-s\">\t<span class=\"pl-pds\">\"\"\"</span></span>\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">keep_prob_w</span>, <span class=\"pl-smi\">keep_prob_u</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-k\">**</span><span class=\"pl-smi\">kwargs</span>):\n\t\t<span class=\"pl-c1\">super</span>(VariationalLSTMCell, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>(num_units, <span class=\"pl-k\">**</span>kwargs)\n\t\t<span class=\"pl-c1\">self</span>._keep_prob_w <span class=\"pl-k\">=</span> keep_prob_w\n\t\t<span class=\"pl-c1\">self</span>._keep_prob_u <span class=\"pl-k\">=</span> keep_prob_u <span class=\"pl-k\">if</span> keep_prob_u <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> keep_prob_w\n\t\t<span class=\"pl-c1\">self</span>._t0_cell <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n\t\t<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Long short-term memory cell (LSTM).<span class=\"pl-pds\">\"\"\"</span></span>\n\t\t<span class=\"pl-k\">with</span> tf.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__name__</span>) <span class=\"pl-k\">as</span> scope:\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> \"VariationalLSTMCell\"</span>\n\t\t\t<span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> scope.reuse:\n\t\t\t\t<span class=\"pl-c1\">self</span>._t0_cell <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n\t\t\t<span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._state_is_tuple:\n\t\t\t\tc, h <span class=\"pl-k\">=</span> state\n\t\t\t<span class=\"pl-k\">else</span>:\n\t\t\t\tc, h <span class=\"pl-k\">=</span> tf.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, state)\n\n\t\t\tdtype <span class=\"pl-k\">=</span> inputs.dtype\n\t\t\tbatch_size, input_size <span class=\"pl-k\">=</span> inputs.get_shape().with_rank(<span class=\"pl-c1\">2</span>)\n\t\t\t<span class=\"pl-k\">if</span> input_size.value <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n\t\t\t\t<span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Could not infer input size from inputs.get_shape()[-1]<span class=\"pl-pds\">\"</span></span>)\n\t\t\t(w_i, w_g, w_f, w_o), (u_i, u_g, u_f, u_o) <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.get_weights(input_size, dtype)\n\t\t\tb_i, b_g, b_f, b_o <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.get_biases(dtype)\n\t\t\t(m_wi, m_wg, m_wf, m_wo), (m_ui, m_ug, m_uf, m_uo) <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.get_dropout_masks(batch_size, input_size)\n\n\t\t\t<span class=\"pl-c\"><span class=\"pl-c\">#</span> i = input_gate, g = new_input, f = forget_gate, o = output_gate</span>\n\t\t\ti <span class=\"pl-k\">=</span> tf.matmul(inputs <span class=\"pl-k\">*</span> m_wi, w_i) <span class=\"pl-k\">+</span> tf.matmul(h <span class=\"pl-k\">*</span> m_ui, u_i) <span class=\"pl-k\">+</span> b_i\n\t\t\tg <span class=\"pl-k\">=</span> tf.matmul(inputs <span class=\"pl-k\">*</span> m_wg, w_g) <span class=\"pl-k\">+</span> tf.matmul(h <span class=\"pl-k\">*</span> m_ug, u_g) <span class=\"pl-k\">+</span> b_g\n\t\t\tf <span class=\"pl-k\">=</span> tf.matmul(inputs <span class=\"pl-k\">*</span> m_wf, w_f) <span class=\"pl-k\">+</span> tf.matmul(h <span class=\"pl-k\">*</span> m_uf, u_f) <span class=\"pl-k\">+</span> b_f\n\t\t\to <span class=\"pl-k\">=</span> tf.matmul(inputs <span class=\"pl-k\">*</span> m_wo, w_o) <span class=\"pl-k\">+</span> tf.matmul(h <span class=\"pl-k\">*</span> m_uo, u_o) <span class=\"pl-k\">+</span> b_o\n\n\t\t\tnew_c <span class=\"pl-k\">=</span> (c <span class=\"pl-k\">*</span> tf.sigmoid(f <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>._forget_bias) <span class=\"pl-k\">+</span> tf.sigmoid(i) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>._activation(g))\n\t\t\tnew_h <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._activation(new_c) <span class=\"pl-k\">*</span> tf.sigmoid(o)\n\n\t\t\t<span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._state_is_tuple:\n\t\t\t\tnew_state <span class=\"pl-k\">=</span> tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n\t\t\t<span class=\"pl-k\">else</span>:\n\t\t\t\tnew_state <span class=\"pl-k\">=</span> tf.concat(<span class=\"pl-c1\">1</span>, [new_c, new_h])\n\t\t<span class=\"pl-k\">return</span> new_h, new_state\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_dropout_masks</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">input_size</span>):\n\t\t<span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>dropout<span class=\"pl-pds\">'</span></span>):\n\t\t\tw_mask <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>w_mask<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">4</span>, batch_size.value, input_size.value], tf.float32, tf.ones_initializer, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\t\t\tu_mask <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>u_mask<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">4</span>, batch_size.value, <span class=\"pl-c1\">self</span>._num_units], tf.float32, tf.ones_initializer, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\t\t\t<span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._t0_cell:\n\t\t\t\tw_mask <span class=\"pl-k\">=</span> w_mask.assign(tf.random_uniform([<span class=\"pl-c1\">4</span>, batch_size.value, input_size.value], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32))\n\t\t\t\tu_mask <span class=\"pl-k\">=</span> u_mask.assign(tf.random_uniform([<span class=\"pl-c1\">4</span>, batch_size.value, <span class=\"pl-c1\">self</span>._num_units], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32))\n\t\t\tw_mask <span class=\"pl-k\">=</span> tf.maximum(tf.sign(<span class=\"pl-c1\">self</span>._keep_prob_w <span class=\"pl-k\">-</span> w_mask), <span class=\"pl-c1\">0</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">self</span>._keep_prob_w\n\t\t\tu_mask <span class=\"pl-k\">=</span> tf.maximum(tf.sign(<span class=\"pl-c1\">self</span>._keep_prob_u <span class=\"pl-k\">-</span> u_mask), <span class=\"pl-c1\">0</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">self</span>._keep_prob_u\n\t\t\tw_masks, u_masks <span class=\"pl-k\">=</span> [tf.unpack(mask) <span class=\"pl-k\">for</span> mask <span class=\"pl-k\">in</span> [w_mask, u_mask]]\n\t\t\t<span class=\"pl-k\">return</span> w_masks, u_masks\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_weights</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_size</span>, <span class=\"pl-smi\">dtype</span>):\n\t\tw_weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>w_weights<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">4</span>, input_size.value, <span class=\"pl-c1\">self</span>._num_units], dtype)\n\t\tu_weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>u_weights<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-c1\">self</span>._num_units], dtype)\n\t\ttf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights<span class=\"pl-pds\">'</span></span>, w_weights)\n\t\ttf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights<span class=\"pl-pds\">'</span></span>, u_weights)\n\t\tw_weights, u_weights <span class=\"pl-k\">=</span> [tf.unpack(weights) <span class=\"pl-k\">for</span> weights <span class=\"pl-k\">in</span> [w_weights, u_weights]]\n\t\t<span class=\"pl-k\">return</span> w_weights, u_weights\n\n\t<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_biases</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">dtype</span>):\n\t\tbiases <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>biases<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">self</span>._num_units], dtype, tf.zeros_initializer)\n\t\ttf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>biases<span class=\"pl-pds\">'</span></span>, biases)\n\t\t<span class=\"pl-k\">return</span> tf.unpack(biases)</pre></div>\n<p>When <code>batch_size &gt; 1</code> this works fine, but when <code>batch_size == 1</code> due to the potential to be broadcast, the dimensions of <code>new_h</code> and <code>new_c</code> cannot be inferred, leading to crashes in higher layers in <code>get_dropout_masks</code>. This works fine however with the change to <code>dynamic_rnn</code> I mentioned above.</p>\n<p>I'm mainly working in <code>r0.11</code>, but I have also observed the issue in master (<code>5657d0d</code>) and <code>0.12.0-rc0</code></p>\n<p>Please let me know if any more information would be helpful.</p>", "body_text": "Minimal example:\nimport tensorflow as tf\n\nsess = tf.Session()\ninputs = tf.zeros([4, 10, 12], tf.float32)\ncell = tf.nn.rnn_cell.BasicLSTMCell(20)\nmulticell = tf.nn.rnn_cell.MultiRNNCell([cell] * 2)\noutput, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\nprint state, '\\n', output\n# LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_3:0' shape=(?, 20) dtype=float32>) \n# Tensor(\"RNN/transpose:0\", shape=(4, 10, 20), dtype=float32)\nThis code seems to be the the where the problem is being caused:\n    input_shape = tuple(array_ops.shape(input_) for input_ in flat_input)\n    batch_size = input_shape[0][1]\n\n    for input_ in input_shape:\n      if input_[1].get_shape() != batch_size.get_shape():\n        raise ValueError(\"All inputs should have the same batch size\")\nFrom what I understand:\n\nflat_input is a tuple of tensors\nthus input_shape is a tuple of 1D tensors\nbatch_size gets the 2nd element of the 1st input_shape tensor, which is a 0D tensor\ninput_[1] is the shape of the 1st element of the 1D input_shape tensor which is a 0D tensor\n=> input_[1].get_shape() == batch_size.get_shape() == TensorShape([])  for all flat_input\n\nThus regardless of the content of flat_input, the ValueError will never be raised and batch_size will always be a 0D tensor, unknown until it is evaluated, as opposed to a number.\nRewriting the above code to something that makes more sense to me (below) causes the tests to fail when dynamic_rnn is instantiated with variable batch size.\n    input_shape = tuple(input_.get_shape() for input_ in flat_input)\n    batch_size = input_shape[0][1]\n \n    for shape in input_shape:\n      if shape[1] != batch_size:\n        raise ValueError(\"All inputs should have the same batch size\")\nI suppose the question I'm asking is whether it is intended behaviour to throw away the batch size information when it is fixed? If it isn't, I'm happy to write a fix.\nThis was brought to my attention by a subtle bug found when implementing a custom LSTM cell:\nclass VariationalLSTMCell(tf.nn.rnn_cell.BasicLSTMCell):\n\t\"\"\"\n\tLong short-term memory unit (LSTM) recurrent network cell based on:\n\n\thttp://arxiv.org/pdf/1512.05287v3.pdf\n\n\tYarin Gal\n\t\"A theoretically grounded application of dropout in recurrent neural networks\"\n\t\"\"\"\n\tdef __init__(self, num_units, keep_prob_w, keep_prob_u=None, **kwargs):\n\t\tsuper(VariationalLSTMCell, self).__init__(num_units, **kwargs)\n\t\tself._keep_prob_w = keep_prob_w\n\t\tself._keep_prob_u = keep_prob_u if keep_prob_u is not None else keep_prob_w\n\t\tself._t0_cell = False\n\n\tdef __call__(self, inputs, state, scope=None):\n\t\t\"\"\"Long short-term memory cell (LSTM).\"\"\"\n\t\twith tf.variable_scope(scope or type(self).__name__) as scope:\t# \"VariationalLSTMCell\"\n\t\t\tif not scope.reuse:\n\t\t\t\tself._t0_cell = True\n\n\t\t\tif self._state_is_tuple:\n\t\t\t\tc, h = state\n\t\t\telse:\n\t\t\t\tc, h = tf.split(1, 2, state)\n\n\t\t\tdtype = inputs.dtype\n\t\t\tbatch_size, input_size = inputs.get_shape().with_rank(2)\n\t\t\tif input_size.value is None:\n\t\t\t\traise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\n\t\t\t(w_i, w_g, w_f, w_o), (u_i, u_g, u_f, u_o) = self.get_weights(input_size, dtype)\n\t\t\tb_i, b_g, b_f, b_o = self.get_biases(dtype)\n\t\t\t(m_wi, m_wg, m_wf, m_wo), (m_ui, m_ug, m_uf, m_uo) = self.get_dropout_masks(batch_size, input_size)\n\n\t\t\t# i = input_gate, g = new_input, f = forget_gate, o = output_gate\n\t\t\ti = tf.matmul(inputs * m_wi, w_i) + tf.matmul(h * m_ui, u_i) + b_i\n\t\t\tg = tf.matmul(inputs * m_wg, w_g) + tf.matmul(h * m_ug, u_g) + b_g\n\t\t\tf = tf.matmul(inputs * m_wf, w_f) + tf.matmul(h * m_uf, u_f) + b_f\n\t\t\to = tf.matmul(inputs * m_wo, w_o) + tf.matmul(h * m_uo, u_o) + b_o\n\n\t\t\tnew_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) * self._activation(g))\n\t\t\tnew_h = self._activation(new_c) * tf.sigmoid(o)\n\n\t\t\tif self._state_is_tuple:\n\t\t\t\tnew_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\n\t\t\telse:\n\t\t\t\tnew_state = tf.concat(1, [new_c, new_h])\n\t\treturn new_h, new_state\n\n\tdef get_dropout_masks(self, batch_size, input_size):\n\t\twith tf.variable_scope('dropout'):\n\t\t\tw_mask = tf.get_variable('w_mask', [4, batch_size.value, input_size.value], tf.float32, tf.ones_initializer, trainable=False)\n\t\t\tu_mask = tf.get_variable('u_mask', [4, batch_size.value, self._num_units], tf.float32, tf.ones_initializer, trainable=False)\n\t\t\tif self._t0_cell:\n\t\t\t\tw_mask = w_mask.assign(tf.random_uniform([4, batch_size.value, input_size.value], dtype=tf.float32))\n\t\t\t\tu_mask = u_mask.assign(tf.random_uniform([4, batch_size.value, self._num_units], dtype=tf.float32))\n\t\t\tw_mask = tf.maximum(tf.sign(self._keep_prob_w - w_mask), 0) / self._keep_prob_w\n\t\t\tu_mask = tf.maximum(tf.sign(self._keep_prob_u - u_mask), 0) / self._keep_prob_u\n\t\t\tw_masks, u_masks = [tf.unpack(mask) for mask in [w_mask, u_mask]]\n\t\t\treturn w_masks, u_masks\n\n\tdef get_weights(self, input_size, dtype):\n\t\tw_weights = tf.get_variable('w_weights', [4, input_size.value, self._num_units], dtype)\n\t\tu_weights = tf.get_variable('u_weights', [4, self._num_units, self._num_units], dtype)\n\t\ttf.add_to_collection('weights', w_weights)\n\t\ttf.add_to_collection('weights', u_weights)\n\t\tw_weights, u_weights = [tf.unpack(weights) for weights in [w_weights, u_weights]]\n\t\treturn w_weights, u_weights\n\n\tdef get_biases(self, dtype):\n\t\tbiases = tf.get_variable('biases', [4, self._num_units], dtype, tf.zeros_initializer)\n\t\ttf.add_to_collection('biases', biases)\n\t\treturn tf.unpack(biases)\nWhen batch_size > 1 this works fine, but when batch_size == 1 due to the potential to be broadcast, the dimensions of new_h and new_c cannot be inferred, leading to crashes in higher layers in get_dropout_masks. This works fine however with the change to dynamic_rnn I mentioned above.\nI'm mainly working in r0.11, but I have also observed the issue in master (5657d0d) and 0.12.0-rc0\nPlease let me know if any more information would be helpful.", "body": "Minimal example:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nsess = tf.Session()\r\ninputs = tf.zeros([4, 10, 12], tf.float32)\r\ncell = tf.nn.rnn_cell.BasicLSTMCell(20)\r\nmulticell = tf.nn.rnn_cell.MultiRNNCell([cell] * 2)\r\noutput, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\r\nprint state, '\\n', output\r\n# LSTMStateTuple(c=<tf.Tensor 'RNN/while/Exit_2:0' shape=(?, 20) dtype=float32>, h=<tf.Tensor 'RNN/while/Exit_3:0' shape=(?, 20) dtype=float32>) \r\n# Tensor(\"RNN/transpose:0\", shape=(4, 10, 20), dtype=float32)\r\n```\r\n\r\n[This code](https://github.com/tensorflow/tensorflow/blob/5657d0dee8d87f4594b3e5902ed3e3ca8d6dfc0a/tensorflow/python/ops/rnn.py#L809) seems to be the the where the problem is being caused:\r\n```python\r\n    input_shape = tuple(array_ops.shape(input_) for input_ in flat_input)\r\n    batch_size = input_shape[0][1]\r\n\r\n    for input_ in input_shape:\r\n      if input_[1].get_shape() != batch_size.get_shape():\r\n        raise ValueError(\"All inputs should have the same batch size\")\r\n```\r\nFrom what I understand:\r\n1. `flat_input` is a tuple of tensors\r\n2. thus `input_shape` is a tuple of 1D tensors\r\n3. `batch_size` gets the 2nd element of the 1st `input_shape` tensor, which is a 0D tensor\r\n4. `input_[1]` is the shape of the 1st element of the 1D `input_shape` tensor which is a 0D tensor\r\n5. => `input_[1].get_shape() == batch_size.get_shape() == TensorShape([]) ` for all `flat_input`\r\n\r\nThus regardless of the content of `flat_input`, the `ValueError` will never be raised and batch_size will always be a 0D tensor, unknown until it is evaluated, as opposed to a number.\r\n\r\nRewriting the above code to something that makes more sense to me (below) causes the tests to fail when `dynamic_rnn` is instantiated with variable batch size.\r\n\r\n```python\r\n    input_shape = tuple(input_.get_shape() for input_ in flat_input)\r\n    batch_size = input_shape[0][1]\r\n \r\n    for shape in input_shape:\r\n      if shape[1] != batch_size:\r\n        raise ValueError(\"All inputs should have the same batch size\")\r\n```\r\n\r\nI suppose the question I'm asking is whether it is intended behaviour to throw away the batch size information when it is fixed? If it isn't, I'm happy to write a fix.\r\n\r\nThis was brought to my attention by a subtle bug found when implementing a custom LSTM cell:\r\n\r\n```python\r\nclass VariationalLSTMCell(tf.nn.rnn_cell.BasicLSTMCell):\r\n\t\"\"\"\r\n\tLong short-term memory unit (LSTM) recurrent network cell based on:\r\n\r\n\thttp://arxiv.org/pdf/1512.05287v3.pdf\r\n\r\n\tYarin Gal\r\n\t\"A theoretically grounded application of dropout in recurrent neural networks\"\r\n\t\"\"\"\r\n\tdef __init__(self, num_units, keep_prob_w, keep_prob_u=None, **kwargs):\r\n\t\tsuper(VariationalLSTMCell, self).__init__(num_units, **kwargs)\r\n\t\tself._keep_prob_w = keep_prob_w\r\n\t\tself._keep_prob_u = keep_prob_u if keep_prob_u is not None else keep_prob_w\r\n\t\tself._t0_cell = False\r\n\r\n\tdef __call__(self, inputs, state, scope=None):\r\n\t\t\"\"\"Long short-term memory cell (LSTM).\"\"\"\r\n\t\twith tf.variable_scope(scope or type(self).__name__) as scope:\t# \"VariationalLSTMCell\"\r\n\t\t\tif not scope.reuse:\r\n\t\t\t\tself._t0_cell = True\r\n\r\n\t\t\tif self._state_is_tuple:\r\n\t\t\t\tc, h = state\r\n\t\t\telse:\r\n\t\t\t\tc, h = tf.split(1, 2, state)\r\n\r\n\t\t\tdtype = inputs.dtype\r\n\t\t\tbatch_size, input_size = inputs.get_shape().with_rank(2)\r\n\t\t\tif input_size.value is None:\r\n\t\t\t\traise ValueError(\"Could not infer input size from inputs.get_shape()[-1]\")\r\n\t\t\t(w_i, w_g, w_f, w_o), (u_i, u_g, u_f, u_o) = self.get_weights(input_size, dtype)\r\n\t\t\tb_i, b_g, b_f, b_o = self.get_biases(dtype)\r\n\t\t\t(m_wi, m_wg, m_wf, m_wo), (m_ui, m_ug, m_uf, m_uo) = self.get_dropout_masks(batch_size, input_size)\r\n\r\n\t\t\t# i = input_gate, g = new_input, f = forget_gate, o = output_gate\r\n\t\t\ti = tf.matmul(inputs * m_wi, w_i) + tf.matmul(h * m_ui, u_i) + b_i\r\n\t\t\tg = tf.matmul(inputs * m_wg, w_g) + tf.matmul(h * m_ug, u_g) + b_g\r\n\t\t\tf = tf.matmul(inputs * m_wf, w_f) + tf.matmul(h * m_uf, u_f) + b_f\r\n\t\t\to = tf.matmul(inputs * m_wo, w_o) + tf.matmul(h * m_uo, u_o) + b_o\r\n\r\n\t\t\tnew_c = (c * tf.sigmoid(f + self._forget_bias) + tf.sigmoid(i) * self._activation(g))\r\n\t\t\tnew_h = self._activation(new_c) * tf.sigmoid(o)\r\n\r\n\t\t\tif self._state_is_tuple:\r\n\t\t\t\tnew_state = tf.nn.rnn_cell.LSTMStateTuple(new_c, new_h)\r\n\t\t\telse:\r\n\t\t\t\tnew_state = tf.concat(1, [new_c, new_h])\r\n\t\treturn new_h, new_state\r\n\r\n\tdef get_dropout_masks(self, batch_size, input_size):\r\n\t\twith tf.variable_scope('dropout'):\r\n\t\t\tw_mask = tf.get_variable('w_mask', [4, batch_size.value, input_size.value], tf.float32, tf.ones_initializer, trainable=False)\r\n\t\t\tu_mask = tf.get_variable('u_mask', [4, batch_size.value, self._num_units], tf.float32, tf.ones_initializer, trainable=False)\r\n\t\t\tif self._t0_cell:\r\n\t\t\t\tw_mask = w_mask.assign(tf.random_uniform([4, batch_size.value, input_size.value], dtype=tf.float32))\r\n\t\t\t\tu_mask = u_mask.assign(tf.random_uniform([4, batch_size.value, self._num_units], dtype=tf.float32))\r\n\t\t\tw_mask = tf.maximum(tf.sign(self._keep_prob_w - w_mask), 0) / self._keep_prob_w\r\n\t\t\tu_mask = tf.maximum(tf.sign(self._keep_prob_u - u_mask), 0) / self._keep_prob_u\r\n\t\t\tw_masks, u_masks = [tf.unpack(mask) for mask in [w_mask, u_mask]]\r\n\t\t\treturn w_masks, u_masks\r\n\r\n\tdef get_weights(self, input_size, dtype):\r\n\t\tw_weights = tf.get_variable('w_weights', [4, input_size.value, self._num_units], dtype)\r\n\t\tu_weights = tf.get_variable('u_weights', [4, self._num_units, self._num_units], dtype)\r\n\t\ttf.add_to_collection('weights', w_weights)\r\n\t\ttf.add_to_collection('weights', u_weights)\r\n\t\tw_weights, u_weights = [tf.unpack(weights) for weights in [w_weights, u_weights]]\r\n\t\treturn w_weights, u_weights\r\n\r\n\tdef get_biases(self, dtype):\r\n\t\tbiases = tf.get_variable('biases', [4, self._num_units], dtype, tf.zeros_initializer)\r\n\t\ttf.add_to_collection('biases', biases)\r\n\t\treturn tf.unpack(biases)\r\n```\r\n\r\nWhen `batch_size > 1` this works fine, but when `batch_size == 1` due to the potential to be broadcast, the dimensions of `new_h` and `new_c` cannot be inferred, leading to crashes in higher layers in `get_dropout_masks`. This works fine however with the change to `dynamic_rnn` I mentioned above.\r\n\r\nI'm mainly working in `r0.11`, but I have also observed the issue in master (`5657d0d`) and `0.12.0-rc0`\r\n\r\nPlease let me know if any more information would be helpful."}