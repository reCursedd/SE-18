{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361427511", "html_url": "https://github.com/tensorflow/tensorflow/issues/11182#issuecomment-361427511", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11182", "id": 361427511, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTQyNzUxMQ==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-29T23:51:25Z", "updated_at": "2018-01-29T23:51:25Z", "author_association": "MEMBER", "body_html": "<p>We're now moving over to TF Lite as the recommended way to run quantized models, partly because it is less complex to integrate eight-bit with than classic TensorFlow. We have a demo of running the MobileNet image classification network on Android, and there should soon be full documentation on training it. I'm going to close this bug as not likely to be fixed, since our efforts will be focused on the TF Lite path.</p>", "body_text": "We're now moving over to TF Lite as the recommended way to run quantized models, partly because it is less complex to integrate eight-bit with than classic TensorFlow. We have a demo of running the MobileNet image classification network on Android, and there should soon be full documentation on training it. I'm going to close this bug as not likely to be fixed, since our efforts will be focused on the TF Lite path.", "body": "We're now moving over to TF Lite as the recommended way to run quantized models, partly because it is less complex to integrate eight-bit with than classic TensorFlow. We have a demo of running the MobileNet image classification network on Android, and there should soon be full documentation on training it. I'm going to close this bug as not likely to be fixed, since our efforts will be focused on the TF Lite path."}