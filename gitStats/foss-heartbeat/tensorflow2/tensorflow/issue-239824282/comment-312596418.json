{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312596418", "html_url": "https://github.com/tensorflow/tensorflow/issues/11182#issuecomment-312596418", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11182", "id": 312596418, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjU5NjQxOA==", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-03T09:28:22Z", "updated_at": "2017-07-03T09:28:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I might have just solved this problem by looking at what <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"218709947\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8897\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8897/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8897\">#8897</a> did. Basically, I had to include the argument <code>--copt=\"-DTENSORFLOW_DISABLE_META\" </code> when building the <code>libtensorflow_inference.so</code> file. This reduced the library size for android and made the performance almost exactly as expected when run on a desktop.</p>\n<p>Here is the full command to build libtensorflow_inference.so:</p>\n<pre><code>bazel build -c opt --copt=\"-DTENSORFLOW_DISABLE_META\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\n</code></pre>\n<p>and then the JAR file subsequently:</p>\n<pre><code>bazel build //tensorflow/contrib/android:android_tensorflow_inference_java\n</code></pre>\n<p>Further, I think there are two problematic transformations: Remove Nodes and Quantize Nodes. Using either one causes the accuracy to be reduced or inference timing to increase by around 5x.<br>\nThe remaining 8 transformations are what I found as optimal:</p>\n<pre><code>/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \\\n--out_graph=./quantized_inception_resnet_v2_for_mobile.pb \\\n--inputs='Placeholder_only' \\\n--outputs='InceptionResnetV2/Logits/Predictions' \\\n--transforms='\n  add_default_attributes\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms\n  quantize_weights\n  strip_unused_nodes\n  sort_by_execution_order'\n</code></pre>", "body_text": "I might have just solved this problem by looking at what #8897 did. Basically, I had to include the argument --copt=\"-DTENSORFLOW_DISABLE_META\"  when building the libtensorflow_inference.so file. This reduced the library size for android and made the performance almost exactly as expected when run on a desktop.\nHere is the full command to build libtensorflow_inference.so:\nbazel build -c opt --copt=\"-DTENSORFLOW_DISABLE_META\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\n\nand then the JAR file subsequently:\nbazel build //tensorflow/contrib/android:android_tensorflow_inference_java\n\nFurther, I think there are two problematic transformations: Remove Nodes and Quantize Nodes. Using either one causes the accuracy to be reduced or inference timing to increase by around 5x.\nThe remaining 8 transformations are what I found as optimal:\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \\\n--out_graph=./quantized_inception_resnet_v2_for_mobile.pb \\\n--inputs='Placeholder_only' \\\n--outputs='InceptionResnetV2/Logits/Predictions' \\\n--transforms='\n  add_default_attributes\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms\n  quantize_weights\n  strip_unused_nodes\n  sort_by_execution_order'", "body": "I might have just solved this problem by looking at what #8897 did. Basically, I had to include the argument `--copt=\"-DTENSORFLOW_DISABLE_META\" ` when building the `libtensorflow_inference.so` file. This reduced the library size for android and made the performance almost exactly as expected when run on a desktop.\r\n\r\nHere is the full command to build libtensorflow_inference.so:\r\n```\r\nbazel build -c opt --copt=\"-DTENSORFLOW_DISABLE_META\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n```\r\n\r\nand then the JAR file subsequently:\r\n\r\n```\r\nbazel build //tensorflow/contrib/android:android_tensorflow_inference_java\r\n```\r\n\r\nFurther, I think there are two problematic transformations: Remove Nodes and Quantize Nodes. Using either one causes the accuracy to be reduced or inference timing to increase by around 5x.\r\nThe remaining 8 transformations are what I found as optimal:\r\n\r\n```\r\n/home/kwotsin/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_inception_resnet_v2_for_mobile.pb \\\r\n--out_graph=./quantized_inception_resnet_v2_for_mobile.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='InceptionResnetV2/Logits/Predictions' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```"}