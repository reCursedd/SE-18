{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237238827", "html_url": "https://github.com/tensorflow/tensorflow/issues/3612#issuecomment-237238827", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3612", "id": 237238827, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzIzODgyNw==", "user": {"login": "serhannn", "id": 4326033, "node_id": "MDQ6VXNlcjQzMjYwMzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/4326033?v=4", "gravatar_id": "", "url": "https://api.github.com/users/serhannn", "html_url": "https://github.com/serhannn", "followers_url": "https://api.github.com/users/serhannn/followers", "following_url": "https://api.github.com/users/serhannn/following{/other_user}", "gists_url": "https://api.github.com/users/serhannn/gists{/gist_id}", "starred_url": "https://api.github.com/users/serhannn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/serhannn/subscriptions", "organizations_url": "https://api.github.com/users/serhannn/orgs", "repos_url": "https://api.github.com/users/serhannn/repos", "events_url": "https://api.github.com/users/serhannn/events{/privacy}", "received_events_url": "https://api.github.com/users/serhannn/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-03T13:36:36Z", "updated_at": "2016-08-03T13:36:36Z", "author_association": "NONE", "body_html": "<p>Ok, by reading through the discussion in <a href=\"https://github.com/tensorflow/tensorflow/issues/152\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/152/hovercard\">152</a>, I figured out that Tensorflow tries to grab all the GPUs it sees from the system. The SegFault probably occurs when there is not enough memory on one or more of the GPUs since Tensorflow tries to grab it anyway.</p>\n<p>The workaround in <a href=\"https://github.com/tensorflow/tensorflow/issues/152\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/152/hovercard\">152</a> is to check nvidia-smi to see which GPUs are free and set CUDA_VISIBLE_DEVICES to these GPUs. When I do this, I do not get any SegFaults error anymore and everthing seems to work fine. I just want to know if there is any way to automate this issue by polling the GPUs beforehand to determine the available ones, or a try-catch kind of script which prevents Tensorflow from segfaulting.</p>", "body_text": "Ok, by reading through the discussion in 152, I figured out that Tensorflow tries to grab all the GPUs it sees from the system. The SegFault probably occurs when there is not enough memory on one or more of the GPUs since Tensorflow tries to grab it anyway.\nThe workaround in 152 is to check nvidia-smi to see which GPUs are free and set CUDA_VISIBLE_DEVICES to these GPUs. When I do this, I do not get any SegFaults error anymore and everthing seems to work fine. I just want to know if there is any way to automate this issue by polling the GPUs beforehand to determine the available ones, or a try-catch kind of script which prevents Tensorflow from segfaulting.", "body": "Ok, by reading through the discussion in [152](https://github.com/tensorflow/tensorflow/issues/152), I figured out that Tensorflow tries to grab all the GPUs it sees from the system. The SegFault probably occurs when there is not enough memory on one or more of the GPUs since Tensorflow tries to grab it anyway.\n\nThe workaround in [152](https://github.com/tensorflow/tensorflow/issues/152) is to check nvidia-smi to see which GPUs are free and set CUDA_VISIBLE_DEVICES to these GPUs. When I do this, I do not get any SegFaults error anymore and everthing seems to work fine. I just want to know if there is any way to automate this issue by polling the GPUs beforehand to determine the available ones, or a try-catch kind of script which prevents Tensorflow from segfaulting.\n"}