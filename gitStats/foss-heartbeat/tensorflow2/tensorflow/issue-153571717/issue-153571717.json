{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2260", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2260/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2260/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2260/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2260", "id": 153571717, "node_id": "MDU6SXNzdWUxNTM1NzE3MTc=", "number": 2260, "title": "tensorflow can't support float64 type when reading from csv data?", "user": {"login": "liumilan", "id": 5533901, "node_id": "MDQ6VXNlcjU1MzM5MDE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5533901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liumilan", "html_url": "https://github.com/liumilan", "followers_url": "https://api.github.com/users/liumilan/followers", "following_url": "https://api.github.com/users/liumilan/following{/other_user}", "gists_url": "https://api.github.com/users/liumilan/gists{/gist_id}", "starred_url": "https://api.github.com/users/liumilan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liumilan/subscriptions", "organizations_url": "https://api.github.com/users/liumilan/orgs", "repos_url": "https://api.github.com/users/liumilan/repos", "events_url": "https://api.github.com/users/liumilan/events{/privacy}", "received_events_url": "https://api.github.com/users/liumilan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-05-07T03:56:44Z", "updated_at": "2016-05-18T23:34:53Z", "closed_at": "2016-05-18T23:34:53Z", "author_association": "NONE", "body_html": "<p>I just follow the  examples,and tried to read csv data with float types</p>\n<p>filename_queue = tf.train.string_input_producer([\"test1.csv\"])</p>\n<p>reader = tf.TextLineReader()<br>\nkey, value = reader.read(filename_queue)</p>\n<p>record_defaults = [[1], [1.0], [1.0], [1.0], [1.0]]<br>\ncol1,col2,col3,col4, col5 = tf.decode_csv(<br>\nvalue, record_defaults=record_defaults)<br>\nfeatures = tf.pack([col1, col2, col3, col4])</p>\n<p>with tf.Session() as sess:<br>\n#print sess.run(value)</p>\n<h1>Start populating the filename queue.</h1>\n<p>coord = tf.train.Coordinator()<br>\nthreads = tf.train.start_queue_runners(coord=coord)<br>\nprint key,value<br>\nfor i in range(1200):<br>\n# Retrieve a single instance:<br>\nexample, label = sess.run([features, col5])</p>\n<p>coord.request_stop()<br>\ncoord.join(threads)<br>\n~<br>\nBut it need to read data into memory first.Because the csv file it too big to read into memory.Can it possbile read with shuffle and minibatch and then training?</p>", "body_text": "I just follow the  examples,and tried to read csv data with float types\nfilename_queue = tf.train.string_input_producer([\"test1.csv\"])\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\nrecord_defaults = [[1], [1.0], [1.0], [1.0], [1.0]]\ncol1,col2,col3,col4, col5 = tf.decode_csv(\nvalue, record_defaults=record_defaults)\nfeatures = tf.pack([col1, col2, col3, col4])\nwith tf.Session() as sess:\n#print sess.run(value)\nStart populating the filename queue.\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(coord=coord)\nprint key,value\nfor i in range(1200):\n# Retrieve a single instance:\nexample, label = sess.run([features, col5])\ncoord.request_stop()\ncoord.join(threads)\n~\nBut it need to read data into memory first.Because the csv file it too big to read into memory.Can it possbile read with shuffle and minibatch and then training?", "body": "I just follow the  examples,and tried to read csv data with float types\n\nfilename_queue = tf.train.string_input_producer([\"test1.csv\"])\n\nreader = tf.TextLineReader()\nkey, value = reader.read(filename_queue)\n\nrecord_defaults = [[1], [1.0], [1.0], [1.0], [1.0]]\ncol1,col2,col3,col4, col5 = tf.decode_csv(\n    value, record_defaults=record_defaults)\nfeatures = tf.pack([col1, col2, col3, col4])\n\nwith tf.Session() as sess:\n  #print sess.run(value)\n  # Start populating the filename queue.\n  coord = tf.train.Coordinator()\n  threads = tf.train.start_queue_runners(coord=coord)\n  print key,value\n  for i in range(1200):\n    # Retrieve a single instance:\n    example, label = sess.run([features, col5])\n\n  coord.request_stop()\n  coord.join(threads)\n~ \nBut it need to read data into memory first.Because the csv file it too big to read into memory.Can it possbile read with shuffle and minibatch and then training?\n"}