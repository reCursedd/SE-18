{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23523", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23523/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23523/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23523/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23523", "id": 377384858, "node_id": "MDU6SXNzdWUzNzczODQ4NTg=", "number": 23523, "title": "Using tf.data.dataset for inference with high efficiency", "user": {"login": "wenmengzhou", "id": 4771825, "node_id": "MDQ6VXNlcjQ3NzE4MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4771825?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wenmengzhou", "html_url": "https://github.com/wenmengzhou", "followers_url": "https://api.github.com/users/wenmengzhou/followers", "following_url": "https://api.github.com/users/wenmengzhou/following{/other_user}", "gists_url": "https://api.github.com/users/wenmengzhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/wenmengzhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wenmengzhou/subscriptions", "organizations_url": "https://api.github.com/users/wenmengzhou/orgs", "repos_url": "https://api.github.com/users/wenmengzhou/repos", "events_url": "https://api.github.com/users/wenmengzhou/events{/privacy}", "received_events_url": "https://api.github.com/users/wenmengzhou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1114343535, "node_id": "MDU6TGFiZWwxMTE0MzQzNTM1", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:data", "name": "comp:data", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-11-05T12:46:14Z", "updated_at": "2018-11-15T15:24:02Z", "closed_at": "2018-11-15T15:24:01Z", "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a documentation issue. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version: master</li>\n<li>Doc Link:  <a href=\"https://www.tensorflow.org/guide/datasets\" rel=\"nofollow\">https://www.tensorflow.org/guide/datasets</a></li>\n</ul>\n<p>tf.data.dataset provide a high level api for data importing and preprocessing. It's convenient to be used in training process, however, it is unclear how to use them in the inference scenario.  We want to reuse the data preprocessing code  for inference which using the interface of tf.data.dataset. By google search and self experiment I found the following ways to use tf.data.dataset for inference:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>using a placeholder</span>\nx <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">2</span>])\ndataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(x)\ndata <span class=\"pl-k\">=</span> np.random.sample((<span class=\"pl-c1\">100</span>,<span class=\"pl-c1\">2</span>))\n<span class=\"pl-c1\">iter</span> <span class=\"pl-k\">=</span> dataset.make_initializable_iterator() <span class=\"pl-c\"><span class=\"pl-c\">#</span> create the iterator</span>\nel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">iter</span>.get_next()\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> feed the placeholder with data</span>\n    sess.run(<span class=\"pl-c1\">iter</span>.initializer, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{ x: data }) \n    <span class=\"pl-c1\">print</span>(sess.run(el)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> output [ 0.52374458  0.71968478]</span></pre></div>\n<p>But the way above will bring some cost, Specifically ,there are two main operations:</p>\n<ol>\n<li>we must run the function sess.run  one more time</li>\n<li>dataset object need be to initialize each time</li>\n</ol>\n<p>is it efficient to do in this way,  Or we need another way to use tf.data.dataset so that we can keep efficiency?</p>\n<hr>\n<p>Update:<br>\nusing tf.contrib.data.get_single_element  or tf.data.experimental.get_single_element</p>", "body_text": "Please make sure that this is a documentation issue. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template\nSystem information\n\nTensorFlow version: master\nDoc Link:  https://www.tensorflow.org/guide/datasets\n\ntf.data.dataset provide a high level api for data importing and preprocessing. It's convenient to be used in training process, however, it is unclear how to use them in the inference scenario.  We want to reuse the data preprocessing code  for inference which using the interface of tf.data.dataset. By google search and self experiment I found the following ways to use tf.data.dataset for inference:\n#using a placeholder\nx = tf.placeholder(tf.float32, shape=[None,2])\ndataset = tf.data.Dataset.from_tensor_slices(x)\ndata = np.random.sample((100,2))\niter = dataset.make_initializable_iterator() # create the iterator\nel = iter.get_next()\nwith tf.Session() as sess:\n    # feed the placeholder with data\n    sess.run(iter.initializer, feed_dict={ x: data }) \n    print(sess.run(el)) # output [ 0.52374458  0.71968478]\nBut the way above will bring some cost, Specifically ,there are two main operations:\n\nwe must run the function sess.run  one more time\ndataset object need be to initialize each time\n\nis it efficient to do in this way,  Or we need another way to use tf.data.dataset so that we can keep efficiency?\n\nUpdate:\nusing tf.contrib.data.get_single_element  or tf.data.experimental.get_single_element", "body": "<em>Please make sure that this is a documentation issue. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:doc_template</em>\r\n\r\n\r\n**System information**\r\n- TensorFlow version: master\r\n- Doc Link:  https://www.tensorflow.org/guide/datasets\r\n\r\ntf.data.dataset provide a high level api for data importing and preprocessing. It's convenient to be used in training process, however, it is unclear how to use them in the inference scenario.  We want to reuse the data preprocessing code  for inference which using the interface of tf.data.dataset. By google search and self experiment I found the following ways to use tf.data.dataset for inference:\r\n\r\n```python\r\n#using a placeholder\r\nx = tf.placeholder(tf.float32, shape=[None,2])\r\ndataset = tf.data.Dataset.from_tensor_slices(x)\r\ndata = np.random.sample((100,2))\r\niter = dataset.make_initializable_iterator() # create the iterator\r\nel = iter.get_next()\r\nwith tf.Session() as sess:\r\n    # feed the placeholder with data\r\n    sess.run(iter.initializer, feed_dict={ x: data }) \r\n    print(sess.run(el)) # output [ 0.52374458  0.71968478]\r\n```\r\n\r\nBut the way above will bring some cost, Specifically ,there are two main operations:\r\n1.   we must run the function sess.run  one more time\r\n2.   dataset object need be to initialize each time\r\n\r\n is it efficient to do in this way,  Or we need another way to use tf.data.dataset so that we can keep efficiency?\r\n\r\n\r\n-------------------------------\r\nUpdate:\r\nusing tf.contrib.data.get_single_element  or tf.data.experimental.get_single_element"}