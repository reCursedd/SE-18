{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1792", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1792/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1792/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1792/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1792", "id": 146377365, "node_id": "MDU6SXNzdWUxNDYzNzczNjU=", "number": 1792, "title": "FIFOQueue stopping early when graph restored from checkpoint", "user": {"login": "drgriffis", "id": 11445546, "node_id": "MDQ6VXNlcjExNDQ1NTQ2", "avatar_url": "https://avatars0.githubusercontent.com/u/11445546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drgriffis", "html_url": "https://github.com/drgriffis", "followers_url": "https://api.github.com/users/drgriffis/followers", "following_url": "https://api.github.com/users/drgriffis/following{/other_user}", "gists_url": "https://api.github.com/users/drgriffis/gists{/gist_id}", "starred_url": "https://api.github.com/users/drgriffis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drgriffis/subscriptions", "organizations_url": "https://api.github.com/users/drgriffis/orgs", "repos_url": "https://api.github.com/users/drgriffis/repos", "events_url": "https://api.github.com/users/drgriffis/events{/privacy}", "received_events_url": "https://api.github.com/users/drgriffis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-04-06T17:14:15Z", "updated_at": "2016-04-07T15:11:51Z", "closed_at": "2016-04-07T15:11:51Z", "author_association": "NONE", "body_html": "<p>I'm running into an issue when evaluating a previously-trained graph on test data: namely, the FIFOQueue returned by <code>tf.train.string_input_producer</code> is iterating fewer times over the data than the <code>num_epochs</code> parameter I give it.</p>\n<p>More specifically, I have my data stored as <code>.tfrecords</code> files, separated into training/validation/test sets.  To both train and evaluate the model, I do the following:</p>\n<ol>\n<li>Get the input data from a call to <code>tf.train.string_input_producer(trainingdata, num_epochs=N, shuffle=True)</code></li>\n<li>instantiate a <code>tf.train.Coordinator()</code></li>\n<li>Call <code>tf.train.start_queue_runners</code></li>\n<li>Run the following loop:</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">try</span>:\n    <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> coord.should_stop():\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> run the appropriate operations</span>\n<span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Halting -- epoch limit reached.<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-k\">finally</span>:\n    coord.request_stop()\ncoord.join(threads)</pre></div>\n<p>For training, I train the model for multiple epochs on the training data, saving a checkpoint every half epoch.</p>\n<p>Then, for evaluation, I restore the graph from one of those checkpoints as follows:</p>\n<ol>\n<li>Instantiate the input variables from the data pipeline as above</li>\n<li>Use those to construct the graph</li>\n<li>Call <code>tf.train.Saver().restore()</code> to restore the graph from the appropriate checkpoint</li>\n</ol>\n<p>The problem is, an <code>OutOfRangeError</code> is getting thrown the very first time I call <code>sess.run()</code> on any node.  If I keep increasing the number of epochs for the evaluation, I can eventually get it to actually iterate over the data (e.g. using <code>num_epochs=4</code> may give me 1 iteration over the data).  It's consistent with the same checkpoint file (e.g. if <code>num_epochs=4</code> gives me 1 iteration, <code>num_epochs=5</code> gives me 2, etc.), but it varies between checkpoint files.  It also does not seem to be consistent with the number of training epochs that had been completed when the checkpoint was saved.</p>\n<p>My expectation is that using <code>num_epochs=1</code> when initializing the data pipeline for the evaluation should be fine for doing a single iteration over those data points.  What am I missing?</p>\n<h2>Environment info</h2>\n<p>Operating System: RHEL<br>\nTF commit hash: f82ad360140a2078afcef6af40ad6ec75bd11c1 (version 7.0)<br>\nPython version: 3.5.1</p>", "body_text": "I'm running into an issue when evaluating a previously-trained graph on test data: namely, the FIFOQueue returned by tf.train.string_input_producer is iterating fewer times over the data than the num_epochs parameter I give it.\nMore specifically, I have my data stored as .tfrecords files, separated into training/validation/test sets.  To both train and evaluate the model, I do the following:\n\nGet the input data from a call to tf.train.string_input_producer(trainingdata, num_epochs=N, shuffle=True)\ninstantiate a tf.train.Coordinator()\nCall tf.train.start_queue_runners\nRun the following loop:\n\ntry:\n    while not coord.should_stop():\n        # run the appropriate operations\nexcept tf.errors.OutOfRangeError:\n    print('Halting -- epoch limit reached.')\nfinally:\n    coord.request_stop()\ncoord.join(threads)\nFor training, I train the model for multiple epochs on the training data, saving a checkpoint every half epoch.\nThen, for evaluation, I restore the graph from one of those checkpoints as follows:\n\nInstantiate the input variables from the data pipeline as above\nUse those to construct the graph\nCall tf.train.Saver().restore() to restore the graph from the appropriate checkpoint\n\nThe problem is, an OutOfRangeError is getting thrown the very first time I call sess.run() on any node.  If I keep increasing the number of epochs for the evaluation, I can eventually get it to actually iterate over the data (e.g. using num_epochs=4 may give me 1 iteration over the data).  It's consistent with the same checkpoint file (e.g. if num_epochs=4 gives me 1 iteration, num_epochs=5 gives me 2, etc.), but it varies between checkpoint files.  It also does not seem to be consistent with the number of training epochs that had been completed when the checkpoint was saved.\nMy expectation is that using num_epochs=1 when initializing the data pipeline for the evaluation should be fine for doing a single iteration over those data points.  What am I missing?\nEnvironment info\nOperating System: RHEL\nTF commit hash: f82ad360140a2078afcef6af40ad6ec75bd11c1 (version 7.0)\nPython version: 3.5.1", "body": "I'm running into an issue when evaluating a previously-trained graph on test data: namely, the FIFOQueue returned by `tf.train.string_input_producer` is iterating fewer times over the data than the `num_epochs` parameter I give it.\n\nMore specifically, I have my data stored as `.tfrecords` files, separated into training/validation/test sets.  To both train and evaluate the model, I do the following:\n1. Get the input data from a call to `tf.train.string_input_producer(trainingdata, num_epochs=N, shuffle=True)`\n2. instantiate a `tf.train.Coordinator()`\n3. Call `tf.train.start_queue_runners`\n4. Run the following loop:\n\n``` python\ntry:\n    while not coord.should_stop():\n        # run the appropriate operations\nexcept tf.errors.OutOfRangeError:\n    print('Halting -- epoch limit reached.')\nfinally:\n    coord.request_stop()\ncoord.join(threads)\n```\n\nFor training, I train the model for multiple epochs on the training data, saving a checkpoint every half epoch.\n\nThen, for evaluation, I restore the graph from one of those checkpoints as follows:\n1. Instantiate the input variables from the data pipeline as above\n2. Use those to construct the graph\n3. Call `tf.train.Saver().restore()` to restore the graph from the appropriate checkpoint\n\nThe problem is, an `OutOfRangeError` is getting thrown the very first time I call `sess.run()` on any node.  If I keep increasing the number of epochs for the evaluation, I can eventually get it to actually iterate over the data (e.g. using `num_epochs=4` may give me 1 iteration over the data).  It's consistent with the same checkpoint file (e.g. if `num_epochs=4` gives me 1 iteration, `num_epochs=5` gives me 2, etc.), but it varies between checkpoint files.  It also does not seem to be consistent with the number of training epochs that had been completed when the checkpoint was saved.\n\nMy expectation is that using `num_epochs=1` when initializing the data pipeline for the evaluation should be fine for doing a single iteration over those data points.  What am I missing?\n## Environment info\n\nOperating System: RHEL\nTF commit hash: f82ad360140a2078afcef6af40ad6ec75bd11c1 (version 7.0)\nPython version: 3.5.1\n"}