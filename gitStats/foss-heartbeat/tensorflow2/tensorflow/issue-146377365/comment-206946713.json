{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/206946713", "html_url": "https://github.com/tensorflow/tensorflow/issues/1792#issuecomment-206946713", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1792", "id": 206946713, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNjk0NjcxMw==", "user": {"login": "drgriffis", "id": 11445546, "node_id": "MDQ6VXNlcjExNDQ1NTQ2", "avatar_url": "https://avatars0.githubusercontent.com/u/11445546?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drgriffis", "html_url": "https://github.com/drgriffis", "followers_url": "https://api.github.com/users/drgriffis/followers", "following_url": "https://api.github.com/users/drgriffis/following{/other_user}", "gists_url": "https://api.github.com/users/drgriffis/gists{/gist_id}", "starred_url": "https://api.github.com/users/drgriffis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drgriffis/subscriptions", "organizations_url": "https://api.github.com/users/drgriffis/orgs", "repos_url": "https://api.github.com/users/drgriffis/repos", "events_url": "https://api.github.com/users/drgriffis/events{/privacy}", "received_events_url": "https://api.github.com/users/drgriffis/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-07T15:11:51Z", "updated_at": "2016-04-07T15:11:51Z", "author_association": "NONE", "body_html": "<p>I've found my problem; I was not aware that using the <code>num_epochs</code> parameter of <code>tf.train.string_input_producer</code> adds an epochs variable to the graph (in <code>tensorflow/python/training/input.py</code>, for the curious).  Since I'm not currently restricting which variables are saved/restored in my model, that tracker is getting restored from the checkpoint, so the model thinks it's read the data more than it has.</p>\n<p>I've implemented two solutions: a hack to reset that variable's value to 0 for models that are already trained, and limiting my save/restore variables to only the graph vars I care about for future models.</p>", "body_text": "I've found my problem; I was not aware that using the num_epochs parameter of tf.train.string_input_producer adds an epochs variable to the graph (in tensorflow/python/training/input.py, for the curious).  Since I'm not currently restricting which variables are saved/restored in my model, that tracker is getting restored from the checkpoint, so the model thinks it's read the data more than it has.\nI've implemented two solutions: a hack to reset that variable's value to 0 for models that are already trained, and limiting my save/restore variables to only the graph vars I care about for future models.", "body": "I've found my problem; I was not aware that using the `num_epochs` parameter of `tf.train.string_input_producer` adds an epochs variable to the graph (in `tensorflow/python/training/input.py`, for the curious).  Since I'm not currently restricting which variables are saved/restored in my model, that tracker is getting restored from the checkpoint, so the model thinks it's read the data more than it has.\n\nI've implemented two solutions: a hack to reset that variable's value to 0 for models that are already trained, and limiting my save/restore variables to only the graph vars I care about for future models.\n"}