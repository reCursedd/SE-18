{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380199854", "html_url": "https://github.com/tensorflow/tensorflow/issues/17939#issuecomment-380199854", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17939", "id": 380199854, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDE5OTg1NA==", "user": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-10T18:22:11Z", "updated_at": "2018-04-10T18:24:54Z", "author_association": "MEMBER", "body_html": "<p>Did you strip the training (i.e. gradient and optimizer nodes) out of your graph. This can typically be done with things like freezing. A lot of times TransposeConv is present only because the gradient of Conv is TranposeConv.</p>\n<p>See example 2  in <a href=\"https://www.tensorflow.org/mobile/tflite/devguide\" rel=\"nofollow\">https://www.tensorflow.org/mobile/tflite/devguide</a></p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nimg <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>img<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">3</span>))\nvar <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">3</span>))\nval <span class=\"pl-k\">=</span> img <span class=\"pl-k\">+</span> var\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">canonical_name</span>(<span class=\"pl-smi\">x</span>):\n  <span class=\"pl-k\">return</span> x.name.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>:<span class=\"pl-pds\">\"</span></span>)[<span class=\"pl-c1\">0</span>]\n\nout <span class=\"pl-k\">=</span> tf.identity(val, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>out<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n  sess.run(tf.global_variables_initializer())\n  out_tensors <span class=\"pl-k\">=</span> [out]\n  frozen_graphdef <span class=\"pl-k\">=</span> tf.graph_util.convert_variables_to_constants(\n      sess, sess.graph_def, <span class=\"pl-c1\">map</span>(canonical_name, out_tensors))\n  tflite_model <span class=\"pl-k\">=</span> tf.contrib.lite.toco_convert(\n      frozen_graphdef, [img], out_tensors)</pre></div>", "body_text": "Did you strip the training (i.e. gradient and optimizer nodes) out of your graph. This can typically be done with things like freezing. A lot of times TransposeConv is present only because the gradient of Conv is TranposeConv.\nSee example 2  in https://www.tensorflow.org/mobile/tflite/devguide\nimport tensorflow as tf\n\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\nvar = tf.get_variable(\"weights\", dtype=tf.float32, shape=(1,64,64,3))\nval = img + var\n\ndef canonical_name(x):\n  return x.name.split(\":\")[0]\n\nout = tf.identity(val, name=\"out\")\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n  out_tensors = [out]\n  frozen_graphdef = tf.graph_util.convert_variables_to_constants(\n      sess, sess.graph_def, map(canonical_name, out_tensors))\n  tflite_model = tf.contrib.lite.toco_convert(\n      frozen_graphdef, [img], out_tensors)", "body": "Did you strip the training (i.e. gradient and optimizer nodes) out of your graph. This can typically be done with things like freezing. A lot of times TransposeConv is present only because the gradient of Conv is TranposeConv. \r\n\r\n\r\nSee example 2  in https://www.tensorflow.org/mobile/tflite/devguide\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nimg = tf.placeholder(name=\"img\", dtype=tf.float32, shape=(1, 64, 64, 3))\r\nvar = tf.get_variable(\"weights\", dtype=tf.float32, shape=(1,64,64,3))\r\nval = img + var\r\n\r\ndef canonical_name(x):\r\n  return x.name.split(\":\")[0]\r\n\r\nout = tf.identity(val, name=\"out\")\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n  out_tensors = [out]\r\n  frozen_graphdef = tf.graph_util.convert_variables_to_constants(\r\n      sess, sess.graph_def, map(canonical_name, out_tensors))\r\n  tflite_model = tf.contrib.lite.toco_convert(\r\n      frozen_graphdef, [img], out_tensors)\r\n```"}