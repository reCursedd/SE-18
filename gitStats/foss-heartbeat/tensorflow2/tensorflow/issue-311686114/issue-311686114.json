{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18268", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18268/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18268/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18268/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18268", "id": 311686114, "node_id": "MDU6SXNzdWUzMTE2ODYxMTQ=", "number": 18268, "title": "Distributed Tensorflow :: Worker is getting terminated after running few training steps", "user": {"login": "deepak-2017", "id": 32203263, "node_id": "MDQ6VXNlcjMyMjAzMjYz", "avatar_url": "https://avatars3.githubusercontent.com/u/32203263?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deepak-2017", "html_url": "https://github.com/deepak-2017", "followers_url": "https://api.github.com/users/deepak-2017/followers", "following_url": "https://api.github.com/users/deepak-2017/following{/other_user}", "gists_url": "https://api.github.com/users/deepak-2017/gists{/gist_id}", "starred_url": "https://api.github.com/users/deepak-2017/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deepak-2017/subscriptions", "organizations_url": "https://api.github.com/users/deepak-2017/orgs", "repos_url": "https://api.github.com/users/deepak-2017/repos", "events_url": "https://api.github.com/users/deepak-2017/events{/privacy}", "received_events_url": "https://api.github.com/users/deepak-2017/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-04-05T16:40:45Z", "updated_at": "2018-08-15T08:11:55Z", "closed_at": "2018-06-02T07:06:53Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I am having 2 nodes cluster and running parameter server and first worker on one machine, and second worker on another machine.<br>\nEnvironment and version details are below,<br>\nHave I written custom code - Yes<br>\nOS Platform and Distribution - CentOS 7.2.1511<br>\nTensorFlow installed from - Binary<br>\nTensorFlow version - 1.3.0<br>\nBazel version - N/A<br>\nCUDA/cuDNN version - N/A<br>\nGPU model and memory - N/A<br>\nExact command to reproduce - N/A</p>\n<p>Parameter server and first worker that is also the chief(master) worker, starts correctly.<br>\nWhen I start send worker that also starts and training runs for few steps and then the worker is terminated with below exception.<br>\nPlz noote all are CPU machines (no GPUs)</p>\n<p>INFO:tensorflow:global step 482: loss: 0.8154 (15.70 sec/step)<br>\nINFO:tensorflow:global step 483: loss: 1.1079 (15.20 sec/step)<br>\nINFO:tensorflow:global step 485: loss: 0.8833 (15.21 sec/step)<br>\nINFO:tensorflow:global step 487: loss: 1.0541 (15.78 sec/step)<br>\nINFO:tensorflow:global step 489: loss: 0.8346 (14.84 sec/step)<br>\nNow run summeries<br>\nINFO:tensorflow:global step 491: loss: 1.0193 (14.91 sec/step)<br>\nloss is done<br>\n<strong>INFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framewo                                                                                           rk.errors_impl.CancelledError'&gt;, Step was cancelled by an explicit call to <code>Session::Close()</code>.</strong><br>\nTraceback (most recent call last):<br>\nFile \"./DTF_train_image_classifier.py\", line 464, in <br>\ntf.app.run()</p>\n<p>On parameter server, I see below error logs,</p>\n<p><strong>2018-04-05 14:47:12.860894: W tensorflow/core/kernels/queue_base.cc:295] _0_parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed<br>\n2018-04-05 14:47:12.861414: W tensorflow/core/kernels/queue_base.cc:295] _2_parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed<br>\n2018-04-05 14:47:12.861552: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br>\n2018-04-05 14:47:12.864402: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br>\n2018-04-05 14:47:12.864472: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed<br>\n2018-04-05 14:47:12.864504: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed</strong></p>\n<p>I am attaching below my code snippet ,</p>\n<p>server = tf.train.Server(<br>\ncluster,<br>\njob_name=FLAGS.job_name,<br>\ntask_index=FLAGS.task_index)</p>\n<pre><code>if FLAGS.pipeline_id is None:\n    raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\n\n#print('job name '+FLAGS.job_name)\n#print('task index name ' + FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\n    server.join()\nelif FLAGS.job_name == \"worker\":\ndataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\nprint 'num of classes -------------------&gt;', dataset.num_classes\nimages, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\n                                   width=FLAGS.image_resize, is_training=True)\n\n# Get number of steps to decay\nnum_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\nnum_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\ndecay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\n\nwith tf.device(tf.train.replica_device_setter(\n\t\tworker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n\t\tcluster=cluster)):\n\t\t\t\t\twith slim.arg_scope(inception_v3_arg_scope()):\n\t\tlogits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\n\n\n\n\texclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\n\t#exclude = get_variables_to_exclude()\n\tfor i in exclude:\n\t   print \"var to exclude -&gt; \",i\n\tvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n\n\t#Perform one-hot-encoding of the labels\n\tone_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n\n\t#Calculate loss\n\tloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n\ttotal_loss = tf.losses.get_total_loss()\n\n\t#Create the global step\n\tglobal_step = get_or_create_global_step()\n\n\n\t#Define your exponentially decaying learning rate\n\tlr = tf.train.exponential_decay(\n\t\tlearning_rate = FLAGS.initial_learning_rate,\n\t\tglobal_step = global_step,\n\t\tdecay_steps = decay_steps,\n\t\tdecay_rate = FLAGS.learning_rate_decay_factor,\n\t\tstaircase = True)\n\n\t#Get optimizer as configured by user\n\toptimizer = tf.train.AdamOptimizer(learning_rate = lr)\n\t#optimizer = getOptimizer(learning_rate = lr)\n\n\t#Create the train_op.\n\tvariables_to_train = get_variables_to_train()\n\t#for j in variables_to_train:\n\t  #print \"var to train \",j\n\t#vn = tf.trainable_variables()\n\ttrain_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\n\n\n\tpredictions = tf.argmax(end_points['Predictions'], 1)\n\tprobabilities = end_points['Predictions']\n\taccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n\tmetrics_op = tf.group(accuracy_update, probabilities)\n\n\n\t#create all the summaries you need to monitor\n\ttf.summary.scalar('losses/Total_Loss', total_loss)\n\ttf.summary.scalar('accuracy', accuracy)\n\ttf.summary.scalar('learning_rate', lr)\n\tmy_summary_op = tf.summary.merge_all()\n\n\t#Define train step to run training operation\n\tdef train_step(sess, train_op, global_step):\n\n\t\tstart_time = time.time()\n\t\ttotal_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n\t\ttime_elapsed = time.time() - start_time\n\n\n\t\tlogging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n\n\t\treturn total_loss, global_step_count\n\t\t\nsaver = tf.train.Saver(variables_to_restore)\n    def restore_fn(sess):\n        return saver.restore(sess, FLAGS.checkpoint_path)\n\n#Create supervisor\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path, summary_op = None, init_fn = restore_fn)\n\n\n#Run the managed session\nwith sv.prepare_or_wait_for_session(server.target) as sess:\n\tfor step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\n\t\t#Log info at each epoch:\n\t\tif step % num_batches_per_epoch == 0:\n\t\t\tlogging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\n\t\t\tlearning_rate_value, accuracy_value = sess.run([lr, accuracy])\n\t\t\tlogging.info('Current Learning Rate: %s', learning_rate_value)\n\t\t\tlogging.info('Current Streaming Training Accuracy: %s', accuracy_value)\n\n\n\t\t\tlogits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n\t\t\tprint 'logits: \\n', logits_value\n\t\t\tprint 'Probabilities: \\n', probabilities_value\n\t\t\tprint 'predictions: \\n', predictions_value\n\t\t\tprint 'Labels:\\n:', labels_value\n\n\t\t#Log the summaries every 10 step.\n\t\tif step % FLAGS.steps_update_frequency == 0 and step != 0:\n\t\t\tloss, gs = train_step(sess, train_op, sv.global_step)\n\t\t\tsummaries = sess.run(my_summary_op)\n\t\t\tsv.summary_computed(sess, summaries)\n\t\t\tprint \"**** SAVE THE MODEL ****\"\n\t\t\tsv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n\t\t\t##\n\n\t\t\tcheckpoint_path = tf.train.latest_checkpoint(train_logs_path)\n\t\t\ttraining_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\n\t\t\tcurrent_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\t\t\t#dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\n\n\t\t\t\n\n\n\t\telse:\n\t\t\tloss, _ = train_step(sess, train_op, sv.global_step)\n</code></pre>", "body_text": "Hi,\nI am having 2 nodes cluster and running parameter server and first worker on one machine, and second worker on another machine.\nEnvironment and version details are below,\nHave I written custom code - Yes\nOS Platform and Distribution - CentOS 7.2.1511\nTensorFlow installed from - Binary\nTensorFlow version - 1.3.0\nBazel version - N/A\nCUDA/cuDNN version - N/A\nGPU model and memory - N/A\nExact command to reproduce - N/A\nParameter server and first worker that is also the chief(master) worker, starts correctly.\nWhen I start send worker that also starts and training runs for few steps and then the worker is terminated with below exception.\nPlz noote all are CPU machines (no GPUs)\nINFO:tensorflow:global step 482: loss: 0.8154 (15.70 sec/step)\nINFO:tensorflow:global step 483: loss: 1.1079 (15.20 sec/step)\nINFO:tensorflow:global step 485: loss: 0.8833 (15.21 sec/step)\nINFO:tensorflow:global step 487: loss: 1.0541 (15.78 sec/step)\nINFO:tensorflow:global step 489: loss: 0.8346 (14.84 sec/step)\nNow run summeries\nINFO:tensorflow:global step 491: loss: 1.0193 (14.91 sec/step)\nloss is done\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framewo                                                                                           rk.errors_impl.CancelledError'>, Step was cancelled by an explicit call to Session::Close().\nTraceback (most recent call last):\nFile \"./DTF_train_image_classifier.py\", line 464, in \ntf.app.run()\nOn parameter server, I see below error logs,\n2018-04-05 14:47:12.860894: W tensorflow/core/kernels/queue_base.cc:295] _0_parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\n2018-04-05 14:47:12.861414: W tensorflow/core/kernels/queue_base.cc:295] _2_parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed\n2018-04-05 14:47:12.861552: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\n2018-04-05 14:47:12.864402: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\n2018-04-05 14:47:12.864472: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\n2018-04-05 14:47:12.864504: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\nI am attaching below my code snippet ,\nserver = tf.train.Server(\ncluster,\njob_name=FLAGS.job_name,\ntask_index=FLAGS.task_index)\nif FLAGS.pipeline_id is None:\n    raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\n\n#print('job name '+FLAGS.job_name)\n#print('task index name ' + FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\n    server.join()\nelif FLAGS.job_name == \"worker\":\ndataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\nprint 'num of classes ------------------->', dataset.num_classes\nimages, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\n                                   width=FLAGS.image_resize, is_training=True)\n\n# Get number of steps to decay\nnum_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\nnum_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\ndecay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\n\nwith tf.device(tf.train.replica_device_setter(\n\t\tworker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n\t\tcluster=cluster)):\n\t\t\t\t\twith slim.arg_scope(inception_v3_arg_scope()):\n\t\tlogits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\n\n\n\n\texclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\n\t#exclude = get_variables_to_exclude()\n\tfor i in exclude:\n\t   print \"var to exclude -> \",i\n\tvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)\n\n\t#Perform one-hot-encoding of the labels\n\tone_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\n\n\t#Calculate loss\n\tloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\n\ttotal_loss = tf.losses.get_total_loss()\n\n\t#Create the global step\n\tglobal_step = get_or_create_global_step()\n\n\n\t#Define your exponentially decaying learning rate\n\tlr = tf.train.exponential_decay(\n\t\tlearning_rate = FLAGS.initial_learning_rate,\n\t\tglobal_step = global_step,\n\t\tdecay_steps = decay_steps,\n\t\tdecay_rate = FLAGS.learning_rate_decay_factor,\n\t\tstaircase = True)\n\n\t#Get optimizer as configured by user\n\toptimizer = tf.train.AdamOptimizer(learning_rate = lr)\n\t#optimizer = getOptimizer(learning_rate = lr)\n\n\t#Create the train_op.\n\tvariables_to_train = get_variables_to_train()\n\t#for j in variables_to_train:\n\t  #print \"var to train \",j\n\t#vn = tf.trainable_variables()\n\ttrain_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\n\n\n\tpredictions = tf.argmax(end_points['Predictions'], 1)\n\tprobabilities = end_points['Predictions']\n\taccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\n\tmetrics_op = tf.group(accuracy_update, probabilities)\n\n\n\t#create all the summaries you need to monitor\n\ttf.summary.scalar('losses/Total_Loss', total_loss)\n\ttf.summary.scalar('accuracy', accuracy)\n\ttf.summary.scalar('learning_rate', lr)\n\tmy_summary_op = tf.summary.merge_all()\n\n\t#Define train step to run training operation\n\tdef train_step(sess, train_op, global_step):\n\n\t\tstart_time = time.time()\n\t\ttotal_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\n\t\ttime_elapsed = time.time() - start_time\n\n\n\t\tlogging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\n\n\t\treturn total_loss, global_step_count\n\t\t\nsaver = tf.train.Saver(variables_to_restore)\n    def restore_fn(sess):\n        return saver.restore(sess, FLAGS.checkpoint_path)\n\n#Create supervisor\nsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path, summary_op = None, init_fn = restore_fn)\n\n\n#Run the managed session\nwith sv.prepare_or_wait_for_session(server.target) as sess:\n\tfor step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\n\t\t#Log info at each epoch:\n\t\tif step % num_batches_per_epoch == 0:\n\t\t\tlogging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\n\t\t\tlearning_rate_value, accuracy_value = sess.run([lr, accuracy])\n\t\t\tlogging.info('Current Learning Rate: %s', learning_rate_value)\n\t\t\tlogging.info('Current Streaming Training Accuracy: %s', accuracy_value)\n\n\n\t\t\tlogits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\n\t\t\tprint 'logits: \\n', logits_value\n\t\t\tprint 'Probabilities: \\n', probabilities_value\n\t\t\tprint 'predictions: \\n', predictions_value\n\t\t\tprint 'Labels:\\n:', labels_value\n\n\t\t#Log the summaries every 10 step.\n\t\tif step % FLAGS.steps_update_frequency == 0 and step != 0:\n\t\t\tloss, gs = train_step(sess, train_op, sv.global_step)\n\t\t\tsummaries = sess.run(my_summary_op)\n\t\t\tsv.summary_computed(sess, summaries)\n\t\t\tprint \"**** SAVE THE MODEL ****\"\n\t\t\tsv.saver.save(sess,sv.save_path,global_step=sv.global_step)\n\t\t\t##\n\n\t\t\tcheckpoint_path = tf.train.latest_checkpoint(train_logs_path)\n\t\t\ttraining_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\n\t\t\tcurrent_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n\t\t\t#dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\n\n\t\t\t\n\n\n\t\telse:\n\t\t\tloss, _ = train_step(sess, train_op, sv.global_step)", "body": "Hi,\r\n\r\nI am having 2 nodes cluster and running parameter server and first worker on one machine, and second worker on another machine.\r\nEnvironment and version details are below,\r\nHave I written custom code - Yes\r\nOS Platform and Distribution - CentOS 7.2.1511\r\nTensorFlow installed from - Binary\r\nTensorFlow version - 1.3.0\r\nBazel version - N/A\r\nCUDA/cuDNN version - N/A\r\nGPU model and memory - N/A\r\nExact command to reproduce - N/A\r\n\r\nParameter server and first worker that is also the chief(master) worker, starts correctly.\r\nWhen I start send worker that also starts and training runs for few steps and then the worker is terminated with below exception.\r\nPlz noote all are CPU machines (no GPUs)\r\n\r\nINFO:tensorflow:global step 482: loss: 0.8154 (15.70 sec/step)\r\nINFO:tensorflow:global step 483: loss: 1.1079 (15.20 sec/step)\r\nINFO:tensorflow:global step 485: loss: 0.8833 (15.21 sec/step)\r\nINFO:tensorflow:global step 487: loss: 1.0541 (15.78 sec/step)\r\nINFO:tensorflow:global step 489: loss: 0.8346 (14.84 sec/step)\r\nNow run summeries\r\nINFO:tensorflow:global step 491: loss: 1.0193 (14.91 sec/step)\r\nloss is done\r\n**INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framewo                                                                                           rk.errors_impl.CancelledError'>, Step was cancelled by an explicit call to `Session::Close()`.**\r\nTraceback (most recent call last):\r\n  File \"./DTF_train_image_classifier.py\", line 464, in <module>\r\n    tf.app.run()\r\n\t\r\nOn parameter server, I see below error logs,\r\n\r\n**2018-04-05 14:47:12.860894: W tensorflow/core/kernels/queue_base.cc:295] _0_parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.861414: W tensorflow/core/kernels/queue_base.cc:295] _2_parallel_read/common_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.861552: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.864402: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.864472: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed\r\n2018-04-05 14:47:12.864504: W tensorflow/core/kernels/queue_base.cc:295] _4_batch/fifo_queue: Skipping cancelled enqueue attempt with queue not closed**\r\n\r\nI am attaching below my code snippet ,\r\n\r\nserver = tf.train.Server(\r\n        cluster,\r\n        job_name=FLAGS.job_name,\r\n        task_index=FLAGS.task_index)\r\n   \r\n    if FLAGS.pipeline_id is None:\r\n        raise ValueError('pipeline_id [%s] was not recognized', FLAGS.pipeline_id)\r\n\r\n    #print('job name '+FLAGS.job_name)\r\n    #print('task index name ' + FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n    elif FLAGS.job_name == \"worker\":\r\n\tdataset = get_split(FLAGS.dataset_split_name, FLAGS.tf_records_dir, 'flowers', FLAGS.num_classes, labels_file)\r\n    print 'num of classes ------------------->', dataset.num_classes\r\n    images, _, labels = load_batch(FLAGS, dataset, batch_size=FLAGS.training_batch_size, height=FLAGS.image_resize,\r\n                                       width=FLAGS.image_resize, is_training=True)\r\n\r\n    # Get number of steps to decay\r\n    num_batches_per_epoch = int(dataset.num_samples / FLAGS.training_batch_size)\r\n    num_steps_per_epoch = num_batches_per_epoch  # Because one step is one batch processed\r\n    decay_steps = int(FLAGS.num_epochs_before_decay * num_steps_per_epoch)\r\n\r\n\twith tf.device(tf.train.replica_device_setter(\r\n\t\t\tworker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n\t\t\tcluster=cluster)):\r\n\t\t\t\t\t\twith slim.arg_scope(inception_v3_arg_scope()):\r\n\t\t\tlogits, end_points = inception_v3(images, num_classes = dataset.num_classes, is_training = True)\r\n\r\n\r\n\r\n\t\texclude = ['InceptionV3/Logits', 'InceptionV3/AuxLogits']\r\n\t\t#exclude = get_variables_to_exclude()\r\n\t\tfor i in exclude:\r\n\t\t   print \"var to exclude -> \",i\r\n\t\tvariables_to_restore = slim.get_variables_to_restore(exclude = exclude)\r\n\r\n\t\t#Perform one-hot-encoding of the labels\r\n\t\tone_hot_labels = slim.one_hot_encoding(labels, dataset.num_classes)\r\n\r\n\t\t#Calculate loss\r\n\t\tloss = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits = logits)\r\n\t\ttotal_loss = tf.losses.get_total_loss()\r\n\r\n\t\t#Create the global step\r\n\t\tglobal_step = get_or_create_global_step()\r\n\r\n\r\n\t\t#Define your exponentially decaying learning rate\r\n\t\tlr = tf.train.exponential_decay(\r\n\t\t\tlearning_rate = FLAGS.initial_learning_rate,\r\n\t\t\tglobal_step = global_step,\r\n\t\t\tdecay_steps = decay_steps,\r\n\t\t\tdecay_rate = FLAGS.learning_rate_decay_factor,\r\n\t\t\tstaircase = True)\r\n\r\n\t\t#Get optimizer as configured by user\r\n\t\toptimizer = tf.train.AdamOptimizer(learning_rate = lr)\r\n\t\t#optimizer = getOptimizer(learning_rate = lr)\r\n\r\n\t\t#Create the train_op.\r\n\t\tvariables_to_train = get_variables_to_train()\r\n\t\t#for j in variables_to_train:\r\n\t\t  #print \"var to train \",j\r\n\t\t#vn = tf.trainable_variables()\r\n\t\ttrain_op = slim.learning.create_train_op(total_loss, optimizer,variables_to_train=variables_to_train)\r\n\r\n\r\n\t\tpredictions = tf.argmax(end_points['Predictions'], 1)\r\n\t\tprobabilities = end_points['Predictions']\r\n\t\taccuracy, accuracy_update = tf.contrib.metrics.streaming_accuracy(predictions, labels)\r\n\t\tmetrics_op = tf.group(accuracy_update, probabilities)\r\n\r\n\r\n\t\t#create all the summaries you need to monitor\r\n\t\ttf.summary.scalar('losses/Total_Loss', total_loss)\r\n\t\ttf.summary.scalar('accuracy', accuracy)\r\n\t\ttf.summary.scalar('learning_rate', lr)\r\n\t\tmy_summary_op = tf.summary.merge_all()\r\n\r\n\t\t#Define train step to run training operation\r\n\t\tdef train_step(sess, train_op, global_step):\r\n\r\n\t\t\tstart_time = time.time()\r\n\t\t\ttotal_loss, global_step_count, _ = sess.run([train_op, global_step, metrics_op])\r\n\t\t\ttime_elapsed = time.time() - start_time\r\n\r\n\r\n\t\t\tlogging.info('global step %s: loss: %.4f (%.2f sec/step)', global_step_count, total_loss, time_elapsed)\r\n\r\n\t\t\treturn total_loss, global_step_count\r\n\t\t\t\r\n\tsaver = tf.train.Saver(variables_to_restore)\r\n        def restore_fn(sess):\r\n            return saver.restore(sess, FLAGS.checkpoint_path)\r\n\r\n\t#Create supervisor\r\n\tsv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),global_step=global_step,logdir = train_logs_path, summary_op = None, init_fn = restore_fn)\r\n\r\n\r\n\t#Run the managed session\r\n\twith sv.prepare_or_wait_for_session(server.target) as sess:\r\n\t\tfor step in xrange(num_steps_per_epoch * FLAGS.training_num_epochs):\r\n\t\t\t#Log info at each epoch:\r\n\t\t\tif step % num_batches_per_epoch == 0:\r\n\t\t\t\tlogging.info('Epoch %s/%s', step/num_batches_per_epoch + 1, FLAGS.training_num_epochs)\r\n\t\t\t\tlearning_rate_value, accuracy_value = sess.run([lr, accuracy])\r\n\t\t\t\tlogging.info('Current Learning Rate: %s', learning_rate_value)\r\n\t\t\t\tlogging.info('Current Streaming Training Accuracy: %s', accuracy_value)\r\n\r\n\r\n\t\t\t\tlogits_value, probabilities_value, predictions_value, labels_value = sess.run([logits, probabilities, predictions, labels])\r\n\t\t\t\tprint 'logits: \\n', logits_value\r\n\t\t\t\tprint 'Probabilities: \\n', probabilities_value\r\n\t\t\t\tprint 'predictions: \\n', predictions_value\r\n\t\t\t\tprint 'Labels:\\n:', labels_value\r\n\r\n\t\t\t#Log the summaries every 10 step.\r\n\t\t\tif step % FLAGS.steps_update_frequency == 0 and step != 0:\r\n\t\t\t\tloss, gs = train_step(sess, train_op, sv.global_step)\r\n\t\t\t\tsummaries = sess.run(my_summary_op)\r\n\t\t\t\tsv.summary_computed(sess, summaries)\r\n\t\t\t\tprint \"**** SAVE THE MODEL ****\"\r\n\t\t\t\tsv.saver.save(sess,sv.save_path,global_step=sv.global_step)\r\n\t\t\t\t##\r\n\r\n\t\t\t\tcheckpoint_path = tf.train.latest_checkpoint(train_logs_path)\r\n\t\t\t\ttraining_json = '{\"model_path\":\"'+ checkpoint_path +'\",\"no_of_steps\":' +str(gs)+',\"loss\":'+str(loss)+'}'\r\n\t\t\t\tcurrent_time = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')\r\n\t\t\t\t#dbClient.store_metrices([(FLAGS.pipeline_id,FLAGS.operation_type,training_json,current_time)])\r\n\r\n\t\t\t\t\r\n\r\n\r\n\t\t\telse:\r\n\t\t\t\tloss, _ = train_step(sess, train_op, sv.global_step)\r\n\r\n"}