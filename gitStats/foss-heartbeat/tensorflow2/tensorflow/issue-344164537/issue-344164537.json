{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21099", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21099/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21099/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21099/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21099", "id": 344164537, "node_id": "MDU6SXNzdWUzNDQxNjQ1Mzc=", "number": 21099, "title": "Tensorflow Optimizer cannot optimize gradients when they are the output of tf.cond", "user": {"login": "issa-s-ayoub", "id": 26584101, "node_id": "MDQ6VXNlcjI2NTg0MTAx", "avatar_url": "https://avatars2.githubusercontent.com/u/26584101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/issa-s-ayoub", "html_url": "https://github.com/issa-s-ayoub", "followers_url": "https://api.github.com/users/issa-s-ayoub/followers", "following_url": "https://api.github.com/users/issa-s-ayoub/following{/other_user}", "gists_url": "https://api.github.com/users/issa-s-ayoub/gists{/gist_id}", "starred_url": "https://api.github.com/users/issa-s-ayoub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/issa-s-ayoub/subscriptions", "organizations_url": "https://api.github.com/users/issa-s-ayoub/orgs", "repos_url": "https://api.github.com/users/issa-s-ayoub/repos", "events_url": "https://api.github.com/users/issa-s-ayoub/events{/privacy}", "received_events_url": "https://api.github.com/users/issa-s-ayoub/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-07-24T19:04:29Z", "updated_at": "2018-08-13T16:14:33Z", "closed_at": "2018-08-09T16:38:29Z", "author_association": "NONE", "body_html": "<p>I have the following code in tensorflow where I cam trying to compute the gradients in 2 ways:</p>\n<pre><code>def optimize(loss):\n    \n    with tf.name_scope('Optimizer'):\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    \n        with tf.control_dependencies(update_ops):\n            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n\t\t\n            def prev_grads_are_not_None():\n            \n                ....... (Not important for now)....\n                return grads_and_vars, grads_wrt_initial_state\n        \n            # This is executed when feeding the first batch only. =&gt; there is no previously collected gradients.\n            def prev_grads_are_None():\n            \n                # This will compute the local gradient given the loss function (taking into account all trainable variables)\n                local_grads = tf.gradients(loss, tf.trainable_variables())\n                grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\n            \n                grads_and_vars = list(zip(local_grads, tf.trainable_variables()))\n                return grads_and_vars, grads_wrt_initial_state\n        \n            grads_and_vars, grads_wrt_initial_state = tf.cond(this_is_last_batch, lambda: prev_grads_are_None(), lambda: prev_grads_are_None())\n        \n            train_step = optimizer.apply_gradients(grads_and_vars)            \n            return train_step, grads_wrt_initial_state\n</code></pre>\n<p>Please note that I have used the same methods 2 times on purpose just for testing tf.cond.<br>\nand this will throw the following error:</p>\n<pre><code>~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in update_op(self, optimizer, g)\n    199 \n    200   def update_op(self, optimizer, g):\n--&gt; 201     raise NotImplementedError(\"Trying to update a Tensor \", self._v)\n    202 \n    203 \n\nNotImplementedError: ('Trying to update a Tensor ', &lt;tf.Tensor 'Optimizer/cond/Merge_1:0' shape=(264, 128) dtype=float32&gt;)\n</code></pre>\n<p>so what is the reason for this bug?<br>\nPlease note that I am trying to train a 2 layer GRU.</p>\n<p>I have tried the following code as well:</p>\n<p>def optimize(loss):</p>\n<pre><code>with tf.name_scope('Optimizer'):\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    \n    with tf.control_dependencies(update_ops):\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        \n        # When training we start from the end and proceed back ward. while we are in the middle of the training\n        # then we will have previously collected gradients to be used. \n        def prev_grads_are_not_None():\n            \n            # ..... Not needed for now .....\n            return grads, grads_wrt_initial_state\n        \n        # This is executed when feeding the first batch only. =&gt; there is no previously collected gradients.\n        def prev_grads_are_None():\n            \n            # This will compute the local gradient given the loss function (taking into account all trainable variables)\n            local_grads_ = tf.gradients(loss, tf.trainable_variables())\n            grads_wrt_initial_state_ = tf.gradients(loss, initial_state)\n            \n            local_grads = [g for g in local_grads_]\n            grads_wrt_initial_state = [g for g in grads_wrt_initial_state_]\n            \n            print(\"---&lt;&lt;&lt;\")\n            for h in local_grads:\n                print(h)\n                \n            return local_grads, grads_wrt_initial_state\n        \n        grads_r, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\n        \n        print(\"----&gt;&gt;\")\n        for r in grads_r:\n            print(r)\n        \n        grads_and_vars = list(zip(grads_r, tf.trainable_variables()))\n\n        train_step = optimizer.apply_gradients(grads_and_vars)            \n        return train_step, grads_wrt_initial_state\n</code></pre>\n<p>But that will run and then stall, but showing kernel is busy in jupyter notebook.</p>\n<p>The printed tensors shows:</p>\n<pre><code>---&lt;&lt;&lt;\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(264, 128), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(128,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(264, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(96, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(96, 32), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(32,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/output_layer/MatMul_grad/MatMul_1:0\", shape=(32, 3), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/output_layer/add_grad/Reshape_1:0\", shape=(3,), dtype=float32)\n----&gt;&gt;\nTensor(\"Optimizer/case/cond/Merge:0\", shape=(264, 128), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_1:0\", shape=(128,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_2:0\", shape=(264, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_3:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_4:0\", shape=(96, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_5:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_6:0\", shape=(96, 32), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_7:0\", shape=(32,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_8:0\", shape=(32, 3), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_9:0\", shape=(3,), dtype=float32)\n</code></pre>\n<p>Therefore, is the difference in the type of the tensor considered the reason for this problem?<br>\nAny help is much appreciated!!</p>\n<p><strong>System Information</strong></p>\n<ul>\n<li>Python version: 3.6</li>\n<li>My tensorflow version is 1.8</li>\n<li>CUDA: 7/ cuDNN: 9</li>\n<li>GPU Model: GeForce 1080 Ti/ Memory: 12 GB</li>\n<li>tensorflow installed from binaries</li>\n<li>OS: windows 10.</li>\n<li>Have I written custom code: N/A</li>\n<li>Bazel Version: N/A</li>\n<li>Mobile Device N/A</li>\n<li>Exact command to reproduce: N/A</li>\n</ul>", "body_text": "I have the following code in tensorflow where I cam trying to compute the gradients in 2 ways:\ndef optimize(loss):\n    \n    with tf.name_scope('Optimizer'):\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    \n        with tf.control_dependencies(update_ops):\n            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n\t\t\n            def prev_grads_are_not_None():\n            \n                ....... (Not important for now)....\n                return grads_and_vars, grads_wrt_initial_state\n        \n            # This is executed when feeding the first batch only. => there is no previously collected gradients.\n            def prev_grads_are_None():\n            \n                # This will compute the local gradient given the loss function (taking into account all trainable variables)\n                local_grads = tf.gradients(loss, tf.trainable_variables())\n                grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\n            \n                grads_and_vars = list(zip(local_grads, tf.trainable_variables()))\n                return grads_and_vars, grads_wrt_initial_state\n        \n            grads_and_vars, grads_wrt_initial_state = tf.cond(this_is_last_batch, lambda: prev_grads_are_None(), lambda: prev_grads_are_None())\n        \n            train_step = optimizer.apply_gradients(grads_and_vars)            \n            return train_step, grads_wrt_initial_state\n\nPlease note that I have used the same methods 2 times on purpose just for testing tf.cond.\nand this will throw the following error:\n~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in update_op(self, optimizer, g)\n    199 \n    200   def update_op(self, optimizer, g):\n--> 201     raise NotImplementedError(\"Trying to update a Tensor \", self._v)\n    202 \n    203 \n\nNotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'Optimizer/cond/Merge_1:0' shape=(264, 128) dtype=float32>)\n\nso what is the reason for this bug?\nPlease note that I am trying to train a 2 layer GRU.\nI have tried the following code as well:\ndef optimize(loss):\nwith tf.name_scope('Optimizer'):\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n    \n    with tf.control_dependencies(update_ops):\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n        \n        # When training we start from the end and proceed back ward. while we are in the middle of the training\n        # then we will have previously collected gradients to be used. \n        def prev_grads_are_not_None():\n            \n            # ..... Not needed for now .....\n            return grads, grads_wrt_initial_state\n        \n        # This is executed when feeding the first batch only. => there is no previously collected gradients.\n        def prev_grads_are_None():\n            \n            # This will compute the local gradient given the loss function (taking into account all trainable variables)\n            local_grads_ = tf.gradients(loss, tf.trainable_variables())\n            grads_wrt_initial_state_ = tf.gradients(loss, initial_state)\n            \n            local_grads = [g for g in local_grads_]\n            grads_wrt_initial_state = [g for g in grads_wrt_initial_state_]\n            \n            print(\"---<<<\")\n            for h in local_grads:\n                print(h)\n                \n            return local_grads, grads_wrt_initial_state\n        \n        grads_r, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\n        \n        print(\"---->>\")\n        for r in grads_r:\n            print(r)\n        \n        grads_and_vars = list(zip(grads_r, tf.trainable_variables()))\n\n        train_step = optimizer.apply_gradients(grads_and_vars)            \n        return train_step, grads_wrt_initial_state\n\nBut that will run and then stall, but showing kernel is busy in jupyter notebook.\nThe printed tensors shows:\n---<<<\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(264, 128), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(128,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(264, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(96, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(96, 32), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(32,), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/output_layer/MatMul_grad/MatMul_1:0\", shape=(32, 3), dtype=float32)\nTensor(\"Optimizer/case/cond/gradients/output_layer/add_grad/Reshape_1:0\", shape=(3,), dtype=float32)\n---->>\nTensor(\"Optimizer/case/cond/Merge:0\", shape=(264, 128), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_1:0\", shape=(128,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_2:0\", shape=(264, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_3:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_4:0\", shape=(96, 64), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_5:0\", shape=(64,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_6:0\", shape=(96, 32), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_7:0\", shape=(32,), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_8:0\", shape=(32, 3), dtype=float32)\nTensor(\"Optimizer/case/cond/Merge_9:0\", shape=(3,), dtype=float32)\n\nTherefore, is the difference in the type of the tensor considered the reason for this problem?\nAny help is much appreciated!!\nSystem Information\n\nPython version: 3.6\nMy tensorflow version is 1.8\nCUDA: 7/ cuDNN: 9\nGPU Model: GeForce 1080 Ti/ Memory: 12 GB\ntensorflow installed from binaries\nOS: windows 10.\nHave I written custom code: N/A\nBazel Version: N/A\nMobile Device N/A\nExact command to reproduce: N/A", "body": "I have the following code in tensorflow where I cam trying to compute the gradients in 2 ways:\r\n\r\n    def optimize(loss):\r\n        \r\n        with tf.name_scope('Optimizer'):\r\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        \r\n            with tf.control_dependencies(update_ops):\r\n                optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n\t\t\t\r\n                def prev_grads_are_not_None():\r\n                \r\n                    ....... (Not important for now)....\r\n                    return grads_and_vars, grads_wrt_initial_state\r\n            \r\n                # This is executed when feeding the first batch only. => there is no previously collected gradients.\r\n                def prev_grads_are_None():\r\n                \r\n                    # This will compute the local gradient given the loss function (taking into account all trainable variables)\r\n                    local_grads = tf.gradients(loss, tf.trainable_variables())\r\n                    grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\r\n                \r\n                    grads_and_vars = list(zip(local_grads, tf.trainable_variables()))\r\n                    return grads_and_vars, grads_wrt_initial_state\r\n            \r\n                grads_and_vars, grads_wrt_initial_state = tf.cond(this_is_last_batch, lambda: prev_grads_are_None(), lambda: prev_grads_are_None())\r\n            \r\n                train_step = optimizer.apply_gradients(grads_and_vars)            \r\n                return train_step, grads_wrt_initial_state\r\nPlease note that I have used the same methods 2 times on purpose just for testing tf.cond.\r\nand this will throw the following error:\r\n\r\n    ~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py in update_op(self, optimizer, g)\r\n        199 \r\n        200   def update_op(self, optimizer, g):\r\n    --> 201     raise NotImplementedError(\"Trying to update a Tensor \", self._v)\r\n        202 \r\n        203 \r\n\r\n    NotImplementedError: ('Trying to update a Tensor ', <tf.Tensor 'Optimizer/cond/Merge_1:0' shape=(264, 128) dtype=float32>)\r\n\r\nso what is the reason for this bug?\r\nPlease note that I am trying to train a 2 layer GRU.\r\n\r\nI have tried the following code as well:\r\n\r\ndef optimize(loss):\r\n        \r\n    with tf.name_scope('Optimizer'):\r\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n        \r\n        with tf.control_dependencies(update_ops):\r\n            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n            \r\n            # When training we start from the end and proceed back ward. while we are in the middle of the training\r\n            # then we will have previously collected gradients to be used. \r\n            def prev_grads_are_not_None():\r\n                \r\n                # ..... Not needed for now .....\r\n                return grads, grads_wrt_initial_state\r\n            \r\n            # This is executed when feeding the first batch only. => there is no previously collected gradients.\r\n            def prev_grads_are_None():\r\n                \r\n                # This will compute the local gradient given the loss function (taking into account all trainable variables)\r\n                local_grads_ = tf.gradients(loss, tf.trainable_variables())\r\n                grads_wrt_initial_state_ = tf.gradients(loss, initial_state)\r\n                \r\n                local_grads = [g for g in local_grads_]\r\n                grads_wrt_initial_state = [g for g in grads_wrt_initial_state_]\r\n                \r\n                print(\"---<<<\")\r\n                for h in local_grads:\r\n                    print(h)\r\n                    \r\n                return local_grads, grads_wrt_initial_state\r\n            \r\n            grads_r, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\r\n            \r\n            print(\"---->>\")\r\n            for r in grads_r:\r\n                print(r)\r\n            \r\n            grads_and_vars = list(zip(grads_r, tf.trainable_variables()))\r\n\r\n            train_step = optimizer.apply_gradients(grads_and_vars)            \r\n            return train_step, grads_wrt_initial_state\r\n\r\nBut that will run and then stall, but showing kernel is busy in jupyter notebook. \r\n\r\nThe printed tensors shows:\r\n\r\n\t---<<<\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(264, 128), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(128,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(264, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_0/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul/Enter_grad/b_acc_3:0\", shape=(96, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd/Enter_grad/b_acc_3:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/MatMul_1/Enter_grad/b_acc_3:0\", shape=(96, 32), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/GRU/rnn/while/rnn/multi_rnn_cell/cell_1/gru_cell/BiasAdd_1/Enter_grad/b_acc_3:0\", shape=(32,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/output_layer/MatMul_grad/MatMul_1:0\", shape=(32, 3), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/gradients/output_layer/add_grad/Reshape_1:0\", shape=(3,), dtype=float32)\r\n\t---->>\r\n\tTensor(\"Optimizer/case/cond/Merge:0\", shape=(264, 128), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_1:0\", shape=(128,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_2:0\", shape=(264, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_3:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_4:0\", shape=(96, 64), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_5:0\", shape=(64,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_6:0\", shape=(96, 32), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_7:0\", shape=(32,), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_8:0\", shape=(32, 3), dtype=float32)\r\n\tTensor(\"Optimizer/case/cond/Merge_9:0\", shape=(3,), dtype=float32)\r\n\r\nTherefore, is the difference in the type of the tensor considered the reason for this problem?\r\nAny help is much appreciated!! \r\n\r\n**System Information**\r\n\r\n- Python version: 3.6\r\n- My tensorflow version is 1.8\r\n- CUDA: 7/ cuDNN: 9\r\n- GPU Model: GeForce 1080 Ti/ Memory: 12 GB\r\n- tensorflow installed from binaries \r\n- OS: windows 10.\r\n- Have I written custom code: N/A\r\n- Bazel Version: N/A\r\n- Mobile Device N/A\r\n- Exact command to reproduce: N/A"}