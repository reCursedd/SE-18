{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/434065751", "html_url": "https://github.com/tensorflow/tensorflow/pull/23011#issuecomment-434065751", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23011", "id": 434065751, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDA2NTc1MQ==", "user": {"login": "calid", "id": 494405, "node_id": "MDQ6VXNlcjQ5NDQwNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/494405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/calid", "html_url": "https://github.com/calid", "followers_url": "https://api.github.com/users/calid/followers", "following_url": "https://api.github.com/users/calid/following{/other_user}", "gists_url": "https://api.github.com/users/calid/gists{/gist_id}", "starred_url": "https://api.github.com/users/calid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/calid/subscriptions", "organizations_url": "https://api.github.com/users/calid/orgs", "repos_url": "https://api.github.com/users/calid/repos", "events_url": "https://api.github.com/users/calid/events{/privacy}", "received_events_url": "https://api.github.com/users/calid/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-29T20:25:25Z", "updated_at": "2018-10-29T20:25:25Z", "author_association": "NONE", "body_html": "<p>It looks like the difference between your code and the unit test code is the unit test code explicitly calls <code>enqueuer.stop()</code>.  The  EOF/BrokenPipe is because the MultiProcess Manager continues to run in the background trying to put new data from the generator on the queue, but it exits once the generator is exhausted.  Calling stop after <code>acc</code> is populated shuts it down cleanly.  That is, this fixed things for me:</p>\n<div class=\"highlight highlight-source-diff\"><pre>acc = [i for i,_ in zip(g,range(100))]\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span> gen.stop()</span></pre></div>\n<p>As far as the performance, my code is not fundamentally different from the original aside from synchronizing things correctly.  Keep in mind the old code was really \"fast\" but it was wrong because it was running a duplicate generator in each process (since with multiprocess you are forking and creating a copy of the process memory, not shared memory as with threading), this returns duplicate values and caused the original <code>test_generator_enqueuer_processes</code> to fail... I'm guessing why it was disabled.  If I enable it (flip the assertion to be <code>assertEqual</code>) with the old code I get:</p>\n<pre><code>AssertionError: Lists differ: [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 6,[320 chars], 30] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13[340 chars], 99]\n</code></pre>\n<p>Which shows the duplicate results.</p>\n<p>I'm not sure how you  would parallelize the generator itself?  What you can for sure parallelize is any preprocessing of what the generator produces, which I'm guessing is what your sleep is meant to simulate?  Right now there is no hook for that, but it would be easy to add support for passing in a user defined function that encapsulates any preprocesing logic, and parallelize that across the workers.  What do you think?</p>\n<p>Noted about the CI time on tensorflow, I'll go ahead and open a separate PR on keras.</p>", "body_text": "It looks like the difference between your code and the unit test code is the unit test code explicitly calls enqueuer.stop().  The  EOF/BrokenPipe is because the MultiProcess Manager continues to run in the background trying to put new data from the generator on the queue, but it exits once the generator is exhausted.  Calling stop after acc is populated shuts it down cleanly.  That is, this fixed things for me:\nacc = [i for i,_ in zip(g,range(100))]\n+ gen.stop()\nAs far as the performance, my code is not fundamentally different from the original aside from synchronizing things correctly.  Keep in mind the old code was really \"fast\" but it was wrong because it was running a duplicate generator in each process (since with multiprocess you are forking and creating a copy of the process memory, not shared memory as with threading), this returns duplicate values and caused the original test_generator_enqueuer_processes to fail... I'm guessing why it was disabled.  If I enable it (flip the assertion to be assertEqual) with the old code I get:\nAssertionError: Lists differ: [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 6,[320 chars], 30] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13[340 chars], 99]\n\nWhich shows the duplicate results.\nI'm not sure how you  would parallelize the generator itself?  What you can for sure parallelize is any preprocessing of what the generator produces, which I'm guessing is what your sleep is meant to simulate?  Right now there is no hook for that, but it would be easy to add support for passing in a user defined function that encapsulates any preprocesing logic, and parallelize that across the workers.  What do you think?\nNoted about the CI time on tensorflow, I'll go ahead and open a separate PR on keras.", "body": "It looks like the difference between your code and the unit test code is the unit test code explicitly calls `enqueuer.stop()`.  The  EOF/BrokenPipe is because the MultiProcess Manager continues to run in the background trying to put new data from the generator on the queue, but it exits once the generator is exhausted.  Calling stop after `acc` is populated shuts it down cleanly.  That is, this fixed things for me:\r\n\r\n```diff\r\nacc = [i for i,_ in zip(g,range(100))]\r\n+ gen.stop()\r\n```\r\n\r\nAs far as the performance, my code is not fundamentally different from the original aside from synchronizing things correctly.  Keep in mind the old code was really \"fast\" but it was wrong because it was running a duplicate generator in each process (since with multiprocess you are forking and creating a copy of the process memory, not shared memory as with threading), this returns duplicate values and caused the original `test_generator_enqueuer_processes` to fail... I'm guessing why it was disabled.  If I enable it (flip the assertion to be `assertEqual`) with the old code I get:\r\n\r\n```\r\nAssertionError: Lists differ: [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 5, 6,[320 chars], 30] != [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13[340 chars], 99]\r\n```\r\n\r\nWhich shows the duplicate results.\r\n\r\nI'm not sure how you  would parallelize the generator itself?  What you can for sure parallelize is any preprocessing of what the generator produces, which I'm guessing is what your sleep is meant to simulate?  Right now there is no hook for that, but it would be easy to add support for passing in a user defined function that encapsulates any preprocesing logic, and parallelize that across the workers.  What do you think?\r\n\r\nNoted about the CI time on tensorflow, I'll go ahead and open a separate PR on keras.\r\n"}