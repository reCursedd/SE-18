{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23870", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23870/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23870/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23870/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23870", "id": 382498550, "node_id": "MDU6SXNzdWUzODI0OTg1NTA=", "number": 23870, "title": "In implementation of ConvLSTM,I  want to know where is the w_ci dot C_t-1 in paper formula (3)", "user": {"login": "TolicWang", "id": 16443701, "node_id": "MDQ6VXNlcjE2NDQzNzAx", "avatar_url": "https://avatars3.githubusercontent.com/u/16443701?v=4", "gravatar_id": "", "url": "https://api.github.com/users/TolicWang", "html_url": "https://github.com/TolicWang", "followers_url": "https://api.github.com/users/TolicWang/followers", "following_url": "https://api.github.com/users/TolicWang/following{/other_user}", "gists_url": "https://api.github.com/users/TolicWang/gists{/gist_id}", "starred_url": "https://api.github.com/users/TolicWang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/TolicWang/subscriptions", "organizations_url": "https://api.github.com/users/TolicWang/orgs", "repos_url": "https://api.github.com/users/TolicWang/repos", "events_url": "https://api.github.com/users/TolicWang/events{/privacy}", "received_events_url": "https://api.github.com/users/TolicWang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-11-20T04:44:20Z", "updated_at": "2018-11-20T10:03:39Z", "closed_at": null, "author_association": "NONE", "body_html": "<pre><code>\n  def call(self, inputs, state, scope=None):\n    cell, hidden = state\n    new_hidden = _conv([inputs, hidden], self._kernel_shape,\n                       4 * self._output_channels, self._use_bias)\n    gates = array_ops.split(\n        value=new_hidden, num_or_size_splits=4, axis=self._conv_ndims + 1)\n\n    input_gate, new_input, forget_gate, output_gate = gates\n    new_cell = math_ops.sigmoid(forget_gate + self._forget_bias) * cell\n    new_cell += math_ops.sigmoid(input_gate) * math_ops.tanh(new_input)\n    output = math_ops.tanh(new_cell) * math_ops.sigmoid(output_gate)\n\n    if self._skip_connection:\n      output = array_ops.concat([output, inputs], axis=-1)\n    new_state = rnn_cell_impl.LSTMStateTuple(new_cell, output)\nreturn output, new_state\n</code></pre>\n<p>In this implementation, I wonder where is  w_ci dot C_t-1 appearing in paper formula (3)</p>", "body_text": "def call(self, inputs, state, scope=None):\n    cell, hidden = state\n    new_hidden = _conv([inputs, hidden], self._kernel_shape,\n                       4 * self._output_channels, self._use_bias)\n    gates = array_ops.split(\n        value=new_hidden, num_or_size_splits=4, axis=self._conv_ndims + 1)\n\n    input_gate, new_input, forget_gate, output_gate = gates\n    new_cell = math_ops.sigmoid(forget_gate + self._forget_bias) * cell\n    new_cell += math_ops.sigmoid(input_gate) * math_ops.tanh(new_input)\n    output = math_ops.tanh(new_cell) * math_ops.sigmoid(output_gate)\n\n    if self._skip_connection:\n      output = array_ops.concat([output, inputs], axis=-1)\n    new_state = rnn_cell_impl.LSTMStateTuple(new_cell, output)\nreturn output, new_state\n\nIn this implementation, I wonder where is  w_ci dot C_t-1 appearing in paper formula (3)", "body": "```\r\n\r\n  def call(self, inputs, state, scope=None):\r\n    cell, hidden = state\r\n    new_hidden = _conv([inputs, hidden], self._kernel_shape,\r\n                       4 * self._output_channels, self._use_bias)\r\n    gates = array_ops.split(\r\n        value=new_hidden, num_or_size_splits=4, axis=self._conv_ndims + 1)\r\n\r\n    input_gate, new_input, forget_gate, output_gate = gates\r\n    new_cell = math_ops.sigmoid(forget_gate + self._forget_bias) * cell\r\n    new_cell += math_ops.sigmoid(input_gate) * math_ops.tanh(new_input)\r\n    output = math_ops.tanh(new_cell) * math_ops.sigmoid(output_gate)\r\n\r\n    if self._skip_connection:\r\n      output = array_ops.concat([output, inputs], axis=-1)\r\n    new_state = rnn_cell_impl.LSTMStateTuple(new_cell, output)\r\nreturn output, new_state\r\n```\r\n\r\nIn this implementation, I wonder where is  w_ci dot C_t-1 appearing in paper formula (3)"}