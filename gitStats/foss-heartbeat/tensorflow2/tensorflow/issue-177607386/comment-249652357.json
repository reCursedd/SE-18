{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/249652357", "html_url": "https://github.com/tensorflow/tensorflow/issues/4427#issuecomment-249652357", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427", "id": 249652357, "node_id": "MDEyOklzc3VlQ29tbWVudDI0OTY1MjM1Nw==", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-26T18:18:09Z", "updated_at": "2016-09-26T18:19:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1500400\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Monnoroch\">@Monnoroch</a> to the best of my understanding of the code, the math described in the Magenta post does <em>not</em> match the math implemented in <code>AttentionCellWrapper</code>, though it does match the math in the paper you cited. Note that the original version of the Magenta code used its own version of attention, but was later changed to use <code>AttentionCellWrapper</code>. The math I wrote above should match the code of <code>AttentionCellWrapper</code>, unless I made a mistake somewhere.</p>", "body_text": "@Monnoroch to the best of my understanding of the code, the math described in the Magenta post does not match the math implemented in AttentionCellWrapper, though it does match the math in the paper you cited. Note that the original version of the Magenta code used its own version of attention, but was later changed to use AttentionCellWrapper. The math I wrote above should match the code of AttentionCellWrapper, unless I made a mistake somewhere.", "body": "@Monnoroch to the best of my understanding of the code, the math described in the Magenta post does _not_ match the math implemented in `AttentionCellWrapper`, though it does match the math in the paper you cited. Note that the original version of the Magenta code used its own version of attention, but was later changed to use `AttentionCellWrapper`. The math I wrote above should match the code of `AttentionCellWrapper`, unless I made a mistake somewhere.\n"}