{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4427", "id": 177607386, "node_id": "MDU6SXNzdWUxNzc2MDczODY=", "number": 4427, "title": "Clarify math or paper reference behind AttentionCellWrapper", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2016-09-17T20:41:02Z", "updated_at": "2017-12-21T15:04:43Z", "closed_at": "2017-01-27T20:00:47Z", "author_association": "NONE", "body_html": "<p>The documentation for <code>AttentionCellWrapper</code> in contrib states that it's based on this <a href=\"https://arxiv.org/abs/1601.06733\" rel=\"nofollow\">paper</a>. However, the attention mechanism in the paper appears to be distinct from that in <code>AttentionCellWrapper</code>. For example the TF implementation involves convolutions and requires a fixed attention window, neither of which are a feature of the referenced paper. I suggest that either the correct paper is referenced, or the description is updated to reflect the actual math being implemented. The biggest issue right now is that it's unclear what the various options really mean (<code>attn_length</code>, <code>attn_size</code>, <code>attn_vec_size</code>, etc) as their descriptions are very concise and without a paper reference to ground what the documentation is referring to, it's impossible to know what the attention mechanism is doing short of going through the code line-by-line.</p>", "body_text": "The documentation for AttentionCellWrapper in contrib states that it's based on this paper. However, the attention mechanism in the paper appears to be distinct from that in AttentionCellWrapper. For example the TF implementation involves convolutions and requires a fixed attention window, neither of which are a feature of the referenced paper. I suggest that either the correct paper is referenced, or the description is updated to reflect the actual math being implemented. The biggest issue right now is that it's unclear what the various options really mean (attn_length, attn_size, attn_vec_size, etc) as their descriptions are very concise and without a paper reference to ground what the documentation is referring to, it's impossible to know what the attention mechanism is doing short of going through the code line-by-line.", "body": "The documentation for `AttentionCellWrapper` in contrib states that it's based on this [paper](https://arxiv.org/abs/1601.06733). However, the attention mechanism in the paper appears to be distinct from that in `AttentionCellWrapper`. For example the TF implementation involves convolutions and requires a fixed attention window, neither of which are a feature of the referenced paper. I suggest that either the correct paper is referenced, or the description is updated to reflect the actual math being implemented. The biggest issue right now is that it's unclear what the various options really mean (`attn_length`, `attn_size`, `attn_vec_size`, etc) as their descriptions are very concise and without a paper reference to ground what the documentation is referring to, it's impossible to know what the attention mechanism is doing short of going through the code line-by-line.\n"}