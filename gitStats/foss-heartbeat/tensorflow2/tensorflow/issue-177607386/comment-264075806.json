{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264075806", "html_url": "https://github.com/tensorflow/tensorflow/issues/4427#issuecomment-264075806", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427", "id": 264075806, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDA3NTgwNg==", "user": {"login": "dillonalaird", "id": 3057947, "node_id": "MDQ6VXNlcjMwNTc5NDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3057947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dillonalaird", "html_url": "https://github.com/dillonalaird", "followers_url": "https://api.github.com/users/dillonalaird/followers", "following_url": "https://api.github.com/users/dillonalaird/following{/other_user}", "gists_url": "https://api.github.com/users/dillonalaird/gists{/gist_id}", "starred_url": "https://api.github.com/users/dillonalaird/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dillonalaird/subscriptions", "organizations_url": "https://api.github.com/users/dillonalaird/orgs", "repos_url": "https://api.github.com/users/dillonalaird/repos", "events_url": "https://api.github.com/users/dillonalaird/events{/privacy}", "received_events_url": "https://api.github.com/users/dillonalaird/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-01T04:26:48Z", "updated_at": "2016-12-01T04:26:48Z", "author_association": "NONE", "body_html": "<p>I haven't checked all the math but this implementation slices off the first encoded hidden state and appends on the most recent decoded hidden state which is not what's done in Bahdanau. Attention is typically used for translation tasks, this looks like a good technique for music generation but I think the class name and the paper reference are misleading.</p>", "body_text": "I haven't checked all the math but this implementation slices off the first encoded hidden state and appends on the most recent decoded hidden state which is not what's done in Bahdanau. Attention is typically used for translation tasks, this looks like a good technique for music generation but I think the class name and the paper reference are misleading.", "body": "I haven't checked all the math but this implementation slices off the first encoded hidden state and appends on the most recent decoded hidden state which is not what's done in Bahdanau. Attention is typically used for translation tasks, this looks like a good technique for music generation but I think the class name and the paper reference are misleading."}