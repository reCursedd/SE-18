{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/265046241", "html_url": "https://github.com/tensorflow/tensorflow/issues/4427#issuecomment-265046241", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427", "id": 265046241, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NTA0NjI0MQ==", "user": {"login": "dillonalaird", "id": 3057947, "node_id": "MDQ6VXNlcjMwNTc5NDc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3057947?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dillonalaird", "html_url": "https://github.com/dillonalaird", "followers_url": "https://api.github.com/users/dillonalaird/followers", "following_url": "https://api.github.com/users/dillonalaird/following{/other_user}", "gists_url": "https://api.github.com/users/dillonalaird/gists{/gist_id}", "starred_url": "https://api.github.com/users/dillonalaird/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dillonalaird/subscriptions", "organizations_url": "https://api.github.com/users/dillonalaird/orgs", "repos_url": "https://api.github.com/users/dillonalaird/repos", "events_url": "https://api.github.com/users/dillonalaird/events{/privacy}", "received_events_url": "https://api.github.com/users/dillonalaird/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-06T02:52:23Z", "updated_at": "2016-12-06T02:52:23Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1547979\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jessedaniels\">@jessedaniels</a> The author in that post gives an incorrect explanation of the conv2d operation. The conv2d is just a clever trick to calculate W*h_t over all time steps. You can see the original code where he got that from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L589\">here</a>. You can also verify it yourself by running:</p>\n<div class=\"highlight highlight-source-python\"><pre>[tf.matmul(tf.squeeze(ai, [<span class=\"pl-c1\">1</span>]), tf.squeeze(k, [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">1</span>])) <span class=\"pl-k\">for</span> ai <span class=\"pl-k\">in</span> tf.split(<span class=\"pl-c1\">1</span>, attn_length, attention_states)]</pre></div>\n<p>on some random matrices and seeing that you get the same values.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1500400\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Monnoroch\">@Monnoroch</a> The issue with AttentionCellWrapper is <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1129\">here</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1101\">here</a>. This is slicing off the oldest encoded hidden state and appending the newest decoding hidden state and then passing those to the next time step. This means that for translation tasks, after you decode the first word you would never be able to run attention over the first encoded word again. Which is not what's done in Bahdanau.</p>", "body_text": "@jessedaniels The author in that post gives an incorrect explanation of the conv2d operation. The conv2d is just a clever trick to calculate W*h_t over all time steps. You can see the original code where he got that from here. You can also verify it yourself by running:\n[tf.matmul(tf.squeeze(ai, [1]), tf.squeeze(k, [0,1])) for ai in tf.split(1, attn_length, attention_states)]\non some random matrices and seeing that you get the same values.\n@Monnoroch The issue with AttentionCellWrapper is here and here. This is slicing off the oldest encoded hidden state and appending the newest decoding hidden state and then passing those to the next time step. This means that for translation tasks, after you decode the first word you would never be able to run attention over the first encoded word again. Which is not what's done in Bahdanau.", "body": "@jessedaniels The author in that post gives an incorrect explanation of the conv2d operation. The conv2d is just a clever trick to calculate W*h_t over all time steps. You can see the original code where he got that from [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L589). You can also verify it yourself by running:\r\n```python\r\n[tf.matmul(tf.squeeze(ai, [1]), tf.squeeze(k, [0,1])) for ai in tf.split(1, attn_length, attention_states)]\r\n```\r\non some random matrices and seeing that you get the same values.\r\n\r\n@Monnoroch The issue with AttentionCellWrapper is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1129) and [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/rnn/python/ops/rnn_cell.py#L1101). This is slicing off the oldest encoded hidden state and appending the newest decoding hidden state and then passing those to the next time step. This means that for translation tasks, after you decode the first word you would never be able to run attention over the first encoded word again. Which is not what's done in Bahdanau. "}