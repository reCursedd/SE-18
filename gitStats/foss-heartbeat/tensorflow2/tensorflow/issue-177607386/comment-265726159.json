{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/265726159", "html_url": "https://github.com/tensorflow/tensorflow/issues/4427#issuecomment-265726159", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4427", "id": 265726159, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NTcyNjE1OQ==", "user": {"login": "kho", "id": 460372, "node_id": "MDQ6VXNlcjQ2MDM3Mg==", "avatar_url": "https://avatars2.githubusercontent.com/u/460372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kho", "html_url": "https://github.com/kho", "followers_url": "https://api.github.com/users/kho/followers", "following_url": "https://api.github.com/users/kho/following{/other_user}", "gists_url": "https://api.github.com/users/kho/gists{/gist_id}", "starred_url": "https://api.github.com/users/kho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kho/subscriptions", "organizations_url": "https://api.github.com/users/kho/orgs", "repos_url": "https://api.github.com/users/kho/repos", "events_url": "https://api.github.com/users/kho/events{/privacy}", "received_events_url": "https://api.github.com/users/kho/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-08T12:12:36Z", "updated_at": "2016-12-08T12:12:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1500400\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Monnoroch\">@Monnoroch</a> Another problem with AttentionCellWrapper is that the conv2d operation, which in the cited paper can be precomputed because the hidden states (input context) remain unchanged throughout the recurrent steps, is now repeated each time, where (1-1/attn_length) of the computation is not necessary.</p>", "body_text": "@Monnoroch Another problem with AttentionCellWrapper is that the conv2d operation, which in the cited paper can be precomputed because the hidden states (input context) remain unchanged throughout the recurrent steps, is now repeated each time, where (1-1/attn_length) of the computation is not necessary.", "body": "@Monnoroch Another problem with AttentionCellWrapper is that the conv2d operation, which in the cited paper can be precomputed because the hidden states (input context) remain unchanged throughout the recurrent steps, is now repeated each time, where (1-1/attn_length) of the computation is not necessary."}