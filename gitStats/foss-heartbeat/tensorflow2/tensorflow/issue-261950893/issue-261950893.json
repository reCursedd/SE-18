{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13436", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13436/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13436/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13436/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13436", "id": 261950893, "node_id": "MDU6SXNzdWUyNjE5NTA4OTM=", "number": 13436, "title": "\"Variable rnn/basic_rnn_cell/kernel already exists, disallowed.\" error while defining dynamic_rnn", "user": {"login": "HiFromGops", "id": 10350149, "node_id": "MDQ6VXNlcjEwMzUwMTQ5", "avatar_url": "https://avatars0.githubusercontent.com/u/10350149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HiFromGops", "html_url": "https://github.com/HiFromGops", "followers_url": "https://api.github.com/users/HiFromGops/followers", "following_url": "https://api.github.com/users/HiFromGops/following{/other_user}", "gists_url": "https://api.github.com/users/HiFromGops/gists{/gist_id}", "starred_url": "https://api.github.com/users/HiFromGops/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HiFromGops/subscriptions", "organizations_url": "https://api.github.com/users/HiFromGops/orgs", "repos_url": "https://api.github.com/users/HiFromGops/repos", "events_url": "https://api.github.com/users/HiFromGops/events{/privacy}", "received_events_url": "https://api.github.com/users/HiFromGops/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 12, "created_at": "2017-10-01T22:13:59Z", "updated_at": "2018-06-02T07:18:09Z", "closed_at": "2018-06-02T07:05:11Z", "author_association": "NONE", "body_html": "<p>I was writing a simple code to define an RNN and the code goes thus:</p>\n<pre><code>n_steps = 28\nn_inputs = 28\nn_neurons = 150\nn_outputs = 10\nn_epochs = 100\nbatch_sz = 150\nl_rate = 0.001\n\nX0 = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\nY0 = tf.placeholder(tf.int32, [None])\ninit_state = tf.zeros([n_steps, n_inputs])\n\nbasic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\nouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\n\nlogits = layers.fully_connected(states, n_outputs, activation_fn = None)\n</code></pre>\n<p>Executing the above code gave the below error with traceback:</p>\n<pre><code>&gt; ---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-67-05674d7f7864&gt; in &lt;module&gt;()\n     16 \n     17 basic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\n---&gt; 18 ouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\n     19 \n     20 logits = layers.fully_connected(states, n_outputs, activation_fn = None)\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n    572         swap_memory=swap_memory,\n    573         sequence_length=sequence_length,\n--&gt; 574         dtype=dtype)\n    575 \n    576     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n    735       loop_vars=(time, output_ta, state),\n    736       parallel_iterations=parallel_iterations,\n--&gt; 737       swap_memory=swap_memory)\n    738 \n    739   # Unpack final output if not using output tuples.\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\n   2768     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\n   2769     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\n-&gt; 2770     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   2771     return result\n   2772 \n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n   2597       self.Enter()\n   2598       original_body_result, exit_vars = self._BuildLoop(\n-&gt; 2599           pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2600     finally:\n   2601       self.Exit()\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2547         structure=original_loop_vars,\n   2548         flat_sequence=vars_for_body_with_tensor_arrays)\n-&gt; 2549     body_result = body(*packed_vars_for_body)\n   2550     if not nest.is_sequence(body_result):\n   2551       body_result = [body_result]\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _time_step(time, output_ta_t, state)\n    720           skip_conditionals=True)\n    721     else:\n--&gt; 722       (output, new_state) = call_cell()\n    723 \n    724     # Pack state if using state tuples\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in &lt;lambda&gt;()\n    706 \n    707     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n--&gt; 708     call_cell = lambda: cell(input_t, state)\n    709 \n    710     if sequence_length is not None:\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    178       with vs.variable_scope(vs.get_variable_scope(),\n    179                              custom_getter=self._rnn_get_variable):\n--&gt; 180         return super(RNNCell, self).__call__(inputs, state)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\n    439         # Check input assumptions set after layer building, e.g. input shape.\n    440         self._assert_input_compatibility(inputs)\n--&gt; 441         outputs = self.call(inputs, *args, **kwargs)\n    442 \n    443         # Apply activity regularization.\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\n    256   def call(self, inputs, state):\n    257     \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n--&gt; 258     output = self._activation(_linear([inputs, state], self._num_units, True))\n    259     return output, output\n    260 \n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)\n   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n   1016         dtype=dtype,\n-&gt; 1017         initializer=kernel_initializer)\n   1018     if len(args) == 1:\n   1019       res = math_ops.matmul(args[0], weights)\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n   1063       collections=collections, caching_device=caching_device,\n   1064       partitioner=partitioner, validate_shape=validate_shape,\n-&gt; 1065       use_resource=use_resource, custom_getter=custom_getter)\n   1066 get_variable_or_local_docstring = (\n   1067     \"\"\"%s\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n    960           collections=collections, caching_device=caching_device,\n    961           partitioner=partitioner, validate_shape=validate_shape,\n--&gt; 962           use_resource=use_resource, custom_getter=custom_getter)\n    963 \n    964   def _get_partitioned_variable(self,\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n    358           reuse=reuse, trainable=trainable, collections=collections,\n    359           caching_device=caching_device, partitioner=partitioner,\n--&gt; 360           validate_shape=validate_shape, use_resource=use_resource)\n    361     else:\n    362       return _true_getter(\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n--&gt; 183     variable = getter(*args, **kwargs)\n    184     trainable = (variable in tf_variables.trainable_variables() or\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\n    350           trainable=trainable, collections=collections,\n    351           caching_device=caching_device, validate_shape=validate_shape,\n--&gt; 352           use_resource=use_resource)\n    353 \n    354     if custom_getter is not None:\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\n    662                          \" Did you mean to set reuse=True in VarScope? \"\n    663                          \"Originally defined at:\\n\\n%s\" % (\n--&gt; 664                              name, \"\".join(traceback.format_list(tb))))\n    665       found_var = self._vars[name]\n    666       if not shape.is_compatible_with(found_var.get_shape()):\n\nValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)\n</code></pre>", "body_text": "I was writing a simple code to define an RNN and the code goes thus:\nn_steps = 28\nn_inputs = 28\nn_neurons = 150\nn_outputs = 10\nn_epochs = 100\nbatch_sz = 150\nl_rate = 0.001\n\nX0 = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\nY0 = tf.placeholder(tf.int32, [None])\ninit_state = tf.zeros([n_steps, n_inputs])\n\nbasic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\nouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\n\nlogits = layers.fully_connected(states, n_outputs, activation_fn = None)\n\nExecuting the above code gave the below error with traceback:\n> ---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-67-05674d7f7864> in <module>()\n     16 \n     17 basic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\n---> 18 ouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\n     19 \n     20 logits = layers.fully_connected(states, n_outputs, activation_fn = None)\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n    572         swap_memory=swap_memory,\n    573         sequence_length=sequence_length,\n--> 574         dtype=dtype)\n    575 \n    576     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n    735       loop_vars=(time, output_ta, state),\n    736       parallel_iterations=parallel_iterations,\n--> 737       swap_memory=swap_memory)\n    738 \n    739   # Unpack final output if not using output tuples.\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\n   2768     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\n   2769     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\n-> 2770     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   2771     return result\n   2772 \n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n   2597       self.Enter()\n   2598       original_body_result, exit_vars = self._BuildLoop(\n-> 2599           pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2600     finally:\n   2601       self.Exit()\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2547         structure=original_loop_vars,\n   2548         flat_sequence=vars_for_body_with_tensor_arrays)\n-> 2549     body_result = body(*packed_vars_for_body)\n   2550     if not nest.is_sequence(body_result):\n   2551       body_result = [body_result]\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _time_step(time, output_ta_t, state)\n    720           skip_conditionals=True)\n    721     else:\n--> 722       (output, new_state) = call_cell()\n    723 \n    724     # Pack state if using state tuples\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in <lambda>()\n    706 \n    707     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n--> 708     call_cell = lambda: cell(input_t, state)\n    709 \n    710     if sequence_length is not None:\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\n    178       with vs.variable_scope(vs.get_variable_scope(),\n    179                              custom_getter=self._rnn_get_variable):\n--> 180         return super(RNNCell, self).__call__(inputs, state)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\n    439         # Check input assumptions set after layer building, e.g. input shape.\n    440         self._assert_input_compatibility(inputs)\n--> 441         outputs = self.call(inputs, *args, **kwargs)\n    442 \n    443         # Apply activity regularization.\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\n    256   def call(self, inputs, state):\n    257     \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\n--> 258     output = self._activation(_linear([inputs, state], self._num_units, True))\n    259     return output, output\n    260 \n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)\n   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n   1016         dtype=dtype,\n-> 1017         initializer=kernel_initializer)\n   1018     if len(args) == 1:\n   1019       res = math_ops.matmul(args[0], weights)\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n   1063       collections=collections, caching_device=caching_device,\n   1064       partitioner=partitioner, validate_shape=validate_shape,\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\n   1066 get_variable_or_local_docstring = (\n   1067     \"\"\"%s\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n    960           collections=collections, caching_device=caching_device,\n    961           partitioner=partitioner, validate_shape=validate_shape,\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\n    963 \n    964   def _get_partitioned_variable(self,\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\n    358           reuse=reuse, trainable=trainable, collections=collections,\n    359           caching_device=caching_device, partitioner=partitioner,\n--> 360           validate_shape=validate_shape, use_resource=use_resource)\n    361     else:\n    362       return _true_getter(\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\n    181 \n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\n--> 183     variable = getter(*args, **kwargs)\n    184     trainable = (variable in tf_variables.trainable_variables() or\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\n    350           trainable=trainable, collections=collections,\n    351           caching_device=caching_device, validate_shape=validate_shape,\n--> 352           use_resource=use_resource)\n    353 \n    354     if custom_getter is not None:\n\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\n    662                          \" Did you mean to set reuse=True in VarScope? \"\n    663                          \"Originally defined at:\\n\\n%s\" % (\n--> 664                              name, \"\".join(traceback.format_list(tb))))\n    665       found_var = self._vars[name]\n    666       if not shape.is_compatible_with(found_var.get_shape()):\n\nValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\n\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\n    self._traceback = _extract_stack()\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\n    op_def=op_def)", "body": "I was writing a simple code to define an RNN and the code goes thus:\r\n\r\n```\r\nn_steps = 28\r\nn_inputs = 28\r\nn_neurons = 150\r\nn_outputs = 10\r\nn_epochs = 100\r\nbatch_sz = 150\r\nl_rate = 0.001\r\n\r\nX0 = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\r\nY0 = tf.placeholder(tf.int32, [None])\r\ninit_state = tf.zeros([n_steps, n_inputs])\r\n\r\nbasic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\r\nouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\r\n\r\nlogits = layers.fully_connected(states, n_outputs, activation_fn = None)\r\n```\r\n\r\nExecuting the above code gave the below error with traceback:\r\n\r\n```\r\n> ---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-67-05674d7f7864> in <module>()\r\n     16 \r\n     17 basic_r_cell = rnn.BasicRNNCell(num_units = n_neurons)\r\n---> 18 ouputs, states = tf.nn.dynamic_rnn(basic_r_cell, X0, initial_state = init_state)\r\n     19 \r\n     20 logits = layers.fully_connected(states, n_outputs, activation_fn = None)\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    572         swap_memory=swap_memory,\r\n    573         sequence_length=sequence_length,\r\n--> 574         dtype=dtype)\r\n    575 \r\n    576     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    735       loop_vars=(time, output_ta, state),\r\n    736       parallel_iterations=parallel_iterations,\r\n--> 737       swap_memory=swap_memory)\r\n    738 \r\n    739   # Unpack final output if not using output tuples.\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2768     context = WhileContext(parallel_iterations, back_prop, swap_memory, name)\r\n   2769     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, context)\r\n-> 2770     result = context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2771     return result\r\n   2772 \r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2597       self.Enter()\r\n   2598       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2599           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2600     finally:\r\n   2601       self.Exit()\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\control_flow_ops.py in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2547         structure=original_loop_vars,\r\n   2548         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2549     body_result = body(*packed_vars_for_body)\r\n   2550     if not nest.is_sequence(body_result):\r\n   2551       body_result = [body_result]\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in _time_step(time, output_ta_t, state)\r\n    720           skip_conditionals=True)\r\n    721     else:\r\n--> 722       (output, new_state) = call_cell()\r\n    723 \r\n    724     # Pack state if using state tuples\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py in <lambda>()\r\n    706 \r\n    707     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 708     call_cell = lambda: cell(input_t, state)\r\n    709 \r\n    710     if sequence_length is not None:\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in __call__(self, inputs, state, scope)\r\n    178       with vs.variable_scope(vs.get_variable_scope(),\r\n    179                              custom_getter=self._rnn_get_variable):\r\n--> 180         return super(RNNCell, self).__call__(inputs, state)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\layers\\base.py in __call__(self, inputs, *args, **kwargs)\r\n    439         # Check input assumptions set after layer building, e.g. input shape.\r\n    440         self._assert_input_compatibility(inputs)\r\n--> 441         outputs = self.call(inputs, *args, **kwargs)\r\n    442 \r\n    443         # Apply activity regularization.\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in call(self, inputs, state)\r\n    256   def call(self, inputs, state):\r\n    257     \"\"\"Most basic RNN: output = new_state = act(W * input + U * state + B).\"\"\"\r\n--> 258     output = self._activation(_linear([inputs, state], self._num_units, True))\r\n    259     return output, output\r\n    260 \r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _linear(args, output_size, bias, bias_initializer, kernel_initializer)\r\n   1015         _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\r\n   1016         dtype=dtype,\r\n-> 1017         initializer=kernel_initializer)\r\n   1018     if len(args) == 1:\r\n   1019       res = math_ops.matmul(args[0], weights)\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n   1063       collections=collections, caching_device=caching_device,\r\n   1064       partitioner=partitioner, validate_shape=validate_shape,\r\n-> 1065       use_resource=use_resource, custom_getter=custom_getter)\r\n   1066 get_variable_or_local_docstring = (\r\n   1067     \"\"\"%s\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    960           collections=collections, caching_device=caching_device,\r\n    961           partitioner=partitioner, validate_shape=validate_shape,\r\n--> 962           use_resource=use_resource, custom_getter=custom_getter)\r\n    963 \r\n    964   def _get_partitioned_variable(self,\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter)\r\n    358           reuse=reuse, trainable=trainable, collections=collections,\r\n    359           caching_device=caching_device, partitioner=partitioner,\r\n--> 360           validate_shape=validate_shape, use_resource=use_resource)\r\n    361     else:\r\n    362       return _true_getter(\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    181 \r\n    182   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 183     variable = getter(*args, **kwargs)\r\n    184     trainable = (variable in tf_variables.trainable_variables() or\r\n    185                  (isinstance(variable, tf_variables.PartitionedVariable) and\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource)\r\n    350           trainable=trainable, collections=collections,\r\n    351           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 352           use_resource=use_resource)\r\n    353 \r\n    354     if custom_getter is not None:\r\n\r\nc:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\variable_scope.py in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource)\r\n    662                          \" Did you mean to set reuse=True in VarScope? \"\r\n    663                          \"Originally defined at:\\n\\n%s\" % (\r\n--> 664                              name, \"\".join(traceback.format_list(tb))))\r\n    665       found_var = self._vars[name]\r\n    666       if not shape.is_compatible_with(found_var.get_shape()):\r\n\r\nValueError: Variable rnn/basic_rnn_cell/kernel already exists, disallowed. Did you mean to set reuse=True in VarScope? Originally defined at:\r\n\r\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"c:\\users\\antunnug\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n```"}