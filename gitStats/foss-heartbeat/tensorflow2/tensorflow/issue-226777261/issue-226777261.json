{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9724", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9724/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9724/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9724/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9724", "id": 226777261, "node_id": "MDU6SXNzdWUyMjY3NzcyNjE=", "number": 9724, "title": "Freeze_graph results in very poor accuracy compared to manually exporting and freezing the graph?", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2017-05-06T14:48:17Z", "updated_at": "2018-11-14T11:44:19Z", "closed_at": "2017-10-17T01:33:28Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/5.1</li>\n<li><strong>GPU model and memory</strong>: GTX 860M</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>From using the official freeze_graph.py file from TF, I am getting a very low accuracy in prediction as compared to manually exporting the graph using a file I wrote called <code>write_pb.py</code>, I get a much higher accuracy.</p>\n<p>To be specific, here are the differences:</p>\n<p><strong>Differences:</strong></p>\n<ol>\n<li>Using <code>write_pb.py</code> to manually export the graph converted way more variables to constants, even with the same checkpoint files.</li>\n<li>It takes a long, long time for <code>freeze_graph.py</code> to actually complete the exporting.</li>\n<li>Very importantly: I get a very low accuracy from using freeze_graph. Meanwhile, by exporting the graph manually, I get a nearly identical accuracy as if I predicted an image right from the checkpoint without freezing.</li>\n<li>Manually exporting the graph results in a smaller file size (just 1-2MB of difference).</li>\n<li>The manually exported graph has a faster inference time than the graph obtained from <code>freeze_graph.py</code>.</li>\n</ol>\n<p>This is the freeze_graph.py file I got from the TF repo: <a href=\"https://gist.github.com/kwotsin/b9dae8246a30371a1a10690e2fa27cb7\">https://gist.github.com/kwotsin/b9dae8246a30371a1a10690e2fa27cb7</a></p>\n<p>This is the <code>write_pb</code> file I wrote:<br>\n<a href=\"https://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b\">https://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b</a></p>\n<h3>Source code / logs</h3>\n<p>Using freeze_graph with this command: <code>python freeze_graph.py --input_checkpoint=model.ckpt-18279 --input_graph=graph.pbtxt --output_graph=frozen_model_from_freeze_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions</code></p>\n<p>I get this output:</p>\n<pre><code>2017-05-06 22:27:22.967898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-05-06 22:27:22.968268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX 860M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 3.34GiB\n2017-05-06 22:27:22.968283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-06 22:27:22.968288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-06 22:27:22.968306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\n2017-05-06 22:27:33.499491: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:27:33.499519: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:27:33.509659: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ced220 executing computations on platform Host. Devices:\n2017-05-06 22:27:33.509678: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-06 22:27:33.509827: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:27:33.509837: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:27:33.512265: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ce6210 executing computations on platform CUDA. Devices:\n2017-05-06 22:27:33.512279: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\nConverted 490 variables to const ops.\n7871 ops in the final graph.\n</code></pre>\n<p>Meanwhile, if I manually freeze the graph using <code>write_pb.py</code>,</p>\n<p>I get this output:</p>\n<pre><code>2017-05-06 22:39:00.197711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-05-06 22:39:00.198096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX 860M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 3.29GiB\n2017-05-06 22:39:00.198120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-06 22:39:00.198125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-06 22:39:00.198143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\n2017-05-06 22:39:00.709417: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:39:00.709452: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:39:00.710238: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef16320 executing computations on platform Host. Devices:\n2017-05-06 22:39:00.710252: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-06 22:39:00.710374: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:39:00.710384: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:39:00.710645: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef18180 executing computations on platform CUDA. Devices:\n2017-05-06 22:39:00.710654: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\nExporting graph...\nConverted 898 variables to const ops.\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04 LTS\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.1\nBazel version (if compiling from source): 4.5\nCUDA/cuDNN version: 8.0/5.1\nGPU model and memory: GTX 860M\n\nDescribe the problem\nFrom using the official freeze_graph.py file from TF, I am getting a very low accuracy in prediction as compared to manually exporting the graph using a file I wrote called write_pb.py, I get a much higher accuracy.\nTo be specific, here are the differences:\nDifferences:\n\nUsing write_pb.py to manually export the graph converted way more variables to constants, even with the same checkpoint files.\nIt takes a long, long time for freeze_graph.py to actually complete the exporting.\nVery importantly: I get a very low accuracy from using freeze_graph. Meanwhile, by exporting the graph manually, I get a nearly identical accuracy as if I predicted an image right from the checkpoint without freezing.\nManually exporting the graph results in a smaller file size (just 1-2MB of difference).\nThe manually exported graph has a faster inference time than the graph obtained from freeze_graph.py.\n\nThis is the freeze_graph.py file I got from the TF repo: https://gist.github.com/kwotsin/b9dae8246a30371a1a10690e2fa27cb7\nThis is the write_pb file I wrote:\nhttps://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b\nSource code / logs\nUsing freeze_graph with this command: python freeze_graph.py --input_checkpoint=model.ckpt-18279 --input_graph=graph.pbtxt --output_graph=frozen_model_from_freeze_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions\nI get this output:\n2017-05-06 22:27:22.967898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-05-06 22:27:22.968268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX 860M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 3.34GiB\n2017-05-06 22:27:22.968283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-06 22:27:22.968288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-06 22:27:22.968306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\n2017-05-06 22:27:33.499491: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:27:33.499519: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:27:33.509659: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ced220 executing computations on platform Host. Devices:\n2017-05-06 22:27:33.509678: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\n2017-05-06 22:27:33.509827: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:27:33.509837: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:27:33.512265: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ce6210 executing computations on platform CUDA. Devices:\n2017-05-06 22:27:33.512279: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\nConverted 490 variables to const ops.\n7871 ops in the final graph.\n\nMeanwhile, if I manually freeze the graph using write_pb.py,\nI get this output:\n2017-05-06 22:39:00.197711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-05-06 22:39:00.198096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX 860M\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\npciBusID 0000:01:00.0\nTotal memory: 3.95GiB\nFree memory: 3.29GiB\n2017-05-06 22:39:00.198120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \n2017-05-06 22:39:00.198125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \n2017-05-06 22:39:00.198143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\n2017-05-06 22:39:00.709417: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:39:00.709452: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:39:00.710238: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef16320 executing computations on platform Host. Devices:\n2017-05-06 22:39:00.710252: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\n2017-05-06 22:39:00.710374: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\n2017-05-06 22:39:00.710384: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-06 22:39:00.710645: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef18180 executing computations on platform CUDA. Devices:\n2017-05-06 22:39:00.710654: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\nExporting graph...\nConverted 898 variables to const ops.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: 4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX 860M\r\n\r\n### Describe the problem\r\nFrom using the official freeze_graph.py file from TF, I am getting a very low accuracy in prediction as compared to manually exporting the graph using a file I wrote called `write_pb.py`, I get a much higher accuracy.\r\n\r\nTo be specific, here are the differences:\r\n\r\n**Differences:**\r\n1. Using `write_pb.py` to manually export the graph converted way more variables to constants, even with the same checkpoint files.\r\n2.  It takes a long, long time for `freeze_graph.py` to actually complete the exporting.\r\n3. Very importantly: I get a very low accuracy from using freeze_graph. Meanwhile, by exporting the graph manually, I get a nearly identical accuracy as if I predicted an image right from the checkpoint without freezing.\r\n4. Manually exporting the graph results in a smaller file size (just 1-2MB of difference).\r\n5. The manually exported graph has a faster inference time than the graph obtained from `freeze_graph.py`.\r\n\r\nThis is the freeze_graph.py file I got from the TF repo: https://gist.github.com/kwotsin/b9dae8246a30371a1a10690e2fa27cb7\r\n\r\nThis is the `write_pb` file I wrote:\r\nhttps://gist.github.com/kwotsin/8e43f5db4815e1f1af37da70d0933d8b\r\n\r\n\r\n### Source code / logs\r\nUsing freeze_graph with this command: `python freeze_graph.py --input_checkpoint=model.ckpt-18279 --input_graph=graph.pbtxt --output_graph=frozen_model_from_freeze_graph.pb --output_node_names=InceptionResnetV2/Logits/Predictions`\r\n\r\nI get this output:\r\n```\r\n2017-05-06 22:27:22.967898: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-06 22:27:22.968268: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 860M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.34GiB\r\n2017-05-06 22:27:22.968283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-06 22:27:22.968288: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-06 22:27:22.968306: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\r\n2017-05-06 22:27:33.499491: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:27:33.499519: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:27:33.509659: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ced220 executing computations on platform Host. Devices:\r\n2017-05-06 22:27:33.509678: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-06 22:27:33.509827: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:27:33.509837: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:27:33.512265: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0x36ce6210 executing computations on platform CUDA. Devices:\r\n2017-05-06 22:27:33.512279: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\r\nConverted 490 variables to const ops.\r\n7871 ops in the final graph.\r\n```\r\nMeanwhile, if I manually freeze the graph using `write_pb.py`,\r\n\r\nI get this output:\r\n```\r\n2017-05-06 22:39:00.197711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-05-06 22:39:00.198096: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 860M\r\nmajor: 5 minor: 0 memoryClockRate (GHz) 1.0195\r\npciBusID 0000:01:00.0\r\nTotal memory: 3.95GiB\r\nFree memory: 3.29GiB\r\n2017-05-06 22:39:00.198120: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 \r\n2017-05-06 22:39:00.198125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y \r\n2017-05-06 22:39:00.198143: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0)\r\n2017-05-06 22:39:00.709417: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:39:00.709452: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:39:00.710238: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef16320 executing computations on platform Host. Devices:\r\n2017-05-06 22:39:00.710252: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-06 22:39:00.710374: I tensorflow/compiler/xla/service/platform_util.cc:58] platform CUDA present with 1 visible devices\r\n2017-05-06 22:39:00.710384: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-06 22:39:00.710645: I tensorflow/compiler/xla/service/service.cc:183] XLA service 0xef18180 executing computations on platform CUDA. Devices:\r\n2017-05-06 22:39:00.710654: I tensorflow/compiler/xla/service/service.cc:191]   StreamExecutor device (0): GeForce GTX 860M, Compute Capability 5.0\r\nExporting graph...\r\nConverted 898 variables to const ops.\r\n```\r\n\r\n\r\n\r\n\r\n\r\n"}