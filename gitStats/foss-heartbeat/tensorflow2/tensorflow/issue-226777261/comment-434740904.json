{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/434740904", "html_url": "https://github.com/tensorflow/tensorflow/issues/9724#issuecomment-434740904", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9724", "id": 434740904, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNDc0MDkwNA==", "user": {"login": "ktiwary2", "id": 25421727, "node_id": "MDQ6VXNlcjI1NDIxNzI3", "avatar_url": "https://avatars2.githubusercontent.com/u/25421727?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ktiwary2", "html_url": "https://github.com/ktiwary2", "followers_url": "https://api.github.com/users/ktiwary2/followers", "following_url": "https://api.github.com/users/ktiwary2/following{/other_user}", "gists_url": "https://api.github.com/users/ktiwary2/gists{/gist_id}", "starred_url": "https://api.github.com/users/ktiwary2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ktiwary2/subscriptions", "organizations_url": "https://api.github.com/users/ktiwary2/orgs", "repos_url": "https://api.github.com/users/ktiwary2/repos", "events_url": "https://api.github.com/users/ktiwary2/events{/privacy}", "received_events_url": "https://api.github.com/users/ktiwary2/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-31T15:51:23Z", "updated_at": "2018-10-31T15:51:23Z", "author_association": "NONE", "body_html": "<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=34572512\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ambr89\">@ambr89</a> : I fixed it by importing meta graph and applying transformations on graph with TransformGraph.</p>\n<pre><code>input_checkpoint = \"chckpoint path\"\nwith tf.Session(graph=tf.Graph()) as sess:\n        # We import the meta graph in the current default Graph\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n\n        # We restore the weights\n        saver.restore(sess, input_checkpoint)\n\n        from tensorflow.tools.graph_transforms import TransformGraph\n        transforms = ['add_default_attributes',\n                      'remove_nodes(op=Identity, op=CheckNumerics)',\n                      'fold_batch_norms', 'fold_old_batch_norms',\n                      'strip_unused_nodes', 'sort_by_execution_order']\n        transformed_graph_def = TransformGraph(tf.get_default_graph().as_graph_def(),'Placeholder', output_node_names.split(\",\"), transforms)\n\n# We use a built-in TF helper to export variables to constants\n       output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,  # The session is used to retrieve the weights\n            transformed_graph_def,  # The graph_def is used to retrieve the nodes\n            output_node_names.split(\",\")  # The output node names are used to select the useful nodes\n        )\n      with tf.gfile.GFile(\"optimised_model.pb\", \"wb\") as f:\n            f.write(output_graph_def.SerializeToString())\n</code></pre>\n<p>Change Placeholder to input node name and fill in your output nodes.</p>\n</blockquote>\n<p>Yes, you're right to do that. This particular function literally just converts all the variables into constants without doing any kind of transformations to it. This is a bug I found to mess completely with my accuracy as well. So, it expects a subgarph to be fed in with only those inputs and outputs that are necessary. I suggest using <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py\">this</a> since this strips the graphs of any Restore Tensor ops and file loading calls.</p>", "body_text": "@ambr89 : I fixed it by importing meta graph and applying transformations on graph with TransformGraph.\ninput_checkpoint = \"chckpoint path\"\nwith tf.Session(graph=tf.Graph()) as sess:\n        # We import the meta graph in the current default Graph\n        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n\n        # We restore the weights\n        saver.restore(sess, input_checkpoint)\n\n        from tensorflow.tools.graph_transforms import TransformGraph\n        transforms = ['add_default_attributes',\n                      'remove_nodes(op=Identity, op=CheckNumerics)',\n                      'fold_batch_norms', 'fold_old_batch_norms',\n                      'strip_unused_nodes', 'sort_by_execution_order']\n        transformed_graph_def = TransformGraph(tf.get_default_graph().as_graph_def(),'Placeholder', output_node_names.split(\",\"), transforms)\n\n# We use a built-in TF helper to export variables to constants\n       output_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess,  # The session is used to retrieve the weights\n            transformed_graph_def,  # The graph_def is used to retrieve the nodes\n            output_node_names.split(\",\")  # The output node names are used to select the useful nodes\n        )\n      with tf.gfile.GFile(\"optimised_model.pb\", \"wb\") as f:\n            f.write(output_graph_def.SerializeToString())\n\nChange Placeholder to input node name and fill in your output nodes.\n\nYes, you're right to do that. This particular function literally just converts all the variables into constants without doing any kind of transformations to it. This is a bug I found to mess completely with my accuracy as well. So, it expects a subgarph to be fed in with only those inputs and outputs that are necessary. I suggest using this since this strips the graphs of any Restore Tensor ops and file loading calls.", "body": "> @ambr89 : I fixed it by importing meta graph and applying transformations on graph with TransformGraph.\r\n> \r\n> ```\r\n> input_checkpoint = \"chckpoint path\"\r\n> with tf.Session(graph=tf.Graph()) as sess:\r\n>         # We import the meta graph in the current default Graph\r\n>         saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\r\n> \r\n>         # We restore the weights\r\n>         saver.restore(sess, input_checkpoint)\r\n> \r\n>         from tensorflow.tools.graph_transforms import TransformGraph\r\n>         transforms = ['add_default_attributes',\r\n>                       'remove_nodes(op=Identity, op=CheckNumerics)',\r\n>                       'fold_batch_norms', 'fold_old_batch_norms',\r\n>                       'strip_unused_nodes', 'sort_by_execution_order']\r\n>         transformed_graph_def = TransformGraph(tf.get_default_graph().as_graph_def(),'Placeholder', output_node_names.split(\",\"), transforms)\r\n> \r\n> # We use a built-in TF helper to export variables to constants\r\n>        output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n>             sess,  # The session is used to retrieve the weights\r\n>             transformed_graph_def,  # The graph_def is used to retrieve the nodes\r\n>             output_node_names.split(\",\")  # The output node names are used to select the useful nodes\r\n>         )\r\n>       with tf.gfile.GFile(\"optimised_model.pb\", \"wb\") as f:\r\n>             f.write(output_graph_def.SerializeToString())\r\n> ```\r\n> Change Placeholder to input node name and fill in your output nodes.\r\n\r\nYes, you're right to do that. This particular function literally just converts all the variables into constants without doing any kind of transformations to it. This is a bug I found to mess completely with my accuracy as well. So, it expects a subgarph to be fed in with only those inputs and outputs that are necessary. I suggest using [this](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/freeze_graph.py) since this strips the graphs of any Restore Tensor ops and file loading calls. "}