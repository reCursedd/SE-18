{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13325", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13325/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13325/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13325/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/13325", "id": 260804193, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQzMjY4NzYz", "number": 13325, "title": "Give accumulate_n op a gradient (version 2)", "user": {"login": "frreiss", "id": 12436991, "node_id": "MDQ6VXNlcjEyNDM2OTkx", "avatar_url": "https://avatars1.githubusercontent.com/u/12436991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frreiss", "html_url": "https://github.com/frreiss", "followers_url": "https://api.github.com/users/frreiss/followers", "following_url": "https://api.github.com/users/frreiss/following{/other_user}", "gists_url": "https://api.github.com/users/frreiss/gists{/gist_id}", "starred_url": "https://api.github.com/users/frreiss/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frreiss/subscriptions", "organizations_url": "https://api.github.com/users/frreiss/orgs", "repos_url": "https://api.github.com/users/frreiss/repos", "events_url": "https://api.github.com/users/frreiss/events{/privacy}", "received_events_url": "https://api.github.com/users/frreiss/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 419840263, "node_id": "MDU6TGFiZWw0MTk4NDAyNjM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20testing%20(then%20merge)", "name": "awaiting testing (then merge)", "color": "c2e0c6", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-09-26T23:50:42Z", "updated_at": "2018-10-05T18:41:47Z", "closed_at": "2017-10-05T21:22:27Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13325", "html_url": "https://github.com/tensorflow/tensorflow/pull/13325", "diff_url": "https://github.com/tensorflow/tensorflow/pull/13325.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/13325.patch"}, "body_html": "<p>This pull request is a restructuring of the changes in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"257528157\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13022\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/13022/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/13022\">#13022</a> to address high-level comments. Two major changes to call out vis a vis the previous PR:</p>\n<ul>\n<li>Instead of replacing <code>accumulate_n</code>, this PR now adds a new op, <code>accumulate_n_v2</code>. This new op is defined under <code>contrib/framework</code>.</li>\n<li>Refactoring changes have been broken out into a separate PR, <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"258895052\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/13159\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/13159/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/13159\">#13159</a></li>\n</ul>\n<p>Overall, this pull request addresses issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"234982067\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/10607\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/10607/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/10607\">#10607</a> by adding a gradient to the existing <code>accumulate_n</code> operator. I followed the approach suggested by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a>: rewrite <code>accumulate_n</code> as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.</p>\n<p><strong>Implementation Details</strong><br>\nI have added a new C++ op, <code>AccumulateNV2</code>, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in <code>accumulate_n_optimizer.cc</code>, replaces this placeholder with a group of <code>AssignAdd</code> ops and some additional ops that create, initialize, and destroy temporary variables.</p>\n<p>I wrote a new Python wrapper function <code>accumulate_n_v2</code> that has the same signature as the original <code>accumulate_n</code> function. Unlike the original, <code>accumulate_n_v2</code> only validates its arguments and creates an instance of the <code>AccumulateNV2</code> placeholder op.</p>\n<p><strong>Testing</strong><br>\nI added a more complete set of tests for <code>accumulate_n</code> in a previous pull request (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"249513778\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/12196\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/12196/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/12196\">#12196</a>) to ensure that the op would still be correct after the changes in the current pull request. I copied all of the existing tests for <code>accumulate_n</code> into a test suite for the new <code>accumulate_n_v2</code> op (see tensorflow/contrib/framework/python/ops/accumulate_n_v2_test.py). I also added one additional test to verify that <code>accumulate_n_v2</code> now has a gradient. All the tests under <code>//tensorflow/python/...</code> currently pass on my MacOS and Linux test machines.</p>\n<p><strong>Things to Note</strong><br>\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to <code>accumulate_n</code> to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the <code>shape</code> argument to the <code>accumulate_n</code> function.</p>\n<p>I had to put the graph rewrite code into the main build to get the rewrite to compile and run properly. It looks like some part of the API for adding an optimizer pass instantiates some static global data structures and can't be called from a dynamically-linked library.</p>", "body_text": "This pull request is a restructuring of the changes in #13022 to address high-level comments. Two major changes to call out vis a vis the previous PR:\n\nInstead of replacing accumulate_n, this PR now adds a new op, accumulate_n_v2. This new op is defined under contrib/framework.\nRefactoring changes have been broken out into a separate PR, #13159\n\nOverall, this pull request addresses issue #10607 by adding a gradient to the existing accumulate_n operator. I followed the approach suggested by @alextp: rewrite accumulate_n as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.\nImplementation Details\nI have added a new C++ op, AccumulateNV2, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in accumulate_n_optimizer.cc, replaces this placeholder with a group of AssignAdd ops and some additional ops that create, initialize, and destroy temporary variables.\nI wrote a new Python wrapper function accumulate_n_v2 that has the same signature as the original accumulate_n function. Unlike the original, accumulate_n_v2 only validates its arguments and creates an instance of the AccumulateNV2 placeholder op.\nTesting\nI added a more complete set of tests for accumulate_n in a previous pull request (#12196) to ensure that the op would still be correct after the changes in the current pull request. I copied all of the existing tests for accumulate_n into a test suite for the new accumulate_n_v2 op (see tensorflow/contrib/framework/python/ops/accumulate_n_v2_test.py). I also added one additional test to verify that accumulate_n_v2 now has a gradient. All the tests under //tensorflow/python/... currently pass on my MacOS and Linux test machines.\nThings to Note\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to accumulate_n to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the shape argument to the accumulate_n function.\nI had to put the graph rewrite code into the main build to get the rewrite to compile and run properly. It looks like some part of the API for adding an optimizer pass instantiates some static global data structures and can't be called from a dynamically-linked library.", "body": "This pull request is a restructuring of the changes in https://github.com/tensorflow/tensorflow/pull/13022 to address high-level comments. Two major changes to call out vis a vis the previous PR:\r\n* Instead of replacing `accumulate_n`, this PR now adds a new op, `accumulate_n_v2`. This new op is defined under `contrib/framework`.\r\n* Refactoring changes have been broken out into a separate PR, https://github.com/tensorflow/tensorflow/pull/13159\r\n\r\nOverall, this pull request addresses issue #10607 by adding a gradient to the existing `accumulate_n` operator. I followed the approach suggested by @alextp: rewrite `accumulate_n` as an atomic op which has a gradient defined for it and which gets rewritten by the runtime into the current implementation. Previously, this op had been implemented in Python as a constellation of lower-level ops, some of which are not differentiable.\r\n\r\n**Implementation Details**\r\nI have added a new C++ op, `AccumulateNV2`, which serves as a placeholder for type inference and gradient computation. A new rewrite, implemented in `accumulate_n_optimizer.cc`, replaces this placeholder with a group of `AssignAdd` ops and some additional ops that create, initialize, and destroy temporary variables.\r\n\r\nI wrote a new Python wrapper function `accumulate_n_v2` that has the same signature as the original `accumulate_n` function. Unlike the original, `accumulate_n_v2` only validates its arguments and creates an instance of the `AccumulateNV2` placeholder op.\r\n\r\n**Testing**\r\nI added a more complete set of tests for `accumulate_n` in a previous pull request (https://github.com/tensorflow/tensorflow/pull/12196) to ensure that the op would still be correct after the changes in the current pull request. I copied all of the existing tests for `accumulate_n` into a test suite for the new `accumulate_n_v2` op (see tensorflow/contrib/framework/python/ops/accumulate_n_v2_test.py). I also added one additional test to verify that `accumulate_n_v2` now has a gradient. All the tests under `//tensorflow/python/...` currently pass on my MacOS and Linux test machines.\r\n\r\n**Things to Note**\r\nThe semantics of the new implementation are broadly the same as the original, with the exception of one corner case. The original implementation allowed all the inputs to `accumulate_n` to have an undefined shape. My new code requires that at least one input have a defined shape; or that the user provides a shape using the `shape` argument to the `accumulate_n` function.\r\n\r\nI had to put the graph rewrite code into the main build to get the rewrite to compile and run properly. It looks like some part of the API for adding an optimizer pass instantiates some static global data structures and can't be called from a dynamically-linked library.\r\n"}