{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22361", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22361/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22361/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22361/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22361", "id": 361498658, "node_id": "MDU6SXNzdWUzNjE0OTg2NTg=", "number": 22361, "title": "Conv1d ops don't get quantized", "user": {"login": "joe-antognini", "id": 7061933, "node_id": "MDQ6VXNlcjcwNjE5MzM=", "avatar_url": "https://avatars1.githubusercontent.com/u/7061933?v=4", "gravatar_id": "", "url": "https://api.github.com/users/joe-antognini", "html_url": "https://github.com/joe-antognini", "followers_url": "https://api.github.com/users/joe-antognini/followers", "following_url": "https://api.github.com/users/joe-antognini/following{/other_user}", "gists_url": "https://api.github.com/users/joe-antognini/gists{/gist_id}", "starred_url": "https://api.github.com/users/joe-antognini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/joe-antognini/subscriptions", "organizations_url": "https://api.github.com/users/joe-antognini/orgs", "repos_url": "https://api.github.com/users/joe-antognini/repos", "events_url": "https://api.github.com/users/joe-antognini/events{/privacy}", "received_events_url": "https://api.github.com/users/joe-antognini/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-09-18T21:53:24Z", "updated_at": "2018-10-13T02:03:52Z", "closed_at": "2018-10-13T02:03:52Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: OS X</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.1</li>\n<li><strong>Python version</strong>: 2.7.10</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: See below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Currently <code>tf.contrib.quantize.create_eval_graph</code> and <code>tf.contrib.quantize.create_training_graph</code> will create a quantized version of the graph if the graph contains 2d convolutions.  The pattern matching that these functions use breaks in the case of 1d convolutions, however.  This is because 1d convolutions are transformed to 2d convolutions under the hood with an <code>ExpandDims</code> Op, and this Op breaks the expected pattern of Ops used by <code>_FindLayersToQuantize</code>.</p>\n<p>It would be great if the pattern in <code>_FindLayersToQuantize</code> could be changed from</p>\n<pre><code>weight|folded_weight --&gt; conv|fc --&gt; [batch_to_space_nd] --&gt; ...\n</code></pre>\n<p>to</p>\n<pre><code>weight|folded_weight --&gt; [expand_dims] --&gt; conv|fc --&gt; [batch_to_space_nd] --&gt; ...\n</code></pre>\n<p>This should allow conv1d ops to quantize exactly like conv2d ops.</p>\n<h3>Source code / logs</h3>\n<pre><code># Create and quantize a graph with 2d convolution.\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\nconv = tf.layers.conv2d(X, 64, 3, padding='same', activation=tf.nn.relu)\ntf.contrib.quantize.create_eval_graph()\nlen([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) &gt; 0\n</code></pre>\n<p>This should be <code>True</code>.</p>\n<pre><code># Create and quantize a graph with 1d convolution.\nX = tf.placeholder(tf.float32, [None, 28, 1])\nconv = tf.layers.conv1d(X, 64, 3, padding='same', activation=tf.nn.relu)\ntf.contrib.quantize.create_eval_graph()\nlen([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) &gt; 0\n</code></pre>\n<p>This should be <code>False</code>.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): OS X\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.10.1\nPython version: 2.7.10\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See below.\n\nDescribe the problem\nCurrently tf.contrib.quantize.create_eval_graph and tf.contrib.quantize.create_training_graph will create a quantized version of the graph if the graph contains 2d convolutions.  The pattern matching that these functions use breaks in the case of 1d convolutions, however.  This is because 1d convolutions are transformed to 2d convolutions under the hood with an ExpandDims Op, and this Op breaks the expected pattern of Ops used by _FindLayersToQuantize.\nIt would be great if the pattern in _FindLayersToQuantize could be changed from\nweight|folded_weight --> conv|fc --> [batch_to_space_nd] --> ...\n\nto\nweight|folded_weight --> [expand_dims] --> conv|fc --> [batch_to_space_nd] --> ...\n\nThis should allow conv1d ops to quantize exactly like conv2d ops.\nSource code / logs\n# Create and quantize a graph with 2d convolution.\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\nconv = tf.layers.conv2d(X, 64, 3, padding='same', activation=tf.nn.relu)\ntf.contrib.quantize.create_eval_graph()\nlen([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) > 0\n\nThis should be True.\n# Create and quantize a graph with 1d convolution.\nX = tf.placeholder(tf.float32, [None, 28, 1])\nconv = tf.layers.conv1d(X, 64, 3, padding='same', activation=tf.nn.relu)\ntf.contrib.quantize.create_eval_graph()\nlen([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) > 0\n\nThis should be False.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: OS X\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.10.1\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below.\r\n\r\n### Describe the problem\r\n\r\nCurrently `tf.contrib.quantize.create_eval_graph` and `tf.contrib.quantize.create_training_graph` will create a quantized version of the graph if the graph contains 2d convolutions.  The pattern matching that these functions use breaks in the case of 1d convolutions, however.  This is because 1d convolutions are transformed to 2d convolutions under the hood with an `ExpandDims` Op, and this Op breaks the expected pattern of Ops used by `_FindLayersToQuantize`.\r\n\r\nIt would be great if the pattern in `_FindLayersToQuantize` could be changed from\r\n\r\n```\r\nweight|folded_weight --> conv|fc --> [batch_to_space_nd] --> ...\r\n```\r\n\r\nto\r\n\r\n```\r\nweight|folded_weight --> [expand_dims] --> conv|fc --> [batch_to_space_nd] --> ...\r\n```\r\n\r\nThis should allow conv1d ops to quantize exactly like conv2d ops.\r\n\r\n### Source code / logs\r\n\r\n```\r\n# Create and quantize a graph with 2d convolution.\r\nX = tf.placeholder(tf.float32, [None, 28, 28, 1])\r\nconv = tf.layers.conv2d(X, 64, 3, padding='same', activation=tf.nn.relu)\r\ntf.contrib.quantize.create_eval_graph()\r\nlen([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) > 0\r\n```\r\n\r\nThis should be `True`.\r\n\r\n```\r\n# Create and quantize a graph with 1d convolution.\r\nX = tf.placeholder(tf.float32, [None, 28, 1])\r\nconv = tf.layers.conv1d(X, 64, 3, padding='same', activation=tf.nn.relu)\r\ntf.contrib.quantize.create_eval_graph()\r\nlen([n.name for n in tf.get_default_graph().as_graph_def().node if 'quant' in n.name]) > 0\r\n```\r\n\r\nThis should be `False`."}