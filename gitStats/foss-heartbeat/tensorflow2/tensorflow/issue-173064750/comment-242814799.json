{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242814799", "html_url": "https://github.com/tensorflow/tensorflow/issues/4025#issuecomment-242814799", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4025", "id": 242814799, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjgxNDc5OQ==", "user": {"login": "craymichael", "id": 20629897, "node_id": "MDQ6VXNlcjIwNjI5ODk3", "avatar_url": "https://avatars2.githubusercontent.com/u/20629897?v=4", "gravatar_id": "", "url": "https://api.github.com/users/craymichael", "html_url": "https://github.com/craymichael", "followers_url": "https://api.github.com/users/craymichael/followers", "following_url": "https://api.github.com/users/craymichael/following{/other_user}", "gists_url": "https://api.github.com/users/craymichael/gists{/gist_id}", "starred_url": "https://api.github.com/users/craymichael/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/craymichael/subscriptions", "organizations_url": "https://api.github.com/users/craymichael/orgs", "repos_url": "https://api.github.com/users/craymichael/repos", "events_url": "https://api.github.com/users/craymichael/events{/privacy}", "received_events_url": "https://api.github.com/users/craymichael/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-26T18:30:14Z", "updated_at": "2016-08-26T18:30:14Z", "author_association": "NONE", "body_html": "<p>Another update:<br>\nI was able to get all metrics I tried to work (confusion matrix, precision, recall etc.) by changing <code>_get_eval_ops</code> in <code>tensorflow/contrib/learn/python/learn/estimators/random_forest.py</code> as follows:</p>\n<pre><code> def _get_eval_ops(self, features, targets, metrics):\n  ...\n  if metrics is None:\n    metrics = {self.accuracy_metric:\n               eval_metrics.get_metric(self.accuracy_metric)}\n\n  result = {}\n\n  predictions = math_ops.argmax(probabilities, 1)\n  # undo one-hot\n  labels = math_ops.argmax(labels, 1)\n  for name, metric in six.iteritems(metrics):\n    result[name] = metric(predictions, labels)\n\n  return result\n</code></pre>\n<p>This was broken because the probabilities were being fed to the metrics instead of the predictions and same situation with the labels essentially. In the <code>eval_ops</code> file for the tensor forest, the accuracy metric won't work with these changes as it handles it in the same manner I did</p>", "body_text": "Another update:\nI was able to get all metrics I tried to work (confusion matrix, precision, recall etc.) by changing _get_eval_ops in tensorflow/contrib/learn/python/learn/estimators/random_forest.py as follows:\n def _get_eval_ops(self, features, targets, metrics):\n  ...\n  if metrics is None:\n    metrics = {self.accuracy_metric:\n               eval_metrics.get_metric(self.accuracy_metric)}\n\n  result = {}\n\n  predictions = math_ops.argmax(probabilities, 1)\n  # undo one-hot\n  labels = math_ops.argmax(labels, 1)\n  for name, metric in six.iteritems(metrics):\n    result[name] = metric(predictions, labels)\n\n  return result\n\nThis was broken because the probabilities were being fed to the metrics instead of the predictions and same situation with the labels essentially. In the eval_ops file for the tensor forest, the accuracy metric won't work with these changes as it handles it in the same manner I did", "body": "Another update:\nI was able to get all metrics I tried to work (confusion matrix, precision, recall etc.) by changing `_get_eval_ops` in `tensorflow/contrib/learn/python/learn/estimators/random_forest.py` as follows:\n\n```\n def _get_eval_ops(self, features, targets, metrics):\n  ...\n  if metrics is None:\n    metrics = {self.accuracy_metric:\n               eval_metrics.get_metric(self.accuracy_metric)}\n\n  result = {}\n\n  predictions = math_ops.argmax(probabilities, 1)\n  # undo one-hot\n  labels = math_ops.argmax(labels, 1)\n  for name, metric in six.iteritems(metrics):\n    result[name] = metric(predictions, labels)\n\n  return result\n```\n\nThis was broken because the probabilities were being fed to the metrics instead of the predictions and same situation with the labels essentially. In the `eval_ops` file for the tensor forest, the accuracy metric won't work with these changes as it handles it in the same manner I did\n"}