{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21318", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21318/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21318/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21318/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/21318", "id": 346771061, "node_id": "MDExOlB1bGxSZXF1ZXN0MjA1NTU0OTQ2", "number": 21318, "title": "Intel MKL DNN: Adding support of fusing Pad and Conv2D Operators in MKL DNN", "user": {"login": "ashraf-bhuiyan", "id": 8062406, "node_id": "MDQ6VXNlcjgwNjI0MDY=", "avatar_url": "https://avatars2.githubusercontent.com/u/8062406?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashraf-bhuiyan", "html_url": "https://github.com/ashraf-bhuiyan", "followers_url": "https://api.github.com/users/ashraf-bhuiyan/followers", "following_url": "https://api.github.com/users/ashraf-bhuiyan/following{/other_user}", "gists_url": "https://api.github.com/users/ashraf-bhuiyan/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashraf-bhuiyan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashraf-bhuiyan/subscriptions", "organizations_url": "https://api.github.com/users/ashraf-bhuiyan/orgs", "repos_url": "https://api.github.com/users/ashraf-bhuiyan/repos", "events_url": "https://api.github.com/users/ashraf-bhuiyan/events{/privacy}", "received_events_url": "https://api.github.com/users/ashraf-bhuiyan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 474725938, "node_id": "MDU6TGFiZWw0NzQ3MjU5Mzg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stalled", "name": "stalled", "color": "d4c5f9", "default": false}], "state": "open", "locked": false, "assignee": {"login": "case540", "id": 1299636, "node_id": "MDQ6VXNlcjEyOTk2MzY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1299636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/case540", "html_url": "https://github.com/case540", "followers_url": "https://api.github.com/users/case540/followers", "following_url": "https://api.github.com/users/case540/following{/other_user}", "gists_url": "https://api.github.com/users/case540/gists{/gist_id}", "starred_url": "https://api.github.com/users/case540/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/case540/subscriptions", "organizations_url": "https://api.github.com/users/case540/orgs", "repos_url": "https://api.github.com/users/case540/repos", "events_url": "https://api.github.com/users/case540/events{/privacy}", "received_events_url": "https://api.github.com/users/case540/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "case540", "id": 1299636, "node_id": "MDQ6VXNlcjEyOTk2MzY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1299636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/case540", "html_url": "https://github.com/case540", "followers_url": "https://api.github.com/users/case540/followers", "following_url": "https://api.github.com/users/case540/following{/other_user}", "gists_url": "https://api.github.com/users/case540/gists{/gist_id}", "starred_url": "https://api.github.com/users/case540/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/case540/subscriptions", "organizations_url": "https://api.github.com/users/case540/orgs", "repos_url": "https://api.github.com/users/case540/repos", "events_url": "https://api.github.com/users/case540/events{/privacy}", "received_events_url": "https://api.github.com/users/case540/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2018-08-01T21:12:28Z", "updated_at": "2018-11-23T19:53:43Z", "closed_at": null, "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21318", "html_url": "https://github.com/tensorflow/tensorflow/pull/21318", "diff_url": "https://github.com/tensorflow/tensorflow/pull/21318.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/21318.patch"}, "body_html": "<p><strong>Idea of fusing Pad and Conv2d:</strong></p>\n<p>A = input(such as image), B = input(paddings), C= Pad = input of conv2D,<br>\nD=input(filter), E = Conv2D, Z = Zeta<br>\nC=Pad(A,B); E=Conv2D(C,D); Z=Zeta(E,Y)<br>\nAfter layout pass (merge), the new Op will be the following:<br>\n_MklPadWithConv2D(A, D, B, DMT/_0, DMT/_1, DMT/_2)<br>\nHere, DMT/_X are the mkl layout inputs, corresponding to A, D and B.</p>\n<p><strong>Explanation of the Pad + Conv2D Fusion with Example:</strong><br>\nInput = y<br>\nx = pad(y, paddings) ; for example: x = pad ( y, {(0,0), (2=top,5=bottom), (6=left,9=right), (0,0)}<br>\npaddings = { (0,0) (2,5) (6,9) (0,0)}</p>\n<p>Eigen version of conv2D:<br>\nZ = conv2D(x, filter, stride, dilation, \"padding\") // size of z depends on x, filter, stride, dilation, \u201cPadding\u201d, paddings</p>\n<p>MKL version of conv2D :<br>\nY = MKL_Conv2d(input, filter, padding_left, padding_right, output)<br>\n// padding_left = (2,6), padding_right =(5,9)<br>\nPadding in conv2D = \"VALID\", or \u201cSAME\"</p>\n<p><strong>Restriction:</strong><br>\nPad op changes the size of input. Then, in the conv2d, if \"SAME\" padding is assigned, then additional padding are applied. We calculate the output size based on the Padding. Currently this \"Pad and Conv2D fusion\" implementation does not support SAME padding.</p>\n<p>MKL DNN conv2D API only can accept two arrays for padding. They are: pad_left(top, left) and pad_write(bottom, right). There will be one value of each of the padding, such as top, left, bottom and right.</p>\n<p>Y = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)</p>\n<p><strong>How we handle the pad+conv fusion:</strong></p>\n<p>Changes in Layout pass:</p>\n<p>We check, if pad and conv2D are in correct pattern, if yes, we merge them<br>\nWe check if VALID padding option is selected in the conv2d op, if yes, then only we merge. We primarily support pad+conv2D fusion for \u201cVALID\u201d type of conv2D padding. This is the usual use cases, such as found in Yolo v2. If needed later, fusion with SAME padding will be supported.<br>\nWe replace the merged dummy fused op by the correct fused op.</p>\n<p>Changes in Conv2d op:</p>\n<p>The tensor format of padding in Pad operator is different than the format of padding to pass to the MKL DNN fused conv2D operator. Needed to convert the format so that proper format is passed to the MKL DNN conv2D op.<br>\nIf VALID padding option in the conv2D op is passed, then there will be no padding requested from the Conv2D op. Then, when we fuse Pad and conv2D, then we calculate proper format of pad (such as the right, left, top, bottom padding). Then pass them to the MKL DNN op and execute in mkl_conv2d_ops.cc file. We calculate proper Conv2D output size based on the padding from the Pad op in mkl_conv2d_ops.h file.<br>\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)</p>\n<p><strong>Unit tests:</strong><br>\nMerge test: Two tests are written to make sure, 1) the Pad and Conv2D are merged within constrain and 2) Pad and Conv2D dont merge if the constrain does not meet<br>\nProducing correct output: Two tests are written to make sure they produce correct output, 1) NHWC format and 2) NCHW format.</p>", "body_text": "Idea of fusing Pad and Conv2d:\nA = input(such as image), B = input(paddings), C= Pad = input of conv2D,\nD=input(filter), E = Conv2D, Z = Zeta\nC=Pad(A,B); E=Conv2D(C,D); Z=Zeta(E,Y)\nAfter layout pass (merge), the new Op will be the following:\n_MklPadWithConv2D(A, D, B, DMT/_0, DMT/_1, DMT/_2)\nHere, DMT/_X are the mkl layout inputs, corresponding to A, D and B.\nExplanation of the Pad + Conv2D Fusion with Example:\nInput = y\nx = pad(y, paddings) ; for example: x = pad ( y, {(0,0), (2=top,5=bottom), (6=left,9=right), (0,0)}\npaddings = { (0,0) (2,5) (6,9) (0,0)}\nEigen version of conv2D:\nZ = conv2D(x, filter, stride, dilation, \"padding\") // size of z depends on x, filter, stride, dilation, \u201cPadding\u201d, paddings\nMKL version of conv2D :\nY = MKL_Conv2d(input, filter, padding_left, padding_right, output)\n// padding_left = (2,6), padding_right =(5,9)\nPadding in conv2D = \"VALID\", or \u201cSAME\"\nRestriction:\nPad op changes the size of input. Then, in the conv2d, if \"SAME\" padding is assigned, then additional padding are applied. We calculate the output size based on the Padding. Currently this \"Pad and Conv2D fusion\" implementation does not support SAME padding.\nMKL DNN conv2D API only can accept two arrays for padding. They are: pad_left(top, left) and pad_write(bottom, right). There will be one value of each of the padding, such as top, left, bottom and right.\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)\nHow we handle the pad+conv fusion:\nChanges in Layout pass:\nWe check, if pad and conv2D are in correct pattern, if yes, we merge them\nWe check if VALID padding option is selected in the conv2d op, if yes, then only we merge. We primarily support pad+conv2D fusion for \u201cVALID\u201d type of conv2D padding. This is the usual use cases, such as found in Yolo v2. If needed later, fusion with SAME padding will be supported.\nWe replace the merged dummy fused op by the correct fused op.\nChanges in Conv2d op:\nThe tensor format of padding in Pad operator is different than the format of padding to pass to the MKL DNN fused conv2D operator. Needed to convert the format so that proper format is passed to the MKL DNN conv2D op.\nIf VALID padding option in the conv2D op is passed, then there will be no padding requested from the Conv2D op. Then, when we fuse Pad and conv2D, then we calculate proper format of pad (such as the right, left, top, bottom padding). Then pass them to the MKL DNN op and execute in mkl_conv2d_ops.cc file. We calculate proper Conv2D output size based on the padding from the Pad op in mkl_conv2d_ops.h file.\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)\nUnit tests:\nMerge test: Two tests are written to make sure, 1) the Pad and Conv2D are merged within constrain and 2) Pad and Conv2D dont merge if the constrain does not meet\nProducing correct output: Two tests are written to make sure they produce correct output, 1) NHWC format and 2) NCHW format.", "body": "**Idea of fusing Pad and Conv2d:**\r\n\r\nA = input(such as image), B = input(paddings), C= Pad = input of conv2D, \r\nD=input(filter), E = Conv2D, Z = Zeta\r\nC=Pad(A,B); E=Conv2D(C,D); Z=Zeta(E,Y)\r\nAfter layout pass (merge), the new Op will be the following:\r\n_MklPadWithConv2D(A, D, B, DMT/_0, DMT/_1, DMT/_2)\r\nHere, DMT/_X are the mkl layout inputs, corresponding to A, D and B.\r\n\r\n**Explanation of the Pad + Conv2D Fusion with Example:**\r\nInput = y\r\nx = pad(y, paddings) ; for example: x = pad ( y, {(0,0), (2=top,5=bottom), (6=left,9=right), (0,0)}\r\npaddings = { (0,0) (2,5) (6,9) (0,0)}\r\n\r\nEigen version of conv2D:\r\nZ = conv2D(x, filter, stride, dilation, \"padding\") // size of z depends on x, filter, stride, dilation, \u201cPadding\u201d, paddings\r\n\r\nMKL version of conv2D :\r\nY = MKL_Conv2d(input, filter, padding_left, padding_right, output) \r\n// padding_left = (2,6), padding_right =(5,9)\r\nPadding in conv2D = \"VALID\", or \u201cSAME\"\r\n\r\n**Restriction:**\r\nPad op changes the size of input. Then, in the conv2d, if \"SAME\" padding is assigned, then additional padding are applied. We calculate the output size based on the Padding. Currently this \"Pad and Conv2D fusion\" implementation does not support SAME padding. \r\n\r\nMKL DNN conv2D API only can accept two arrays for padding. They are: pad_left(top, left) and pad_write(bottom, right). There will be one value of each of the padding, such as top, left, bottom and right.\r\n\r\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)\r\n\r\n**How we handle the pad+conv fusion:**\r\n\r\nChanges in Layout pass:\r\n\r\nWe check, if pad and conv2D are in correct pattern, if yes, we merge them\r\nWe check if VALID padding option is selected in the conv2d op, if yes, then only we merge. We primarily support pad+conv2D fusion for \u201cVALID\u201d type of conv2D padding. This is the usual use cases, such as found in Yolo v2. If needed later, fusion with SAME padding will be supported.\r\nWe replace the merged dummy fused op by the correct fused op.\r\n\r\nChanges in Conv2d op:\r\n\r\nThe tensor format of padding in Pad operator is different than the format of padding to pass to the MKL DNN fused conv2D operator. Needed to convert the format so that proper format is passed to the MKL DNN conv2D op.\r\nIf VALID padding option in the conv2D op is passed, then there will be no padding requested from the Conv2D op. Then, when we fuse Pad and conv2D, then we calculate proper format of pad (such as the right, left, top, bottom padding). Then pass them to the MKL DNN op and execute in mkl_conv2d_ops.cc file. We calculate proper Conv2D output size based on the padding from the Pad op in mkl_conv2d_ops.h file.\r\nY = MKL_DNN_Conv2d(input, filter, padding_left, padding_right, output)\r\n\r\n**Unit tests:**\r\nMerge test: Two tests are written to make sure, 1) the Pad and Conv2D are merged within constrain and 2) Pad and Conv2D dont merge if the constrain does not meet\r\nProducing correct output: Two tests are written to make sure they produce correct output, 1) NHWC format and 2) NCHW format.\r\n"}