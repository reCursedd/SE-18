{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/322014573", "html_url": "https://github.com/tensorflow/tensorflow/issues/12093#issuecomment-322014573", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12093", "id": 322014573, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMjAxNDU3Mw==", "user": {"login": "GrandathePanda", "id": 6426407, "node_id": "MDQ6VXNlcjY0MjY0MDc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6426407?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GrandathePanda", "html_url": "https://github.com/GrandathePanda", "followers_url": "https://api.github.com/users/GrandathePanda/followers", "following_url": "https://api.github.com/users/GrandathePanda/following{/other_user}", "gists_url": "https://api.github.com/users/GrandathePanda/gists{/gist_id}", "starred_url": "https://api.github.com/users/GrandathePanda/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GrandathePanda/subscriptions", "organizations_url": "https://api.github.com/users/GrandathePanda/orgs", "repos_url": "https://api.github.com/users/GrandathePanda/repos", "events_url": "https://api.github.com/users/GrandathePanda/events{/privacy}", "received_events_url": "https://api.github.com/users/GrandathePanda/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-13T00:42:46Z", "updated_at": "2017-08-13T00:42:46Z", "author_association": "NONE", "body_html": "<p>Yup I was aware of that. Assuming loss has been steadily decreasing with no abnormality the final minibatch loss should still be just as useful for tuning assuming either overfitting is not an issue or you don't care about over fitting, which in my case is the latter. What I'm hearing though is that there is an easier way through an alternate method. I'd honestly forgotten I opened this suggestion till I just checked my alerts on here for another lib, my life hasn't been deeply halted by this and for that reason I'm going to close this since I just expect it to collect dust.</p>", "body_text": "Yup I was aware of that. Assuming loss has been steadily decreasing with no abnormality the final minibatch loss should still be just as useful for tuning assuming either overfitting is not an issue or you don't care about over fitting, which in my case is the latter. What I'm hearing though is that there is an easier way through an alternate method. I'd honestly forgotten I opened this suggestion till I just checked my alerts on here for another lib, my life hasn't been deeply halted by this and for that reason I'm going to close this since I just expect it to collect dust.", "body": "Yup I was aware of that. Assuming loss has been steadily decreasing with no abnormality the final minibatch loss should still be just as useful for tuning assuming either overfitting is not an issue or you don't care about over fitting, which in my case is the latter. What I'm hearing though is that there is an easier way through an alternate method. I'd honestly forgotten I opened this suggestion till I just checked my alerts on here for another lib, my life hasn't been deeply halted by this and for that reason I'm going to close this since I just expect it to collect dust."}