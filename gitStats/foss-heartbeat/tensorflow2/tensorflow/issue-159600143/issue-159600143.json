{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2779", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2779/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2779/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2779/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2779", "id": 159600143, "node_id": "MDU6SXNzdWUxNTk2MDAxNDM=", "number": 2779, "title": "dynamic_rnn execution with EmbeddingWrapper", "user": {"login": "lhlmgr", "id": 400331, "node_id": "MDQ6VXNlcjQwMDMzMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/400331?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lhlmgr", "html_url": "https://github.com/lhlmgr", "followers_url": "https://api.github.com/users/lhlmgr/followers", "following_url": "https://api.github.com/users/lhlmgr/following{/other_user}", "gists_url": "https://api.github.com/users/lhlmgr/gists{/gist_id}", "starred_url": "https://api.github.com/users/lhlmgr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lhlmgr/subscriptions", "organizations_url": "https://api.github.com/users/lhlmgr/orgs", "repos_url": "https://api.github.com/users/lhlmgr/repos", "events_url": "https://api.github.com/users/lhlmgr/events{/privacy}", "received_events_url": "https://api.github.com/users/lhlmgr/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-06-10T09:56:04Z", "updated_at": "2016-06-20T13:12:21Z", "closed_at": "2016-06-20T13:12:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Guys,</p>\n<p>I'm currently working on a sequence to sequence model for sequences with variable length. Instead of using buckets (like in the example of seq2seq) I used the <code>rnn.dynamic_rnn</code> function. But when I use an EmbeddingWrapper around my RNNCell I'll get exceptions because of two reasons:</p>\n<ul>\n<li>EmbeddingWrapper doesn't implement the derived property <code>def output_size(self):</code> (used in the function <code>dynamic_rnn</code>)</li>\n<li>Since I want to use word embeddings and <code>dynamic_rnn</code>, my input has the datatype <code>tf.int32</code> (like in the seq2seq example) and shape (batch_size, n_steps, 1). But then I get following error:</li>\n</ul>\n<blockquote>\n<p>ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"RNN/while/BasicRNNCell/Tanh:0\", shape=(3, 4), dtype=float32)'</p>\n</blockquote>\n<p>Since I'm new to tensorflow I don't know whats the best way to fix this issue. However, If I change the datatype of the input to <code>tf.float32</code> and the modify EmbeddingWrapper class as follows it works.</p>\n<pre><code>class EmbeddingWrapper(RNNCell):\n    if not isinstance(cell, RNNCell):\n      raise TypeError(\"The parameter cell is not RNNCell.\")\n    if embedding_classes &lt;= 0 or embedding_size &lt;= 0:\n      raise ValueError(\"Both embedding_classes and embedding_size must be &gt; 0: \"\n                       \"%d, %d.\" % (embedding_classes, embedding_size))\n    self._cell = cell\n    self._embedding_classes = embedding_classes\n    self._embedding_size = embedding_size\n    self._initializer = initializer\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def __call__(self, inputs, state, scope=None):\n    inputs = tf.cast(inputs, tf.int32) # &lt;&lt;&lt;&lt; if I cast the input (tf.float32) to tf.int32 it works\n\n    with vs.variable_scope(scope or type(self).__name__):  # \"EmbeddingWrapper\"\n      with ops.device(\"/cpu:0\"):\n        if self._initializer:\n          initializer = self._initializer\n        elif vs.get_variable_scope().initializer:\n          initializer = vs.get_variable_scope().initializer\n        else:\n          # Default initializer for embeddings should have variance=1.\n          sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n          initializer = init_ops.random_uniform_initializer(-sqrt3, sqrt3)\n        embedding = vs.get_variable(\"embedding\", [self._embedding_classes,\n                                                  self._embedding_size],\n                                    initializer=initializer)\n        embedded = embedding_ops.embedding_lookup(\n            embedding, array_ops.reshape(inputs, [-1]))\n    return self._cell(embedded, state)\n</code></pre>\n<p>I attached a simple working example: <a href=\"https://github.com/tensorflow/tensorflow/files/308766/simplernn.txt\">simplernn.txt</a></p>\n<p>My Environment:</p>\n<ul>\n<li>Ubuntu 16.04, installed tensorflow 0.9rc0 over pip, python 3.5.1</li>\n</ul>\n<p>Thanks in advance!</p>", "body_text": "Hi Guys,\nI'm currently working on a sequence to sequence model for sequences with variable length. Instead of using buckets (like in the example of seq2seq) I used the rnn.dynamic_rnn function. But when I use an EmbeddingWrapper around my RNNCell I'll get exceptions because of two reasons:\n\nEmbeddingWrapper doesn't implement the derived property def output_size(self): (used in the function dynamic_rnn)\nSince I want to use word embeddings and dynamic_rnn, my input has the datatype tf.int32 (like in the seq2seq example) and shape (batch_size, n_steps, 1). But then I get following error:\n\n\nValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"RNN/while/BasicRNNCell/Tanh:0\", shape=(3, 4), dtype=float32)'\n\nSince I'm new to tensorflow I don't know whats the best way to fix this issue. However, If I change the datatype of the input to tf.float32 and the modify EmbeddingWrapper class as follows it works.\nclass EmbeddingWrapper(RNNCell):\n    if not isinstance(cell, RNNCell):\n      raise TypeError(\"The parameter cell is not RNNCell.\")\n    if embedding_classes <= 0 or embedding_size <= 0:\n      raise ValueError(\"Both embedding_classes and embedding_size must be > 0: \"\n                       \"%d, %d.\" % (embedding_classes, embedding_size))\n    self._cell = cell\n    self._embedding_classes = embedding_classes\n    self._embedding_size = embedding_size\n    self._initializer = initializer\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def __call__(self, inputs, state, scope=None):\n    inputs = tf.cast(inputs, tf.int32) # <<<< if I cast the input (tf.float32) to tf.int32 it works\n\n    with vs.variable_scope(scope or type(self).__name__):  # \"EmbeddingWrapper\"\n      with ops.device(\"/cpu:0\"):\n        if self._initializer:\n          initializer = self._initializer\n        elif vs.get_variable_scope().initializer:\n          initializer = vs.get_variable_scope().initializer\n        else:\n          # Default initializer for embeddings should have variance=1.\n          sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n          initializer = init_ops.random_uniform_initializer(-sqrt3, sqrt3)\n        embedding = vs.get_variable(\"embedding\", [self._embedding_classes,\n                                                  self._embedding_size],\n                                    initializer=initializer)\n        embedded = embedding_ops.embedding_lookup(\n            embedding, array_ops.reshape(inputs, [-1]))\n    return self._cell(embedded, state)\n\nI attached a simple working example: simplernn.txt\nMy Environment:\n\nUbuntu 16.04, installed tensorflow 0.9rc0 over pip, python 3.5.1\n\nThanks in advance!", "body": "Hi Guys,\n\nI'm currently working on a sequence to sequence model for sequences with variable length. Instead of using buckets (like in the example of seq2seq) I used the `rnn.dynamic_rnn` function. But when I use an EmbeddingWrapper around my RNNCell I'll get exceptions because of two reasons:\n- EmbeddingWrapper doesn't implement the derived property `def output_size(self):` (used in the function `dynamic_rnn`)\n- Since I want to use word embeddings and `dynamic_rnn`, my input has the datatype `tf.int32` (like in the seq2seq example) and shape (batch_size, n_steps, 1). But then I get following error: \n\n> ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"RNN/while/BasicRNNCell/Tanh:0\", shape=(3, 4), dtype=float32)'\n\nSince I'm new to tensorflow I don't know whats the best way to fix this issue. However, If I change the datatype of the input to `tf.float32` and the modify EmbeddingWrapper class as follows it works.\n\n```\nclass EmbeddingWrapper(RNNCell):\n    if not isinstance(cell, RNNCell):\n      raise TypeError(\"The parameter cell is not RNNCell.\")\n    if embedding_classes <= 0 or embedding_size <= 0:\n      raise ValueError(\"Both embedding_classes and embedding_size must be > 0: \"\n                       \"%d, %d.\" % (embedding_classes, embedding_size))\n    self._cell = cell\n    self._embedding_classes = embedding_classes\n    self._embedding_size = embedding_size\n    self._initializer = initializer\n\n  @property\n  def state_size(self):\n    return self._cell.state_size\n\n  @property\n  def output_size(self):\n    return self._cell.output_size\n\n  def __call__(self, inputs, state, scope=None):\n    inputs = tf.cast(inputs, tf.int32) # <<<< if I cast the input (tf.float32) to tf.int32 it works\n\n    with vs.variable_scope(scope or type(self).__name__):  # \"EmbeddingWrapper\"\n      with ops.device(\"/cpu:0\"):\n        if self._initializer:\n          initializer = self._initializer\n        elif vs.get_variable_scope().initializer:\n          initializer = vs.get_variable_scope().initializer\n        else:\n          # Default initializer for embeddings should have variance=1.\n          sqrt3 = math.sqrt(3)  # Uniform(-sqrt(3), sqrt(3)) has variance=1.\n          initializer = init_ops.random_uniform_initializer(-sqrt3, sqrt3)\n        embedding = vs.get_variable(\"embedding\", [self._embedding_classes,\n                                                  self._embedding_size],\n                                    initializer=initializer)\n        embedded = embedding_ops.embedding_lookup(\n            embedding, array_ops.reshape(inputs, [-1]))\n    return self._cell(embedded, state)\n```\n\nI attached a simple working example: [simplernn.txt](https://github.com/tensorflow/tensorflow/files/308766/simplernn.txt)\n\nMy Environment:\n- Ubuntu 16.04, installed tensorflow 0.9rc0 over pip, python 3.5.1\n\nThanks in advance!\n"}