{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14900", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14900/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14900/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14900/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14900", "id": 276900785, "node_id": "MDU6SXNzdWUyNzY5MDA3ODU=", "number": 14900, "title": "How to control the thread number on Tensorflow?", "user": {"login": "Ellie68", "id": 30722660, "node_id": "MDQ6VXNlcjMwNzIyNjYw", "avatar_url": "https://avatars1.githubusercontent.com/u/30722660?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Ellie68", "html_url": "https://github.com/Ellie68", "followers_url": "https://api.github.com/users/Ellie68/followers", "following_url": "https://api.github.com/users/Ellie68/following{/other_user}", "gists_url": "https://api.github.com/users/Ellie68/gists{/gist_id}", "starred_url": "https://api.github.com/users/Ellie68/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Ellie68/subscriptions", "organizations_url": "https://api.github.com/users/Ellie68/orgs", "repos_url": "https://api.github.com/users/Ellie68/repos", "events_url": "https://api.github.com/users/Ellie68/events{/privacy}", "received_events_url": "https://api.github.com/users/Ellie68/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-11-27T04:53:57Z", "updated_at": "2018-01-03T19:28:34Z", "closed_at": "2018-01-03T19:28:34Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>Do we have any options to control the number of threads in TF-Slim both in training and evaluation processes?</p>\n<p>Specifically, I use <a href=\"https://github.com/pudae/tensorflow-densenet\">this network</a> for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like <a href=\"https://github.com/mnuke/tf-slim-mnist\">this code</a>. I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error \"resource temporarily unavailable\".</p>\n<p>This error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:</p>\n<pre><code>INFO:tensorflow:Restoring parameters from ./model.ckpt-0\nINFO:tensorflow:Starting evaluation at\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]\nINFO:tensorflow:Evaluation [1/60]\n</code></pre>\n<p>However, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:</p>\n<pre><code>INFO:tensorflow:Restoring parameters from ./model.ckpt-0\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\n</code></pre>\n<p>I tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:</p>\n<pre><code> FLAGS.num_preprocessing_threads=1\n  config = tf.ConfigProto()\n  config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads\n  config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads\n\n    slim.evaluation.evaluation_loop(\n        master=FLAGS.master,\n        checkpoint_path=each_ckpt,\n        logdir=FLAGS.eval_dir,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()) + print_ops,\n        variables_to_restore=variables_to_restore,\n        session_config=config)\n</code></pre>\n<p>But unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:</p>\n<p>slurm_script\u2500\u252c\u2500python\u2500\u2500\u2500128*[{python}]<br>\n\u2514\u2500python\u2500\u2500\u25008*[{python}]</p>\n<p>Training script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).</p>\n<p>Any idea on the way to control the thread numbers will be highly appreciated because I do need to fix this issue urgently.<br>\nEllie</p>\n<p>P.S. I'm using Python 2.7.13 and Tensorflow 1.3.0.</p>", "body_text": "Hi all,\nDo we have any options to control the number of threads in TF-Slim both in training and evaluation processes?\nSpecifically, I use this network for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like this code. I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error \"resource temporarily unavailable\".\nThis error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:\nINFO:tensorflow:Restoring parameters from ./model.ckpt-0\nINFO:tensorflow:Starting evaluation at\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]\nINFO:tensorflow:Evaluation [1/60]\n\nHowever, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:\nINFO:tensorflow:Restoring parameters from ./model.ckpt-0\nterminate called after throwing an instance of 'std::system_error'\n  what():  Resource temporarily unavailable\n\nI tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:\n FLAGS.num_preprocessing_threads=1\n  config = tf.ConfigProto()\n  config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads\n  config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads\n\n    slim.evaluation.evaluation_loop(\n        master=FLAGS.master,\n        checkpoint_path=each_ckpt,\n        logdir=FLAGS.eval_dir,\n        num_evals=num_batches,\n        eval_op=list(names_to_updates.values()) + print_ops,\n        variables_to_restore=variables_to_restore,\n        session_config=config)\n\nBut unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:\nslurm_script\u2500\u252c\u2500python\u2500\u2500\u2500128*[{python}]\n\u2514\u2500python\u2500\u2500\u25008*[{python}]\nTraining script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).\nAny idea on the way to control the thread numbers will be highly appreciated because I do need to fix this issue urgently.\nEllie\nP.S. I'm using Python 2.7.13 and Tensorflow 1.3.0.", "body": "Hi all,\r\n\r\nDo we have any options to control the number of threads in TF-Slim both in training and evaluation processes?\r\n\r\nSpecifically, I use [this network](https://github.com/pudae/tensorflow-densenet) for my classification problem. I changed the evaluation part in a way that runs train and evaluation in parallel like [this code](https://github.com/mnuke/tf-slim-mnist). I can run it on my own CPU without any problem. But I can't execute them on a supercomputer. It seems that it is related to the very large number of threads which are being created by Tensorflow. If the number of threads exceeds the maximum number of threads pre-set in SLURM (= 28) then the job will fail. Since it's unable to create new threads it will end up with error \"resource temporarily unavailable\".\r\n\r\nThis error provided when the code tries to restore parameters from checkpoints. If there is no limitation on the number of threads (like on my pc) it works fine:\r\n```\r\nINFO:tensorflow:Restoring parameters from ./model.ckpt-0\r\nINFO:tensorflow:Starting evaluation at\r\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Accuracy[0]\r\nI tensorflow/core/kernels/logging_ops.cc:79] eval/Recall_5[0]\r\nINFO:tensorflow:Evaluation [1/60]\r\n```\r\n\r\nHowever, when there is a limitation on the number of threads (like SLURM job submission on supercomputers) we get:\r\n```\r\nINFO:tensorflow:Restoring parameters from ./model.ckpt-0\r\nterminate called after throwing an instance of 'std::system_error'\r\n  what():  Resource temporarily unavailable\r\n```\r\n\r\nI tried to limit the number of CPU threads used by Tensorflow to 1 by creating config like:\r\n\r\n     FLAGS.num_preprocessing_threads=1\r\n      config = tf.ConfigProto()\r\n      config.intra_op_parallelism_threads = FLAGS.num_preprocessing_threads\r\n      config.inter_op_parallelism_threads = FLAGS.num_preprocessing_threads\r\n    \r\n        slim.evaluation.evaluation_loop(\r\n            master=FLAGS.master,\r\n            checkpoint_path=each_ckpt,\r\n            logdir=FLAGS.eval_dir,\r\n            num_evals=num_batches,\r\n            eval_op=list(names_to_updates.values()) + print_ops,\r\n            variables_to_restore=variables_to_restore,\r\n            session_config=config)\r\nBut unfortunately, that didn't help. In my opinion, the main problem we are having here is the fact that we are not able to control the number of threads here. Although we set it to 1 with various TF options you can actually see that this job is creating many more threads on the node:\r\n\r\n\r\nslurm_script\u2500\u252c\u2500python\u2500\u2500\u2500128*[{python}]\r\n             \u2514\u2500python\u2500\u2500\u25008*[{python}]\r\n\r\nTraining script is creating 128 threads and evaluation script is creating 8 (both numbers vary over time).  \r\n\r\nAny idea on the way to control the thread numbers will be highly appreciated because I do need to fix this issue urgently.\r\nEllie\r\n\r\nP.S. I'm using Python 2.7.13 and Tensorflow 1.3.0."}