{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18334", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18334/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18334/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18334/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18334", "id": 312350352, "node_id": "MDU6SXNzdWUzMTIzNTAzNTI=", "number": 18334, "title": "always get negative moving_variance in batchnormalization", "user": {"login": "redhumor", "id": 12136729, "node_id": "MDQ6VXNlcjEyMTM2NzI5", "avatar_url": "https://avatars2.githubusercontent.com/u/12136729?v=4", "gravatar_id": "", "url": "https://api.github.com/users/redhumor", "html_url": "https://github.com/redhumor", "followers_url": "https://api.github.com/users/redhumor/followers", "following_url": "https://api.github.com/users/redhumor/following{/other_user}", "gists_url": "https://api.github.com/users/redhumor/gists{/gist_id}", "starred_url": "https://api.github.com/users/redhumor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/redhumor/subscriptions", "organizations_url": "https://api.github.com/users/redhumor/orgs", "repos_url": "https://api.github.com/users/redhumor/repos", "events_url": "https://api.github.com/users/redhumor/events{/privacy}", "received_events_url": "https://api.github.com/users/redhumor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-04-08T22:57:23Z", "updated_at": "2018-04-12T15:45:03Z", "closed_at": "2018-04-12T15:45:03Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nMacOS 10.12.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\npycharm</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.5.0</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nno CUDA</li>\n<li><strong>GPU model and memory</strong>:<br>\n0</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I trained a simple NN on my local mac without GPU. training data is random generated and validation data is exactly same as training data. During validation,  i always got negative moving_variance in some dimension in batchnormalization, which as a result, causes the network to output NaN issue.<br>\nBy the way, frontend is Keras 2.1.5.<br>\nHere's the code</p>\n<p>def create_pointwise_model():<br>\ninputs = Input(shape=(13,), name='input_layer')<br>\nnormalized_inputs = BatchNormalization(momentum=0.01)(inputs) # i incline to take the value of current batch<br>\nsum_inputs = Dense(1, name=\"final_dense\", use_bias=False, kernel_initializer='glorot_normal')(normalized_inputs)<br>\nsum_inputs = Activation('selu')(sum_inputs)</p>\n<pre><code>    model = Model(inputs=inputs, outputs=sum_inputs, name='pointwise_model')\n    model.compile(optimizer='Adam', loss='binary_crossentropy')\n    return model\n</code></pre>\n<p>#here's the test I wrote:<br>\nsample  = np.random.rand(4,13)</p>\n<p>def get_data():<br>\nwhile 1:<br>\nyield (sample, np.array([[1],[1],[0],[0]]))</p>\n<p>model = create_pointwise_model()<br>\nmodel.fit_generator(get_data(), steps_per_epoch=1, epochs=1000, validation_data= get_data().next())</p>\n<p>During the training, I got NaN error and pinpointed that it's caused by negative moving_variance in batchnormalization. The way I fix this is that inside<br>\ndef batch_normalization(x,<br>\nmean,<br>\nvariance,<br>\noffset,<br>\nscale,<br>\nvariance_epsilon,<br>\nname=None):</p>\n<p>I added this line:<br>\nvariance = tf.multiply(variance, tf.cast(tf.greater_equal(variance, 0),tf.float32))</p>\n<p>But that's not the right way...</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMacOS 10.12.6\nTensorFlow installed from (source or binary):\npycharm\nTensorFlow version (use command below):\n1.5.0\nPython version:\n2.7\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nno CUDA\nGPU model and memory:\n0\nExact command to reproduce:\n\nDescribe the problem\nI trained a simple NN on my local mac without GPU. training data is random generated and validation data is exactly same as training data. During validation,  i always got negative moving_variance in some dimension in batchnormalization, which as a result, causes the network to output NaN issue.\nBy the way, frontend is Keras 2.1.5.\nHere's the code\ndef create_pointwise_model():\ninputs = Input(shape=(13,), name='input_layer')\nnormalized_inputs = BatchNormalization(momentum=0.01)(inputs) # i incline to take the value of current batch\nsum_inputs = Dense(1, name=\"final_dense\", use_bias=False, kernel_initializer='glorot_normal')(normalized_inputs)\nsum_inputs = Activation('selu')(sum_inputs)\n    model = Model(inputs=inputs, outputs=sum_inputs, name='pointwise_model')\n    model.compile(optimizer='Adam', loss='binary_crossentropy')\n    return model\n\n#here's the test I wrote:\nsample  = np.random.rand(4,13)\ndef get_data():\nwhile 1:\nyield (sample, np.array([[1],[1],[0],[0]]))\nmodel = create_pointwise_model()\nmodel.fit_generator(get_data(), steps_per_epoch=1, epochs=1000, validation_data= get_data().next())\nDuring the training, I got NaN error and pinpointed that it's caused by negative moving_variance in batchnormalization. The way I fix this is that inside\ndef batch_normalization(x,\nmean,\nvariance,\noffset,\nscale,\nvariance_epsilon,\nname=None):\nI added this line:\nvariance = tf.multiply(variance, tf.cast(tf.greater_equal(variance, 0),tf.float32))\nBut that's not the right way...", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n   Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nMacOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**:\r\npycharm\r\n- **TensorFlow version (use command below)**:\r\n1.5.0\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\nno CUDA\r\n- **GPU model and memory**:\r\n0\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI trained a simple NN on my local mac without GPU. training data is random generated and validation data is exactly same as training data. During validation,  i always got negative moving_variance in some dimension in batchnormalization, which as a result, causes the network to output NaN issue.\r\nBy the way, frontend is Keras 2.1.5.\r\nHere's the code\r\n\r\ndef create_pointwise_model():\r\n        inputs = Input(shape=(13,), name='input_layer')\r\n        normalized_inputs = BatchNormalization(momentum=0.01)(inputs) # i incline to take the value of current batch\r\n        sum_inputs = Dense(1, name=\"final_dense\", use_bias=False, kernel_initializer='glorot_normal')(normalized_inputs)\r\n        sum_inputs = Activation('selu')(sum_inputs)\r\n        \r\n        model = Model(inputs=inputs, outputs=sum_inputs, name='pointwise_model')\r\n        model.compile(optimizer='Adam', loss='binary_crossentropy')\r\n        return model\r\n\r\n#here's the test I wrote:\r\nsample  = np.random.rand(4,13)\r\n\r\ndef get_data():\r\n    while 1:\r\n          yield (sample, np.array([[1],[1],[0],[0]]))\r\n\r\nmodel = create_pointwise_model()\r\nmodel.fit_generator(get_data(), steps_per_epoch=1, epochs=1000, validation_data= get_data().next())\r\n\r\nDuring the training, I got NaN error and pinpointed that it's caused by negative moving_variance in batchnormalization. The way I fix this is that inside \r\ndef batch_normalization(x,\r\n                        mean,\r\n                        variance,\r\n                        offset,\r\n                        scale,\r\n                        variance_epsilon,\r\n                        name=None):\r\n\r\nI added this line:\r\n    variance = tf.multiply(variance, tf.cast(tf.greater_equal(variance, 0),tf.float32))\r\n\r\nBut that's not the right way...\r\n\r\n"}