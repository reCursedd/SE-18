{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342256834", "html_url": "https://github.com/tensorflow/tensorflow/issues/14169#issuecomment-342256834", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14169", "id": 342256834, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjI1NjgzNA==", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-06T19:22:49Z", "updated_at": "2017-11-06T19:22:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8793654\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vincentfung13\">@vincentfung13</a> . Thanks for reporting the issue. We have encountered similar issues internally when two or more processes are running TensorFlow with limited GPU memory. In our case, a simple const tensor or random tensor creation will cause garbage values instead of any OOM error when one process has already taken most GPU memory.<br>\nThe short answer to the issue is that: it is due to CUDA runtime silently failing to initialize with insufficient GPU memory. It will cause all the CUDA calls afterwards to produce incorrect results, but not fail with errors. We currently have a workaround which is to initialize CUDA runtime before any of our large GPU memory allocation. It should go in within a week. I'm also investigating the reason for the silent failure. I will provide update when I have something to share with you.</p>", "body_text": "Hi @vincentfung13 . Thanks for reporting the issue. We have encountered similar issues internally when two or more processes are running TensorFlow with limited GPU memory. In our case, a simple const tensor or random tensor creation will cause garbage values instead of any OOM error when one process has already taken most GPU memory.\nThe short answer to the issue is that: it is due to CUDA runtime silently failing to initialize with insufficient GPU memory. It will cause all the CUDA calls afterwards to produce incorrect results, but not fail with errors. We currently have a workaround which is to initialize CUDA runtime before any of our large GPU memory allocation. It should go in within a week. I'm also investigating the reason for the silent failure. I will provide update when I have something to share with you.", "body": "Hi @vincentfung13 . Thanks for reporting the issue. We have encountered similar issues internally when two or more processes are running TensorFlow with limited GPU memory. In our case, a simple const tensor or random tensor creation will cause garbage values instead of any OOM error when one process has already taken most GPU memory.\r\nThe short answer to the issue is that: it is due to CUDA runtime silently failing to initialize with insufficient GPU memory. It will cause all the CUDA calls afterwards to produce incorrect results, but not fail with errors. We currently have a workaround which is to initialize CUDA runtime before any of our large GPU memory allocation. It should go in within a week. I'm also investigating the reason for the silent failure. I will provide update when I have something to share with you."}