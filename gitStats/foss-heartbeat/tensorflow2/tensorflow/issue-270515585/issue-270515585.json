{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14169", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14169/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14169/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14169/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14169", "id": 270515585, "node_id": "MDU6SXNzdWUyNzA1MTU1ODU=", "number": 14169, "title": "Tensorflow predicts odd results with insufficient GPU memory", "user": {"login": "vincentfung13", "id": 8793654, "node_id": "MDQ6VXNlcjg3OTM2NTQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/8793654?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vincentfung13", "html_url": "https://github.com/vincentfung13", "followers_url": "https://api.github.com/users/vincentfung13/followers", "following_url": "https://api.github.com/users/vincentfung13/following{/other_user}", "gists_url": "https://api.github.com/users/vincentfung13/gists{/gist_id}", "starred_url": "https://api.github.com/users/vincentfung13/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vincentfung13/subscriptions", "organizations_url": "https://api.github.com/users/vincentfung13/orgs", "repos_url": "https://api.github.com/users/vincentfung13/repos", "events_url": "https://api.github.com/users/vincentfung13/events{/privacy}", "received_events_url": "https://api.github.com/users/vincentfung13/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-11-02T02:49:38Z", "updated_at": "2018-01-05T08:26:58Z", "closed_at": "2018-01-05T08:25:55Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>I am currently having a problem that, when my code tries to initialize two or more predict instances in a GPU with insufficient memory, instead of throwing an OOM exception, the instances are initialized normally. However, when I try to predict an image with these instances, they produce weird results like [1.0, 0.0, 0.0, \u2026].</p>\n<p>Is this a bug when two or more sessions try to race for limited amount of GPU memory?</p>\n<p>Please see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look.</p>\n<p>Cheers,<br>\nVincent</p>\n<p>System information</p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes</li>\n<li>OS Platform and Distribution: CentOS Linux release 7.2.1511</li>\n<li>TensorFlow installed from (source or binary): official binaries</li>\n<li>TensorFlow version (use command below): 1.3.0/ 1.2.0/ 1.1.0</li>\n<li>Python version: 2.7.5</li>\n<li>CUDA/cuDNN version: CUDA 8.0.61, CUDNN 6.0</li>\n<li>GPU model and memory: 1080 Ti (11172MB)</li>\n<li>Exact steps to reproduce:</li>\n</ul>\n<ol>\n<li>Download and uncompress the file below with two scripts;</li>\n<li>Download the official inception_v1 imagenet pretrained model  from <a href=\"http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\" rel=\"nofollow\">http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz</a></li>\n<li>Try to occupy most of the memory in the GPU to run on - the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem, in my case, I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti.</li>\n<li>Run test_monitor.py in two separate command prompts, we should see weird results within a few minutes (the scripts stop when encounter such results).</li>\n</ol>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/1436321/reproduce_code.zip\">reproduce_code.zip</a></p>", "body_text": "Hi all,\nI am currently having a problem that, when my code tries to initialize two or more predict instances in a GPU with insufficient memory, instead of throwing an OOM exception, the instances are initialized normally. However, when I try to predict an image with these instances, they produce weird results like [1.0, 0.0, 0.0, \u2026].\nIs this a bug when two or more sessions try to race for limited amount of GPU memory?\nPlease see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look.\nCheers,\nVincent\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution: CentOS Linux release 7.2.1511\nTensorFlow installed from (source or binary): official binaries\nTensorFlow version (use command below): 1.3.0/ 1.2.0/ 1.1.0\nPython version: 2.7.5\nCUDA/cuDNN version: CUDA 8.0.61, CUDNN 6.0\nGPU model and memory: 1080 Ti (11172MB)\nExact steps to reproduce:\n\n\nDownload and uncompress the file below with two scripts;\nDownload the official inception_v1 imagenet pretrained model  from http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\nTry to occupy most of the memory in the GPU to run on - the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem, in my case, I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti.\nRun test_monitor.py in two separate command prompts, we should see weird results within a few minutes (the scripts stop when encounter such results).\n\nreproduce_code.zip", "body": "Hi all, \r\n\r\nI am currently having a problem that, when my code tries to initialize two or more predict instances in a GPU with insufficient memory, instead of throwing an OOM exception, the instances are initialized normally. However, when I try to predict an image with these instances, they produce weird results like [1.0, 0.0, 0.0, \u2026].\r\n\r\nIs this a bug when two or more sessions try to race for limited amount of GPU memory? \r\n\r\nPlease see below for my system information and the exact steps to reproduce the problem. I would very much appreciate it if you could take a look.\r\n\r\nCheers, \r\nVincent\r\n\r\nSystem information\r\n\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\r\n- OS Platform and Distribution: CentOS Linux release 7.2.1511\r\n- TensorFlow installed from (source or binary): official binaries\r\n- TensorFlow version (use command below): 1.3.0/ 1.2.0/ 1.1.0\r\n- Python version: 2.7.5\r\n- CUDA/cuDNN version: CUDA 8.0.61, CUDNN 6.0\r\n- GPU model and memory: 1080 Ti (11172MB)\r\n- Exact steps to reproduce:\r\n1. Download and uncompress the file below with two scripts;\r\n2. Download the official inception_v1 imagenet pretrained model  from http://download.tensorflow.org/models/inception_v1_2016_08_28.tar.gz\r\n3. Try to occupy most of the memory in the GPU to run on - the exact amount of memory to occupy needs to be carefully tuned to reproduce the problem, in my case, I use pycuda to allocate 10720 out of the 11172MB of my 1080 Ti. \r\n4. Run test_monitor.py in two separate command prompts, we should see weird results within a few minutes (the scripts stop when encounter such results).\r\n\r\n[reproduce_code.zip](https://github.com/tensorflow/tensorflow/files/1436321/reproduce_code.zip)\r\n"}