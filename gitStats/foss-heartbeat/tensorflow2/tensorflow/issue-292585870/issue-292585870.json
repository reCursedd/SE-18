{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16556", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16556/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16556/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16556/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16556", "id": 292585870, "node_id": "MDU6SXNzdWUyOTI1ODU4NzA=", "number": 16556, "title": "tf.argmax appears to be functioning incorrectly on occasion", "user": {"login": "KevOBrien", "id": 16003438, "node_id": "MDQ6VXNlcjE2MDAzNDM4", "avatar_url": "https://avatars3.githubusercontent.com/u/16003438?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KevOBrien", "html_url": "https://github.com/KevOBrien", "followers_url": "https://api.github.com/users/KevOBrien/followers", "following_url": "https://api.github.com/users/KevOBrien/following{/other_user}", "gists_url": "https://api.github.com/users/KevOBrien/gists{/gist_id}", "starred_url": "https://api.github.com/users/KevOBrien/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KevOBrien/subscriptions", "organizations_url": "https://api.github.com/users/KevOBrien/orgs", "repos_url": "https://api.github.com/users/KevOBrien/repos", "events_url": "https://api.github.com/users/KevOBrien/events{/privacy}", "received_events_url": "https://api.github.com/users/KevOBrien/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-01-29T22:34:01Z", "updated_at": "2018-01-31T02:34:55Z", "closed_at": "2018-01-31T02:34:55Z", "author_association": "NONE", "body_html": "<p><strong>EDIT</strong></p>\n<p><a href=\"https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH\" rel=\"nofollow\">Link to script and input/label data as pickle files to reproduce the error.</a></p>\n<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nI have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nManjaro 17.1.3 Kernel 4.14</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\npython pip</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntensorflow-gpu 1.5.0</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\n3.6.4</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nCUDA 9.0<br>\ncuDNN 7.0</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nNvidia GeForce GTX 1050 8GB</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>tf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.</p>\n<h3>Source code / logs</h3>\n<p>To debug this, I have printed out the following operations:</p>\n<pre><code>print(self.session.run(\n    tf.equal(tf.argmax(self.predictions, axis=-1),\n             tf.argmax(self.labelsUnrolled, axis=-1)),\n    self.batchDict))\nprint(\"\")\nprint(self.session.run(self.predictions, self.batchDict))\nprint(\"\")\nprint(self.session.run(self.labelsUnrolled, self.batchDict))\nprint(\"\\n********\\n\")\n</code></pre>\n<p>Which on two consecutive iterations output the following:</p>\n<pre><code>[[ True  True  True]\n [ True  True  True]]\n\n[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],\n        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],\n        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), \narray([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],\n        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],\n        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32),\n array([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n\n********\n\n[[False False  True]\n [ True  True  True]]\n\n [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],\n         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],\n         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),\n array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],\n         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],\n         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 1., 0.]], dtype=float32), \n array([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n\n********\n</code></pre>\n<p>Isn't the very first <code>True</code> in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the <code>tf.argmax</code> function?</p>\n<p>Printing out the same operations, but not inside the session gives the following shapes:</p>\n<pre><code>Tensor(\"Equal_175:0\", shape=(2, ?), dtype=bool)\n\n[&lt;tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32&gt;, &lt;tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32&gt;]\n\n[&lt;tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32&gt;, &lt;tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32&gt;]\n</code></pre>\n<p>Is it a problem that the number associated with the tensor name \"Equal_XXX:0\" is incrementing each iteration?</p>\n<p>I have also tried changing the axis argument in both argmax functions to <code>axis=2</code>, giving the \"Equal\" tensor a shape of (2, 3) again, but there are still similar errors.</p>\n<p>Here is an example:</p>\n<pre><code>[[False False  True]\n [ True  True False]]\n\n[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],\n        [0.04962843, 0.43777955, 0.46654516, 0.04604685],\n        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),\n array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],\n        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],\n        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]], dtype=float32),\n array([[0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n</code></pre>\n<p>I would expect this to be:</p>\n<pre><code>[[ True False  True]\n [ True False False]]\n</code></pre>\n<p>I thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:</p>\n<pre><code>[[False  True False]\n [False False False]]\n\n[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],\n        [0.04910654, 0.44086066, 0.46013904, 0.04989377],\n        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), \narray([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],\n        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],\n        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.]], dtype=float32),\narray([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n</code></pre>", "body_text": "EDIT\nLink to script and input/label data as pickle files to reproduce the error.\nSystem information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nManjaro 17.1.3 Kernel 4.14\n\n\nTensorFlow installed from (source or binary):\npython pip\n\n\nTensorFlow version (use command below):\ntensorflow-gpu 1.5.0\n\n\nPython version:\n3.6.4\n\n\nCUDA/cuDNN version:\nCUDA 9.0\ncuDNN 7.0\n\n\nGPU model and memory:\nNvidia GeForce GTX 1050 8GB\n\n\nDescribe the problem\ntf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.\nSource code / logs\nTo debug this, I have printed out the following operations:\nprint(self.session.run(\n    tf.equal(tf.argmax(self.predictions, axis=-1),\n             tf.argmax(self.labelsUnrolled, axis=-1)),\n    self.batchDict))\nprint(\"\")\nprint(self.session.run(self.predictions, self.batchDict))\nprint(\"\")\nprint(self.session.run(self.labelsUnrolled, self.batchDict))\nprint(\"\\n********\\n\")\n\nWhich on two consecutive iterations output the following:\n[[ True  True  True]\n [ True  True  True]]\n\n[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],\n        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],\n        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), \narray([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],\n        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],\n        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32),\n array([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n\n********\n\n[[False False  True]\n [ True  True  True]]\n\n [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],\n         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],\n         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),\n array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],\n         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],\n         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 1., 0.]], dtype=float32), \n array([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n\n********\n\nIsn't the very first True in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the tf.argmax function?\nPrinting out the same operations, but not inside the session gives the following shapes:\nTensor(\"Equal_175:0\", shape=(2, ?), dtype=bool)\n\n[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]\n\n[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]\n\nIs it a problem that the number associated with the tensor name \"Equal_XXX:0\" is incrementing each iteration?\nI have also tried changing the axis argument in both argmax functions to axis=2, giving the \"Equal\" tensor a shape of (2, 3) again, but there are still similar errors.\nHere is an example:\n[[False False  True]\n [ True  True False]]\n\n[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],\n        [0.04962843, 0.43777955, 0.46654516, 0.04604685],\n        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),\n array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],\n        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],\n        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]], dtype=float32),\n array([[0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]\n\nI would expect this to be:\n[[ True False  True]\n [ True False False]]\n\nI thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:\n[[False  True False]\n [False False False]]\n\n[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],\n        [0.04910654, 0.44086066, 0.46013904, 0.04989377],\n        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), \narray([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],\n        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],\n        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]\n\n[array([[0., 0., 1., 0.],\n        [0., 0., 1., 0.],\n        [0., 1., 0., 0.]], dtype=float32),\narray([[0., 1., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 1., 0., 0.]], dtype=float32)]", "body": "**EDIT**\r\n\r\n[Link to script and input/label data as pickle files to reproduce the error.](https://drive.google.com/open?id=13Kice6p3IQvRVOKIlW0DSvD9wwJDL-YH)\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have written my own code. My code base, data set and batch generating algorithm are quite large, so I am attempting to illustrate this as best as possible. If no mistake of mine can be seen in this post and the examples I have given, then I will provide further code/data. I have asked on StackOverflow, but have got not replies. If  I am missing something simple, then I'm sure it would have been pointed out on StackOverflow by now.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  \r\nManjaro 17.1.3 Kernel 4.14\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npython pip\r\n\r\n- **TensorFlow version (use command below)**:\r\ntensorflow-gpu 1.5.0\r\n\r\n- **Python version**:\r\n3.6.4\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 9.0\r\ncuDNN 7.0\r\n\r\n- **GPU model and memory**: \r\nNvidia GeForce GTX 1050 8GB\r\n\r\n### Describe the problem\r\ntf.argmax seems to be occasionally producing incorrect results when used on the last axis of a 3-dimensional tensor.\r\n\r\n### Source code / logs\r\nTo debug this, I have printed out the following operations:\r\n\r\n```\r\nprint(self.session.run(\r\n    tf.equal(tf.argmax(self.predictions, axis=-1),\r\n             tf.argmax(self.labelsUnrolled, axis=-1)),\r\n    self.batchDict))\r\nprint(\"\")\r\nprint(self.session.run(self.predictions, self.batchDict))\r\nprint(\"\")\r\nprint(self.session.run(self.labelsUnrolled, self.batchDict))\r\nprint(\"\\n********\\n\")\r\n```\r\n\r\nWhich on two consecutive iterations output the following:\r\n\r\n```\r\n[[ True  True  True]\r\n [ True  True  True]]\r\n\r\n[array([[0.06275553, 0.44493628, 0.42474008, 0.06756803],\r\n        [0.06320112, 0.49631155, 0.4021484 , 0.03833894],\r\n        [0.04378054, 0.59403986, 0.3236889 , 0.03849069]], dtype=float32), \r\narray([[8.1677590e-06, 9.9997127e-01, 2.0200867e-05, 3.6184446e-07],\r\n        [4.3686719e-06, 9.9992716e-01, 6.6905603e-05, 1.5286902e-06],\r\n        [1.3270236e-05, 9.9986196e-01, 1.1622251e-04, 8.5579613e-06]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32),\r\n array([[0., 1., 0., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n\r\n********\r\n\r\n[[False False  True]\r\n [ True  True  True]]\r\n\r\n [array([[0.0466171 , 0.53605616, 0.37778312, 0.03954368],\r\n         [0.05007472, 0.4603508 , 0.44400516, 0.0455693 ],\r\n         [0.06134444, 0.38073638, 0.504286  , 0.05363319]], dtype=float32),\r\n array([[9.6363285e-05, 9.9861979e-01, 1.2741127e-03, 9.7381862e-06],\r\n         [1.6185455e-05, 9.9977034e-01, 2.0742408e-04, 6.0521238e-06],\r\n         [2.9893983e-05, 9.9954152e-01, 4.2436572e-04, 4.2021661e-06]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 0., 1., 0.]], dtype=float32), \r\n array([[0., 1., 0., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n\r\n********\r\n```\r\n\r\nIsn't the very first `True` in the first iteration incorrect? There are many more examples being executed where these are wrong (although it is right the majority of the time). Am I going wrong somewhere with the `tf.argmax` function?\r\n\r\nPrinting out the same operations, but not inside the session gives the following shapes:\r\n\r\n```\r\nTensor(\"Equal_175:0\", shape=(2, ?), dtype=bool)\r\n\r\n[<tf.Tensor 'unstack_2:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_2:1' shape=(?, 4) dtype=float32>]\r\n\r\n[<tf.Tensor 'unstack_1:0' shape=(?, 4) dtype=float32>, <tf.Tensor 'unstack_1:1' shape=(?, 4) dtype=float32>]\r\n```\r\n\r\nIs it a problem that the number associated with the tensor name \"Equal_XXX:0\" is incrementing each iteration?\r\n\r\nI have also tried changing the axis argument in both argmax functions to `axis=2`, giving the \"Equal\" tensor a shape of (2, 3) again, but there are still similar errors.\r\n\r\nHere is an example:\r\n\r\n```\r\n[[False False  True]\r\n [ True  True False]]\r\n\r\n[array([[0.09075877, 0.41096467, 0.4460272 , 0.05224944],\r\n        [0.04962843, 0.43777955, 0.46654516, 0.04604685],\r\n        [0.07901238, 0.40768984, 0.46641603, 0.04688181]], dtype=float32),\r\n array([[0.04444276, 0.49195835, 0.42141557, 0.04218334],\r\n        [0.02372498, 0.47147286, 0.4679979 , 0.03680426],\r\n        [0.03707527, 0.435518  , 0.48937747, 0.03802926]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [1., 0., 0., 0.],\r\n        [0., 0., 1., 0.]], dtype=float32),\r\n array([[0., 1., 0., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n```\r\n\r\nI would expect this to be:\r\n\r\n```\r\n[[ True False  True]\r\n [ True False False]]\r\n```\r\n\r\nI thought this may have been a problem with parallel computations on the GPU, but I tried the same execution on just my CPU and got the following, similar miscalculation, too:\r\n\r\n```\r\n[[False  True False]\r\n [False False False]]\r\n\r\n[array([[0.07774187, 0.40993363, 0.47022063, 0.04210386],\r\n        [0.04910654, 0.44086066, 0.46013904, 0.04989377],\r\n        [0.06700655, 0.37128285, 0.51324743, 0.04846317]], dtype=float32), \r\narray([[0.07584244, 0.3863555 , 0.5090046 , 0.02879751],\r\n        [0.06959026, 0.3221606 , 0.5715027 , 0.03674646],\r\n        [0.09042579, 0.32515866, 0.5385905 , 0.04582503]], dtype=float32)]\r\n\r\n[array([[0., 0., 1., 0.],\r\n        [0., 0., 1., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32),\r\narray([[0., 1., 0., 0.],\r\n        [0., 1., 0., 0.],\r\n        [0., 1., 0., 0.]], dtype=float32)]\r\n```\r\n\r\n"}