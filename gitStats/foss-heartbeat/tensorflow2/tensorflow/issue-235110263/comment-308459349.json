{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308459349", "html_url": "https://github.com/tensorflow/tensorflow/issues/10641#issuecomment-308459349", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10641", "id": 308459349, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODQ1OTM0OQ==", "user": {"login": "JerrikEph", "id": 17830427, "node_id": "MDQ6VXNlcjE3ODMwNDI3", "avatar_url": "https://avatars1.githubusercontent.com/u/17830427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JerrikEph", "html_url": "https://github.com/JerrikEph", "followers_url": "https://api.github.com/users/JerrikEph/followers", "following_url": "https://api.github.com/users/JerrikEph/following{/other_user}", "gists_url": "https://api.github.com/users/JerrikEph/gists{/gist_id}", "starred_url": "https://api.github.com/users/JerrikEph/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JerrikEph/subscriptions", "organizations_url": "https://api.github.com/users/JerrikEph/orgs", "repos_url": "https://api.github.com/users/JerrikEph/repos", "events_url": "https://api.github.com/users/JerrikEph/events{/privacy}", "received_events_url": "https://api.github.com/users/JerrikEph/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-14T15:01:17Z", "updated_at": "2017-06-14T15:40:30Z", "author_association": "CONTRIBUTOR", "body_html": "<pre><code>def initialize(self, name=None):\n    \"\"\"Initialize the decoder.\n\n    Args:\n      name: Name scope for any created operations.\n\n    Returns:\n      `(finished, start_inputs, initial_state)`.\n    \"\"\"\n    finished, start_inputs = self._finished, self._start_inputs\n\n    log_prob_mask = array_ops.one_hot(                          # shape(batch_sz, beam_sz)\n        array_ops.ones([self._batch_size], dtype=dtypes.int32),\n        depth=self._beam_width, dtype=dtypes.bool)\n\n    log_prob_zeros = array_ops.zeros([self._batch_size, self._beam_width],  # shape(batch_sz, beam_sz)\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype)\n    log_prob_neg_inf = array_ops.ones([self._batch_size, self._beam_width],  #shape(batch_sz, beam_sz)\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype) * -float('inf')\n\n    log_probs = array_ops.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)\n\n    initial_state = BeamSearchDecoderState(\n        cell_state=self._initial_cell_state,\n        log_probs=log_probs,\n        finished=finished,\n        lengths=array_ops.zeros(\n            [self._batch_size, self._beam_width], dtype=dtypes.int32))\n\n    return (finished, start_inputs, initial_state)\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a><br>\nIt probably is not  a good idea to push tensors with variant shape to TensorArray.<br>\nActually I think it's a good idea to just set <code>log_probs[:, 1:weight_width]</code> to negative infinity in initialize function.<br>\nand of course set</p>\n<pre><code>  scores_flat = control_flow_ops.cond(\n      time &gt; 0,\n      lambda: array_ops.reshape(scores, [batch_size, -1]),\n      lambda: scores[:, 0])\n  num_available_beam = control_flow_ops.cond(\n      time &gt; 0,\n      lambda: math_ops.reduce_prod(scores_shape[1:]),\n      lambda: math_ops.reduce_prod(scores_shape[2:]))\n\n  # Pick the next beams according to the specified successors function\n  next_beam_size = math_ops.minimum(\n      ops.convert_to_tensor(\n          beam_width, dtype=dtypes.int32, name=\"beam_width\"),\n      num_available_beam)\n</code></pre>\n<p>to a  simple reshape</p>\n<pre><code>  scores_flat = array_ops.reshape(scores, [batch_size, -1])\n</code></pre>\n<p>I will add a test unit and test it if you think it's ok.</p>", "body_text": "def initialize(self, name=None):\n    \"\"\"Initialize the decoder.\n\n    Args:\n      name: Name scope for any created operations.\n\n    Returns:\n      `(finished, start_inputs, initial_state)`.\n    \"\"\"\n    finished, start_inputs = self._finished, self._start_inputs\n\n    log_prob_mask = array_ops.one_hot(                          # shape(batch_sz, beam_sz)\n        array_ops.ones([self._batch_size], dtype=dtypes.int32),\n        depth=self._beam_width, dtype=dtypes.bool)\n\n    log_prob_zeros = array_ops.zeros([self._batch_size, self._beam_width],  # shape(batch_sz, beam_sz)\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype)\n    log_prob_neg_inf = array_ops.ones([self._batch_size, self._beam_width],  #shape(batch_sz, beam_sz)\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype) * -float('inf')\n\n    log_probs = array_ops.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)\n\n    initial_state = BeamSearchDecoderState(\n        cell_state=self._initial_cell_state,\n        log_probs=log_probs,\n        finished=finished,\n        lengths=array_ops.zeros(\n            [self._batch_size, self._beam_width], dtype=dtypes.int32))\n\n    return (finished, start_inputs, initial_state)\n\n@ebrevdo\nIt probably is not  a good idea to push tensors with variant shape to TensorArray.\nActually I think it's a good idea to just set log_probs[:, 1:weight_width] to negative infinity in initialize function.\nand of course set\n  scores_flat = control_flow_ops.cond(\n      time > 0,\n      lambda: array_ops.reshape(scores, [batch_size, -1]),\n      lambda: scores[:, 0])\n  num_available_beam = control_flow_ops.cond(\n      time > 0,\n      lambda: math_ops.reduce_prod(scores_shape[1:]),\n      lambda: math_ops.reduce_prod(scores_shape[2:]))\n\n  # Pick the next beams according to the specified successors function\n  next_beam_size = math_ops.minimum(\n      ops.convert_to_tensor(\n          beam_width, dtype=dtypes.int32, name=\"beam_width\"),\n      num_available_beam)\n\nto a  simple reshape\n  scores_flat = array_ops.reshape(scores, [batch_size, -1])\n\nI will add a test unit and test it if you think it's ok.", "body": "```\r\ndef initialize(self, name=None):\r\n    \"\"\"Initialize the decoder.\r\n\r\n    Args:\r\n      name: Name scope for any created operations.\r\n\r\n    Returns:\r\n      `(finished, start_inputs, initial_state)`.\r\n    \"\"\"\r\n    finished, start_inputs = self._finished, self._start_inputs\r\n\r\n    log_prob_mask = array_ops.one_hot(                          # shape(batch_sz, beam_sz)\r\n        array_ops.ones([self._batch_size], dtype=dtypes.int32),\r\n        depth=self._beam_width, dtype=dtypes.bool)\r\n\r\n    log_prob_zeros = array_ops.zeros([self._batch_size, self._beam_width],  # shape(batch_sz, beam_sz)\r\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype)\r\n    log_prob_neg_inf = array_ops.ones([self._batch_size, self._beam_width],  #shape(batch_sz, beam_sz)\r\n                    dtype=nest.flatten(self._initial_cell_state)[0].dtype) * -float('inf')\r\n\r\n    log_probs = array_ops.where(log_prob_mask, log_prob_zeros, log_prob_neg_inf)\r\n\r\n    initial_state = BeamSearchDecoderState(\r\n        cell_state=self._initial_cell_state,\r\n        log_probs=log_probs,\r\n        finished=finished,\r\n        lengths=array_ops.zeros(\r\n            [self._batch_size, self._beam_width], dtype=dtypes.int32))\r\n\r\n    return (finished, start_inputs, initial_state)\r\n```\r\n@ebrevdo \r\nIt probably is not  a good idea to push tensors with variant shape to TensorArray.\r\nActually I think it's a good idea to just set `log_probs[:, 1:weight_width]` to negative infinity in initialize function. \r\nand of course set\r\n```\r\n  scores_flat = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: array_ops.reshape(scores, [batch_size, -1]),\r\n      lambda: scores[:, 0])\r\n  num_available_beam = control_flow_ops.cond(\r\n      time > 0,\r\n      lambda: math_ops.reduce_prod(scores_shape[1:]),\r\n      lambda: math_ops.reduce_prod(scores_shape[2:]))\r\n\r\n  # Pick the next beams according to the specified successors function\r\n  next_beam_size = math_ops.minimum(\r\n      ops.convert_to_tensor(\r\n          beam_width, dtype=dtypes.int32, name=\"beam_width\"),\r\n      num_available_beam)\r\n```\r\nto a  simple reshape\r\n```\r\n  scores_flat = array_ops.reshape(scores, [batch_size, -1])\r\n```\r\n\r\n I will add a test unit and test it if you think it's ok."}