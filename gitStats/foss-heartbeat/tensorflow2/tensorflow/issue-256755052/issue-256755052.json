{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12968", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12968/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12968/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12968/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12968", "id": 256755052, "node_id": "MDU6SXNzdWUyNTY3NTUwNTI=", "number": 12968, "title": "Feature Request / Workaround for Variable size multi-label candidate sampling in TensorFlow.", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-09-11T16:00:00Z", "updated_at": "2017-11-28T04:58:58Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: +</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS/Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: both</li>\n<li><strong>TensorFlow version (use command below)</strong>:  ('v1.3.0-rc2-20-g0787eee', '1.3.0')</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>: n/a</li>\n<li><strong>CUDA/cuDNN version</strong>: n/a</li>\n<li><strong>GPU model and memory</strong>: n/a</li>\n<li><strong>Exact command to reproduce</strong>: n/a</li>\n</ul>\n<h3>Context:</h3>\n<p>Suppose we have a dataset with an arbitrary amount of labels per each training example (image segmentation, multi-label classification, etc.). Labels (classes) are NOT mutually exclusive, thus vary in size between examples.</p>\n<h3>Problem:</h3>\n<p>When trying to use provided standard <code>nce_loss()</code> -  a static <code>int</code> value for <code>num_true</code> option is required.</p>\n<ul>\n<li><code>num_true</code>: An <code>int</code>. The number of target classes per training example.</li>\n</ul>\n<p>This probably works well for problems where we have same amount of labels per training examples and we know them in advance.<br>\nWhen labels have a variable shape <code>[None]</code>, (and in our case, they are also being batched and bucketed by bucket size with <code>.padded_batch()</code> + <code>.group_by_window()</code>) it is necessary to supply a variable size <code>num_true</code> in order to accustom for all training examples. This is currently unsupported to my knowledge (correct me if I'm wrong).</p>\n<h3>Statement:</h3>\n<p>Is there a proper way to do this or any other workarounds? If not I would like to request a feature. If yes, I would appreciate a working example here or on stackoverflow.</p>\n<p>Related <a href=\"https://stackoverflow.com/questions/46085454/variable-size-multi-label-candidate-sampling-in-tensorflow\" rel=\"nofollow\">stackoverflow question</a>.</p>\n<p>Desired behaviour:</p>\n<pre><code>nce_loss = tf.nn.nce_loss(\n    weights=nce_weights,\n    biases=nce_biases,\n    labels=labels,\n    inputs=inputs,\n    num_sampled=num_sampled,\n    num_true=(tf.shape(labels)[-1]), # or tf.placeholder(\"int32\", [], name=\"num_trve\")\n    num_classes=self.num_classes)\n</code></pre>\n<p>Also, is it possible to add support for weighted losses to <code>nce_loss()</code> (specifically to <code>_compute_sampled_logits()</code> as it is partially generated from a .cc) in the same fashion as it is implemented in <code>tf.losses.sparse_softmax_cross_entropy</code> or <code>tf.losses.sigmoid_cross_entropy</code>?<br>\nThanks in advance.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): +\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS/Ubuntu 16.04\nTensorFlow installed from (source or binary): both\nTensorFlow version (use command below):  ('v1.3.0-rc2-20-g0787eee', '1.3.0')\nPython version: 3.5\nBazel version (if compiling from source): n/a\nCUDA/cuDNN version: n/a\nGPU model and memory: n/a\nExact command to reproduce: n/a\n\nContext:\nSuppose we have a dataset with an arbitrary amount of labels per each training example (image segmentation, multi-label classification, etc.). Labels (classes) are NOT mutually exclusive, thus vary in size between examples.\nProblem:\nWhen trying to use provided standard nce_loss() -  a static int value for num_true option is required.\n\nnum_true: An int. The number of target classes per training example.\n\nThis probably works well for problems where we have same amount of labels per training examples and we know them in advance.\nWhen labels have a variable shape [None], (and in our case, they are also being batched and bucketed by bucket size with .padded_batch() + .group_by_window()) it is necessary to supply a variable size num_true in order to accustom for all training examples. This is currently unsupported to my knowledge (correct me if I'm wrong).\nStatement:\nIs there a proper way to do this or any other workarounds? If not I would like to request a feature. If yes, I would appreciate a working example here or on stackoverflow.\nRelated stackoverflow question.\nDesired behaviour:\nnce_loss = tf.nn.nce_loss(\n    weights=nce_weights,\n    biases=nce_biases,\n    labels=labels,\n    inputs=inputs,\n    num_sampled=num_sampled,\n    num_true=(tf.shape(labels)[-1]), # or tf.placeholder(\"int32\", [], name=\"num_trve\")\n    num_classes=self.num_classes)\n\nAlso, is it possible to add support for weighted losses to nce_loss() (specifically to _compute_sampled_logits() as it is partially generated from a .cc) in the same fashion as it is implemented in tf.losses.sparse_softmax_cross_entropy or tf.losses.sigmoid_cross_entropy?\nThanks in advance.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS/Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: both\r\n- **TensorFlow version (use command below)**:  ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: n/a\r\n- **Exact command to reproduce**: n/a\r\n\r\n\r\n### Context:\r\nSuppose we have a dataset with an arbitrary amount of labels per each training example (image segmentation, multi-label classification, etc.). Labels (classes) are NOT mutually exclusive, thus vary in size between examples. \r\n\r\n### Problem:\r\nWhen trying to use provided standard `nce_loss()` -  a static `int` value for `num_true` option is required. \r\n- `num_true`: An `int`. The number of target classes per training example.\r\n\r\nThis probably works well for problems where we have same amount of labels per training examples and we know them in advance.\r\nWhen labels have a variable shape `[None]`, (and in our case, they are also being batched and bucketed by bucket size with `.padded_batch()` + `.group_by_window()`) it is necessary to supply a variable size `num_true` in order to accustom for all training examples. This is currently unsupported to my knowledge (correct me if I'm wrong).\r\n\r\n### Statement:\r\nIs there a proper way to do this or any other workarounds? If not I would like to request a feature. If yes, I would appreciate a working example here or on stackoverflow.\r\n\r\nRelated [stackoverflow question](https://stackoverflow.com/questions/46085454/variable-size-multi-label-candidate-sampling-in-tensorflow).\r\n\r\nDesired behaviour:\r\n```\r\nnce_loss = tf.nn.nce_loss(\r\n    weights=nce_weights,\r\n    biases=nce_biases,\r\n    labels=labels,\r\n    inputs=inputs,\r\n    num_sampled=num_sampled,\r\n    num_true=(tf.shape(labels)[-1]), # or tf.placeholder(\"int32\", [], name=\"num_trve\")\r\n    num_classes=self.num_classes)\r\n```\r\n\r\nAlso, is it possible to add support for weighted losses to `nce_loss()` (specifically to `_compute_sampled_logits()` as it is partially generated from a .cc) in the same fashion as it is implemented in `tf.losses.sparse_softmax_cross_entropy` or `tf.losses.sigmoid_cross_entropy`?\r\nThanks in advance."}