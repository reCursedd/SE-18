{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/260728576", "html_url": "https://github.com/tensorflow/tensorflow/issues/5390#issuecomment-260728576", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5390", "id": 260728576, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MDcyODU3Ng==", "user": {"login": "ashishkr23", "id": 23434229, "node_id": "MDQ6VXNlcjIzNDM0MjI5", "avatar_url": "https://avatars0.githubusercontent.com/u/23434229?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ashishkr23", "html_url": "https://github.com/ashishkr23", "followers_url": "https://api.github.com/users/ashishkr23/followers", "following_url": "https://api.github.com/users/ashishkr23/following{/other_user}", "gists_url": "https://api.github.com/users/ashishkr23/gists{/gist_id}", "starred_url": "https://api.github.com/users/ashishkr23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ashishkr23/subscriptions", "organizations_url": "https://api.github.com/users/ashishkr23/orgs", "repos_url": "https://api.github.com/users/ashishkr23/repos", "events_url": "https://api.github.com/users/ashishkr23/events{/privacy}", "received_events_url": "https://api.github.com/users/ashishkr23/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-15T18:42:26Z", "updated_at": "2016-11-15T18:49:02Z", "author_association": "NONE", "body_html": "<p>Hi ,<br>\nI have been studying the pinning of variables and ops in a multi-gpu enviroment. I am following the multi-gpu training methodology  as shown in <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py\">multi-gpu inception_train example</a></p>\n<p>I observed different variable placement in two scenarios:</p>\n<ol>\n<li>As shown in the <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py\">example</a>  which uses a deprecated <a href=\"https://github.com/tensorflow/models/tree/master/inception/inception/slim\">inception.slim</a> <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"175440669\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4247\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4247/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4247\">#4247</a></li>\n<li>Using the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v3.py\">tf.contrib.slim</a> , as mentioned in this thread</li>\n</ol>\n<p><strong>Enviroment:</strong><br>\nTensorflow version 0.11.0 (last stable binary)<br>\nCUDA 8.0<br>\nCUDnn 5.1<br>\n2 * K20 GPU Cards</p>\n<p><strong>Scenario 1:</strong><br>\nUsing the deprecated inception.slim library which is the same used in the inception_train example , i observed that the variables are placed on the CPU and all the ops are separately placed on the respective  GPU with a prefix 'TOWER_0' or 'TOWER_1'<br>\nUsing the below <strong>Test Code</strong> this <a href=\"http://www.filedropper.com/logdeprecated\" rel=\"nofollow\">Full Device Placement Log</a>  was generated:</p>\n<pre><code>import tensorflow as tf\nfrom inception.slim import slim\nfrom inception.slim.inception_model import inception_v3\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variables.variable], device='/cpu:0'):\n                    # Parameters for BatchNorm.\n                    batch_norm_params = {\n                    # Decay for the moving averages.\n                        'decay': 0.9997,\n                    # epsilon to prevent 0s in variance.\n                        'epsilon': 0.001,\n                    }\n                    # Set weight_decay for weights in Conv and FC layers.\n                    with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n                        with slim.arg_scope([slim.ops.conv2d],\n                                stddev=0.1,\n                                activation=tf.nn.relu,\n                                batch_norm_params=batch_norm_params):\n                            logits, endpoints = slim.inception.inception_v3(\n                            images,\n                            dropout_keep_prob=0.8,\n                            num_classes=1001,\n                            is_training=True,\n                            restore_logits=False,\n                            scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n</code></pre>\n<p><strong>Scenario 2:</strong><br>\nUsing the [tf.contrib.slim] , the same '<strong>VALUE ERROR</strong>' occurs as stated by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12427390\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pranavvm26\">@pranavvm26</a></p>\n<p><strong>Test Code:</strong></p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_arg_scope\nslim = tf.contrib.slim\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    labels= tf.placeholder(tf.float32,shape=([1,1]),name='labels')\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variable], device='/cpu:0'):\n                    with slim.arg_scope(inception_v3_arg_scope()):\n                            logits, endpoints = inception_v3(\n                              images,\n                              dropout_keep_prob=0.8,\n                              num_classes=1001,\n                              is_training=True,\n                              scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n</code></pre>\n<p><strong>ERROR:</strong></p>\n<pre><code>ValueError: Variable TOWER_1//Conv2d_1a_3x3/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n</code></pre>\n<p>On not passing scope value to the inception_v3 model, the Value Error is gone but the device placement log shows variables pinned on GPU:0 and OPS on both GPU:0 and GPU:1<br>\n<a href=\"http://www.filedropper.com/newlog\" rel=\"nofollow\">FULL DEVICE PLACEMENT LOG</a></p>\n<p>As far as i understand and checking the cifar log device placement the variables should be pinned to the cpu.<br>\nAny thoughts on the difference in behavior ?</p>\n<p>Thanks</p>", "body_text": "Hi ,\nI have been studying the pinning of variables and ops in a multi-gpu enviroment. I am following the multi-gpu training methodology  as shown in multi-gpu inception_train example\nI observed different variable placement in two scenarios:\n\nAs shown in the example  which uses a deprecated inception.slim #4247\nUsing the tf.contrib.slim , as mentioned in this thread\n\nEnviroment:\nTensorflow version 0.11.0 (last stable binary)\nCUDA 8.0\nCUDnn 5.1\n2 * K20 GPU Cards\nScenario 1:\nUsing the deprecated inception.slim library which is the same used in the inception_train example , i observed that the variables are placed on the CPU and all the ops are separately placed on the respective  GPU with a prefix 'TOWER_0' or 'TOWER_1'\nUsing the below Test Code this Full Device Placement Log  was generated:\nimport tensorflow as tf\nfrom inception.slim import slim\nfrom inception.slim.inception_model import inception_v3\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variables.variable], device='/cpu:0'):\n                    # Parameters for BatchNorm.\n                    batch_norm_params = {\n                    # Decay for the moving averages.\n                        'decay': 0.9997,\n                    # epsilon to prevent 0s in variance.\n                        'epsilon': 0.001,\n                    }\n                    # Set weight_decay for weights in Conv and FC layers.\n                    with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n                        with slim.arg_scope([slim.ops.conv2d],\n                                stddev=0.1,\n                                activation=tf.nn.relu,\n                                batch_norm_params=batch_norm_params):\n                            logits, endpoints = slim.inception.inception_v3(\n                            images,\n                            dropout_keep_prob=0.8,\n                            num_classes=1001,\n                            is_training=True,\n                            restore_logits=False,\n                            scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n\nScenario 2:\nUsing the [tf.contrib.slim] , the same 'VALUE ERROR' occurs as stated by @pranavvm26\nTest Code:\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_arg_scope\nslim = tf.contrib.slim\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    labels= tf.placeholder(tf.float32,shape=([1,1]),name='labels')\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variable], device='/cpu:0'):\n                    with slim.arg_scope(inception_v3_arg_scope()):\n                            logits, endpoints = inception_v3(\n                              images,\n                              dropout_keep_prob=0.8,\n                              num_classes=1001,\n                              is_training=True,\n                              scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n\nERROR:\nValueError: Variable TOWER_1//Conv2d_1a_3x3/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n\nOn not passing scope value to the inception_v3 model, the Value Error is gone but the device placement log shows variables pinned on GPU:0 and OPS on both GPU:0 and GPU:1\nFULL DEVICE PLACEMENT LOG\nAs far as i understand and checking the cifar log device placement the variables should be pinned to the cpu.\nAny thoughts on the difference in behavior ?\nThanks", "body": "Hi , \nI have been studying the pinning of variables and ops in a multi-gpu enviroment. I am following the multi-gpu training methodology  as shown in [multi-gpu inception_train example](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py) \n\nI observed different variable placement in two scenarios:\n1. As shown in the [example](https://github.com/tensorflow/models/blob/master/inception/inception/inception_train.py)  which uses a deprecated [inception.slim](https://github.com/tensorflow/models/tree/master/inception/inception/slim) #4247\n2.  Using the [tf.contrib.slim](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/inception_v3.py) , as mentioned in this thread  \n\n**Enviroment:**\nTensorflow version 0.11.0 (last stable binary)\nCUDA 8.0\nCUDnn 5.1\n2 \\* K20 GPU Cards\n\n**Scenario 1:**\nUsing the deprecated inception.slim library which is the same used in the inception_train example , i observed that the variables are placed on the CPU and all the ops are separately placed on the respective  GPU with a prefix 'TOWER_0' or 'TOWER_1' \nUsing the below **Test Code** this [Full Device Placement Log](http://www.filedropper.com/logdeprecated)  was generated: \n\n```\nimport tensorflow as tf\nfrom inception.slim import slim\nfrom inception.slim.inception_model import inception_v3\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variables.variable], device='/cpu:0'):\n                    # Parameters for BatchNorm.\n                    batch_norm_params = {\n                    # Decay for the moving averages.\n                        'decay': 0.9997,\n                    # epsilon to prevent 0s in variance.\n                        'epsilon': 0.001,\n                    }\n                    # Set weight_decay for weights in Conv and FC layers.\n                    with slim.arg_scope([slim.ops.conv2d, slim.ops.fc], weight_decay=0.00004):\n                        with slim.arg_scope([slim.ops.conv2d],\n                                stddev=0.1,\n                                activation=tf.nn.relu,\n                                batch_norm_params=batch_norm_params):\n                            logits, endpoints = slim.inception.inception_v3(\n                            images,\n                            dropout_keep_prob=0.8,\n                            num_classes=1001,\n                            is_training=True,\n                            restore_logits=False,\n                            scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n```\n\n**Scenario 2:**\nUsing the [tf.contrib.slim] , the same '**VALUE ERROR**' occurs as stated by @pranavvm26  \n\n**Test Code:**\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3\nfrom tensorflow.contrib.slim.python.slim.nets.inception_v3 import inception_v3_arg_scope\nslim = tf.contrib.slim\nwith tf.Graph().as_default(), tf.device('/cpu:0'):\n    imgPath=tf.placeholder(tf.string)\n    labels= tf.placeholder(tf.float32,shape=([1,1]),name='labels')\n    imageString=tf.read_file(imgPath)\n    imageJpeg=tf.image.decode_jpeg(imageString, channels=3)\n    inputImage=tf.image.resize_images(imageJpeg, [299,299])\n    images= tf.expand_dims(inputImage, 0)\n    for i in range(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n                with slim.arg_scope([slim.variable], device='/cpu:0'):\n                    with slim.arg_scope(inception_v3_arg_scope()):\n                            logits, endpoints = inception_v3(\n                              images,\n                              dropout_keep_prob=0.8,\n                              num_classes=1001,\n                              is_training=True,\n                              scope=scope)\n            tf.get_variable_scope().reuse_variables()\n\n    with tf.Session(config=tf.ConfigProto(allow_soft_placement=True,log_device_placement=True)) as sess:\n        tf.initialize_all_variables().run()\n    exit(0)\n```\n\n**ERROR:**\n\n```\nValueError: Variable TOWER_1//Conv2d_1a_3x3/weights does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n```\n\nOn not passing scope value to the inception_v3 model, the Value Error is gone but the device placement log shows variables pinned on GPU:0 and OPS on both GPU:0 and GPU:1\n[FULL DEVICE PLACEMENT LOG](http://www.filedropper.com/newlog)\n\nAs far as i understand and checking the cifar log device placement the variables should be pinned to the cpu.\nAny thoughts on the difference in behavior ?\n\nThanks \n"}