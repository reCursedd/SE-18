{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261057004", "html_url": "https://github.com/tensorflow/tensorflow/issues/5390#issuecomment-261057004", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5390", "id": 261057004, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTA1NzAwNA==", "user": {"login": "pranavvm26", "id": 12427390, "node_id": "MDQ6VXNlcjEyNDI3Mzkw", "avatar_url": "https://avatars2.githubusercontent.com/u/12427390?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pranavvm26", "html_url": "https://github.com/pranavvm26", "followers_url": "https://api.github.com/users/pranavvm26/followers", "following_url": "https://api.github.com/users/pranavvm26/following{/other_user}", "gists_url": "https://api.github.com/users/pranavvm26/gists{/gist_id}", "starred_url": "https://api.github.com/users/pranavvm26/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pranavvm26/subscriptions", "organizations_url": "https://api.github.com/users/pranavvm26/orgs", "repos_url": "https://api.github.com/users/pranavvm26/repos", "events_url": "https://api.github.com/users/pranavvm26/events{/privacy}", "received_events_url": "https://api.github.com/users/pranavvm26/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-16T20:08:51Z", "updated_at": "2016-11-16T20:25:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23434229\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ashishkr23\">@ashishkr23</a> I came across the same issue when training using the newer inception model. To enable variable placement on CPU rather than GPU:0 for the scenario you just mentioned, there might be a different workaround. Try the following,<br>\n$  The conv, fc, etc ops are sourced from the following location:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py</a></p>\n<p>This script is responsible for the creation of variables for the ops.</p>\n<p>Side note: In the default case the variables lie in GPU:0 in my system too but my GPUs are in a P2P connectivity so the variables residing in the GPU:0 are supplied to all the other GPUs and also updated per session (this may be problematic in your case).</p>\n<p>$  To pin the variables to the CPU, add a param called <strong>device</strong> to to the ops in layers.py</p>\n<p>example:</p>\n<pre><code>variables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable)\n</code></pre>\n<p>changed to</p>\n<pre><code>variables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable,\n                                       device='/cpu:0')\n</code></pre>\n<p>within the layers.py (for all ops).</p>\n<p>This would create the model with op variables pinned to the CPU instead of the GPU. The variable.model_variable is being called from the following location within tensorflow (<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a> please correct me if I am wrong),</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/beb10ceb086fe94a6b1247b45397aafddd47e05d/tensorflow/contrib/framework/python/ops/variables.py\">https://github.com/tensorflow/tensorflow/blob/beb10ceb086fe94a6b1247b45397aafddd47e05d/tensorflow/contrib/framework/python/ops/variables.py</a></p>\n<pre><code>def model_variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n                   regularizer=None, trainable=True, collections=None,\n                   caching_device=None, **device=None**)\n</code></pre>\n<p>In my case it didnt make much difference because of my system architecture.</p>\n<p>$  To be able to pass tower as a name scope you could try the workaround suggested by copying the inception_v3.py file over to your project root and changing variable_scope to name_scope. If you do try this please let me know if these suggestion help with your case of distributed GPU system (without a P2P connectivity).</p>", "body_text": "@ashishkr23 I came across the same issue when training using the newer inception model. To enable variable placement on CPU rather than GPU:0 for the scenario you just mentioned, there might be a different workaround. Try the following,\n$  The conv, fc, etc ops are sourced from the following location:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py\nThis script is responsible for the creation of variables for the ops.\nSide note: In the default case the variables lie in GPU:0 in my system too but my GPUs are in a P2P connectivity so the variables residing in the GPU:0 are supplied to all the other GPUs and also updated per session (this may be problematic in your case).\n$  To pin the variables to the CPU, add a param called device to to the ops in layers.py\nexample:\nvariables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable)\n\nchanged to\nvariables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable,\n                                       device='/cpu:0')\n\nwithin the layers.py (for all ops).\nThis would create the model with op variables pinned to the CPU instead of the GPU. The variable.model_variable is being called from the following location within tensorflow (@sguada please correct me if I am wrong),\nhttps://github.com/tensorflow/tensorflow/blob/beb10ceb086fe94a6b1247b45397aafddd47e05d/tensorflow/contrib/framework/python/ops/variables.py\ndef model_variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n                   regularizer=None, trainable=True, collections=None,\n                   caching_device=None, **device=None**)\n\nIn my case it didnt make much difference because of my system architecture.\n$  To be able to pass tower as a name scope you could try the workaround suggested by copying the inception_v3.py file over to your project root and changing variable_scope to name_scope. If you do try this please let me know if these suggestion help with your case of distributed GPU system (without a P2P connectivity).", "body": "@ashishkr23 I came across the same issue when training using the newer inception model. To enable variable placement on CPU rather than GPU:0 for the scenario you just mentioned, there might be a different workaround. Try the following,\n$  The conv, fc, etc ops are sourced from the following location:\n                          https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/layers/python/layers/layers.py\n\nThis script is responsible for the creation of variables for the ops. \n\nSide note: In the default case the variables lie in GPU:0 in my system too but my GPUs are in a P2P connectivity so the variables residing in the GPU:0 are supplied to all the other GPUs and also updated per session (this may be problematic in your case).\n\n$  To pin the variables to the CPU, add a param called **device** to to the ops in layers.py\n\nexample:\n\n```\nvariables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable)\n```\n\nchanged to \n\n```\nvariables.model_variable('weights',\n                                       shape=weights_shape,\n                                       dtype=dtype,\n                                       initializer=weights_initializer,\n                                       regularizer=weights_regularizer,\n                                       collections=weights_collections,\n                                       trainable=trainable,\n                                       device='/cpu:0')\n```\n\nwithin the layers.py (for all ops).\n\nThis would create the model with op variables pinned to the CPU instead of the GPU. The variable.model_variable is being called from the following location within tensorflow (@sguada please correct me if I am wrong),\n\nhttps://github.com/tensorflow/tensorflow/blob/beb10ceb086fe94a6b1247b45397aafddd47e05d/tensorflow/contrib/framework/python/ops/variables.py\n\n```\ndef model_variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n                   regularizer=None, trainable=True, collections=None,\n                   caching_device=None, **device=None**)\n```\n\nIn my case it didnt make much difference because of my system architecture.\n\n$  To be able to pass tower as a name scope you could try the workaround suggested by copying the inception_v3.py file over to your project root and changing variable_scope to name_scope. If you do try this please let me know if these suggestion help with your case of distributed GPU system (without a P2P connectivity).\n"}