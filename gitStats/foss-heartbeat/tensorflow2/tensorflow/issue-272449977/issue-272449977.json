{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14392", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14392/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14392/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14392/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14392", "id": 272449977, "node_id": "MDU6SXNzdWUyNzI0NDk5Nzc=", "number": 14392, "title": "The weights argument in Keras's Embedding does not work", "user": {"login": "hsm207", "id": 2398765, "node_id": "MDQ6VXNlcjIzOTg3NjU=", "avatar_url": "https://avatars0.githubusercontent.com/u/2398765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hsm207", "html_url": "https://github.com/hsm207", "followers_url": "https://api.github.com/users/hsm207/followers", "following_url": "https://api.github.com/users/hsm207/following{/other_user}", "gists_url": "https://api.github.com/users/hsm207/gists{/gist_id}", "starred_url": "https://api.github.com/users/hsm207/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hsm207/subscriptions", "organizations_url": "https://api.github.com/users/hsm207/orgs", "repos_url": "https://api.github.com/users/hsm207/repos", "events_url": "https://api.github.com/users/hsm207/events{/privacy}", "received_events_url": "https://api.github.com/users/hsm207/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-11-09T06:04:00Z", "updated_at": "2018-06-07T23:02:38Z", "closed_at": "2017-11-09T08:02:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am using Tensorflow 1.4.0.</p>\n<p>According to this <a href=\"https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html\" rel=\"nofollow\">blog post</a>, we can use the weights argument in the call to Embedding to specify some matrix that represents a pre-trained word embeddings (see the section titled Preparing the Embedding Layer).</p>\n<p>However, this code does not work:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python.estimator.model_fn import EstimatorSpec\nfrom tensorflow.contrib.keras.api.keras.layers import Embedding, Dense\nfrom tensorflow.contrib.keras.api.keras.initializers import Constant\n\n\ndef model_fn(features, labels, mode):\n    x = tf.constant([[1]])\n    labels = tf.constant([[10.]])\n    \n    # Let m be our pre-trained word embeddings\n    m = np.array([[1, 2], [3, 4]], np.float32)\n    with tf.name_scope('Embedding_Layer'):\n        # Create an embedding layer and load m into it\n        n = Embedding(2, 2, weights=[m], input_length=1, name='embedding_matrix_1', trainable=False)\n\n    lookup = n(x)\n    lookup = tf.Print(lookup, [lookup])\n\n    preds = Dense(1)(lookup)\n    loss = tf.reduce_mean(labels - preds)\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss, tf.train.get_global_step())\n\n    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels, preds)}\n    return EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\n\n\nmodel = tf.estimator.Estimator(model_fn)\nmodel.train(input_fn=lambda: None, steps=1)\n</code></pre>\n<p><code>lookup</code> should print [[3 4]] but instead random numbers are printed.</p>\n<p>The solution is to define <code>n</code> as follows:</p>\n<pre><code>n = Embedding(2, 2, embeddings_initializer=Constant(m), input_length=1, name='embedding_matrix_1',\n                      trainable=False)\n</code></pre>", "body_text": "I am using Tensorflow 1.4.0.\nAccording to this blog post, we can use the weights argument in the call to Embedding to specify some matrix that represents a pre-trained word embeddings (see the section titled Preparing the Embedding Layer).\nHowever, this code does not work:\nimport tensorflow as tf\nimport numpy as np\n\nfrom tensorflow.python.estimator.model_fn import EstimatorSpec\nfrom tensorflow.contrib.keras.api.keras.layers import Embedding, Dense\nfrom tensorflow.contrib.keras.api.keras.initializers import Constant\n\n\ndef model_fn(features, labels, mode):\n    x = tf.constant([[1]])\n    labels = tf.constant([[10.]])\n    \n    # Let m be our pre-trained word embeddings\n    m = np.array([[1, 2], [3, 4]], np.float32)\n    with tf.name_scope('Embedding_Layer'):\n        # Create an embedding layer and load m into it\n        n = Embedding(2, 2, weights=[m], input_length=1, name='embedding_matrix_1', trainable=False)\n\n    lookup = n(x)\n    lookup = tf.Print(lookup, [lookup])\n\n    preds = Dense(1)(lookup)\n    loss = tf.reduce_mean(labels - preds)\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss, tf.train.get_global_step())\n\n    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels, preds)}\n    return EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\n\n\nmodel = tf.estimator.Estimator(model_fn)\nmodel.train(input_fn=lambda: None, steps=1)\n\nlookup should print [[3 4]] but instead random numbers are printed.\nThe solution is to define n as follows:\nn = Embedding(2, 2, embeddings_initializer=Constant(m), input_length=1, name='embedding_matrix_1',\n                      trainable=False)", "body": "I am using Tensorflow 1.4.0.\r\n\r\nAccording to this [blog post](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html), we can use the weights argument in the call to Embedding to specify some matrix that represents a pre-trained word embeddings (see the section titled Preparing the Embedding Layer).\r\n\r\nHowever, this code does not work:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nfrom tensorflow.python.estimator.model_fn import EstimatorSpec\r\nfrom tensorflow.contrib.keras.api.keras.layers import Embedding, Dense\r\nfrom tensorflow.contrib.keras.api.keras.initializers import Constant\r\n\r\n\r\ndef model_fn(features, labels, mode):\r\n    x = tf.constant([[1]])\r\n    labels = tf.constant([[10.]])\r\n    \r\n    # Let m be our pre-trained word embeddings\r\n    m = np.array([[1, 2], [3, 4]], np.float32)\r\n    with tf.name_scope('Embedding_Layer'):\r\n        # Create an embedding layer and load m into it\r\n        n = Embedding(2, 2, weights=[m], input_length=1, name='embedding_matrix_1', trainable=False)\r\n\r\n    lookup = n(x)\r\n    lookup = tf.Print(lookup, [lookup])\r\n\r\n    preds = Dense(1)(lookup)\r\n    loss = tf.reduce_mean(labels - preds)\r\n    train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss, tf.train.get_global_step())\r\n\r\n    eval_metric_ops = {'accuracy': tf.metrics.accuracy(labels, preds)}\r\n    return EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)\r\n\r\n\r\nmodel = tf.estimator.Estimator(model_fn)\r\nmodel.train(input_fn=lambda: None, steps=1)\r\n```\r\n\r\n`lookup` should print [[3 4]] but instead random numbers are printed.\r\n\r\nThe solution is to define `n` as follows:\r\n\r\n```\r\nn = Embedding(2, 2, embeddings_initializer=Constant(m), input_length=1, name='embedding_matrix_1',\r\n                      trainable=False)\r\n```"}