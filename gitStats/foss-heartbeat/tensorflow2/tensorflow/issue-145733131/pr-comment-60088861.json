{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/60088861", "pull_request_review_id": null, "id": 60088861, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMDg4ODYx", "diff_hunk": "@@ -431,6 +431,304 @@ all\n sorts of domains. We hope this small example gives you some ideas on how to use\n TensorFlow within your own products.\n \n+## Usage with the Go API\n+\n+You can run the same [Inception-v3] model in Go for use in production\n+environments. You can download the archive containing the GraphDef that defines\n+the model like this (running from the root directory of the TensorFlow\n+repository):\n+\n+```bash\n+wget https://storage.googleapis.com/download.tensorflow.org/models/inception_dec_2015.zip -O tensorflow/examples/label_image_go/data/inception_dec_2015.zip\n+\n+unzip tensorflow/examples/label_image_go/data/inception_dec_2015.zip -d tensorflow/examples/label_image_go/data/\n+```\n+\n+Now we are ready to run the example code using the provided test image:\n+\n+```bash\n+go run tensorflow/examples/label_image_go/main.go tensorflow/examples/label_image/data/grace_hopper.jpg\n+```\n+\n+You can also compile the code and run it:\n+\n+```bash\n+go build -o label_image tensorflow/examples/label_image_go/main.go\n+\n+./label_image tensorflow/examples/label_image/data/grace_hopper.jpg\n+```\n+\n+In this case, we're using the default image of [Admiral Grace\n+Hopper](https://en.wikipedia.org/wiki/Grace_Hopper), and you can see the\n+network correctly identifies she's wearing a military uniform, with a high\n+score of 0.6.\n+\n+\n+<div style=\"width:45%; margin:auto; margin-bottom:10px; margin-top:20px;\">\n+  <img style=\"width:100%\" src=\"../../images/grace_hopper.jpg\">\n+</div>\n+\n+Next, try it out on your own images by supplying the as first parameter, e.g.\n+\n+```bash\n+./label_image my_image.png\n+```\n+\n+If you look inside the [`tensorflow/examples/label_image/main.cc`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image_go/main.go)\n+file, you can find out\n+how it works. We hope this code will help you integrate TensorFlow into\n+your own applications, so we will walk step by step through the main functions:\n+\n+The constants defines where the files are loaded from, and properties of the input images.\n+The model expects to get square 299x299 RGB images, so those are the `cInputWidth`\n+and `cInputHeight` flags. We also need to scale the pixel values from integers that\n+are between 0 and 255 to the floating point values that the graph operates on.\n+We control the scaling with the `cInputMean` and `cInputStd` constants: we first subtract\n+`cInputMean` from each pixel value, then divide it by `cInputStd`.\n+\n+These values probably look somewhat magical, but they are just defined by the \n+original model author based on what he/she wanted to use as input images for \n+training. If you have a graph that you've trained yourself, you'll just need\n+to adjust the values to match whatever you used during your training process.\n+\n+You can see how they're applied to an image in the [`readTensorFromImageFile()`]\n+(https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/label_image_go/main.go#L64)\n+function.\n+\n+```Go\n+// readTensorFromImageFile Given an image file name, read in the data, try to\n+// decode it as an image, resize it to the requested size, and then scale the\n+// values as desired.\n+func readTensorFromImageFile(filePath string) *tf.Tensor\n+```\n+We start by creating a `GraphDefBuilder`, which is an object we can use to\n+specify a model to run or load.\n+\n+```Go\n+graph := tf.NewGraph()\n+\n+// Load the image file path into the graph as a constant, and read its\n+// content.\n+fileNameNode, _ := graph.Constant(\"file_name\", filePath)\n+fileReader, _ := graph.Op(\"ReadFile\", \"file_reader\", []*tf.GraphNode{fileNameNode}, \"\", nil)\n+```\n+We then start creating nodes for the small model we want to run\n+to load, resize, and scale the pixel values to get the result the main model\n+expects as its input. The first node we create is just a `Const` op that holds a\n+tensor with the file name of the image we want to load. That's then passed as the\n+first input to the `ReadFile` op. As you can see from the example, the second parameter\n+corresponds to the name of the Graph node to be added, the third are the inputs of this node, the\n+fourth  the device where we want this node to be executer, if we leave this parameter as an empty\n+string we are not specifying any special preference, and the last paramte corresponds to the\n+necessary attributes to execute this operation.\n+\n+```Go\n+// Now try to figure out what kind of file it is and decode it.\n+if filePath[len(filePath)-4:] == \".png\" {\n+\timageReader, _ = graph.Op(\"DecodePng\", \"png_reader\", []*tf.GraphNode{fileReader}, \"\", map[string]interface{}{\n+\t\t\"channels\": int64(3),\n+\t})\n+} else {\n+\t// Assume if it's not a PNG then it must be a JPEG.\n+\timageReader, _ = graph.Op(\"DecodeJpeg\", \"jpeg_reader\", []*tf.GraphNode{fileReader}, \"\", map[string]interface{}{\n+\t\t\"channels\": int64(3),\n+\t})\n+}\n+\n+// Now cast the image data to float so we can do normal math on it. In\n+// the attributes we have to specify the output datatype that we want.\n+floatCaster, _ := graph.Op(\"Cast\", \"float_caster\", []*tf.GraphNode{imageReader}, \"\", map[string]interface{}{\n+\t\"DstT\": tf.DtFloat,\n+})\n+\n+// The convention for image ops in TensorFlow is that all images are expected\n+// to be in batches, so that they're four-dimensional arrays with indices of\n+// [batch, height, width, channel]. Because we only have a single image, we\n+// have to add a batch dimension of 1 to the start with ExpandDims operation.\n+dimIndex, _ := graph.Constant(\"dim_index\", []int32{0})\n+dimsExpander, _ := graph.Op(\"ExpandDims\", \"dims_expander\", []*tf.GraphNode{floatCaster, dimIndex}, \"\", map[string]interface{}{\n+\t\"T\":   tf.DtFloat,\n+\t\"dim\": 0,\n+})\n+\n+// Bilinearly resize the image to fit the required dimensions.\n+sizeDims, _ := graph.Constant(\"size_dims\", []int32{cInputWidth, cInputHeight})\n+size, _ := graph.Op(\"ResizeBilinear\", \"size\", []*tf.GraphNode{dimsExpander, sizeDims}, \"\", map[string]interface{}{\n+\t\"T\": tf.DtFloat,\n+})\n+\n+// Subtract the mean and divide by the scale.\n+inputMean, _ := graph.Constant(\"input_mean\", float32(cInputMean))\n+subMean, _ := graph.Op(\"Sub\", \"sub_mean\", []*tf.GraphNode{size, inputMean}, \"\", nil)\n+inputStd, _ := graph.Constant(\"input_std\", float32(cInputStd))\n+_, _ = graph.Op(\"Div\", \"normalized\", []*tf.GraphNode{subMean, inputStd}, \"\", nil)\n+```\n+\n+We then keep adding more nodes, to decode the file data as an image, to cast the\n+integers into floating point values, to resize it, and then finally to run the\n+subtraction and division operations on the pixel values.\n+\n+```Go\n+// Create the session and extend the Graph\n+s, err := tf.NewSession()\n+s.ExtendGraph(graph)\n+```\n+At the end of this we have a model definition stored in the graph variable, which we\n+load into a new session with `ExtendGraph`\n+\n+```Go\n+/// This runs the GraphDef network definition that we've just constructed, and\n+// returns the results in the output tensor.\n+out, err := s.Run(nil, []string{\"normalized\"}, nil)\n+```\n+Then we execute the loaded graph on the session with `Run` specifying which\n+node or nodes we want to get the output from as second parameter. The function\n+Run will return the outputs as []\\*tensorflow.Tensor in the same order\n+specified on the second input paramater\n+\n+This gives us a slice of `\\*tensorflow.Tensor`, which in this case we know will only be a", "path": "tensorflow/g3doc/tutorials/image_recognition/index.md", "position": null, "original_position": 159, "commit_id": "942760424141d0f5a930982d5e78aaeb05869488", "original_commit_id": "a33e29358d08de8231b38d57ac08ed5257dc1095", "user": {"login": "tmc", "id": 3977, "node_id": "MDQ6VXNlcjM5Nzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3977?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tmc", "html_url": "https://github.com/tmc", "followers_url": "https://api.github.com/users/tmc/followers", "following_url": "https://api.github.com/users/tmc/following{/other_user}", "gists_url": "https://api.github.com/users/tmc/gists{/gist_id}", "starred_url": "https://api.github.com/users/tmc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tmc/subscriptions", "organizations_url": "https://api.github.com/users/tmc/orgs", "repos_url": "https://api.github.com/users/tmc/repos", "events_url": "https://api.github.com/users/tmc/events{/privacy}", "received_events_url": "https://api.github.com/users/tmc/received_events", "type": "User", "site_admin": false}, "body": "s/\\//\n", "created_at": "2016-04-18T16:23:49Z", "updated_at": "2016-05-05T08:12:14Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1771#discussion_r60088861", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1771", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/60088861"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1771#discussion_r60088861"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1771"}}, "body_html": "<p>s///</p>", "body_text": "s///"}