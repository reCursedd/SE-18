{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7199", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7199/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7199/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7199/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7199", "id": 204739102, "node_id": "MDU6SXNzdWUyMDQ3MzkxMDI=", "number": 7199, "title": "Incorrect computation results on /gpu:1 on dual K80 servers", "user": {"login": "neggert", "id": 1271014, "node_id": "MDQ6VXNlcjEyNzEwMTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/1271014?v=4", "gravatar_id": "", "url": "https://api.github.com/users/neggert", "html_url": "https://github.com/neggert", "followers_url": "https://api.github.com/users/neggert/followers", "following_url": "https://api.github.com/users/neggert/following{/other_user}", "gists_url": "https://api.github.com/users/neggert/gists{/gist_id}", "starred_url": "https://api.github.com/users/neggert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/neggert/subscriptions", "organizations_url": "https://api.github.com/users/neggert/orgs", "repos_url": "https://api.github.com/users/neggert/repos", "events_url": "https://api.github.com/users/neggert/events{/privacy}", "received_events_url": "https://api.github.com/users/neggert/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-02-01T23:36:14Z", "updated_at": "2017-03-03T23:44:23Z", "closed_at": "2017-03-03T23:44:23Z", "author_association": "NONE", "body_html": "<p>We have a bunch of servers with 2 K80 cards each (2 GPUs per card, for a total of 4 GPUs per machine). We're having problems with even simple computations on <code>/gpu:1</code> returning wrong results. This happens across multiple machines. Interestingly, if we re-map the GPUs, using <code>CUDA_VISIBLE_DEVICES</code> (e.g. <code>CUDA_VISIBLE_DEVICES=\"3,2,1,0\"</code>), it's still always <code>/gpu:1</code> that has problems.</p>\n<p>To reproduce, I wrote a short script that multiplies random matrices together on each GPU, comparing the results with Numpy:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef test(sess, device):\n    x = tf.placeholder(tf.float32, (500, 500))\n    w_np = np.random.random((500, 20)).astype(np.float32)\n    w = tf.Variable(w_np, dtype=tf.float32)\n\n    with tf.device(\"/\" + device):\n        y = tf.matmul(x, w)\n\n    x_np = np.random.random((500, 500)).astype(np.float32)\n    y_np = np.dot(x_np, w_np)\n\n    with sess.as_default():\n        sess.run([tf.initialize_all_variables()])\n        y_tf = sess.run([y], feed_dict={x: x_np})\n\n    assert(np.all(np.abs(y_tf - y_np) &lt; 1e-3))\n\n\ndef test_all():\n    sess = tf.Session()\n    for i in range(4):\n        device = \"/gpu:{}\".format(i)\n        try:\n            for _ in range(10):\n                test(sess, device)\n        except AssertionError:\n            print \"GPU {} FAILED!\".format(i)\n            continue\n        print \"GPU {} passed\".format(i)\n\n\nif __name__ == \"__main__\":\n    test_all()\n</code></pre>\n<p>After some initialization logging, this produces the output:</p>\n<pre><code>GPU 0 passed\nGPU 1 FAILED!\nGPU 2 passed\nGPU 3 passed\n</code></pre>\n<p>Environment Information (we've tried several different permutations):<br>\nOS: CentOS 7.2<br>\nCUDA: 8.0.44 and 7.5.17<br>\nCUDNN: 5.1 and 5.0<br>\nTensorflow: 0.11.0rc0, 0.11.0, 0.12.1, 1.0.0rc0<br>\nNvidia drivers: 352.39, 367.48</p>\n<p>We have another machine running Ubuntu 16.04 with 4x Titan Xs. All 4 GPUs pass this test script there.</p>", "body_text": "We have a bunch of servers with 2 K80 cards each (2 GPUs per card, for a total of 4 GPUs per machine). We're having problems with even simple computations on /gpu:1 returning wrong results. This happens across multiple machines. Interestingly, if we re-map the GPUs, using CUDA_VISIBLE_DEVICES (e.g. CUDA_VISIBLE_DEVICES=\"3,2,1,0\"), it's still always /gpu:1 that has problems.\nTo reproduce, I wrote a short script that multiplies random matrices together on each GPU, comparing the results with Numpy:\nimport tensorflow as tf\nimport numpy as np\n\ndef test(sess, device):\n    x = tf.placeholder(tf.float32, (500, 500))\n    w_np = np.random.random((500, 20)).astype(np.float32)\n    w = tf.Variable(w_np, dtype=tf.float32)\n\n    with tf.device(\"/\" + device):\n        y = tf.matmul(x, w)\n\n    x_np = np.random.random((500, 500)).astype(np.float32)\n    y_np = np.dot(x_np, w_np)\n\n    with sess.as_default():\n        sess.run([tf.initialize_all_variables()])\n        y_tf = sess.run([y], feed_dict={x: x_np})\n\n    assert(np.all(np.abs(y_tf - y_np) < 1e-3))\n\n\ndef test_all():\n    sess = tf.Session()\n    for i in range(4):\n        device = \"/gpu:{}\".format(i)\n        try:\n            for _ in range(10):\n                test(sess, device)\n        except AssertionError:\n            print \"GPU {} FAILED!\".format(i)\n            continue\n        print \"GPU {} passed\".format(i)\n\n\nif __name__ == \"__main__\":\n    test_all()\n\nAfter some initialization logging, this produces the output:\nGPU 0 passed\nGPU 1 FAILED!\nGPU 2 passed\nGPU 3 passed\n\nEnvironment Information (we've tried several different permutations):\nOS: CentOS 7.2\nCUDA: 8.0.44 and 7.5.17\nCUDNN: 5.1 and 5.0\nTensorflow: 0.11.0rc0, 0.11.0, 0.12.1, 1.0.0rc0\nNvidia drivers: 352.39, 367.48\nWe have another machine running Ubuntu 16.04 with 4x Titan Xs. All 4 GPUs pass this test script there.", "body": "We have a bunch of servers with 2 K80 cards each (2 GPUs per card, for a total of 4 GPUs per machine). We're having problems with even simple computations on `/gpu:1` returning wrong results. This happens across multiple machines. Interestingly, if we re-map the GPUs, using `CUDA_VISIBLE_DEVICES` (e.g. `CUDA_VISIBLE_DEVICES=\"3,2,1,0\"`), it's still always `/gpu:1` that has problems.\r\n\r\nTo reproduce, I wrote a short script that multiplies random matrices together on each GPU, comparing the results with Numpy:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef test(sess, device):\r\n    x = tf.placeholder(tf.float32, (500, 500))\r\n    w_np = np.random.random((500, 20)).astype(np.float32)\r\n    w = tf.Variable(w_np, dtype=tf.float32)\r\n\r\n    with tf.device(\"/\" + device):\r\n        y = tf.matmul(x, w)\r\n\r\n    x_np = np.random.random((500, 500)).astype(np.float32)\r\n    y_np = np.dot(x_np, w_np)\r\n\r\n    with sess.as_default():\r\n        sess.run([tf.initialize_all_variables()])\r\n        y_tf = sess.run([y], feed_dict={x: x_np})\r\n\r\n    assert(np.all(np.abs(y_tf - y_np) < 1e-3))\r\n\r\n\r\ndef test_all():\r\n    sess = tf.Session()\r\n    for i in range(4):\r\n        device = \"/gpu:{}\".format(i)\r\n        try:\r\n            for _ in range(10):\r\n                test(sess, device)\r\n        except AssertionError:\r\n            print \"GPU {} FAILED!\".format(i)\r\n            continue\r\n        print \"GPU {} passed\".format(i)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    test_all()\r\n```\r\n\r\nAfter some initialization logging, this produces the output:\r\n\r\n```\r\nGPU 0 passed\r\nGPU 1 FAILED!\r\nGPU 2 passed\r\nGPU 3 passed\r\n```\r\n\r\nEnvironment Information (we've tried several different permutations):\r\nOS: CentOS 7.2\r\nCUDA: 8.0.44 and 7.5.17\r\nCUDNN: 5.1 and 5.0\r\nTensorflow: 0.11.0rc0, 0.11.0, 0.12.1, 1.0.0rc0\r\nNvidia drivers: 352.39, 367.48\r\n\r\nWe have another machine running Ubuntu 16.04 with 4x Titan Xs. All 4 GPUs pass this test script there.\r\n"}