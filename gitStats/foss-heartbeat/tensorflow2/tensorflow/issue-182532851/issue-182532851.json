{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4914", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4914/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4914/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4914/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4914", "id": 182532851, "node_id": "MDU6SXNzdWUxODI1MzI4NTE=", "number": 4914, "title": "Keeping gradient of sqrt(x) stable for x = 0", "user": {"login": "egpbos", "id": 6146598, "node_id": "MDQ6VXNlcjYxNDY1OTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/6146598?v=4", "gravatar_id": "", "url": "https://api.github.com/users/egpbos", "html_url": "https://github.com/egpbos", "followers_url": "https://api.github.com/users/egpbos/followers", "following_url": "https://api.github.com/users/egpbos/following{/other_user}", "gists_url": "https://api.github.com/users/egpbos/gists{/gist_id}", "starred_url": "https://api.github.com/users/egpbos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/egpbos/subscriptions", "organizations_url": "https://api.github.com/users/egpbos/orgs", "repos_url": "https://api.github.com/users/egpbos/repos", "events_url": "https://api.github.com/users/egpbos/events{/privacy}", "received_events_url": "https://api.github.com/users/egpbos/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2016-10-12T13:57:36Z", "updated_at": "2018-02-22T21:42:26Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I'm minimizing a function that contains a few <code>tf.sqrt(c * x)</code> terms. The <code>x</code> is a <code>tf.Variable</code> and <code>c</code> is a <code>tf.constant</code> that is sometimes zero. A <code>NaN</code> inevitably presents itself. In my case, the gradient is to <code>c</code>, which is <code>x * 0.5/sqrt(c * x)</code> and which equals <code>0 * inf = NaN</code> when <code>c</code> is <code>0</code>.</p>\n<p>When such a <code>sqrt</code> is deeply buried in your function, it can be quite an effort to dig out where the <code>NaN</code> is coming from. I can understand and appreciate the fact that there is no check for zero in the <code>sqrt_grad</code> operator. However, I feel that debugging could be easier for ops that are known to be unstable in some numerical range.</p>\n<p>Two possible fixes would be:</p>\n<ol>\n<li>Add exceptions to the documentation of these ops. Right now this is not even indicated for <code>tf.div</code>, for instance. Since the use-cases of TensorFlow almost always mean that gradients will be involved, the allowed range should also be mentioned for the gradient, if different from that of the op itself.</li>\n<li>Add debug-mode versions of the ops. These could include <code>NaN</code> and <code>inf</code> checks.</li>\n</ol>\n<p>By the way, I was using the <code>tf.contrib.opt.ScipyOptimizerInterface</code> for the minimization, which does not support manually changing the gradients by using <code>compute_gradients</code> and <code>apply_gradients</code>. That's beside the point, though.</p>\n<p>Below some example code for completeness' sake. The differences in outcome only add to the confusion.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> absolute_import, division, print_function\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nc <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.0</span>)\n\nsqrt_grad <span class=\"pl-k\">=</span> tf.gradients(tf.sqrt(c), c)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> another possibility is when another factor in the argument is zero</span>\nx <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1</span>.)\nsqrt_x_grad <span class=\"pl-k\">=</span> tf.gradients(tf.sqrt(x <span class=\"pl-k\">*</span> c), x)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> try to use select to filter out the NaN</span>\nselsqrt_grad <span class=\"pl-k\">=</span> tf.gradients(tf.select(c <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>, tf.sqrt(c), <span class=\"pl-c1\">0</span>), c)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> try clipping of the sqrt</span>\nclipsqrt_grad <span class=\"pl-k\">=</span> tf.gradients(tf.clip_by_value(tf.sqrt(c), <span class=\"pl-c1\">1e-10</span>, <span class=\"pl-c1\">1</span>), c)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> clip the argument of the sqrt --&gt; only numerically stable option</span>\nclipargsqrt_grad <span class=\"pl-k\">=</span> tf.gradients(tf.sqrt(tf.clip_by_value(c, <span class=\"pl-c1\">1e-10</span>, <span class=\"pl-c1\">1</span>)), c)\n\ninit_op <span class=\"pl-k\">=</span> tf.initialize_all_variables()\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(init_op)\n\n    <span class=\"pl-c1\">print</span>(sess.run([sqrt_grad, sqrt_x_grad, selsqrt_grad,\n                    clipsqrt_grad, clipargsqrt_grad]))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> [[inf], [nan], [nan], [nan], [0.0]]</span></pre></div>", "body_text": "I'm minimizing a function that contains a few tf.sqrt(c * x) terms. The x is a tf.Variable and c is a tf.constant that is sometimes zero. A NaN inevitably presents itself. In my case, the gradient is to c, which is x * 0.5/sqrt(c * x) and which equals 0 * inf = NaN when c is 0.\nWhen such a sqrt is deeply buried in your function, it can be quite an effort to dig out where the NaN is coming from. I can understand and appreciate the fact that there is no check for zero in the sqrt_grad operator. However, I feel that debugging could be easier for ops that are known to be unstable in some numerical range.\nTwo possible fixes would be:\n\nAdd exceptions to the documentation of these ops. Right now this is not even indicated for tf.div, for instance. Since the use-cases of TensorFlow almost always mean that gradients will be involved, the allowed range should also be mentioned for the gradient, if different from that of the op itself.\nAdd debug-mode versions of the ops. These could include NaN and inf checks.\n\nBy the way, I was using the tf.contrib.opt.ScipyOptimizerInterface for the minimization, which does not support manually changing the gradients by using compute_gradients and apply_gradients. That's beside the point, though.\nBelow some example code for completeness' sake. The differences in outcome only add to the confusion.\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nc = tf.Variable(0.0)\n\nsqrt_grad = tf.gradients(tf.sqrt(c), c)\n\n# another possibility is when another factor in the argument is zero\nx = tf.Variable(1.)\nsqrt_x_grad = tf.gradients(tf.sqrt(x * c), x)\n\n# try to use select to filter out the NaN\nselsqrt_grad = tf.gradients(tf.select(c > 0, tf.sqrt(c), 0), c)\n\n# try clipping of the sqrt\nclipsqrt_grad = tf.gradients(tf.clip_by_value(tf.sqrt(c), 1e-10, 1), c)\n\n# clip the argument of the sqrt --> only numerically stable option\nclipargsqrt_grad = tf.gradients(tf.sqrt(tf.clip_by_value(c, 1e-10, 1)), c)\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n\n    print(sess.run([sqrt_grad, sqrt_x_grad, selsqrt_grad,\n                    clipsqrt_grad, clipargsqrt_grad]))\n    # [[inf], [nan], [nan], [nan], [0.0]]", "body": "I'm minimizing a function that contains a few `tf.sqrt(c * x)` terms. The `x` is a `tf.Variable` and `c` is a `tf.constant` that is sometimes zero. A `NaN` inevitably presents itself. In my case, the gradient is to `c`, which is `x * 0.5/sqrt(c * x)` and which equals `0 * inf = NaN` when `c` is `0`.\n\nWhen such a `sqrt` is deeply buried in your function, it can be quite an effort to dig out where the `NaN` is coming from. I can understand and appreciate the fact that there is no check for zero in the `sqrt_grad` operator. However, I feel that debugging could be easier for ops that are known to be unstable in some numerical range.\n\nTwo possible fixes would be:\n1. Add exceptions to the documentation of these ops. Right now this is not even indicated for `tf.div`, for instance. Since the use-cases of TensorFlow almost always mean that gradients will be involved, the allowed range should also be mentioned for the gradient, if different from that of the op itself.\n2. Add debug-mode versions of the ops. These could include `NaN` and `inf` checks.\n\nBy the way, I was using the `tf.contrib.opt.ScipyOptimizerInterface` for the minimization, which does not support manually changing the gradients by using `compute_gradients` and `apply_gradients`. That's beside the point, though.\n\nBelow some example code for completeness' sake. The differences in outcome only add to the confusion.\n\n``` python\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\n\nc = tf.Variable(0.0)\n\nsqrt_grad = tf.gradients(tf.sqrt(c), c)\n\n# another possibility is when another factor in the argument is zero\nx = tf.Variable(1.)\nsqrt_x_grad = tf.gradients(tf.sqrt(x * c), x)\n\n# try to use select to filter out the NaN\nselsqrt_grad = tf.gradients(tf.select(c > 0, tf.sqrt(c), 0), c)\n\n# try clipping of the sqrt\nclipsqrt_grad = tf.gradients(tf.clip_by_value(tf.sqrt(c), 1e-10, 1), c)\n\n# clip the argument of the sqrt --> only numerically stable option\nclipargsqrt_grad = tf.gradients(tf.sqrt(tf.clip_by_value(c, 1e-10, 1)), c)\n\ninit_op = tf.initialize_all_variables()\n\nwith tf.Session() as sess:\n    sess.run(init_op)\n\n    print(sess.run([sqrt_grad, sqrt_x_grad, selsqrt_grad,\n                    clipsqrt_grad, clipargsqrt_grad]))\n    # [[inf], [nan], [nan], [nan], [0.0]]\n```\n"}