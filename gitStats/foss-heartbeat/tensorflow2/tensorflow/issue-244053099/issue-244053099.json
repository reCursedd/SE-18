{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11608", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11608/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11608/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11608/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11608", "id": 244053099, "node_id": "MDU6SXNzdWUyNDQwNTMwOTk=", "number": 11608, "title": "Distributed Tensorflow : Cannot assign a device for operation...", "user": {"login": "SebastianPopescu", "id": 19705620, "node_id": "MDQ6VXNlcjE5NzA1NjIw", "avatar_url": "https://avatars2.githubusercontent.com/u/19705620?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SebastianPopescu", "html_url": "https://github.com/SebastianPopescu", "followers_url": "https://api.github.com/users/SebastianPopescu/followers", "following_url": "https://api.github.com/users/SebastianPopescu/following{/other_user}", "gists_url": "https://api.github.com/users/SebastianPopescu/gists{/gist_id}", "starred_url": "https://api.github.com/users/SebastianPopescu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SebastianPopescu/subscriptions", "organizations_url": "https://api.github.com/users/SebastianPopescu/orgs", "repos_url": "https://api.github.com/users/SebastianPopescu/repos", "events_url": "https://api.github.com/users/SebastianPopescu/events{/privacy}", "received_events_url": "https://api.github.com/users/SebastianPopescu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-07-19T14:06:22Z", "updated_at": "2017-07-19T21:26:06Z", "closed_at": "2017-07-19T21:26:06Z", "author_association": "NONE", "body_html": "<p>I am trying to use a distributed version of Self-Normalizing Networks on MNIST</p>\n<p>The full code is provided here:<br>\n`<br>\nclass DNN_forward_MC(object):</p>\n<pre><code>    def __init__(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch,X_train,Y_train,X_test,Y_test):\n\n\n            # cluster specification\n            parameter_servers = [\"localhost:2222\"]\n            workers = [     \"localhost:2223\", \n                    \"localhost:2224\",\n                    \"localhost:2225\"]\n\n            cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\n\n            # input flags\n            tf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\n            tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n            FLAGS = tf.app.flags.FLAGS\n\n            # start a server for a specific task\n            server = tf.train.Server(\n                                    cluster,\n                                    job_name=FLAGS.job_name,\n                                    task_index=FLAGS.task_index)\n\n\n            is_chief=(FLAGS.task_index == 0)\n            if FLAGS.job_name == \"ps\":\n                    server.join()\n\n            elif FLAGS.job_name == \"worker\":\n                    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d/cpu:0\" % FLAGS.task_index,\n            cluster=cluster,ps_device=\"/job:ps/cpu:0\")):\n\n                            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n                            self.initialize_model(num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch)\n\n                            predictions = self.predict_training(tip='redus')\n                            predictions_training_full = self.predict_training(tip='full')\n\n                            predictions_testing_full = self.predict_testing()\n\n                            cost =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y_train, logits=predictions))\n                            opt = tf.train.GradientDescentOptimizer(1e-3)\n                            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=3,total_num_replicas=3)\n                            train_op = opt.minimize(cost,global_step=global_step)\n                            init_op = tf.global_variables_initializer()\n\n                            #grads_and_vars = optimizer.compute_gradients(cost, tf.trainable_variables())\n\n                            local_init_op = opt.local_step_init_op\n                            if is_chief:\n                                    local_init_op = opt.chief_init_op\n\n                            ready_for_local_init_op = opt.ready_for_local_init_op\n\n                            # Initial token and chief queue runners required by the sync_replicas mode\n                            chief_queue_runner = opt.get_chief_queue_runner()\n                            sync_init_op = opt.get_init_tokens_op()\n\n                    train_dir = tempfile.mkdtemp()\n                    sv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)\n\n                    if is_chief:\n                            print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n                    else:\n                            print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\n\n\n                    sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=False,device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\n\n                    with sv.prepare_or_wait_for_session(server.target,config=sess_config) as sess:\n                    #with tf.train.MonitoredTrainingSession(master=server.target,is_chief=is_chief,checkpoint_dir='./logs/',save_checkpoint_secs=60,save_summaries_steps=1) as sess:        \n                            while True:\n\n                                    lista = np.arange(self.num_data)\n                                    np.random.shuffle(lista)\n                                    current_index = lista[:self.num_minibatch]\n\n                                    likelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})\n                                    printare = 'nll for minibatch at iteration '+str(i)+' is '+str(likelihood_now)\n                                    print(printare)\n                                    predictions_now = self.sess.run(predictions_training_full,feed_dict={self.X_train_full:X_train})\n                                    print 'accuracy is : '+str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_train,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))\n                                    if step_now&gt;10:\n                                            break\n\n                            print '*******training last iteration**********'\n                            \n                            if is_chief:\n\n                                    predictions_now = self.sess.run(predictions_testing_full,feed_dict={self.X_test:X_test})\n                                    print '***********testing*************'\n\n                                    print 'accuracy at testing time is :' +str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_test,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))\n\n                    sv.stop()\n    def initialize_model(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch):\n\n            ### model using just Hinton's Dropout\n            self.sess = tf.Session()\n            self.num_minibatch = num_minibatch\n            self.num_data = num_data\n            self.num_test = num_test\n            self.dim_input = dim_input\n            self.dim_output = dim_output\n            self.num_hidden_layers = num_hidden_layers\n            self.num_hidden_dims = num_hidden_dims\n\n            self.keep_prob = keep_prob\n            self.alpha_p = -1.7580993408473766\n            q = 1.0 - keep_prob\n            prod  = q + np.power(self.alpha_p,2)*q*(1-q)\n            self.a_affine = np.power(prod,-0.5)\n            self.b_affine = -self.a_affine * (self.alpha_p*(1-q))\n\n            self.X_train_full = tf.placeholder(shape=(self.num_data,dim_input),dtype=tf.float32)\n    \n            self.X_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_input))\n            self.Y_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_output))\n            self.X_test = tf.placeholder(tf.float32,shape=(num_test,dim_input))\n            self.Y_test = tf.placeholder(tf.float32,shape=(num_test,dim_output))\n\n            self.weights = []\n            self.biases = []\n\n            ### create parameters for Global DNN\n            for j in range(self.num_hidden_layers+1):                                                                                                              \n                    self.weights.append(tf.Variable(tf.random_normal(shape=(num_hidden_dims[j-1],self.num_hidden_dims[j]),stddev=1.0/num_hidden_dims[j-1]),dtype=tf.float32,name='weights_'+str(j)))\n                    self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[j],)),dtype=tf.float32,name='biases'+str(j)))          \n            \n            self.weights.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers],self.num_hidden_dims[self.num_hidden_layers+1])\n            ,stddev=1.0/self.num_hidden_dims[self.num_hidden_layers]),dtype=tf.float32,name='weights_'+str(self.num_hidden_layers+1)))\n            self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers+1],)),dtype=tf.float32,name='biases_'+str(self.num_hidden_layers+1)))\n\n           \n    def selu(self,x):\n\n            alpha= 1.6732632423543772848170429916717\n            lamb = 1.0507009873554804934193349852946\n            return lamb * tf.where(x&gt;0.0,x,alpha * tf.nn.elu(x))\n\n    def alpha_dropout(self,x):\n\n            mask = tf.cast(tf.random_uniform(shape=x.get_shape()) &lt; ( 1.0 - self.keep_prob ),dtype=tf.float32)\n            x_masked = tf.multiply(x,mask)\n            out = x_masked * self.a_affine + self.b_affine\n            return out      \n\n    def model(self,input,apply_dropout=True):\n\n            for j in range(1,self.num_hidden_layers +1):\n\n                    input = self.selu(tf.add(tf.matmul(input,self.weights[j]),self.biases[j]))\n                    if apply_dropout:\n\n                            input = self.alpha_dropout(input)\n\n            final_output = tf.add(tf.matmul(input,self.weights[self.num_hidden_layers+1]),self.biases[self.num_hidden_layers+1])\n\n            return final_output\n\n\n    def predict_training(self,tip):\n\n            if tip == 'full':\n                    prediction_DNN_global = self.model(input=self.X_train_full,apply_dropout=False)\n            else:\n                    prediction_DNN_global = self.model(input = self.X_train,apply_dropout=True)\n\n            return prediction_DNN_global\n\n            \n    def predict_testing(self):\n\n            prediction_DNN_global = self.model(input = self.X_test,apply_dropout=False)\n\n            return prediction_DNN_global\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':</p>\n<pre><code>    training_data = np.genfromtxt('/home/spopescu/mnist_train.csv',dtype=np.float64,delimiter=',')\n    testing_data = np.genfromtxt('/home/spopescu/mnist_test.csv',dtype=np.float64,delimiter=',')\n\n    X_training = training_data[:,1:]\n    X_testing = testing_data[:,1:]\n    Y_training = one_hot_encoder(training_data[:,0].reshape(X_training.shape[0],1))\n    Y_testing = one_hot_encoder(testing_data[:,0].reshape(X_testing.shape[0],1))\n\n    obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\n</code></pre>\n<p>`</p>\n<p>I am using the following Slurm bash script to send an array job:<br>\n`<br>\n#!/bin/sh<br>\n#SBATCH --mem=20G<br>\n#SBATCH -J SNN<br>\n#SBATCH --array=0-3<br>\n#SBATCH -o slurm-%A_%a.out<br>\n#SBATCH -e slurm-%A_%a.err<br>\n#SBATCH --nodelist=compute-2-3</p>\n<p>task_type=(\"ps\" \"worker\" \"worker\" \"worker\")<br>\ntask_index_array=(0 0 1 2)<br>\nsrun --exclusive -N1 -n1 /home/spopescu/anaconda/bin/python2.7 DNN_forward_MC.py --job_name=${task_type[$SLURM_ARRAY_TASK_ID]} --task_index=${task_index_array[$SLURM_ARRAY_TASK_ID]}<br>\n`</p>\n<p>I am using the latest version of Tensorflow as of today.</p>\n<p>I am getting the following error on my chief worker:</p>\n<p>`<br>\n2017-07-19 14:49:14.087646: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; localhost:2222}<br>\n2017-07-19 14:49:14.087702: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; localhost:2223, 1 -&gt; localhost:2224, 2 -&gt; localhost:2225}<br>\n2017-07-19 14:49:14.091606: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223<br>\n2017-07-19 14:49:15.137109: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 4b5afb77d3aa2b77 with config:<br>\ndevice_filters: \"/job:ps\"<br>\ndevice_filters: \"/job:worker/task:0\"<br>\nallow_soft_placement: true</p>\n<p>Traceback (most recent call last):<br>\nFile \"DNN_forward_MC.py\", line 221, in <br>\nobiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)<br>\nFile \"DNN_forward_MC.py\", line 108, in <strong>init</strong><br>\nlikelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run<br>\nrun_metadata_ptr)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run<br>\nfeed_dict_string, options, run_metadata)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run<br>\ntarget_list, options, run_metadata)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call<br>\nraise type(e)(node_def, op, message)<br>\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.<br>\n[[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]</p>\n<p>Caused by op u'save/RestoreV2_10', defined at:<br>\nFile \"DNN_forward_MC.py\", line 221, in <br>\nobiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)<br>\nFile \"DNN_forward_MC.py\", line 90, in <strong>init</strong><br>\nsv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 300, in <strong>init</strong><br>\nself._init_saver(saver=saver)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 448, in _init_saver<br>\nsaver = saver_mod.Saver()<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1139, in <strong>init</strong><br>\nself.build()<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1170, in build<br>\nrestore_sequentially=self._restore_sequentially)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 691, in build<br>\nrestore_sequentially, reshape)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps<br>\ntensors = self.restore_op(filename_tensor, saveable, preferred_shard)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op<br>\n[spec.tensor.dtype])[0])<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2<br>\ndtypes=dtypes, name=name)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op<br>\nop_def=op_def)</p>\n<p>File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in <strong>init</strong><br>\nself._traceback = _extract_stack()</p>\n<p>InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.<br>\n[[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]<br>\n`</p>\n<p>I have mostly seen the \"cannot assign a device for operation ...\" error in the case of people using GPU without specifying allow_soft_placement=True but in my case I am using just CPUs.</p>\n<p>I have seen another instance of this error when users were not specifying server.target as the device of a session,thereby creating a local session but this is not the case in my code.</p>\n<p>Any help would be much appreciated.</p>", "body_text": "I am trying to use a distributed version of Self-Normalizing Networks on MNIST\nThe full code is provided here:\n`\nclass DNN_forward_MC(object):\n    def __init__(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch,X_train,Y_train,X_test,Y_test):\n\n\n            # cluster specification\n            parameter_servers = [\"localhost:2222\"]\n            workers = [     \"localhost:2223\", \n                    \"localhost:2224\",\n                    \"localhost:2225\"]\n\n            cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\n\n            # input flags\n            tf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\n            tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\n            FLAGS = tf.app.flags.FLAGS\n\n            # start a server for a specific task\n            server = tf.train.Server(\n                                    cluster,\n                                    job_name=FLAGS.job_name,\n                                    task_index=FLAGS.task_index)\n\n\n            is_chief=(FLAGS.task_index == 0)\n            if FLAGS.job_name == \"ps\":\n                    server.join()\n\n            elif FLAGS.job_name == \"worker\":\n                    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d/cpu:0\" % FLAGS.task_index,\n            cluster=cluster,ps_device=\"/job:ps/cpu:0\")):\n\n                            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n                            self.initialize_model(num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch)\n\n                            predictions = self.predict_training(tip='redus')\n                            predictions_training_full = self.predict_training(tip='full')\n\n                            predictions_testing_full = self.predict_testing()\n\n                            cost =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y_train, logits=predictions))\n                            opt = tf.train.GradientDescentOptimizer(1e-3)\n                            opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=3,total_num_replicas=3)\n                            train_op = opt.minimize(cost,global_step=global_step)\n                            init_op = tf.global_variables_initializer()\n\n                            #grads_and_vars = optimizer.compute_gradients(cost, tf.trainable_variables())\n\n                            local_init_op = opt.local_step_init_op\n                            if is_chief:\n                                    local_init_op = opt.chief_init_op\n\n                            ready_for_local_init_op = opt.ready_for_local_init_op\n\n                            # Initial token and chief queue runners required by the sync_replicas mode\n                            chief_queue_runner = opt.get_chief_queue_runner()\n                            sync_init_op = opt.get_init_tokens_op()\n\n                    train_dir = tempfile.mkdtemp()\n                    sv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)\n\n                    if is_chief:\n                            print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\n                    else:\n                            print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\n\n\n                    sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=False,device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\n\n                    with sv.prepare_or_wait_for_session(server.target,config=sess_config) as sess:\n                    #with tf.train.MonitoredTrainingSession(master=server.target,is_chief=is_chief,checkpoint_dir='./logs/',save_checkpoint_secs=60,save_summaries_steps=1) as sess:        \n                            while True:\n\n                                    lista = np.arange(self.num_data)\n                                    np.random.shuffle(lista)\n                                    current_index = lista[:self.num_minibatch]\n\n                                    likelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})\n                                    printare = 'nll for minibatch at iteration '+str(i)+' is '+str(likelihood_now)\n                                    print(printare)\n                                    predictions_now = self.sess.run(predictions_training_full,feed_dict={self.X_train_full:X_train})\n                                    print 'accuracy is : '+str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_train,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))\n                                    if step_now>10:\n                                            break\n\n                            print '*******training last iteration**********'\n                            \n                            if is_chief:\n\n                                    predictions_now = self.sess.run(predictions_testing_full,feed_dict={self.X_test:X_test})\n                                    print '***********testing*************'\n\n                                    print 'accuracy at testing time is :' +str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_test,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))\n\n                    sv.stop()\n    def initialize_model(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch):\n\n            ### model using just Hinton's Dropout\n            self.sess = tf.Session()\n            self.num_minibatch = num_minibatch\n            self.num_data = num_data\n            self.num_test = num_test\n            self.dim_input = dim_input\n            self.dim_output = dim_output\n            self.num_hidden_layers = num_hidden_layers\n            self.num_hidden_dims = num_hidden_dims\n\n            self.keep_prob = keep_prob\n            self.alpha_p = -1.7580993408473766\n            q = 1.0 - keep_prob\n            prod  = q + np.power(self.alpha_p,2)*q*(1-q)\n            self.a_affine = np.power(prod,-0.5)\n            self.b_affine = -self.a_affine * (self.alpha_p*(1-q))\n\n            self.X_train_full = tf.placeholder(shape=(self.num_data,dim_input),dtype=tf.float32)\n    \n            self.X_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_input))\n            self.Y_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_output))\n            self.X_test = tf.placeholder(tf.float32,shape=(num_test,dim_input))\n            self.Y_test = tf.placeholder(tf.float32,shape=(num_test,dim_output))\n\n            self.weights = []\n            self.biases = []\n\n            ### create parameters for Global DNN\n            for j in range(self.num_hidden_layers+1):                                                                                                              \n                    self.weights.append(tf.Variable(tf.random_normal(shape=(num_hidden_dims[j-1],self.num_hidden_dims[j]),stddev=1.0/num_hidden_dims[j-1]),dtype=tf.float32,name='weights_'+str(j)))\n                    self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[j],)),dtype=tf.float32,name='biases'+str(j)))          \n            \n            self.weights.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers],self.num_hidden_dims[self.num_hidden_layers+1])\n            ,stddev=1.0/self.num_hidden_dims[self.num_hidden_layers]),dtype=tf.float32,name='weights_'+str(self.num_hidden_layers+1)))\n            self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers+1],)),dtype=tf.float32,name='biases_'+str(self.num_hidden_layers+1)))\n\n           \n    def selu(self,x):\n\n            alpha= 1.6732632423543772848170429916717\n            lamb = 1.0507009873554804934193349852946\n            return lamb * tf.where(x>0.0,x,alpha * tf.nn.elu(x))\n\n    def alpha_dropout(self,x):\n\n            mask = tf.cast(tf.random_uniform(shape=x.get_shape()) < ( 1.0 - self.keep_prob ),dtype=tf.float32)\n            x_masked = tf.multiply(x,mask)\n            out = x_masked * self.a_affine + self.b_affine\n            return out      \n\n    def model(self,input,apply_dropout=True):\n\n            for j in range(1,self.num_hidden_layers +1):\n\n                    input = self.selu(tf.add(tf.matmul(input,self.weights[j]),self.biases[j]))\n                    if apply_dropout:\n\n                            input = self.alpha_dropout(input)\n\n            final_output = tf.add(tf.matmul(input,self.weights[self.num_hidden_layers+1]),self.biases[self.num_hidden_layers+1])\n\n            return final_output\n\n\n    def predict_training(self,tip):\n\n            if tip == 'full':\n                    prediction_DNN_global = self.model(input=self.X_train_full,apply_dropout=False)\n            else:\n                    prediction_DNN_global = self.model(input = self.X_train,apply_dropout=True)\n\n            return prediction_DNN_global\n\n            \n    def predict_testing(self):\n\n            prediction_DNN_global = self.model(input = self.X_test,apply_dropout=False)\n\n            return prediction_DNN_global\n\nif name == 'main':\n    training_data = np.genfromtxt('/home/spopescu/mnist_train.csv',dtype=np.float64,delimiter=',')\n    testing_data = np.genfromtxt('/home/spopescu/mnist_test.csv',dtype=np.float64,delimiter=',')\n\n    X_training = training_data[:,1:]\n    X_testing = testing_data[:,1:]\n    Y_training = one_hot_encoder(training_data[:,0].reshape(X_training.shape[0],1))\n    Y_testing = one_hot_encoder(testing_data[:,0].reshape(X_testing.shape[0],1))\n\n    obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\n\n`\nI am using the following Slurm bash script to send an array job:\n`\n#!/bin/sh\n#SBATCH --mem=20G\n#SBATCH -J SNN\n#SBATCH --array=0-3\n#SBATCH -o slurm-%A_%a.out\n#SBATCH -e slurm-%A_%a.err\n#SBATCH --nodelist=compute-2-3\ntask_type=(\"ps\" \"worker\" \"worker\" \"worker\")\ntask_index_array=(0 0 1 2)\nsrun --exclusive -N1 -n1 /home/spopescu/anaconda/bin/python2.7 DNN_forward_MC.py --job_name=${task_type[$SLURM_ARRAY_TASK_ID]} --task_index=${task_index_array[$SLURM_ARRAY_TASK_ID]}\n`\nI am using the latest version of Tensorflow as of today.\nI am getting the following error on my chief worker:\n`\n2017-07-19 14:49:14.087646: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\n2017-07-19 14:49:14.087702: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225}\n2017-07-19 14:49:14.091606: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223\n2017-07-19 14:49:15.137109: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 4b5afb77d3aa2b77 with config:\ndevice_filters: \"/job:ps\"\ndevice_filters: \"/job:worker/task:0\"\nallow_soft_placement: true\nTraceback (most recent call last):\nFile \"DNN_forward_MC.py\", line 221, in \nobiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\nFile \"DNN_forward_MC.py\", line 108, in init\nlikelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\nrun_metadata_ptr)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\nfeed_dict_string, options, run_metadata)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\ntarget_list, options, run_metadata)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\nraise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.\n[[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]\nCaused by op u'save/RestoreV2_10', defined at:\nFile \"DNN_forward_MC.py\", line 221, in \nobiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\nFile \"DNN_forward_MC.py\", line 90, in init\nsv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 300, in init\nself._init_saver(saver=saver)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 448, in _init_saver\nsaver = saver_mod.Saver()\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1139, in init\nself.build()\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\nrestore_sequentially=self._restore_sequentially)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 691, in build\nrestore_sequentially, reshape)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\ntensors = self.restore_op(filename_tensor, saveable, preferred_shard)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n[spec.tensor.dtype])[0])\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\ndtypes=dtypes, name=name)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\nop_def=op_def)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in init\nself._traceback = _extract_stack()\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.\n[[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]\n`\nI have mostly seen the \"cannot assign a device for operation ...\" error in the case of people using GPU without specifying allow_soft_placement=True but in my case I am using just CPUs.\nI have seen another instance of this error when users were not specifying server.target as the device of a session,thereby creating a local session but this is not the case in my code.\nAny help would be much appreciated.", "body": "I am trying to use a distributed version of Self-Normalizing Networks on MNIST \r\n\r\nThe full code is provided here:\r\n`\r\nclass DNN_forward_MC(object):\r\n\r\n        def __init__(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch,X_train,Y_train,X_test,Y_test):\r\n\r\n\r\n                # cluster specification\r\n                parameter_servers = [\"localhost:2222\"]\r\n                workers = [     \"localhost:2223\", \r\n                        \"localhost:2224\",\r\n                        \"localhost:2225\"]\r\n\r\n                cluster = tf.train.ClusterSpec({\"ps\":parameter_servers, \"worker\":workers})\r\n\r\n                # input flags\r\n                tf.app.flags.DEFINE_string(\"job_name\", \"\", \"Either 'ps' or 'worker'\")\r\n                tf.app.flags.DEFINE_integer(\"task_index\", 0, \"Index of task within the job\")\r\n                FLAGS = tf.app.flags.FLAGS\r\n\r\n                # start a server for a specific task\r\n                server = tf.train.Server(\r\n                                        cluster,\r\n                                        job_name=FLAGS.job_name,\r\n                                        task_index=FLAGS.task_index)\r\n\r\n\r\n                is_chief=(FLAGS.task_index == 0)\r\n                if FLAGS.job_name == \"ps\":\r\n                        server.join()\r\n\r\n                elif FLAGS.job_name == \"worker\":\r\n                        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:worker/task:%d/cpu:0\" % FLAGS.task_index,\r\n                cluster=cluster,ps_device=\"/job:ps/cpu:0\")):\r\n\r\n                                global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n                                self.initialize_model(num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch)\r\n\r\n                                predictions = self.predict_training(tip='redus')\r\n                                predictions_training_full = self.predict_training(tip='full')\r\n\r\n                                predictions_testing_full = self.predict_testing()\r\n\r\n                                cost =  tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=self.Y_train, logits=predictions))\r\n                                opt = tf.train.GradientDescentOptimizer(1e-3)\r\n                                opt = tf.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=3,total_num_replicas=3)\r\n                                train_op = opt.minimize(cost,global_step=global_step)\r\n                                init_op = tf.global_variables_initializer()\r\n\r\n                                #grads_and_vars = optimizer.compute_gradients(cost, tf.trainable_variables())\r\n\r\n                                local_init_op = opt.local_step_init_op\r\n                                if is_chief:\r\n                                        local_init_op = opt.chief_init_op\r\n\r\n                                ready_for_local_init_op = opt.ready_for_local_init_op\r\n\r\n                                # Initial token and chief queue runners required by the sync_replicas mode\r\n                                chief_queue_runner = opt.get_chief_queue_runner()\r\n                                sync_init_op = opt.get_init_tokens_op()\r\n\r\n                        train_dir = tempfile.mkdtemp()\r\n                        sv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)\r\n\r\n                        if is_chief:\r\n                                print(\"Worker %d: Initializing session...\" % FLAGS.task_index)\r\n                        else:\r\n                                print(\"Worker %d: Waiting for session to be initialized...\" % FLAGS.task_index)\r\n\r\n\r\n                        sess_config = tf.ConfigProto(allow_soft_placement=True,log_device_placement=False,device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\r\n\r\n                        with sv.prepare_or_wait_for_session(server.target,config=sess_config) as sess:\r\n                        #with tf.train.MonitoredTrainingSession(master=server.target,is_chief=is_chief,checkpoint_dir='./logs/',save_checkpoint_secs=60,save_summaries_steps=1) as sess:        \r\n                                while True:\r\n\r\n                                        lista = np.arange(self.num_data)\r\n                                        np.random.shuffle(lista)\r\n                                        current_index = lista[:self.num_minibatch]\r\n\r\n                                        likelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})\r\n                                        printare = 'nll for minibatch at iteration '+str(i)+' is '+str(likelihood_now)\r\n                                        print(printare)\r\n                                        predictions_now = self.sess.run(predictions_training_full,feed_dict={self.X_train_full:X_train})\r\n                                        print 'accuracy is : '+str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_train,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))\r\n                                        if step_now>10:\r\n                                                break\r\n\r\n                                print '*******training last iteration**********'\r\n                                \r\n                                if is_chief:\r\n\r\n                                        predictions_now = self.sess.run(predictions_testing_full,feed_dict={self.X_test:X_test})\r\n                                        print '***********testing*************'\r\n\r\n                                        print 'accuracy at testing time is :' +str(self.sess.run(tf.reduce_mean(tf.cast(tf.equal(tf.argmax(Y_test,1),tf.argmax(tf.nn.softmax(predictions_now),1)),tf.float32))))\r\n\r\n                        sv.stop()\r\n        def initialize_model(self,num_data,num_test,dim_input,dim_output,num_hidden_layers,num_hidden_dims,keep_prob,num_minibatch):\r\n\r\n                ### model using just Hinton's Dropout\r\n                self.sess = tf.Session()\r\n                self.num_minibatch = num_minibatch\r\n                self.num_data = num_data\r\n                self.num_test = num_test\r\n                self.dim_input = dim_input\r\n                self.dim_output = dim_output\r\n                self.num_hidden_layers = num_hidden_layers\r\n                self.num_hidden_dims = num_hidden_dims\r\n\r\n                self.keep_prob = keep_prob\r\n                self.alpha_p = -1.7580993408473766\r\n                q = 1.0 - keep_prob\r\n                prod  = q + np.power(self.alpha_p,2)*q*(1-q)\r\n                self.a_affine = np.power(prod,-0.5)\r\n                self.b_affine = -self.a_affine * (self.alpha_p*(1-q))\r\n\r\n                self.X_train_full = tf.placeholder(shape=(self.num_data,dim_input),dtype=tf.float32)\r\n        \r\n                self.X_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_input))\r\n                self.Y_train = tf.placeholder(tf.float32,shape=(num_minibatch,dim_output))\r\n                self.X_test = tf.placeholder(tf.float32,shape=(num_test,dim_input))\r\n                self.Y_test = tf.placeholder(tf.float32,shape=(num_test,dim_output))\r\n\r\n                self.weights = []\r\n                self.biases = []\r\n\r\n                ### create parameters for Global DNN\r\n                for j in range(self.num_hidden_layers+1):                                                                                                              \r\n                        self.weights.append(tf.Variable(tf.random_normal(shape=(num_hidden_dims[j-1],self.num_hidden_dims[j]),stddev=1.0/num_hidden_dims[j-1]),dtype=tf.float32,name='weights_'+str(j)))\r\n                        self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[j],)),dtype=tf.float32,name='biases'+str(j)))          \r\n                \r\n                self.weights.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers],self.num_hidden_dims[self.num_hidden_layers+1])\r\n                ,stddev=1.0/self.num_hidden_dims[self.num_hidden_layers]),dtype=tf.float32,name='weights_'+str(self.num_hidden_layers+1)))\r\n                self.biases.append(tf.Variable(tf.random_normal(shape=(self.num_hidden_dims[self.num_hidden_layers+1],)),dtype=tf.float32,name='biases_'+str(self.num_hidden_layers+1)))\r\n\r\n               \r\n        def selu(self,x):\r\n\r\n                alpha= 1.6732632423543772848170429916717\r\n                lamb = 1.0507009873554804934193349852946\r\n                return lamb * tf.where(x>0.0,x,alpha * tf.nn.elu(x))\r\n\r\n        def alpha_dropout(self,x):\r\n\r\n                mask = tf.cast(tf.random_uniform(shape=x.get_shape()) < ( 1.0 - self.keep_prob ),dtype=tf.float32)\r\n                x_masked = tf.multiply(x,mask)\r\n                out = x_masked * self.a_affine + self.b_affine\r\n                return out      \r\n\r\n        def model(self,input,apply_dropout=True):\r\n\r\n                for j in range(1,self.num_hidden_layers +1):\r\n\r\n                        input = self.selu(tf.add(tf.matmul(input,self.weights[j]),self.biases[j]))\r\n                        if apply_dropout:\r\n\r\n                                input = self.alpha_dropout(input)\r\n\r\n                final_output = tf.add(tf.matmul(input,self.weights[self.num_hidden_layers+1]),self.biases[self.num_hidden_layers+1])\r\n\r\n                return final_output\r\n\r\n\r\n        def predict_training(self,tip):\r\n\r\n                if tip == 'full':\r\n                        prediction_DNN_global = self.model(input=self.X_train_full,apply_dropout=False)\r\n                else:\r\n                        prediction_DNN_global = self.model(input = self.X_train,apply_dropout=True)\r\n\r\n                return prediction_DNN_global\r\n\r\n                \r\n        def predict_testing(self):\r\n\r\n                prediction_DNN_global = self.model(input = self.X_test,apply_dropout=False)\r\n\r\n                return prediction_DNN_global\r\n\r\nif __name__ == '__main__':\r\n\r\n        training_data = np.genfromtxt('/home/spopescu/mnist_train.csv',dtype=np.float64,delimiter=',')\r\n        testing_data = np.genfromtxt('/home/spopescu/mnist_test.csv',dtype=np.float64,delimiter=',')\r\n\r\n        X_training = training_data[:,1:]\r\n        X_testing = testing_data[:,1:]\r\n        Y_training = one_hot_encoder(training_data[:,0].reshape(X_training.shape[0],1))\r\n        Y_testing = one_hot_encoder(testing_data[:,0].reshape(X_testing.shape[0],1))\r\n\r\n        obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\r\n\r\n`\r\n\r\nI am using the following Slurm bash script to send an array job:\r\n`\r\n#!/bin/sh\r\n#SBATCH --mem=20G\r\n#SBATCH -J SNN\r\n#SBATCH --array=0-3\r\n#SBATCH -o slurm-%A_%a.out\r\n#SBATCH -e slurm-%A_%a.err\r\n#SBATCH --nodelist=compute-2-3\r\n\r\ntask_type=(\"ps\" \"worker\" \"worker\" \"worker\")\r\ntask_index_array=(0 0 1 2)\r\nsrun --exclusive -N1 -n1 /home/spopescu/anaconda/bin/python2.7 DNN_forward_MC.py --job_name=${task_type[$SLURM_ARRAY_TASK_ID]} --task_index=${task_index_array[$SLURM_ARRAY_TASK_ID]}\r\n`\r\n\r\nI am using the latest version of Tensorflow as of today.\r\n\r\nI am getting the following error on my chief worker:\r\n\r\n`\r\n2017-07-19 14:49:14.087646: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> localhost:2222}\r\n2017-07-19 14:49:14.087702: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> localhost:2223, 1 -> localhost:2224, 2 -> localhost:2225}\r\n2017-07-19 14:49:14.091606: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:316] Started server with target: grpc://localhost:2223\r\n2017-07-19 14:49:15.137109: I tensorflow/core/distributed_runtime/master_session.cc:999] Start master session 4b5afb77d3aa2b77 with config: \r\ndevice_filters: \"/job:ps\"\r\ndevice_filters: \"/job:worker/task:0\"\r\nallow_soft_placement: true\r\n\r\nTraceback (most recent call last):\r\n  File \"DNN_forward_MC.py\", line 221, in <module>\r\n    obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\r\n  File \"DNN_forward_MC.py\", line 108, in __init__\r\n    likelihood_now,_,step_now = self.sess.run([cost,train_op,global_step],feed_dict={self.X_train:X_train[current_index],self.Y_train:Y_train[current_index]})\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 789, in run\r\n    run_metadata_ptr)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 997, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1132, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1152, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.\r\n         [[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]\r\n\r\nCaused by op u'save/RestoreV2_10', defined at:\r\n  File \"DNN_forward_MC.py\", line 221, in <module>\r\n    obiect = DNN_forward_MC(num_data=X_training.shape[0],num_test=X_testing.shape[0],dim_input=X_training.shape[1],dim_output=Y_training.shape[1],num_hidden_layers=3,num_hidden_dims=[X_training.shape[1],200,50,20,Y_training.shape[1]],keep_prob=0.80,num_minibatch=20,X_train=X_training,Y_train=Y_training,X_test=X_testing,Y_test=Y_testing)\r\n  File \"DNN_forward_MC.py\", line 90, in __init__\r\n    sv = tf.train.Supervisor(logdir=train_dir,is_chief=(FLAGS.task_index == 0),init_op=init_op,local_init_op=local_init_op,recovery_wait_secs=1,ready_for_local_init_op=ready_for_local_init_op)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 300, in __init__\r\n    self._init_saver(saver=saver)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/supervisor.py\", line 448, in _init_saver\r\n    saver = saver_mod.Saver()\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1139, in __init__\r\n    self.build()\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1170, in build\r\n    restore_sequentially=self._restore_sequentially)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 691, in build\r\n    restore_sequentially, reshape)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\r\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\r\n    [spec.tensor.dtype])[0])\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 640, in restore_v2\r\n    dtypes=dtypes, name=name)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 767, in apply_op\r\n    op_def=op_def)\r\n\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2506, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/spopescu/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1269, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Cannot assign a device for operation 'save/RestoreV2_10': Operation was explicitly assigned to /job:ps/task:0/device:CPU:0 but available devices are [ /job:localhost/replica:0/task:0/cpu:0 ]. Make sure the device specification refers to a valid device.\r\n         [[Node: save/RestoreV2_10 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:ps/task:0/device:CPU:0\"](save/Const, save/RestoreV2_10/tensor_names, save/RestoreV2_10/shape_and_slices)]]\r\n`\r\n\r\n\r\n\r\nI have mostly seen the \"cannot assign a device for operation ...\" error in the case of people using GPU without specifying allow_soft_placement=True but in my case I am using just CPUs.\r\n\r\nI have seen another instance of this error when users were not specifying server.target as the device of a session,thereby creating a local session but this is not the case in my code.\r\n\r\nAny help would be much appreciated.\r\n\r\n\r\n"}