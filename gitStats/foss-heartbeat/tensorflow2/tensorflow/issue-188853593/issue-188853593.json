{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5553", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5553/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5553/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5553/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5553", "id": 188853593, "node_id": "MDU6SXNzdWUxODg4NTM1OTM=", "number": 5553, "title": "Running own TensorFlow model on Android gives native inference error: \u201cSession was not created with a graph before Run()!\u201d", "user": {"login": "abdoelali", "id": 15015817, "node_id": "MDQ6VXNlcjE1MDE1ODE3", "avatar_url": "https://avatars0.githubusercontent.com/u/15015817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/abdoelali", "html_url": "https://github.com/abdoelali", "followers_url": "https://api.github.com/users/abdoelali/followers", "following_url": "https://api.github.com/users/abdoelali/following{/other_user}", "gists_url": "https://api.github.com/users/abdoelali/gists{/gist_id}", "starred_url": "https://api.github.com/users/abdoelali/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/abdoelali/subscriptions", "organizations_url": "https://api.github.com/users/abdoelali/orgs", "repos_url": "https://api.github.com/users/abdoelali/repos", "events_url": "https://api.github.com/users/abdoelali/events{/privacy}", "received_events_url": "https://api.github.com/users/abdoelali/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 25, "created_at": "2016-11-11T21:20:54Z", "updated_at": "2017-01-26T10:33:01Z", "closed_at": "2017-01-06T14:50:31Z", "author_association": "NONE", "body_html": "<p>I was able to run the Inception-v3 model on Android just fine, and I now want to run my own trained TensorFlow model on Android. I'm following the approach from <a href=\"https://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html\" rel=\"nofollow\">TensorFlow's image recognition tutorial</a> and the Android TensorFlow demo, and adapting as necessary. My changes include: (a) integrating Android OpenCV as part of the bazel build (b) using own model and label file and (c) adjusting parameters (img_size, input_mean, input_std, etc.) accordingly.</p>\n<p>From Android logcat, running my model with the tensorflow android demo app gives:</p>\n<pre><code>E/native: tensorflow_inference_jni.cc:202 Error during inference: Invalid argument: Session was not created with a graph before Run()!\n...\nE/native: tensorflow_inference_jni.cc:159 Output [output/Softmax:0] not found, aborting!\n\n</code></pre>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>Own (duplicate) SO thread: <a href=\"http://stackoverflow.com/questions/40555749/running-own-tensorflow-model-on-android-gives-native-inference-error-session-w\" rel=\"nofollow\">http://stackoverflow.com/questions/40555749/running-own-tensorflow-model-on-android-gives-native-inference-error-session-w</a></p>\n<h3>Environment info</h3>\n<p>OS X Yosemite (10.10.5), LGE Nexus 5 (Android 6.0.1), Android SDK 23, Android OpenCV SDK 23, Bazel 0.4.0.</p>\n<h3>Steps taken</h3>\n<ol>\n<li>Saved own model's checkpoint (.ckpt) and graph definition (.pb) files separately using <code>tf.train.Saver()</code> then <code>tf.train.write_graph()</code></li>\n<li>Froze graph using freeze_graph.py (using bazel), gives 227.5 MB file</li>\n<li>Optimized the graph using optimize_for_inference.py (additionally tried strip_unused.py)</li>\n<li>Copied frozen, optimized, or stripped graph to android/assets</li>\n<li>Doubled the total byte limit using <code>coded_stream.SetTotalBytesLimit()</code> in jni_utils.cc to handle my large model size</li>\n<li>Built the tensorflow android app using bazel</li>\n<li>Installed on android device using adb and bazel</li>\n</ol>\n<p>As a sanity check, I have tested my model in C++ built with bazel following the tutorial here <a href=\"https://www.tensorflow.org/versions/r0.8/how_tos/image_retraining/index.html\" rel=\"nofollow\">label_image</a>, and my model correctly outputs a prediction. I have also tried playing with the order by which I save my graph def and checkpoint files before freezing, but no change.</p>\n<p>Any help would be great.<br>\ncc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a></p>", "body_text": "I was able to run the Inception-v3 model on Android just fine, and I now want to run my own trained TensorFlow model on Android. I'm following the approach from TensorFlow's image recognition tutorial and the Android TensorFlow demo, and adapting as necessary. My changes include: (a) integrating Android OpenCV as part of the bazel build (b) using own model and label file and (c) adjusting parameters (img_size, input_mean, input_std, etc.) accordingly.\nFrom Android logcat, running my model with the tensorflow android demo app gives:\nE/native: tensorflow_inference_jni.cc:202 Error during inference: Invalid argument: Session was not created with a graph before Run()!\n...\nE/native: tensorflow_inference_jni.cc:159 Output [output/Softmax:0] not found, aborting!\n\n\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nOwn (duplicate) SO thread: http://stackoverflow.com/questions/40555749/running-own-tensorflow-model-on-android-gives-native-inference-error-session-w\nEnvironment info\nOS X Yosemite (10.10.5), LGE Nexus 5 (Android 6.0.1), Android SDK 23, Android OpenCV SDK 23, Bazel 0.4.0.\nSteps taken\n\nSaved own model's checkpoint (.ckpt) and graph definition (.pb) files separately using tf.train.Saver() then tf.train.write_graph()\nFroze graph using freeze_graph.py (using bazel), gives 227.5 MB file\nOptimized the graph using optimize_for_inference.py (additionally tried strip_unused.py)\nCopied frozen, optimized, or stripped graph to android/assets\nDoubled the total byte limit using coded_stream.SetTotalBytesLimit() in jni_utils.cc to handle my large model size\nBuilt the tensorflow android app using bazel\nInstalled on android device using adb and bazel\n\nAs a sanity check, I have tested my model in C++ built with bazel following the tutorial here label_image, and my model correctly outputs a prediction. I have also tried playing with the order by which I save my graph def and checkpoint files before freezing, but no change.\nAny help would be great.\ncc @drpngx @andrewharp", "body": "I was able to run the Inception-v3 model on Android just fine, and I now want to run my own trained TensorFlow model on Android. I'm following the approach from [TensorFlow's image recognition tutorial](https://www.tensorflow.org/versions/r0.11/tutorials/image_recognition/index.html) and the Android TensorFlow demo, and adapting as necessary. My changes include: (a) integrating Android OpenCV as part of the bazel build (b) using own model and label file and (c) adjusting parameters (img_size, input_mean, input_std, etc.) accordingly.\r\n\r\nFrom Android logcat, running my model with the tensorflow android demo app gives:\r\n\r\n```\r\nE/native: tensorflow_inference_jni.cc:202 Error during inference: Invalid argument: Session was not created with a graph before Run()!\r\n...\r\nE/native: tensorflow_inference_jni.cc:159 Output [output/Softmax:0] not found, aborting!\r\n\r\n```\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nOwn (duplicate) SO thread: http://stackoverflow.com/questions/40555749/running-own-tensorflow-model-on-android-gives-native-inference-error-session-w\r\n\r\n### Environment info\r\nOS X Yosemite (10.10.5), LGE Nexus 5 (Android 6.0.1), Android SDK 23, Android OpenCV SDK 23, Bazel 0.4.0.\r\n\r\n### Steps taken\r\n\r\n1. Saved own model's checkpoint (.ckpt) and graph definition (.pb) files separately using `tf.train.Saver()` then `tf.train.write_graph()`\r\n2. Froze graph using freeze_graph.py (using bazel), gives 227.5 MB file\r\n3. Optimized the graph using optimize_for_inference.py (additionally tried strip_unused.py)\r\n4. Copied frozen, optimized, or stripped graph to android/assets\r\n5. Doubled the total byte limit using `coded_stream.SetTotalBytesLimit()` in jni_utils.cc to handle my large model size\r\n6. Built the tensorflow android app using bazel\r\n7. Installed on android device using adb and bazel\r\n\r\nAs a sanity check, I have tested my model in C++ built with bazel following the tutorial here [label_image](https://www.tensorflow.org/versions/r0.8/how_tos/image_retraining/index.html), and my model correctly outputs a prediction. I have also tried playing with the order by which I save my graph def and checkpoint files before freezing, but no change.\r\n\r\nAny help would be great. \r\ncc @drpngx @andrewharp \r\n"}