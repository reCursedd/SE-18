{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3440", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3440/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3440/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3440/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3440", "id": 166792659, "node_id": "MDU6SXNzdWUxNjY3OTI2NTk=", "number": 3440, "title": "How to modify the seq2seq cost function for padded vectors?", "user": {"login": "krayush07", "id": 9397134, "node_id": "MDQ6VXNlcjkzOTcxMzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/9397134?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krayush07", "html_url": "https://github.com/krayush07", "followers_url": "https://api.github.com/users/krayush07/followers", "following_url": "https://api.github.com/users/krayush07/following{/other_user}", "gists_url": "https://api.github.com/users/krayush07/gists{/gist_id}", "starred_url": "https://api.github.com/users/krayush07/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krayush07/subscriptions", "organizations_url": "https://api.github.com/users/krayush07/orgs", "repos_url": "https://api.github.com/users/krayush07/repos", "events_url": "https://api.github.com/users/krayush07/events{/privacy}", "received_events_url": "https://api.github.com/users/krayush07/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-07-21T11:01:10Z", "updated_at": "2016-07-22T05:24:43Z", "closed_at": "2016-07-22T05:24:43Z", "author_association": "NONE", "body_html": "<p>Tensorflow supports dynamic length sequence by use of the parameter: <code>sequence_length</code> while constructing the RNN layer, wherein the model does not learn the sequence after the sequence size = 'sequence_length' i.e, returns zero vector.</p>\n<p>However, how can the cost function at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L890\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L890</a> be modified to encounter the masked sequences, so that cost and perplexity are calculated only on the actual sequences rather than whole padded sequence?</p>\n<pre><code>def sequence_loss_by_example(logits, targets, weights, average_across_timesteps=True,  softmax_loss_function=None, name=None):\n\n    if len(targets) != len(logits) or len(weights) != len(logits):\n        raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n                         \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n      with ops.op_scope(logits + targets + weights, name,\n                        \"sequence_loss_by_example\"):\n        log_perp_list = []\n        for logit, target, weight in zip(logits, targets, weights):\n          if softmax_loss_function is None:\n            # TODO(irving,ebrevdo): This reshape is needed because\n            # sequence_loss_by_example is called with scalars sometimes, which\n            # violates our general scalar strictness policy.\n            target = array_ops.reshape(target, [-1])\n            crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n                logit, target)\n          else:\n            crossent = softmax_loss_function(logit, target)\n          log_perp_list.append(crossent * weight)\n        log_perps = math_ops.add_n(log_perp_list)\n        if average_across_timesteps:\n          total_size = math_ops.add_n(weights)\n          total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n          log_perps /= total_size\n    return log_perps\n</code></pre>", "body_text": "Tensorflow supports dynamic length sequence by use of the parameter: sequence_length while constructing the RNN layer, wherein the model does not learn the sequence after the sequence size = 'sequence_length' i.e, returns zero vector.\nHowever, how can the cost function at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L890 be modified to encounter the masked sequences, so that cost and perplexity are calculated only on the actual sequences rather than whole padded sequence?\ndef sequence_loss_by_example(logits, targets, weights, average_across_timesteps=True,  softmax_loss_function=None, name=None):\n\n    if len(targets) != len(logits) or len(weights) != len(logits):\n        raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n                         \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n      with ops.op_scope(logits + targets + weights, name,\n                        \"sequence_loss_by_example\"):\n        log_perp_list = []\n        for logit, target, weight in zip(logits, targets, weights):\n          if softmax_loss_function is None:\n            # TODO(irving,ebrevdo): This reshape is needed because\n            # sequence_loss_by_example is called with scalars sometimes, which\n            # violates our general scalar strictness policy.\n            target = array_ops.reshape(target, [-1])\n            crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n                logit, target)\n          else:\n            crossent = softmax_loss_function(logit, target)\n          log_perp_list.append(crossent * weight)\n        log_perps = math_ops.add_n(log_perp_list)\n        if average_across_timesteps:\n          total_size = math_ops.add_n(weights)\n          total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n          log_perps /= total_size\n    return log_perps", "body": "Tensorflow supports dynamic length sequence by use of the parameter: `sequence_length` while constructing the RNN layer, wherein the model does not learn the sequence after the sequence size = 'sequence_length' i.e, returns zero vector.\n\nHowever, how can the cost function at https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/seq2seq.py#L890 be modified to encounter the masked sequences, so that cost and perplexity are calculated only on the actual sequences rather than whole padded sequence? \n\n```\ndef sequence_loss_by_example(logits, targets, weights, average_across_timesteps=True,  softmax_loss_function=None, name=None):\n\n    if len(targets) != len(logits) or len(weights) != len(logits):\n        raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n                         \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n      with ops.op_scope(logits + targets + weights, name,\n                        \"sequence_loss_by_example\"):\n        log_perp_list = []\n        for logit, target, weight in zip(logits, targets, weights):\n          if softmax_loss_function is None:\n            # TODO(irving,ebrevdo): This reshape is needed because\n            # sequence_loss_by_example is called with scalars sometimes, which\n            # violates our general scalar strictness policy.\n            target = array_ops.reshape(target, [-1])\n            crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(\n                logit, target)\n          else:\n            crossent = softmax_loss_function(logit, target)\n          log_perp_list.append(crossent * weight)\n        log_perps = math_ops.add_n(log_perp_list)\n        if average_across_timesteps:\n          total_size = math_ops.add_n(weights)\n          total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n          log_perps /= total_size\n    return log_perps\n```\n"}