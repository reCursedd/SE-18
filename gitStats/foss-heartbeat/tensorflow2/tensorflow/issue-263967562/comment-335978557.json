{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335978557", "html_url": "https://github.com/tensorflow/tensorflow/issues/13591#issuecomment-335978557", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591", "id": 335978557, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTk3ODU1Nw==", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T23:38:09Z", "updated_at": "2017-10-11T23:51:52Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a> I made another test as below, where I removed <code>tf.reduce_sum(t)</code>. And my test verifies your conclusion. When no  <code>tf.reduce_sum(t)</code> is required, the <code> NUM_ELEMS = int((3.5 * GPU_MEMORY_BYTES / 8) / 4 )</code> will run without any problem for single GPU, and the <code>NUM_ELEMS = int((6 * GPU_MEMORY_BYTES / 8) / 4 )</code> will run successfully for two GPUs.</p>\n<p>I do think this is a serious problem. When I am training my data, I can only use batch size of 1/4 or even 1/8 compared with people using other platforms. Batch size is so important for performance. If you could fix this memory allocation issue asap, it would be really great, especially for regular users, like me, who do not have so big GPUs as Google has. Thank you.</p>\n<pre><code>def Test7():\n  with tf.device(\"/gpu:0\"):\n\tt = tf.ones([2, NUM_ELEMS], tf.float32)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\n  with tf.Session(config=config) as sess:\n\tsess.run(t)\n\ndef Test8():\n  with tf.device(\"/gpu:0\"):\n\tt = tf.ones([1, NUM_ELEMS], tf.float32)\n  with tf.device(\"/gpu:1\"):\n\tt1 = tf.ones([1, NUM_ELEMS], tf.float32)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\n  with tf.Session(config=config) as sess:\n\tsess.run([t, t1])\n</code></pre>", "body_text": "@reedwm I made another test as below, where I removed tf.reduce_sum(t). And my test verifies your conclusion. When no  tf.reduce_sum(t) is required, the  NUM_ELEMS = int((3.5 * GPU_MEMORY_BYTES / 8) / 4 ) will run without any problem for single GPU, and the NUM_ELEMS = int((6 * GPU_MEMORY_BYTES / 8) / 4 ) will run successfully for two GPUs.\nI do think this is a serious problem. When I am training my data, I can only use batch size of 1/4 or even 1/8 compared with people using other platforms. Batch size is so important for performance. If you could fix this memory allocation issue asap, it would be really great, especially for regular users, like me, who do not have so big GPUs as Google has. Thank you.\ndef Test7():\n  with tf.device(\"/gpu:0\"):\n\tt = tf.ones([2, NUM_ELEMS], tf.float32)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\n  with tf.Session(config=config) as sess:\n\tsess.run(t)\n\ndef Test8():\n  with tf.device(\"/gpu:0\"):\n\tt = tf.ones([1, NUM_ELEMS], tf.float32)\n  with tf.device(\"/gpu:1\"):\n\tt1 = tf.ones([1, NUM_ELEMS], tf.float32)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\n  with tf.Session(config=config) as sess:\n\tsess.run([t, t1])", "body": "@reedwm I made another test as below, where I removed `tf.reduce_sum(t)`. And my test verifies your conclusion. When no  `tf.reduce_sum(t)` is required, the ` NUM_ELEMS = int((3.5 * GPU_MEMORY_BYTES / 8) / 4 )` will run without any problem for single GPU, and the `NUM_ELEMS = int((6 * GPU_MEMORY_BYTES / 8) / 4 )` will run successfully for two GPUs.\r\n\r\nI do think this is a serious problem. When I am training my data, I can only use batch size of 1/4 or even 1/8 compared with people using other platforms. Batch size is so important for performance. If you could fix this memory allocation issue asap, it would be really great, especially for regular users, like me, who do not have so big GPUs as Google has. Thank you.\r\n\r\n    def Test7():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([2, NUM_ELEMS], tf.float32)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tsess.run(t)\r\n\r\n\tdef Test8():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([1, NUM_ELEMS], tf.float32)\r\n\t  with tf.device(\"/gpu:1\"):\r\n\t\tt1 = tf.ones([1, NUM_ELEMS], tf.float32)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0, 1'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tsess.run([t, t1])"}