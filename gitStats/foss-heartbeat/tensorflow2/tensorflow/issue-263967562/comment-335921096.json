{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335921096", "html_url": "https://github.com/tensorflow/tensorflow/issues/13591#issuecomment-335921096", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591", "id": 335921096, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTkyMTA5Ng==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T19:27:24Z", "updated_at": "2017-10-11T19:27:24Z", "author_association": "MEMBER", "body_html": "<p>When allocating half the GPU memory, the BFC allocator will actually allocate all the memory. This is because of an internal design decision of the BFC allocator: When a chunk is found for an allocation, it will only be split if the allocation is less than half the chunk size. When you allocate 4 GB on an 8 GB GPU, you are allocating a bit more than half the free memory, so the chunk holding almost all the free memory will not be split.</p>\n<p>As a result, allocating more than half the free GPU memory will often end up allocating all the GPU memory, causing future allocations to fail until memory is freed. <code>tf.reduce_sum(t)</code> requires 4 bytes to be allocated for the output, which causes the OOM error. In practice, it is rare that a single allocation allocates more than half the GPU memory.</p>\n<p>See <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/bfc_allocator.cc#L304\">here</a> for the logic of when to split a chunk.</p>\n<p>I have been assigned an internal bug to fix this for a few months, but unfortunately fixing this is not a priority right now. Fixing it would require doing a large amount of benchmarking to ensure no performance regressions from the change.</p>", "body_text": "When allocating half the GPU memory, the BFC allocator will actually allocate all the memory. This is because of an internal design decision of the BFC allocator: When a chunk is found for an allocation, it will only be split if the allocation is less than half the chunk size. When you allocate 4 GB on an 8 GB GPU, you are allocating a bit more than half the free memory, so the chunk holding almost all the free memory will not be split.\nAs a result, allocating more than half the free GPU memory will often end up allocating all the GPU memory, causing future allocations to fail until memory is freed. tf.reduce_sum(t) requires 4 bytes to be allocated for the output, which causes the OOM error. In practice, it is rare that a single allocation allocates more than half the GPU memory.\nSee here for the logic of when to split a chunk.\nI have been assigned an internal bug to fix this for a few months, but unfortunately fixing this is not a priority right now. Fixing it would require doing a large amount of benchmarking to ensure no performance regressions from the change.", "body": "When allocating half the GPU memory, the BFC allocator will actually allocate all the memory. This is because of an internal design decision of the BFC allocator: When a chunk is found for an allocation, it will only be split if the allocation is less than half the chunk size. When you allocate 4 GB on an 8 GB GPU, you are allocating a bit more than half the free memory, so the chunk holding almost all the free memory will not be split.\r\n\r\nAs a result, allocating more than half the free GPU memory will often end up allocating all the GPU memory, causing future allocations to fail until memory is freed. `tf.reduce_sum(t)` requires 4 bytes to be allocated for the output, which causes the OOM error. In practice, it is rare that a single allocation allocates more than half the GPU memory.\r\n\r\nSee [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/bfc_allocator.cc#L304) for the logic of when to split a chunk.\r\n\r\nI have been assigned an internal bug to fix this for a few months, but unfortunately fixing this is not a priority right now. Fixing it would require doing a large amount of benchmarking to ensure no performance regressions from the change.\r\n\r\n"}