{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335889082", "html_url": "https://github.com/tensorflow/tensorflow/issues/13591#issuecomment-335889082", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591", "id": 335889082, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTg4OTA4Mg==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-11T17:36:44Z", "updated_at": "2017-10-11T17:36:44Z", "author_association": "MEMBER", "body_html": "<p>I cannot reproduce. For me, changing the type to <code>int32</code> does not cause out of memory errors, and outputs the result. I am running the code</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.core.protobuf <span class=\"pl-k\">import</span> rewriter_config_pb2\n\n<span class=\"pl-c1\">GPU_MEMORY_BYTES</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">8</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">30</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> Assuming your GPU has 8GB of memory, adjust accordingly</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Number of float32 elements (4 bytes) that consume 7/8 of GPU memory</span>\n<span class=\"pl-c1\">NUM_ELEMS</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>((<span class=\"pl-c1\">7</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">GPU_MEMORY_BYTES</span> <span class=\"pl-k\">/</span> <span class=\"pl-c1\">8</span>) <span class=\"pl-k\">/</span> <span class=\"pl-c1\">4</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">Test1</span>():\n  <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>):\n    t <span class=\"pl-k\">=</span> tf.ones([<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">NUM_ELEMS</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n  s <span class=\"pl-k\">=</span> tf.reduce_sum(t)\n  config <span class=\"pl-k\">=</span> tf.ConfigProto(<span class=\"pl-v\">gpu_options</span><span class=\"pl-k\">=</span>tf.GPUOptions(<span class=\"pl-v\">visible_device_list</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>0<span class=\"pl-pds\">'</span></span>),\n                          <span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n  config.graph_options.optimizer_options.opt_level <span class=\"pl-k\">=</span> tf.OptimizerOptions.L0\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF</span>\n  <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-c1\">print</span>(sess.run(s)) <span class=\"pl-c\"><span class=\"pl-c\">#</span> This should fail since it consumes more memory than exists in the GPU</span>\n\nTest1()</pre></div>\n<p>The reason there is no OOM error is that for <code>int32</code> types, many ops are done on the CPU with CPU memory, not GPU memory. For example, see the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/constant_op.cc#L133\"><code>Const</code> op kernel</a> (used by <code>tf.ones</code>) and the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_sum.cc#L48\"><code>Sum</code> op kernel</a> (used by <code>tf.reduce_sum</code>). See <a href=\"https://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op\" rel=\"nofollow\">this StackOverflow answer</a> for more information.</p>\n<p>Can you post the code that causes the error message to show but still outputs a value? Also can you post the output when you run that code?</p>", "body_text": "I cannot reproduce. For me, changing the type to int32 does not cause out of memory errors, and outputs the result. I am running the code\nimport tensorflow as tf\nfrom tensorflow.core.protobuf import rewriter_config_pb2\n\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\n\n# Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\nNUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\n\ndef Test1():\n  with tf.device(\"/gpu:0\"):\n    t = tf.ones([2, NUM_ELEMS], dtype=tf.int32)\n  s = tf.reduce_sum(t)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\n                          allow_soft_placement=False)\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\n  # config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\n  with tf.Session(config=config) as sess:\n    print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\n\nTest1()\nThe reason there is no OOM error is that for int32 types, many ops are done on the CPU with CPU memory, not GPU memory. For example, see the Const op kernel (used by tf.ones) and the Sum op kernel (used by tf.reduce_sum). See this StackOverflow answer for more information.\nCan you post the code that causes the error message to show but still outputs a value? Also can you post the output when you run that code?", "body": "I cannot reproduce. For me, changing the type to `int32` does not cause out of memory errors, and outputs the result. I am running the code \r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.core.protobuf import rewriter_config_pb2\r\n\r\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n# Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\r\nNUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\r\n\r\ndef Test1():\r\n  with tf.device(\"/gpu:0\"):\r\n    t = tf.ones([2, NUM_ELEMS], dtype=tf.int32)\r\n  s = tf.reduce_sum(t)\r\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n                          allow_soft_placement=False)\r\n  config.graph_options.optimizer_options.opt_level = tf.OptimizerOptions.L0\r\n  # config.graph_options.rewrite_options.constant_folding = rewriter_config_pb2.RewriterConfig.OFF\r\n  with tf.Session(config=config) as sess:\r\n    print(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n\r\nTest1()\r\n```\r\n\r\nThe reason there is no OOM error is that for `int32` types, many ops are done on the CPU with CPU memory, not GPU memory. For example, see the [`Const` op kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/constant_op.cc#L133) (used by `tf.ones`) and the [`Sum` op kernel](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/reduction_ops_sum.cc#L48) (used by `tf.reduce_sum`). See [this StackOverflow answer](https://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op) for more information.\r\n\r\nCan you post the code that causes the error message to show but still outputs a value? Also can you post the output when you run that code?"}