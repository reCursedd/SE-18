{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13591/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13591", "id": 263967562, "node_id": "MDU6SXNzdWUyNjM5Njc1NjI=", "number": 13591, "title": "GPU Allocation and Results Unexpected for Simple Test codes", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2017-10-09T17:17:45Z", "updated_at": "2018-04-10T18:28:43Z", "closed_at": "2017-12-20T01:41:21Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/ 6.0</li>\n<li><strong>GPU model and memory</strong>:two of Nvidia Quadro M4000</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have two Nvidia Quadro M4000 GPUs; each has a 8G Memory. My regular memory has 64G space.</p>\n<p>I tested a simple GPU memory allocation for tensorflow and found that the allocation seems larger than the GPU memory. My test codes are as below:</p>\n<pre><code>GPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\n\n# Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\nNUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\n\ndef Test1():\n  with tf.device(\"/gpu:0\"):\n\tt = tf.ones([2, NUM_ELEMS])\n  s = tf.reduce_sum(t)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  with tf.Session(config=config) as sess:\n\tprint(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\n\ndef Test2():\n  with tf.device(\"/gpu:0\"):\n\tt0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\n\ts0 = tf.reduce_sum(t0)\n  with tf.device(\"/gpu:1\"):\n\tt1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\n\ts1 = tf.reduce_sum(t1)\n  s = tf.add(s0, s1)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0,1'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  with tf.Session(config=config) as sess:\n\tprint(sess.run(s))\n</code></pre>\n<p>I expect the Test2() should work well but Test1() should fail as one GPU only has 8G memory. However, both test functions succeeded but the results are incorrect!</p>\n<p>NUM_ELEMS is about 1.879*e+9, and I expected that it should be half of the output of Test2() (Test1() should fail).</p>\n<p>However, I got Test1's output as 1.07374e+09, and Test2's output as 2.14748e+09.</p>\n<p>It seems that Both Test1 and Test2 clip the number of elements to 2^30! As NUM_ELEMS is int32, even clipping should be clipped to 2^31 but not 2^30.</p>\n<p>In Test1(), if I change the first dimension in (tf.ones([2, NUM_ELEMS])) from 2 to 15, the outputs are always 1.07374e+09; if the first dimension is no less than 16, it starts to crash showing out of GPU memory.</p>\n<p>My questions are:</p>\n<ol>\n<li>\n<p>Why the GPU allocation did not crash for larger than 8G for single GPU?</p>\n</li>\n<li>\n<p>Why the output results are clipped to 2^30 for single GPU and 2^31 for two GPUs</p>\n</li>\n<li>\n<p>How could I get the correct outputs?</p>\n</li>\n</ol>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below): 1.3.0\nPython version: 3.6\nBazel version (if compiling from source):\nCUDA/cuDNN version: 8.0/ 6.0\nGPU model and memory:two of Nvidia Quadro M4000\nExact command to reproduce:\n\nDescribe the problem\nI have two Nvidia Quadro M4000 GPUs; each has a 8G Memory. My regular memory has 64G space.\nI tested a simple GPU memory allocation for tensorflow and found that the allocation seems larger than the GPU memory. My test codes are as below:\nGPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\n\n# Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\nNUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\n\ndef Test1():\n  with tf.device(\"/gpu:0\"):\n\tt = tf.ones([2, NUM_ELEMS])\n  s = tf.reduce_sum(t)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  with tf.Session(config=config) as sess:\n\tprint(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\n\ndef Test2():\n  with tf.device(\"/gpu:0\"):\n\tt0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\n\ts0 = tf.reduce_sum(t0)\n  with tf.device(\"/gpu:1\"):\n\tt1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\n\ts1 = tf.reduce_sum(t1)\n  s = tf.add(s0, s1)\n  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0,1'),\n\t\t\t\t\t\t  allow_soft_placement=False)\n  with tf.Session(config=config) as sess:\n\tprint(sess.run(s))\n\nI expect the Test2() should work well but Test1() should fail as one GPU only has 8G memory. However, both test functions succeeded but the results are incorrect!\nNUM_ELEMS is about 1.879*e+9, and I expected that it should be half of the output of Test2() (Test1() should fail).\nHowever, I got Test1's output as 1.07374e+09, and Test2's output as 2.14748e+09.\nIt seems that Both Test1 and Test2 clip the number of elements to 2^30! As NUM_ELEMS is int32, even clipping should be clipped to 2^31 but not 2^30.\nIn Test1(), if I change the first dimension in (tf.ones([2, NUM_ELEMS])) from 2 to 15, the outputs are always 1.07374e+09; if the first dimension is no less than 16, it starts to crash showing out of GPU memory.\nMy questions are:\n\n\nWhy the GPU allocation did not crash for larger than 8G for single GPU?\n\n\nWhy the output results are clipped to 2^30 for single GPU and 2^31 for two GPUs\n\n\nHow could I get the correct outputs?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: \r\n- **TensorFlow version (use command below)**: 1.3.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0/ 6.0\r\n- **GPU model and memory**:two of Nvidia Quadro M4000\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI have two Nvidia Quadro M4000 GPUs; each has a 8G Memory. My regular memory has 64G space. \r\n\r\nI tested a simple GPU memory allocation for tensorflow and found that the allocation seems larger than the GPU memory. My test codes are as below:\r\n\r\n    GPU_MEMORY_BYTES = 8 * 2**30 # Assuming your GPU has 8GB of memory, adjust accordingly\r\n\r\n    # Number of float32 elements (4 bytes) that consume 7/8 of GPU memory\r\n    NUM_ELEMS = int((7 * GPU_MEMORY_BYTES / 8) / 4)\r\n\r\n\tdef Test1():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt = tf.ones([2, NUM_ELEMS])\r\n\t  s = tf.reduce_sum(t)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tprint(sess.run(s)) # This should fail since it consumes more memory than exists in the GPU\r\n\r\n\tdef Test2():\r\n\t  with tf.device(\"/gpu:0\"):\r\n\t\tt0 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU0's memory\r\n\t\ts0 = tf.reduce_sum(t0)\r\n\t  with tf.device(\"/gpu:1\"):\r\n\t\tt1 = tf.ones([NUM_ELEMS])  # Tensor that consumes 80% of GPU1's memory\r\n\t\ts1 = tf.reduce_sum(t1)\r\n\t  s = tf.add(s0, s1)\r\n\t  config = tf.ConfigProto(gpu_options=tf.GPUOptions(visible_device_list='0,1'),\r\n\t\t\t\t\t\t\t  allow_soft_placement=False)\r\n\t  with tf.Session(config=config) as sess:\r\n\t\tprint(sess.run(s))\r\n\r\nI expect the Test2() should work well but Test1() should fail as one GPU only has 8G memory. However, both test functions succeeded but the results are incorrect!\r\n\r\nNUM_ELEMS is about 1.879*e+9, and I expected that it should be half of the output of Test2() (Test1() should fail). \r\n\r\nHowever, I got Test1's output as 1.07374e+09, and Test2's output as 2.14748e+09.\r\n\r\nIt seems that Both Test1 and Test2 clip the number of elements to 2^30! As NUM_ELEMS is int32, even clipping should be clipped to 2^31 but not 2^30. \r\n\r\nIn Test1(), if I change the first dimension in (tf.ones([2, NUM_ELEMS])) from 2 to 15, the outputs are always 1.07374e+09; if the first dimension is no less than 16, it starts to crash showing out of GPU memory.\r\n\r\nMy questions are:\r\n\r\n 1. Why the GPU allocation did not crash for larger than 8G for single GPU?\r\n\r\n 2. Why the output results are clipped to 2^30 for single GPU and 2^31 for two GPUs\r\n\r\n 3. How could I get the correct outputs? "}