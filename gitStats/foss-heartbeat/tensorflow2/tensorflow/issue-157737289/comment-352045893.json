{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/352045893", "html_url": "https://github.com/tensorflow/tensorflow/issues/2594#issuecomment-352045893", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2594", "id": 352045893, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MjA0NTg5Mw==", "user": {"login": "xtknight", "id": 350525, "node_id": "MDQ6VXNlcjM1MDUyNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/350525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xtknight", "html_url": "https://github.com/xtknight", "followers_url": "https://api.github.com/users/xtknight/followers", "following_url": "https://api.github.com/users/xtknight/following{/other_user}", "gists_url": "https://api.github.com/users/xtknight/gists{/gist_id}", "starred_url": "https://api.github.com/users/xtknight/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xtknight/subscriptions", "organizations_url": "https://api.github.com/users/xtknight/orgs", "repos_url": "https://api.github.com/users/xtknight/repos", "events_url": "https://api.github.com/users/xtknight/events{/privacy}", "received_events_url": "https://api.github.com/users/xtknight/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-15T16:15:16Z", "updated_at": "2017-12-15T16:45:03Z", "author_association": "NONE", "body_html": "<p>I can't really quite understand the implications of this. Does this prevent LSTMs from being placed on the GPU?</p>\n<p>I'm trying to do multi-GPU which requires explicit assignment of GPU devices, and suddenly when I start explicitly designing devices I start hitting this assert op that's apparently not supported on GPU. But I don't understand how it was running fine on a single GPU in the first place? Or does it mean that the RNN code was running on the CPU? I can't believe that would be the case. But if this were to be the case, wouldn't it kind of be...a big deal and a big enough performance hit to be worth looking at? I haven't really figured out what elements of the LSTM I need to be \"placing\" on the GPU for best performance. My LSTMCells are on the GPU but because this function always gives errors I have to leave it on the CPU, which also limits me from placing functions depending on it on the GPU as well, eventually killing the performance. What I do know is that I commented out the assert, explicitly defined GPU devices and multi-GPU code is working very well now. But I am left with a lot of confusion as well with what's being placed where.</p>\n<p>I'm not sure what it takes to make an assert op on the GPU, but it would be very nice in the meantime if this were documented somehow or if an alternate dynamic_rnn function were provided without the assert so that we could reap performance benefits of GPU at our own risk until assert on GPU works properly. It does sound messy, but from the developer's point of view, networks using Bi-LSTM and CRF (crf code also hits this assert) are exactly the ones we might want to use multi-GPU with and there are some workloads that have a big need for multi-GPU. Again, I'm not sure if this has implications for single GPU also or just multi-GPU, but I'm just relaying my experiences.</p>\n<p>Update:   I realized there was an issue with my soft placement that caused the assert to still occur. Apparently I didn't put soft placement config every time I called tf.Session and even when I put it in the training part, the assert still occurred. odd.. anyways the performance seems to be about the same as with commenting out the assert, so I'm assuming the solution for this is to just use soft device placement and that there is no big issue with using RNN on GPU</p>", "body_text": "I can't really quite understand the implications of this. Does this prevent LSTMs from being placed on the GPU?\nI'm trying to do multi-GPU which requires explicit assignment of GPU devices, and suddenly when I start explicitly designing devices I start hitting this assert op that's apparently not supported on GPU. But I don't understand how it was running fine on a single GPU in the first place? Or does it mean that the RNN code was running on the CPU? I can't believe that would be the case. But if this were to be the case, wouldn't it kind of be...a big deal and a big enough performance hit to be worth looking at? I haven't really figured out what elements of the LSTM I need to be \"placing\" on the GPU for best performance. My LSTMCells are on the GPU but because this function always gives errors I have to leave it on the CPU, which also limits me from placing functions depending on it on the GPU as well, eventually killing the performance. What I do know is that I commented out the assert, explicitly defined GPU devices and multi-GPU code is working very well now. But I am left with a lot of confusion as well with what's being placed where.\nI'm not sure what it takes to make an assert op on the GPU, but it would be very nice in the meantime if this were documented somehow or if an alternate dynamic_rnn function were provided without the assert so that we could reap performance benefits of GPU at our own risk until assert on GPU works properly. It does sound messy, but from the developer's point of view, networks using Bi-LSTM and CRF (crf code also hits this assert) are exactly the ones we might want to use multi-GPU with and there are some workloads that have a big need for multi-GPU. Again, I'm not sure if this has implications for single GPU also or just multi-GPU, but I'm just relaying my experiences.\nUpdate:   I realized there was an issue with my soft placement that caused the assert to still occur. Apparently I didn't put soft placement config every time I called tf.Session and even when I put it in the training part, the assert still occurred. odd.. anyways the performance seems to be about the same as with commenting out the assert, so I'm assuming the solution for this is to just use soft device placement and that there is no big issue with using RNN on GPU", "body": "I can't really quite understand the implications of this. Does this prevent LSTMs from being placed on the GPU?\r\n\r\nI'm trying to do multi-GPU which requires explicit assignment of GPU devices, and suddenly when I start explicitly designing devices I start hitting this assert op that's apparently not supported on GPU. But I don't understand how it was running fine on a single GPU in the first place? Or does it mean that the RNN code was running on the CPU? I can't believe that would be the case. But if this were to be the case, wouldn't it kind of be...a big deal and a big enough performance hit to be worth looking at? I haven't really figured out what elements of the LSTM I need to be \"placing\" on the GPU for best performance. My LSTMCells are on the GPU but because this function always gives errors I have to leave it on the CPU, which also limits me from placing functions depending on it on the GPU as well, eventually killing the performance. What I do know is that I commented out the assert, explicitly defined GPU devices and multi-GPU code is working very well now. But I am left with a lot of confusion as well with what's being placed where.\r\n\r\nI'm not sure what it takes to make an assert op on the GPU, but it would be very nice in the meantime if this were documented somehow or if an alternate dynamic_rnn function were provided without the assert so that we could reap performance benefits of GPU at our own risk until assert on GPU works properly. It does sound messy, but from the developer's point of view, networks using Bi-LSTM and CRF (crf code also hits this assert) are exactly the ones we might want to use multi-GPU with and there are some workloads that have a big need for multi-GPU. Again, I'm not sure if this has implications for single GPU also or just multi-GPU, but I'm just relaying my experiences.\r\n\r\nUpdate:   I realized there was an issue with my soft placement that caused the assert to still occur. Apparently I didn't put soft placement config every time I called tf.Session and even when I put it in the training part, the assert still occurred. odd.. anyways the performance seems to be about the same as with commenting out the assert, so I'm assuming the solution for this is to just use soft device placement and that there is no big issue with using RNN on GPU"}