{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12143", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12143/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12143/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12143/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12143", "id": 249024983, "node_id": "MDU6SXNzdWUyNDkwMjQ5ODM=", "number": 12143, "title": "const variable placement issue in distributed tensorflow", "user": {"login": "YongCHN", "id": 6441772, "node_id": "MDQ6VXNlcjY0NDE3NzI=", "avatar_url": "https://avatars2.githubusercontent.com/u/6441772?v=4", "gravatar_id": "", "url": "https://api.github.com/users/YongCHN", "html_url": "https://github.com/YongCHN", "followers_url": "https://api.github.com/users/YongCHN/followers", "following_url": "https://api.github.com/users/YongCHN/following{/other_user}", "gists_url": "https://api.github.com/users/YongCHN/gists{/gist_id}", "starred_url": "https://api.github.com/users/YongCHN/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/YongCHN/subscriptions", "organizations_url": "https://api.github.com/users/YongCHN/orgs", "repos_url": "https://api.github.com/users/YongCHN/repos", "events_url": "https://api.github.com/users/YongCHN/events{/privacy}", "received_events_url": "https://api.github.com/users/YongCHN/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 17, "created_at": "2017-08-09T13:08:33Z", "updated_at": "2018-04-03T01:23:21Z", "closed_at": "2018-04-03T01:23:21Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>In my model, I use the FTRL Optimizer like below:</p>\n<p><code>self.optimizer = tf.train.FtrlOptimizer(0.005, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=1.0, l2_regularization_strength=0.00001) </code></p>\n<p>Inside the FtrlOptimizer it will create several const variables for the parameters, such as learning rate, learning rate power, etc.</p>\n<p>When I run the distributed tensorflow job, from the timeline I can see that for each session run I can see that the worker will send the above const variables to all the ps nodes. This is a cost since the variables are const and not needed to sent to ps nodes repeatedly.</p>\n<p>I was wondering is there a way to pin those const variables to the ps and save the transferring cost during each session run.</p>\n<p>Thanks.</p>", "body_text": "Describe the problem\nIn my model, I use the FTRL Optimizer like below:\nself.optimizer = tf.train.FtrlOptimizer(0.005, learning_rate_power=-0.5, initial_accumulator_value=0.1, l1_regularization_strength=1.0, l2_regularization_strength=0.00001) \nInside the FtrlOptimizer it will create several const variables for the parameters, such as learning rate, learning rate power, etc.\nWhen I run the distributed tensorflow job, from the timeline I can see that for each session run I can see that the worker will send the above const variables to all the ps nodes. This is a cost since the variables are const and not needed to sent to ps nodes repeatedly.\nI was wondering is there a way to pin those const variables to the ps and save the transferring cost during each session run.\nThanks.", "body": "### Describe the problem\r\nIn my model, I use the FTRL Optimizer like below:\r\n\r\n`self.optimizer = tf.train.FtrlOptimizer(0.005,\r\n                learning_rate_power=-0.5,\r\n                initial_accumulator_value=0.1,\r\n                l1_regularization_strength=1.0,\r\n                l2_regularization_strength=0.00001)\r\n`\r\n\r\nInside the FtrlOptimizer it will create several const variables for the parameters, such as learning rate, learning rate power, etc. \r\n\r\nWhen I run the distributed tensorflow job, from the timeline I can see that for each session run I can see that the worker will send the above const variables to all the ps nodes. This is a cost since the variables are const and not needed to sent to ps nodes repeatedly.  \r\n\r\nI was wondering is there a way to pin those const variables to the ps and save the transferring cost during each session run.\r\n\r\nThanks.\r\n"}