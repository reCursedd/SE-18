{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321343908", "html_url": "https://github.com/tensorflow/tensorflow/issues/12143#issuecomment-321343908", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12143", "id": 321343908, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTM0MzkwOA==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T18:36:43Z", "updated_at": "2017-08-09T18:37:23Z", "author_association": "MEMBER", "body_html": "<p>By 'const variable' (!!), do you mean that the <code>FtrlOptimizer</code> creates a <code>tf.Variable</code> for learning rate, that is placed on the PS, and is read each step even though the value of that variable never changes?  (I could imagine this would be useful if you ever wanted an adjustable learning rate schedule)</p>\n<p>Or... does this produce a <code>Const</code> op that is placed on the PS and used on each worker, and the graph partitioner doesn't create a duplicate const on <code>/cpu:0</code> of the worker?</p>\n<p>Attaching a <code>GraphDef</code> would be helpful...</p>", "body_text": "By 'const variable' (!!), do you mean that the FtrlOptimizer creates a tf.Variable for learning rate, that is placed on the PS, and is read each step even though the value of that variable never changes?  (I could imagine this would be useful if you ever wanted an adjustable learning rate schedule)\nOr... does this produce a Const op that is placed on the PS and used on each worker, and the graph partitioner doesn't create a duplicate const on /cpu:0 of the worker?\nAttaching a GraphDef would be helpful...", "body": "By 'const variable' (!!), do you mean that the `FtrlOptimizer` creates a `tf.Variable` for learning rate, that is placed on the PS, and is read each step even though the value of that variable never changes?  (I could imagine this would be useful if you ever wanted an adjustable learning rate schedule)\r\n\r\nOr... does this produce a `Const` op that is placed on the PS and used on each worker, and the graph partitioner doesn't create a duplicate const on `/cpu:0` of the worker?\r\n\r\nAttaching a `GraphDef` would be helpful..."}