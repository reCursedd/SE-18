{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17588", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17588/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17588/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17588/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17588", "id": 303755087, "node_id": "MDU6SXNzdWUzMDM3NTUwODc=", "number": 17588, "title": "AttentionWrapper bug with shape inference", "user": {"login": "soloice", "id": 8534653, "node_id": "MDQ6VXNlcjg1MzQ2NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/8534653?v=4", "gravatar_id": "", "url": "https://api.github.com/users/soloice", "html_url": "https://github.com/soloice", "followers_url": "https://api.github.com/users/soloice/followers", "following_url": "https://api.github.com/users/soloice/following{/other_user}", "gists_url": "https://api.github.com/users/soloice/gists{/gist_id}", "starred_url": "https://api.github.com/users/soloice/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/soloice/subscriptions", "organizations_url": "https://api.github.com/users/soloice/orgs", "repos_url": "https://api.github.com/users/soloice/repos", "events_url": "https://api.github.com/users/soloice/events{/privacy}", "received_events_url": "https://api.github.com/users/soloice/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-03-09T07:59:00Z", "updated_at": "2018-07-25T16:28:37Z", "closed_at": "2018-07-19T15:48:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is my <a href=\"https://github.com/soloice/tf-tutorial/blob/master/src/att_seq2seq_minimal_error.py\">source code</a>, which receives an integer sequence, delete odd numbers, and copy the remaining even number sequence twice. For example, input: [1, 2, 3, 4, 5, 6], output: [2, 4, 6, 2, 4, 6]. Of course, padding is used during training.</p>\n<p>If I comment <a href=\"https://github.com/soloice/tf-tutorial/blob/6675b3b4d3b0b1dbcc3d2541a0637cabbdd27681/src/att_seq2seq_minimal_error.py#L124\">L124</a> out, the code runs very well. However, if this line is enabled, it will raise the following error:</p>\n<blockquote>\n<p>ValueError: The shape for decoder/decoder/while/Merge_7:0 is not an invariant for the loop. It enters the loop with shape (64, 50), but has shape (?, 50) after one iteration. Provide shape invariants using either the <code>shape_invariants</code> argument of tf.while_loop or set_shape() on the loop variables.</p>\n</blockquote>\n<p>This line is intended to disable input feeding scheme in paper \"Effective Approaches to Attention-based Neural Machine Translation\", which is the default behavior of AttentionWrapper.</p>\n<p>Currently both my batch_size and num_steps are <code>None</code>. If I fix the batch size, i.e.: change placeholders to</p>\n<pre><code>encoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='encoder_inputs')\ndecoder_targets = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_targets')\ndecoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_inputs')\nencoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='encoder_length')\ndecoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='decoder_length')\n</code></pre>\n<p>, everything works fine again.</p>\n<p>The code is so simple, so probably it is a bug with shape inference.</p>", "body_text": "This is my source code, which receives an integer sequence, delete odd numbers, and copy the remaining even number sequence twice. For example, input: [1, 2, 3, 4, 5, 6], output: [2, 4, 6, 2, 4, 6]. Of course, padding is used during training.\nIf I comment L124 out, the code runs very well. However, if this line is enabled, it will raise the following error:\n\nValueError: The shape for decoder/decoder/while/Merge_7:0 is not an invariant for the loop. It enters the loop with shape (64, 50), but has shape (?, 50) after one iteration. Provide shape invariants using either the shape_invariants argument of tf.while_loop or set_shape() on the loop variables.\n\nThis line is intended to disable input feeding scheme in paper \"Effective Approaches to Attention-based Neural Machine Translation\", which is the default behavior of AttentionWrapper.\nCurrently both my batch_size and num_steps are None. If I fix the batch size, i.e.: change placeholders to\nencoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='encoder_inputs')\ndecoder_targets = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_targets')\ndecoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_inputs')\nencoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='encoder_length')\ndecoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='decoder_length')\n\n, everything works fine again.\nThe code is so simple, so probably it is a bug with shape inference.", "body": "This is my [source code](https://github.com/soloice/tf-tutorial/blob/master/src/att_seq2seq_minimal_error.py), which receives an integer sequence, delete odd numbers, and copy the remaining even number sequence twice. For example, input: [1, 2, 3, 4, 5, 6], output: [2, 4, 6, 2, 4, 6]. Of course, padding is used during training.\r\n\r\nIf I comment [L124](https://github.com/soloice/tf-tutorial/blob/6675b3b4d3b0b1dbcc3d2541a0637cabbdd27681/src/att_seq2seq_minimal_error.py#L124) out, the code runs very well. However, if this line is enabled, it will raise the following error:\r\n\r\n> ValueError: The shape for decoder/decoder/while/Merge_7:0 is not an invariant for the loop. It enters the loop with shape (64, 50), but has shape (?, 50) after one iteration. Provide shape invariants using either the `shape_invariants` argument of tf.while_loop or set_shape() on the loop variables.\r\n\r\nThis line is intended to disable input feeding scheme in paper \"Effective Approaches to Attention-based Neural Machine Translation\", which is the default behavior of AttentionWrapper.\r\n\r\nCurrently both my batch_size and num_steps are `None`. If I fix the batch size, i.e.: change placeholders to\r\n```\r\nencoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='encoder_inputs')\r\ndecoder_targets = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_targets')\r\ndecoder_inputs = tf.placeholder(shape=[batch_size, None], dtype=tf.int32, name='decoder_inputs')\r\nencoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='encoder_length')\r\ndecoder_length = tf.placeholder(shape=[batch_size], dtype=tf.int32, name='decoder_length')\r\n```\r\n, everything works fine again.\r\n\r\nThe code is so simple, so probably it is a bug with shape inference.\r\n\r\n"}