{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282743979", "html_url": "https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-282743979", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817", "id": 282743979, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mjc0Mzk3OQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-27T14:59:49Z", "updated_at": "2017-02-27T14:59:49Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">You could narrow it down by making your program single threaded - first\npreload the queue and then consume it in the same thread. If it's still\nslow, then could narrow it down to a case that spends most time in dequeue\nand is single threaded, at which point CPU profile can tell where the time\nis spent</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Feb 27, 2017 2:43 AM, \"akaitsuki-ii\" ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> &lt;<a href=\"https://github.com/yaroslavvb\">https://github.com/yaroslavvb</a>&gt; Thank you for reminding me of\n checking in TensorBoard.\n I do some test and it seems that *enqueue* part is not a bottleneck of\n the program, it's full (and capacity is large enough) all the time. There\n is no problem of starvation.\n And I find that *dequeue* operation really cause problems, according to\n the compute time statistics in TensorBoard, *dequeue* cost 6x compute\n time than my model, which result in low GPU-Util and even worse performance\n than using feed_dict.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"209753013\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/7817\" href=\"https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-282686023\">#7817 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AABaHL5LVWVVBOMnyKTCJPSkiGVC56Ioks5rgqi3gaJpZM4MJ7ZR\">https://github.com/notifications/unsubscribe-auth/AABaHL5LVWVVBOMnyKTCJPSkiGVC56Ioks5rgqi3gaJpZM4MJ7ZR</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "You could narrow it down by making your program single threaded - first\npreload the queue and then consume it in the same thread. If it's still\nslow, then could narrow it down to a case that spends most time in dequeue\nand is single threaded, at which point CPU profile can tell where the time\nis spent\n\u2026\nOn Feb 27, 2017 2:43 AM, \"akaitsuki-ii\" ***@***.***> wrote:\n @yaroslavvb <https://github.com/yaroslavvb> Thank you for reminding me of\n checking in TensorBoard.\n I do some test and it seems that *enqueue* part is not a bottleneck of\n the program, it's full (and capacity is large enough) all the time. There\n is no problem of starvation.\n And I find that *dequeue* operation really cause problems, according to\n the compute time statistics in TensorBoard, *dequeue* cost 6x compute\n time than my model, which result in low GPU-Util and even worse performance\n than using feed_dict.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#7817 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AABaHL5LVWVVBOMnyKTCJPSkiGVC56Ioks5rgqi3gaJpZM4MJ7ZR>\n .", "body": "You could narrow it down by making your program single threaded - first\npreload the queue and then consume it in the same thread. If it's still\nslow, then could narrow it down to a case that spends most time in dequeue\nand is single threaded, at which point CPU profile can tell where the time\nis spent\n\nOn Feb 27, 2017 2:43 AM, \"akaitsuki-ii\" <notifications@github.com> wrote:\n\n> @yaroslavvb <https://github.com/yaroslavvb> Thank you for reminding me of\n> checking in TensorBoard.\n> I do some test and it seems that *enqueue* part is not a bottleneck of\n> the program, it's full (and capacity is large enough) all the time. There\n> is no problem of starvation.\n> And I find that *dequeue* operation really cause problems, according to\n> the compute time statistics in TensorBoard, *dequeue* cost 6x compute\n> time than my model, which result in low GPU-Util and even worse performance\n> than using feed_dict.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-282686023>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AABaHL5LVWVVBOMnyKTCJPSkiGVC56Ioks5rgqi3gaJpZM4MJ7ZR>\n> .\n>\n"}