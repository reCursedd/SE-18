{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7817", "id": 209753013, "node_id": "MDU6SXNzdWUyMDk3NTMwMTM=", "number": 7817, "title": "Reading data with queue is even slower than using feed_dict", "user": {"login": "akaitsuki-ii", "id": 8386553, "node_id": "MDQ6VXNlcjgzODY1NTM=", "avatar_url": "https://avatars2.githubusercontent.com/u/8386553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akaitsuki-ii", "html_url": "https://github.com/akaitsuki-ii", "followers_url": "https://api.github.com/users/akaitsuki-ii/followers", "following_url": "https://api.github.com/users/akaitsuki-ii/following{/other_user}", "gists_url": "https://api.github.com/users/akaitsuki-ii/gists{/gist_id}", "starred_url": "https://api.github.com/users/akaitsuki-ii/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akaitsuki-ii/subscriptions", "organizations_url": "https://api.github.com/users/akaitsuki-ii/orgs", "repos_url": "https://api.github.com/users/akaitsuki-ii/repos", "events_url": "https://api.github.com/users/akaitsuki-ii/events{/privacy}", "received_events_url": "https://api.github.com/users/akaitsuki-ii/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-02-23T12:52:19Z", "updated_at": "2018-04-22T13:01:40Z", "closed_at": "2017-02-24T18:10:06Z", "author_association": "NONE", "body_html": "<p>It seems reading data with feed_dict is inefficient in tensorflow (I found it 3-4 times slower than a theano-based implementation with a totally same network structure), I turn to a queue-based method as official recommendation. However, my experiment result give a even worse performance. Is there anything wrong ?</p>\n<h3>Related issue</h3>\n<p><a href=\"https://github.com/tensorflow/tensorflow/issues/3377\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3377/hovercard\">#3377 Moving data from CPU to GPU is slow</a></p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 16.04<br>\nCPU: Intel i4790-k<br>\nGPU: Nvidia GTX 1070 (8 GB)<br>\nMemory: 16 GB<br>\nTensorflow version: 1.0<br>\nCUDA version: 8.0<br>\ncuDNN version: 5.0</p>\n<h3>Implementation Example</h3>\n<ol>\n<li>Using feed_dict</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>x, y <span class=\"pl-k\">=</span> np.array(<span class=\"pl-c1\">...</span>)\nin_x, in_y <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-c1\">...</span>)\n<span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span> f() is some computation in the network, including embedding_lookup, </span>\n<span class=\"pl-s\">bidirectional_dynamic_rnn, dense <span class=\"pl-pds\">\"\"\"</span></span>\ntrain_op <span class=\"pl-k\">=</span> f(in_x, in_y) \nsess <span class=\"pl-k\">=</span> tf.Session()\n<span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_epochs): <span class=\"pl-c\"><span class=\"pl-c\">#</span> num_epochs is 100 here</span>\n    sess.run(train_op, {in_x: x, in_y: y})</pre></div>\n<ol start=\"2\">\n<li>Using queue</li>\n</ol>\n<div class=\"highlight highlight-source-python\"><pre>x, y <span class=\"pl-k\">=</span> np.array(<span class=\"pl-c1\">...</span>)\nx, y <span class=\"pl-k\">=</span> tf.convert_to_tensor(<span class=\"pl-c1\">...</span>)\nin_x, in_y <span class=\"pl-k\">=</span> tf.train.slice_input_producer([x, y], <span class=\"pl-v\">num_epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">100</span>)\nin_x, in_y <span class=\"pl-k\">=</span> tf.train.batch([in_x, in_y], <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>)\ntrain_op <span class=\"pl-k\">=</span> f(in_x, in_y)\nsess <span class=\"pl-k\">=</span> tf.Session()\ncoord <span class=\"pl-k\">=</span> tf.train.Coordinator()\nthreads <span class=\"pl-k\">=</span> tf.train.start_queue_runners(sess, coord)\n<span class=\"pl-k\">try</span>:\n    <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> coord.should_stop():\n        sess.run(train_op)\n    <span class=\"pl-k\">except</span> <span class=\"pl-c1\">Exception</span> <span class=\"pl-k\">as</span> e:\n        coord.request_stop(e)\n    <span class=\"pl-k\">finally</span>:\n        coord.request_stop()\ncoord.join(threads)</pre></div>", "body_text": "It seems reading data with feed_dict is inefficient in tensorflow (I found it 3-4 times slower than a theano-based implementation with a totally same network structure), I turn to a queue-based method as official recommendation. However, my experiment result give a even worse performance. Is there anything wrong ?\nRelated issue\n#3377 Moving data from CPU to GPU is slow\nEnvironment info\nOperating System: Ubuntu 16.04\nCPU: Intel i4790-k\nGPU: Nvidia GTX 1070 (8 GB)\nMemory: 16 GB\nTensorflow version: 1.0\nCUDA version: 8.0\ncuDNN version: 5.0\nImplementation Example\n\nUsing feed_dict\n\nx, y = np.array(...)\nin_x, in_y = tf.placeholder(...)\n\"\"\" f() is some computation in the network, including embedding_lookup, \nbidirectional_dynamic_rnn, dense \"\"\"\ntrain_op = f(in_x, in_y) \nsess = tf.Session()\nfor _ in range(num_epochs): # num_epochs is 100 here\n    sess.run(train_op, {in_x: x, in_y: y})\n\nUsing queue\n\nx, y = np.array(...)\nx, y = tf.convert_to_tensor(...)\nin_x, in_y = tf.train.slice_input_producer([x, y], num_epochs=100)\nin_x, in_y = tf.train.batch([in_x, in_y], batch_size=32)\ntrain_op = f(in_x, in_y)\nsess = tf.Session()\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess, coord)\ntry:\n    while not coord.should_stop():\n        sess.run(train_op)\n    except Exception as e:\n        coord.request_stop(e)\n    finally:\n        coord.request_stop()\ncoord.join(threads)", "body": "It seems reading data with feed_dict is inefficient in tensorflow (I found it 3-4 times slower than a theano-based implementation with a totally same network structure), I turn to a queue-based method as official recommendation. However, my experiment result give a even worse performance. Is there anything wrong ?\r\n\r\n### Related issue\r\n[#3377 Moving data from CPU to GPU is slow](https://github.com/tensorflow/tensorflow/issues/3377)\r\n\r\n### Environment info\r\nOperating System: Ubuntu 16.04\r\nCPU: Intel i4790-k\r\nGPU: Nvidia GTX 1070 (8 GB)\r\nMemory: 16 GB\r\nTensorflow version: 1.0\r\nCUDA version: 8.0\r\ncuDNN version: 5.0\r\n\r\n### Implementation Example\r\n1) Using feed_dict\r\n```python\r\nx, y = np.array(...)\r\nin_x, in_y = tf.placeholder(...)\r\n\"\"\" f() is some computation in the network, including embedding_lookup, \r\nbidirectional_dynamic_rnn, dense \"\"\"\r\ntrain_op = f(in_x, in_y) \r\nsess = tf.Session()\r\nfor _ in range(num_epochs): # num_epochs is 100 here\r\n    sess.run(train_op, {in_x: x, in_y: y})\r\n```\r\n2) Using queue\r\n```python\r\nx, y = np.array(...)\r\nx, y = tf.convert_to_tensor(...)\r\nin_x, in_y = tf.train.slice_input_producer([x, y], num_epochs=100)\r\nin_x, in_y = tf.train.batch([in_x, in_y], batch_size=32)\r\ntrain_op = f(in_x, in_y)\r\nsess = tf.Session()\r\ncoord = tf.train.Coordinator()\r\nthreads = tf.train.start_queue_runners(sess, coord)\r\ntry:\r\n    while not coord.should_stop():\r\n        sess.run(train_op)\r\n    except Exception as e:\r\n        coord.request_stop(e)\r\n    finally:\r\n        coord.request_stop()\r\ncoord.join(threads)\r\n```"}