{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282491525", "html_url": "https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-282491525", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817", "id": 282491525, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjQ5MTUyNQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-25T15:37:20Z", "updated_at": "2017-02-25T15:40:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>What I meant was -- make sure your <code>dequeue</code> part of the graph is slower than your <code>enqueue</code> part. Pre-loading the queue is nota solution on its own because if you dequeue faster than you enqueue, eventually you'll consume all the elements on the queue and be back to queue starvation (check your queue size, there's queue fullness statistic in TensorBoard and also q.size() operation). The reality is that queues are bad when your computation is extremely cheap because of the way Python multi-threading works, feed_dict may actually be better for those cases.</p>", "body_text": "What I meant was -- make sure your dequeue part of the graph is slower than your enqueue part. Pre-loading the queue is nota solution on its own because if you dequeue faster than you enqueue, eventually you'll consume all the elements on the queue and be back to queue starvation (check your queue size, there's queue fullness statistic in TensorBoard and also q.size() operation). The reality is that queues are bad when your computation is extremely cheap because of the way Python multi-threading works, feed_dict may actually be better for those cases.", "body": "What I meant was -- make sure your `dequeue` part of the graph is slower than your `enqueue` part. Pre-loading the queue is nota solution on its own because if you dequeue faster than you enqueue, eventually you'll consume all the elements on the queue and be back to queue starvation (check your queue size, there's queue fullness statistic in TensorBoard and also q.size() operation). The reality is that queues are bad when your computation is extremely cheap because of the way Python multi-threading works, feed_dict may actually be better for those cases."}