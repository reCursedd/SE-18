{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286160994", "html_url": "https://github.com/tensorflow/tensorflow/issues/7817#issuecomment-286160994", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7817", "id": 286160994, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjE2MDk5NA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-13T16:25:55Z", "updated_at": "2017-03-13T16:25:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There are about 1 million ops being executed (<code>grep node_stats stepstats-1.json | wc</code>), and it takes 4.72 second on my 2014 MacBook, which seems fairly fast for 1 million ops. Such large computation graphs are outside of typical range of cases TensorFlow is tuned for, so there's significant overhead involved.</p>\n<p>As to why I said it is a false positive, if you open <code>stepstats-1.json</code> generated by the commit I referenced, you will see the following</p>\n<pre><code>    node_stats {\n      node_name: \"random_shuffle_queue_DequeueMany/n\"\n      all_start_micros: 1489378683948609\n      op_start_rel_micros: 1\n      op_end_rel_micros: 1\n      all_end_rel_micros: 52\n      memory {\n        allocator_name: \"cpu\"\n</code></pre>\n<p>So the Dequeue op takes 51 microseconds.</p>\n<p>As to why <code>feed_dict</code> is faster, it's an interesting question, and would require some digging to getto. One theory is that <code>dequeue</code> op requires a kernel launch, so when you already have a million ops being launched, perhaps the thread scheduling system is overloaded,  any new kernel launches are slow. Meanwhile, <code>feed_dict</code> approach does not add an additional op for reading data from queue, so no kernel launch.</p>", "body_text": "There are about 1 million ops being executed (grep node_stats stepstats-1.json | wc), and it takes 4.72 second on my 2014 MacBook, which seems fairly fast for 1 million ops. Such large computation graphs are outside of typical range of cases TensorFlow is tuned for, so there's significant overhead involved.\nAs to why I said it is a false positive, if you open stepstats-1.json generated by the commit I referenced, you will see the following\n    node_stats {\n      node_name: \"random_shuffle_queue_DequeueMany/n\"\n      all_start_micros: 1489378683948609\n      op_start_rel_micros: 1\n      op_end_rel_micros: 1\n      all_end_rel_micros: 52\n      memory {\n        allocator_name: \"cpu\"\n\nSo the Dequeue op takes 51 microseconds.\nAs to why feed_dict is faster, it's an interesting question, and would require some digging to getto. One theory is that dequeue op requires a kernel launch, so when you already have a million ops being launched, perhaps the thread scheduling system is overloaded,  any new kernel launches are slow. Meanwhile, feed_dict approach does not add an additional op for reading data from queue, so no kernel launch.", "body": "There are about 1 million ops being executed (`grep node_stats stepstats-1.json | wc`), and it takes 4.72 second on my 2014 MacBook, which seems fairly fast for 1 million ops. Such large computation graphs are outside of typical range of cases TensorFlow is tuned for, so there's significant overhead involved.\r\n\r\nAs to why I said it is a false positive, if you open `stepstats-1.json` generated by the commit I referenced, you will see the following\r\n\r\n```\r\n    node_stats {\r\n      node_name: \"random_shuffle_queue_DequeueMany/n\"\r\n      all_start_micros: 1489378683948609\r\n      op_start_rel_micros: 1\r\n      op_end_rel_micros: 1\r\n      all_end_rel_micros: 52\r\n      memory {\r\n        allocator_name: \"cpu\"\r\n```\r\n\r\nSo the Dequeue op takes 51 microseconds.\r\n\r\n\r\nAs to why `feed_dict` is faster, it's an interesting question, and would require some digging to getto. One theory is that `dequeue` op requires a kernel launch, so when you already have a million ops being launched, perhaps the thread scheduling system is overloaded,  any new kernel launches are slow. Meanwhile, `feed_dict` approach does not add an additional op for reading data from queue, so no kernel launch."}