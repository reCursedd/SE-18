{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407505651", "html_url": "https://github.com/tensorflow/tensorflow/issues/14699#issuecomment-407505651", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14699", "id": 407505651, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzUwNTY1MQ==", "user": {"login": "issa-s-ayoub", "id": 26584101, "node_id": "MDQ6VXNlcjI2NTg0MTAx", "avatar_url": "https://avatars2.githubusercontent.com/u/26584101?v=4", "gravatar_id": "", "url": "https://api.github.com/users/issa-s-ayoub", "html_url": "https://github.com/issa-s-ayoub", "followers_url": "https://api.github.com/users/issa-s-ayoub/followers", "following_url": "https://api.github.com/users/issa-s-ayoub/following{/other_user}", "gists_url": "https://api.github.com/users/issa-s-ayoub/gists{/gist_id}", "starred_url": "https://api.github.com/users/issa-s-ayoub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/issa-s-ayoub/subscriptions", "organizations_url": "https://api.github.com/users/issa-s-ayoub/orgs", "repos_url": "https://api.github.com/users/issa-s-ayoub/repos", "events_url": "https://api.github.com/users/issa-s-ayoub/events{/privacy}", "received_events_url": "https://api.github.com/users/issa-s-ayoub/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-24T18:25:43Z", "updated_at": "2018-07-24T20:48:09Z", "author_association": "NONE", "body_html": "<p>I have a similar issue with using the optimizer and tf.cond. Here is the simplified code:</p>\n<pre><code>def optimize(loss):\n\n    with tf.name_scope('Optimizer'):\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        with tf.control_dependencies(update_ops):\n            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n                def prev_grads_are_None():\n        \n                    # This will compute the local gradient given the loss function (taking into account all trainable variables)\n                    train_step = optimizer.minimize(loss)\n                    grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\n        \n                    return train_step, grads_wrt_initial_state\n    \n            train_step, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\n            return train_step, grads_wrt_initial_state\n</code></pre>\n<p>So when running the code in Jupyter Notebook, the kernel shows busy, but nothing seems to continue running. Please note that I am training a 2 layer GRU.</p>", "body_text": "I have a similar issue with using the optimizer and tf.cond. Here is the simplified code:\ndef optimize(loss):\n\n    with tf.name_scope('Optimizer'):\n        update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n        with tf.control_dependencies(update_ops):\n            optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n                def prev_grads_are_None():\n        \n                    # This will compute the local gradient given the loss function (taking into account all trainable variables)\n                    train_step = optimizer.minimize(loss)\n                    grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\n        \n                    return train_step, grads_wrt_initial_state\n    \n            train_step, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\n            return train_step, grads_wrt_initial_state\n\nSo when running the code in Jupyter Notebook, the kernel shows busy, but nothing seems to continue running. Please note that I am training a 2 layer GRU.", "body": "I have a similar issue with using the optimizer and tf.cond. Here is the simplified code:\r\n\r\n    def optimize(loss):\r\n    \r\n        with tf.name_scope('Optimizer'):\r\n            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n    \r\n            with tf.control_dependencies(update_ops):\r\n                optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n                    def prev_grads_are_None():\r\n            \r\n                        # This will compute the local gradient given the loss function (taking into account all trainable variables)\r\n                        train_step = optimizer.minimize(loss)\r\n                        grads_wrt_initial_state = tf.gradients(loss, initial_state, grad_ys=None)\r\n            \r\n                        return train_step, grads_wrt_initial_state\r\n        \r\n                train_step, grads_wrt_initial_state = tf.cond(this_is_last_batch, prev_grads_are_None, prev_grads_are_None)\r\n                return train_step, grads_wrt_initial_state\r\n\r\nSo when running the code in Jupyter Notebook, the kernel shows busy, but nothing seems to continue running. Please note that I am training a 2 layer GRU."}