{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356501397", "html_url": "https://github.com/tensorflow/tensorflow/issues/14699#issuecomment-356501397", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14699", "id": 356501397, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjUwMTM5Nw==", "user": {"login": "Multihuntr", "id": 10515040, "node_id": "MDQ6VXNlcjEwNTE1MDQw", "avatar_url": "https://avatars2.githubusercontent.com/u/10515040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Multihuntr", "html_url": "https://github.com/Multihuntr", "followers_url": "https://api.github.com/users/Multihuntr/followers", "following_url": "https://api.github.com/users/Multihuntr/following{/other_user}", "gists_url": "https://api.github.com/users/Multihuntr/gists{/gist_id}", "starred_url": "https://api.github.com/users/Multihuntr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Multihuntr/subscriptions", "organizations_url": "https://api.github.com/users/Multihuntr/orgs", "repos_url": "https://api.github.com/users/Multihuntr/repos", "events_url": "https://api.github.com/users/Multihuntr/events{/privacy}", "received_events_url": "https://api.github.com/users/Multihuntr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-10T05:09:41Z", "updated_at": "2018-01-10T05:29:12Z", "author_association": "NONE", "body_html": "<p>I was unable to reproduce this bug given the information provided. The following script runs as expected (running release v1.3.0).</p>\n<pre lang=\"{py}\"><code>import numpy as np\nimport tensorflow as tf\n\ndef my_op(inputs, name=None):\n  with tf.variable_scope(name, default_name='my_scope', reuse=False):\n    count = tf.get_variable('count', shape=[],\n      initializer=tf.zeros_initializer(), trainable=False)\n\n    def myfunc1():\n      return 1\n\n    def myfunc2():\n      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, count.assign_add(10.0))\n      return 1\n\n    tf.cond(tf.less(count, 2), myfunc1, myfunc2)\n    return inputs\n\nglobal_step = tf.get_variable('global_step', [],\n  initializer=tf.zeros_initializer(), trainable=False, dtype=tf.int32)\na = tf.placeholder(tf.float32, [None, 5])\nb = tf.placeholder(tf.float32, [None, 3])\na_out = tf.layers.dense(my_op(a, 'somewhere'), 3)\n\nloss = tf.losses.mean_squared_error(b, a_out)\nopt = tf.train.AdamOptimizer(learning_rate=1e-2)\n\ntrain_op = opt.minimize(loss, global_step)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n\n  print(sess.run(global_step)) # prints 0\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]] })\n  print(sess.run(global_step)) # prints 1\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]] })\n  print(sess.run(global_step)) # prints 2\n  sess.run(train_op, { a: np.empty(shape=[0,5]), b: np.empty(shape=[0,3]) })\n  print(sess.run(global_step)) # prints 3\n</code></pre>\n<p>As does changing the <code>tf.less(count, 2)</code> to <code>tf.less(1,2)</code>.<br>\nAs does using <code>a</code> directly in the <code>tf.layers.dense</code>, but still calling <code>my_op(a, 'somewhere')</code> elsewhere.<br>\nAs does using a different <code>Optimizer</code>.</p>\n<p>So there's another piece to this puzzle; there's something else going on to cause the issue.</p>", "body_text": "I was unable to reproduce this bug given the information provided. The following script runs as expected (running release v1.3.0).\nimport numpy as np\nimport tensorflow as tf\n\ndef my_op(inputs, name=None):\n  with tf.variable_scope(name, default_name='my_scope', reuse=False):\n    count = tf.get_variable('count', shape=[],\n      initializer=tf.zeros_initializer(), trainable=False)\n\n    def myfunc1():\n      return 1\n\n    def myfunc2():\n      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, count.assign_add(10.0))\n      return 1\n\n    tf.cond(tf.less(count, 2), myfunc1, myfunc2)\n    return inputs\n\nglobal_step = tf.get_variable('global_step', [],\n  initializer=tf.zeros_initializer(), trainable=False, dtype=tf.int32)\na = tf.placeholder(tf.float32, [None, 5])\nb = tf.placeholder(tf.float32, [None, 3])\na_out = tf.layers.dense(my_op(a, 'somewhere'), 3)\n\nloss = tf.losses.mean_squared_error(b, a_out)\nopt = tf.train.AdamOptimizer(learning_rate=1e-2)\n\ntrain_op = opt.minimize(loss, global_step)\n\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n\n  print(sess.run(global_step)) # prints 0\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]] })\n  print(sess.run(global_step)) # prints 1\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]] })\n  print(sess.run(global_step)) # prints 2\n  sess.run(train_op, { a: np.empty(shape=[0,5]), b: np.empty(shape=[0,3]) })\n  print(sess.run(global_step)) # prints 3\n\nAs does changing the tf.less(count, 2) to tf.less(1,2).\nAs does using a directly in the tf.layers.dense, but still calling my_op(a, 'somewhere') elsewhere.\nAs does using a different Optimizer.\nSo there's another piece to this puzzle; there's something else going on to cause the issue.", "body": "I was unable to reproduce this bug given the information provided. The following script runs as expected (running release v1.3.0). \r\n\r\n``` {py}\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\ndef my_op(inputs, name=None):\r\n  with tf.variable_scope(name, default_name='my_scope', reuse=False):\r\n    count = tf.get_variable('count', shape=[],\r\n      initializer=tf.zeros_initializer(), trainable=False)\r\n\r\n    def myfunc1():\r\n      return 1\r\n\r\n    def myfunc2():\r\n      tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, count.assign_add(10.0))\r\n      return 1\r\n\r\n    tf.cond(tf.less(count, 2), myfunc1, myfunc2)\r\n    return inputs\r\n\r\nglobal_step = tf.get_variable('global_step', [],\r\n  initializer=tf.zeros_initializer(), trainable=False, dtype=tf.int32)\r\na = tf.placeholder(tf.float32, [None, 5])\r\nb = tf.placeholder(tf.float32, [None, 3])\r\na_out = tf.layers.dense(my_op(a, 'somewhere'), 3)\r\n\r\nloss = tf.losses.mean_squared_error(b, a_out)\r\nopt = tf.train.AdamOptimizer(learning_rate=1e-2)\r\n\r\ntrain_op = opt.minimize(loss, global_step)\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n\r\n  print(sess.run(global_step)) # prints 0\r\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]] })\r\n  print(sess.run(global_step)) # prints 1\r\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]] })\r\n  print(sess.run(global_step)) # prints 2\r\n  sess.run(train_op, { a: np.empty(shape=[0,5]), b: np.empty(shape=[0,3]) })\r\n  print(sess.run(global_step)) # prints 3\r\n```\r\n\r\nAs does changing the `tf.less(count, 2)` to `tf.less(1,2)`.\r\nAs does using `a` directly in the `tf.layers.dense`, but still calling `my_op(a, 'somewhere')` elsewhere.\r\nAs does using a different `Optimizer`.\r\n\r\nSo there's another piece to this puzzle; there's something else going on to cause the issue.\r\n  "}