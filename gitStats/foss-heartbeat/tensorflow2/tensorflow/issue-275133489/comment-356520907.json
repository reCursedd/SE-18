{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356520907", "html_url": "https://github.com/tensorflow/tensorflow/issues/14699#issuecomment-356520907", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14699", "id": 356520907, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjUyMDkwNw==", "user": {"login": "Multihuntr", "id": 10515040, "node_id": "MDQ6VXNlcjEwNTE1MDQw", "avatar_url": "https://avatars2.githubusercontent.com/u/10515040?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Multihuntr", "html_url": "https://github.com/Multihuntr", "followers_url": "https://api.github.com/users/Multihuntr/followers", "following_url": "https://api.github.com/users/Multihuntr/following{/other_user}", "gists_url": "https://api.github.com/users/Multihuntr/gists{/gist_id}", "starred_url": "https://api.github.com/users/Multihuntr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Multihuntr/subscriptions", "organizations_url": "https://api.github.com/users/Multihuntr/orgs", "repos_url": "https://api.github.com/users/Multihuntr/repos", "events_url": "https://api.github.com/users/Multihuntr/events{/privacy}", "received_events_url": "https://api.github.com/users/Multihuntr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-10T07:23:08Z", "updated_at": "2018-01-10T07:23:08Z", "author_association": "NONE", "body_html": "<p>I figured out the missing ingredient to reproduce the bug: you need to put your <code>opt.minimize</code> inside a <code>with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS))</code></p>\n<p>Turns out this is a rather dangerous bug: Adding an operation to a collection inside a function used in a <code>tf.cond</code> <strong>silently</strong> prevents any operations that use that collection as a control dependency, regardless of whether that <code>tf.cond</code> is ever used.</p>\n<p>So, it's not just the global_step; it's everything! Here's the minimal code I put together to highlight the bug in action.</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nflag = tf.placeholder(tf.bool, [])\nglobal_step = tf.train.get_or_create_global_step()\n\n# Include breaking code, putting a no_op inside a cond,\n# which isn't even used anywhere, and shouldn't impact anything else\ndef myfunc():\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, tf.no_op())\n  return 1\ntf.cond(flag, myfunc, lambda: 1)\n\n# Make our ad hoc model\na = tf.placeholder(tf.float32, [None, 5])\nb = tf.placeholder(tf.float32, [None, 3])\na_out = tf.layers.dense(a, 3)\ngraph = tf.get_default_graph()\nbias = graph.get_tensor_by_name('dense/bias:0')\nweights = graph.get_tensor_by_name('dense/kernel:0')\n\n# Define our train_op\nloss = tf.losses.mean_squared_error(b, a_out)\nopt = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n  train_op = opt.minimize(loss, global_step)\n\n# Test everything\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n\n  print('BEFORE ANYTHING IS RUN')\n  print(sess.run(global_step))\n  print(sess.run(bias))\n  print(sess.run(weights))\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]], flag: False })\n  print('AFTER FIRST RUN:  we do not follow the path with the update op added')\n  print(sess.run(global_step))\n  print(sess.run(bias))\n  print(sess.run(weights))\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]], flag: True })\n  print('AFTER SECOND RUN: we follow the path with the update op added')\n  print(sess.run(global_step))\n  print(sess.run(bias))\n  print(sess.run(weights))\n</code></pre>\n<p>Running the above code gives</p>\n<pre><code>BEFORE ANYTHING IS RUN\n0\n[ 0.  0.  0.]\n[[-0.37039062 -0.37549886  0.73892838]\n [-0.20615649  0.03782606  0.50503689]\n [-0.43216512  0.73242658  0.42330664]\n [ 0.11180687  0.07816333 -0.02583456]\n [ 0.15668219  0.58735186  0.83961934]]\nAFTER FIRST RUN:  we do not follow the path with the update op added\n0\n[ 0.  0.  0.]\n[[-0.37039062 -0.37549886  0.73892838]\n [-0.20615649  0.03782606  0.50503689]\n [-0.43216512  0.73242658  0.42330664]\n [ 0.11180687  0.07816333 -0.02583456]\n [ 0.15668219  0.58735186  0.83961934]]\nAFTER SECOND RUN: we follow the path with the update op added\n1\n[ 0.00723237  0.0099021   0.01525755]\n[[-0.36966738 -0.37450865  0.74045414]\n [-0.20471002  0.03980648  0.50808841]\n [-0.42999542  0.73539722  0.42788389]\n [ 0.11469982  0.08212417 -0.01973154]\n [ 0.16029838  0.59230292  0.84724814]]\n</code></pre>\n<p>I looked around for other people experiencing problems with adding ops to the UPDATE_OPS collection inside a <code>tf.cond</code> and couldn't find anything. The fact that it completely halts training despite not being a required node for the <code>train_op</code> and without any errors means that you could silently break your whole training with just those few lines.</p>", "body_text": "I figured out the missing ingredient to reproduce the bug: you need to put your opt.minimize inside a with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS))\nTurns out this is a rather dangerous bug: Adding an operation to a collection inside a function used in a tf.cond silently prevents any operations that use that collection as a control dependency, regardless of whether that tf.cond is ever used.\nSo, it's not just the global_step; it's everything! Here's the minimal code I put together to highlight the bug in action.\nimport numpy as np\nimport tensorflow as tf\n\nflag = tf.placeholder(tf.bool, [])\nglobal_step = tf.train.get_or_create_global_step()\n\n# Include breaking code, putting a no_op inside a cond,\n# which isn't even used anywhere, and shouldn't impact anything else\ndef myfunc():\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, tf.no_op())\n  return 1\ntf.cond(flag, myfunc, lambda: 1)\n\n# Make our ad hoc model\na = tf.placeholder(tf.float32, [None, 5])\nb = tf.placeholder(tf.float32, [None, 3])\na_out = tf.layers.dense(a, 3)\ngraph = tf.get_default_graph()\nbias = graph.get_tensor_by_name('dense/bias:0')\nweights = graph.get_tensor_by_name('dense/kernel:0')\n\n# Define our train_op\nloss = tf.losses.mean_squared_error(b, a_out)\nopt = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n  train_op = opt.minimize(loss, global_step)\n\n# Test everything\nwith tf.Session() as sess:\n  sess.run(tf.global_variables_initializer())\n\n  print('BEFORE ANYTHING IS RUN')\n  print(sess.run(global_step))\n  print(sess.run(bias))\n  print(sess.run(weights))\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]], flag: False })\n  print('AFTER FIRST RUN:  we do not follow the path with the update op added')\n  print(sess.run(global_step))\n  print(sess.run(bias))\n  print(sess.run(weights))\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]], flag: True })\n  print('AFTER SECOND RUN: we follow the path with the update op added')\n  print(sess.run(global_step))\n  print(sess.run(bias))\n  print(sess.run(weights))\n\nRunning the above code gives\nBEFORE ANYTHING IS RUN\n0\n[ 0.  0.  0.]\n[[-0.37039062 -0.37549886  0.73892838]\n [-0.20615649  0.03782606  0.50503689]\n [-0.43216512  0.73242658  0.42330664]\n [ 0.11180687  0.07816333 -0.02583456]\n [ 0.15668219  0.58735186  0.83961934]]\nAFTER FIRST RUN:  we do not follow the path with the update op added\n0\n[ 0.  0.  0.]\n[[-0.37039062 -0.37549886  0.73892838]\n [-0.20615649  0.03782606  0.50503689]\n [-0.43216512  0.73242658  0.42330664]\n [ 0.11180687  0.07816333 -0.02583456]\n [ 0.15668219  0.58735186  0.83961934]]\nAFTER SECOND RUN: we follow the path with the update op added\n1\n[ 0.00723237  0.0099021   0.01525755]\n[[-0.36966738 -0.37450865  0.74045414]\n [-0.20471002  0.03980648  0.50808841]\n [-0.42999542  0.73539722  0.42788389]\n [ 0.11469982  0.08212417 -0.01973154]\n [ 0.16029838  0.59230292  0.84724814]]\n\nI looked around for other people experiencing problems with adding ops to the UPDATE_OPS collection inside a tf.cond and couldn't find anything. The fact that it completely halts training despite not being a required node for the train_op and without any errors means that you could silently break your whole training with just those few lines.", "body": "I figured out the missing ingredient to reproduce the bug: you need to put your `opt.minimize` inside a `with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS))`\r\n\r\nTurns out this is a rather dangerous bug: Adding an operation to a collection inside a function used in a `tf.cond` **silently** prevents any operations that use that collection as a control dependency, regardless of whether that `tf.cond` is ever used.\r\n\r\nSo, it's not just the global_step; it's everything! Here's the minimal code I put together to highlight the bug in action.\r\n\r\n``` \r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nflag = tf.placeholder(tf.bool, [])\r\nglobal_step = tf.train.get_or_create_global_step()\r\n\r\n# Include breaking code, putting a no_op inside a cond,\r\n# which isn't even used anywhere, and shouldn't impact anything else\r\ndef myfunc():\r\n  tf.add_to_collection(tf.GraphKeys.UPDATE_OPS, tf.no_op())\r\n  return 1\r\ntf.cond(flag, myfunc, lambda: 1)\r\n\r\n# Make our ad hoc model\r\na = tf.placeholder(tf.float32, [None, 5])\r\nb = tf.placeholder(tf.float32, [None, 3])\r\na_out = tf.layers.dense(a, 3)\r\ngraph = tf.get_default_graph()\r\nbias = graph.get_tensor_by_name('dense/bias:0')\r\nweights = graph.get_tensor_by_name('dense/kernel:0')\r\n\r\n# Define our train_op\r\nloss = tf.losses.mean_squared_error(b, a_out)\r\nopt = tf.train.GradientDescentOptimizer(learning_rate=1e-2)\r\nwith tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\r\n  train_op = opt.minimize(loss, global_step)\r\n\r\n# Test everything\r\nwith tf.Session() as sess:\r\n  sess.run(tf.global_variables_initializer())\r\n\r\n  print('BEFORE ANYTHING IS RUN')\r\n  print(sess.run(global_step))\r\n  print(sess.run(bias))\r\n  print(sess.run(weights))\r\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]], flag: False })\r\n  print('AFTER FIRST RUN:  we do not follow the path with the update op added')\r\n  print(sess.run(global_step))\r\n  print(sess.run(bias))\r\n  print(sess.run(weights))\r\n  sess.run(train_op, { a: [[0.1, 0.2, 0.3, 0.4, 0.5]], b: [[1., 2., 3.]], flag: True })\r\n  print('AFTER SECOND RUN: we follow the path with the update op added')\r\n  print(sess.run(global_step))\r\n  print(sess.run(bias))\r\n  print(sess.run(weights))\r\n```\r\n\r\nRunning the above code gives\r\n\r\n```\r\nBEFORE ANYTHING IS RUN\r\n0\r\n[ 0.  0.  0.]\r\n[[-0.37039062 -0.37549886  0.73892838]\r\n [-0.20615649  0.03782606  0.50503689]\r\n [-0.43216512  0.73242658  0.42330664]\r\n [ 0.11180687  0.07816333 -0.02583456]\r\n [ 0.15668219  0.58735186  0.83961934]]\r\nAFTER FIRST RUN:  we do not follow the path with the update op added\r\n0\r\n[ 0.  0.  0.]\r\n[[-0.37039062 -0.37549886  0.73892838]\r\n [-0.20615649  0.03782606  0.50503689]\r\n [-0.43216512  0.73242658  0.42330664]\r\n [ 0.11180687  0.07816333 -0.02583456]\r\n [ 0.15668219  0.58735186  0.83961934]]\r\nAFTER SECOND RUN: we follow the path with the update op added\r\n1\r\n[ 0.00723237  0.0099021   0.01525755]\r\n[[-0.36966738 -0.37450865  0.74045414]\r\n [-0.20471002  0.03980648  0.50808841]\r\n [-0.42999542  0.73539722  0.42788389]\r\n [ 0.11469982  0.08212417 -0.01973154]\r\n [ 0.16029838  0.59230292  0.84724814]]\r\n```\r\n\r\nI looked around for other people experiencing problems with adding ops to the UPDATE_OPS collection inside a `tf.cond` and couldn't find anything. The fact that it completely halts training despite not being a required node for the `train_op` and without any errors means that you could silently break your whole training with just those few lines."}