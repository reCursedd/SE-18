{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/419461999", "html_url": "https://github.com/tensorflow/tensorflow/issues/7065#issuecomment-419461999", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7065", "id": 419461999, "node_id": "MDEyOklzc3VlQ29tbWVudDQxOTQ2MTk5OQ==", "user": {"login": "SeguinBe", "id": 7132817, "node_id": "MDQ6VXNlcjcxMzI4MTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/7132817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SeguinBe", "html_url": "https://github.com/SeguinBe", "followers_url": "https://api.github.com/users/SeguinBe/followers", "following_url": "https://api.github.com/users/SeguinBe/following{/other_user}", "gists_url": "https://api.github.com/users/SeguinBe/gists{/gist_id}", "starred_url": "https://api.github.com/users/SeguinBe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SeguinBe/subscriptions", "organizations_url": "https://api.github.com/users/SeguinBe/orgs", "repos_url": "https://api.github.com/users/SeguinBe/repos", "events_url": "https://api.github.com/users/SeguinBe/events{/privacy}", "received_events_url": "https://api.github.com/users/SeguinBe/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-07T14:44:19Z", "updated_at": "2018-09-07T14:51:18Z", "author_association": "NONE", "body_html": "<p>So I'm coming back to some old issues and tried again this one:</p>\n<h2>Setup</h2>\n<pre><code>conda create -n deepl python=3.6\nconda activate deepl\nconda install tensorflow-gpu=1.9 jupyter\nconda install pytorch torchvision -c pytorch\n    \nwget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\ntar -xvf vgg_16_2016_08_28.tar.gz\n</code></pre>\n<p>Ubuntu 18.04 + Titan X Maxwell<br>\nTF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)</p>\n<h2>Commands</h2>\n<h4>Tensorflow</h4>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n<span class=\"pl-c\"><span class=\"pl-c\">#</span># Does not change anything</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> slim\n<span class=\"pl-k\">from</span> tensorflow.contrib.slim <span class=\"pl-k\">import</span> nets\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\ntf.reset_default_graph()\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">False</span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 'NHWC' format</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Use RNG to avoid the feed_dict argument</span>\n    input_images <span class=\"pl-k\">=</span> tf.constant(np.random.randn(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">3</span>).astype(np.float32))\n    net <span class=\"pl-k\">=</span> nets.vgg.vgg_16(input_images, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">spatial_squeeze</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    conv_layer <span class=\"pl-k\">=</span> net[<span class=\"pl-c1\">1</span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>vgg_16/pool5<span class=\"pl-pds\">'</span></span>]\n    preds <span class=\"pl-k\">=</span> net[<span class=\"pl-c1\">0</span>]\n<span class=\"pl-k\">else</span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 'NCHW' format</span>\n    input_images <span class=\"pl-k\">=</span> tf.constant(np.random.randn(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>).astype(np.float32))\n    <span class=\"pl-k\">with</span> slim.arg_scope([slim.conv2d, slim.max_pool2d], <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>NCHW<span class=\"pl-pds\">'</span></span>):\n        net <span class=\"pl-k\">=</span> nets.vgg.vgg_16(input_images, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">spatial_squeeze</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        conv_layer <span class=\"pl-k\">=</span> net[<span class=\"pl-c1\">1</span>][<span class=\"pl-s\"><span class=\"pl-pds\">'</span>vgg_16/pool5<span class=\"pl-pds\">'</span></span>]\n        preds <span class=\"pl-k\">=</span> net[<span class=\"pl-c1\">0</span>]\nsaver <span class=\"pl-k\">=</span> tf.train.Saver()\n\nconfig <span class=\"pl-k\">=</span> tf.ConfigProto(<span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config)\nsaver.restore(sess, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>./vgg_16.ckpt<span class=\"pl-pds\">'</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> With jupyter notebook magic</span>\n<span class=\"pl-k\">%</span>timeit sess.run(conv_layer)\n<span class=\"pl-k\">%</span>timeit sess.run(preds)</pre></div>\n<h4>pyTorch</h4>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> torch\n<span class=\"pl-k\">import</span> torchvision.models <span class=\"pl-k\">as</span> models\ntorch.backends.cudnn.benchmark <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\nnet <span class=\"pl-k\">=</span> models.vgg16()\nnet.cuda()\n\n_in <span class=\"pl-k\">=</span> torch.from_numpy(np.random.randn(<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">224</span>, <span class=\"pl-c1\">224</span>).astype(np.float32)).cuda()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> With jupyter notebook magic</span>\n<span class=\"pl-k\">%</span>timeit net.features(_in).data.cpu().numpy()\n<span class=\"pl-k\">%</span>timeit net(_in).data.cpu().numpy()</pre></div>\n<h2>Results</h2>\n<table>\n<thead>\n<tr>\n<th>Framework</th>\n<th>TF-NHWC</th>\n<th>TF-NCHW</th>\n<th>pyTorch</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>pool5 output</td>\n<td>72.5</td>\n<td>60.1</td>\n<td>59.1</td>\n</tr>\n<tr>\n<td>fc8 output</td>\n<td>73.6</td>\n<td>131.0</td>\n<td>60.8</td>\n</tr>\n</tbody>\n</table>\n<h3>Conclusion</h3>\n<p>Performance is the same as long as the same data format is used.</p>\n<p>However, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues.</p>", "body_text": "So I'm coming back to some old issues and tried again this one:\nSetup\nconda create -n deepl python=3.6\nconda activate deepl\nconda install tensorflow-gpu=1.9 jupyter\nconda install pytorch torchvision -c pytorch\n    \nwget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\ntar -xvf vgg_16_2016_08_28.tar.gz\n\nUbuntu 18.04 + Titan X Maxwell\nTF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)\nCommands\nTensorflow\nimport os\n## Does not change anything\n#os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\nimport tensorflow as tf\nfrom tensorflow.contrib import slim\nfrom tensorflow.contrib.slim import nets\nimport numpy as np\n\ntf.reset_default_graph()\nif False:  # 'NHWC' format\n    # Use RNG to avoid the feed_dict argument\n    input_images = tf.constant(np.random.randn(16, 224, 224, 3).astype(np.float32))\n    net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\n    conv_layer = net[1]['vgg_16/pool5']\n    preds = net[0]\nelse:  # 'NCHW' format\n    input_images = tf.constant(np.random.randn(16, 3, 224, 224).astype(np.float32))\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):\n        net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\n        conv_layer = net[1]['vgg_16/pool5']\n        preds = net[0]\nsaver = tf.train.Saver()\n\nconfig = tf.ConfigProto(log_device_placement=True)\nsess = tf.InteractiveSession(config=config)\nsaver.restore(sess, './vgg_16.ckpt')\n\n# With jupyter notebook magic\n%timeit sess.run(conv_layer)\n%timeit sess.run(preds)\npyTorch\nimport numpy as np\nimport torch\nimport torchvision.models as models\ntorch.backends.cudnn.benchmark = True\n\nnet = models.vgg16()\nnet.cuda()\n\n_in = torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda()\n\n# With jupyter notebook magic\n%timeit net.features(_in).data.cpu().numpy()\n%timeit net(_in).data.cpu().numpy()\nResults\n\n\n\nFramework\nTF-NHWC\nTF-NCHW\npyTorch\n\n\n\n\npool5 output\n72.5\n60.1\n59.1\n\n\nfc8 output\n73.6\n131.0\n60.8\n\n\n\nConclusion\nPerformance is the same as long as the same data format is used.\nHowever, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues.", "body": "So I'm coming back to some old issues and tried again this one:\r\n## Setup\r\n```\r\nconda create -n deepl python=3.6\r\nconda activate deepl\r\nconda install tensorflow-gpu=1.9 jupyter\r\nconda install pytorch torchvision -c pytorch\r\n    \r\nwget http://download.tensorflow.org/models/vgg_16_2016_08_28.tar.gz\r\ntar -xvf vgg_16_2016_08_28.tar.gz\r\n```\r\nUbuntu 18.04 + Titan X Maxwell\r\nTF 1.9, Cuda 9.0, cuDNN 7.1.2 (have to update the drivers to go to 1.10, 9.2, and 7.2)\r\n\r\n## Commands\r\n#### Tensorflow\r\n```python\r\nimport os\r\n## Does not change anything\r\n#os.environ['TF_ENABLE_WINOGRAD_NONFUSED'] = '1'\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import slim\r\nfrom tensorflow.contrib.slim import nets\r\nimport numpy as np\r\n\r\ntf.reset_default_graph()\r\nif False:  # 'NHWC' format\r\n    # Use RNG to avoid the feed_dict argument\r\n    input_images = tf.constant(np.random.randn(16, 224, 224, 3).astype(np.float32))\r\n    net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\r\n    conv_layer = net[1]['vgg_16/pool5']\r\n    preds = net[0]\r\nelse:  # 'NCHW' format\r\n    input_images = tf.constant(np.random.randn(16, 3, 224, 224).astype(np.float32))\r\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d], data_format='NCHW'):\r\n        net = nets.vgg.vgg_16(input_images, is_training=False, spatial_squeeze=False)\r\n        conv_layer = net[1]['vgg_16/pool5']\r\n        preds = net[0]\r\nsaver = tf.train.Saver()\r\n\r\nconfig = tf.ConfigProto(log_device_placement=True)\r\nsess = tf.InteractiveSession(config=config)\r\nsaver.restore(sess, './vgg_16.ckpt')\r\n\r\n# With jupyter notebook magic\r\n%timeit sess.run(conv_layer)\r\n%timeit sess.run(preds)\r\n``` \r\n#### pyTorch\r\n```python\r\nimport numpy as np\r\nimport torch\r\nimport torchvision.models as models\r\ntorch.backends.cudnn.benchmark = True\r\n\r\nnet = models.vgg16()\r\nnet.cuda()\r\n\r\n_in = torch.from_numpy(np.random.randn(16, 3, 224, 224).astype(np.float32)).cuda()\r\n\r\n# With jupyter notebook magic\r\n%timeit net.features(_in).data.cpu().numpy()\r\n%timeit net(_in).data.cpu().numpy()\r\n```\r\n\r\n## Results\r\n\r\n| Framework    | TF-NHWC | TF-NCHW | pyTorch |\r\n|--------------|---------|---------|---------|\r\n| pool5 output | 72.5    | 60.1    | 59.1    |\r\n| fc8 output   | 73.6    | 131.0   | 60.8    |\r\n\r\n### Conclusion\r\n\r\nPerformance is the same as long as the same data format is used.\r\n\r\nHowever, as we stated before the TF version is implementing the FC layers as convolutions instead of actual FC layers. For the NHWC case, an optimization makes this difference transparent but it is not the case for the NCHW layout, which explains the difference here. However, this is more about the networks being defined differently than actual performance issues."}