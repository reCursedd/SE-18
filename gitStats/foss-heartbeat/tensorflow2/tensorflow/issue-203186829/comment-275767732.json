{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/275767732", "html_url": "https://github.com/tensorflow/tensorflow/issues/7065#issuecomment-275767732", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7065", "id": 275767732, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NTc2NzczMg==", "user": {"login": "gpapan", "id": 6232317, "node_id": "MDQ6VXNlcjYyMzIzMTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/6232317?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gpapan", "html_url": "https://github.com/gpapan", "followers_url": "https://api.github.com/users/gpapan/followers", "following_url": "https://api.github.com/users/gpapan/following{/other_user}", "gists_url": "https://api.github.com/users/gpapan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gpapan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gpapan/subscriptions", "organizations_url": "https://api.github.com/users/gpapan/orgs", "repos_url": "https://api.github.com/users/gpapan/repos", "events_url": "https://api.github.com/users/gpapan/events{/privacy}", "received_events_url": "https://api.github.com/users/gpapan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-27T20:35:03Z", "updated_at": "2017-01-27T20:35:03Z", "author_association": "NONE", "body_html": "<p>Looped in by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1766524\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sguada\">@sguada</a>.</p>\n<p>The discrepancy between using convolutional vs. fully connected layers should be fixed.</p>\n<p>The convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L439\">see here</a>).<br>\nUsing <code>conv2d</code> vs. <code>fully_connected</code> should not make a difference for 'fc7' or 'fc8' -- <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7132817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/SeguinBe\">@SeguinBe</a> can you verify that please?</p>\n<p>However, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when<br>\nconvolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC</p>", "body_text": "Looped in by @sguada.\nThe discrepancy between using convolutional vs. fully connected layers should be fixed.\nThe convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format (see here).\nUsing conv2d vs. fully_connected should not make a difference for 'fc7' or 'fc8' -- @SeguinBe can you verify that please?\nHowever, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when\nconvolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC", "body": "Looped in by @sguada.\r\n\r\nThe discrepancy between using convolutional vs. fully connected layers should be fixed.\r\n\r\nThe convolution kernel correctly calls CuBlas gemm for 1x1 convolutions and NHWC format ([see here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/conv_ops.cc#L439)).\r\nUsing `conv2d` vs. `fully_connected` should not make a difference for 'fc7' or 'fc8' -- @SeguinBe can you verify that please?\r\n\r\nHowever, the convolution kernel does not currently call CuBlas gemm for the 7x7 convolution in 'fc5'. I can add one more branch and also call gemm when\r\nconvolution_type is 'VALID' and kernel_height==height and kernel_width==width and format==NHWC"}