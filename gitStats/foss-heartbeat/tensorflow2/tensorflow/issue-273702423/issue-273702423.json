{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14535", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14535/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14535/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14535/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14535", "id": 273702423, "node_id": "MDU6SXNzdWUyNzM3MDI0MjM=", "number": 14535, "title": "Can not import transformed and quantized model using tf.import_graph_def", "user": {"login": "phongnhhn92", "id": 11288381, "node_id": "MDQ6VXNlcjExMjg4Mzgx", "avatar_url": "https://avatars3.githubusercontent.com/u/11288381?v=4", "gravatar_id": "", "url": "https://api.github.com/users/phongnhhn92", "html_url": "https://github.com/phongnhhn92", "followers_url": "https://api.github.com/users/phongnhhn92/followers", "following_url": "https://api.github.com/users/phongnhhn92/following{/other_user}", "gists_url": "https://api.github.com/users/phongnhhn92/gists{/gist_id}", "starred_url": "https://api.github.com/users/phongnhhn92/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/phongnhhn92/subscriptions", "organizations_url": "https://api.github.com/users/phongnhhn92/orgs", "repos_url": "https://api.github.com/users/phongnhhn92/repos", "events_url": "https://api.github.com/users/phongnhhn92/events{/privacy}", "received_events_url": "https://api.github.com/users/phongnhhn92/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 18, "created_at": "2017-11-14T08:16:34Z", "updated_at": "2018-06-26T01:03:28Z", "closed_at": "2018-06-26T01:03:28Z", "author_association": "NONE", "body_html": "<p>Hello guys,<br>\nI am trying to quantized the pretrain SSD_mobilenet_v1_coco from the Tensorflow Object Detection API using the following command:<br>\n<code>bazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=MobileNetSSD.pb \\ --out_graph=optimized_SSD.pb \\ --inputs='image_tensor' \\ --outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\ --transforms=' add_default_attributes strip_unused_nodes(type=uint8, shape=\"1,300,300,3\")   fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'</code></p>\n<p>The command run successfully and I had the optimized_SSD.pb with only 6MB. However, when I try to use this pb model in the provided IPython Notebook file <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb\">title</a> I received this error:<br>\n<code>Traceback (most recent call last): File \"/home/phong/PycharmProjects/ConvertDarknet2VOC/evaluation.py\", line 52, in &lt;module&gt; tf.import_graph_def(od_graph_def, name='') File \"/home/phong/.virtualenvs/tensorflow_pycharm/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 283, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named QuantizedResizeBilinear in defined operations.</code></p>\n<p>Is there any way to load the quantized model using tensorflow ?</p>", "body_text": "Hello guys,\nI am trying to quantized the pretrain SSD_mobilenet_v1_coco from the Tensorflow Object Detection API using the following command:\nbazel build tensorflow/tools/graph_transforms:transform_graph bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\ --in_graph=MobileNetSSD.pb \\ --out_graph=optimized_SSD.pb \\ --inputs='image_tensor' \\ --outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\ --transforms=' add_default_attributes strip_unused_nodes(type=uint8, shape=\"1,300,300,3\")   fold_constants(ignore_errors=true) fold_batch_norms fold_old_batch_norms quantize_weights quantize_nodes strip_unused_nodes sort_by_execution_order'\nThe command run successfully and I had the optimized_SSD.pb with only 6MB. However, when I try to use this pb model in the provided IPython Notebook file title I received this error:\nTraceback (most recent call last): File \"/home/phong/PycharmProjects/ConvertDarknet2VOC/evaluation.py\", line 52, in <module> tf.import_graph_def(od_graph_def, name='') File \"/home/phong/.virtualenvs/tensorflow_pycharm/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 283, in import_graph_def raise ValueError('No op named %s in defined operations.' % node.op) ValueError: No op named QuantizedResizeBilinear in defined operations.\nIs there any way to load the quantized model using tensorflow ?", "body": "Hello guys,\r\nI am trying to quantized the pretrain SSD_mobilenet_v1_coco from the Tensorflow Object Detection API using the following command:\r\n`bazel build tensorflow/tools/graph_transforms:transform_graph\r\nbazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=MobileNetSSD.pb \\\r\n--out_graph=optimized_SSD.pb \\\r\n--inputs='image_tensor' \\\r\n--outputs='detection_boxes,detection_scores,detection_classes,num_detections' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=uint8, shape=\"1,300,300,3\")  \r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'`\r\n\r\nThe command run successfully and I had the optimized_SSD.pb with only 6MB. However, when I try to use this pb model in the provided IPython Notebook file [title](https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb) I received this error:\r\n`Traceback (most recent call last):\r\n  File \"/home/phong/PycharmProjects/ConvertDarknet2VOC/evaluation.py\", line 52, in <module>\r\n    tf.import_graph_def(od_graph_def, name='')\r\n  File \"/home/phong/.virtualenvs/tensorflow_pycharm/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 283, in import_graph_def\r\n    raise ValueError('No op named %s in defined operations.' % node.op)\r\nValueError: No op named QuantizedResizeBilinear in defined operations.`\r\n\r\nIs there any way to load the quantized model using tensorflow ? \r\n\r\n\r\n"}