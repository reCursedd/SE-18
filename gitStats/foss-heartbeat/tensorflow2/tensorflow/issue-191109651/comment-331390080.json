{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331390080", "html_url": "https://github.com/tensorflow/tensorflow/issues/5789#issuecomment-331390080", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5789", "id": 331390080, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTM5MDA4MA==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-22T08:51:16Z", "updated_at": "2017-09-22T09:56:10Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> Even with the latest P100 and V100, the problem still remains. If you take a look of the DGX-1 system bus topology (hypercube alike), it is unlikely that all-to-all peering could be achieved with uniform latency/bandwidth provision.</p>\n<p>NCCL claims to handle this case optimally, but when I tested vgg16 on my local box with 4 K40m GPUs in a PCI-passthrough VM:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Parameters</th>\n<th align=\"center\">Bare Metal</th>\n<th align=\"center\">VM</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\"><code>variable_update=independent</code></td>\n<td align=\"center\">150 img/sec</td>\n<td align=\"center\">150 img/sec</td>\n</tr>\n<tr>\n<td align=\"center\"><code>variable_update=parameter_server</code> <br> <code>local_parameter_device=cpu</code></td>\n<td align=\"center\">150 img/sec</td>\n<td align=\"center\">150 img/sec</td>\n</tr>\n<tr>\n<td align=\"center\"><code>variable_update=replicated</code> <br> <code>local_parameter_device=cpu</code> <br> <code>use_nccl=False</code></td>\n<td align=\"center\">146 img/sec</td>\n<td align=\"center\">146 img/sec</td>\n</tr>\n<tr>\n<td align=\"center\"><code>variable_update=parameter_server</code> <br> <code>local_parameter_device=gpu</code></td>\n<td align=\"center\">130 img/sec (OOM)</td>\n<td align=\"center\">130 img/sec (OOM)</td>\n</tr>\n<tr>\n<td align=\"center\"><code>variable_update=replicated</code><br> <code>local_parameter_device=gpu</code> <br> <code>use_nccl=False</code></td>\n<td align=\"center\">122 img/sec (OOM)</td>\n<td align=\"center\">119 img/sec (OOM)</td>\n</tr>\n<tr>\n<td align=\"center\"><code>variable_update=replicated</code><br> <code>local_parameter_device=cpu</code> <br> <code>use_nccl=True</code></td>\n<td align=\"center\">94 img/sec</td>\n<td align=\"center\">107 img/sec</td>\n</tr>\n<tr>\n<td align=\"center\"><code>variable_update=replicated</code><br> <code>local_parameter_device=gpu</code> <br> <code>use_nccl=True</code></td>\n<td align=\"center\">94 img/sec</td>\n<td align=\"center\">98 img/sec</td>\n</tr>\n</tbody>\n</table>", "body_text": "@tfboyd Even with the latest P100 and V100, the problem still remains. If you take a look of the DGX-1 system bus topology (hypercube alike), it is unlikely that all-to-all peering could be achieved with uniform latency/bandwidth provision.\nNCCL claims to handle this case optimally, but when I tested vgg16 on my local box with 4 K40m GPUs in a PCI-passthrough VM:\n\n\n\nParameters\nBare Metal\nVM\n\n\n\n\nvariable_update=independent\n150 img/sec\n150 img/sec\n\n\nvariable_update=parameter_server  local_parameter_device=cpu\n150 img/sec\n150 img/sec\n\n\nvariable_update=replicated  local_parameter_device=cpu  use_nccl=False\n146 img/sec\n146 img/sec\n\n\nvariable_update=parameter_server  local_parameter_device=gpu\n130 img/sec (OOM)\n130 img/sec (OOM)\n\n\nvariable_update=replicated local_parameter_device=gpu  use_nccl=False\n122 img/sec (OOM)\n119 img/sec (OOM)\n\n\nvariable_update=replicated local_parameter_device=cpu  use_nccl=True\n94 img/sec\n107 img/sec\n\n\nvariable_update=replicated local_parameter_device=gpu  use_nccl=True\n94 img/sec\n98 img/sec", "body": "@tfboyd Even with the latest P100 and V100, the problem still remains. If you take a look of the DGX-1 system bus topology (hypercube alike), it is unlikely that all-to-all peering could be achieved with uniform latency/bandwidth provision. \r\n\r\nNCCL claims to handle this case optimally, but when I tested vgg16 on my local box with 4 K40m GPUs in a PCI-passthrough VM:\r\n\r\n|Parameters|Bare Metal|VM|\r\n|:-:|:-:|:-:|\r\n|`variable_update=independent`| 150 img/sec | 150 img/sec |\r\n|`variable_update=parameter_server` <br> `local_parameter_device=cpu` | 150 img/sec | 150 img/sec |\r\n| `variable_update=replicated` <br> `local_parameter_device=cpu` <br> `use_nccl=False` | 146 img/sec | 146 img/sec |\r\n| `variable_update=parameter_server` <br> `local_parameter_device=gpu` | 130 img/sec (OOM) | 130 img/sec (OOM) |\r\n| `variable_update=replicated`<br> `local_parameter_device=gpu` <br> `use_nccl=False` | 122 img/sec (OOM) | 119 img/sec (OOM) |\r\n| `variable_update=replicated`<br> `local_parameter_device=cpu` <br> `use_nccl=True` | 94 img/sec | 107 img/sec |\r\n| `variable_update=replicated`<br> `local_parameter_device=gpu` <br> `use_nccl=True` | 94 img/sec | 98 img/sec |"}