{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331490515", "html_url": "https://github.com/tensorflow/tensorflow/issues/5789#issuecomment-331490515", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5789", "id": 331490515, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTQ5MDUxNQ==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-22T16:09:14Z", "updated_at": "2017-09-22T16:09:32Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2613663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/byronyi\">@byronyi</a><br>\nHere are some numbers from AWS</p>\n<p>On K80 and I would assume this is also true for K40 even if peering is turned on nccl is often slower due to the sync calls ending up being other work in the thread.  But even on a DGX-1 the best (although this is changing) approach was to put the shared parameters on the CPU for resnet and inception but for VGG16 which has a lot more parameters replicated NCCL was optimal.</p>\n<p>You can set an NVIDIA env variable:  <code>CUDA_DEVICE_MAX_CONNECTIONS=12</code>.  I believe the default for K80 and K40 is 8.  For me this improved my VGG16 batch-size:32 per GPU time on p2.8xlarge to 266 images/sec with real data and 277 for synthetic (not that that matters for anything).  This also made replicated NCCL a viable option.  For resnet and inception it made replicated NCCL better and viable but still not as good as the other options.   Previously my best for VGG16 on AWS was ~242 images/sec with real data.  So a pretty good gain.  Not sure if it will help on K40s.</p>\n<p>If I find time I will link all of my results so someone can get value out of them.  It is really hard for me to make a simple Google Sheet public from my google employee account.</p>\n<p>I am about to test an all reduce solution that should work in distributed mode on AWS that I am told may make VGG16 scale.  I think I remember seeing VGG scale on MPI but not on a normal network.  Also just because I have not seen it does not mean it has not happened but I am excited to try it myself.</p>\n<p>I was aware of the DGX-1 topology and I have seen P100s also setup with a ring topology.  I am far from an expert but with a movement toward all reduce, I do not think there is a need for direct 1:1 communication.  The aggregation is going to go in a ring.</p>", "body_text": "@byronyi\nHere are some numbers from AWS\nOn K80 and I would assume this is also true for K40 even if peering is turned on nccl is often slower due to the sync calls ending up being other work in the thread.  But even on a DGX-1 the best (although this is changing) approach was to put the shared parameters on the CPU for resnet and inception but for VGG16 which has a lot more parameters replicated NCCL was optimal.\nYou can set an NVIDIA env variable:  CUDA_DEVICE_MAX_CONNECTIONS=12.  I believe the default for K80 and K40 is 8.  For me this improved my VGG16 batch-size:32 per GPU time on p2.8xlarge to 266 images/sec with real data and 277 for synthetic (not that that matters for anything).  This also made replicated NCCL a viable option.  For resnet and inception it made replicated NCCL better and viable but still not as good as the other options.   Previously my best for VGG16 on AWS was ~242 images/sec with real data.  So a pretty good gain.  Not sure if it will help on K40s.\nIf I find time I will link all of my results so someone can get value out of them.  It is really hard for me to make a simple Google Sheet public from my google employee account.\nI am about to test an all reduce solution that should work in distributed mode on AWS that I am told may make VGG16 scale.  I think I remember seeing VGG scale on MPI but not on a normal network.  Also just because I have not seen it does not mean it has not happened but I am excited to try it myself.\nI was aware of the DGX-1 topology and I have seen P100s also setup with a ring topology.  I am far from an expert but with a movement toward all reduce, I do not think there is a need for direct 1:1 communication.  The aggregation is going to go in a ring.", "body": "@byronyi \r\nHere are some numbers from AWS\r\n\r\nOn K80 and I would assume this is also true for K40 even if peering is turned on nccl is often slower due to the sync calls ending up being other work in the thread.  But even on a DGX-1 the best (although this is changing) approach was to put the shared parameters on the CPU for resnet and inception but for VGG16 which has a lot more parameters replicated NCCL was optimal.  \r\n\r\nYou can set an NVIDIA env variable:  ```CUDA_DEVICE_MAX_CONNECTIONS=12```.  I believe the default for K80 and K40 is 8.  For me this improved my VGG16 batch-size:32 per GPU time on p2.8xlarge to 266 images/sec with real data and 277 for synthetic (not that that matters for anything).  This also made replicated NCCL a viable option.  For resnet and inception it made replicated NCCL better and viable but still not as good as the other options.   Previously my best for VGG16 on AWS was ~242 images/sec with real data.  So a pretty good gain.  Not sure if it will help on K40s.  \r\n\r\nIf I find time I will link all of my results so someone can get value out of them.  It is really hard for me to make a simple Google Sheet public from my google employee account.  \r\n\r\nI am about to test an all reduce solution that should work in distributed mode on AWS that I am told may make VGG16 scale.  I think I remember seeing VGG scale on MPI but not on a normal network.  Also just because I have not seen it does not mean it has not happened but I am excited to try it myself.\r\n\r\nI was aware of the DGX-1 topology and I have seen P100s also setup with a ring topology.  I am far from an expert but with a movement toward all reduce, I do not think there is a need for direct 1:1 communication.  The aggregation is going to go in a ring."}