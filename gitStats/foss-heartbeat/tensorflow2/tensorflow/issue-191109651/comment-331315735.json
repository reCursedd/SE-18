{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331315735", "html_url": "https://github.com/tensorflow/tensorflow/issues/5789#issuecomment-331315735", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5789", "id": 331315735, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTMxNTczNQ==", "user": {"login": "byronyi", "id": 2613663, "node_id": "MDQ6VXNlcjI2MTM2NjM=", "avatar_url": "https://avatars2.githubusercontent.com/u/2613663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/byronyi", "html_url": "https://github.com/byronyi", "followers_url": "https://api.github.com/users/byronyi/followers", "following_url": "https://api.github.com/users/byronyi/following{/other_user}", "gists_url": "https://api.github.com/users/byronyi/gists{/gist_id}", "starred_url": "https://api.github.com/users/byronyi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/byronyi/subscriptions", "organizations_url": "https://api.github.com/users/byronyi/orgs", "repos_url": "https://api.github.com/users/byronyi/repos", "events_url": "https://api.github.com/users/byronyi/events{/privacy}", "received_events_url": "https://api.github.com/users/byronyi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-22T00:15:42Z", "updated_at": "2017-09-22T00:15:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> Thanks for the heads up!</p>\n<p>I'm actually quite interested in the 16 GPUs setup on AWS. All to all peering seems to be unlikely as those 16 GPUs are not connected directly through a single PCIe switch (according to Mu Li's PhD thesis).</p>\n<p>Any reason why we don't have single node 16 GPUs benchmark result included in the TF CNN benchmarks page?</p>", "body_text": "@tfboyd Thanks for the heads up!\nI'm actually quite interested in the 16 GPUs setup on AWS. All to all peering seems to be unlikely as those 16 GPUs are not connected directly through a single PCIe switch (according to Mu Li's PhD thesis).\nAny reason why we don't have single node 16 GPUs benchmark result included in the TF CNN benchmarks page?", "body": "@tfboyd Thanks for the heads up! \n\nI'm actually quite interested in the 16 GPUs setup on AWS. All to all peering seems to be unlikely as those 16 GPUs are not connected directly through a single PCIe switch (according to Mu Li's PhD thesis). \n\nAny reason why we don't have single node 16 GPUs benchmark result included in the TF CNN benchmarks page?"}