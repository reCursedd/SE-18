{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/431281279", "html_url": "https://github.com/tensorflow/tensorflow/issues/22769#issuecomment-431281279", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22769", "id": 431281279, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMTI4MTI3OQ==", "user": {"login": "lenassero", "id": 21358816, "node_id": "MDQ6VXNlcjIxMzU4ODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/21358816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lenassero", "html_url": "https://github.com/lenassero", "followers_url": "https://api.github.com/users/lenassero/followers", "following_url": "https://api.github.com/users/lenassero/following{/other_user}", "gists_url": "https://api.github.com/users/lenassero/gists{/gist_id}", "starred_url": "https://api.github.com/users/lenassero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lenassero/subscriptions", "organizations_url": "https://api.github.com/users/lenassero/orgs", "repos_url": "https://api.github.com/users/lenassero/repos", "events_url": "https://api.github.com/users/lenassero/events{/privacy}", "received_events_url": "https://api.github.com/users/lenassero/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-19T08:08:55Z", "updated_at": "2018-10-19T08:08:55Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42785337\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wt-huang\">@wt-huang</a>, thank you for your response!</p>\n<p>Well, the <code>alignment_history</code> does change the <code>decoder_cell</code> used by the decoder but it should not impact the weights to my knowledge. One can find in the <code>call</code> method of <code>AttentionWrapper</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">...</span>\n    next_state <span class=\"pl-k\">=</span> AttentionWrapperState(\n        <span class=\"pl-v\">time</span><span class=\"pl-k\">=</span>state.time <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>,\n        <span class=\"pl-v\">cell_state</span><span class=\"pl-k\">=</span>next_cell_state,\n        <span class=\"pl-v\">attention</span><span class=\"pl-k\">=</span>attention,\n        <span class=\"pl-v\">attention_state</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._item_or_tuple(all_attention_states),\n        <span class=\"pl-v\">alignments</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._item_or_tuple(all_alignments),\n        <span class=\"pl-v\">alignment_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._item_or_tuple(maybe_all_histories))\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._output_attention:\n      <span class=\"pl-k\">return</span> attention, next_state\n    <span class=\"pl-k\">else</span>:\n      <span class=\"pl-k\">return</span> cell_output, next_state</pre></div>\n<p>Basically, <code>alignment_history</code> in the named tuple <code>next_state</code> is either empty (<code>alignment_history = False</code>) or it stores the history of alignments (<code>alignment_history = True</code>). To me, this should not change the way the graph is initialized, and thus the weights at initialization as I mentioned..</p>", "body_text": "Hi @wt-huang, thank you for your response!\nWell, the alignment_history does change the decoder_cell used by the decoder but it should not impact the weights to my knowledge. One can find in the call method of AttentionWrapper:\n...\n    next_state = AttentionWrapperState(\n        time=state.time + 1,\n        cell_state=next_cell_state,\n        attention=attention,\n        attention_state=self._item_or_tuple(all_attention_states),\n        alignments=self._item_or_tuple(all_alignments),\n        alignment_history=self._item_or_tuple(maybe_all_histories))\n\n    if self._output_attention:\n      return attention, next_state\n    else:\n      return cell_output, next_state\nBasically, alignment_history in the named tuple next_state is either empty (alignment_history = False) or it stores the history of alignments (alignment_history = True). To me, this should not change the way the graph is initialized, and thus the weights at initialization as I mentioned..", "body": "Hi @wt-huang, thank you for your response! \r\n\r\nWell, the `alignment_history` does change the `decoder_cell` used by the decoder but it should not impact the weights to my knowledge. One can find in the `call` method of `AttentionWrapper`:\r\n\r\n```python\r\n...\r\n    next_state = AttentionWrapperState(\r\n        time=state.time + 1,\r\n        cell_state=next_cell_state,\r\n        attention=attention,\r\n        attention_state=self._item_or_tuple(all_attention_states),\r\n        alignments=self._item_or_tuple(all_alignments),\r\n        alignment_history=self._item_or_tuple(maybe_all_histories))\r\n\r\n    if self._output_attention:\r\n      return attention, next_state\r\n    else:\r\n      return cell_output, next_state\r\n```\r\n\r\nBasically, `alignment_history` in the named tuple `next_state` is either empty (`alignment_history = False`) or it stores the history of alignments (`alignment_history = True`). To me, this should not change the way the graph is initialized, and thus the weights at initialization as I mentioned.."}