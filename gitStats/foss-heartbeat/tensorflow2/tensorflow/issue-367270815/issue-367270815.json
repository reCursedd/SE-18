{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22769", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22769/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22769/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22769/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22769", "id": 367270815, "node_id": "MDU6SXNzdWUzNjcyNzA4MTU=", "number": 22769, "title": "Different weights initializations for different values of `alignment_history` in `tf.contrib.seq2seq.AttentionWrapper` (fixed graph random seed)", "user": {"login": "lenassero", "id": 21358816, "node_id": "MDQ6VXNlcjIxMzU4ODE2", "avatar_url": "https://avatars0.githubusercontent.com/u/21358816?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lenassero", "html_url": "https://github.com/lenassero", "followers_url": "https://api.github.com/users/lenassero/followers", "following_url": "https://api.github.com/users/lenassero/following{/other_user}", "gists_url": "https://api.github.com/users/lenassero/gists{/gist_id}", "starred_url": "https://api.github.com/users/lenassero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lenassero/subscriptions", "organizations_url": "https://api.github.com/users/lenassero/orgs", "repos_url": "https://api.github.com/users/lenassero/repos", "events_url": "https://api.github.com/users/lenassero/events{/privacy}", "received_events_url": "https://api.github.com/users/lenassero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097545817, "node_id": "MDU6TGFiZWwxMDk3NTQ1ODE3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:apis", "name": "comp:apis", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-10-05T15:54:18Z", "updated_at": "2018-10-25T23:44:02Z", "closed_at": "2018-10-25T23:44:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Mac OS X 10.13.6</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: -</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (wheel)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0</li>\n<li><strong>Python version</strong>: 2.7.15</li>\n<li><strong>Bazel version (if compiling from source)</strong>: -</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: tested on CPU</li>\n<li><strong>GPU model and memory</strong>: -</li>\n<li><strong>Exact command to reproduce</strong>: code below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>The initialization of the weights of the tensors in the decoder change when<br>\nchanging the <code>alignment_history</code> parameter in <code>tf.contrib.seq2seq.AttentionWrapper</code><br>\n(<code>True</code> or <code>False</code>). This occurs even if the graph's random seed is fixed<br>\n(<code>tf.set_random_seed(1)</code>).</p>\n<h3>Source code / logs</h3>\n<p>Below, <code>create_graph</code> creates the graph consisting of a decoder with Bahdanau<br>\nattention and <code>test_attention_wrapper</code> creates a session in order to check the<br>\nvalues of the weight matrices at initialization.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>! /usr/bin/env</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> encoding: utf-8</span>\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_graph</span>(<span class=\"pl-smi\">memory</span>,\n                 <span class=\"pl-smi\">memory_sequence_length</span>,\n                 <span class=\"pl-smi\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                 <span class=\"pl-smi\">vocab_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">30</span>,\n                 <span class=\"pl-smi\">num_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">16</span>,\n                 <span class=\"pl-smi\">attention_layer_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">16</span>,\n                 <span class=\"pl-smi\">attention_alignment_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                 <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>decoder<span class=\"pl-pds\">\"</span></span>):\n\n    <span class=\"pl-k\">with</span> tf.variable_scope(name) <span class=\"pl-k\">as</span> scope:\n        decoder_cell <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMCell(num_units, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>lstm<span class=\"pl-pds\">\"</span></span>)\n        attention_mechanism <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BahdanauAttention(\n            attention_layer_size,\n            memory,\n            memory_sequence_length)\n        decoder_cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n            decoder_cell,\n            attention_mechanism,\n            <span class=\"pl-v\">attention_layer_size</span><span class=\"pl-k\">=</span>attention_layer_size,\n            <span class=\"pl-v\">alignment_history</span><span class=\"pl-k\">=</span>attention_alignment_history,\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>attention<span class=\"pl-pds\">\"</span></span>)\n\n        decoder_initial_state <span class=\"pl-k\">=</span> decoder_cell.zero_state(batch_size, tf.float32)\n\n        sos_id <span class=\"pl-k\">=</span> tf.cast(vocab_size<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>, tf.int32)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> End token id</span>\n        eos_id <span class=\"pl-k\">=</span> tf.cast(vocab_size<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, tf.int32)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Start tokens for the batch</span>\n        start_tokens <span class=\"pl-k\">=</span> tf.fill([batch_size], sos_id)\n        end_token <span class=\"pl-k\">=</span> eos_id\n\n        embedding <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">ids</span>: tf.one_hot(ids, vocab_size)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Helper for the decoder (performs sampling)</span>\n        helper <span class=\"pl-k\">=</span> tf.contrib.seq2seq.GreedyEmbeddingHelper(\n            embedding,\n            start_tokens,\n            end_token\n            )\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Decoder</span>\n        decoder <span class=\"pl-k\">=</span> tf.contrib.seq2seq.BasicDecoder(\n            <span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span>decoder_cell,\n            <span class=\"pl-v\">helper</span><span class=\"pl-k\">=</span>helper,\n            <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span>decoder_initial_state,\n            <span class=\"pl-v\">output_layer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>\n            )\n\n        final_outputs, final_state, _ <span class=\"pl-k\">=</span> tf.contrib.seq2seq.dynamic_decode(\n            decoder,\n            <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope\n            )\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test_attention_wrapper</span>(<span class=\"pl-smi\">batch</span>, <span class=\"pl-smi\">max_time</span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">memory_arr</span>,\n                           <span class=\"pl-smi\">memory_sequence_length_arr</span>,\n                           <span class=\"pl-smi\">attention_alignment_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                           <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>decoder<span class=\"pl-pds\">\"</span></span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set the random graph seed</span>\n    tf.set_random_seed(<span class=\"pl-c1\">1</span>)\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        memory <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [batch, max_time, num_units],\n                                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>memory<span class=\"pl-pds\">\"</span></span>)\n        memory_sequence_length <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [batch],\n                                                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>memory_sequence_length<span class=\"pl-pds\">\"</span></span>)\n        create_graph(memory, memory_sequence_length,\n                     <span class=\"pl-v\">attention_alignment_history</span><span class=\"pl-k\">=</span>attention_alignment_history)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize the variables</span>\n        variables <span class=\"pl-k\">=</span> tf.VariableScope(<span class=\"pl-c1\">None</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>decoder<span class=\"pl-pds\">\"</span></span>).global_variables()\n        sess.run([v.initializer <span class=\"pl-k\">for</span> v <span class=\"pl-k\">in</span> variables])\n\n        variables_out <span class=\"pl-k\">=</span> sess.run(\n            variables,\n            <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{memory: memory_arr,\n                       memory_sequence_length: memory_sequence_length_arr}\n            )\n\n    <span class=\"pl-k\">return</span> variables_out, variables</pre></div>\n<p>The full code used for reproducing the issue can be found below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>! /usr/bin/env</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> encoding: utf-8</span>\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Parameters of the graph</span>\nbatch <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nmax_time <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\nnum_units <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nmemory_arr <span class=\"pl-k\">=</span> np.ones((batch, max_time, num_units))\nmemory_sequence_length_arr <span class=\"pl-k\">=</span> max_time <span class=\"pl-k\">*</span> np.ones(batch)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create two graphs (with and without attention history)</span>\ntf.reset_default_graph()\nvariables_no, variables <span class=\"pl-k\">=</span> test_attention_wrapper(\n    batch, max_time, num_units, memory_arr,\n    memory_sequence_length_arr,\n    <span class=\"pl-v\">attention_alignment_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\ntf.reset_default_graph()\nvariables_yes, variables <span class=\"pl-k\">=</span> test_attention_wrapper(\n    batch, max_time, num_units, memory_arr,\n    memory_sequence_length_arr,\n    <span class=\"pl-v\">attention_alignment_history</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">for</span> i, (var_out_no, var_out_yes) <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">zip</span>(variables_no, variables_yes)):\n    var_name <span class=\"pl-k\">=</span> variables[i].name\n    <span class=\"pl-c1\">print</span>(var_name)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>-<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">len</span>(var_name))\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>     print \"NO:\", var_out_no</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>     print \"YES:\", var_out_yes</span>\n\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>--&gt; The arrays are equal:<span class=\"pl-pds\">\"</span></span>, np.array_equal(var_out_no, var_out_yes)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>The output obtained is the following:</p>\n<pre><code>decoder/memory_layer/kernel:0\n-----------------------------\n--&gt; The arrays are equal: True\n\n\ndecoder/attention/lstm/kernel:0\n-------------------------------\n--&gt; The arrays are equal: False\n\n\ndecoder/attention/lstm/bias:0\n-----------------------------\n--&gt; The arrays are equal: True\n\n\ndecoder/attention/bahdanau_attention/query_layer/kernel:0\n---------------------------------------------------------\n--&gt; The arrays are equal: False\n\n\ndecoder/attention/bahdanau_attention/attention_v:0\n--------------------------------------------------\n--&gt; The arrays are equal: False\n\n\ndecoder/attention/attention_layer/kernel:0\n------------------------------------------\n--&gt; The arrays are equal: False\n\n</code></pre>\n<p>More precisely, here are the two different weights for the LSTM cell (<code>LAS/decoder/attention/multi_rnn_cell/cell_0/lstm_cell/kernel:0</code>) obtained in each of the two cases for <code>alignment_history</code>:</p>\n<ul>\n<li><code>alignment_history = False</code>:</li>\n</ul>\n<pre><code>[[  1.93134502e-01   5.19581586e-02  -2.18084604e-01 ...,   1.71998128e-01\n    2.81669050e-02  -1.70363069e-01]\n [  1.95658550e-01   7.72098750e-02   2.17303529e-01 ...,   2.07259521e-01\n    5.78315109e-02  -1.17801599e-01]\n [ -6.88283443e-02  -1.35597080e-01   1.65620014e-01 ...,  -1.32969454e-01\n   -2.11523965e-01  -1.86820269e-01]\n ..., \n [ -1.15171000e-01  -8.01331848e-02   1.30112335e-01 ...,   2.73928046e-04\n    1.80437252e-01   1.90824643e-01]\n [ -7.53998458e-02   1.86289057e-01   1.80155411e-01 ...,  -1.64348409e-01\n    2.10424170e-01  -1.46689758e-01]\n [ -2.08476633e-02   7.97681957e-02  -2.03553244e-01 ...,  -1.91280752e-01\n    8.57728869e-02   1.46612525e-04]]\n</code></pre>\n<ul>\n<li><code>alignment_history = True</code>:</li>\n</ul>\n<pre><code>[[-0.18967885 -0.20777287  0.03707646 ...,  0.17251725  0.20710425\n  -0.03941825]\n [-0.13857554 -0.05786179  0.17680736 ...,  0.01303713  0.05177127\n  -0.12967519]\n [-0.18074478 -0.09467114  0.09963275 ..., -0.11447592  0.19544493\n  -0.19714527]\n ..., \n [ 0.16114177  0.14009587  0.11265792 ..., -0.11863185 -0.08480376\n  -0.19559079]\n [-0.17472327  0.11717187  0.21214487 ...,  0.17373656 -0.15397248\n   0.04700263]\n [-0.05792974  0.18947266  0.06573398 ..., -0.0308952   0.18018191\n  -0.20467089]]\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Mac OS X 10.13.6\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\nTensorFlow installed from (source or binary): binary (wheel)\nTensorFlow version (use command below): 1.8.0\nPython version: 2.7.15\nBazel version (if compiling from source): -\nGCC/Compiler version (if compiling from source): -\nCUDA/cuDNN version: tested on CPU\nGPU model and memory: -\nExact command to reproduce: code below\n\nDescribe the problem\nThe initialization of the weights of the tensors in the decoder change when\nchanging the alignment_history parameter in tf.contrib.seq2seq.AttentionWrapper\n(True or False). This occurs even if the graph's random seed is fixed\n(tf.set_random_seed(1)).\nSource code / logs\nBelow, create_graph creates the graph consisting of a decoder with Bahdanau\nattention and test_attention_wrapper creates a session in order to check the\nvalues of the weight matrices at initialization.\n#! /usr/bin/env\n# encoding: utf-8\n\nimport tensorflow as tf\n\n\ndef create_graph(memory,\n                 memory_sequence_length,\n                 batch_size=1,\n                 vocab_size=30,\n                 num_units=16,\n                 attention_layer_size=16,\n                 attention_alignment_history=False,\n                 name=\"decoder\"):\n\n    with tf.variable_scope(name) as scope:\n        decoder_cell = tf.contrib.rnn.LSTMCell(num_units, name=\"lstm\")\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\n            attention_layer_size,\n            memory,\n            memory_sequence_length)\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\n            decoder_cell,\n            attention_mechanism,\n            attention_layer_size=attention_layer_size,\n            alignment_history=attention_alignment_history,\n            name=\"attention\")\n\n        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\n\n        sos_id = tf.cast(vocab_size-2, tf.int32)\n        # End token id\n        eos_id = tf.cast(vocab_size-1, tf.int32)\n\n        # Start tokens for the batch\n        start_tokens = tf.fill([batch_size], sos_id)\n        end_token = eos_id\n\n        embedding = lambda ids: tf.one_hot(ids, vocab_size)\n        # Helper for the decoder (performs sampling)\n        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\n            embedding,\n            start_tokens,\n            end_token\n            )\n\n        # Decoder\n        decoder = tf.contrib.seq2seq.BasicDecoder(\n            cell=decoder_cell,\n            helper=helper,\n            initial_state=decoder_initial_state,\n            output_layer=None\n            )\n\n        final_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(\n            decoder,\n            scope=scope\n            )\n\n\ndef test_attention_wrapper(batch, max_time, num_units, memory_arr,\n                           memory_sequence_length_arr,\n                           attention_alignment_history=False,\n                           name=\"decoder\"):\n    # Set the random graph seed\n    tf.set_random_seed(1)\n    with tf.Session() as sess:\n        memory = tf.placeholder(tf.float32, [batch, max_time, num_units],\n                                name=\"memory\")\n        memory_sequence_length = tf.placeholder(tf.int32, [batch],\n                                                name=\"memory_sequence_length\")\n        create_graph(memory, memory_sequence_length,\n                     attention_alignment_history=attention_alignment_history)\n        # Initialize the variables\n        variables = tf.VariableScope(None, name=\"decoder\").global_variables()\n        sess.run([v.initializer for v in variables])\n\n        variables_out = sess.run(\n            variables,\n            feed_dict={memory: memory_arr,\n                       memory_sequence_length: memory_sequence_length_arr}\n            )\n\n    return variables_out, variables\nThe full code used for reproducing the issue can be found below:\n#! /usr/bin/env\n# encoding: utf-8\n\nimport numpy as np\nimport tensorflow as tf\n\n# Parameters of the graph\nbatch = 1\nmax_time = 3\nnum_units = 1\nmemory_arr = np.ones((batch, max_time, num_units))\nmemory_sequence_length_arr = max_time * np.ones(batch)\n\n# Create two graphs (with and without attention history)\ntf.reset_default_graph()\nvariables_no, variables = test_attention_wrapper(\n    batch, max_time, num_units, memory_arr,\n    memory_sequence_length_arr,\n    attention_alignment_history=False)\ntf.reset_default_graph()\nvariables_yes, variables = test_attention_wrapper(\n    batch, max_time, num_units, memory_arr,\n    memory_sequence_length_arr,\n    attention_alignment_history=True)\n\nfor i, (var_out_no, var_out_yes) in enumerate(zip(variables_no, variables_yes)):\n    var_name = variables[i].name\n    print(var_name)\n    print(\"-\" * len(var_name))\n#     print \"NO:\", var_out_no\n#     print \"YES:\", var_out_yes\n\n    print \"--> The arrays are equal:\", np.array_equal(var_out_no, var_out_yes)\n    print(\"\\n\")\nThe output obtained is the following:\ndecoder/memory_layer/kernel:0\n-----------------------------\n--> The arrays are equal: True\n\n\ndecoder/attention/lstm/kernel:0\n-------------------------------\n--> The arrays are equal: False\n\n\ndecoder/attention/lstm/bias:0\n-----------------------------\n--> The arrays are equal: True\n\n\ndecoder/attention/bahdanau_attention/query_layer/kernel:0\n---------------------------------------------------------\n--> The arrays are equal: False\n\n\ndecoder/attention/bahdanau_attention/attention_v:0\n--------------------------------------------------\n--> The arrays are equal: False\n\n\ndecoder/attention/attention_layer/kernel:0\n------------------------------------------\n--> The arrays are equal: False\n\n\nMore precisely, here are the two different weights for the LSTM cell (LAS/decoder/attention/multi_rnn_cell/cell_0/lstm_cell/kernel:0) obtained in each of the two cases for alignment_history:\n\nalignment_history = False:\n\n[[  1.93134502e-01   5.19581586e-02  -2.18084604e-01 ...,   1.71998128e-01\n    2.81669050e-02  -1.70363069e-01]\n [  1.95658550e-01   7.72098750e-02   2.17303529e-01 ...,   2.07259521e-01\n    5.78315109e-02  -1.17801599e-01]\n [ -6.88283443e-02  -1.35597080e-01   1.65620014e-01 ...,  -1.32969454e-01\n   -2.11523965e-01  -1.86820269e-01]\n ..., \n [ -1.15171000e-01  -8.01331848e-02   1.30112335e-01 ...,   2.73928046e-04\n    1.80437252e-01   1.90824643e-01]\n [ -7.53998458e-02   1.86289057e-01   1.80155411e-01 ...,  -1.64348409e-01\n    2.10424170e-01  -1.46689758e-01]\n [ -2.08476633e-02   7.97681957e-02  -2.03553244e-01 ...,  -1.91280752e-01\n    8.57728869e-02   1.46612525e-04]]\n\n\nalignment_history = True:\n\n[[-0.18967885 -0.20777287  0.03707646 ...,  0.17251725  0.20710425\n  -0.03941825]\n [-0.13857554 -0.05786179  0.17680736 ...,  0.01303713  0.05177127\n  -0.12967519]\n [-0.18074478 -0.09467114  0.09963275 ..., -0.11447592  0.19544493\n  -0.19714527]\n ..., \n [ 0.16114177  0.14009587  0.11265792 ..., -0.11863185 -0.08480376\n  -0.19559079]\n [-0.17472327  0.11717187  0.21214487 ...,  0.17373656 -0.15397248\n   0.04700263]\n [-0.05792974  0.18947266  0.06573398 ..., -0.0308952   0.18018191\n  -0.20467089]]", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Mac OS X 10.13.6\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary (wheel)\r\n- **TensorFlow version (use command below)**: 1.8.0\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: -\r\n- **GCC/Compiler version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: tested on CPU\r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: code below\r\n\r\n### Describe the problem\r\nThe initialization of the weights of the tensors in the decoder change when\r\nchanging the `alignment_history` parameter in `tf.contrib.seq2seq.AttentionWrapper`\r\n(`True` or `False`). This occurs even if the graph's random seed is fixed\r\n(`tf.set_random_seed(1)`).\r\n\r\n### Source code / logs\r\nBelow, `create_graph` creates the graph consisting of a decoder with Bahdanau\r\nattention and `test_attention_wrapper` creates a session in order to check the\r\nvalues of the weight matrices at initialization.\r\n```python\r\n#! /usr/bin/env\r\n# encoding: utf-8\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef create_graph(memory,\r\n                 memory_sequence_length,\r\n                 batch_size=1,\r\n                 vocab_size=30,\r\n                 num_units=16,\r\n                 attention_layer_size=16,\r\n                 attention_alignment_history=False,\r\n                 name=\"decoder\"):\r\n\r\n    with tf.variable_scope(name) as scope:\r\n        decoder_cell = tf.contrib.rnn.LSTMCell(num_units, name=\"lstm\")\r\n        attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(\r\n            attention_layer_size,\r\n            memory,\r\n            memory_sequence_length)\r\n        decoder_cell = tf.contrib.seq2seq.AttentionWrapper(\r\n            decoder_cell,\r\n            attention_mechanism,\r\n            attention_layer_size=attention_layer_size,\r\n            alignment_history=attention_alignment_history,\r\n            name=\"attention\")\r\n\r\n        decoder_initial_state = decoder_cell.zero_state(batch_size, tf.float32)\r\n\r\n        sos_id = tf.cast(vocab_size-2, tf.int32)\r\n        # End token id\r\n        eos_id = tf.cast(vocab_size-1, tf.int32)\r\n\r\n        # Start tokens for the batch\r\n        start_tokens = tf.fill([batch_size], sos_id)\r\n        end_token = eos_id\r\n\r\n        embedding = lambda ids: tf.one_hot(ids, vocab_size)\r\n        # Helper for the decoder (performs sampling)\r\n        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(\r\n            embedding,\r\n            start_tokens,\r\n            end_token\r\n            )\r\n\r\n        # Decoder\r\n        decoder = tf.contrib.seq2seq.BasicDecoder(\r\n            cell=decoder_cell,\r\n            helper=helper,\r\n            initial_state=decoder_initial_state,\r\n            output_layer=None\r\n            )\r\n\r\n        final_outputs, final_state, _ = tf.contrib.seq2seq.dynamic_decode(\r\n            decoder,\r\n            scope=scope\r\n            )\r\n\r\n\r\ndef test_attention_wrapper(batch, max_time, num_units, memory_arr,\r\n                           memory_sequence_length_arr,\r\n                           attention_alignment_history=False,\r\n                           name=\"decoder\"):\r\n    # Set the random graph seed\r\n    tf.set_random_seed(1)\r\n    with tf.Session() as sess:\r\n        memory = tf.placeholder(tf.float32, [batch, max_time, num_units],\r\n                                name=\"memory\")\r\n        memory_sequence_length = tf.placeholder(tf.int32, [batch],\r\n                                                name=\"memory_sequence_length\")\r\n        create_graph(memory, memory_sequence_length,\r\n                     attention_alignment_history=attention_alignment_history)\r\n        # Initialize the variables\r\n        variables = tf.VariableScope(None, name=\"decoder\").global_variables()\r\n        sess.run([v.initializer for v in variables])\r\n\r\n        variables_out = sess.run(\r\n            variables,\r\n            feed_dict={memory: memory_arr,\r\n                       memory_sequence_length: memory_sequence_length_arr}\r\n            )\r\n\r\n    return variables_out, variables\r\n```\r\n\r\nThe full code used for reproducing the issue can be found below:\r\n\r\n```python\r\n#! /usr/bin/env\r\n# encoding: utf-8\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# Parameters of the graph\r\nbatch = 1\r\nmax_time = 3\r\nnum_units = 1\r\nmemory_arr = np.ones((batch, max_time, num_units))\r\nmemory_sequence_length_arr = max_time * np.ones(batch)\r\n\r\n# Create two graphs (with and without attention history)\r\ntf.reset_default_graph()\r\nvariables_no, variables = test_attention_wrapper(\r\n    batch, max_time, num_units, memory_arr,\r\n    memory_sequence_length_arr,\r\n    attention_alignment_history=False)\r\ntf.reset_default_graph()\r\nvariables_yes, variables = test_attention_wrapper(\r\n    batch, max_time, num_units, memory_arr,\r\n    memory_sequence_length_arr,\r\n    attention_alignment_history=True)\r\n\r\nfor i, (var_out_no, var_out_yes) in enumerate(zip(variables_no, variables_yes)):\r\n    var_name = variables[i].name\r\n    print(var_name)\r\n    print(\"-\" * len(var_name))\r\n#     print \"NO:\", var_out_no\r\n#     print \"YES:\", var_out_yes\r\n\r\n    print \"--> The arrays are equal:\", np.array_equal(var_out_no, var_out_yes)\r\n    print(\"\\n\")\r\n```\r\n\r\nThe output obtained is the following:\r\n\r\n```\r\ndecoder/memory_layer/kernel:0\r\n-----------------------------\r\n--> The arrays are equal: True\r\n\r\n\r\ndecoder/attention/lstm/kernel:0\r\n-------------------------------\r\n--> The arrays are equal: False\r\n\r\n\r\ndecoder/attention/lstm/bias:0\r\n-----------------------------\r\n--> The arrays are equal: True\r\n\r\n\r\ndecoder/attention/bahdanau_attention/query_layer/kernel:0\r\n---------------------------------------------------------\r\n--> The arrays are equal: False\r\n\r\n\r\ndecoder/attention/bahdanau_attention/attention_v:0\r\n--------------------------------------------------\r\n--> The arrays are equal: False\r\n\r\n\r\ndecoder/attention/attention_layer/kernel:0\r\n------------------------------------------\r\n--> The arrays are equal: False\r\n\r\n```\r\n\r\nMore precisely, here are the two different weights for the LSTM cell (`LAS/decoder/attention/multi_rnn_cell/cell_0/lstm_cell/kernel:0`) obtained in each of the two cases for `alignment_history`:\r\n\r\n- `alignment_history = False`:\r\n\r\n```\r\n[[  1.93134502e-01   5.19581586e-02  -2.18084604e-01 ...,   1.71998128e-01\r\n    2.81669050e-02  -1.70363069e-01]\r\n [  1.95658550e-01   7.72098750e-02   2.17303529e-01 ...,   2.07259521e-01\r\n    5.78315109e-02  -1.17801599e-01]\r\n [ -6.88283443e-02  -1.35597080e-01   1.65620014e-01 ...,  -1.32969454e-01\r\n   -2.11523965e-01  -1.86820269e-01]\r\n ..., \r\n [ -1.15171000e-01  -8.01331848e-02   1.30112335e-01 ...,   2.73928046e-04\r\n    1.80437252e-01   1.90824643e-01]\r\n [ -7.53998458e-02   1.86289057e-01   1.80155411e-01 ...,  -1.64348409e-01\r\n    2.10424170e-01  -1.46689758e-01]\r\n [ -2.08476633e-02   7.97681957e-02  -2.03553244e-01 ...,  -1.91280752e-01\r\n    8.57728869e-02   1.46612525e-04]]\r\n```\r\n\r\n- `alignment_history = True`:\r\n\r\n```\r\n[[-0.18967885 -0.20777287  0.03707646 ...,  0.17251725  0.20710425\r\n  -0.03941825]\r\n [-0.13857554 -0.05786179  0.17680736 ...,  0.01303713  0.05177127\r\n  -0.12967519]\r\n [-0.18074478 -0.09467114  0.09963275 ..., -0.11447592  0.19544493\r\n  -0.19714527]\r\n ..., \r\n [ 0.16114177  0.14009587  0.11265792 ..., -0.11863185 -0.08480376\r\n  -0.19559079]\r\n [-0.17472327  0.11717187  0.21214487 ...,  0.17373656 -0.15397248\r\n   0.04700263]\r\n [-0.05792974  0.18947266  0.06573398 ..., -0.0308952   0.18018191\r\n  -0.20467089]]\r\n```"}