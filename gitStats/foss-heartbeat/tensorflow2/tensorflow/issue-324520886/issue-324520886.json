{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19395", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19395/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19395/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19395/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19395", "id": 324520886, "node_id": "MDU6SXNzdWUzMjQ1MjA4ODY=", "number": 19395, "title": "Gridrnn (Grid2LSTM) tied behaviour is inverted", "user": {"login": "Abhishek8394", "id": 5524161, "node_id": "MDQ6VXNlcjU1MjQxNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5524161?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Abhishek8394", "html_url": "https://github.com/Abhishek8394", "followers_url": "https://api.github.com/users/Abhishek8394/followers", "following_url": "https://api.github.com/users/Abhishek8394/following{/other_user}", "gists_url": "https://api.github.com/users/Abhishek8394/gists{/gist_id}", "starred_url": "https://api.github.com/users/Abhishek8394/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Abhishek8394/subscriptions", "organizations_url": "https://api.github.com/users/Abhishek8394/orgs", "repos_url": "https://api.github.com/users/Abhishek8394/repos", "events_url": "https://api.github.com/users/Abhishek8394/events{/privacy}", "received_events_url": "https://api.github.com/users/Abhishek8394/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-18T19:08:42Z", "updated_at": "2018-11-14T19:18:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8</li>\n<li><strong>Python version</strong>: 3.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When trying to create a unrolled sequence for a <code>Grid2LSTMCell</code>, weights are shared among two states (the two dimensions) for a given step, when the property <code>tied</code> is set to <em>False</em> and they are not shared when <code>tied</code> is set to <em>True</em>. If this is indeed the behaviour that was desired, then it deviates from the original paper based on which it is implemented, otherwise it seems to be a bug in variable reuse.</p>\n<p><strong>Expected Behaviour</strong>: Share weights when <code>tied</code> is True and do not share when <code>tied</code> is False</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.rnn.python.ops <span class=\"pl-k\">import</span> rnn_cell\n<span class=\"pl-k\">from</span> tensorflow.contrib <span class=\"pl-k\">import</span> grid_rnn\n<span class=\"pl-k\">from</span> tensorflow.python.ops <span class=\"pl-k\">import</span> variable_scope\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nseq_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> inp =&gt; sequence_length x batch_size x embedding_size</span>\ninp_grid <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(seq_len, batch_size, <span class=\"pl-c1\">2048</span>)) \nrnnargs <span class=\"pl-k\">=</span> {\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>use_peepholes<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">True</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>forget_bias<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">1.0</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>state_is_tuple<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">False</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output_is_tuple<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">False</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>output_is_tuple<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">True</span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>state_is_tuple<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">True</span>,\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tied<span class=\"pl-pds\">'</span></span>: <span class=\"pl-c1\">False</span>\n            }\nglstm <span class=\"pl-k\">=</span> grid_rnn.Grid2LSTMCell(<span class=\"pl-c1\">1024</span>, <span class=\"pl-k\">**</span>rnnargs)\nstate <span class=\"pl-k\">=</span> glstm.zero_state(batch_size, tf.float32)\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(seq_len):\n    <span class=\"pl-k\">if</span> i<span class=\"pl-k\">&gt;</span><span class=\"pl-c1\">0</span>:\n        variable_scope.get_variable_scope().reuse_variables()\n    state_left, state_top <span class=\"pl-k\">=</span> state[<span class=\"pl-c1\">0</span>], state[<span class=\"pl-c1\">1</span>] <span class=\"pl-c\"><span class=\"pl-c\">#</span> since two dimensions</span>\n    out, state <span class=\"pl-k\">=</span> glstm(inp_grid[i], (state_left, state_top))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> If you observe the actual tensor (name) of the two states, </span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> they will be same when tied is False </span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> and different when tied is true</span>\n    <span class=\"pl-c1\">print</span>(state[<span class=\"pl-c1\">0</span>],<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\t</span><span class=\"pl-pds\">\"</span></span>,state[<span class=\"pl-c1\">1</span>],<span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>I think this is a bug related to variable reuse instead of how cells are defined in <code>grid_rnn</code> constructor. I would like to work on this issue if this indeed is an issue, or else a explanation of why this is the correct behaviour will do.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.8\nPython version: 3.5\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nWhen trying to create a unrolled sequence for a Grid2LSTMCell, weights are shared among two states (the two dimensions) for a given step, when the property tied is set to False and they are not shared when tied is set to True. If this is indeed the behaviour that was desired, then it deviates from the original paper based on which it is implemented, otherwise it seems to be a bug in variable reuse.\nExpected Behaviour: Share weights when tied is True and do not share when tied is False\nSource code / logs\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn.python.ops import rnn_cell\nfrom tensorflow.contrib import grid_rnn\nfrom tensorflow.python.ops import variable_scope\n\nbatch_size = 32\nseq_len = 10\n# inp => sequence_length x batch_size x embedding_size\ninp_grid = tf.placeholder(tf.float32, shape=(seq_len, batch_size, 2048)) \nrnnargs = {\n            'use_peepholes': True, 'forget_bias': 1.0,\n            'state_is_tuple': False, 'output_is_tuple': False,\n            'output_is_tuple': True, 'state_is_tuple': True,\n            'tied': False\n            }\nglstm = grid_rnn.Grid2LSTMCell(1024, **rnnargs)\nstate = glstm.zero_state(batch_size, tf.float32)\nfor i in range(seq_len):\n    if i>0:\n        variable_scope.get_variable_scope().reuse_variables()\n    state_left, state_top = state[0], state[1] # since two dimensions\n    out, state = glstm(inp_grid[i], (state_left, state_top))\n    # If you observe the actual tensor (name) of the two states, \n    # they will be same when tied is False \n    # and different when tied is true\n    print(state[0],\"\\t\",state[1],\"\\n\")\nI think this is a bug related to variable reuse instead of how cells are defined in grid_rnn constructor. I would like to work on this issue if this indeed is an issue, or else a explanation of why this is the correct behaviour will do.", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.8\r\n- **Python version**: 3.5\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nWhen trying to create a unrolled sequence for a `Grid2LSTMCell`, weights are shared among two states (the two dimensions) for a given step, when the property `tied` is set to *False* and they are not shared when `tied` is set to *True*. If this is indeed the behaviour that was desired, then it deviates from the original paper based on which it is implemented, otherwise it seems to be a bug in variable reuse.\r\n\r\n**Expected Behaviour**: Share weights when `tied` is True and do not share when `tied` is False\r\n\r\n### Source code / logs\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn.python.ops import rnn_cell\r\nfrom tensorflow.contrib import grid_rnn\r\nfrom tensorflow.python.ops import variable_scope\r\n\r\nbatch_size = 32\r\nseq_len = 10\r\n# inp => sequence_length x batch_size x embedding_size\r\ninp_grid = tf.placeholder(tf.float32, shape=(seq_len, batch_size, 2048)) \r\nrnnargs = {\r\n            'use_peepholes': True, 'forget_bias': 1.0,\r\n            'state_is_tuple': False, 'output_is_tuple': False,\r\n            'output_is_tuple': True, 'state_is_tuple': True,\r\n            'tied': False\r\n            }\r\nglstm = grid_rnn.Grid2LSTMCell(1024, **rnnargs)\r\nstate = glstm.zero_state(batch_size, tf.float32)\r\nfor i in range(seq_len):\r\n    if i>0:\r\n        variable_scope.get_variable_scope().reuse_variables()\r\n    state_left, state_top = state[0], state[1] # since two dimensions\r\n    out, state = glstm(inp_grid[i], (state_left, state_top))\r\n    # If you observe the actual tensor (name) of the two states, \r\n    # they will be same when tied is False \r\n    # and different when tied is true\r\n    print(state[0],\"\\t\",state[1],\"\\n\")\r\n```\r\n\r\nI think this is a bug related to variable reuse instead of how cells are defined in `grid_rnn` constructor. I would like to work on this issue if this indeed is an issue, or else a explanation of why this is the correct behaviour will do."}