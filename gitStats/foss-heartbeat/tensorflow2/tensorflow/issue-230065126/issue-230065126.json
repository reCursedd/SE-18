{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10043", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10043/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10043/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10043/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10043", "id": 230065126, "node_id": "MDU6SXNzdWUyMzAwNjUxMjY=", "number": 10043, "title": "Dynamic_attention_wrapper using rnn output or state to caculate next attention?", "user": {"login": "yanyankangkang", "id": 7603198, "node_id": "MDQ6VXNlcjc2MDMxOTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/7603198?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yanyankangkang", "html_url": "https://github.com/yanyankangkang", "followers_url": "https://api.github.com/users/yanyankangkang/followers", "following_url": "https://api.github.com/users/yanyankangkang/following{/other_user}", "gists_url": "https://api.github.com/users/yanyankangkang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yanyankangkang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yanyankangkang/subscriptions", "organizations_url": "https://api.github.com/users/yanyankangkang/orgs", "repos_url": "https://api.github.com/users/yanyankangkang/repos", "events_url": "https://api.github.com/users/yanyankangkang/events{/privacy}", "received_events_url": "https://api.github.com/users/yanyankangkang/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-19T19:11:28Z", "updated_at": "2017-05-20T00:43:16Z", "closed_at": "2017-05-20T00:43:16Z", "author_association": "NONE", "body_html": "<p>In tensorflow 1.1,  <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py#L535\">https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py#L535</a>, dynamic_attention_wrapper use RNN output to calculate next attention<br>\nbut in previous seq2seq api, <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L692\">https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L692</a>, attention_decoder use RNN state to calculate next attention.<br>\nIn paper, <a href=\"https://arxiv.org/pdf/1409.0473.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1409.0473.pdf</a>. page 3, under equation 6, it use RNN state. I hope some could tell me whether it will affect model in practice.<br>\nThere is also a stackoverflow post without satisfied answer <a href=\"http://stackoverflow.com/questions/43248613/attention-decoder-implementation-in-tensorflow-r1-0\" rel=\"nofollow\">http://stackoverflow.com/questions/43248613/attention-decoder-implementation-in-tensorflow-r1-0</a></p>", "body_text": "In tensorflow 1.1,  https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py#L535, dynamic_attention_wrapper use RNN output to calculate next attention\nbut in previous seq2seq api, https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L692, attention_decoder use RNN state to calculate next attention.\nIn paper, https://arxiv.org/pdf/1409.0473.pdf. page 3, under equation 6, it use RNN state. I hope some could tell me whether it will affect model in practice.\nThere is also a stackoverflow post without satisfied answer http://stackoverflow.com/questions/43248613/attention-decoder-implementation-in-tensorflow-r1-0", "body": "In tensorflow 1.1,  https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/seq2seq/python/ops/dynamic_attention_wrapper.py#L535, dynamic_attention_wrapper use RNN output to calculate next attention \r\nbut in previous seq2seq api, https://github.com/tensorflow/tensorflow/blob/r1.1/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py#L692, attention_decoder use RNN state to calculate next attention. \r\nIn paper, https://arxiv.org/pdf/1409.0473.pdf. page 3, under equation 6, it use RNN state. I hope some could tell me whether it will affect model in practice.\r\nThere is also a stackoverflow post without satisfied answer http://stackoverflow.com/questions/43248613/attention-decoder-implementation-in-tensorflow-r1-0\r\n"}