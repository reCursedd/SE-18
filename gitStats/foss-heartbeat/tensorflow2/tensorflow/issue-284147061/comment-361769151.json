{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361769151", "html_url": "https://github.com/tensorflow/tensorflow/issues/15585#issuecomment-361769151", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585", "id": 361769151, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTc2OTE1MQ==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-30T23:23:54Z", "updated_at": "2018-01-30T23:23:54Z", "author_association": "MEMBER", "body_html": "<p>In general, fp16 on Pascal GPUs (like your P100) will not be much faster, if faster at all. In your cuBlas example, you pass CUDA_R_16F as the second-to-last parameter, <code>computeType</code>, to <code>cublasGemmEx()</code>. In TensorFlow, we use fp32 as a compute type, since models do not work well in practice if a lower precision is used as the compute type. This can be changed by setting the evironmental variables <a href=\"https://github.com/tensorflow/tensorflow/blob/bffa3e10bf4886f03a68f7e93ba39c91d447f101/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2236\">TF_FP16_CONV_USE_FP32_COMPUTE</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/de0904e90669d345e0feaa89d1e99b7618fb6677/tensorflow/core/util/matmul_autotune.cc#L44\">TF_FP16_MATMUL_USE_FP32_COMPUTE</a> to 0. But if you do that, the models will probably not train as well.</p>\n<p>Note everything I said applies to Pascal. On Volta, fp16 is significantly faster.</p>\n<p>Also, I'm not sure where this tutorial/images/mnist example is. Can you clarify what the example is so I can reproduce? Additionally, I cannot run the TensorFlow example you provided, since FLAGs and get_dtype are undefined. Can you post a complete example?</p>", "body_text": "In general, fp16 on Pascal GPUs (like your P100) will not be much faster, if faster at all. In your cuBlas example, you pass CUDA_R_16F as the second-to-last parameter, computeType, to cublasGemmEx(). In TensorFlow, we use fp32 as a compute type, since models do not work well in practice if a lower precision is used as the compute type. This can be changed by setting the evironmental variables TF_FP16_CONV_USE_FP32_COMPUTE and TF_FP16_MATMUL_USE_FP32_COMPUTE to 0. But if you do that, the models will probably not train as well.\nNote everything I said applies to Pascal. On Volta, fp16 is significantly faster.\nAlso, I'm not sure where this tutorial/images/mnist example is. Can you clarify what the example is so I can reproduce? Additionally, I cannot run the TensorFlow example you provided, since FLAGs and get_dtype are undefined. Can you post a complete example?", "body": "In general, fp16 on Pascal GPUs (like your P100) will not be much faster, if faster at all. In your cuBlas example, you pass CUDA_R_16F as the second-to-last parameter, `computeType`, to `cublasGemmEx()`. In TensorFlow, we use fp32 as a compute type, since models do not work well in practice if a lower precision is used as the compute type. This can be changed by setting the evironmental variables [TF_FP16_CONV_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/bffa3e10bf4886f03a68f7e93ba39c91d447f101/tensorflow/stream_executor/cuda/cuda_dnn.cc#L2236) and [TF_FP16_MATMUL_USE_FP32_COMPUTE](https://github.com/tensorflow/tensorflow/blob/de0904e90669d345e0feaa89d1e99b7618fb6677/tensorflow/core/util/matmul_autotune.cc#L44) to 0. But if you do that, the models will probably not train as well.\r\n\r\nNote everything I said applies to Pascal. On Volta, fp16 is significantly faster.\r\n\r\nAlso, I'm not sure where this tutorial/images/mnist example is. Can you clarify what the example is so I can reproduce? Additionally, I cannot run the TensorFlow example you provided, since FLAGs and get_dtype are undefined. Can you post a complete example?"}