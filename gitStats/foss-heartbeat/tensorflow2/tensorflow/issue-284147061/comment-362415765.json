{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362415765", "html_url": "https://github.com/tensorflow/tensorflow/issues/15585#issuecomment-362415765", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585", "id": 362415765, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjQxNTc2NQ==", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T21:58:03Z", "updated_at": "2018-02-01T21:58:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11447989\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/KillPinguin\">@KillPinguin</a> Thanks for pointing out. This was a change I made a couple of months ago due to a cublas bug back then. I will try to fix the issue in two steps:</p>\n<ol>\n<li>enable autotune support for fp16. I need to do some tests to make sure that no bug shows up.</li>\n<li>enable autotune for matmul by default in general. Last time I tried to enable it by default I got mixed performance results (for some shapes autotune helps, for some shapes it hurts the performance).</li>\n</ol>\n<p>Both step involves simple change but extensive tests/benchmarks, so do not expect the change to happen within weeks.</p>\n<p>Also, just so you know, as long as the first step is done, you should see a difference:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_blas.cc#L1863\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_blas.cc#L1863</a><br>\nsince matmul for half is calling SgemmEx() now, just the compute type is still FP32.</p>", "body_text": "@KillPinguin Thanks for pointing out. This was a change I made a couple of months ago due to a cublas bug back then. I will try to fix the issue in two steps:\n\nenable autotune support for fp16. I need to do some tests to make sure that no bug shows up.\nenable autotune for matmul by default in general. Last time I tried to enable it by default I got mixed performance results (for some shapes autotune helps, for some shapes it hurts the performance).\n\nBoth step involves simple change but extensive tests/benchmarks, so do not expect the change to happen within weeks.\nAlso, just so you know, as long as the first step is done, you should see a difference:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_blas.cc#L1863\nsince matmul for half is calling SgemmEx() now, just the compute type is still FP32.", "body": "@KillPinguin Thanks for pointing out. This was a change I made a couple of months ago due to a cublas bug back then. I will try to fix the issue in two steps:\r\n1) enable autotune support for fp16. I need to do some tests to make sure that no bug shows up.\r\n2) enable autotune for matmul by default in general. Last time I tried to enable it by default I got mixed performance results (for some shapes autotune helps, for some shapes it hurts the performance).\r\n\r\nBoth step involves simple change but extensive tests/benchmarks, so do not expect the change to happen within weeks.\r\n\r\nAlso, just so you know, as long as the first step is done, you should see a difference:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/stream_executor/cuda/cuda_blas.cc#L1863\r\nsince matmul for half is calling SgemmEx() now, just the compute type is still FP32."}