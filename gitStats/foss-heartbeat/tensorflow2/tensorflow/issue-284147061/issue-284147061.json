{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15585", "id": 284147061, "node_id": "MDU6SXNzdWUyODQxNDcwNjE=", "number": 15585, "title": "FP16 slower than FP32", "user": {"login": "KillPinguin", "id": 11447989, "node_id": "MDQ6VXNlcjExNDQ3OTg5", "avatar_url": "https://avatars3.githubusercontent.com/u/11447989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KillPinguin", "html_url": "https://github.com/KillPinguin", "followers_url": "https://api.github.com/users/KillPinguin/followers", "following_url": "https://api.github.com/users/KillPinguin/following{/other_user}", "gists_url": "https://api.github.com/users/KillPinguin/gists{/gist_id}", "starred_url": "https://api.github.com/users/KillPinguin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KillPinguin/subscriptions", "organizations_url": "https://api.github.com/users/KillPinguin/orgs", "repos_url": "https://api.github.com/users/KillPinguin/repos", "events_url": "https://api.github.com/users/KillPinguin/events{/privacy}", "received_events_url": "https://api.github.com/users/KillPinguin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 21, "created_at": "2017-12-22T12:03:07Z", "updated_at": "2018-04-26T17:16:21Z", "closed_at": "2018-04-26T17:16:21Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nPartly</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nRHEL 7</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nunknown</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n('unknown', '1.4.0')</li>\n<li><strong>Python version</strong>:<br>\npython2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nunknown</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\ngcc 4.8</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.0 / 7.0</li>\n<li><strong>GPU model and memory</strong>:<br>\nPascal P-100</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nN/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I tried running the tutorial/rnn/ptb and tutorial/images/mnist examples with the --use_fp16 option, to see if speedup was achievable by utilizing the new half-precision features.<br>\nIt turned out that a single training step for MNIST with FP32 took 3.3ms, with FP16 it was 4ms. For PTB small (I had to use lstm_cell=basic, because other types are not yet supported in FP16), the WPS dropped from 24000 to 22000 when switching to FP16.</p>\n<p>So the performance <strong>decreases</strong> when using FP16 in real-world examples.</p>\n<p>To check this, I've created a small benchmark for myself (since I can't get the nightly tf build on my machine, I can't run the official benchmarks), which is basically one big matrix multiplication in Tensorflow.</p>\n<p>I've run it with square matrices with a width of 8k, 16k and 32k. In each case, FP16 and FP32 yielded nearly the same runtime. Profiling with nvprof I found out that the used CUDA function is <strong>maxwell_fp16_segmemm_fp16_128x128_nn</strong> for FP16 and <strong>sgemm_128x128x8_NN_vec</strong> for FP32.</p>\n<p>Since I wanted to double check if matrix multiplication in FP16 is really slower than in FP32 on my GPU, I tried to directly benchmark the GPU using cuBlas with a similar operation. It turns out that here, FP16 is nearly twice as fast as FP32. CuBlas internally uses <strong>maxwell_hgemm_256x128_nn</strong> for matrix multiplication of 16k x 16k square matrices in FP16. (again  according to the nvprof profiler)</p>\n<p>So I'm wondering why Tensorflow is unable to achieve similar results in terms of speed, or if I'm doing something wrong in my tests.</p>\n<h3>Source code</h3>\n<p>Tensorflow Code snippet:</p>\n<pre><code>    graph = tf.Graph()\n        with graph.as_default():\n          tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n          tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n          tf_output = tf.matmul(tf_input1, tf_input2)\n  \n      with tf.Session(graph=graph) as session:\n          tf.global_variables_initializer().run()\n          print(\"Initialized\")\n          for i in range(FLAGS.times):\n              out = session.run([tf_output])#, feed_dict=feed_dict)\n          print(\"Done\")\n</code></pre>\n<p>cuBlas Code FP16 (Snippet):</p>\n<pre><code>        uint16_t *d_a;          // d_a - a on the device\n        uint16_t *d_b;          // d_b - b on the device\n        uint16_t *d_c;          // d_c - c on the device\n        cudaStat = cudaMalloc ((void **) &amp;d_a, m * k * sizeof (*a));    // device memory alloc for a\n        cudaStat = cudaMalloc ((void **) &amp;d_b, k * n * sizeof (*b));    // device memory alloc for b\n        cudaStat = cudaMalloc ((void **) &amp;d_c, m * n * sizeof (*c));    // device memory alloc for c\n        stat = cublasCreate (&amp;handle);  // initialize CUBLAS context\n        // copy matrices from the host to the device\n        stat = cublasSetMatrix (m, k, sizeof (*a), a, m, d_a, m);   //a -&gt; d_a\n        stat = cublasSetMatrix (k, n, sizeof (*b), b, k, d_b, k);   //b -&gt; d_b\n        stat = cublasSetMatrix (m, n, sizeof (*c), c, m, d_c, m);   //c -&gt; d_c\n        uint16_t al = FP_16_ONE;       // al = 1 \n        uint16_t bet = FP_16_ONE;      // bet =1\n        // matrix - matrix multiplication : d_c = al*d_a *d_b + bet *d_c\n        // d_a -mxk matrix , d_b -kxn matrix , d_c -mxn matrix ;\n        // al ,bet -scalars\n\n        stat = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &amp;al, d_a, CUDA_R_16F, m, d_b, CUDA_R_16F, k, &amp;bet, d_c, CUDA_R_16F, m, CUDA_R_16F, CUBLAS_GEMM_DEFAULT); \n\n        stat = cublasGetMatrix (m, n, sizeof (*c), d_c, m, c, m);   // cp d_c - &gt;c\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nPartly\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nRHEL 7\nTensorFlow installed from (source or binary):\nunknown\nTensorFlow version (use command below):\n('unknown', '1.4.0')\nPython version:\npython2.7\nBazel version (if compiling from source):\nunknown\nGCC/Compiler version (if compiling from source):\ngcc 4.8\nCUDA/cuDNN version:\n9.0 / 7.0\nGPU model and memory:\nPascal P-100\nExact command to reproduce:\nN/A\n\nDescribe the problem\nI tried running the tutorial/rnn/ptb and tutorial/images/mnist examples with the --use_fp16 option, to see if speedup was achievable by utilizing the new half-precision features.\nIt turned out that a single training step for MNIST with FP32 took 3.3ms, with FP16 it was 4ms. For PTB small (I had to use lstm_cell=basic, because other types are not yet supported in FP16), the WPS dropped from 24000 to 22000 when switching to FP16.\nSo the performance decreases when using FP16 in real-world examples.\nTo check this, I've created a small benchmark for myself (since I can't get the nightly tf build on my machine, I can't run the official benchmarks), which is basically one big matrix multiplication in Tensorflow.\nI've run it with square matrices with a width of 8k, 16k and 32k. In each case, FP16 and FP32 yielded nearly the same runtime. Profiling with nvprof I found out that the used CUDA function is maxwell_fp16_segmemm_fp16_128x128_nn for FP16 and sgemm_128x128x8_NN_vec for FP32.\nSince I wanted to double check if matrix multiplication in FP16 is really slower than in FP32 on my GPU, I tried to directly benchmark the GPU using cuBlas with a similar operation. It turns out that here, FP16 is nearly twice as fast as FP32. CuBlas internally uses maxwell_hgemm_256x128_nn for matrix multiplication of 16k x 16k square matrices in FP16. (again  according to the nvprof profiler)\nSo I'm wondering why Tensorflow is unable to achieve similar results in terms of speed, or if I'm doing something wrong in my tests.\nSource code\nTensorflow Code snippet:\n    graph = tf.Graph()\n        with graph.as_default():\n          tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n          tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n          tf_output = tf.matmul(tf_input1, tf_input2)\n  \n      with tf.Session(graph=graph) as session:\n          tf.global_variables_initializer().run()\n          print(\"Initialized\")\n          for i in range(FLAGS.times):\n              out = session.run([tf_output])#, feed_dict=feed_dict)\n          print(\"Done\")\n\ncuBlas Code FP16 (Snippet):\n        uint16_t *d_a;          // d_a - a on the device\n        uint16_t *d_b;          // d_b - b on the device\n        uint16_t *d_c;          // d_c - c on the device\n        cudaStat = cudaMalloc ((void **) &d_a, m * k * sizeof (*a));    // device memory alloc for a\n        cudaStat = cudaMalloc ((void **) &d_b, k * n * sizeof (*b));    // device memory alloc for b\n        cudaStat = cudaMalloc ((void **) &d_c, m * n * sizeof (*c));    // device memory alloc for c\n        stat = cublasCreate (&handle);  // initialize CUBLAS context\n        // copy matrices from the host to the device\n        stat = cublasSetMatrix (m, k, sizeof (*a), a, m, d_a, m);   //a -> d_a\n        stat = cublasSetMatrix (k, n, sizeof (*b), b, k, d_b, k);   //b -> d_b\n        stat = cublasSetMatrix (m, n, sizeof (*c), c, m, d_c, m);   //c -> d_c\n        uint16_t al = FP_16_ONE;       // al = 1 \n        uint16_t bet = FP_16_ONE;      // bet =1\n        // matrix - matrix multiplication : d_c = al*d_a *d_b + bet *d_c\n        // d_a -mxk matrix , d_b -kxn matrix , d_c -mxn matrix ;\n        // al ,bet -scalars\n\n        stat = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &al, d_a, CUDA_R_16F, m, d_b, CUDA_R_16F, k, &bet, d_c, CUDA_R_16F, m, CUDA_R_16F, CUBLAS_GEMM_DEFAULT); \n\n        stat = cublasGetMatrix (m, n, sizeof (*c), d_c, m, c, m);   // cp d_c - >c", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nPartly\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nRHEL 7\r\n- **TensorFlow installed from (source or binary)**:\r\nunknown\r\n- **TensorFlow version (use command below)**:\r\n('unknown', '1.4.0')\r\n- **Python version**: \r\npython2.7\r\n- **Bazel version (if compiling from source)**:\r\nunknown\r\n- **GCC/Compiler version (if compiling from source)**:\r\ngcc 4.8\r\n- **CUDA/cuDNN version**:\r\n9.0 / 7.0\r\n- **GPU model and memory**:\r\nPascal P-100\r\n- **Exact command to reproduce**:\r\nN/A\r\n\r\n### Describe the problem\r\nI tried running the tutorial/rnn/ptb and tutorial/images/mnist examples with the --use_fp16 option, to see if speedup was achievable by utilizing the new half-precision features.\r\nIt turned out that a single training step for MNIST with FP32 took 3.3ms, with FP16 it was 4ms. For PTB small (I had to use lstm_cell=basic, because other types are not yet supported in FP16), the WPS dropped from 24000 to 22000 when switching to FP16.\r\n\r\nSo the performance **decreases** when using FP16 in real-world examples.\r\n\r\nTo check this, I've created a small benchmark for myself (since I can't get the nightly tf build on my machine, I can't run the official benchmarks), which is basically one big matrix multiplication in Tensorflow.\r\n\r\nI've run it with square matrices with a width of 8k, 16k and 32k. In each case, FP16 and FP32 yielded nearly the same runtime. Profiling with nvprof I found out that the used CUDA function is **maxwell_fp16_segmemm_fp16_128x128_nn** for FP16 and **sgemm_128x128x8_NN_vec** for FP32.\r\n\r\nSince I wanted to double check if matrix multiplication in FP16 is really slower than in FP32 on my GPU, I tried to directly benchmark the GPU using cuBlas with a similar operation. It turns out that here, FP16 is nearly twice as fast as FP32. CuBlas internally uses **maxwell_hgemm_256x128_nn** for matrix multiplication of 16k x 16k square matrices in FP16. (again  according to the nvprof profiler)\r\n\r\nSo I'm wondering why Tensorflow is unable to achieve similar results in terms of speed, or if I'm doing something wrong in my tests.\r\n\r\n### Source code\r\n\r\nTensorflow Code snippet:\r\n```\r\n    graph = tf.Graph()\r\n        with graph.as_default():\r\n          tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n          tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n          tf_output = tf.matmul(tf_input1, tf_input2)\r\n  \r\n      with tf.Session(graph=graph) as session:\r\n          tf.global_variables_initializer().run()\r\n          print(\"Initialized\")\r\n          for i in range(FLAGS.times):\r\n              out = session.run([tf_output])#, feed_dict=feed_dict)\r\n          print(\"Done\")\r\n```\r\n\r\n\r\ncuBlas Code FP16 (Snippet):\r\n```\r\n        uint16_t *d_a;          // d_a - a on the device\r\n        uint16_t *d_b;          // d_b - b on the device\r\n        uint16_t *d_c;          // d_c - c on the device\r\n        cudaStat = cudaMalloc ((void **) &d_a, m * k * sizeof (*a));    // device memory alloc for a\r\n        cudaStat = cudaMalloc ((void **) &d_b, k * n * sizeof (*b));    // device memory alloc for b\r\n        cudaStat = cudaMalloc ((void **) &d_c, m * n * sizeof (*c));    // device memory alloc for c\r\n        stat = cublasCreate (&handle);  // initialize CUBLAS context\r\n        // copy matrices from the host to the device\r\n        stat = cublasSetMatrix (m, k, sizeof (*a), a, m, d_a, m);   //a -> d_a\r\n        stat = cublasSetMatrix (k, n, sizeof (*b), b, k, d_b, k);   //b -> d_b\r\n        stat = cublasSetMatrix (m, n, sizeof (*c), c, m, d_c, m);   //c -> d_c\r\n        uint16_t al = FP_16_ONE;       // al = 1 \r\n        uint16_t bet = FP_16_ONE;      // bet =1\r\n        // matrix - matrix multiplication : d_c = al*d_a *d_b + bet *d_c\r\n        // d_a -mxk matrix , d_b -kxn matrix , d_c -mxn matrix ;\r\n        // al ,bet -scalars\r\n\r\n        stat = cublasGemmEx(handle, CUBLAS_OP_N, CUBLAS_OP_N, m, n, k, &al, d_a, CUDA_R_16F, m, d_b, CUDA_R_16F, k, &bet, d_c, CUDA_R_16F, m, CUDA_R_16F, CUBLAS_GEMM_DEFAULT); \r\n\r\n        stat = cublasGetMatrix (m, n, sizeof (*c), d_c, m, c, m);   // cp d_c - >c\r\n```\r\n  "}