{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362096567", "html_url": "https://github.com/tensorflow/tensorflow/issues/15585#issuecomment-362096567", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585", "id": 362096567, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjA5NjU2Nw==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T22:44:41Z", "updated_at": "2018-01-31T22:44:41Z", "author_association": "MEMBER", "body_html": "<p>Looking at <a href=\"https://github.com/tensorflow/tensorflow/blob/de0904e90669d345e0feaa89d1e99b7618fb6677/tensorflow/core/kernels/matmul_op.cc#L388\">matmul_op.cc</a>, it looks like we sometimes do not pass the compute type from <code>TF_FP16_MATMUL_USE_FP32_COMPUTE</code> to cuBLAS. Adding a log statement, this seems to be occurring. /CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> can you comment?</p>\n<p>Looking at tutorials/image/mnist, I'm not sure why fp16 is slower. When I generate Timelines for that example, the overhead of generating a timeline causes both to take the same amount of time, so it's hard to debug this. Since the example is so simple and fp16 has little performance benefit from P100s anywhere, it's not worth spending more time to debug this.</p>", "body_text": "Looking at matmul_op.cc, it looks like we sometimes do not pass the compute type from TF_FP16_MATMUL_USE_FP32_COMPUTE to cuBLAS. Adding a log statement, this seems to be occurring. /CC @yzhwang can you comment?\nLooking at tutorials/image/mnist, I'm not sure why fp16 is slower. When I generate Timelines for that example, the overhead of generating a timeline causes both to take the same amount of time, so it's hard to debug this. Since the example is so simple and fp16 has little performance benefit from P100s anywhere, it's not worth spending more time to debug this.", "body": "Looking at [matmul_op.cc](https://github.com/tensorflow/tensorflow/blob/de0904e90669d345e0feaa89d1e99b7618fb6677/tensorflow/core/kernels/matmul_op.cc#L388), it looks like we sometimes do not pass the compute type from `TF_FP16_MATMUL_USE_FP32_COMPUTE` to cuBLAS. Adding a log statement, this seems to be occurring. /CC @yzhwang can you comment?\r\n\r\nLooking at tutorials/image/mnist, I'm not sure why fp16 is slower. When I generate Timelines for that example, the overhead of generating a timeline causes both to take the same amount of time, so it's hard to debug this. Since the example is so simple and fp16 has little performance benefit from P100s anywhere, it's not worth spending more time to debug this."}