{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/361869800", "html_url": "https://github.com/tensorflow/tensorflow/issues/15585#issuecomment-361869800", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15585", "id": 361869800, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MTg2OTgwMA==", "user": {"login": "KillPinguin", "id": 11447989, "node_id": "MDQ6VXNlcjExNDQ3OTg5", "avatar_url": "https://avatars3.githubusercontent.com/u/11447989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KillPinguin", "html_url": "https://github.com/KillPinguin", "followers_url": "https://api.github.com/users/KillPinguin/followers", "following_url": "https://api.github.com/users/KillPinguin/following{/other_user}", "gists_url": "https://api.github.com/users/KillPinguin/gists{/gist_id}", "starred_url": "https://api.github.com/users/KillPinguin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KillPinguin/subscriptions", "organizations_url": "https://api.github.com/users/KillPinguin/orgs", "repos_url": "https://api.github.com/users/KillPinguin/repos", "events_url": "https://api.github.com/users/KillPinguin/events{/privacy}", "received_events_url": "https://api.github.com/users/KillPinguin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-31T09:13:01Z", "updated_at": "2018-01-31T09:13:01Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a> Thank you for your response.</p>\n<p>I did try to set the environmental variables, but it didn't change anything. The called CUDA functions remained the same, and it had no effect on timing.</p>\n<p>The tutorials/image/mnist example can be found here: <a href=\"https://github.com/tensorflow/models/tree/master/tutorials/image/mnist\">https://github.com/tensorflow/models/tree/master/tutorials/image/mnist</a><br>\nI just used it for trying a prebuilt model with fp16 vs fp32. (again: running with or without the environment variables set didn't change anything)</p>\n<p>My code:</p>\n<pre><code>#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\n\nFLAGS = None\n\ndef get_dtype():\n  return tf.float16 if FLAGS.use_fp16 else tf.float32\n\ndef benchmark():\n  graph = tf.Graph()\n  with graph.as_default():\n    tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n    tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n    tf_output = tf.matmul(tf_input1, tf_input2)\n\n  with tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\"Initialized\")\n    for i in range(FLAGS.times):\n      out = session.run([tf_output])\n    print(\"Done\")\n\ndef parse():\n  global FLAGS\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--use_fp16', default=False, help='use FP16 for benchmarking', action='store_true')\n  parser.add_argument('--size', default=4096, type=int, help='size of matrices to multiply')\n  parser.add_argument('--times', default=10, type=int, help='amount of multiplications')\n  FLAGS, unparsed = parser.parse_known_args()\n\nif __name__ == \"__main__\":\n  parse()\n  benchmark()\n</code></pre>\n<p>What I did:</p>\n<pre><code>export TF_FP16_CONV_USE_FP32_COMPUTE=0\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=0\nnvprof ./Matrixmul.py --use_fp16 --size 16000\n</code></pre>\n<p>Output:</p>\n<pre><code>[...]\n==56407== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\n==56407== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   96.94%  8.16365s        10  816.37ms  814.54ms  819.59ms  maxwell_fp16_sgemm_fp16_128x128_nn\n</code></pre>\n<p>Without the environment variables:</p>\n<pre><code>export TF_FP16_CONV_USE_FP32_COMPUTE=1\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=1\nnvprof ./Matrixmul.py --use_fp16 --size 16000\n</code></pre>\n<p>Output:</p>\n<pre><code>==70743== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\n==70743== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   96.49%  8.17353s        10  817.35ms  814.50ms  824.69ms  maxwell_fp16_sgemm_fp16_128x128_nn\n</code></pre>\n<p>With FP32:</p>\n<pre><code>nvprof ./Matrixmul.py --size 16000\n</code></pre>\n<p>Output:</p>\n<pre><code>[...]\n==89305== Profiling application: python ./Matrixmul.py --size 16000\n==89305== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   93.54%  8.14831s        10  814.83ms  810.68ms  818.11ms  sgemm_128x128x8_NN_vec\n</code></pre>\n<p>Note that <strong>maxwell_fp16_sgmemm_fp16_128x128_nn</strong> uses only FP16 storage but computes in FP32. So why is this function called when explicitly setting the TF_FP16_MATMUL_USE_FP32_COMPUTE to 0?</p>", "body_text": "@reedwm Thank you for your response.\nI did try to set the environmental variables, but it didn't change anything. The called CUDA functions remained the same, and it had no effect on timing.\nThe tutorials/image/mnist example can be found here: https://github.com/tensorflow/models/tree/master/tutorials/image/mnist\nI just used it for trying a prebuilt model with fp16 vs fp32. (again: running with or without the environment variables set didn't change anything)\nMy code:\n#!/usr/bin/env python\n\nfrom __future__ import print_function\n\nimport tensorflow as tf\nimport numpy as np\nimport argparse\n\nFLAGS = None\n\ndef get_dtype():\n  return tf.float16 if FLAGS.use_fp16 else tf.float32\n\ndef benchmark():\n  graph = tf.Graph()\n  with graph.as_default():\n    tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n    tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\n    tf_output = tf.matmul(tf_input1, tf_input2)\n\n  with tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    print(\"Initialized\")\n    for i in range(FLAGS.times):\n      out = session.run([tf_output])\n    print(\"Done\")\n\ndef parse():\n  global FLAGS\n  parser = argparse.ArgumentParser()\n  parser.add_argument('--use_fp16', default=False, help='use FP16 for benchmarking', action='store_true')\n  parser.add_argument('--size', default=4096, type=int, help='size of matrices to multiply')\n  parser.add_argument('--times', default=10, type=int, help='amount of multiplications')\n  FLAGS, unparsed = parser.parse_known_args()\n\nif __name__ == \"__main__\":\n  parse()\n  benchmark()\n\nWhat I did:\nexport TF_FP16_CONV_USE_FP32_COMPUTE=0\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=0\nnvprof ./Matrixmul.py --use_fp16 --size 16000\n\nOutput:\n[...]\n==56407== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\n==56407== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   96.94%  8.16365s        10  816.37ms  814.54ms  819.59ms  maxwell_fp16_sgemm_fp16_128x128_nn\n\nWithout the environment variables:\nexport TF_FP16_CONV_USE_FP32_COMPUTE=1\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=1\nnvprof ./Matrixmul.py --use_fp16 --size 16000\n\nOutput:\n==70743== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\n==70743== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   96.49%  8.17353s        10  817.35ms  814.50ms  824.69ms  maxwell_fp16_sgemm_fp16_128x128_nn\n\nWith FP32:\nnvprof ./Matrixmul.py --size 16000\n\nOutput:\n[...]\n==89305== Profiling application: python ./Matrixmul.py --size 16000\n==89305== Profiling result:\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\n GPU activities:   93.54%  8.14831s        10  814.83ms  810.68ms  818.11ms  sgemm_128x128x8_NN_vec\n\nNote that maxwell_fp16_sgmemm_fp16_128x128_nn uses only FP16 storage but computes in FP32. So why is this function called when explicitly setting the TF_FP16_MATMUL_USE_FP32_COMPUTE to 0?", "body": "@reedwm Thank you for your response.\r\n\r\nI did try to set the environmental variables, but it didn't change anything. The called CUDA functions remained the same, and it had no effect on timing.\r\n\r\nThe tutorials/image/mnist example can be found here: https://github.com/tensorflow/models/tree/master/tutorials/image/mnist\r\nI just used it for trying a prebuilt model with fp16 vs fp32. (again: running with or without the environment variables set didn't change anything)\r\n\r\nMy code:\r\n\r\n```\r\n#!/usr/bin/env python\r\n\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport argparse\r\n\r\nFLAGS = None\r\n\r\ndef get_dtype():\r\n  return tf.float16 if FLAGS.use_fp16 else tf.float32\r\n\r\ndef benchmark():\r\n  graph = tf.Graph()\r\n  with graph.as_default():\r\n    tf_input1 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n    tf_input2 = tf.Variable(tf.truncated_normal([FLAGS.size, FLAGS.size], dtype=get_dtype()))\r\n    tf_output = tf.matmul(tf_input1, tf_input2)\r\n\r\n  with tf.Session(graph=graph) as session:\r\n    tf.global_variables_initializer().run()\r\n    print(\"Initialized\")\r\n    for i in range(FLAGS.times):\r\n      out = session.run([tf_output])\r\n    print(\"Done\")\r\n\r\ndef parse():\r\n  global FLAGS\r\n  parser = argparse.ArgumentParser()\r\n  parser.add_argument('--use_fp16', default=False, help='use FP16 for benchmarking', action='store_true')\r\n  parser.add_argument('--size', default=4096, type=int, help='size of matrices to multiply')\r\n  parser.add_argument('--times', default=10, type=int, help='amount of multiplications')\r\n  FLAGS, unparsed = parser.parse_known_args()\r\n\r\nif __name__ == \"__main__\":\r\n  parse()\r\n  benchmark()\r\n```\r\n\r\nWhat I did:\r\n```\r\nexport TF_FP16_CONV_USE_FP32_COMPUTE=0\r\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=0\r\nnvprof ./Matrixmul.py --use_fp16 --size 16000\r\n```\r\nOutput:\r\n```\r\n[...]\r\n==56407== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\r\n==56407== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   96.94%  8.16365s        10  816.37ms  814.54ms  819.59ms  maxwell_fp16_sgemm_fp16_128x128_nn\r\n```\r\n\r\n\r\nWithout the environment variables:\r\n```\r\nexport TF_FP16_CONV_USE_FP32_COMPUTE=1\r\nexport TF_FP16_MATMUL_USE_FP32_COMPUTE=1\r\nnvprof ./Matrixmul.py --use_fp16 --size 16000\r\n```\r\nOutput:\r\n```\r\n==70743== Profiling application: python ./Matrixmul.py --use_fp16 --size 16000\r\n==70743== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   96.49%  8.17353s        10  817.35ms  814.50ms  824.69ms  maxwell_fp16_sgemm_fp16_128x128_nn\r\n```\r\n\r\n\r\nWith FP32:\r\n```\r\nnvprof ./Matrixmul.py --size 16000\r\n```\r\nOutput:\r\n```\r\n[...]\r\n==89305== Profiling application: python ./Matrixmul.py --size 16000\r\n==89305== Profiling result:\r\n            Type  Time(%)      Time     Calls       Avg       Min       Max  Name\r\n GPU activities:   93.54%  8.14831s        10  814.83ms  810.68ms  818.11ms  sgemm_128x128x8_NN_vec\r\n```\r\n\r\nNote that **maxwell_fp16_sgmemm_fp16_128x128_nn** uses only FP16 storage but computes in FP32. So why is this function called when explicitly setting the TF_FP16_MATMUL_USE_FP32_COMPUTE to 0?"}