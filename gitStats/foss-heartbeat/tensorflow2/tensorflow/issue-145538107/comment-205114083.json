{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/205114083", "html_url": "https://github.com/tensorflow/tensorflow/issues/1757#issuecomment-205114083", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1757", "id": 205114083, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNTExNDA4Mw==", "user": {"login": "johnrowlay", "id": 17486673, "node_id": "MDQ6VXNlcjE3NDg2Njcz", "avatar_url": "https://avatars3.githubusercontent.com/u/17486673?v=4", "gravatar_id": "", "url": "https://api.github.com/users/johnrowlay", "html_url": "https://github.com/johnrowlay", "followers_url": "https://api.github.com/users/johnrowlay/followers", "following_url": "https://api.github.com/users/johnrowlay/following{/other_user}", "gists_url": "https://api.github.com/users/johnrowlay/gists{/gist_id}", "starred_url": "https://api.github.com/users/johnrowlay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/johnrowlay/subscriptions", "organizations_url": "https://api.github.com/users/johnrowlay/orgs", "repos_url": "https://api.github.com/users/johnrowlay/repos", "events_url": "https://api.github.com/users/johnrowlay/events{/privacy}", "received_events_url": "https://api.github.com/users/johnrowlay/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-04T03:04:50Z", "updated_at": "2016-04-04T03:04:50Z", "author_association": "NONE", "body_html": "<p>Thanks for your reply. No, I use a batch size of 1 here, just for debugging; it seems that the error is nothing to do with the training batch, and more to do with reserving room for the weights.</p>\n<p>The batch size is defined in the line <code>batch_xs, batch_ys = mnist.train.next_batch(1)</code></p>\n<p>I'm a little confused where the tensor with <code>shape[10000,23000]</code> comes from... The weights in the first layer should be [724,23000], and in the second layer they should be [23000,10].</p>\n<p>Even still, it seems strange that this relatively small network, with a batch size of 1, will cause a memory issue....</p>", "body_text": "Thanks for your reply. No, I use a batch size of 1 here, just for debugging; it seems that the error is nothing to do with the training batch, and more to do with reserving room for the weights.\nThe batch size is defined in the line batch_xs, batch_ys = mnist.train.next_batch(1)\nI'm a little confused where the tensor with shape[10000,23000] comes from... The weights in the first layer should be [724,23000], and in the second layer they should be [23000,10].\nEven still, it seems strange that this relatively small network, with a batch size of 1, will cause a memory issue....", "body": "Thanks for your reply. No, I use a batch size of 1 here, just for debugging; it seems that the error is nothing to do with the training batch, and more to do with reserving room for the weights.\n\nThe batch size is defined in the line `batch_xs, batch_ys = mnist.train.next_batch(1)`\n\nI'm a little confused where the tensor with `shape[10000,23000]` comes from... The weights in the first layer should be [724,23000], and in the second layer they should be [23000,10].\n\nEven still, it seems strange that this relatively small network, with a batch size of 1, will cause a memory issue....\n"}