{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10433", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10433/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10433/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10433/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10433", "id": 233493195, "node_id": "MDU6SXNzdWUyMzM0OTMxOTU=", "number": 10433, "title": "BeamsearchDecoder w/ AttentionWrapper ", "user": {"login": "JayParks", "id": 6487834, "node_id": "MDQ6VXNlcjY0ODc4MzQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/6487834?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JayParks", "html_url": "https://github.com/JayParks", "followers_url": "https://api.github.com/users/JayParks/followers", "following_url": "https://api.github.com/users/JayParks/following{/other_user}", "gists_url": "https://api.github.com/users/JayParks/gists{/gist_id}", "starred_url": "https://api.github.com/users/JayParks/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JayParks/subscriptions", "organizations_url": "https://api.github.com/users/JayParks/orgs", "repos_url": "https://api.github.com/users/JayParks/repos", "events_url": "https://api.github.com/users/JayParks/events{/privacy}", "received_events_url": "https://api.github.com/users/JayParks/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-06-05T04:42:30Z", "updated_at": "2017-06-05T17:32:49Z", "closed_at": "2017-06-05T17:32:49Z", "author_association": "NONE", "body_html": "<p>Ubuntu 16.04<br>\nTF version; 1.2.0.rc1</p>\n<p>Hi Team, I'm implementing seq2seq framework using BeamSearchDecoder and got an error when using with AttentionWrapper that has no predefined time-steps.</p>\n<p>Here's my rough code</p>\n<hr>\n<p>....<br>\nself.encoder_cell = build_encoder_cell()<br>\nself.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='encoder_inputs')<br>\nself.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_inputs_length')<br>\n....<br>\nself.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(<br>\ncell=self.encoder_cell, inputs=self.encoder_inputs_embedded,<br>\nsequence_length=self.encoder_inputs_length, dtype=self.dtype,<br>\ntime_major=False)<br>\n....<br>\nself.decoder_cell = build_decoder_cell()<br>\nself.decoder_cell = seq2seq.AttentionWrapper(<br>\ncell=self.decoder_cell,<br>\nattention_mechanism=self.attention_mechanism,<br>\nattention_layer_size=self.hidden_units,<br>\ncell_input_fn=attn_decoder_input_fn,<br>\ninitial_cell_state=encoder_last_state,<br>\nalignment_history=False,<br>\nname='Attention_Wrapper')</p>\n<p>tiled_batch_size = self.batch_size * self.beam_width<br>\nself.decoder_initial_state = self.decoder_cell.zero_state(batch_size=tiled_batch_size)<br>\n....<br>\ninference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,<br>\nembedding=embed_and_input_proj,<br>\nstart_tokens=start_tokens,<br>\nend_token=end_token,<br>\ninitial_state=self.decoder_initial_state,<br>\nbeam_width=self.beam_width,<br>\noutput_layer=output_layer,)</p>\n<hr>\n<p>running the code made an error as below,</p>\n<hr>\n<p>/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in <strong>init</strong>(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)<br>\n173     self._initial_cell_state = nest.map_structure(<br>\n174         self._maybe_split_batch_beams,<br>\n--&gt; 175         initial_state, self._cell.state_size)<br>\n176     self._start_tokens = array_ops.tile(<br>\n177         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.pyc in map_structure(func, *structure, **check_types_dict)<br>\n323<br>\n324   return pack_sequence_as(<br>\n--&gt; 325       structure[0], [func(*x) for x in entries])<br>\n326<br>\n327</p>\n<p>/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _maybe_split_batch_beams(self, t, s)<br>\n363       print t<br>\n364       print s<br>\n--&gt; 365       return self._split_batch_beams(t, s)<br>\n366     else:<br>\n367       return t</p>\n<p>/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _split_batch_beams(self, t, s)<br>\n314     print s<br>\n315     if isinstance(s, ops.Tensor):<br>\n--&gt; 316       s = tensor_util.constant_value_as_shape(s)<br>\n317     else:<br>\n318       s = tensor_shape.TensorShape(s)</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in constant_value_as_shape(tensor)<br>\n732     A <code>TensorShape</code> based on the constant value of the given <code>tensor</code>.<br>\n733   \"\"\"<br>\n--&gt; 734   shape = tensor.get_shape().with_rank(1)<br>\n735   if tensor.get_shape() == [0]:<br>\n736     return tensor_shape.scalar()</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in with_rank(self, rank)<br>\n630       return self.merge_with(unknown_shape(ndims=rank))<br>\n631     except ValueError:<br>\n--&gt; 632       raise ValueError(\"Shape %s must have rank %d\" % (self, rank))<br>\n633<br>\n634   def with_rank_at_least(self, rank):</p>\n<p>ValueError: Shape () must have rank 1</p>\n<hr>\n<p>I added a log to track the tensor argument (t) and the shape argument (s) of the _split_batch_beams function<br>\nIn the log, 1024 is the size of hidden units. batch_size is 80 and beam_width is 12. For each cell, I used tf.contrib.rnn.LSTMCell()</p>\n<hr>\n<p>Tensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32) (t)<br>\n1024 (s)<br>\n1024 (s)<br>\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)<br>\n1024<br>\n1024<br>\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)<br>\n1024<br>\n1024<br>\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, ?), dtype=float32)<br>\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)<br>\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)</p>\n<hr>\n<p>It turned out that the reason of the error was 'None' argument in the second dimension of self.encoder_inputs placeholder (which denotes max-time-steps).<br>\nI implemented self.encoder_inputs placeholder to have (None, None) shape to support dynamic batch_size and time-steps of input feeds.</p>\n<p>When i hardcoded max-time-steps to be 80, the code worked well.<br>\n(self.encoder_inputs = tf.placeholder(tf.int32, shape=(None, \"\"80\"\")),</p>\n<p>The input argument of _split_batch_beams was like this</p>\n<p>Tensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32)<br>\n1024<br>\n1024<br>\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)<br>\n1024<br>\n1024<br>\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)<br>\n1024<br>\n1024<br>\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, \"\"80\"\"), dtype=float32)<br>\n\"\"80\"\"<br>\n\"\"80\"\"</p>\n<hr>\n<p>Also the error did not occur if i used normal decoder cell (not with AttentionWrapper)<br>\nI'm wondering if this is a bug or a strict design rule for BeamSearchDecoder to support only AttentionWrapper with static_time_steps</p>", "body_text": "Ubuntu 16.04\nTF version; 1.2.0.rc1\nHi Team, I'm implementing seq2seq framework using BeamSearchDecoder and got an error when using with AttentionWrapper that has no predefined time-steps.\nHere's my rough code\n\n....\nself.encoder_cell = build_encoder_cell()\nself.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='encoder_inputs')\nself.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\n....\nself.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(\ncell=self.encoder_cell, inputs=self.encoder_inputs_embedded,\nsequence_length=self.encoder_inputs_length, dtype=self.dtype,\ntime_major=False)\n....\nself.decoder_cell = build_decoder_cell()\nself.decoder_cell = seq2seq.AttentionWrapper(\ncell=self.decoder_cell,\nattention_mechanism=self.attention_mechanism,\nattention_layer_size=self.hidden_units,\ncell_input_fn=attn_decoder_input_fn,\ninitial_cell_state=encoder_last_state,\nalignment_history=False,\nname='Attention_Wrapper')\ntiled_batch_size = self.batch_size * self.beam_width\nself.decoder_initial_state = self.decoder_cell.zero_state(batch_size=tiled_batch_size)\n....\ninference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,\nembedding=embed_and_input_proj,\nstart_tokens=start_tokens,\nend_token=end_token,\ninitial_state=self.decoder_initial_state,\nbeam_width=self.beam_width,\noutput_layer=output_layer,)\n\nrunning the code made an error as below,\n\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in init(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)\n173     self._initial_cell_state = nest.map_structure(\n174         self._maybe_split_batch_beams,\n--> 175         initial_state, self._cell.state_size)\n176     self._start_tokens = array_ops.tile(\n177         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.pyc in map_structure(func, *structure, **check_types_dict)\n323\n324   return pack_sequence_as(\n--> 325       structure[0], [func(*x) for x in entries])\n326\n327\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _maybe_split_batch_beams(self, t, s)\n363       print t\n364       print s\n--> 365       return self._split_batch_beams(t, s)\n366     else:\n367       return t\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _split_batch_beams(self, t, s)\n314     print s\n315     if isinstance(s, ops.Tensor):\n--> 316       s = tensor_util.constant_value_as_shape(s)\n317     else:\n318       s = tensor_shape.TensorShape(s)\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in constant_value_as_shape(tensor)\n732     A TensorShape based on the constant value of the given tensor.\n733   \"\"\"\n--> 734   shape = tensor.get_shape().with_rank(1)\n735   if tensor.get_shape() == [0]:\n736     return tensor_shape.scalar()\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in with_rank(self, rank)\n630       return self.merge_with(unknown_shape(ndims=rank))\n631     except ValueError:\n--> 632       raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\n633\n634   def with_rank_at_least(self, rank):\nValueError: Shape () must have rank 1\n\nI added a log to track the tensor argument (t) and the shape argument (s) of the _split_batch_beams function\nIn the log, 1024 is the size of hidden units. batch_size is 80 and beam_width is 12. For each cell, I used tf.contrib.rnn.LSTMCell()\n\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32) (t)\n1024 (s)\n1024 (s)\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)\n1024\n1024\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)\n1024\n1024\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, ?), dtype=float32)\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)\n\nIt turned out that the reason of the error was 'None' argument in the second dimension of self.encoder_inputs placeholder (which denotes max-time-steps).\nI implemented self.encoder_inputs placeholder to have (None, None) shape to support dynamic batch_size and time-steps of input feeds.\nWhen i hardcoded max-time-steps to be 80, the code worked well.\n(self.encoder_inputs = tf.placeholder(tf.int32, shape=(None, \"\"80\"\")),\nThe input argument of _split_batch_beams was like this\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32)\n1024\n1024\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)\n1024\n1024\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)\n1024\n1024\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, \"\"80\"\"), dtype=float32)\n\"\"80\"\"\n\"\"80\"\"\n\nAlso the error did not occur if i used normal decoder cell (not with AttentionWrapper)\nI'm wondering if this is a bug or a strict design rule for BeamSearchDecoder to support only AttentionWrapper with static_time_steps", "body": "Ubuntu 16.04\r\nTF version; 1.2.0.rc1\r\n\r\nHi Team, I'm implementing seq2seq framework using BeamSearchDecoder and got an error when using with AttentionWrapper that has no predefined time-steps.\r\n\r\nHere's my rough code\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n....\r\nself.encoder_cell = build_encoder_cell()\r\nself.encoder_inputs = tf.placeholder(dtype=tf.int32, shape=(None, None), name='encoder_inputs')\r\nself.encoder_inputs_length = tf.placeholder(dtype=tf.int32, shape=(None,), name='encoder_inputs_length')\r\n....\r\nself.encoder_outputs, self.encoder_last_state = tf.nn.dynamic_rnn(\r\n                cell=self.encoder_cell, inputs=self.encoder_inputs_embedded,\r\n                sequence_length=self.encoder_inputs_length, dtype=self.dtype,\r\n                time_major=False)\r\n....\r\nself.decoder_cell = build_decoder_cell()\r\nself.decoder_cell = seq2seq.AttentionWrapper(\r\n            cell=self.decoder_cell,\r\n            attention_mechanism=self.attention_mechanism,\r\n            attention_layer_size=self.hidden_units,\r\n            cell_input_fn=attn_decoder_input_fn,\r\n            initial_cell_state=encoder_last_state,\r\n            alignment_history=False,\r\n            name='Attention_Wrapper')\r\n\r\ntiled_batch_size = self.batch_size * self.beam_width\r\nself.decoder_initial_state = self.decoder_cell.zero_state(batch_size=tiled_batch_size)\r\n....\r\ninference_decoder = beam_search_decoder.BeamSearchDecoder(cell=self.decoder_cell,\r\n                                                               embedding=embed_and_input_proj,\r\n                                                               start_tokens=start_tokens,\r\n                                                               end_token=end_token,\r\n                                                               initial_state=self.decoder_initial_state,\r\n                                                               beam_width=self.beam_width,\r\n                                                               output_layer=output_layer,)\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nrunning the code made an error as below,\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in __init__(self, cell, embedding, start_tokens, end_token, initial_state, beam_width, output_layer, length_penalty_weight)\r\n    173     self._initial_cell_state = nest.map_structure(\r\n    174         self._maybe_split_batch_beams,\r\n--> 175         initial_state, self._cell.state_size)\r\n    176     self._start_tokens = array_ops.tile(\r\n    177         array_ops.expand_dims(self._start_tokens, 1), [1, self._beam_width])\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/nest.pyc in map_structure(func, *structure, **check_types_dict)\r\n    323 \r\n    324   return pack_sequence_as(\r\n--> 325       structure[0], [func(*x) for x in entries])\r\n    326 \r\n    327 \r\n\r\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _maybe_split_batch_beams(self, t, s)\r\n    363       print t\r\n    364       print s\r\n--> 365       return self._split_batch_beams(t, s)\r\n    366     else:\r\n    367       return t\r\n\r\n/home/pjh/ml_tutorials/tensorflow/tf-nmt/beam_search_decoder.pyc in _split_batch_beams(self, t, s)\r\n    314     print s\r\n    315     if isinstance(s, ops.Tensor):\r\n--> 316       s = tensor_util.constant_value_as_shape(s)\r\n    317     else:\r\n    318       s = tensor_shape.TensorShape(s)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_util.pyc in constant_value_as_shape(tensor)\r\n    732     A `TensorShape` based on the constant value of the given `tensor`.\r\n    733   \"\"\"\r\n--> 734   shape = tensor.get_shape().with_rank(1)\r\n    735   if tensor.get_shape() == [0]:\r\n    736     return tensor_shape.scalar()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.pyc in with_rank(self, rank)\r\n    630       return self.merge_with(unknown_shape(ndims=rank))\r\n    631     except ValueError:\r\n--> 632       raise ValueError(\"Shape %s must have rank %d\" % (self, rank))\r\n    633 \r\n    634   def with_rank_at_least(self, rank):\r\n\r\nValueError: Shape () must have rank 1\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nI added a log to track the tensor argument (t) and the shape argument (s) of the _split_batch_beams function\r\nIn the log, 1024 is the size of hidden units. batch_size is 80 and beam_width is 12. For each cell, I used tf.contrib.rnn.LSTMCell()\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32) (t)\r\n1024 (s) \r\n1024 (s)\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, ?), dtype=float32)\r\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)\r\nTensor(\"decoder/LuongAttention/strided_slice_2:0\", shape=(), dtype=int32)\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\n\r\nIt turned out that the reason of the error was 'None' argument in the second dimension of self.encoder_inputs placeholder (which denotes max-time-steps).\r\nI implemented self.encoder_inputs placeholder to have (None, None) shape to support dynamic batch_size and time-steps of input feeds.\r\n\r\nWhen i hardcoded max-time-steps to be 80, the code worked well.\r\n(self.encoder_inputs = tf.placeholder(tf.int32, shape=(None, \"\"80\"\")), \r\n\r\nThe input argument of _split_batch_beams was like this\r\n\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state:0\", shape=(?, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/checked_cell_state_1:0\", shape=(?, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_1:0\", shape=(960, 1024), dtype=float32)\r\n1024\r\n1024\r\nTensor(\"decoder/AttentionWrapperZeroState/zeros_2:0\", shape=(960, \"\"80\"\"), dtype=float32)\r\n\"\"80\"\"\r\n\"\"80\"\"\r\n\r\n--------------------------------------------------------------------------------------------------------------------------\r\nAlso the error did not occur if i used normal decoder cell (not with AttentionWrapper)\r\nI'm wondering if this is a bug or a strict design rule for BeamSearchDecoder to support only AttentionWrapper with static_time_steps "}