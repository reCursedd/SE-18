{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155502573", "html_url": "https://github.com/tensorflow/tensorflow/issues/63#issuecomment-155502573", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/63", "id": 155502573, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTUwMjU3Mw==", "user": {"login": "an-kumar", "id": 13493636, "node_id": "MDQ6VXNlcjEzNDkzNjM2", "avatar_url": "https://avatars0.githubusercontent.com/u/13493636?v=4", "gravatar_id": "", "url": "https://api.github.com/users/an-kumar", "html_url": "https://github.com/an-kumar", "followers_url": "https://api.github.com/users/an-kumar/followers", "following_url": "https://api.github.com/users/an-kumar/following{/other_user}", "gists_url": "https://api.github.com/users/an-kumar/gists{/gist_id}", "starred_url": "https://api.github.com/users/an-kumar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/an-kumar/subscriptions", "organizations_url": "https://api.github.com/users/an-kumar/orgs", "repos_url": "https://api.github.com/users/an-kumar/repos", "events_url": "https://api.github.com/users/an-kumar/events{/privacy}", "received_events_url": "https://api.github.com/users/an-kumar/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-10T17:36:45Z", "updated_at": "2015-11-10T17:37:20Z", "author_association": "NONE", "body_html": "<p>yeah -- say i want to do, W1 * x1 + W2 * x2, where x1 and x2 are the outputs of respective subgraphs. It is more efficient to specifically tell each subgraph to write x1 and x2 to a contiguous block of memory, such that W1 * x1 + W2 * x2 becomes W * [x1 x2]. This comes up in complicated recurrent networks, some video-based convolutional networks, etc</p>\n<p>I was looking at the code and I see that the op kernels themselves do take outputs to fill. Is it hard to expose that to the python interface? if not, could you maybe tell me how to start and I look into it myself?</p>", "body_text": "yeah -- say i want to do, W1 * x1 + W2 * x2, where x1 and x2 are the outputs of respective subgraphs. It is more efficient to specifically tell each subgraph to write x1 and x2 to a contiguous block of memory, such that W1 * x1 + W2 * x2 becomes W * [x1 x2]. This comes up in complicated recurrent networks, some video-based convolutional networks, etc\nI was looking at the code and I see that the op kernels themselves do take outputs to fill. Is it hard to expose that to the python interface? if not, could you maybe tell me how to start and I look into it myself?", "body": "yeah -- say i want to do, W1 \\* x1 + W2 \\* x2, where x1 and x2 are the outputs of respective subgraphs. It is more efficient to specifically tell each subgraph to write x1 and x2 to a contiguous block of memory, such that W1 \\* x1 + W2 \\* x2 becomes W \\* [x1 x2]. This comes up in complicated recurrent networks, some video-based convolutional networks, etc\n\nI was looking at the code and I see that the op kernels themselves do take outputs to fill. Is it hard to expose that to the python interface? if not, could you maybe tell me how to start and I look into it myself?\n"}