{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/155528552", "html_url": "https://github.com/tensorflow/tensorflow/issues/63#issuecomment-155528552", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/63", "id": 155528552, "node_id": "MDEyOklzc3VlQ29tbWVudDE1NTUyODU1Mg==", "user": {"login": "jendap", "id": 567848, "node_id": "MDQ6VXNlcjU2Nzg0OA==", "avatar_url": "https://avatars0.githubusercontent.com/u/567848?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jendap", "html_url": "https://github.com/jendap", "followers_url": "https://api.github.com/users/jendap/followers", "following_url": "https://api.github.com/users/jendap/following{/other_user}", "gists_url": "https://api.github.com/users/jendap/gists{/gist_id}", "starred_url": "https://api.github.com/users/jendap/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jendap/subscriptions", "organizations_url": "https://api.github.com/users/jendap/orgs", "repos_url": "https://api.github.com/users/jendap/repos", "events_url": "https://api.github.com/users/jendap/events{/privacy}", "received_events_url": "https://api.github.com/users/jendap/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-10T18:49:46Z", "updated_at": "2015-11-10T18:49:46Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ok, I understand what you are trying to do. But I'm not sure why would you want to do it from python. Python is great high level abstraction to express your algorithm. The memory layout has to be under control of execution framework (C++ code in tf). Don't worry it is doing a good job there. And certainly will do even better in future.</p>\n<p>If you really want to do it... I don't think it is currently exposed in python. But take a look at swig wrappers. There is already some swig infrastructure in place. You can expose a lot more (all of it) in swig wrappers and you will have access to everything.</p>\n<p>BTW: Just the example you say sounds like premature optimization you may not want to do by hand. Think you it. You will have 2 GPUs. Tensorflow will compute those two subgraphs outputs one on each GPU. x1 on gpu0 and x2 on gpu1. It may already have there even the weights - W1 on gpu0 and W2 on gpu1. It can do those two multiplications on the two gpus and just add the result. If you try manually force creating one vector W and one X you will spend a lot of time copying a lot of data. And that is slow.</p>", "body_text": "Ok, I understand what you are trying to do. But I'm not sure why would you want to do it from python. Python is great high level abstraction to express your algorithm. The memory layout has to be under control of execution framework (C++ code in tf). Don't worry it is doing a good job there. And certainly will do even better in future.\nIf you really want to do it... I don't think it is currently exposed in python. But take a look at swig wrappers. There is already some swig infrastructure in place. You can expose a lot more (all of it) in swig wrappers and you will have access to everything.\nBTW: Just the example you say sounds like premature optimization you may not want to do by hand. Think you it. You will have 2 GPUs. Tensorflow will compute those two subgraphs outputs one on each GPU. x1 on gpu0 and x2 on gpu1. It may already have there even the weights - W1 on gpu0 and W2 on gpu1. It can do those two multiplications on the two gpus and just add the result. If you try manually force creating one vector W and one X you will spend a lot of time copying a lot of data. And that is slow.", "body": "Ok, I understand what you are trying to do. But I'm not sure why would you want to do it from python. Python is great high level abstraction to express your algorithm. The memory layout has to be under control of execution framework (C++ code in tf). Don't worry it is doing a good job there. And certainly will do even better in future. \n\nIf you really want to do it... I don't think it is currently exposed in python. But take a look at swig wrappers. There is already some swig infrastructure in place. You can expose a lot more (all of it) in swig wrappers and you will have access to everything.\n\nBTW: Just the example you say sounds like premature optimization you may not want to do by hand. Think you it. You will have 2 GPUs. Tensorflow will compute those two subgraphs outputs one on each GPU. x1 on gpu0 and x2 on gpu1. It may already have there even the weights - W1 on gpu0 and W2 on gpu1. It can do those two multiplications on the two gpus and just add the result. If you try manually force creating one vector W and one X you will spend a lot of time copying a lot of data. And that is slow.\n"}