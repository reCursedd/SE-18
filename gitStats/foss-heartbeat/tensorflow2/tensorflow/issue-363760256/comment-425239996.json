{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425239996", "html_url": "https://github.com/tensorflow/tensorflow/issues/22511#issuecomment-425239996", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22511", "id": 425239996, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTIzOTk5Ng==", "user": {"login": "terrykong", "id": 7576060, "node_id": "MDQ6VXNlcjc1NzYwNjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7576060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/terrykong", "html_url": "https://github.com/terrykong", "followers_url": "https://api.github.com/users/terrykong/followers", "following_url": "https://api.github.com/users/terrykong/following{/other_user}", "gists_url": "https://api.github.com/users/terrykong/gists{/gist_id}", "starred_url": "https://api.github.com/users/terrykong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/terrykong/subscriptions", "organizations_url": "https://api.github.com/users/terrykong/orgs", "repos_url": "https://api.github.com/users/terrykong/repos", "events_url": "https://api.github.com/users/terrykong/events{/privacy}", "received_events_url": "https://api.github.com/users/terrykong/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-27T20:55:17Z", "updated_at": "2018-09-27T20:55:17Z", "author_association": "NONE", "body_html": "<p>Cool, so I've tried my best to create a self contained working example but it isn't working and I'm not quite sure what's wrong. Here is what I'm running (this is a box with two GPUs):</p>\n<ol>\n<li>Running <code>CUDA_VISIBLE_DEVICES=0 python train.py --task_type=worker --task_index=0 &amp;</code></li>\n<li>Running <code>CUDA_VISIBLE_DEVICES=1 python train.py --task_type=worker --task_index=1 &amp;</code></li>\n<li>Running <code>CUDA_VISIBLE_DEVICES=-1 python train.py --task_type=chief --task_index=0</code></li>\n</ol>\n<pre><code>Traceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 69, in get\n    return self._index[device]\nKeyError: '/replica:0/task:0/device:CPU:0'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"train.py\", line 165, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"train.py\", line 154, in main\n    master=server.target) as sess:\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 195, in finalize\n    default_ready_op)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 265, in get_or_default\n    op = default_constructor()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 190, in default_ready_op\n    variables.report_uninitialized_variables(),\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in report_uninitialized_variables\n    [s.op.name for s in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in &lt;listcomp&gt;\n    [s.op.name for s in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 305, in op\n    return self.get().op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 73, in get\n    (device, self._index.keys(), device_util.current())), e)\n  File \"&lt;string&gt;\", line 3, in raise_from\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/job:chief/replica:0/task:0/device:GPU:0']) (current device /device:CPU:0)\n</code></pre>\n<p>Here's the training script:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> tensorflow.contrib <span class=\"pl-k\">as</span> tfc\n<span class=\"pl-k\">from</span> enum <span class=\"pl-k\">import</span> Enum\n<span class=\"pl-k\">import</span> json\n\nflags <span class=\"pl-k\">=</span> tf.flags\n\nflags.DEFINE_integer(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task_index<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task_index<span class=\"pl-pds\">\"</span></span>)\nflags.DEFINE_enum(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task_type<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">None</span>, (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>chief<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>worker<span class=\"pl-pds\">'</span></span>), <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task type<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c1\">FLAGS</span> <span class=\"pl-k\">=</span> flags.<span class=\"pl-c1\">FLAGS</span>\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Mode</span>(<span class=\"pl-e\">Enum</span>):\n    <span class=\"pl-c1\">TRAINING</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-c1\">VALIDATING</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>\n    <span class=\"pl-c1\">INFERENCE</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">3</span>\n\ntf.set_random_seed(<span class=\"pl-c1\">4</span>)\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">4</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">get_dataset</span>(<span class=\"pl-smi\">x</span>,<span class=\"pl-smi\">y</span>):\n    dataset1 <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(x).batch(batch_size).map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: tf.reshape(x,[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>]))\n    dataset2 <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(y).batch(batch_size).map(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: tf.reshape(x,[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>]))\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.zip((dataset1,dataset2))\n    \n    <span class=\"pl-k\">return</span> dataset\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Model</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">mode</span>, <span class=\"pl-smi\">iterator</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>):\n        <span class=\"pl-c1\">self</span>.task_index <span class=\"pl-k\">=</span> task_index\n        <span class=\"pl-c1\">self</span>.iterator <span class=\"pl-k\">=</span> iterator\n        <span class=\"pl-c1\">self</span>.distribution <span class=\"pl-k\">=</span> tf.contrib.distribute.get_distribution_strategy()\n        \n        <span class=\"pl-c1\">self</span>.input_data <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input_data<span class=\"pl-pds\">\"</span></span>)\n        \n        <span class=\"pl-k\">if</span> mode <span class=\"pl-k\">==</span> Mode.<span class=\"pl-c1\">TRAINING</span>:\n            tower_train_ops <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.distribution.call_for_each_tower(<span class=\"pl-c1\">self</span>.tower_loss_and_backprop, <span class=\"pl-c1\">self</span>.iterator.get_next(), <span class=\"pl-c1\">True</span>)\n            <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> tf.group(<span class=\"pl-c1\">self</span>.distribution.unwrap(tower_train_ops))\n            \n        <span class=\"pl-k\">elif</span> mode <span class=\"pl-k\">==</span> Mode.<span class=\"pl-c1\">VALIDATING</span>:\n            total_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.distribution.call_for_each_tower(<span class=\"pl-c1\">self</span>.tower_loss, <span class=\"pl-k\">*</span><span class=\"pl-c1\">self</span>.iterator.get_next())\n            <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.distribution.unwrap(<span class=\"pl-c1\">self</span>.distribution.reduce(tf.VariableAggregation.<span class=\"pl-c1\">SUM</span>, total_loss, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/device:GPU:0<span class=\"pl-pds\">\"</span></span>))[<span class=\"pl-c1\">0</span>]\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.tower_loss(<span class=\"pl-c1\">self</span>.input_data)\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">tower_loss</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">labels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        <span class=\"pl-c1\">self</span>.w <span class=\"pl-k\">=</span> w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>,<span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>))\n        <span class=\"pl-c1\">self</span>.b <span class=\"pl-k\">=</span> b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>,<span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>), <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>))\n        \n        <span class=\"pl-c1\">self</span>.y_pred <span class=\"pl-k\">=</span> tf.matmul(inputs,w) <span class=\"pl-k\">+</span> b\n        <span class=\"pl-k\">if</span> labels <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">None</span>\n        loss <span class=\"pl-k\">=</span> (labels <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.y_pred)<span class=\"pl-k\">**</span><span class=\"pl-c1\">2</span>\n        \n        <span class=\"pl-k\">return</span> tf.reduce_sum(loss)\n    \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">tower_loss_and_backprop</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">iter_get_next</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n        inputs, labels <span class=\"pl-k\">=</span> iter_get_next\n        \n        <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.tower_loss(inputs,labels)\n        \n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> is_training:\n            <span class=\"pl-k\">return</span> <span class=\"pl-c1\">None</span>\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(<span class=\"pl-c1\">0.01</span>)\n        gradients, variables <span class=\"pl-k\">=</span> <span class=\"pl-c1\">zip</span>(<span class=\"pl-k\">*</span>optimizer.compute_gradients(<span class=\"pl-c1\">self</span>.loss))\n        gradients, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(gradients, <span class=\"pl-c1\">5.0</span>)\n        <span class=\"pl-k\">with</span> tf.variable_scope(tf.get_variable_scope(), <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>tf.<span class=\"pl-c1\">AUTO_REUSE</span>):\n            train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(gradients, variables), <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>tf.train.get_or_create_global_step())\n        \n        <span class=\"pl-k\">return</span> train_op\n        \n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>distribution = tfc.distribute.OneDeviceStrategy('device:CPU:0')</span>\n    distribution <span class=\"pl-k\">=</span> tfc.distribute.CollectiveAllReduceStrategy(<span class=\"pl-v\">num_gpus_per_worker</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    \n    worker_hosts <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>localhost:1111<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>localhost:1112<span class=\"pl-pds\">'</span></span>]\n    chief_host <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>localhost:1113<span class=\"pl-pds\">'</span></span>]\n    cluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: worker_hosts, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>chief<span class=\"pl-pds\">\"</span></span>: chief_host})\n    task_type <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.task_type\n    task_index <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.task_index\n    is_chief <span class=\"pl-k\">=</span> <span class=\"pl-c1\">FLAGS</span>.task_type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>chief<span class=\"pl-pds\">\"</span></span>\n    \n    sess_config <span class=\"pl-k\">=</span> tf.ConfigProto(\n                    <span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                    <span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Choose existing device to run ops in case specified one doesn't exist</span>\n                )\n    sess_config.gpu_options.allow_growth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> This is to avoid using all RAM and to only use what it needs</span>\n    \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> If this is a worker, then set environment variable and join</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> is_chief:\n        os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TF_CONFIG<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> json.dumps({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cluster<span class=\"pl-pds\">\"</span></span>:\n                                                  {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>worker<span class=\"pl-pds\">\"</span></span>: worker_hosts, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>chief<span class=\"pl-pds\">\"</span></span>: chief_host},\n                                              <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>task<span class=\"pl-pds\">\"</span></span>:\n                                                  {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>type<span class=\"pl-pds\">\"</span></span>: task_type,<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>index<span class=\"pl-pds\">\"</span></span>: task_index}\n                                              })\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TF_CONFIG=<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> os.environ[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>TF_CONFIG<span class=\"pl-pds\">'</span></span>])\n        tf.contrib.distribute.run_standard_tensorflow_server(sess_config).join()\n    \n    server <span class=\"pl-k\">=</span> tf.train.Server(\n            cluster, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span>task_type, <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span>task_index)\n    \n    <span class=\"pl-k\">if</span> task_type <span class=\"pl-k\">and</span> task_index <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">type</span>(distribution) <span class=\"pl-k\">==</span> tfc.distribute.OneDeviceStrategy:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Using OneDeviceStrategy<span class=\"pl-pds\">'</span></span>)\n            distribution.configure(<span class=\"pl-v\">session_config</span><span class=\"pl-k\">=</span>sess_config)\n        <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">type</span>(distribution) <span class=\"pl-k\">==</span> tfc.distribute.CollectiveAllReduceStrategy:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Using CollectiveAllReduceStrategy<span class=\"pl-pds\">'</span></span>)\n            distribution.configure(<span class=\"pl-v\">session_config</span><span class=\"pl-k\">=</span>sess_config,\n                                   <span class=\"pl-v\">cluster_spec</span><span class=\"pl-k\">=</span>cluster,\n                                   <span class=\"pl-v\">task_type</span><span class=\"pl-k\">=</span>task_type,\n                                   <span class=\"pl-v\">task_id</span><span class=\"pl-k\">=</span>task_index)\n    \n    foo <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">a</span>: <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: <span class=\"pl-c1\">3.0</span><span class=\"pl-k\">*</span>x <span class=\"pl-k\">+</span> <span class=\"pl-c1\">5.0</span>, a))\n    x_train <span class=\"pl-k\">=</span> [i<span class=\"pl-k\">*</span><span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-k\">-</span><span class=\"pl-c1\">100</span>,<span class=\"pl-c1\">100</span>)]\n    x_valid <span class=\"pl-k\">=</span> [i<span class=\"pl-k\">*</span><span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-k\">-</span><span class=\"pl-c1\">10</span>,<span class=\"pl-c1\">10</span>)]\n    \n    <span class=\"pl-k\">with</span> distribution.scope():\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Train<span class=\"pl-pds\">'</span></span>):\n            train_dataset <span class=\"pl-k\">=</span> get_dataset(x_train, foo(x_train))\n            train_iterator <span class=\"pl-k\">=</span> distribution.distribute_dataset(<span class=\"pl-k\">lambda</span>: train_dataset).make_initializable_iterator()\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Model<span class=\"pl-pds\">'</span></span>):\n                m <span class=\"pl-k\">=</span> Model(Mode.<span class=\"pl-c1\">TRAINING</span>, train_iterator)\n        \n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Valid<span class=\"pl-pds\">'</span></span>):\n            valid_dataset <span class=\"pl-k\">=</span> get_dataset(x_valid, foo(x_valid))\n            valid_iterator <span class=\"pl-k\">=</span> distribution.distribute_dataset(<span class=\"pl-k\">lambda</span>: valid_dataset).make_initializable_iterator()\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Model<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n                m_valid <span class=\"pl-k\">=</span> Model(Mode.<span class=\"pl-c1\">VALIDATING</span>, valid_iterator)\n        \n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Serving<span class=\"pl-pds\">'</span></span>):\n            <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Model<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n                m_serve <span class=\"pl-k\">=</span> Model(Mode.<span class=\"pl-c1\">INFERENCE</span>)\n        \n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">run_epoch</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">train_op</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n        sess.run(model.iterator.initializer)\n        total_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>\n        \n        fetches <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>:model.loss}\n        <span class=\"pl-k\">if</span> train_op <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n            fetches[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_op<span class=\"pl-pds\">'</span></span>] <span class=\"pl-k\">=</span> train_op\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            <span class=\"pl-k\">try</span>:\n                vals <span class=\"pl-k\">=</span> sess.run(fetches)\n            <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n                <span class=\"pl-k\">break</span>\n            total_loss <span class=\"pl-k\">+=</span> vals[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>]\n        <span class=\"pl-k\">return</span> total_loss\n    \n    <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>sess_config,\n                                          <span class=\"pl-v\">is_chief</span><span class=\"pl-k\">=</span>is_chief,\n                                          <span class=\"pl-v\">master</span><span class=\"pl-k\">=</span>server.target) <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-k\">for</span> epoch_id <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">50</span>):\n            t_loss <span class=\"pl-k\">=</span> run_epoch(m, m.train_op)\n            <span class=\"pl-k\">if</span> epoch_id <span class=\"pl-k\">%</span> <span class=\"pl-c1\">10</span> <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Epoch:<span class=\"pl-pds\">'</span></span>, epoch_id)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Training loss:<span class=\"pl-pds\">'</span></span>,t_loss)\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Validating loss:<span class=\"pl-pds\">'</span></span>, run_epoch(m_valid))\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>w = <span class=\"pl-c1\">{}</span> | b = <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(<span class=\"pl-k\">*</span>sess.run([m.w, m.b])))\n                <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>----<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">*</span><span class=\"pl-c1\">10</span>)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    tf.app.run()\n</pre></div>\n<p>I also don't understand why this script complains when I don't specify the \"destinations\" when calling <code>self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0]</code>.  If I let \"destinations\" default to None, I get this error</p>\n<pre><code>Traceback (most recent call last):\n  File \"train.py\", line 165, in &lt;module&gt;\n    tf.app.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"train.py\", line 131, in main\n    m_valid = Model(Mode.VALIDATING, valid_iterator)\n  File \"train.py\", line 44, in __init__\n    self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss))[0]\nTypeError: reduce() missing 1 required positional argument: 'destinations'\n</code></pre>", "body_text": "Cool, so I've tried my best to create a self contained working example but it isn't working and I'm not quite sure what's wrong. Here is what I'm running (this is a box with two GPUs):\n\nRunning CUDA_VISIBLE_DEVICES=0 python train.py --task_type=worker --task_index=0 &\nRunning CUDA_VISIBLE_DEVICES=1 python train.py --task_type=worker --task_index=1 &\nRunning CUDA_VISIBLE_DEVICES=-1 python train.py --task_type=chief --task_index=0\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 69, in get\n    return self._index[device]\nKeyError: '/replica:0/task:0/device:CPU:0'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"train.py\", line 165, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"train.py\", line 154, in main\n    master=server.target) as sess:\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\n    stop_grace_period_secs=stop_grace_period_secs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\n    self._sess = _RecoverableSession(self._coordinated_creator)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\n    _WrappedSession.__init__(self, self._create_session())\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\n    return self._sess_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\n    self.tf_sess = self._session_creator.create_session()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\n    self._scaffold.finalize()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 195, in finalize\n    default_ready_op)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 265, in get_or_default\n    op = default_constructor()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 190, in default_ready_op\n    variables.report_uninitialized_variables(),\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\n    return _add_should_use_warning(fn(*args, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in report_uninitialized_variables\n    [s.op.name for s in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in <listcomp>\n    [s.op.name for s in var_list])\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 305, in op\n    return self.get().op\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 73, in get\n    (device, self._index.keys(), device_util.current())), e)\n  File \"<string>\", line 3, in raise_from\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/job:chief/replica:0/task:0/device:GPU:0']) (current device /device:CPU:0)\n\nHere's the training script:\n#!/usr/bin/env python\n\nimport os\nimport tensorflow as tf\nimport tensorflow.contrib as tfc\nfrom enum import Enum\nimport json\n\nflags = tf.flags\n\nflags.DEFINE_integer(\"task_index\", None, \"task_index\")\nflags.DEFINE_enum(\"task_type\", None, ('chief', 'worker'), \"task type\")\n\nFLAGS = flags.FLAGS\n\nclass Mode(Enum):\n    TRAINING = 1\n    VALIDATING = 2\n    INFERENCE = 3\n\ntf.set_random_seed(4)\nbatch_size = 4\ndef get_dataset(x,y):\n    dataset1 = tf.data.Dataset.from_tensor_slices(x).batch(batch_size).map(lambda x: tf.reshape(x,[-1,1]))\n    dataset2 = tf.data.Dataset.from_tensor_slices(y).batch(batch_size).map(lambda x: tf.reshape(x,[-1,1]))\n    dataset = tf.data.Dataset.zip((dataset1,dataset2))\n    \n    return dataset\n\nclass Model(object):\n    def __init__(self, mode, iterator=None, task_index=0):\n        self.task_index = task_index\n        self.iterator = iterator\n        self.distribution = tf.contrib.distribute.get_distribution_strategy()\n        \n        self.input_data = tf.placeholder(tf.float32, [None,1], name=\"input_data\")\n        \n        if mode == Mode.TRAINING:\n            tower_train_ops = self.distribution.call_for_each_tower(self.tower_loss_and_backprop, self.iterator.get_next(), True)\n            self.train_op = tf.group(self.distribution.unwrap(tower_train_ops))\n            \n        elif mode == Mode.VALIDATING:\n            total_loss = self.distribution.call_for_each_tower(self.tower_loss, *self.iterator.get_next())\n            self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0]\n        else:\n            self.loss = self.tower_loss(self.input_data)\n        \n    def tower_loss(self, inputs, labels=None):\n        self.w = w = tf.get_variable('w',initializer=tf.random_normal_initializer, shape=(1,1))\n        self.b = b = tf.get_variable('b',initializer=tf.constant_initializer(0.0), shape=(1,1))\n        \n        self.y_pred = tf.matmul(inputs,w) + b\n        if labels is None:\n            return None\n        loss = (labels - self.y_pred)**2\n        \n        return tf.reduce_sum(loss)\n    \n    def tower_loss_and_backprop(self, iter_get_next, is_training=False):\n        inputs, labels = iter_get_next\n        \n        self.loss = self.tower_loss(inputs,labels)\n        \n        if not is_training:\n            return None\n        optimizer = tf.train.AdamOptimizer(0.01)\n        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\n        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n            train_op = optimizer.apply_gradients(zip(gradients, variables), global_step=tf.train.get_or_create_global_step())\n        \n        return train_op\n        \n\ndef main(_):\n    #distribution = tfc.distribute.OneDeviceStrategy('device:CPU:0')\n    distribution = tfc.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=1)\n    \n    worker_hosts = ['localhost:1111', 'localhost:1112']\n    chief_host = ['localhost:1113']\n    cluster = tf.train.ClusterSpec({\"worker\": worker_hosts, \"chief\": chief_host})\n    task_type = FLAGS.task_type\n    task_index = FLAGS.task_index\n    is_chief = FLAGS.task_type == \"chief\"\n    \n    sess_config = tf.ConfigProto(\n                    log_device_placement=True,\n                    allow_soft_placement=True,  # Choose existing device to run ops in case specified one doesn't exist\n                )\n    sess_config.gpu_options.allow_growth = True  # This is to avoid using all RAM and to only use what it needs\n    \n    # If this is a worker, then set environment variable and join\n    if not is_chief:\n        os.environ['TF_CONFIG'] = json.dumps({\"cluster\":\n                                                  {\"worker\": worker_hosts, \"chief\": chief_host},\n                                              \"task\":\n                                                  {\"type\": task_type,\"index\": task_index}\n                                              })\n        print('TF_CONFIG=' + os.environ['TF_CONFIG'])\n        tf.contrib.distribute.run_standard_tensorflow_server(sess_config).join()\n    \n    server = tf.train.Server(\n            cluster, job_name=task_type, task_index=task_index)\n    \n    if task_type and task_index is not None:\n        if type(distribution) == tfc.distribute.OneDeviceStrategy:\n            print('Using OneDeviceStrategy')\n            distribution.configure(session_config=sess_config)\n        elif type(distribution) == tfc.distribute.CollectiveAllReduceStrategy:\n            print('Using CollectiveAllReduceStrategy')\n            distribution.configure(session_config=sess_config,\n                                   cluster_spec=cluster,\n                                   task_type=task_type,\n                                   task_id=task_index)\n    \n    foo = lambda a: list(map(lambda x: 3.0*x + 5.0, a))\n    x_train = [i*1.0 for i in range(-100,100)]\n    x_valid = [i*1.0 for i in range(-10,10)]\n    \n    with distribution.scope():\n        with tf.name_scope('Train'):\n            train_dataset = get_dataset(x_train, foo(x_train))\n            train_iterator = distribution.distribute_dataset(lambda: train_dataset).make_initializable_iterator()\n            with tf.variable_scope('Model'):\n                m = Model(Mode.TRAINING, train_iterator)\n        \n        with tf.name_scope('Valid'):\n            valid_dataset = get_dataset(x_valid, foo(x_valid))\n            valid_iterator = distribution.distribute_dataset(lambda: valid_dataset).make_initializable_iterator()\n            with tf.variable_scope('Model', reuse=True):\n                m_valid = Model(Mode.VALIDATING, valid_iterator)\n        \n        with tf.name_scope('Serving'):\n            with tf.variable_scope('Model', reuse=True):\n                m_serve = Model(Mode.INFERENCE)\n        \n    def run_epoch(model, train_op=None):\n        sess.run(model.iterator.initializer)\n        total_loss = 0\n        \n        fetches = {'loss':model.loss}\n        if train_op is not None:\n            fetches['train_op'] = train_op\n        while True:\n            try:\n                vals = sess.run(fetches)\n            except tf.errors.OutOfRangeError:\n                break\n            total_loss += vals['loss']\n        return total_loss\n    \n    with tf.train.MonitoredTrainingSession(config=sess_config,\n                                          is_chief=is_chief,\n                                          master=server.target) as sess:\n        for epoch_id in range(50):\n            t_loss = run_epoch(m, m.train_op)\n            if epoch_id % 10 == 0:\n                print('Epoch:', epoch_id)\n                print('Training loss:',t_loss)\n                print('Validating loss:', run_epoch(m_valid))\n                print('w = {} | b = {}'.format(*sess.run([m.w, m.b])))\n                print('----'*10)\n\nif __name__ == \"__main__\":\n    tf.app.run()\n\nI also don't understand why this script complains when I don't specify the \"destinations\" when calling self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0].  If I let \"destinations\" default to None, I get this error\nTraceback (most recent call last):\n  File \"train.py\", line 165, in <module>\n    tf.app.run()\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n    _sys.exit(main(argv))\n  File \"train.py\", line 131, in main\n    m_valid = Model(Mode.VALIDATING, valid_iterator)\n  File \"train.py\", line 44, in __init__\n    self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss))[0]\nTypeError: reduce() missing 1 required positional argument: 'destinations'", "body": "Cool, so I've tried my best to create a self contained working example but it isn't working and I'm not quite sure what's wrong. Here is what I'm running (this is a box with two GPUs):\r\n\r\n1. Running `CUDA_VISIBLE_DEVICES=0 python train.py --task_type=worker --task_index=0 &`\r\n2. Running `CUDA_VISIBLE_DEVICES=1 python train.py --task_type=worker --task_index=1 &`\r\n3. Running `CUDA_VISIBLE_DEVICES=-1 python train.py --task_type=chief --task_index=0`\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 69, in get\r\n    return self._index[device]\r\nKeyError: '/replica:0/task:0/device:CPU:0'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 165, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 154, in main\r\n    master=server.target) as sess:\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 504, in MonitoredTrainingSession\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 921, in __init__\r\n    stop_grace_period_secs=stop_grace_period_secs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 643, in __init__\r\n    self._sess = _RecoverableSession(self._coordinated_creator)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1107, in __init__\r\n    _WrappedSession.__init__(self, self._create_session())\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 1112, in _create_session\r\n    return self._sess_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 800, in create_session\r\n    self.tf_sess = self._session_creator.create_session()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 557, in create_session\r\n    self._scaffold.finalize()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 195, in finalize\r\n    default_ready_op)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 265, in get_or_default\r\n    op = default_constructor()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/monitored_session.py\", line 190, in default_ready_op\r\n    variables.report_uninitialized_variables(),\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py\", line 189, in wrapped\r\n    return _add_should_use_warning(fn(*args, **kwargs))\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in report_uninitialized_variables\r\n    [s.op.name for s in var_list])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\", line 2720, in <listcomp>\r\n    [s.op.name for s in var_list])\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 305, in op\r\n    return self.get().op\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/contrib/distribute/python/values.py\", line 73, in get\r\n    (device, self._index.keys(), device_util.current())), e)\r\n  File \"<string>\", line 3, in raise_from\r\nValueError: Device /replica:0/task:0/device:CPU:0 not found in dict_keys(['/job:chief/replica:0/task:0/device:GPU:0']) (current device /device:CPU:0)\r\n```\r\n\r\nHere's the training script:\r\n\r\n```python\r\n#!/usr/bin/env python\r\n\r\nimport os\r\nimport tensorflow as tf\r\nimport tensorflow.contrib as tfc\r\nfrom enum import Enum\r\nimport json\r\n\r\nflags = tf.flags\r\n\r\nflags.DEFINE_integer(\"task_index\", None, \"task_index\")\r\nflags.DEFINE_enum(\"task_type\", None, ('chief', 'worker'), \"task type\")\r\n\r\nFLAGS = flags.FLAGS\r\n\r\nclass Mode(Enum):\r\n    TRAINING = 1\r\n    VALIDATING = 2\r\n    INFERENCE = 3\r\n\r\ntf.set_random_seed(4)\r\nbatch_size = 4\r\ndef get_dataset(x,y):\r\n    dataset1 = tf.data.Dataset.from_tensor_slices(x).batch(batch_size).map(lambda x: tf.reshape(x,[-1,1]))\r\n    dataset2 = tf.data.Dataset.from_tensor_slices(y).batch(batch_size).map(lambda x: tf.reshape(x,[-1,1]))\r\n    dataset = tf.data.Dataset.zip((dataset1,dataset2))\r\n    \r\n    return dataset\r\n\r\nclass Model(object):\r\n    def __init__(self, mode, iterator=None, task_index=0):\r\n        self.task_index = task_index\r\n        self.iterator = iterator\r\n        self.distribution = tf.contrib.distribute.get_distribution_strategy()\r\n        \r\n        self.input_data = tf.placeholder(tf.float32, [None,1], name=\"input_data\")\r\n        \r\n        if mode == Mode.TRAINING:\r\n            tower_train_ops = self.distribution.call_for_each_tower(self.tower_loss_and_backprop, self.iterator.get_next(), True)\r\n            self.train_op = tf.group(self.distribution.unwrap(tower_train_ops))\r\n            \r\n        elif mode == Mode.VALIDATING:\r\n            total_loss = self.distribution.call_for_each_tower(self.tower_loss, *self.iterator.get_next())\r\n            self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0]\r\n        else:\r\n            self.loss = self.tower_loss(self.input_data)\r\n        \r\n    def tower_loss(self, inputs, labels=None):\r\n        self.w = w = tf.get_variable('w',initializer=tf.random_normal_initializer, shape=(1,1))\r\n        self.b = b = tf.get_variable('b',initializer=tf.constant_initializer(0.0), shape=(1,1))\r\n        \r\n        self.y_pred = tf.matmul(inputs,w) + b\r\n        if labels is None:\r\n            return None\r\n        loss = (labels - self.y_pred)**2\r\n        \r\n        return tf.reduce_sum(loss)\r\n    \r\n    def tower_loss_and_backprop(self, iter_get_next, is_training=False):\r\n        inputs, labels = iter_get_next\r\n        \r\n        self.loss = self.tower_loss(inputs,labels)\r\n        \r\n        if not is_training:\r\n            return None\r\n        optimizer = tf.train.AdamOptimizer(0.01)\r\n        gradients, variables = zip(*optimizer.compute_gradients(self.loss))\r\n        gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\r\n        with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\r\n            train_op = optimizer.apply_gradients(zip(gradients, variables), global_step=tf.train.get_or_create_global_step())\r\n        \r\n        return train_op\r\n        \r\n\r\ndef main(_):\r\n    #distribution = tfc.distribute.OneDeviceStrategy('device:CPU:0')\r\n    distribution = tfc.distribute.CollectiveAllReduceStrategy(num_gpus_per_worker=1)\r\n    \r\n    worker_hosts = ['localhost:1111', 'localhost:1112']\r\n    chief_host = ['localhost:1113']\r\n    cluster = tf.train.ClusterSpec({\"worker\": worker_hosts, \"chief\": chief_host})\r\n    task_type = FLAGS.task_type\r\n    task_index = FLAGS.task_index\r\n    is_chief = FLAGS.task_type == \"chief\"\r\n    \r\n    sess_config = tf.ConfigProto(\r\n                    log_device_placement=True,\r\n                    allow_soft_placement=True,  # Choose existing device to run ops in case specified one doesn't exist\r\n                )\r\n    sess_config.gpu_options.allow_growth = True  # This is to avoid using all RAM and to only use what it needs\r\n    \r\n    # If this is a worker, then set environment variable and join\r\n    if not is_chief:\r\n        os.environ['TF_CONFIG'] = json.dumps({\"cluster\":\r\n                                                  {\"worker\": worker_hosts, \"chief\": chief_host},\r\n                                              \"task\":\r\n                                                  {\"type\": task_type,\"index\": task_index}\r\n                                              })\r\n        print('TF_CONFIG=' + os.environ['TF_CONFIG'])\r\n        tf.contrib.distribute.run_standard_tensorflow_server(sess_config).join()\r\n    \r\n    server = tf.train.Server(\r\n            cluster, job_name=task_type, task_index=task_index)\r\n    \r\n    if task_type and task_index is not None:\r\n        if type(distribution) == tfc.distribute.OneDeviceStrategy:\r\n            print('Using OneDeviceStrategy')\r\n            distribution.configure(session_config=sess_config)\r\n        elif type(distribution) == tfc.distribute.CollectiveAllReduceStrategy:\r\n            print('Using CollectiveAllReduceStrategy')\r\n            distribution.configure(session_config=sess_config,\r\n                                   cluster_spec=cluster,\r\n                                   task_type=task_type,\r\n                                   task_id=task_index)\r\n    \r\n    foo = lambda a: list(map(lambda x: 3.0*x + 5.0, a))\r\n    x_train = [i*1.0 for i in range(-100,100)]\r\n    x_valid = [i*1.0 for i in range(-10,10)]\r\n    \r\n    with distribution.scope():\r\n        with tf.name_scope('Train'):\r\n            train_dataset = get_dataset(x_train, foo(x_train))\r\n            train_iterator = distribution.distribute_dataset(lambda: train_dataset).make_initializable_iterator()\r\n            with tf.variable_scope('Model'):\r\n                m = Model(Mode.TRAINING, train_iterator)\r\n        \r\n        with tf.name_scope('Valid'):\r\n            valid_dataset = get_dataset(x_valid, foo(x_valid))\r\n            valid_iterator = distribution.distribute_dataset(lambda: valid_dataset).make_initializable_iterator()\r\n            with tf.variable_scope('Model', reuse=True):\r\n                m_valid = Model(Mode.VALIDATING, valid_iterator)\r\n        \r\n        with tf.name_scope('Serving'):\r\n            with tf.variable_scope('Model', reuse=True):\r\n                m_serve = Model(Mode.INFERENCE)\r\n        \r\n    def run_epoch(model, train_op=None):\r\n        sess.run(model.iterator.initializer)\r\n        total_loss = 0\r\n        \r\n        fetches = {'loss':model.loss}\r\n        if train_op is not None:\r\n            fetches['train_op'] = train_op\r\n        while True:\r\n            try:\r\n                vals = sess.run(fetches)\r\n            except tf.errors.OutOfRangeError:\r\n                break\r\n            total_loss += vals['loss']\r\n        return total_loss\r\n    \r\n    with tf.train.MonitoredTrainingSession(config=sess_config,\r\n                                          is_chief=is_chief,\r\n                                          master=server.target) as sess:\r\n        for epoch_id in range(50):\r\n            t_loss = run_epoch(m, m.train_op)\r\n            if epoch_id % 10 == 0:\r\n                print('Epoch:', epoch_id)\r\n                print('Training loss:',t_loss)\r\n                print('Validating loss:', run_epoch(m_valid))\r\n                print('w = {} | b = {}'.format(*sess.run([m.w, m.b])))\r\n                print('----'*10)\r\n\r\nif __name__ == \"__main__\":\r\n    tf.app.run()\r\n\r\n```\r\n\r\nI also don't understand why this script complains when I don't specify the \"destinations\" when calling `self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss, \"/device:GPU:0\"))[0]`.  If I let \"destinations\" default to None, I get this error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"train.py\", line 165, in <module>\r\n    tf.app.run()\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n    _sys.exit(main(argv))\r\n  File \"train.py\", line 131, in main\r\n    m_valid = Model(Mode.VALIDATING, valid_iterator)\r\n  File \"train.py\", line 44, in __init__\r\n    self.loss = self.distribution.unwrap(self.distribution.reduce(tf.VariableAggregation.SUM, total_loss))[0]\r\nTypeError: reduce() missing 1 required positional argument: 'destinations'\r\n```"}