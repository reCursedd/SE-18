{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/160063986", "pull_request_review_id": 87113345, "id": 160063986, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE2MDA2Mzk4Ng==", "diff_hunk": "@@ -622,6 +629,10 @@ def _beam_search_step(time, logits, next_cell_state, beam_state, batch_size,\n       range_size=beam_width,\n       gather_shape=[-1])\n   next_prediction_len += lengths_to_add\n+  next_prediction_len = array_ops.where(next_beam_probs > -np.Inf,", "path": "tensorflow/contrib/seq2seq/python/ops/beam_search_decoder.py", "position": null, "original_position": 62, "commit_id": "272326f441428ea47e78e2c9a2a7ec109327bec7", "original_commit_id": "ea4ddf2563be37329e0becf2176e7a1d8958c4df", "user": {"login": "oahziur", "id": 4604464, "node_id": "MDQ6VXNlcjQ2MDQ0NjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4604464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oahziur", "html_url": "https://github.com/oahziur", "followers_url": "https://api.github.com/users/oahziur/followers", "following_url": "https://api.github.com/users/oahziur/following{/other_user}", "gists_url": "https://api.github.com/users/oahziur/gists{/gist_id}", "starred_url": "https://api.github.com/users/oahziur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oahziur/subscriptions", "organizations_url": "https://api.github.com/users/oahziur/orgs", "repos_url": "https://api.github.com/users/oahziur/repos", "events_url": "https://api.github.com/users/oahziur/events{/privacy}", "received_events_url": "https://api.github.com/users/oahziur/received_events", "type": "User", "site_admin": false}, "body": "I think this can be true when t>0. I think it is generally ok since as long as vocab_size != 1, the available beam width will expand towards to the beam_width.\r\n\r\nI think we can be more careful about `finished`, for example, if all finished == False entries' log score is -neg_inf, we terminate the decoding process. This can be done in the `step()` method.  wdyt?\r\n\r\n  ", "created_at": "2018-01-07T23:26:25Z", "updated_at": "2018-01-20T13:07:31Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/10752#discussion_r160063986", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/10752", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/160063986"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/10752#discussion_r160063986"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/10752"}}, "body_html": "<p>I think this can be true when t&gt;0. I think it is generally ok since as long as vocab_size != 1, the available beam width will expand towards to the beam_width.</p>\n<p>I think we can be more careful about <code>finished</code>, for example, if all finished == False entries' log score is -neg_inf, we terminate the decoding process. This can be done in the <code>step()</code> method.  wdyt?</p>", "body_text": "I think this can be true when t>0. I think it is generally ok since as long as vocab_size != 1, the available beam width will expand towards to the beam_width.\nI think we can be more careful about finished, for example, if all finished == False entries' log score is -neg_inf, we terminate the decoding process. This can be done in the step() method.  wdyt?", "in_reply_to_id": 159772312}