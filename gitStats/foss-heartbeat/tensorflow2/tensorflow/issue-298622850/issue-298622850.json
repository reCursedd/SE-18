{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17153", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17153/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17153/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17153/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17153", "id": 298622850, "node_id": "MDU6SXNzdWUyOTg2MjI4NTA=", "number": 17153, "title": "How to sync worker models of KMeansClustering in distributed tensorflow?", "user": {"login": "sandeepkumar8713", "id": 13407698, "node_id": "MDQ6VXNlcjEzNDA3Njk4", "avatar_url": "https://avatars2.githubusercontent.com/u/13407698?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sandeepkumar8713", "html_url": "https://github.com/sandeepkumar8713", "followers_url": "https://api.github.com/users/sandeepkumar8713/followers", "following_url": "https://api.github.com/users/sandeepkumar8713/following{/other_user}", "gists_url": "https://api.github.com/users/sandeepkumar8713/gists{/gist_id}", "starred_url": "https://api.github.com/users/sandeepkumar8713/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sandeepkumar8713/subscriptions", "organizations_url": "https://api.github.com/users/sandeepkumar8713/orgs", "repos_url": "https://api.github.com/users/sandeepkumar8713/repos", "events_url": "https://api.github.com/users/sandeepkumar8713/events{/privacy}", "received_events_url": "https://api.github.com/users/sandeepkumar8713/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-02-20T14:53:47Z", "updated_at": "2018-02-23T06:42:01Z", "closed_at": "2018-02-23T06:42:01Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>-<strong>Have I written custom code</strong>: yes<br>\n-<strong>OS Platform and Distribution</strong>: Open SUSE Leap 42.3<br>\n-<strong>TensorFlow installed from:</strong> python pip<br>\n-<strong>TensorFlow version</strong>: 1.6.0<br>\n-<strong>Python version</strong>: 2.7<br>\n-<strong>Bazel version</strong>  :N/A<br>\n-<strong>CUDA/cuDNN version</strong> : N/A<br>\n-<strong>GPU model and memory</strong> : N/A<br>\n-<strong>Exact command to reproduce</strong> : N/A</p>\n<p>Hi,<br>\nI am trying to use distributed tensorflow over KMeansClustering. I have one <strong>parameter server</strong> and two <strong>workers</strong>. <strong>Training data</strong> in both the workers are <strong>different</strong>.  After training, cluster centers in the two workers are different. Is there a function in tensorflow which can be called to sync the models while training so that the cluster   centers are similar if not same.</p>\n<p><strong>Source Code</strong></p>\n<pre><code>def startCluster(jobName,taskId):\n    parameter_servers = [\"localhost:2222\"]\n    workers = [\"localhost:2223\",\n               \"localhost:2224\"]\n\n    cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n    server = tf.train.Server(\n        cluster,\n        job_name=jobName,\n        task_index=taskId)\n\n    return cluster,server\n\ndef trainModelInParallel(cluster,server,taskId):\n    k = 4\n    n = 1000\n    variables = 2\n\n    points = np.random.uniform(0, 1, [n, variables])\n\n    # Between-graph replication\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % taskId,\n            cluster=cluster)):\n\n        input_fn = lambda: tf.train.limit_epochs(tf.convert_to_tensor(\n                                    points, dtype=tf.float32), num_epochs=1)\n        kmeans = tf.contrib.factorization.KMeansClustering(\n            num_clusters=k, use_mini_batch=False,model_dir=defaultModel)\n\n\n    sv = tf.train.Supervisor(is_chief=(taskId == 0),save_model_secs=1)\n\n    with sv.prepare_or_wait_for_session(server.target) as sess:\n\n        for _ in xrange(10):\n            kmeans.train(input_fn)\n            centers = kmeans.cluster_centers()\n            print centers[0],centers[1],centers[2],centers[3]\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        print\"Please pass job name and task ID\"\n        sys.exit()\n\n    jobName = sys.argv[1]\n    taskId = (sys.argv[2])\n    defaultModel += taskId\n    cluster, server = startCluster(jobName, int(taskId))\n\n    if jobName == \"ps\":\n        server.join()\n    else:\n        trainModelInParallel(cluster,server,int(taskId))\n\n</code></pre>\n<p><strong>Command Line</strong><br>\nTo execute the above code please enter following commands in 3 different terminals:</p>\n<pre><code>python kmeansDistributed.py ps 0\npython kmeansDistributed.py worker 0\npython kmeansDistributed.py worker 1\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15960281\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ccolby\">@ccolby</a> Your inputs will be very helpful.</p>", "body_text": "System information\n-Have I written custom code: yes\n-OS Platform and Distribution: Open SUSE Leap 42.3\n-TensorFlow installed from: python pip\n-TensorFlow version: 1.6.0\n-Python version: 2.7\n-Bazel version  :N/A\n-CUDA/cuDNN version : N/A\n-GPU model and memory : N/A\n-Exact command to reproduce : N/A\nHi,\nI am trying to use distributed tensorflow over KMeansClustering. I have one parameter server and two workers. Training data in both the workers are different.  After training, cluster centers in the two workers are different. Is there a function in tensorflow which can be called to sync the models while training so that the cluster   centers are similar if not same.\nSource Code\ndef startCluster(jobName,taskId):\n    parameter_servers = [\"localhost:2222\"]\n    workers = [\"localhost:2223\",\n               \"localhost:2224\"]\n\n    cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\n    server = tf.train.Server(\n        cluster,\n        job_name=jobName,\n        task_index=taskId)\n\n    return cluster,server\n\ndef trainModelInParallel(cluster,server,taskId):\n    k = 4\n    n = 1000\n    variables = 2\n\n    points = np.random.uniform(0, 1, [n, variables])\n\n    # Between-graph replication\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % taskId,\n            cluster=cluster)):\n\n        input_fn = lambda: tf.train.limit_epochs(tf.convert_to_tensor(\n                                    points, dtype=tf.float32), num_epochs=1)\n        kmeans = tf.contrib.factorization.KMeansClustering(\n            num_clusters=k, use_mini_batch=False,model_dir=defaultModel)\n\n\n    sv = tf.train.Supervisor(is_chief=(taskId == 0),save_model_secs=1)\n\n    with sv.prepare_or_wait_for_session(server.target) as sess:\n\n        for _ in xrange(10):\n            kmeans.train(input_fn)\n            centers = kmeans.cluster_centers()\n            print centers[0],centers[1],centers[2],centers[3]\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 1:\n        print\"Please pass job name and task ID\"\n        sys.exit()\n\n    jobName = sys.argv[1]\n    taskId = (sys.argv[2])\n    defaultModel += taskId\n    cluster, server = startCluster(jobName, int(taskId))\n\n    if jobName == \"ps\":\n        server.join()\n    else:\n        trainModelInParallel(cluster,server,int(taskId))\n\n\nCommand Line\nTo execute the above code please enter following commands in 3 different terminals:\npython kmeansDistributed.py ps 0\npython kmeansDistributed.py worker 0\npython kmeansDistributed.py worker 1\n\n@ccolby Your inputs will be very helpful.", "body": "### System information\r\n-**Have I written custom code**: yes\r\n-**OS Platform and Distribution**: Open SUSE Leap 42.3\r\n-**TensorFlow installed from:** python pip\r\n-**TensorFlow version**: 1.6.0\r\n-**Python version**: 2.7\r\n-**Bazel version**  :N/A\r\n-**CUDA/cuDNN version** : N/A\r\n-**GPU model and memory** : N/A\r\n-**Exact command to reproduce** : N/A\r\n\r\nHi, \r\nI am trying to use distributed tensorflow over KMeansClustering. I have one **parameter server** and two **workers**. **Training data** in both the workers are **different**.  After training, cluster centers in the two workers are different. Is there a function in tensorflow which can be called to sync the models while training so that the cluster   centers are similar if not same.\r\n\r\n**Source Code** \r\n\r\n```\r\ndef startCluster(jobName,taskId):\r\n    parameter_servers = [\"localhost:2222\"]\r\n    workers = [\"localhost:2223\",\r\n               \"localhost:2224\"]\r\n\r\n    cluster = tf.train.ClusterSpec({\"ps\": parameter_servers, \"worker\": workers})\r\n    server = tf.train.Server(\r\n        cluster,\r\n        job_name=jobName,\r\n        task_index=taskId)\r\n\r\n    return cluster,server\r\n\r\ndef trainModelInParallel(cluster,server,taskId):\r\n    k = 4\r\n    n = 1000\r\n    variables = 2\r\n\r\n    points = np.random.uniform(0, 1, [n, variables])\r\n\r\n    # Between-graph replication\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d\" % taskId,\r\n            cluster=cluster)):\r\n\r\n        input_fn = lambda: tf.train.limit_epochs(tf.convert_to_tensor(\r\n                                    points, dtype=tf.float32), num_epochs=1)\r\n        kmeans = tf.contrib.factorization.KMeansClustering(\r\n            num_clusters=k, use_mini_batch=False,model_dir=defaultModel)\r\n\r\n\r\n    sv = tf.train.Supervisor(is_chief=(taskId == 0),save_model_secs=1)\r\n\r\n    with sv.prepare_or_wait_for_session(server.target) as sess:\r\n\r\n        for _ in xrange(10):\r\n            kmeans.train(input_fn)\r\n            centers = kmeans.cluster_centers()\r\n            print centers[0],centers[1],centers[2],centers[3]\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    if len(sys.argv) == 1:\r\n        print\"Please pass job name and task ID\"\r\n        sys.exit()\r\n\r\n    jobName = sys.argv[1]\r\n    taskId = (sys.argv[2])\r\n    defaultModel += taskId\r\n    cluster, server = startCluster(jobName, int(taskId))\r\n\r\n    if jobName == \"ps\":\r\n        server.join()\r\n    else:\r\n        trainModelInParallel(cluster,server,int(taskId))\r\n\r\n```\r\n**Command Line** \r\nTo execute the above code please enter following commands in 3 different terminals:\r\n```\r\npython kmeansDistributed.py ps 0\r\npython kmeansDistributed.py worker 0\r\npython kmeansDistributed.py worker 1\r\n```\r\n\r\n@ccolby Your inputs will be very helpful."}