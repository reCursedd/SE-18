{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413279895", "html_url": "https://github.com/tensorflow/tensorflow/pull/20412#issuecomment-413279895", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20412", "id": 413279895, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzI3OTg5NQ==", "user": {"login": "bioothod", "id": 1847575, "node_id": "MDQ6VXNlcjE4NDc1NzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1847575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bioothod", "html_url": "https://github.com/bioothod", "followers_url": "https://api.github.com/users/bioothod/followers", "following_url": "https://api.github.com/users/bioothod/following{/other_user}", "gists_url": "https://api.github.com/users/bioothod/gists{/gist_id}", "starred_url": "https://api.github.com/users/bioothod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bioothod/subscriptions", "organizations_url": "https://api.github.com/users/bioothod/orgs", "repos_url": "https://api.github.com/users/bioothod/repos", "events_url": "https://api.github.com/users/bioothod/events{/privacy}", "received_events_url": "https://api.github.com/users/bioothod/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-15T17:52:36Z", "updated_at": "2018-08-15T17:54:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank you for response <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16018\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/asimshankar\">@asimshankar</a> !</p>\n<p>I have to disagree - using <code>TF_BindToDevice</code> allows to bind whole graph to specified device and it will be executed there. I can not argue about int32 operations, but I do see that my large graphs are not executed on the GPU at all currently without this patch. So, basically, currently TF c/c++/go bindings are broken, all graphs are always executed on CPU.</p>\n<p>With this patch nodes (all or only float32 for example, I can not say for sure) are executed on GPU. This is being confirmed not only by device placement log, which you say is misleading (and that's a bug too imho), but also HW monitoring tools (like <code>nvidia-smi</code>).</p>\n<p>It could be interesting to check float32 operations (I'm currently away from the servers and can not run similar float32 ops test), but what's the point? We have a graph which is supposed to be executed on GPU by documentation, and it is not. Hence the patch.</p>\n<p>As you said, second call for <code>TF_BindToDevice</code> will not replace graph if session has been created already, but it should not be a problem, <code>TF_BindToDevice</code> is not a strict order but a suggestion, a hint for execution, which can be clearly stated in the documentation with the example you have made.</p>\n<p>Session config already had this virtual-physical device mapping, and TF developers decided that it must not be used at all - at least go bindings crash if visible device list does not match per-process device list (CUDA_VISIBLE_DEVICES for instance) or is not empty. Logic that multiple virtual devices will point to the same physical remains, although I personally never saw such operations that require changing physical device properties.</p>\n<p><code>TF_BindToDevice</code> is actually just a helper, if you are strongly against adding it (with appropriate documentation), I think updating <code>TF_GraphImportGraphDef</code> will work too, although a bit more clumsy.</p>\n<p>So, let the solution be to extend <code>TF_GraphImportGraphDef</code>?</p>", "body_text": "Thank you for response @asimshankar !\nI have to disagree - using TF_BindToDevice allows to bind whole graph to specified device and it will be executed there. I can not argue about int32 operations, but I do see that my large graphs are not executed on the GPU at all currently without this patch. So, basically, currently TF c/c++/go bindings are broken, all graphs are always executed on CPU.\nWith this patch nodes (all or only float32 for example, I can not say for sure) are executed on GPU. This is being confirmed not only by device placement log, which you say is misleading (and that's a bug too imho), but also HW monitoring tools (like nvidia-smi).\nIt could be interesting to check float32 operations (I'm currently away from the servers and can not run similar float32 ops test), but what's the point? We have a graph which is supposed to be executed on GPU by documentation, and it is not. Hence the patch.\nAs you said, second call for TF_BindToDevice will not replace graph if session has been created already, but it should not be a problem, TF_BindToDevice is not a strict order but a suggestion, a hint for execution, which can be clearly stated in the documentation with the example you have made.\nSession config already had this virtual-physical device mapping, and TF developers decided that it must not be used at all - at least go bindings crash if visible device list does not match per-process device list (CUDA_VISIBLE_DEVICES for instance) or is not empty. Logic that multiple virtual devices will point to the same physical remains, although I personally never saw such operations that require changing physical device properties.\nTF_BindToDevice is actually just a helper, if you are strongly against adding it (with appropriate documentation), I think updating TF_GraphImportGraphDef will work too, although a bit more clumsy.\nSo, let the solution be to extend TF_GraphImportGraphDef?", "body": "Thank you for response @asimshankar !\r\n\r\nI have to disagree - using `TF_BindToDevice` allows to bind whole graph to specified device and it will be executed there. I can not argue about int32 operations, but I do see that my large graphs are not executed on the GPU at all currently without this patch. So, basically, currently TF c/c++/go bindings are broken, all graphs are always executed on CPU.\r\n\r\nWith this patch nodes (all or only float32 for example, I can not say for sure) are executed on GPU. This is being confirmed not only by device placement log, which you say is misleading (and that's a bug too imho), but also HW monitoring tools (like `nvidia-smi`).\r\n\r\nIt could be interesting to check float32 operations (I'm currently away from the servers and can not run similar float32 ops test), but what's the point? We have a graph which is supposed to be executed on GPU by documentation, and it is not. Hence the patch.\r\n\r\nAs you said, second call for `TF_BindToDevice` will not replace graph if session has been created already, but it should not be a problem, `TF_BindToDevice` is not a strict order but a suggestion, a hint for execution, which can be clearly stated in the documentation with the example you have made.\r\n\r\nSession config already had this virtual-physical device mapping, and TF developers decided that it must not be used at all - at least go bindings crash if visible device list does not match per-process device list (CUDA_VISIBLE_DEVICES for instance) or is not empty. Logic that multiple virtual devices will point to the same physical remains, although I personally never saw such operations that require changing physical device properties.\r\n\r\n`TF_BindToDevice` is actually just a helper, if you are strongly against adding it (with appropriate documentation), I think updating `TF_GraphImportGraphDef` will work too, although a bit more clumsy. \r\n\r\nSo, let the solution be to extend `TF_GraphImportGraphDef`?"}