{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404292611", "html_url": "https://github.com/tensorflow/tensorflow/pull/20412#issuecomment-404292611", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20412", "id": 404292611, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDI5MjYxMQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-11T20:02:24Z", "updated_at": "2018-07-12T00:09:48Z", "author_association": "MEMBER", "body_html": "<p>Thanks for the note. Let's separate the two issues.</p>\n<p>The GPU should work fine in 1.6+. I just tried with 1.9 and it does seem to work.<br>\nSee <a href=\"https://gist.github.com/asimshankar/ef367e4897e248466c42c2dc629814e0\">https://gist.github.com/asimshankar/ef367e4897e248466c42c2dc629814e0</a></p>\n<p>Particularly the lines in the output:</p>\n<pre><code>MatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n</code></pre>\n<p>If the same is not happening in the setup you're running it, it's worth investigating. So this PR shouldn't be required to enable use of the GPU.</p>\n<p>That said, I appreciate that running the same model on multiple devices isn't as smooth as it should be and that experience can be improved. However, I don't think that this approach is the best one. As I mentioned earlier, I'm weary of mutations to the graph since that can lead to confusing behavior with multiple sessions. Furthermore, the implementation here is forcing every node to run on GPU - which will be problematic if the graph has nodes that only have CPU kernels (unless the user set <code>soft_device_placement=True</code>). Thus, I don't think this works out as an appropriate general purpose solution (though it may work out just fine for specific models).</p>\n<p>In both cases (whether the approach in this PR, or via having the process that writes out the graph to import) we're creating multiple sessions, each with their own copy of the graph.</p>\n<p>One somewhat ugly workaround is to have the program that creates the graph create a single saved model, with one tag per GPU. Then the Go program can create one session per GPU by providing the right tag to <code>LoadSavedModel</code>. Though, yeah, I admit this is a bit cumbersome. Another possibly cleaner option would be to have <code>ImportGraphDef</code> take a device specification and use that (similar to how <code>tf.import_graph_def</code> in Python respects the device stack). Though, that will be a bit more work.</p>\n<p>Thanks for your understanding.</p>", "body_text": "Thanks for the note. Let's separate the two issues.\nThe GPU should work fine in 1.6+. I just tried with 1.9 and it does seem to work.\nSee https://gist.github.com/asimshankar/ef367e4897e248466c42c2dc629814e0\nParticularly the lines in the output:\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\n\nIf the same is not happening in the setup you're running it, it's worth investigating. So this PR shouldn't be required to enable use of the GPU.\nThat said, I appreciate that running the same model on multiple devices isn't as smooth as it should be and that experience can be improved. However, I don't think that this approach is the best one. As I mentioned earlier, I'm weary of mutations to the graph since that can lead to confusing behavior with multiple sessions. Furthermore, the implementation here is forcing every node to run on GPU - which will be problematic if the graph has nodes that only have CPU kernels (unless the user set soft_device_placement=True). Thus, I don't think this works out as an appropriate general purpose solution (though it may work out just fine for specific models).\nIn both cases (whether the approach in this PR, or via having the process that writes out the graph to import) we're creating multiple sessions, each with their own copy of the graph.\nOne somewhat ugly workaround is to have the program that creates the graph create a single saved model, with one tag per GPU. Then the Go program can create one session per GPU by providing the right tag to LoadSavedModel. Though, yeah, I admit this is a bit cumbersome. Another possibly cleaner option would be to have ImportGraphDef take a device specification and use that (similar to how tf.import_graph_def in Python respects the device stack). Though, that will be a bit more work.\nThanks for your understanding.", "body": "Thanks for the note. Let's separate the two issues.\r\n\r\nThe GPU should work fine in 1.6+. I just tried with 1.9 and it does seem to work.\r\nSee https://gist.github.com/asimshankar/ef367e4897e248466c42c2dc629814e0\r\n\r\nParticularly the lines in the output:\r\n```\r\nMatMul: (MatMul): /job:localhost/replica:0/task:0/device:GPU:0\r\nPlaceholder: (Placeholder): /job:localhost/replica:0/task:0/device:GPU:0\r\n```\r\n\r\nIf the same is not happening in the setup you're running it, it's worth investigating. So this PR shouldn't be required to enable use of the GPU.\r\n\r\nThat said, I appreciate that running the same model on multiple devices isn't as smooth as it should be and that experience can be improved. However, I don't think that this approach is the best one. As I mentioned earlier, I'm weary of mutations to the graph since that can lead to confusing behavior with multiple sessions. Furthermore, the implementation here is forcing every node to run on GPU - which will be problematic if the graph has nodes that only have CPU kernels (unless the user set `soft_device_placement=True`). Thus, I don't think this works out as an appropriate general purpose solution (though it may work out just fine for specific models).\r\n\r\nIn both cases (whether the approach in this PR, or via having the process that writes out the graph to import) we're creating multiple sessions, each with their own copy of the graph.\r\n\r\nOne somewhat ugly workaround is to have the program that creates the graph create a single saved model, with one tag per GPU. Then the Go program can create one session per GPU by providing the right tag to `LoadSavedModel`. Though, yeah, I admit this is a bit cumbersome. Another possibly cleaner option would be to have `ImportGraphDef` take a device specification and use that (similar to how `tf.import_graph_def` in Python respects the device stack). Though, that will be a bit more work.\r\n\r\nThanks for your understanding.\r\n\r\n"}