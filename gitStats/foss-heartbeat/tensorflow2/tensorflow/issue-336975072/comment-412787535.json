{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412787535", "html_url": "https://github.com/tensorflow/tensorflow/pull/20412#issuecomment-412787535", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20412", "id": 412787535, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjc4NzUzNQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-14T07:59:03Z", "updated_at": "2018-08-14T07:59:03Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1847575\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bioothod\">@bioothod</a> : Took a look at your example and had some comments/observations.</p>\n<p>As mentioned above, the story will be much different if you're using types other than <code>int32</code> (for example, with <code>float32</code>). Support for 32-bit integer operations on GPU is unfortunately a bit iffy in TensorFlow at the moment and the log message you're observing might be misleading as in both cases the operation is actually executing on CPU. The <a href=\"https://github.com/tensorflow/tensorflow/blob/a8e78e2e617b6ca10f4878fe99fdf43ddedfa7c6/tensorflow/core/kernels/matmul_op.cc#L622\"><code>MatMul</code> kernel does not have a GPU implementation for <code>int32</code></a> and thus the device annotation is ignored (since you set <code>AllowSoftPlacement: true</code>). The <code>Sum</code> kernel (from <code>tf.reduce_sum</code>) requires the input and output for the <code>int32</code> type to be in host memory (not device memory) and <a href=\"https://github.com/tensorflow/tensorflow/blob/1c1dad105a57bb13711492a8ba5ab9d10c91b5df/tensorflow/core/kernels/reduction_ops_sum.cc#L58\">also just executes on CPU</a> (the log messaging is misleading there because of the hack mentioned in the comment of the kernel registration).</p>\n<p>Long story short, the <code>BindToDevice</code> call in your example has no real effect (the unfortunately misleading log message aside).</p>\n<p>Furthermore, going back to my original reservation to adding this mutation to the graph - we've consciously avoided C APIs to mutate existing nodes in the graph as it can be hard to determine whether or not the mutations apply correctly. For example, consider the following:</p>\n<div class=\"highlight highlight-source-c\"><pre>TF_Graph* graph = MakeMyGraph();\n<span class=\"pl-en\">TF_BindToDevice</span>(graph, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>);\nTF_Session* session = TF_NewSession(graph, ...);\n<span class=\"pl-en\">TF_SessionRun</span>(session, ...);\n<span class=\"pl-en\">TF_BindToDevice</span>(graph, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:0<span class=\"pl-pds\">\"</span></span>);\n<span class=\"pl-en\">TF_SessionRun</span>(session, ...);</pre></div>\n<p>In this snippet, the second call to <code>TF_BindToDevice</code> has no effect, and all operations will execute on CPU. I'm hesitant to add a function which will have such an esoteric and hard to explain contract.</p>\n<p>Alternatives would be the following:</p>\n<ol>\n<li>Do this binding as an option to <code>TF_ImportGraphDef</code>, so it applies only to nodes being imported.</li>\n<li>Rewrite the <code>GraphDef</code> proto before importing (see <a href=\"https://stackoverflow.com/questions/47799972/tensorflow-java-multi-gpu-inference/47915987#47915987\" rel=\"nofollow\">similar snippet for Java</a>)</li>\n<li>As suggested in the StackOverflow question above, somehow add a notion of virtual devices so that each <code>Session</code> can map the device in the graph to a different device at runtime.</li>\n</ol>\n<p>Sound reasonable?</p>", "body_text": "@bioothod : Took a look at your example and had some comments/observations.\nAs mentioned above, the story will be much different if you're using types other than int32 (for example, with float32). Support for 32-bit integer operations on GPU is unfortunately a bit iffy in TensorFlow at the moment and the log message you're observing might be misleading as in both cases the operation is actually executing on CPU. The MatMul kernel does not have a GPU implementation for int32 and thus the device annotation is ignored (since you set AllowSoftPlacement: true). The Sum kernel (from tf.reduce_sum) requires the input and output for the int32 type to be in host memory (not device memory) and also just executes on CPU (the log messaging is misleading there because of the hack mentioned in the comment of the kernel registration).\nLong story short, the BindToDevice call in your example has no real effect (the unfortunately misleading log message aside).\nFurthermore, going back to my original reservation to adding this mutation to the graph - we've consciously avoided C APIs to mutate existing nodes in the graph as it can be hard to determine whether or not the mutations apply correctly. For example, consider the following:\nTF_Graph* graph = MakeMyGraph();\nTF_BindToDevice(graph, \"/cpu:0\");\nTF_Session* session = TF_NewSession(graph, ...);\nTF_SessionRun(session, ...);\nTF_BindToDevice(graph, \"/gpu:0\");\nTF_SessionRun(session, ...);\nIn this snippet, the second call to TF_BindToDevice has no effect, and all operations will execute on CPU. I'm hesitant to add a function which will have such an esoteric and hard to explain contract.\nAlternatives would be the following:\n\nDo this binding as an option to TF_ImportGraphDef, so it applies only to nodes being imported.\nRewrite the GraphDef proto before importing (see similar snippet for Java)\nAs suggested in the StackOverflow question above, somehow add a notion of virtual devices so that each Session can map the device in the graph to a different device at runtime.\n\nSound reasonable?", "body": "@bioothod : Took a look at your example and had some comments/observations.\r\n\r\nAs mentioned above, the story will be much different if you're using types other than `int32` (for example, with `float32`). Support for 32-bit integer operations on GPU is unfortunately a bit iffy in TensorFlow at the moment and the log message you're observing might be misleading as in both cases the operation is actually executing on CPU. The [`MatMul` kernel does not have a GPU implementation for `int32`](https://github.com/tensorflow/tensorflow/blob/a8e78e2e617b6ca10f4878fe99fdf43ddedfa7c6/tensorflow/core/kernels/matmul_op.cc#L622) and thus the device annotation is ignored (since you set `AllowSoftPlacement: true`). The `Sum` kernel (from `tf.reduce_sum`) requires the input and output for the `int32` type to be in host memory (not device memory) and [also just executes on CPU](https://github.com/tensorflow/tensorflow/blob/1c1dad105a57bb13711492a8ba5ab9d10c91b5df/tensorflow/core/kernels/reduction_ops_sum.cc#L58) (the log messaging is misleading there because of the hack mentioned in the comment of the kernel registration). \r\n\r\nLong story short, the `BindToDevice` call in your example has no real effect (the unfortunately misleading log message aside).\r\n\r\nFurthermore, going back to my original reservation to adding this mutation to the graph - we've consciously avoided C APIs to mutate existing nodes in the graph as it can be hard to determine whether or not the mutations apply correctly. For example, consider the following:\r\n\r\n```c\r\nTF_Graph* graph = MakeMyGraph();\r\nTF_BindToDevice(graph, \"/cpu:0\");\r\nTF_Session* session = TF_NewSession(graph, ...);\r\nTF_SessionRun(session, ...);\r\nTF_BindToDevice(graph, \"/gpu:0\");\r\nTF_SessionRun(session, ...);\r\n```\r\n\r\nIn this snippet, the second call to `TF_BindToDevice` has no effect, and all operations will execute on CPU. I'm hesitant to add a function which will have such an esoteric and hard to explain contract.\r\n\r\nAlternatives would be the following:\r\n\r\n1. Do this binding as an option to `TF_ImportGraphDef`, so it applies only to nodes being imported.\r\n2. Rewrite the `GraphDef` proto before importing (see [similar snippet for Java](https://stackoverflow.com/questions/47799972/tensorflow-java-multi-gpu-inference/47915987#47915987))\r\n3. As suggested in the StackOverflow question above, somehow add a notion of virtual devices so that each `Session` can map the device in the graph to a different device at runtime.\r\n\r\nSound reasonable?"}