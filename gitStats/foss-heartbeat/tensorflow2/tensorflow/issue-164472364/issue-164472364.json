{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3234", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3234/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3234/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3234/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3234", "id": 164472364, "node_id": "MDU6SXNzdWUxNjQ0NzIzNjQ=", "number": 3234, "title": "distributed tensorflow does not use GPU 1,2,3 on server B ", "user": {"login": "hellf", "id": 9377459, "node_id": "MDQ6VXNlcjkzNzc0NTk=", "avatar_url": "https://avatars0.githubusercontent.com/u/9377459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hellf", "html_url": "https://github.com/hellf", "followers_url": "https://api.github.com/users/hellf/followers", "following_url": "https://api.github.com/users/hellf/following{/other_user}", "gists_url": "https://api.github.com/users/hellf/gists{/gist_id}", "starred_url": "https://api.github.com/users/hellf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hellf/subscriptions", "organizations_url": "https://api.github.com/users/hellf/orgs", "repos_url": "https://api.github.com/users/hellf/repos", "events_url": "https://api.github.com/users/hellf/events{/privacy}", "received_events_url": "https://api.github.com/users/hellf/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-07-08T07:13:38Z", "updated_at": "2016-08-08T17:52:06Z", "closed_at": "2016-08-08T17:52:05Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04 desktop<br>\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5<br>\ntensorflow 0.9.0.rc0 is installed from source.</p>\n<hr>\n<p>I'm using distributed tensorflow with 8 Titan-x GPUs in two servers.<br>\n4 GPUs are in one server.</p>\n<p>GU=gpu utilization</p>\n<p>These are what I tested, changing ps and workers.<br>\nCifar-10, ResNet. batchsize=32.</p>\n<ol>\n<li>\n<p>serverA (1 ps, 1 worker),  serverB (None) ==&gt; Mem allocated on (serverA: GPU 0: GU&gt;30%)</p>\n</li>\n<li>\n<p>serverA (1 ps, 4 workers),  serverB (None)  ==&gt; Mem allocated on (serverA: GPU 0,1,2,3: GU&gt;30%)</p>\n</li>\n<li>\n<p>serverA (1 ps, 1 workers),  serverB (1 ps, 1 worker)  ==&gt; Mem allocated on (serverA: GPU 0,1: GU&gt;30%), (serverB: GPU 0: GU ~8%).</p>\n</li>\n<li>\n<p>serverA (1 ps, 1 workers),  serverB (1 ps, 2 workers)  ==&gt; Mem allocated on (serverA: GPU 0,1,2: GU&gt;30%), (serverB: GPU 0: GU ~8%).</p>\n</li>\n<li>\n<p>serverA (1 ps, 1 workers),  serverB (1 ps, 3 workers)  ==&gt; Mem allocated on (serverA: GPU 0,1,2,3: GU&gt;30%), (serverB: GPU 0: GU ~8%).</p>\n</li>\n<li>\n<p>serverA (1 ps, 4 workers),  serverB (1 ps, 4 workers)  ==&gt; Mem allocated on (serverA: GPU 0,1,2,3: GU&gt;50%), (serverB: GPU 0: GU ~8%).</p>\n</li>\n</ol>\n<p>It looks dist tensorflow does not use GPU on serverB.</p>\n<p>next is \"nvidia-smi\" in case of 6)</p>\n<p>((serverA)) Mem allocated on GPU 0,1,2,3<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png\"><img src=\"https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>((serverB)) Mem allocated on GPU 0 (last one is 0)<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png\"><img src=\"https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>I use replica_device_setter to allocate workers to GPU.</p>\n<pre><code>if FLAGS.job == \"ps\":\n    server.join()\nelif FLAGS.job == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster)):\n</code></pre>\n<p>please help me.</p>", "body_text": "Environment info\nOperating System: Ubuntu 14.04 desktop\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5\ntensorflow 0.9.0.rc0 is installed from source.\n\nI'm using distributed tensorflow with 8 Titan-x GPUs in two servers.\n4 GPUs are in one server.\nGU=gpu utilization\nThese are what I tested, changing ps and workers.\nCifar-10, ResNet. batchsize=32.\n\n\nserverA (1 ps, 1 worker),  serverB (None) ==> Mem allocated on (serverA: GPU 0: GU>30%)\n\n\nserverA (1 ps, 4 workers),  serverB (None)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%)\n\n\nserverA (1 ps, 1 workers),  serverB (1 ps, 1 worker)  ==> Mem allocated on (serverA: GPU 0,1: GU>30%), (serverB: GPU 0: GU ~8%).\n\n\nserverA (1 ps, 1 workers),  serverB (1 ps, 2 workers)  ==> Mem allocated on (serverA: GPU 0,1,2: GU>30%), (serverB: GPU 0: GU ~8%).\n\n\nserverA (1 ps, 1 workers),  serverB (1 ps, 3 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%), (serverB: GPU 0: GU ~8%).\n\n\nserverA (1 ps, 4 workers),  serverB (1 ps, 4 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>50%), (serverB: GPU 0: GU ~8%).\n\n\nIt looks dist tensorflow does not use GPU on serverB.\nnext is \"nvidia-smi\" in case of 6)\n((serverA)) Mem allocated on GPU 0,1,2,3\n\n((serverB)) Mem allocated on GPU 0 (last one is 0)\n\nI use replica_device_setter to allocate workers to GPU.\nif FLAGS.job == \"ps\":\n    server.join()\nelif FLAGS.job == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster)):\n\nplease help me.", "body": "### Environment info\n\nOperating System: Ubuntu 14.04 desktop\nInstalled version of CUDA and cuDNN: 7.5,  5.0.5\n tensorflow 0.9.0.rc0 is installed from source.\n\n---\n\nI'm using distributed tensorflow with 8 Titan-x GPUs in two servers.\n4 GPUs are in one server.\n\nGU=gpu utilization\n\nThese are what I tested, changing ps and workers.\nCifar-10, ResNet. batchsize=32.\n\n1) serverA (1 ps, 1 worker),  serverB (None) ==> Mem allocated on (serverA: GPU 0: GU>30%)\n\n2) serverA (1 ps, 4 workers),  serverB (None)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%)\n\n3) serverA (1 ps, 1 workers),  serverB (1 ps, 1 worker)  ==> Mem allocated on (serverA: GPU 0,1: GU>30%), (serverB: GPU 0: GU ~8%). \n\n4) serverA (1 ps, 1 workers),  serverB (1 ps, 2 workers)  ==> Mem allocated on (serverA: GPU 0,1,2: GU>30%), (serverB: GPU 0: GU ~8%).\n\n5) serverA (1 ps, 1 workers),  serverB (1 ps, 3 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>30%), (serverB: GPU 0: GU ~8%).\n\n6) serverA (1 ps, 4 workers),  serverB (1 ps, 4 workers)  ==> Mem allocated on (serverA: GPU 0,1,2,3: GU>50%), (serverB: GPU 0: GU ~8%).\n\nIt looks dist tensorflow does not use GPU on serverB.\n\nnext is \"nvidia-smi\" in case of 6)\n\n((serverA)) Mem allocated on GPU 0,1,2,3\n![image](https://cloud.githubusercontent.com/assets/9377459/16659014/75379c00-44a3-11e6-957c-845bed33f492.png)\n\n((serverB)) Mem allocated on GPU 0 (last one is 0)\n![image](https://cloud.githubusercontent.com/assets/9377459/16658977/5748e794-44a3-11e6-94d1-f2eac24639cf.png)\n\nI use replica_device_setter to allocate workers to GPU.\n\n```\nif FLAGS.job == \"ps\":\n    server.join()\nelif FLAGS.job == \"worker\":\n    # Assigns ops to the local worker by default.\n    with tf.device(tf.train.replica_device_setter(worker_device=\"/gpu:%d\" % (FLAGS.task_id%4), cluster=cluster)):\n```\n\nplease help me.\n"}