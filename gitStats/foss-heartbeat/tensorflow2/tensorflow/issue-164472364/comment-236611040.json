{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/236611040", "html_url": "https://github.com/tensorflow/tensorflow/issues/3234#issuecomment-236611040", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3234", "id": 236611040, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNjYxMTA0MA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-01T15:15:53Z", "updated_at": "2016-08-01T15:15:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It looks like the problem is in your <code>worker_device=\"/gpu:%d\" % (FLAGS.task_id%4)</code> argument to <code>tf.train.replica_device_setter()</code>. There are two parts to the problem:</p>\n<ol>\n<li>The device string doesn't specify a task ID (i.e. <code>\"/task:%d\" % (FLAGS.task_id)</code>). Unless you have specified <code>device_filters</code> in your session configuration, this will result in all ops being placed in task 0, which runs in server A.</li>\n<li>Each process on a particular server is creating devices <code>\"/gpu:0\"</code>, ... ,<code>\"/gpu:3\"</code>, because by default a server (or a single-process <code>tf.Session</code>) will create one TensorFlow device per physical device on the system. This will lead to inefficient memory allocation between the processes. You should use the <code>CUDA_VISIBLE_DEVICES</code> environment variable to limit each server to being able to see only a single device, which will be available as <code>\"/gpu:0\"</code> in that process.</li>\n</ol>\n<p>After setting <code>CUDA_VISIBLE_DEVICES</code> appropriately, you can use <code>worker_device=\"/job:worker/task:%d/gpu:0\" % (FLAGS.task_id)</code> as the argument to <code>tf.train.replica_device_setter()</code>, and the utilization should be balanced across the GPUs (assuming that you build the same graph in each of your worker processes, and use something like the <code>tf.train.Supervisor</code> to manage the distributed execution).</p>", "body_text": "It looks like the problem is in your worker_device=\"/gpu:%d\" % (FLAGS.task_id%4) argument to tf.train.replica_device_setter(). There are two parts to the problem:\n\nThe device string doesn't specify a task ID (i.e. \"/task:%d\" % (FLAGS.task_id)). Unless you have specified device_filters in your session configuration, this will result in all ops being placed in task 0, which runs in server A.\nEach process on a particular server is creating devices \"/gpu:0\", ... ,\"/gpu:3\", because by default a server (or a single-process tf.Session) will create one TensorFlow device per physical device on the system. This will lead to inefficient memory allocation between the processes. You should use the CUDA_VISIBLE_DEVICES environment variable to limit each server to being able to see only a single device, which will be available as \"/gpu:0\" in that process.\n\nAfter setting CUDA_VISIBLE_DEVICES appropriately, you can use worker_device=\"/job:worker/task:%d/gpu:0\" % (FLAGS.task_id) as the argument to tf.train.replica_device_setter(), and the utilization should be balanced across the GPUs (assuming that you build the same graph in each of your worker processes, and use something like the tf.train.Supervisor to manage the distributed execution).", "body": "It looks like the problem is in your `worker_device=\"/gpu:%d\" % (FLAGS.task_id%4)` argument to `tf.train.replica_device_setter()`. There are two parts to the problem:\n1. The device string doesn't specify a task ID (i.e. `\"/task:%d\" % (FLAGS.task_id)`). Unless you have specified `device_filters` in your session configuration, this will result in all ops being placed in task 0, which runs in server A.\n2. Each process on a particular server is creating devices `\"/gpu:0\"`, ... ,`\"/gpu:3\"`, because by default a server (or a single-process `tf.Session`) will create one TensorFlow device per physical device on the system. This will lead to inefficient memory allocation between the processes. You should use the `CUDA_VISIBLE_DEVICES` environment variable to limit each server to being able to see only a single device, which will be available as `\"/gpu:0\"` in that process. \n\nAfter setting `CUDA_VISIBLE_DEVICES` appropriately, you can use `worker_device=\"/job:worker/task:%d/gpu:0\" % (FLAGS.task_id)` as the argument to `tf.train.replica_device_setter()`, and the utilization should be balanced across the GPUs (assuming that you build the same graph in each of your worker processes, and use something like the `tf.train.Supervisor` to manage the distributed execution).\n"}