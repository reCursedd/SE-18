{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3560", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3560/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3560/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3560/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3560", "id": 168255435, "node_id": "MDU6SXNzdWUxNjgyNTU0MzU=", "number": 3560, "title": "Inception retraining / transfer learning fails when running with GPU", "user": {"login": "HadronCloud", "id": 13091475, "node_id": "MDEyOk9yZ2FuaXphdGlvbjEzMDkxNDc1", "avatar_url": "https://avatars0.githubusercontent.com/u/13091475?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HadronCloud", "html_url": "https://github.com/HadronCloud", "followers_url": "https://api.github.com/users/HadronCloud/followers", "following_url": "https://api.github.com/users/HadronCloud/following{/other_user}", "gists_url": "https://api.github.com/users/HadronCloud/gists{/gist_id}", "starred_url": "https://api.github.com/users/HadronCloud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HadronCloud/subscriptions", "organizations_url": "https://api.github.com/users/HadronCloud/orgs", "repos_url": "https://api.github.com/users/HadronCloud/repos", "events_url": "https://api.github.com/users/HadronCloud/events{/privacy}", "received_events_url": "https://api.github.com/users/HadronCloud/received_events", "type": "Organization", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-07-29T05:49:01Z", "updated_at": "2018-01-03T12:24:25Z", "closed_at": "2016-08-01T07:55:59Z", "author_association": "NONE", "body_html": "<p>Thanks so much for releasing TensorFlow.  We're experimenting with the image retraining example as described here: <a href=\"https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html</a></p>\n<p>Everything in TensorFlow has worked perfectly for us, including the test and GPU setup validation samples.  However, when running the Inception retraining code and using the GPU, TensorFlow errors.  Since the error occurs in <code>check_numerics_op.cc</code>, I thought it might be worth reporting.</p>\n<p>For example, CPU-only bottleneck generation for 600 classes on a recent 36-core machine takes nearly a month, so working multi-GPU support for bottleneck creation would be really great.  It would help us learn faster and hopefully contribute to the project sooner.</p>\n<p>Abbreviated output (full output is attached):<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/389942/TensorFlow_Retraining_Error.txt\">TensorFlow_Retraining_Error.txt</a></p>\n<p>python tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos<br>\nMon Jul 25 00:54:39 PDT 2016<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\nname: GeForce GTX 1080<br>\n...<br>\nCreating bottleneck at /tmp/bottleneck/dandelion/3365850019_8158a161a8_n.jpg.txt<br>\n<strong>E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x1000e000300 = {1, 0} activation input is not finite.</strong><br>\nTraceback (most recent call last):<br>\nFile \"tensorflow/examples/image_retraining/retrain.py\", line 824, in <br>\ntf.app.run()</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04<br>\nGPU: GTX 1080 (two cards)</p>\n<p>Installed version of CUDA and cuDNN:<br>\n/usr/local/cuda/lib/libcudadevrt.a<br>\n/usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.5<br>\n/usr/local/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n/usr/local/cuda/lib/libcudart.so.7.5.18<br>\n/usr/local/cuda/lib/libcudart_static.a</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\nv0.9.0 <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/25023dffcf88f46777b5ddab457ac84a5bed5d2f/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/25023dffcf88f46777b5ddab457ac84a5bed5d2f\"><tt>25023df</tt></a></li>\n<li>The output of <code>bazel version</code><br>\nBuild label: 0.2.3<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Tue May 17 14:21:13 2016 (1463494873)<br>\nBuild timestamp: 1463494873<br>\nBuild timestamp as int: 1463494873</li>\n</ol>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Build tensorflow from source with GPU support; all demos and tests working properly</li>\n<li><code>bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos</code> runs properly but only uses CPU when generating bottleneck files</li>\n<li><code>python tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos</code> utilizes the GPU, but crashes during bottleneck file generation due to \"irregular\" response from GPU.  If the /tmp/bottlenecks directory has not yet been generated, image retraining using the gpu will fail almost immediately after processing a few files.</li>\n</ol>\n<h3>What have you tried?</h3>\n<ol>\n<li>If cached bottleneck files already exist in /tmp/bottlenecks, then the retraining on GPU works properly.  However, building those bottleneck files in the first place depends on the CPU to generate those bottlenecks, which takes forever (nearly a month for a dual socket E5-2699 v3 36-core machine for around 600 classes)</li>\n</ol>", "body_text": "Thanks so much for releasing TensorFlow.  We're experimenting with the image retraining example as described here: https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html\nEverything in TensorFlow has worked perfectly for us, including the test and GPU setup validation samples.  However, when running the Inception retraining code and using the GPU, TensorFlow errors.  Since the error occurs in check_numerics_op.cc, I thought it might be worth reporting.\nFor example, CPU-only bottleneck generation for 600 classes on a recent 36-core machine takes nearly a month, so working multi-GPU support for bottleneck creation would be really great.  It would help us learn faster and hopefully contribute to the project sooner.\nAbbreviated output (full output is attached):\nTensorFlow_Retraining_Error.txt\npython tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos\nMon Jul 25 00:54:39 PDT 2016\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX 1080\n...\nCreating bottleneck at /tmp/bottleneck/dandelion/3365850019_8158a161a8_n.jpg.txt\nE tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x1000e000300 = {1, 0} activation input is not finite.\nTraceback (most recent call last):\nFile \"tensorflow/examples/image_retraining/retrain.py\", line 824, in \ntf.app.run()\nEnvironment info\nOperating System: Ubuntu 14.04\nGPU: GTX 1080 (two cards)\nInstalled version of CUDA and cuDNN:\n/usr/local/cuda/lib/libcudadevrt.a\n/usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\n/usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n/usr/local/cuda/lib/libcudart.so.7.5.18\n/usr/local/cuda/lib/libcudart_static.a\n\nThe commit hash (git rev-parse HEAD)\nv0.9.0 25023df\nThe output of bazel version\nBuild label: 0.2.3\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue May 17 14:21:13 2016 (1463494873)\nBuild timestamp: 1463494873\nBuild timestamp as int: 1463494873\n\nSteps to reproduce\n\nBuild tensorflow from source with GPU support; all demos and tests working properly\nbazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos runs properly but only uses CPU when generating bottleneck files\npython tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos utilizes the GPU, but crashes during bottleneck file generation due to \"irregular\" response from GPU.  If the /tmp/bottlenecks directory has not yet been generated, image retraining using the gpu will fail almost immediately after processing a few files.\n\nWhat have you tried?\n\nIf cached bottleneck files already exist in /tmp/bottlenecks, then the retraining on GPU works properly.  However, building those bottleneck files in the first place depends on the CPU to generate those bottlenecks, which takes forever (nearly a month for a dual socket E5-2699 v3 36-core machine for around 600 classes)", "body": "Thanks so much for releasing TensorFlow.  We're experimenting with the image retraining example as described here: https://www.tensorflow.org/versions/r0.9/how_tos/image_retraining/index.html\n\nEverything in TensorFlow has worked perfectly for us, including the test and GPU setup validation samples.  However, when running the Inception retraining code and using the GPU, TensorFlow errors.  Since the error occurs in `check_numerics_op.cc`, I thought it might be worth reporting.  \n\nFor example, CPU-only bottleneck generation for 600 classes on a recent 36-core machine takes nearly a month, so working multi-GPU support for bottleneck creation would be really great.  It would help us learn faster and hopefully contribute to the project sooner.\n\nAbbreviated output (full output is attached): \n[TensorFlow_Retraining_Error.txt](https://github.com/tensorflow/tensorflow/files/389942/TensorFlow_Retraining_Error.txt)\n\npython tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos\nMon Jul 25 00:54:39 PDT 2016\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 1080\n...\nCreating bottleneck at /tmp/bottleneck/dandelion/3365850019_8158a161a8_n.jpg.txt\n**E tensorflow/core/kernels/check_numerics_op.cc:157] abnormal_detected_host @0x1000e000300 = {1, 0} activation input is not finite.**\nTraceback (most recent call last):\n  File \"tensorflow/examples/image_retraining/retrain.py\", line 824, in <module>\n    tf.app.run()\n### Environment info\n\nOperating System: Ubuntu 14.04\nGPU: GTX 1080 (two cards)\n\nInstalled version of CUDA and cuDNN: \n /usr/local/cuda/lib/libcudadevrt.a\n /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\n /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n /usr/local/cuda/lib/libcudart.so.7.5.18\n /usr/local/cuda/lib/libcudart_static.a\n1. The commit hash (`git rev-parse HEAD`)\n   v0.9.0 25023df\n2. The output of `bazel version`\n   Build label: 0.2.3\n   Build target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\n   Build time: Tue May 17 14:21:13 2016 (1463494873)\n   Build timestamp: 1463494873\n   Build timestamp as int: 1463494873\n### Steps to reproduce\n1. Build tensorflow from source with GPU support; all demos and tests working properly\n2. `bazel-bin/tensorflow/examples/image_retraining/retrain --image_dir ~/flower_photos` runs properly but only uses CPU when generating bottleneck files\n3. `python tensorflow/examples/image_retraining/retrain.py --image_dir ~/flower_photos` utilizes the GPU, but crashes during bottleneck file generation due to \"irregular\" response from GPU.  If the /tmp/bottlenecks directory has not yet been generated, image retraining using the gpu will fail almost immediately after processing a few files.\n### What have you tried?\n1.  If cached bottleneck files already exist in /tmp/bottlenecks, then the retraining on GPU works properly.  However, building those bottleneck files in the first place depends on the CPU to generate those bottlenecks, which takes forever (nearly a month for a dual socket E5-2699 v3 36-core machine for around 600 classes)\n"}