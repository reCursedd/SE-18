{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/869", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/869/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/869/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/869/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/869", "id": 128429001, "node_id": "MDU6SXNzdWUxMjg0MjkwMDE=", "number": 869, "title": "Error in description of outputs of rnn_decoder", "user": {"login": "pender", "id": 1160207, "node_id": "MDQ6VXNlcjExNjAyMDc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1160207?v=4", "gravatar_id": "", "url": "https://api.github.com/users/pender", "html_url": "https://github.com/pender", "followers_url": "https://api.github.com/users/pender/followers", "following_url": "https://api.github.com/users/pender/following{/other_user}", "gists_url": "https://api.github.com/users/pender/gists{/gist_id}", "starred_url": "https://api.github.com/users/pender/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/pender/subscriptions", "organizations_url": "https://api.github.com/users/pender/orgs", "repos_url": "https://api.github.com/users/pender/repos", "events_url": "https://api.github.com/users/pender/events{/privacy}", "received_events_url": "https://api.github.com/users/pender/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2016-01-24T22:14:39Z", "updated_at": "2016-01-28T07:02:21Z", "closed_at": "2016-01-28T07:02:21Z", "author_association": "NONE", "body_html": "<p>rnn_decoder is described as follows:</p>\n<pre><code>def rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  \"\"\"RNN decoder for the sequence-to-sequence model.\n...\nReturns:\n...\n    states: The state of each cell in each time-step. This is a list with\n      length len(decoder_inputs) -- one item for each time-step.\n</code></pre>\n<p>States output does not appear to have the length as the decoder inputs. For example, the following script --</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.models.rnn import rnn_cell\nfrom tensorflow.models.rnn import seq2seq\nbatch_size = 4\nseq_len = 5\nnum_layers = 3\nvocab_size = 26\nrnn_size = 100\ncell = rnn_cell.MultiRNNCell([rnn_cell.BasicLSTMCell(rnn_size)] * num_layers)\ninput_data = tf.placeholder(tf.int32, [batch_size, seq_len])\nembedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\ndecoder_inputs = tf.split(1, seq_len, tf.nn.embedding_lookup(embedding, input_data))\ndecoder_inputs = [tf.squeeze(input_, [1]) for input_ in decoder_inputs]\noutputs, states = seq2seq.rnn_decoder(decoder_inputs, cell.zero_state(batch_size, tf.float32), cell)\nprint \"Length of states is {}, but length of decoder inputs is {}\".format(len(states), len(decoder_inputs))\n</code></pre>\n<p>gives the following output:</p>\n<pre><code>Length of states is 6, but length of decoder inputs is 5\n</code></pre>\n<p>This is still true if you replace BasicLSTMCell with GRUCell.</p>\n<p>Is it correct to say that the length of states is equal to <em>one greater than</em> the length of decoder inputs? Seems to be true from playing around with it. Is this a fencepost error, where the states include the zeroth state <em>plus</em> the state following each decoder input?</p>", "body_text": "rnn_decoder is described as follows:\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  \"\"\"RNN decoder for the sequence-to-sequence model.\n...\nReturns:\n...\n    states: The state of each cell in each time-step. This is a list with\n      length len(decoder_inputs) -- one item for each time-step.\n\nStates output does not appear to have the length as the decoder inputs. For example, the following script --\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn_cell\nfrom tensorflow.models.rnn import seq2seq\nbatch_size = 4\nseq_len = 5\nnum_layers = 3\nvocab_size = 26\nrnn_size = 100\ncell = rnn_cell.MultiRNNCell([rnn_cell.BasicLSTMCell(rnn_size)] * num_layers)\ninput_data = tf.placeholder(tf.int32, [batch_size, seq_len])\nembedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\ndecoder_inputs = tf.split(1, seq_len, tf.nn.embedding_lookup(embedding, input_data))\ndecoder_inputs = [tf.squeeze(input_, [1]) for input_ in decoder_inputs]\noutputs, states = seq2seq.rnn_decoder(decoder_inputs, cell.zero_state(batch_size, tf.float32), cell)\nprint \"Length of states is {}, but length of decoder inputs is {}\".format(len(states), len(decoder_inputs))\n\ngives the following output:\nLength of states is 6, but length of decoder inputs is 5\n\nThis is still true if you replace BasicLSTMCell with GRUCell.\nIs it correct to say that the length of states is equal to one greater than the length of decoder inputs? Seems to be true from playing around with it. Is this a fencepost error, where the states include the zeroth state plus the state following each decoder input?", "body": "rnn_decoder is described as follows:\n\n```\ndef rnn_decoder(decoder_inputs, initial_state, cell, loop_function=None,\n                scope=None):\n  \"\"\"RNN decoder for the sequence-to-sequence model.\n...\nReturns:\n...\n    states: The state of each cell in each time-step. This is a list with\n      length len(decoder_inputs) -- one item for each time-step.\n```\n\nStates output does not appear to have the length as the decoder inputs. For example, the following script --\n\n```\nimport tensorflow as tf\nfrom tensorflow.models.rnn import rnn_cell\nfrom tensorflow.models.rnn import seq2seq\nbatch_size = 4\nseq_len = 5\nnum_layers = 3\nvocab_size = 26\nrnn_size = 100\ncell = rnn_cell.MultiRNNCell([rnn_cell.BasicLSTMCell(rnn_size)] * num_layers)\ninput_data = tf.placeholder(tf.int32, [batch_size, seq_len])\nembedding = tf.get_variable(\"embedding\", [vocab_size, rnn_size])\ndecoder_inputs = tf.split(1, seq_len, tf.nn.embedding_lookup(embedding, input_data))\ndecoder_inputs = [tf.squeeze(input_, [1]) for input_ in decoder_inputs]\noutputs, states = seq2seq.rnn_decoder(decoder_inputs, cell.zero_state(batch_size, tf.float32), cell)\nprint \"Length of states is {}, but length of decoder inputs is {}\".format(len(states), len(decoder_inputs))\n```\n\ngives the following output:\n\n```\nLength of states is 6, but length of decoder inputs is 5\n```\n\nThis is still true if you replace BasicLSTMCell with GRUCell.\n\nIs it correct to say that the length of states is equal to _one greater than_ the length of decoder inputs? Seems to be true from playing around with it. Is this a fencepost error, where the states include the zeroth state _plus_ the state following each decoder input?\n"}