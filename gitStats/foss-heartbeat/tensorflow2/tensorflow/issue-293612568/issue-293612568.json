{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16665", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16665/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16665/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16665/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16665", "id": 293612568, "node_id": "MDU6SXNzdWUyOTM2MTI1Njg=", "number": 16665, "title": "Android: No OpKernel was registered to support Op 'Min' with these attrs when using custom TensorFlow library built with SELECTIVE_REGISTRATION", "user": {"login": "GauthierChan", "id": 14746791, "node_id": "MDQ6VXNlcjE0NzQ2Nzkx", "avatar_url": "https://avatars1.githubusercontent.com/u/14746791?v=4", "gravatar_id": "", "url": "https://api.github.com/users/GauthierChan", "html_url": "https://github.com/GauthierChan", "followers_url": "https://api.github.com/users/GauthierChan/followers", "following_url": "https://api.github.com/users/GauthierChan/following{/other_user}", "gists_url": "https://api.github.com/users/GauthierChan/gists{/gist_id}", "starred_url": "https://api.github.com/users/GauthierChan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/GauthierChan/subscriptions", "organizations_url": "https://api.github.com/users/GauthierChan/orgs", "repos_url": "https://api.github.com/users/GauthierChan/repos", "events_url": "https://api.github.com/users/GauthierChan/events{/privacy}", "received_events_url": "https://api.github.com/users/GauthierChan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": {"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-02-01T17:13:50Z", "updated_at": "2018-11-14T19:15:01Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes. See below</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: MacOS High Sierra</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.5.0</li>\n<li><strong>Python version</strong>: 2.7.10</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.7.0-homebrew</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.2.1</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When running a <strong>custom TensorFlow library</strong> built with <code>SELECTIVE_REGISTRATION</code>and running our <strong>quantized model</strong> on Android we see this crash log:</p>\n<div class=\"highlight highlight-source-ruby\"><pre> java.lang.<span class=\"pl-c1\">IllegalArgumentException:</span> <span class=\"pl-c1\">No</span> <span class=\"pl-c1\">OpKernel</span> was registered to support <span class=\"pl-c1\">Op</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>Min<span class=\"pl-pds\">'</span></span> with these attrs.  <span class=\"pl-c1\">Registered</span> <span class=\"pl-c1\">devices:</span> [<span class=\"pl-c1\">CPU</span>], <span class=\"pl-c1\">Registered</span> <span class=\"pl-c1\">kernels:</span>\n <span class=\"pl-k\">&lt;</span>no registered kernels<span class=\"pl-k\">&gt;</span>\n\n [[<span class=\"pl-c1\">Node:</span> mul_2_eightbit<span class=\"pl-k\">/</span>mul_2<span class=\"pl-k\">/</span>y<span class=\"pl-k\">/</span>min <span class=\"pl-k\">=</span> <span class=\"pl-c1\">Min</span>[<span class=\"pl-c1\">T</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DT_FLOAT</span>, <span class=\"pl-c1\">Tidx</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DT_INT32</span>, keep_dims<span class=\"pl-k\">=</span><span class=\"pl-c1\">false</span>](mul_2_eightbit<span class=\"pl-k\">/</span>mul_2<span class=\"pl-k\">/</span>y<span class=\"pl-k\">/</span>reshape, mul_2_eightbit<span class=\"pl-k\">/</span>mul_2<span class=\"pl-k\">/</span>y<span class=\"pl-k\">/</span>reduction_dims)]]\n at org.tensorflow.<span class=\"pl-c1\">Session</span>.run(<span class=\"pl-c1\">Native</span> <span class=\"pl-c1\">Method</span>)\n at org.tensorflow.<span class=\"pl-c1\">Session</span>.access<span class=\"pl-smi\">$100</span>(<span class=\"pl-c1\">Session</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">48</span>)\n at org.tensorflow.<span class=\"pl-c1\">Session</span><span class=\"pl-smi\">$Runner</span>.runHelper(<span class=\"pl-c1\">Session</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">298</span>)\n at org.tensorflow.<span class=\"pl-c1\">Session</span><span class=\"pl-smi\">$Runner</span>.run(<span class=\"pl-c1\">Session</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">248</span>)\n at org.tensorflow.contrib.android.<span class=\"pl-c1\">TensorFlowInferenceInterface</span>.run(<span class=\"pl-c1\">TensorFlowInferenceInterface</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">230</span>)\n at org.tensorflow.contrib.android.<span class=\"pl-c1\">TensorFlowInferenceInterface</span>.run(<span class=\"pl-c1\">TensorFlowInferenceInterface</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">197</span>)\n at io.cometapp.tensortest.models.yolo.<span class=\"pl-c1\">YoloClassifier</span>.predict(<span class=\"pl-c1\">YoloClassifier</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">81</span>)\n at io.cometapp.tensortest.<span class=\"pl-c1\">ClassifierActivity</span><span class=\"pl-smi\">$4</span>.run(<span class=\"pl-c1\">ClassifierActivity</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">713</span>)\n at java.lang.<span class=\"pl-c1\">Thread</span>.run(<span class=\"pl-c1\">Thread</span>.<span class=\"pl-c1\">java:</span><span class=\"pl-c1\">764</span>)</pre></div>\n<p>Here is how we build the custom TensorFlow Library</p>\n<p><code>bazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\"  --copt=\"-DTENSORFLOW_DISABLE_META\" --copt=\"-D__ANDROID_TYPES_FULL__\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a</code></p>\n<p>If we do not use <code>--copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" </code>, we are able to run the model successfully.</p>\n<p>Here is the ops_to_register.h that we use</p>\n<div class=\"highlight highlight-source-c\"><pre><span class=\"pl-c\"><span class=\"pl-c\">//</span> This file was autogenerated by print_selective_registration_header.py</span>\n#<span class=\"pl-k\">ifndef</span> OPS_TO_REGISTER\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">OPS_TO_REGISTER</span>\n\n    namespace {\n      constexpr <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* <span class=\"pl-c1\">skip</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* x) {\n        <span class=\"pl-k\">return</span> (*x) ? (*x == <span class=\"pl-s\"><span class=\"pl-pds\">'</span> <span class=\"pl-pds\">'</span></span> ? <span class=\"pl-c1\">skip</span>(x + <span class=\"pl-c1\">1</span>) : x) : x;\n      }\n\n      constexpr <span class=\"pl-k\">bool</span> <span class=\"pl-smi\">isequal</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* x, <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* y) {\n        <span class=\"pl-k\">return</span> (*<span class=\"pl-c1\">skip</span>(x) &amp;&amp; *<span class=\"pl-c1\">skip</span>(y))\n                   ? (*<span class=\"pl-c1\">skip</span>(x) == *<span class=\"pl-c1\">skip</span>(y) &amp;&amp; <span class=\"pl-c1\">isequal</span>(<span class=\"pl-c1\">skip</span>(x) + <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">skip</span>(y) + <span class=\"pl-c1\">1</span>))\n                   : (!*<span class=\"pl-c1\">skip</span>(x) &amp;&amp; !*<span class=\"pl-c1\">skip</span>(y));\n      }\n\n      template&lt;<span class=\"pl-k\">int</span> N&gt;\n      <span class=\"pl-k\">struct</span> find_in {\n        <span class=\"pl-k\">static</span> constexpr <span class=\"pl-k\">bool</span> <span class=\"pl-smi\">f</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* x, <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* <span class=\"pl-k\">const</span> y[N]) {\n          <span class=\"pl-k\">return</span> <span class=\"pl-c1\">isequal</span>(x, y[<span class=\"pl-c1\">0</span>]) || find_in&lt;N - <span class=\"pl-c1\">1</span>&gt;::<span class=\"pl-c1\">f</span>(x, y + <span class=\"pl-c1\">1</span>);\n        }\n      };\n\n      template&lt;&gt;\n      <span class=\"pl-k\">struct</span> find_in&lt;<span class=\"pl-c1\">0</span>&gt; {\n        <span class=\"pl-k\">static</span> constexpr <span class=\"pl-k\">bool</span> <span class=\"pl-smi\">f</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* x, <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* <span class=\"pl-k\">const</span> y[]) {\n          <span class=\"pl-k\">return</span> <span class=\"pl-c1\">false</span>;\n        }\n      };\n    }  <span class=\"pl-c\"><span class=\"pl-c\">//</span> end namespace</span>\n    constexpr <span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span>* <span class=\"pl-c1\">kNecessaryOpKernelClasses</span>[] = {\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ConcatV2Op&lt;CPUDevice, float&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ConstantOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>DequantizeOp&lt;CPUDevice, quint8&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>IdentityOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ReductionOp&lt;CPUDevice, float, Eigen::internal::MaxReducer&lt;float&gt;&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BinaryOp&lt; CPUDevice, functor::maximum&lt;float&gt;&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ReductionOp&lt;CPUDevice, float, Eigen::internal::MinReducer&lt;float&gt;&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NoOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>PadOp&lt;CPUDevice, float&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>PlaceholderOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizeV2Op&lt;CPUDevice, quint8&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedBiasAddOp&lt;quint8, quint8, qint32&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedConv2DOp&lt;quint8, quint8, qint32, Im2ColConvFunctor&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedMaxPoolingOp&lt;CPUDevice, quint8&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedMulOp&lt;quint8, qint32&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BinaryOp&lt; CPUDevice, functor::div&lt;float&gt;&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RequantizationRangeOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RequantizeOp&lt;qint32, quint8&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ReshapeOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>BinaryOp&lt; CPUDevice, functor::sub&lt;float&gt;&gt;<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RecvOp<span class=\"pl-pds\">\"</span></span>,\n<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SendOp<span class=\"pl-pds\">\"</span></span>,\n};\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">SHOULD_REGISTER_OP_KERNEL</span>(<span class=\"pl-v\">clz</span>) (find_in&lt;<span class=\"pl-k\">sizeof</span>(<span class=\"pl-c1\">kNecessaryOpKernelClasses</span>) / <span class=\"pl-k\">sizeof</span>(*<span class=\"pl-c1\">kNecessaryOpKernelClasses</span>)&gt;::f(clz, <span class=\"pl-c1\">kNecessaryOpKernelClasses</span>))\n\nconstexpr <span class=\"pl-k\">inline</span> <span class=\"pl-k\">bool</span> <span class=\"pl-en\">ShouldRegisterOp</span>(<span class=\"pl-k\">const</span> <span class=\"pl-k\">char</span> op[]) {\n  <span class=\"pl-k\">return</span> <span class=\"pl-c1\">false</span>\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>ConcatV2<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Const<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Dequantize<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Identity<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Max<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Maximum<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Min<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>NoOp<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Pad<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Placeholder<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizeV2<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedBiasAdd<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedConv2D<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedMaxPool<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>QuantizedMul<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RealDiv<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RequantizationRange<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Requantize<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Reshape<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Sub<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_Recv<span class=\"pl-pds\">\"</span></span>)\n     || <span class=\"pl-c1\">isequal</span>(op, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>_Send<span class=\"pl-pds\">\"</span></span>)\n  ;\n}\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">SHOULD_REGISTER_OP</span>(<span class=\"pl-v\">op</span>) ShouldRegisterOp(op)\n\n#<span class=\"pl-k\">define</span> <span class=\"pl-en\">SHOULD_REGISTER_OP_GRADIENT</span> <span class=\"pl-c1\">false</span>\n#<span class=\"pl-k\">endif</span>\n</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes. See below\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): MacOS High Sierra\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.5.0\nPython version: 2.7.10\nBazel version (if compiling from source): 0.7.0-homebrew\nGCC/Compiler version (if compiling from source): 4.2.1\n\nDescribe the problem\nWhen running a custom TensorFlow library built with SELECTIVE_REGISTRATIONand running our quantized model on Android we see this crash log:\n java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Min' with these attrs.  Registered devices: [CPU], Registered kernels:\n <no registered kernels>\n\n [[Node: mul_2_eightbit/mul_2/y/min = Min[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](mul_2_eightbit/mul_2/y/reshape, mul_2_eightbit/mul_2/y/reduction_dims)]]\n at org.tensorflow.Session.run(Native Method)\n at org.tensorflow.Session.access$100(Session.java:48)\n at org.tensorflow.Session$Runner.runHelper(Session.java:298)\n at org.tensorflow.Session$Runner.run(Session.java:248)\n at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)\n at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\n at io.cometapp.tensortest.models.yolo.YoloClassifier.predict(YoloClassifier.java:81)\n at io.cometapp.tensortest.ClassifierActivity$4.run(ClassifierActivity.java:713)\n at java.lang.Thread.run(Thread.java:764)\nHere is how we build the custom TensorFlow Library\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\"  --copt=\"-DTENSORFLOW_DISABLE_META\" --copt=\"-D__ANDROID_TYPES_FULL__\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\nIf we do not use --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" , we are able to run the model successfully.\nHere is the ops_to_register.h that we use\n// This file was autogenerated by print_selective_registration_header.py\n#ifndef OPS_TO_REGISTER\n#define OPS_TO_REGISTER\n\n    namespace {\n      constexpr const char* skip(const char* x) {\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\n      }\n\n      constexpr bool isequal(const char* x, const char* y) {\n        return (*skip(x) && *skip(y))\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\n                   : (!*skip(x) && !*skip(y));\n      }\n\n      template<int N>\n      struct find_in {\n        static constexpr bool f(const char* x, const char* const y[N]) {\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\n        }\n      };\n\n      template<>\n      struct find_in<0> {\n        static constexpr bool f(const char* x, const char* const y[]) {\n          return false;\n        }\n      };\n    }  // end namespace\n    constexpr const char* kNecessaryOpKernelClasses[] = {\n\"ConcatV2Op<CPUDevice, float>\",\n\"ConstantOp\",\n\"DequantizeOp<CPUDevice, quint8>\",\n\"IdentityOp\",\n\"ReductionOp<CPUDevice, float, Eigen::internal::MaxReducer<float>>\",\n\"BinaryOp< CPUDevice, functor::maximum<float>>\",\n\"ReductionOp<CPUDevice, float, Eigen::internal::MinReducer<float>>\",\n\"NoOp\",\n\"PadOp<CPUDevice, float>\",\n\"PlaceholderOp\",\n\"QuantizeV2Op<CPUDevice, quint8>\",\n\"QuantizedBiasAddOp<quint8, quint8, qint32>\",\n\"QuantizedConv2DOp<quint8, quint8, qint32, Im2ColConvFunctor>\",\n\"QuantizedMaxPoolingOp<CPUDevice, quint8>\",\n\"QuantizedMulOp<quint8, qint32>\",\n\"BinaryOp< CPUDevice, functor::div<float>>\",\n\"RequantizationRangeOp\",\n\"RequantizeOp<qint32, quint8>\",\n\"ReshapeOp\",\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\n\"RecvOp\",\n\"SendOp\",\n};\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\n\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\n  return false\n     || isequal(op, \"ConcatV2\")\n     || isequal(op, \"Const\")\n     || isequal(op, \"Dequantize\")\n     || isequal(op, \"Identity\")\n     || isequal(op, \"Max\")\n     || isequal(op, \"Maximum\")\n     || isequal(op, \"Min\")\n     || isequal(op, \"NoOp\")\n     || isequal(op, \"Pad\")\n     || isequal(op, \"Placeholder\")\n     || isequal(op, \"QuantizeV2\")\n     || isequal(op, \"QuantizedBiasAdd\")\n     || isequal(op, \"QuantizedConv2D\")\n     || isequal(op, \"QuantizedMaxPool\")\n     || isequal(op, \"QuantizedMul\")\n     || isequal(op, \"RealDiv\")\n     || isequal(op, \"RequantizationRange\")\n     || isequal(op, \"Requantize\")\n     || isequal(op, \"Reshape\")\n     || isequal(op, \"Sub\")\n     || isequal(op, \"_Recv\")\n     || isequal(op, \"_Send\")\n  ;\n}\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\n\n#define SHOULD_REGISTER_OP_GRADIENT false\n#endif", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes. See below\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: MacOS High Sierra\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.5.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**: 0.7.0-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: 4.2.1\r\n\r\n### Describe the problem\r\n\r\nWhen running a **custom TensorFlow library** built with `SELECTIVE_REGISTRATION`and running our **quantized model** on Android we see this crash log:\r\n\r\n```Ruby\r\n java.lang.IllegalArgumentException: No OpKernel was registered to support Op 'Min' with these attrs.  Registered devices: [CPU], Registered kernels:\r\n <no registered kernels>\r\n\r\n [[Node: mul_2_eightbit/mul_2/y/min = Min[T=DT_FLOAT, Tidx=DT_INT32, keep_dims=false](mul_2_eightbit/mul_2/y/reshape, mul_2_eightbit/mul_2/y/reduction_dims)]]\r\n at org.tensorflow.Session.run(Native Method)\r\n at org.tensorflow.Session.access$100(Session.java:48)\r\n at org.tensorflow.Session$Runner.runHelper(Session.java:298)\r\n at org.tensorflow.Session$Runner.run(Session.java:248)\r\n at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:230)\r\n at org.tensorflow.contrib.android.TensorFlowInferenceInterface.run(TensorFlowInferenceInterface.java:197)\r\n at io.cometapp.tensortest.models.yolo.YoloClassifier.predict(YoloClassifier.java:81)\r\n at io.cometapp.tensortest.ClassifierActivity$4.run(ClassifierActivity.java:713)\r\n at java.lang.Thread.run(Thread.java:764)\r\n```\r\n\r\nHere is how we build the custom TensorFlow Library\r\n\r\n`\r\nbazel build -c opt --copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\"  --copt=\"-DTENSORFLOW_DISABLE_META\" --copt=\"-D__ANDROID_TYPES_FULL__\"  //tensorflow/contrib/android:libtensorflow_inference.so    --crosstool_top=//external:android/crosstool    --host_crosstool_top=@bazel_tools//tools/cpp:toolchain    --cpu=armeabi-v7a\r\n`\r\n\r\nIf we do not use `--copt=\"-DSELECTIVE_REGISTRATION\" --copt=\"-DSUPPORT_SELECTIVE_REGISTRATION\" `, we are able to run the model successfully.\r\n\r\n\r\nHere is the ops_to_register.h that we use\r\n\r\n``` C\r\n// This file was autogenerated by print_selective_registration_header.py\r\n#ifndef OPS_TO_REGISTER\r\n#define OPS_TO_REGISTER\r\n\r\n    namespace {\r\n      constexpr const char* skip(const char* x) {\r\n        return (*x) ? (*x == ' ' ? skip(x + 1) : x) : x;\r\n      }\r\n\r\n      constexpr bool isequal(const char* x, const char* y) {\r\n        return (*skip(x) && *skip(y))\r\n                   ? (*skip(x) == *skip(y) && isequal(skip(x) + 1, skip(y) + 1))\r\n                   : (!*skip(x) && !*skip(y));\r\n      }\r\n\r\n      template<int N>\r\n      struct find_in {\r\n        static constexpr bool f(const char* x, const char* const y[N]) {\r\n          return isequal(x, y[0]) || find_in<N - 1>::f(x, y + 1);\r\n        }\r\n      };\r\n\r\n      template<>\r\n      struct find_in<0> {\r\n        static constexpr bool f(const char* x, const char* const y[]) {\r\n          return false;\r\n        }\r\n      };\r\n    }  // end namespace\r\n    constexpr const char* kNecessaryOpKernelClasses[] = {\r\n\"ConcatV2Op<CPUDevice, float>\",\r\n\"ConstantOp\",\r\n\"DequantizeOp<CPUDevice, quint8>\",\r\n\"IdentityOp\",\r\n\"ReductionOp<CPUDevice, float, Eigen::internal::MaxReducer<float>>\",\r\n\"BinaryOp< CPUDevice, functor::maximum<float>>\",\r\n\"ReductionOp<CPUDevice, float, Eigen::internal::MinReducer<float>>\",\r\n\"NoOp\",\r\n\"PadOp<CPUDevice, float>\",\r\n\"PlaceholderOp\",\r\n\"QuantizeV2Op<CPUDevice, quint8>\",\r\n\"QuantizedBiasAddOp<quint8, quint8, qint32>\",\r\n\"QuantizedConv2DOp<quint8, quint8, qint32, Im2ColConvFunctor>\",\r\n\"QuantizedMaxPoolingOp<CPUDevice, quint8>\",\r\n\"QuantizedMulOp<quint8, qint32>\",\r\n\"BinaryOp< CPUDevice, functor::div<float>>\",\r\n\"RequantizationRangeOp\",\r\n\"RequantizeOp<qint32, quint8>\",\r\n\"ReshapeOp\",\r\n\"BinaryOp< CPUDevice, functor::sub<float>>\",\r\n\"RecvOp\",\r\n\"SendOp\",\r\n};\r\n#define SHOULD_REGISTER_OP_KERNEL(clz) (find_in<sizeof(kNecessaryOpKernelClasses) / sizeof(*kNecessaryOpKernelClasses)>::f(clz, kNecessaryOpKernelClasses))\r\n\r\nconstexpr inline bool ShouldRegisterOp(const char op[]) {\r\n  return false\r\n     || isequal(op, \"ConcatV2\")\r\n     || isequal(op, \"Const\")\r\n     || isequal(op, \"Dequantize\")\r\n     || isequal(op, \"Identity\")\r\n     || isequal(op, \"Max\")\r\n     || isequal(op, \"Maximum\")\r\n     || isequal(op, \"Min\")\r\n     || isequal(op, \"NoOp\")\r\n     || isequal(op, \"Pad\")\r\n     || isequal(op, \"Placeholder\")\r\n     || isequal(op, \"QuantizeV2\")\r\n     || isequal(op, \"QuantizedBiasAdd\")\r\n     || isequal(op, \"QuantizedConv2D\")\r\n     || isequal(op, \"QuantizedMaxPool\")\r\n     || isequal(op, \"QuantizedMul\")\r\n     || isequal(op, \"RealDiv\")\r\n     || isequal(op, \"RequantizationRange\")\r\n     || isequal(op, \"Requantize\")\r\n     || isequal(op, \"Reshape\")\r\n     || isequal(op, \"Sub\")\r\n     || isequal(op, \"_Recv\")\r\n     || isequal(op, \"_Send\")\r\n  ;\r\n}\r\n#define SHOULD_REGISTER_OP(op) ShouldRegisterOp(op)\r\n\r\n#define SHOULD_REGISTER_OP_GRADIENT false\r\n#endif\r\n\r\n```\r\n"}