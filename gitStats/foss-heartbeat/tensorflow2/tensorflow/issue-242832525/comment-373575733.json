{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/373575733", "html_url": "https://github.com/tensorflow/tensorflow/issues/11489#issuecomment-373575733", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11489", "id": 373575733, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MzU3NTczMw==", "user": {"login": "cweill", "id": 2343711, "node_id": "MDQ6VXNlcjIzNDM3MTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2343711?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cweill", "html_url": "https://github.com/cweill", "followers_url": "https://api.github.com/users/cweill/followers", "following_url": "https://api.github.com/users/cweill/following{/other_user}", "gists_url": "https://api.github.com/users/cweill/gists{/gist_id}", "starred_url": "https://api.github.com/users/cweill/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cweill/subscriptions", "organizations_url": "https://api.github.com/users/cweill/orgs", "repos_url": "https://api.github.com/users/cweill/repos", "events_url": "https://api.github.com/users/cweill/events{/privacy}", "received_events_url": "https://api.github.com/users/cweill/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-16T01:35:14Z", "updated_at": "2018-03-16T01:35:14Z", "author_association": "NONE", "body_html": "<p>I'm also encountering this issue, and have had to use a similar wrapping mechanism to work around it. Here are my steps to reproduce when using an embedding feature column:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> difflib\n\norig_model <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input_map_orig.pb<span class=\"pl-pds\">\"</span></span>\nmapped_model <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input_map_modified.pb<span class=\"pl-pds\">\"</span></span>\n\ntf.reset_default_graph()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">define_graph</span>(<span class=\"pl-smi\">use_placeholder</span>, <span class=\"pl-smi\">wrap</span>):\n  feature <span class=\"pl-k\">=</span> tf.feature_column.embedding_column(\n      tf.feature_column.categorical_column_with_identity(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feature<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">10</span>),\n      <span class=\"pl-v\">dimension</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\n  feature_columns <span class=\"pl-k\">=</span> [feature]\n  <span class=\"pl-k\">if</span> use_placeholder:\n    features <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>)\n  <span class=\"pl-k\">else</span>:\n    features <span class=\"pl-k\">=</span> tf.convert_to_tensor([<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>)\n  <span class=\"pl-k\">if</span> wrap:\n    features <span class=\"pl-k\">=</span> tf.identity(features, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>colocated<span class=\"pl-pds\">\"</span></span>)\n  features <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feature<span class=\"pl-pds\">\"</span></span>: features}\n  input_layer <span class=\"pl-k\">=</span> tf.feature_column.input_layer(features, feature_columns)\n  int_data <span class=\"pl-k\">=</span> tf.to_float(input_layer)\n  process_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">data</span>: tf.square(data)\n  op <span class=\"pl-k\">=</span> tf.map_fn(process_fn, int_data, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>process<span class=\"pl-pds\">'</span></span>)\n  identity <span class=\"pl-k\">=</span> tf.identity(op, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>id<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Set 'wrap' to False to reproduce bug.</span>\ndefine_graph(<span class=\"pl-v\">use_placeholder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">wrap</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">freeze</span>(<span class=\"pl-smi\">graph</span>, <span class=\"pl-smi\">filename</span>):\n  output_names <span class=\"pl-k\">=</span> [<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>id<span class=\"pl-pds\">\"</span></span>]\n  <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>graph) <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n\n    input_graph_def <span class=\"pl-k\">=</span> sess.graph_def\n    output_graph_def <span class=\"pl-k\">=</span> tf.graph_util.convert_variables_to_constants(\n        sess, input_graph_def, output_names)\n    tf.train.write_graph(output_graph_def, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>./<span class=\"pl-pds\">\"</span></span>, filename, <span class=\"pl-c1\">False</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">load_frozen</span>(<span class=\"pl-smi\">filename</span>, <span class=\"pl-smi\">input_map</span><span class=\"pl-k\">=</span>{}):\n  graph_def <span class=\"pl-k\">=</span> tf.GraphDef()\n  <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(filename, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>rb<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> f:\n    graph_def.ParseFromString(f.read())\n  tf.import_graph_def(graph_def, <span class=\"pl-v\">input_map</span><span class=\"pl-k\">=</span>input_map, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">make_change_in_graph</span>():\n  op <span class=\"pl-k\">=</span> tf.convert_to_tensor([<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>) \n\n  input_map <span class=\"pl-k\">=</span> {\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>input<span class=\"pl-pds\">\"</span></span>: op,\n  }\n  <span class=\"pl-k\">return</span> input_map\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Save graph.</span>\ngraph <span class=\"pl-k\">=</span> tf.get_default_graph()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print(graph.as_graph_def())</span>\ngraph_proto0 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(tf.get_default_graph().as_graph_def()).splitlines()\nfreeze(graph, orig_model)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Load the graph again</span>\ntf.reset_default_graph()\ninput_map <span class=\"pl-k\">=</span> make_change_in_graph()\nload_frozen(orig_model, input_map)\n\ngraph_proto1 <span class=\"pl-k\">=</span> <span class=\"pl-c1\">str</span>(graph.as_graph_def()).splitlines()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print('\\n'.join(difflib.ndiff(graph_proto0, graph_proto1)))</span>\n\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Write the modified graph</span>\ngraph <span class=\"pl-k\">=</span> tf.get_default_graph()\nfreeze(graph, mapped_model)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Load the graph again</span>\ntf.reset_default_graph()\ninput_map <span class=\"pl-k\">=</span> make_change_in_graph()\n\nload_frozen(mapped_model) <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;------ Bug occurs on second load:</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Write the modified graph</span>\ngraph <span class=\"pl-k\">=</span> tf.get_default_graph()\nfreeze(graph, mapped_model)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Load the graph again</span>\ntf.reset_default_graph()\nload_frozen(mapped_model)</pre></div>\n<p>Is there any reason why the colocation op can't be overwritten?</p>", "body_text": "I'm also encountering this issue, and have had to use a similar wrapping mechanism to work around it. Here are my steps to reproduce when using an embedding feature column:\nimport tensorflow as tf\nimport difflib\n\norig_model = \"input_map_orig.pb\"\nmapped_model = \"input_map_modified.pb\"\n\ntf.reset_default_graph()\n\ndef define_graph(use_placeholder, wrap):\n  feature = tf.feature_column.embedding_column(\n      tf.feature_column.categorical_column_with_identity(\"feature\", 10),\n      dimension=10)\n  feature_columns = [feature]\n  if use_placeholder:\n    features = tf.placeholder(dtype=tf.int32, name=\"input\")\n  else:\n    features = tf.convert_to_tensor([3], name=\"input\")\n  if wrap:\n    features = tf.identity(features, name=\"colocated\")\n  features = {\"feature\": features}\n  input_layer = tf.feature_column.input_layer(features, feature_columns)\n  int_data = tf.to_float(input_layer)\n  process_fn = lambda data: tf.square(data)\n  op = tf.map_fn(process_fn, int_data, name='process')\n  identity = tf.identity(op, name=\"id\")\n\n# Set 'wrap' to False to reproduce bug.\ndefine_graph(use_placeholder=True, wrap=True)\n\ndef freeze(graph, filename):\n  output_names = [\"id\"]\n  with tf.Session(graph=graph) as sess:\n    sess.run(tf.global_variables_initializer())\n\n    input_graph_def = sess.graph_def\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\n        sess, input_graph_def, output_names)\n    tf.train.write_graph(output_graph_def, \"./\", filename, False)\n\ndef load_frozen(filename, input_map={}):\n  graph_def = tf.GraphDef()\n  with open(filename, \"rb\") as f:\n    graph_def.ParseFromString(f.read())\n  tf.import_graph_def(graph_def, input_map=input_map, name=\"\")\n\ndef make_change_in_graph():\n  op = tf.convert_to_tensor([3], name=\"input\") \n\n  input_map = {\n    \"input\": op,\n  }\n  return input_map\n\n# Save graph.\ngraph = tf.get_default_graph()\n# print(graph.as_graph_def())\ngraph_proto0 = str(tf.get_default_graph().as_graph_def()).splitlines()\nfreeze(graph, orig_model)\n\n# Load the graph again\ntf.reset_default_graph()\ninput_map = make_change_in_graph()\nload_frozen(orig_model, input_map)\n\ngraph_proto1 = str(graph.as_graph_def()).splitlines()\n# print('\\n'.join(difflib.ndiff(graph_proto0, graph_proto1)))\n\n\n# Write the modified graph\ngraph = tf.get_default_graph()\nfreeze(graph, mapped_model)\n\n# Load the graph again\ntf.reset_default_graph()\ninput_map = make_change_in_graph()\n\nload_frozen(mapped_model) # <------ Bug occurs on second load:\n\n# Write the modified graph\ngraph = tf.get_default_graph()\nfreeze(graph, mapped_model)\n\n# Load the graph again\ntf.reset_default_graph()\nload_frozen(mapped_model)\nIs there any reason why the colocation op can't be overwritten?", "body": "I'm also encountering this issue, and have had to use a similar wrapping mechanism to work around it. Here are my steps to reproduce when using an embedding feature column:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport difflib\r\n\r\norig_model = \"input_map_orig.pb\"\r\nmapped_model = \"input_map_modified.pb\"\r\n\r\ntf.reset_default_graph()\r\n\r\ndef define_graph(use_placeholder, wrap):\r\n  feature = tf.feature_column.embedding_column(\r\n      tf.feature_column.categorical_column_with_identity(\"feature\", 10),\r\n      dimension=10)\r\n  feature_columns = [feature]\r\n  if use_placeholder:\r\n    features = tf.placeholder(dtype=tf.int32, name=\"input\")\r\n  else:\r\n    features = tf.convert_to_tensor([3], name=\"input\")\r\n  if wrap:\r\n    features = tf.identity(features, name=\"colocated\")\r\n  features = {\"feature\": features}\r\n  input_layer = tf.feature_column.input_layer(features, feature_columns)\r\n  int_data = tf.to_float(input_layer)\r\n  process_fn = lambda data: tf.square(data)\r\n  op = tf.map_fn(process_fn, int_data, name='process')\r\n  identity = tf.identity(op, name=\"id\")\r\n\r\n# Set 'wrap' to False to reproduce bug.\r\ndefine_graph(use_placeholder=True, wrap=True)\r\n\r\ndef freeze(graph, filename):\r\n  output_names = [\"id\"]\r\n  with tf.Session(graph=graph) as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    input_graph_def = sess.graph_def\r\n    output_graph_def = tf.graph_util.convert_variables_to_constants(\r\n        sess, input_graph_def, output_names)\r\n    tf.train.write_graph(output_graph_def, \"./\", filename, False)\r\n\r\ndef load_frozen(filename, input_map={}):\r\n  graph_def = tf.GraphDef()\r\n  with open(filename, \"rb\") as f:\r\n    graph_def.ParseFromString(f.read())\r\n  tf.import_graph_def(graph_def, input_map=input_map, name=\"\")\r\n\r\ndef make_change_in_graph():\r\n  op = tf.convert_to_tensor([3], name=\"input\") \r\n\r\n  input_map = {\r\n    \"input\": op,\r\n  }\r\n  return input_map\r\n\r\n# Save graph.\r\ngraph = tf.get_default_graph()\r\n# print(graph.as_graph_def())\r\ngraph_proto0 = str(tf.get_default_graph().as_graph_def()).splitlines()\r\nfreeze(graph, orig_model)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\ninput_map = make_change_in_graph()\r\nload_frozen(orig_model, input_map)\r\n\r\ngraph_proto1 = str(graph.as_graph_def()).splitlines()\r\n# print('\\n'.join(difflib.ndiff(graph_proto0, graph_proto1)))\r\n\r\n\r\n# Write the modified graph\r\ngraph = tf.get_default_graph()\r\nfreeze(graph, mapped_model)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\ninput_map = make_change_in_graph()\r\n\r\nload_frozen(mapped_model) # <------ Bug occurs on second load:\r\n\r\n# Write the modified graph\r\ngraph = tf.get_default_graph()\r\nfreeze(graph, mapped_model)\r\n\r\n# Load the graph again\r\ntf.reset_default_graph()\r\nload_frozen(mapped_model)\r\n```\r\n\r\nIs there any reason why the colocation op can't be overwritten?"}