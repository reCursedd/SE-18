{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/315235292", "html_url": "https://github.com/tensorflow/tensorflow/issues/11489#issuecomment-315235292", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11489", "id": 315235292, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNTIzNTI5Mg==", "user": {"login": "StanislawAntol", "id": 1846231, "node_id": "MDQ6VXNlcjE4NDYyMzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1846231?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StanislawAntol", "html_url": "https://github.com/StanislawAntol", "followers_url": "https://api.github.com/users/StanislawAntol/followers", "following_url": "https://api.github.com/users/StanislawAntol/following{/other_user}", "gists_url": "https://api.github.com/users/StanislawAntol/gists{/gist_id}", "starred_url": "https://api.github.com/users/StanislawAntol/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StanislawAntol/subscriptions", "organizations_url": "https://api.github.com/users/StanislawAntol/orgs", "repos_url": "https://api.github.com/users/StanislawAntol/repos", "events_url": "https://api.github.com/users/StanislawAntol/events{/privacy}", "received_events_url": "https://api.github.com/users/StanislawAntol/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-14T00:13:57Z", "updated_at": "2017-07-14T00:39:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>, thanks for the (quick!) update. Your suggestion of removing <code>graph_util.convert_variables_to_constants()</code> was helpful in avoiding my hacky float32 placeholder-&gt;tf.cast(uint8) prepending. It seems like this fixing this issue would remove extra work-around steps when one is trying to save a model that isn't already turned into constants (e.g., from a checkpoint), so good luck figuring out the best solution.</p>\n<p>EDIT:<br>\nActually, what exactly did you do when you don't perform the 2nd convert? I tried commenting out the convert and changing the write to <code>tf.train.write_graph(input_graph_def, \"./\", mapped_model, False)</code> (i.e., <code>input_graph_def</code> instead of <code>output_graph_def</code>) and, though it still loads, it doesn't cause an error because the 2 nodes that were unconnected are still a part of the graph, but the <code>loc@ToFloat</code> is still present in the scatter node, which I think would result in undesired behavior. (These are just my guesses based on my very limited understanding of TF graphs.)</p>", "body_text": "@mrry, thanks for the (quick!) update. Your suggestion of removing graph_util.convert_variables_to_constants() was helpful in avoiding my hacky float32 placeholder->tf.cast(uint8) prepending. It seems like this fixing this issue would remove extra work-around steps when one is trying to save a model that isn't already turned into constants (e.g., from a checkpoint), so good luck figuring out the best solution.\nEDIT:\nActually, what exactly did you do when you don't perform the 2nd convert? I tried commenting out the convert and changing the write to tf.train.write_graph(input_graph_def, \"./\", mapped_model, False) (i.e., input_graph_def instead of output_graph_def) and, though it still loads, it doesn't cause an error because the 2 nodes that were unconnected are still a part of the graph, but the loc@ToFloat is still present in the scatter node, which I think would result in undesired behavior. (These are just my guesses based on my very limited understanding of TF graphs.)", "body": "@mrry, thanks for the (quick!) update. Your suggestion of removing `graph_util.convert_variables_to_constants()` was helpful in avoiding my hacky float32 placeholder->tf.cast(uint8) prepending. It seems like this fixing this issue would remove extra work-around steps when one is trying to save a model that isn't already turned into constants (e.g., from a checkpoint), so good luck figuring out the best solution.\r\n\r\nEDIT:\r\nActually, what exactly did you do when you don't perform the 2nd convert? I tried commenting out the convert and changing the write to `tf.train.write_graph(input_graph_def, \"./\", mapped_model, False)` (i.e., `input_graph_def` instead of `output_graph_def`) and, though it still loads, it doesn't cause an error because the 2 nodes that were unconnected are still a part of the graph, but the `loc@ToFloat` is still present in the scatter node, which I think would result in undesired behavior. (These are just my guesses based on my very limited understanding of TF graphs.)"}