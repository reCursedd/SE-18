{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354904052", "html_url": "https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-354904052", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15737", "id": 354904052, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDkwNDA1Mg==", "user": {"login": "oahziur", "id": 4604464, "node_id": "MDQ6VXNlcjQ2MDQ0NjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4604464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oahziur", "html_url": "https://github.com/oahziur", "followers_url": "https://api.github.com/users/oahziur/followers", "following_url": "https://api.github.com/users/oahziur/following{/other_user}", "gists_url": "https://api.github.com/users/oahziur/gists{/gist_id}", "starred_url": "https://api.github.com/users/oahziur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oahziur/subscriptions", "organizations_url": "https://api.github.com/users/oahziur/orgs", "repos_url": "https://api.github.com/users/oahziur/repos", "events_url": "https://api.github.com/users/oahziur/events{/privacy}", "received_events_url": "https://api.github.com/users/oahziur/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-02T23:35:08Z", "updated_at": "2018-01-02T23:35:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> The encoder doesn't need to have static batch size, as long as we don't give a static value to the zero_states in this case.</p>\n<p>I think the problem here is clone() copies an encoder state without static batch size to an attention wrapper state that was initialized with static batch size.</p>", "body_text": "@ebrevdo The encoder doesn't need to have static batch size, as long as we don't give a static value to the zero_states in this case.\nI think the problem here is clone() copies an encoder state without static batch size to an attention wrapper state that was initialized with static batch size.", "body": "@ebrevdo The encoder doesn't need to have static batch size, as long as we don't give a static value to the zero_states in this case.\r\n\r\nI think the problem here is clone() copies an encoder state without static batch size to an attention wrapper state that was initialized with static batch size."}