{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/355365961", "html_url": "https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-355365961", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15737", "id": 355365961, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTM2NTk2MQ==", "user": {"login": "oahziur", "id": 4604464, "node_id": "MDQ6VXNlcjQ2MDQ0NjQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/4604464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oahziur", "html_url": "https://github.com/oahziur", "followers_url": "https://api.github.com/users/oahziur/followers", "following_url": "https://api.github.com/users/oahziur/following{/other_user}", "gists_url": "https://api.github.com/users/oahziur/gists{/gist_id}", "starred_url": "https://api.github.com/users/oahziur/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oahziur/subscriptions", "organizations_url": "https://api.github.com/users/oahziur/orgs", "repos_url": "https://api.github.com/users/oahziur/repos", "events_url": "https://api.github.com/users/oahziur/events{/privacy}", "received_events_url": "https://api.github.com/users/oahziur/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T18:51:08Z", "updated_at": "2018-01-04T18:51:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a></p>\n<p><code>clone()</code> maybe the best place to set the new Tensor's batch_size info since that is the only place users may update tensors in the <code>AttentionWrapperState</code>.</p>\n<p>Instead of doing <code>set_shape</code> to copy the static shape information, I think we can use the attention_mechanism.batch_size for the initialization in <code>AttentionWrapper.zero_state()</code> to fix issues like this, so the batch_size info is always consistent with <code>memory</code> and <code>encoder_state</code>.</p>", "body_text": "@ebrevdo\nclone() maybe the best place to set the new Tensor's batch_size info since that is the only place users may update tensors in the AttentionWrapperState.\nInstead of doing set_shape to copy the static shape information, I think we can use the attention_mechanism.batch_size for the initialization in AttentionWrapper.zero_state() to fix issues like this, so the batch_size info is always consistent with memory and encoder_state.", "body": "@ebrevdo \r\n\r\n`clone()` maybe the best place to set the new Tensor's batch_size info since that is the only place users may update tensors in the `AttentionWrapperState`.\r\n\r\nInstead of doing `set_shape` to copy the static shape information, I think we can use the attention_mechanism.batch_size for the initialization in `AttentionWrapper.zero_state()` to fix issues like this, so the batch_size info is always consistent with `memory` and `encoder_state`."}