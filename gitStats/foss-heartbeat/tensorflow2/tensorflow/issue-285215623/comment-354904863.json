{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354904863", "html_url": "https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-354904863", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15737", "id": 354904863, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDkwNDg2Mw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-02T23:40:40Z", "updated_at": "2018-01-02T23:40:40Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Ah!  I see; that's why it lost that size information.  I wonder if we can\nbring that static shape info back in, inside the decode body.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Tue, Jan 2, 2018 at 3:35 PM, Rui Zhao ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt; The encoder doesn't need to have\n static batch size, as long as we don't give a static vale to the\n zero_states in this case.\n\n I think the problem here is clone() copies an encoder state without static\n batch size to an attention wrapper state that was initialized with static\n batch size.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"285215623\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15737\" href=\"https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-354904052\">#15737 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim6yTs3NfmlGXllm2udMKYgVvl5-yks5tGr1QgaJpZM4RPqFa\">https://github.com/notifications/unsubscribe-auth/ABtim6yTs3NfmlGXllm2udMKYgVvl5-yks5tGr1QgaJpZM4RPqFa</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Ah!  I see; that's why it lost that size information.  I wonder if we can\nbring that static shape info back in, inside the decode body.\n\u2026\nOn Tue, Jan 2, 2018 at 3:35 PM, Rui Zhao ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo> The encoder doesn't need to have\n static batch size, as long as we don't give a static vale to the\n zero_states in this case.\n\n I think the problem here is clone() copies an encoder state without static\n batch size to an attention wrapper state that was initialized with static\n batch size.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#15737 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim6yTs3NfmlGXllm2udMKYgVvl5-yks5tGr1QgaJpZM4RPqFa>\n .", "body": "Ah!  I see; that's why it lost that size information.  I wonder if we can\nbring that static shape info back in, inside the decode body.\n\nOn Tue, Jan 2, 2018 at 3:35 PM, Rui Zhao <notifications@github.com> wrote:\n\n> @ebrevdo <https://github.com/ebrevdo> The encoder doesn't need to have\n> static batch size, as long as we don't give a static vale to the\n> zero_states in this case.\n>\n> I think the problem here is clone() copies an encoder state without static\n> batch size to an attention wrapper state that was initialized with static\n> batch size.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/15737#issuecomment-354904052>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6yTs3NfmlGXllm2udMKYgVvl5-yks5tGr1QgaJpZM4RPqFa>\n> .\n>\n"}