{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15331", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15331/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15331/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15331/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15331", "id": 281629232, "node_id": "MDU6SXNzdWUyODE2MjkyMzI=", "number": 15331, "title": "Graph building and optimization is really slow", "user": {"login": "selcouthlyBlue", "id": 13268675, "node_id": "MDQ6VXNlcjEzMjY4Njc1", "avatar_url": "https://avatars2.githubusercontent.com/u/13268675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selcouthlyBlue", "html_url": "https://github.com/selcouthlyBlue", "followers_url": "https://api.github.com/users/selcouthlyBlue/followers", "following_url": "https://api.github.com/users/selcouthlyBlue/following{/other_user}", "gists_url": "https://api.github.com/users/selcouthlyBlue/gists{/gist_id}", "starred_url": "https://api.github.com/users/selcouthlyBlue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selcouthlyBlue/subscriptions", "organizations_url": "https://api.github.com/users/selcouthlyBlue/orgs", "repos_url": "https://api.github.com/users/selcouthlyBlue/repos", "events_url": "https://api.github.com/users/selcouthlyBlue/events{/privacy}", "received_events_url": "https://api.github.com/users/selcouthlyBlue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-12-13T05:26:31Z", "updated_at": "2017-12-13T06:50:06Z", "closed_at": "2017-12-13T06:50:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm using tensorflow 1.4 and tflearn 0.3.2. The os of my machine is Win 8.1. Apparently, the graph building and cost optimization in my code is real slow which I think is a bug in Tensorflow (either that or my machine is just real slow). Here's the code:</p>\n<pre><code>class Model(object):\n    def __init__(self):\n        self.num_classes = 80\n        self.num_time_steps = 1596\n        self.input_dimension = 48\n        self.inputs = network_utils.input_data([None, self.num_time_steps, self.input_dimension], name=\"input\")\n        self.labels = network_utils.sparse_input_data()\n        self.seq_lens = network_utils.input_data([None], name=\"seq_len\", input_type=network_utils.get_type('int32'))\n        self.learning_rate = 0.01\n\n    def _inference(self):\n        model = network_utils.bidirectional_lstm(self.inputs, 50, return_seq=True)\n        model = network_utils.bidirectional_lstm(model, 100, return_seq=True)\n        model = network_utils.bidirectional_lstm(model, 200)\n        logits = network_utils.get_time_major(model, self.num_classes, network_utils.get_shape(self.inputs)[0], 200)\n        return logits\n\n    def loss(self):\n        y_predict = self._inference()\n        loss = network_utils.ctc_loss(predictions=y_predict, labels=self.labels, sequence_length=self.seq_lens)\n        cost = network_utils.cost(loss)\n        decoded = network_utils.decode(inputs=y_predict, sequence_length=self.seq_lens)\n        label_error_rate = network_utils.label_error_rate(y_pred=decoded[0], y_true=self.labels)\n        return loss, label_error_rate, cost\n\n    def optimize(self, cost, optimizer):\n        return network_utils.optimize(loss=cost, optimizer=optimizer, learning_rate=self.learning_rate)\n</code></pre>\n<pre><code>import tensorflow as tf\nimport tflearn\nfrom tflearn import bidirectional_rnn, BasicLSTMCell\n\nfrom optimizer_enum import Optimizers\n\ndef ctc_loss(predictions, labels, sequence_length,\n             preprocess_collapse_repeated_labels=True,\n             ctc_merge_repeated=True,\n             inputs_are_time_major=True):\n    return tf.nn.ctc_loss(inputs=predictions, labels=labels, sequence_length=sequence_length,\n                          preprocess_collapse_repeated=preprocess_collapse_repeated_labels,\n                          ctc_merge_repeated=ctc_merge_repeated,\n                          time_major=inputs_are_time_major)\n\ndef input_data(shape, name: str = 'InputData', input_type=tf.float32):\n    return tflearn.input_data(shape=shape, dtype=input_type, name=name)\n\ndef reshape(tensor: tf.Tensor, new_shape: list):\n    return tf.reshape(tensor, new_shape, name=\"reshape\")\n\ndef bidirectional_lstm(inputs, num_hidden: int, return_seq=False):\n    return bidirectional_rnn(inputs, BasicLSTMCell(num_hidden), BasicLSTMCell(num_hidden), return_seq=return_seq)\n\n\ndef decode(inputs, sequence_length, merge_repeated=True):\n    decoded, _ = tf.nn.ctc_beam_search_decoder(inputs, sequence_length, merge_repeated)\n    return decoded\n\ndef label_error_rate(y_pred, y_true):\n    return tf.reduce_mean(tf.edit_distance(tf.cast(y_pred, tf.int32), y_true))\n\ndef optimize(loss, optimizer, learning_rate):\n    if optimizer == Optimizers.MOMENTUM:\n        return tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss)\n    if optimizer == Optimizers.ADAM:\n        return tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    if optimizer == Optimizers.ADADELTA:\n        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n    if optimizer == Optimizers.RMSPROP:\n        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n    raise NotImplementedError(\"{} is not implemented.\".format(optimizer))\n\ndef sparse_input_data(input_type=tf.int32):\n    return tf.sparse_placeholder(input_type)\n\ndef get_time_major(model, num_classes, batch_size, num_hidden_units):\n    outputs = reshape(model, [-1, num_hidden_units])\n\n    W = tf.Variable(tf.truncated_normal([num_hidden_units,\n                                         num_classes],\n                                        stddev=0.1, dtype=tf.float32), name='W')\n    b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[num_classes], name='b'))\n\n    logits = tf.matmul(outputs, W) + b\n    logits = tf.reshape(logits, [batch_size, -1, num_classes])\n    logits = tf.transpose(logits, (1, 0, 2))\n    return logits\n\ndef cost(loss):\n    return tf.reduce_mean(loss)\n\n\ndef get_type(type_str):\n    if type_str == 'int32':\n        return tf.int32\n    return tf.float32\n\n\ndef get_shape(tensor):\n    return tf.shape(tensor)\n</code></pre>\n<p>Can anyone help me address this issue? To reproduce it, you can run main/train.py in this <a href=\"https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr\">repository</a></p>", "body_text": "I'm using tensorflow 1.4 and tflearn 0.3.2. The os of my machine is Win 8.1. Apparently, the graph building and cost optimization in my code is real slow which I think is a bug in Tensorflow (either that or my machine is just real slow). Here's the code:\nclass Model(object):\n    def __init__(self):\n        self.num_classes = 80\n        self.num_time_steps = 1596\n        self.input_dimension = 48\n        self.inputs = network_utils.input_data([None, self.num_time_steps, self.input_dimension], name=\"input\")\n        self.labels = network_utils.sparse_input_data()\n        self.seq_lens = network_utils.input_data([None], name=\"seq_len\", input_type=network_utils.get_type('int32'))\n        self.learning_rate = 0.01\n\n    def _inference(self):\n        model = network_utils.bidirectional_lstm(self.inputs, 50, return_seq=True)\n        model = network_utils.bidirectional_lstm(model, 100, return_seq=True)\n        model = network_utils.bidirectional_lstm(model, 200)\n        logits = network_utils.get_time_major(model, self.num_classes, network_utils.get_shape(self.inputs)[0], 200)\n        return logits\n\n    def loss(self):\n        y_predict = self._inference()\n        loss = network_utils.ctc_loss(predictions=y_predict, labels=self.labels, sequence_length=self.seq_lens)\n        cost = network_utils.cost(loss)\n        decoded = network_utils.decode(inputs=y_predict, sequence_length=self.seq_lens)\n        label_error_rate = network_utils.label_error_rate(y_pred=decoded[0], y_true=self.labels)\n        return loss, label_error_rate, cost\n\n    def optimize(self, cost, optimizer):\n        return network_utils.optimize(loss=cost, optimizer=optimizer, learning_rate=self.learning_rate)\n\nimport tensorflow as tf\nimport tflearn\nfrom tflearn import bidirectional_rnn, BasicLSTMCell\n\nfrom optimizer_enum import Optimizers\n\ndef ctc_loss(predictions, labels, sequence_length,\n             preprocess_collapse_repeated_labels=True,\n             ctc_merge_repeated=True,\n             inputs_are_time_major=True):\n    return tf.nn.ctc_loss(inputs=predictions, labels=labels, sequence_length=sequence_length,\n                          preprocess_collapse_repeated=preprocess_collapse_repeated_labels,\n                          ctc_merge_repeated=ctc_merge_repeated,\n                          time_major=inputs_are_time_major)\n\ndef input_data(shape, name: str = 'InputData', input_type=tf.float32):\n    return tflearn.input_data(shape=shape, dtype=input_type, name=name)\n\ndef reshape(tensor: tf.Tensor, new_shape: list):\n    return tf.reshape(tensor, new_shape, name=\"reshape\")\n\ndef bidirectional_lstm(inputs, num_hidden: int, return_seq=False):\n    return bidirectional_rnn(inputs, BasicLSTMCell(num_hidden), BasicLSTMCell(num_hidden), return_seq=return_seq)\n\n\ndef decode(inputs, sequence_length, merge_repeated=True):\n    decoded, _ = tf.nn.ctc_beam_search_decoder(inputs, sequence_length, merge_repeated)\n    return decoded\n\ndef label_error_rate(y_pred, y_true):\n    return tf.reduce_mean(tf.edit_distance(tf.cast(y_pred, tf.int32), y_true))\n\ndef optimize(loss, optimizer, learning_rate):\n    if optimizer == Optimizers.MOMENTUM:\n        return tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss)\n    if optimizer == Optimizers.ADAM:\n        return tf.train.AdamOptimizer(learning_rate).minimize(loss)\n    if optimizer == Optimizers.ADADELTA:\n        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\n    if optimizer == Optimizers.RMSPROP:\n        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n    raise NotImplementedError(\"{} is not implemented.\".format(optimizer))\n\ndef sparse_input_data(input_type=tf.int32):\n    return tf.sparse_placeholder(input_type)\n\ndef get_time_major(model, num_classes, batch_size, num_hidden_units):\n    outputs = reshape(model, [-1, num_hidden_units])\n\n    W = tf.Variable(tf.truncated_normal([num_hidden_units,\n                                         num_classes],\n                                        stddev=0.1, dtype=tf.float32), name='W')\n    b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[num_classes], name='b'))\n\n    logits = tf.matmul(outputs, W) + b\n    logits = tf.reshape(logits, [batch_size, -1, num_classes])\n    logits = tf.transpose(logits, (1, 0, 2))\n    return logits\n\ndef cost(loss):\n    return tf.reduce_mean(loss)\n\n\ndef get_type(type_str):\n    if type_str == 'int32':\n        return tf.int32\n    return tf.float32\n\n\ndef get_shape(tensor):\n    return tf.shape(tensor)\n\nCan anyone help me address this issue? To reproduce it, you can run main/train.py in this repository", "body": "I'm using tensorflow 1.4 and tflearn 0.3.2. The os of my machine is Win 8.1. Apparently, the graph building and cost optimization in my code is real slow which I think is a bug in Tensorflow (either that or my machine is just real slow). Here's the code:\r\n\r\n```\r\nclass Model(object):\r\n    def __init__(self):\r\n        self.num_classes = 80\r\n        self.num_time_steps = 1596\r\n        self.input_dimension = 48\r\n        self.inputs = network_utils.input_data([None, self.num_time_steps, self.input_dimension], name=\"input\")\r\n        self.labels = network_utils.sparse_input_data()\r\n        self.seq_lens = network_utils.input_data([None], name=\"seq_len\", input_type=network_utils.get_type('int32'))\r\n        self.learning_rate = 0.01\r\n\r\n    def _inference(self):\r\n        model = network_utils.bidirectional_lstm(self.inputs, 50, return_seq=True)\r\n        model = network_utils.bidirectional_lstm(model, 100, return_seq=True)\r\n        model = network_utils.bidirectional_lstm(model, 200)\r\n        logits = network_utils.get_time_major(model, self.num_classes, network_utils.get_shape(self.inputs)[0], 200)\r\n        return logits\r\n\r\n    def loss(self):\r\n        y_predict = self._inference()\r\n        loss = network_utils.ctc_loss(predictions=y_predict, labels=self.labels, sequence_length=self.seq_lens)\r\n        cost = network_utils.cost(loss)\r\n        decoded = network_utils.decode(inputs=y_predict, sequence_length=self.seq_lens)\r\n        label_error_rate = network_utils.label_error_rate(y_pred=decoded[0], y_true=self.labels)\r\n        return loss, label_error_rate, cost\r\n\r\n    def optimize(self, cost, optimizer):\r\n        return network_utils.optimize(loss=cost, optimizer=optimizer, learning_rate=self.learning_rate)\r\n```\r\n\r\n```\r\nimport tensorflow as tf\r\nimport tflearn\r\nfrom tflearn import bidirectional_rnn, BasicLSTMCell\r\n\r\nfrom optimizer_enum import Optimizers\r\n\r\ndef ctc_loss(predictions, labels, sequence_length,\r\n             preprocess_collapse_repeated_labels=True,\r\n             ctc_merge_repeated=True,\r\n             inputs_are_time_major=True):\r\n    return tf.nn.ctc_loss(inputs=predictions, labels=labels, sequence_length=sequence_length,\r\n                          preprocess_collapse_repeated=preprocess_collapse_repeated_labels,\r\n                          ctc_merge_repeated=ctc_merge_repeated,\r\n                          time_major=inputs_are_time_major)\r\n\r\ndef input_data(shape, name: str = 'InputData', input_type=tf.float32):\r\n    return tflearn.input_data(shape=shape, dtype=input_type, name=name)\r\n\r\ndef reshape(tensor: tf.Tensor, new_shape: list):\r\n    return tf.reshape(tensor, new_shape, name=\"reshape\")\r\n\r\ndef bidirectional_lstm(inputs, num_hidden: int, return_seq=False):\r\n    return bidirectional_rnn(inputs, BasicLSTMCell(num_hidden), BasicLSTMCell(num_hidden), return_seq=return_seq)\r\n\r\n\r\ndef decode(inputs, sequence_length, merge_repeated=True):\r\n    decoded, _ = tf.nn.ctc_beam_search_decoder(inputs, sequence_length, merge_repeated)\r\n    return decoded\r\n\r\ndef label_error_rate(y_pred, y_true):\r\n    return tf.reduce_mean(tf.edit_distance(tf.cast(y_pred, tf.int32), y_true))\r\n\r\ndef optimize(loss, optimizer, learning_rate):\r\n    if optimizer == Optimizers.MOMENTUM:\r\n        return tf.train.MomentumOptimizer(learning_rate, momentum=0.9).minimize(loss)\r\n    if optimizer == Optimizers.ADAM:\r\n        return tf.train.AdamOptimizer(learning_rate).minimize(loss)\r\n    if optimizer == Optimizers.ADADELTA:\r\n        return tf.train.AdadeltaOptimizer(learning_rate).minimize(loss)\r\n    if optimizer == Optimizers.RMSPROP:\r\n        return tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\r\n    raise NotImplementedError(\"{} is not implemented.\".format(optimizer))\r\n\r\ndef sparse_input_data(input_type=tf.int32):\r\n    return tf.sparse_placeholder(input_type)\r\n\r\ndef get_time_major(model, num_classes, batch_size, num_hidden_units):\r\n    outputs = reshape(model, [-1, num_hidden_units])\r\n\r\n    W = tf.Variable(tf.truncated_normal([num_hidden_units,\r\n                                         num_classes],\r\n                                        stddev=0.1, dtype=tf.float32), name='W')\r\n    b = tf.Variable(tf.constant(0., dtype=tf.float32, shape=[num_classes], name='b'))\r\n\r\n    logits = tf.matmul(outputs, W) + b\r\n    logits = tf.reshape(logits, [batch_size, -1, num_classes])\r\n    logits = tf.transpose(logits, (1, 0, 2))\r\n    return logits\r\n\r\ndef cost(loss):\r\n    return tf.reduce_mean(loss)\r\n\r\n\r\ndef get_type(type_str):\r\n    if type_str == 'int32':\r\n        return tf.int32\r\n    return tf.float32\r\n\r\n\r\ndef get_shape(tensor):\r\n    return tf.shape(tensor)\r\n```\r\n\r\nCan anyone help me address this issue? To reproduce it, you can run main/train.py in this [repository](https://github.com/selcouthlyBlue/simplified_bi_lstm_ocr)"}