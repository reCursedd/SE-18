{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/211120165", "html_url": "https://github.com/tensorflow/tensorflow/issues/1940#issuecomment-211120165", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1940", "id": 211120165, "node_id": "MDEyOklzc3VlQ29tbWVudDIxMTEyMDE2NQ==", "user": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-17T21:38:22Z", "updated_at": "2016-04-17T21:38:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am not aware of anyone doing exactly what you are trying. Maybe the following. Say your batch has N sentences, each sentence has up to M words, each word's embedding dimensions is K. For sentence shorter than M, pads it with a special word with a fixed embedding vector of 0s. Then, you can do batch 1D convolution on <a href=\"per-row\">N, M*K</a>. Multiple the result w/ a mask of [ [1,..., 1, 1, ...0, ..., 0, 0], ...[...]], the total number of 1s and 0s on each row can be computed for each batch.</p>", "body_text": "I am not aware of anyone doing exactly what you are trying. Maybe the following. Say your batch has N sentences, each sentence has up to M words, each word's embedding dimensions is K. For sentence shorter than M, pads it with a special word with a fixed embedding vector of 0s. Then, you can do batch 1D convolution on N, M*K. Multiple the result w/ a mask of [ [1,..., 1, 1, ...0, ..., 0, 0], ...[...]], the total number of 1s and 0s on each row can be computed for each batch.", "body": "I am not aware of anyone doing exactly what you are trying. Maybe the following. Say your batch has N sentences, each sentence has up to M words, each word's embedding dimensions is K. For sentence shorter than M, pads it with a special word with a fixed embedding vector of 0s. Then, you can do batch 1D convolution on [N, M*K](per-row). Multiple the result w/ a mask of [ [1,..., 1, 1, ...0, ..., 0, 0], ...[...]], the total number of 1s and 0s on each row can be computed for each batch. \n"}