{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/211229416", "html_url": "https://github.com/tensorflow/tensorflow/issues/1997#issuecomment-211229416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1997", "id": 211229416, "node_id": "MDEyOklzc3VlQ29tbWVudDIxMTIyOTQxNg==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-18T06:46:44Z", "updated_at": "2016-04-18T06:46:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I guess the issue is whether this is due to a bug, or part of normal training procedure. Accuracy suddenly falling to zero is a common occurrence in neural network training. Typically it happens when your neural network weights overflow or underflow. Can you see what the norm of your parameters looks like over time? Also, another suggestion is to try retuning your hyper-parameters. (ie, trying different learning rate, training algorithm, etc) There might be some numeric difference between 0.7 and 0.8, but it's not clear which one is the \"correct\" one.</p>", "body_text": "I guess the issue is whether this is due to a bug, or part of normal training procedure. Accuracy suddenly falling to zero is a common occurrence in neural network training. Typically it happens when your neural network weights overflow or underflow. Can you see what the norm of your parameters looks like over time? Also, another suggestion is to try retuning your hyper-parameters. (ie, trying different learning rate, training algorithm, etc) There might be some numeric difference between 0.7 and 0.8, but it's not clear which one is the \"correct\" one.", "body": "I guess the issue is whether this is due to a bug, or part of normal training procedure. Accuracy suddenly falling to zero is a common occurrence in neural network training. Typically it happens when your neural network weights overflow or underflow. Can you see what the norm of your parameters looks like over time? Also, another suggestion is to try retuning your hyper-parameters. (ie, trying different learning rate, training algorithm, etc) There might be some numeric difference between 0.7 and 0.8, but it's not clear which one is the \"correct\" one. \n"}