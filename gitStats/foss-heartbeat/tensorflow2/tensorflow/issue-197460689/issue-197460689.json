{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6484", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6484/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6484/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6484/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6484", "id": 197460689, "node_id": "MDU6SXNzdWUxOTc0NjA2ODk=", "number": 6484, "title": "Some of tensorflow GPU OpKernel compute by eigen device without stream sync, is that correct?", "user": {"login": "LLLLKKKK", "id": 1978696, "node_id": "MDQ6VXNlcjE5Nzg2OTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1978696?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LLLLKKKK", "html_url": "https://github.com/LLLLKKKK", "followers_url": "https://api.github.com/users/LLLLKKKK/followers", "following_url": "https://api.github.com/users/LLLLKKKK/following{/other_user}", "gists_url": "https://api.github.com/users/LLLLKKKK/gists{/gist_id}", "starred_url": "https://api.github.com/users/LLLLKKKK/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LLLLKKKK/subscriptions", "organizations_url": "https://api.github.com/users/LLLLKKKK/orgs", "repos_url": "https://api.github.com/users/LLLLKKKK/repos", "events_url": "https://api.github.com/users/LLLLKKKK/events{/privacy}", "received_events_url": "https://api.github.com/users/LLLLKKKK/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-12-24T12:11:11Z", "updated_at": "2017-05-14T03:06:39Z", "closed_at": "2016-12-25T16:30:14Z", "author_association": "NONE", "body_html": "<p>from gpu_device.cc</p>\n<pre><code>   // NOTE(tucker): We need to discriminate between Eigen GPU\n   // operations and all others.  If an operation is Eigen\n   // implemented (or otherwise tries to launch a cuda kernel\n   // directly), we need to establish a stacked-scoped environment\n   // that directs it to execute on the proper device.  Otherwise we\n   // expect the Op to use StreamExecutor directly and correctly.  The\n   // way we make this discrimination is quite hacky: At the moment\n   // the only non-Eigen GPU Op is the recv-op, which is known to be\n   // asynchronous.\n</code></pre>\n<p>and gpu_device only waits when there are different contexts. (sync_every_op is false)</p>\n<p>But take argmax_op.h  for example,</p>\n<pre><code>template &lt;typename Device, typename T&gt;\nstruct ArgMin {\n#define DECLARE_COMPUTE_SPEC(Dims)                                     \\\nEIGEN_ALWAYS_INLINE static void Reduce##Dims(                        \\\nconst Device&amp; d, typename TTypes&lt;T, Dims&gt;::ConstTensor input,    \\\nconst int32 dimension,                                           \\\ntypename TTypes&lt;int64, Dims - 1&gt;::Tensor output) {               \\\noutput.device(d) = input.argmin(dimension).template cast&lt;int64&gt;(); \\\n}\n</code></pre>\n<p>use device compute directly. Is that correct? Or I miss something ...<br>\nThank you very much!</p>", "body_text": "from gpu_device.cc\n   // NOTE(tucker): We need to discriminate between Eigen GPU\n   // operations and all others.  If an operation is Eigen\n   // implemented (or otherwise tries to launch a cuda kernel\n   // directly), we need to establish a stacked-scoped environment\n   // that directs it to execute on the proper device.  Otherwise we\n   // expect the Op to use StreamExecutor directly and correctly.  The\n   // way we make this discrimination is quite hacky: At the moment\n   // the only non-Eigen GPU Op is the recv-op, which is known to be\n   // asynchronous.\n\nand gpu_device only waits when there are different contexts. (sync_every_op is false)\nBut take argmax_op.h  for example,\ntemplate <typename Device, typename T>\nstruct ArgMin {\n#define DECLARE_COMPUTE_SPEC(Dims)                                     \\\nEIGEN_ALWAYS_INLINE static void Reduce##Dims(                        \\\nconst Device& d, typename TTypes<T, Dims>::ConstTensor input,    \\\nconst int32 dimension,                                           \\\ntypename TTypes<int64, Dims - 1>::Tensor output) {               \\\noutput.device(d) = input.argmin(dimension).template cast<int64>(); \\\n}\n\nuse device compute directly. Is that correct? Or I miss something ...\nThank you very much!", "body": "from gpu_device.cc\r\n\r\n```\r\n   // NOTE(tucker): We need to discriminate between Eigen GPU\r\n   // operations and all others.  If an operation is Eigen\r\n   // implemented (or otherwise tries to launch a cuda kernel\r\n   // directly), we need to establish a stacked-scoped environment\r\n   // that directs it to execute on the proper device.  Otherwise we\r\n   // expect the Op to use StreamExecutor directly and correctly.  The\r\n   // way we make this discrimination is quite hacky: At the moment\r\n   // the only non-Eigen GPU Op is the recv-op, which is known to be\r\n   // asynchronous.\r\n```\r\n\r\nand gpu_device only waits when there are different contexts. (sync_every_op is false)\r\n\r\nBut take argmax_op.h  for example,\r\n\r\n\r\n    template <typename Device, typename T>\r\n    struct ArgMin {\r\n    #define DECLARE_COMPUTE_SPEC(Dims)                                     \\\r\n    EIGEN_ALWAYS_INLINE static void Reduce##Dims(                        \\\r\n    const Device& d, typename TTypes<T, Dims>::ConstTensor input,    \\\r\n    const int32 dimension,                                           \\\r\n    typename TTypes<int64, Dims - 1>::Tensor output) {               \\\r\n    output.device(d) = input.argmin(dimension).template cast<int64>(); \\\r\n    }\r\n\r\nuse device compute directly. Is that correct? Or I miss something ... \r\nThank you very much!\r\n"}