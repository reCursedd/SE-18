{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335039382", "html_url": "https://github.com/tensorflow/tensorflow/issues/312#issuecomment-335039382", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/312", "id": 335039382, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTAzOTM4Mg==", "user": {"login": "vitalyli", "id": 12679026, "node_id": "MDQ6VXNlcjEyNjc5MDI2", "avatar_url": "https://avatars0.githubusercontent.com/u/12679026?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vitalyli", "html_url": "https://github.com/vitalyli", "followers_url": "https://api.github.com/users/vitalyli/followers", "following_url": "https://api.github.com/users/vitalyli/following{/other_user}", "gists_url": "https://api.github.com/users/vitalyli/gists{/gist_id}", "starred_url": "https://api.github.com/users/vitalyli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vitalyli/subscriptions", "organizations_url": "https://api.github.com/users/vitalyli/orgs", "repos_url": "https://api.github.com/users/vitalyli/repos", "events_url": "https://api.github.com/users/vitalyli/events{/privacy}", "received_events_url": "https://api.github.com/users/vitalyli/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-08T21:20:21Z", "updated_at": "2017-10-08T21:24:57Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=38796628\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dandelionmane\">@dandelionmane</a> thanks for your optimistic_restore function; I've tried it successfully. It really should be part of Saver itself. For my purposes I've modified it only slightly to make function return list of variables given checkpoint path, that way it's not concerned with session yet.</p>\n<pre><code>def optimistic_restore_vars(model_checkpoint_path):\n    reader = tf.train.NewCheckpointReader(model_checkpoint_path)\n    saved_shapes = reader.get_variable_to_shape_map()\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n                        if var.name.split(':')[0] in saved_shapes])\n    restore_vars = []\n    name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\n    with tf.variable_scope('', reuse=True):\n        for var_name, saved_var_name in var_names:\n            curr_var = name2var[saved_var_name]\n            var_shape = curr_var.get_shape().as_list()\n            if var_shape == saved_shapes[saved_var_name]:\n                restore_vars.append(curr_var)\n    return restore_vars\n\nckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint + '/checkpoint'))\nsaver = tf.train.Saver(max_to_keep=20, var_list = optimistic_restore_vars(ckpt.model_checkpoint_path) if checkpoint else None)\n</code></pre>", "body_text": "@dandelionmane thanks for your optimistic_restore function; I've tried it successfully. It really should be part of Saver itself. For my purposes I've modified it only slightly to make function return list of variables given checkpoint path, that way it's not concerned with session yet.\ndef optimistic_restore_vars(model_checkpoint_path):\n    reader = tf.train.NewCheckpointReader(model_checkpoint_path)\n    saved_shapes = reader.get_variable_to_shape_map()\n    var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\n                        if var.name.split(':')[0] in saved_shapes])\n    restore_vars = []\n    name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\n    with tf.variable_scope('', reuse=True):\n        for var_name, saved_var_name in var_names:\n            curr_var = name2var[saved_var_name]\n            var_shape = curr_var.get_shape().as_list()\n            if var_shape == saved_shapes[saved_var_name]:\n                restore_vars.append(curr_var)\n    return restore_vars\n\nckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint + '/checkpoint'))\nsaver = tf.train.Saver(max_to_keep=20, var_list = optimistic_restore_vars(ckpt.model_checkpoint_path) if checkpoint else None)", "body": "@dandelionmane thanks for your optimistic_restore function; I've tried it successfully. It really should be part of Saver itself. For my purposes I've modified it only slightly to make function return list of variables given checkpoint path, that way it's not concerned with session yet.\r\n\r\n    def optimistic_restore_vars(model_checkpoint_path):\r\n        reader = tf.train.NewCheckpointReader(model_checkpoint_path)\r\n        saved_shapes = reader.get_variable_to_shape_map()\r\n        var_names = sorted([(var.name, var.name.split(':')[0]) for var in tf.global_variables()\r\n                            if var.name.split(':')[0] in saved_shapes])\r\n        restore_vars = []\r\n        name2var = dict(zip(map(lambda x:x.name.split(':')[0], tf.global_variables()), tf.global_variables()))\r\n        with tf.variable_scope('', reuse=True):\r\n            for var_name, saved_var_name in var_names:\r\n                curr_var = name2var[saved_var_name]\r\n                var_shape = curr_var.get_shape().as_list()\r\n                if var_shape == saved_shapes[saved_var_name]:\r\n                    restore_vars.append(curr_var)\r\n        return restore_vars\r\n\r\n    ckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint + '/checkpoint'))\r\n    saver = tf.train.Saver(max_to_keep=20, var_list = optimistic_restore_vars(ckpt.model_checkpoint_path) if checkpoint else None)"}