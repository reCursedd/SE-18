{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/312", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/312/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/312/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/312/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/312", "id": 118110091, "node_id": "MDU6SXNzdWUxMTgxMTAwOTE=", "number": 312, "title": "Relax the requirements for saver checkpoints", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 14, "created_at": "2015-11-20T19:30:47Z", "updated_at": "2018-09-27T17:25:08Z", "closed_at": "2016-04-15T20:33:15Z", "author_association": "NONE", "body_html": "<p>I'm working on testing a few modifications to an existing network including attempting to use it as a teacher in a student/teacher network and swapping its optimizer but I get the following error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"knit_net_train_tf.py\", line 40, in &lt;module&gt;\n    saver.restore(sesh, CHECKPOINT_FILE)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 869, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 401, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 477, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"conv1/biases/Adagrad\" not found in checkpoint files knitnet.0.ckpt\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\n</code></pre>\n<p>It appears that the saver requires that every variable that exists in the current graph also be present in the checkpoint and that every variable that that exists in the checkpoint also be in the current graph. I can work around this by making major modifications to the way that my network is constructed but I would be nice to have a feature that says \"don't error out if you don't find this variable, its ok\". Alternatively it appears that I can specify which variables I want to restore, which could also solve the problem, but I have a lot of variable that I haven't explicitly tracked, many of which are created by the optimizer. It would make this feature a lot more useful if there were a function that did \"return all variables that this op depends on\", which I could call on the training variable or the prediction variable and only save those tensors.</p>", "body_text": "I'm working on testing a few modifications to an existing network including attempting to use it as a teacher in a student/teacher network and swapping its optimizer but I get the following error:\nTraceback (most recent call last):\n  File \"knit_net_train_tf.py\", line 40, in <module>\n    saver.restore(sesh, CHECKPOINT_FILE)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 869, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 401, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 477, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"conv1/biases/Adagrad\" not found in checkpoint files knitnet.0.ckpt\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\n\nIt appears that the saver requires that every variable that exists in the current graph also be present in the checkpoint and that every variable that that exists in the checkpoint also be in the current graph. I can work around this by making major modifications to the way that my network is constructed but I would be nice to have a feature that says \"don't error out if you don't find this variable, its ok\". Alternatively it appears that I can specify which variables I want to restore, which could also solve the problem, but I have a lot of variable that I haven't explicitly tracked, many of which are created by the optimizer. It would make this feature a lot more useful if there were a function that did \"return all variables that this op depends on\", which I could call on the training variable or the prediction variable and only save those tensors.", "body": "I'm working on testing a few modifications to an existing network including attempting to use it as a teacher in a student/teacher network and swapping its optimizer but I get the following error:\n\n```\nTraceback (most recent call last):\n  File \"knit_net_train_tf.py\", line 40, in <module>\n    saver.restore(sesh, CHECKPOINT_FILE)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 869, in restore\n    sess.run([self._restore_op_name], {self._filename_tensor_name: save_path})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 401, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 477, in _do_run\n    e.code)\ntensorflow.python.framework.errors.NotFoundError: Tensor name \"conv1/biases/Adagrad\" not found in checkpoint files knitnet.0.ckpt\n         [[Node: save/restore_slice_2 = RestoreSlice[dt=DT_FLOAT, preferred_shard=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/restore_slice_2/tensor_name, save/restore_slice_2/shape_and_slice)]]\n```\n\nIt appears that the saver requires that every variable that exists in the current graph also be present in the checkpoint and that every variable that that exists in the checkpoint also be in the current graph. I can work around this by making major modifications to the way that my network is constructed but I would be nice to have a feature that says \"don't error out if you don't find this variable, its ok\". Alternatively it appears that I can specify which variables I want to restore, which could also solve the problem, but I have a lot of variable that I haven't explicitly tracked, many of which are created by the optimizer. It would make this feature a lot more useful if there were a function that did \"return all variables that this op depends on\", which I could call on the training variable or the prediction variable and only save those tensors.\n"}