{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/332410230", "html_url": "https://github.com/tensorflow/tensorflow/pull/13277#issuecomment-332410230", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13277", "id": 332410230, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjQxMDIzMA==", "user": {"login": "davorrunje", "id": 24715380, "node_id": "MDQ6VXNlcjI0NzE1Mzgw", "avatar_url": "https://avatars3.githubusercontent.com/u/24715380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davorrunje", "html_url": "https://github.com/davorrunje", "followers_url": "https://api.github.com/users/davorrunje/followers", "following_url": "https://api.github.com/users/davorrunje/following{/other_user}", "gists_url": "https://api.github.com/users/davorrunje/gists{/gist_id}", "starred_url": "https://api.github.com/users/davorrunje/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davorrunje/subscriptions", "organizations_url": "https://api.github.com/users/davorrunje/orgs", "repos_url": "https://api.github.com/users/davorrunje/repos", "events_url": "https://api.github.com/users/davorrunje/events{/privacy}", "received_events_url": "https://api.github.com/users/davorrunje/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-27T05:08:28Z", "updated_at": "2017-09-27T05:08:28Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a> yes, the <code>model_fn()</code>  is exactly the same as before externally, and internally it is really just combining of the existing ones. First, I noticed that <code>model_fn()</code> of DNN and linear estimators is the same and there is no need for code duplication. Then I noticed that combined <code>model_fn()</code> is just unrolling of that same function twice and can be written as a loop. This way one function can replace all three written so far.</p>\n<p>The reason why I think it is a good idea to have one<code> model_fn()</code> is when you start writing your own estimators based on existing ones. I am teaching a class on deep learning using Tensorflow and I teach students different techniques and architectures. We start with the existing canned Estimators and then we learn how to extend them with new functionality. With the existing code base, we would need to make numerous copies of the same <code>model_fn()</code> because a typical use case requires new <code>logit_fn()</code> and the rest of the code remains the same. This code duplication becomes obvious after writing more than five different estimators. E.g. adding batch norm layers is one example and using convolutional layers is another, they all require changes to <code>logits_fn()</code> only.</p>", "body_text": "@ispirmustafa yes, the model_fn()  is exactly the same as before externally, and internally it is really just combining of the existing ones. First, I noticed that model_fn() of DNN and linear estimators is the same and there is no need for code duplication. Then I noticed that combined model_fn() is just unrolling of that same function twice and can be written as a loop. This way one function can replace all three written so far.\nThe reason why I think it is a good idea to have one model_fn() is when you start writing your own estimators based on existing ones. I am teaching a class on deep learning using Tensorflow and I teach students different techniques and architectures. We start with the existing canned Estimators and then we learn how to extend them with new functionality. With the existing code base, we would need to make numerous copies of the same model_fn() because a typical use case requires new logit_fn() and the rest of the code remains the same. This code duplication becomes obvious after writing more than five different estimators. E.g. adding batch norm layers is one example and using convolutional layers is another, they all require changes to logits_fn() only.", "body": "@ispirmustafa yes, the `model_fn()`  is exactly the same as before externally, and internally it is really just combining of the existing ones. First, I noticed that `model_fn()` of DNN and linear estimators is the same and there is no need for code duplication. Then I noticed that combined `model_fn()` is just unrolling of that same function twice and can be written as a loop. This way one function can replace all three written so far.\r\n\r\nThe reason why I think it is a good idea to have one` model_fn()` is when you start writing your own estimators based on existing ones. I am teaching a class on deep learning using Tensorflow and I teach students different techniques and architectures. We start with the existing canned Estimators and then we learn how to extend them with new functionality. With the existing code base, we would need to make numerous copies of the same `model_fn()` because a typical use case requires new `logit_fn()` and the rest of the code remains the same. This code duplication becomes obvious after writing more than five different estimators. E.g. adding batch norm layers is one example and using convolutional layers is another, they all require changes to `logits_fn()` only. "}