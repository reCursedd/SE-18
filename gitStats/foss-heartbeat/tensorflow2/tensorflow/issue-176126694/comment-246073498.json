{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246073498", "html_url": "https://github.com/tensorflow/tensorflow/issues/4309#issuecomment-246073498", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4309", "id": 246073498, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NjA3MzQ5OA==", "user": {"login": "alexgkendall", "id": 8551989, "node_id": "MDQ6VXNlcjg1NTE5ODk=", "avatar_url": "https://avatars0.githubusercontent.com/u/8551989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexgkendall", "html_url": "https://github.com/alexgkendall", "followers_url": "https://api.github.com/users/alexgkendall/followers", "following_url": "https://api.github.com/users/alexgkendall/following{/other_user}", "gists_url": "https://api.github.com/users/alexgkendall/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexgkendall/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexgkendall/subscriptions", "organizations_url": "https://api.github.com/users/alexgkendall/orgs", "repos_url": "https://api.github.com/users/alexgkendall/repos", "events_url": "https://api.github.com/users/alexgkendall/events{/privacy}", "received_events_url": "https://api.github.com/users/alexgkendall/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-10T00:16:55Z", "updated_at": "2016-09-10T00:16:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thank you for your reply. Consider a 3d convolutional model (say for example over a video tensor - these are quite commonly used). <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a> in this situation the dominant memory by far is in the feature tensors. Having to create deep copies of these tensors for both activation and dropout severely limits the depth of networks you can train. This contrasts with other libraries (e.g. Caffe/ Torch) which allow in-place operation.</p>", "body_text": "Thank you for your reply. Consider a 3d convolutional model (say for example over a video tensor - these are quite commonly used). @martinwicke in this situation the dominant memory by far is in the feature tensors. Having to create deep copies of these tensors for both activation and dropout severely limits the depth of networks you can train. This contrasts with other libraries (e.g. Caffe/ Torch) which allow in-place operation.", "body": "Thank you for your reply. Consider a 3d convolutional model (say for example over a video tensor - these are quite commonly used). @martinwicke in this situation the dominant memory by far is in the feature tensors. Having to create deep copies of these tensors for both activation and dropout severely limits the depth of networks you can train. This contrasts with other libraries (e.g. Caffe/ Torch) which allow in-place operation.\n"}