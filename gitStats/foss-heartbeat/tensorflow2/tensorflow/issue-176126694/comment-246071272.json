{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246071272", "html_url": "https://github.com/tensorflow/tensorflow/issues/4309#issuecomment-246071272", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4309", "id": 246071272, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NjA3MTI3Mg==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-09T23:56:34Z", "updated_at": "2016-09-09T23:56:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Martin: I wouldn't say that all the memory is always concentrated in weights.</p>\n<p>However, we have not seen proof that you need in-place operations to be memory efficient.</p>\n<ul>\n<li>If there is only one consumer of an activation, the memory will be freed almost instantaneously after the consuming op is completed.  Yes, you can avoid this one memory allocation for an activation buffer, but in practice this is never the dominant memory cost of a model (One case where this would be practically important is if the activation memory of one layer was the majority of the GPU memory, but I don't believe that's common.)</li>\n<li>As some external studies have shown, TensorFlow is already quite efficient when compared to many other frameworks in using GPU memory already, and I'd like to see proof that in-place updates would help dramatically.  Until then, I think we'd rather spend time on the potential bigger wins in placement and scheduling before trying to implement in-place updates.</li>\n</ul>", "body_text": "Martin: I wouldn't say that all the memory is always concentrated in weights.\nHowever, we have not seen proof that you need in-place operations to be memory efficient.\n\nIf there is only one consumer of an activation, the memory will be freed almost instantaneously after the consuming op is completed.  Yes, you can avoid this one memory allocation for an activation buffer, but in practice this is never the dominant memory cost of a model (One case where this would be practically important is if the activation memory of one layer was the majority of the GPU memory, but I don't believe that's common.)\nAs some external studies have shown, TensorFlow is already quite efficient when compared to many other frameworks in using GPU memory already, and I'd like to see proof that in-place updates would help dramatically.  Until then, I think we'd rather spend time on the potential bigger wins in placement and scheduling before trying to implement in-place updates.", "body": "Martin: I wouldn't say that all the memory is always concentrated in weights.\n\nHowever, we have not seen proof that you need in-place operations to be memory efficient.\n- If there is only one consumer of an activation, the memory will be freed almost instantaneously after the consuming op is completed.  Yes, you can avoid this one memory allocation for an activation buffer, but in practice this is never the dominant memory cost of a model (One case where this would be practically important is if the activation memory of one layer was the majority of the GPU memory, but I don't believe that's common.)\n- As some external studies have shown, TensorFlow is already quite efficient when compared to many other frameworks in using GPU memory already, and I'd like to see proof that in-place updates would help dramatically.  Until then, I think we'd rather spend time on the potential bigger wins in placement and scheduling before trying to implement in-place updates.\n"}