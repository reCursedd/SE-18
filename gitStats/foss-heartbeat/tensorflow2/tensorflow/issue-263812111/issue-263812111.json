{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13578", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13578/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13578/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13578/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13578", "id": 263812111, "node_id": "MDU6SXNzdWUyNjM4MTIxMTE=", "number": 13578, "title": "Using an LSTM-CTC Tensorflow Model in Android", "user": {"login": "selcouthlyBlue", "id": 13268675, "node_id": "MDQ6VXNlcjEzMjY4Njc1", "avatar_url": "https://avatars2.githubusercontent.com/u/13268675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/selcouthlyBlue", "html_url": "https://github.com/selcouthlyBlue", "followers_url": "https://api.github.com/users/selcouthlyBlue/followers", "following_url": "https://api.github.com/users/selcouthlyBlue/following{/other_user}", "gists_url": "https://api.github.com/users/selcouthlyBlue/gists{/gist_id}", "starred_url": "https://api.github.com/users/selcouthlyBlue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/selcouthlyBlue/subscriptions", "organizations_url": "https://api.github.com/users/selcouthlyBlue/orgs", "repos_url": "https://api.github.com/users/selcouthlyBlue/repos", "events_url": "https://api.github.com/users/selcouthlyBlue/events{/privacy}", "received_events_url": "https://api.github.com/users/selcouthlyBlue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-09T08:04:24Z", "updated_at": "2017-10-11T01:00:19Z", "closed_at": "2017-10-11T00:53:12Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nWindows 8.1</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nAnaconda Install. <a href=\"https://ci.tensorflow.org/view/Nightly/job/nightly-android/\" rel=\"nofollow\">Prebuilt libraries</a> were used for Tensorflow Android.</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.2.1</li>\n<li><strong>Python version</strong>:<br>\nPython 3.5.3 :: Anaconda custom (64-bit)</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nN/A</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have succeeded in training my bi-lstm-ctc tensorflow model and now I want to use it for my handwriting recognition android application. Here's the part of the code that defines the graph I used:</p>\n<pre><code>self.inputs = tf.placeholder(tf.float32, [None, None, network_config.num_features], name=\"input\")\nself.labels = tf.sparse_placeholder(tf.int32, name=\"label\")\nself.seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len_input\")\n\nlogits = self._bidirectional_lstm_layers(\n   network_config.num_hidden_units,\n   network_config.num_layers,\n   network_config.num_classes\n)\n\nself.global_step = tf.Variable(0, trainable=False)\nself.loss = tf.nn.ctc_loss(labels=self.labels, inputs=logits, sequence_length=self.seq_len)\nself.cost = tf.reduce_mean(self.loss)\n\nself.optimizer = tf.train.AdamOptimizer(network_config.learning_rate).minimize(self.cost)\nself.decoded, self.log_prob = tf.nn.ctc_beam_search_decoder(inputs=logits, sequence_length=self.seq_len, merge_repeated=False)\nself.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1, name=\"output\")\n\n</code></pre>\n<p>I also succeeded in freezing and optimizing the graph using this code:</p>\n<pre><code>def freeze(input_graph_path, checkpoint_path, output_node_names, input_saver_def_path=\"\", input_binary=False,\n           restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\",\n           output_frozen_graph_name=\"frozen_output.pb\",\n           clear_devices=True):\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,\n                              checkpoint_path, output_node_names, restore_op_name, filename_tensor_name,\n                              output_frozen_graph_name, clear_devices, \"\")\n\n\ndef optimize_graph(graph_path, input_nodes, output_nodes):\n    input_graph_def = tf.GraphDef()\n    with tf.gfile.Open(graph_path, \"rb\") as f:\n        data = f.read()\n        input_graph_def.ParseFromString(data)\n\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n        input_graph_def,\n        input_nodes,\n        output_nodes,\n        tf.float32.as_datatype_enum\n    )\n\n    f = tf.gfile.FastGFile(\"optimized_\" + graph_path, \"w\")\n    f.write(output_graph_def.SerializeToString())\n</code></pre>\n<p>And here's the part of the android code that is supposed to run the model:</p>\n<pre><code>bitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128, true);\nint[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];\nbitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\nfloat[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];\nfor (int i = 0; i &lt; intValues.length; ++i) {\n    final int val = intValues[i];\n    floatValues[i] = (((val &gt;&gt; 16) &amp; 0xFF));\n}\nfloat[] result = new float[80];\nlong[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};\ninferenceInterface.feed(config.getInputName(), floatValues, INPUT_SIZE);\ninferenceInterface.feed(\"seq_len_input\", new int[]{bitmap.getWidth()}, 1);\ninferenceInterface.run(config.getOutputNames());\ninferenceInterface.fetch(config.getOutputNames()[0], result);\n\nreturn result.toString();\n</code></pre>\n<p>However, I encounter these problems depending on the model I use. If I use the frozen graph, I encounter this error:</p>\n<pre><code>Caused by: java.lang.IllegalArgumentException: No OpKernel was registered to support\nOp 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\n                                                                                            \n[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]\n</code></pre>\n<p>If I use the optimized frozen graph, I encounter this error:</p>\n<pre><code>java.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs \nspecified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; \nNodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, \nvalue=Tensor&lt;type: int32 shape: [] values: 1&gt;](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)\n</code></pre>\n<p>I have no ideas on what these error messages tell me nor how to resolve these.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nWindows 8.1\nTensorFlow installed from (source or binary):\nAnaconda Install. Prebuilt libraries were used for Tensorflow Android.\nTensorFlow version (use command below):\n1.2.1\nPython version:\nPython 3.5.3 :: Anaconda custom (64-bit)\nBazel version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nN/A\nGPU model and memory:\nN/A\n\nDescribe the problem\nI have succeeded in training my bi-lstm-ctc tensorflow model and now I want to use it for my handwriting recognition android application. Here's the part of the code that defines the graph I used:\nself.inputs = tf.placeholder(tf.float32, [None, None, network_config.num_features], name=\"input\")\nself.labels = tf.sparse_placeholder(tf.int32, name=\"label\")\nself.seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len_input\")\n\nlogits = self._bidirectional_lstm_layers(\n   network_config.num_hidden_units,\n   network_config.num_layers,\n   network_config.num_classes\n)\n\nself.global_step = tf.Variable(0, trainable=False)\nself.loss = tf.nn.ctc_loss(labels=self.labels, inputs=logits, sequence_length=self.seq_len)\nself.cost = tf.reduce_mean(self.loss)\n\nself.optimizer = tf.train.AdamOptimizer(network_config.learning_rate).minimize(self.cost)\nself.decoded, self.log_prob = tf.nn.ctc_beam_search_decoder(inputs=logits, sequence_length=self.seq_len, merge_repeated=False)\nself.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1, name=\"output\")\n\n\nI also succeeded in freezing and optimizing the graph using this code:\ndef freeze(input_graph_path, checkpoint_path, output_node_names, input_saver_def_path=\"\", input_binary=False,\n           restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\",\n           output_frozen_graph_name=\"frozen_output.pb\",\n           clear_devices=True):\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,\n                              checkpoint_path, output_node_names, restore_op_name, filename_tensor_name,\n                              output_frozen_graph_name, clear_devices, \"\")\n\n\ndef optimize_graph(graph_path, input_nodes, output_nodes):\n    input_graph_def = tf.GraphDef()\n    with tf.gfile.Open(graph_path, \"rb\") as f:\n        data = f.read()\n        input_graph_def.ParseFromString(data)\n\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n        input_graph_def,\n        input_nodes,\n        output_nodes,\n        tf.float32.as_datatype_enum\n    )\n\n    f = tf.gfile.FastGFile(\"optimized_\" + graph_path, \"w\")\n    f.write(output_graph_def.SerializeToString())\n\nAnd here's the part of the android code that is supposed to run the model:\nbitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128, true);\nint[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];\nbitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\nfloat[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];\nfor (int i = 0; i < intValues.length; ++i) {\n    final int val = intValues[i];\n    floatValues[i] = (((val >> 16) & 0xFF));\n}\nfloat[] result = new float[80];\nlong[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};\ninferenceInterface.feed(config.getInputName(), floatValues, INPUT_SIZE);\ninferenceInterface.feed(\"seq_len_input\", new int[]{bitmap.getWidth()}, 1);\ninferenceInterface.run(config.getOutputNames());\ninferenceInterface.fetch(config.getOutputNames()[0], result);\n\nreturn result.toString();\n\nHowever, I encounter these problems depending on the model I use. If I use the frozen graph, I encounter this error:\nCaused by: java.lang.IllegalArgumentException: No OpKernel was registered to support\nOp 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\n                                                                                            \n[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]\n\nIf I use the optimized frozen graph, I encounter this error:\njava.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs \nspecified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; \nNodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, \nvalue=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)\n\nI have no ideas on what these error messages tell me nor how to resolve these.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 8.1\r\n- **TensorFlow installed from (source or binary)**:\r\nAnaconda Install. [Prebuilt libraries](https://ci.tensorflow.org/view/Nightly/job/nightly-android/) were used for Tensorflow Android.\r\n- **TensorFlow version (use command below)**:\r\n1.2.1\r\n- **Python version**: \r\nPython 3.5.3 :: Anaconda custom (64-bit)\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A\r\n- **GPU model and memory**:\r\nN/A\r\n\r\n### Describe the problem\r\nI have succeeded in training my bi-lstm-ctc tensorflow model and now I want to use it for my handwriting recognition android application. Here's the part of the code that defines the graph I used:\r\n\r\n```\r\nself.inputs = tf.placeholder(tf.float32, [None, None, network_config.num_features], name=\"input\")\r\nself.labels = tf.sparse_placeholder(tf.int32, name=\"label\")\r\nself.seq_len = tf.placeholder(tf.int32, [None], name=\"seq_len_input\")\r\n\r\nlogits = self._bidirectional_lstm_layers(\r\n   network_config.num_hidden_units,\r\n   network_config.num_layers,\r\n   network_config.num_classes\r\n)\r\n\r\nself.global_step = tf.Variable(0, trainable=False)\r\nself.loss = tf.nn.ctc_loss(labels=self.labels, inputs=logits, sequence_length=self.seq_len)\r\nself.cost = tf.reduce_mean(self.loss)\r\n\r\nself.optimizer = tf.train.AdamOptimizer(network_config.learning_rate).minimize(self.cost)\r\nself.decoded, self.log_prob = tf.nn.ctc_beam_search_decoder(inputs=logits, sequence_length=self.seq_len, merge_repeated=False)\r\nself.dense_decoded = tf.sparse_tensor_to_dense(self.decoded[0], default_value=-1, name=\"output\")\r\n\r\n```\r\n\r\nI also succeeded in freezing and optimizing the graph using this code:\r\n\r\n```\r\ndef freeze(input_graph_path, checkpoint_path, output_node_names, input_saver_def_path=\"\", input_binary=False,\r\n           restore_op_name=\"save/restore_all\", filename_tensor_name=\"save/Const:0\",\r\n           output_frozen_graph_name=\"frozen_output.pb\",\r\n           clear_devices=True):\r\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path, input_binary,\r\n                              checkpoint_path, output_node_names, restore_op_name, filename_tensor_name,\r\n                              output_frozen_graph_name, clear_devices, \"\")\r\n\r\n\r\ndef optimize_graph(graph_path, input_nodes, output_nodes):\r\n    input_graph_def = tf.GraphDef()\r\n    with tf.gfile.Open(graph_path, \"rb\") as f:\r\n        data = f.read()\r\n        input_graph_def.ParseFromString(data)\r\n\r\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\r\n        input_graph_def,\r\n        input_nodes,\r\n        output_nodes,\r\n        tf.float32.as_datatype_enum\r\n    )\r\n\r\n    f = tf.gfile.FastGFile(\"optimized_\" + graph_path, \"w\")\r\n    f.write(output_graph_def.SerializeToString())\r\n```\r\n\r\nAnd here's the part of the android code that is supposed to run the model:\r\n\r\n```\r\nbitmap = Bitmap.createScaledBitmap(bitmap, 1024, 128, true);\r\nint[] intValues = new int[bitmap.getWidth() * bitmap.getHeight()];\r\nbitmap.getPixels(intValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\nfloat[] floatValues = new float[bitmap.getWidth() * bitmap.getHeight()];\r\nfor (int i = 0; i < intValues.length; ++i) {\r\n    final int val = intValues[i];\r\n    floatValues[i] = (((val >> 16) & 0xFF));\r\n}\r\nfloat[] result = new float[80];\r\nlong[] INPUT_SIZE = new long[]{1, bitmap.getHeight(), bitmap.getWidth()};\r\ninferenceInterface.feed(config.getInputName(), floatValues, INPUT_SIZE);\r\ninferenceInterface.feed(\"seq_len_input\", new int[]{bitmap.getWidth()}, 1);\r\ninferenceInterface.run(config.getOutputNames());\r\ninferenceInterface.fetch(config.getOutputNames()[0], result);\r\n\r\nreturn result.toString();\r\n```\r\n\r\nHowever, I encounter these problems depending on the model I use. If I use the frozen graph, I encounter this error:\r\n\r\n```\r\nCaused by: java.lang.IllegalArgumentException: No OpKernel was registered to support\r\nOp 'SparseToDense' with these attrs.  Registered devices: [CPU], Registered kernels:\r\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_STRING]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_BOOL]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_FLOAT]; Tindices in [DT_INT32]\r\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT64]\r\ndevice='CPU'; T in [DT_INT32]; Tindices in [DT_INT32]\r\n                                                                                            \r\n[[Node: output = SparseToDense[T=DT_INT64, Tindices=DT_INT64, validate_indices=true](CTCBeamSearchDecoder, CTCBeamSearchDecoder:2, CTCBeamSearchDecoder:1, output/default_value)]]\r\n```\r\n\r\nIf I use the optimized frozen graph, I encounter this error:\r\n\r\n```\r\njava.io.IOException: Not a valid TensorFlow Graph serialization: NodeDef expected inputs '' do not match 1 inputs \r\nspecified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; \r\nNodeDef: stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/add/y = Const[dtype=DT_INT32, \r\nvalue=Tensor<type: int32 shape: [] values: 1>](stack_bidirectional_rnn/cell_0/bidirectional_rnn/bw/bw/while/Switch:1)\r\n```\r\n\r\nI have no ideas on what these error messages tell me nor how to resolve these."}