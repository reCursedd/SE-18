{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18342", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18342/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18342/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18342/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18342", "id": 312455869, "node_id": "MDU6SXNzdWUzMTI0NTU4Njk=", "number": 18342, "title": "operation concat causes error when using toco converter", "user": {"login": "coutner", "id": 37242917, "node_id": "MDQ6VXNlcjM3MjQyOTE3", "avatar_url": "https://avatars1.githubusercontent.com/u/37242917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/coutner", "html_url": "https://github.com/coutner", "followers_url": "https://api.github.com/users/coutner/followers", "following_url": "https://api.github.com/users/coutner/following{/other_user}", "gists_url": "https://api.github.com/users/coutner/gists{/gist_id}", "starred_url": "https://api.github.com/users/coutner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/coutner/subscriptions", "organizations_url": "https://api.github.com/users/coutner/orgs", "repos_url": "https://api.github.com/users/coutner/repos", "events_url": "https://api.github.com/users/coutner/events{/privacy}", "received_events_url": "https://api.github.com/users/coutner/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "andrewharp", "id": 3376817, "node_id": "MDQ6VXNlcjMzNzY4MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3376817?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrewharp", "html_url": "https://github.com/andrewharp", "followers_url": "https://api.github.com/users/andrewharp/followers", "following_url": "https://api.github.com/users/andrewharp/following{/other_user}", "gists_url": "https://api.github.com/users/andrewharp/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrewharp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrewharp/subscriptions", "organizations_url": "https://api.github.com/users/andrewharp/orgs", "repos_url": "https://api.github.com/users/andrewharp/repos", "events_url": "https://api.github.com/users/andrewharp/events{/privacy}", "received_events_url": "https://api.github.com/users/andrewharp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-04-09T09:23:17Z", "updated_at": "2018-07-16T05:52:30Z", "closed_at": "2018-04-11T10:10:37Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Y</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:source</li>\n<li><strong>TensorFlow version (use command below)</strong>:('v1.7.0-1116-g5d33c1e', '1.7.0')</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:0.11.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:4.8.5</li>\n<li><strong>CUDA/cuDNN version</strong>:9.1/7.1</li>\n<li><strong>GPU model and memory</strong>: TITAN X (Pascal)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>HI,I am trying to quantize ssd_mobilenet_v1 using tensorflow object detection api.First,I replace all the graph_hook_fn with tf.contrib.quantize.create_training_graph and enable fused batch norm in <a href=\"https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L114\">https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L114</a>. After training, I manage to get the mobilenet_ssd.tflite with the commands below.Then,I deploy the tflite model file in  a SAMSUNG GALAXY Note 8 but failed with the following exception,which happens in:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/3e0fd55ccec1f8ac5ca7d11f9999a16871a9198c/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L410-L419\">tensorflow/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 410 to 419\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/3e0fd55ccec1f8ac5ca7d11f9999a16871a9198c\">3e0fd55</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L410\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"410\"></td>\n          <td id=\"LC410\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c\"><span class=\"pl-c\">//</span> allocates memory</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L411\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"411\"></td>\n          <td id=\"LC411\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   status = interpreter-&gt;<span class=\"pl-c1\">AllocateTensors</span>(); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L412\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"412\"></td>\n          <td id=\"LC412\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">if</span> (status != <span class=\"pl-c1\">kTfLiteOk</span>) { </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L413\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"413\"></td>\n          <td id=\"LC413\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-c1\">throwException</span>(env, <span class=\"pl-c1\">kNullPointerException</span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L414\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"414\"></td>\n          <td id=\"LC414\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                    <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Can not allocate memory for the interpreter<span class=\"pl-pds\">\"</span></span>, </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L415\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"415\"></td>\n          <td id=\"LC415\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">                    error_reporter-&gt;<span class=\"pl-c1\">CachedErrorMessage</span>()); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L416\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"416\"></td>\n          <td id=\"LC416\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">     <span class=\"pl-k\">return</span> <span class=\"pl-c1\">0</span>; </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L417\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"417\"></td>\n          <td id=\"LC417\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   } </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L418\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"418\"></td>\n          <td id=\"LC418\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-k\">return</span> <span class=\"pl-k\">reinterpret_cast</span>&lt;jlong&gt;(interpreter.<span class=\"pl-c1\">release</span>()); </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L419\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"419\"></td>\n          <td id=\"LC419\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> } </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<blockquote>\n<pre><code>java.lang.RuntimeException: java.lang.NullPointerException: Can not allocate memory for the interpreter at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:181)\n</code></pre>\n</blockquote>\n<pre><code>    at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:109)\n    at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)\n    at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1162)\n    at android.os.Handler.dispatchMessage(Handler.java:102)\n    at android.os.Looper.loop(Looper.java:135)\n    at android.app.ActivityThread.main(ActivityThread.java:5232)\n    at java.lang.reflect.Method.invoke(Native Method)\n    at java.lang.reflect.Method.invoke(Method.java:372)\n    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:904)\n    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:699)\n Caused by: java.lang.NullPointerException: Can not allocate memory for the interpreter\n    at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\n    at org.tensorflow.lite.NativeInterpreterWrapper.&lt;init&gt;(NativeInterpreterWrapper.java:50)\n    at org.tensorflow.lite.Interpreter.&lt;init&gt;(Interpreter.java:77)\n    at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:179)\n    at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:109)\u00a0\n    at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)\u00a0\n    at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1162)\u00a0\n    at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\n    at android.os.Looper.loop(Looper.java:135)\u00a0\n    at android.app.ActivityThread.main(ActivityThread.java:5232)\u00a0\n    at java.lang.reflect.Method.invoke(Native Method)\u00a0\n    at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\n    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:904)\u00a0\n    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:699)\u00a0\n</code></pre>\n<p>When I convert .tflite back to .pb format and check the inference graph in tensorboard,I find some relu6_unfused nodes are not quantized properly.What is the problem?<br>\n<strong>Before toco convert</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/37242917/38499663-e9870b50-3c3a-11e8-9993-431b392f3104.png\"><img src=\"https://user-images.githubusercontent.com/37242917/38499663-e9870b50-3c3a-11e8-9993-431b392f3104.png\" alt=\"before\" style=\"max-width:100%;\"></a></p>\n<p><strong>After toco convert</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/37242917/38499487-4c8a5212-3c3a-11e8-99d9-a7ad4910257d.png\"><img src=\"https://user-images.githubusercontent.com/37242917/38499487-4c8a5212-3c3a-11e8-99d9-a7ad4910257d.png\" alt=\"relu\" style=\"max-width:100%;\"></a><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/37242917/38533796-71355b52-3cad-11e8-91fa-66e0845de5bc.png\"><img src=\"https://user-images.githubusercontent.com/37242917/38533796-71355b52-3cad-11e8-91fa-66e0845de5bc.png\" alt=\"screenshot from 2018-04-10 10-51-35\" style=\"max-width:100%;\"></a></p>\n<p>By the way,if I use \"dummy-quantization\" to try out quantized inference on a float graph,the tflite model works fine in android,and the structure of Graph in GraphViz Dot format  just looks the same as  my quantized version.</p>\n<h3>Source code / logs</h3>\n<blockquote>\n<p>#Strip out problematic nodes before even letting TOCO see the graphdef<br>\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- <br>\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True <br>\n--input_names=Preprocessor/sub --output_names=concat,concat_1 <br>\n--alsologtostderr</p>\n</blockquote>\n<blockquote>\n<p>#Run TOCO conversion.<br>\nIMAGE_SIZE=300<br>\nbazel run tensorflow/contrib/lite/toco:toco -- <br>\n  --input_file=$STRIPPED_PB <br>\n  --output_file=$DETECT_FB <br>\n  --input_format=TENSORFLOW_GRAPHDEF <br>\n  --output_format=TFLITE <br>\n  --input_shapes=1,${IMAGE_SIZE},${IMAGE_SIZE},3 <br>\n  --input_arrays=Preprocessor/sub <br>\n  --output_arrays=concat,concat_1 <br>\n  --inference_type=QUANTIZED_UINT8 <br>\n  --mean_values=128 <br>\n  --std_values=127 <br>\n  --dump_graphviz=/tmp</p>\n</blockquote>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Y\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):16.04\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):('v1.7.0-1116-g5d33c1e', '1.7.0')\nPython version: 2.7\nBazel version (if compiling from source):0.11.1\nGCC/Compiler version (if compiling from source):4.8.5\nCUDA/cuDNN version:9.1/7.1\nGPU model and memory: TITAN X (Pascal)\n\nDescribe the problem\nHI,I am trying to quantize ssd_mobilenet_v1 using tensorflow object detection api.First,I replace all the graph_hook_fn with tf.contrib.quantize.create_training_graph and enable fused batch norm in https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L114. After training, I manage to get the mobilenet_ssd.tflite with the commands below.Then,I deploy the tflite model file in  a SAMSUNG GALAXY Note 8 but failed with the following exception,which happens in:\n\n  \n    \n      tensorflow/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc\n    \n    \n        Lines 410 to 419\n      in\n      3e0fd55\n    \n    \n    \n    \n\n        \n          \n             // allocates memory \n        \n\n        \n          \n             status = interpreter->AllocateTensors(); \n        \n\n        \n          \n             if (status != kTfLiteOk) { \n        \n\n        \n          \n               throwException(env, kNullPointerException, \n        \n\n        \n          \n                              \"Can not allocate memory for the interpreter\", \n        \n\n        \n          \n                              error_reporter->CachedErrorMessage()); \n        \n\n        \n          \n               return 0; \n        \n\n        \n          \n             } \n        \n\n        \n          \n             return reinterpret_cast<jlong>(interpreter.release()); \n        \n\n        \n          \n           } \n        \n    \n  \n\n\n\njava.lang.RuntimeException: java.lang.NullPointerException: Can not allocate memory for the interpreter at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:181)\n\n\n    at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:109)\n    at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)\n    at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1162)\n    at android.os.Handler.dispatchMessage(Handler.java:102)\n    at android.os.Looper.loop(Looper.java:135)\n    at android.app.ActivityThread.main(ActivityThread.java:5232)\n    at java.lang.reflect.Method.invoke(Native Method)\n    at java.lang.reflect.Method.invoke(Method.java:372)\n    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:904)\n    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:699)\n Caused by: java.lang.NullPointerException: Can not allocate memory for the interpreter\n    at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\n    at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:50)\n    at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:77)\n    at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:179)\n    at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:109)\u00a0\n    at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)\u00a0\n    at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1162)\u00a0\n    at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\n    at android.os.Looper.loop(Looper.java:135)\u00a0\n    at android.app.ActivityThread.main(ActivityThread.java:5232)\u00a0\n    at java.lang.reflect.Method.invoke(Native Method)\u00a0\n    at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\n    at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:904)\u00a0\n    at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:699)\u00a0\n\nWhen I convert .tflite back to .pb format and check the inference graph in tensorboard,I find some relu6_unfused nodes are not quantized properly.What is the problem?\nBefore toco convert\n\nAfter toco convert\n\n\nBy the way,if I use \"dummy-quantization\" to try out quantized inference on a float graph,the tflite model works fine in android,and the structure of Graph in GraphViz Dot format  just looks the same as  my quantized version.\nSource code / logs\n\n#Strip out problematic nodes before even letting TOCO see the graphdef\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \n--input_names=Preprocessor/sub --output_names=concat,concat_1 \n--alsologtostderr\n\n\n#Run TOCO conversion.\nIMAGE_SIZE=300\nbazel run tensorflow/contrib/lite/toco:toco -- \n  --input_file=$STRIPPED_PB \n  --output_file=$DETECT_FB \n  --input_format=TENSORFLOW_GRAPHDEF \n  --output_format=TFLITE \n  --input_shapes=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \n  --input_arrays=Preprocessor/sub \n  --output_arrays=concat,concat_1 \n  --inference_type=QUANTIZED_UINT8 \n  --mean_values=128 \n  --std_values=127 \n  --dump_graphviz=/tmp", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Y\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:16.04\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:('v1.7.0-1116-g5d33c1e', '1.7.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:0.11.1\r\n- **GCC/Compiler version (if compiling from source)**:4.8.5\r\n- **CUDA/cuDNN version**:9.1/7.1\r\n- **GPU model and memory**: TITAN X (Pascal) \r\n\r\n\r\n\r\n### Describe the problem\r\n HI,I am trying to quantize ssd_mobilenet_v1 using tensorflow object detection api.First,I replace all the graph_hook_fn with tf.contrib.quantize.create_training_graph and enable fused batch norm in https://github.com/tensorflow/models/blob/b6bcc450b981eba721ee2760c92d87da86900988/research/object_detection/models/ssd_mobilenet_v1_feature_extractor.py#L114. After training, I manage to get the mobilenet_ssd.tflite with the commands below.Then,I deploy the tflite model file in  a SAMSUNG GALAXY Note 8 but failed with the following exception,which happens in:\r\nhttps://github.com/tensorflow/tensorflow/blob/3e0fd55ccec1f8ac5ca7d11f9999a16871a9198c/tensorflow/contrib/lite/java/src/main/native/nativeinterpreterwrapper_jni.cc#L410-L419\r\n\r\n>     java.lang.RuntimeException: java.lang.NullPointerException: Can not allocate memory for the interpreter at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:181)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:109)\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1162)\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\r\n        at android.os.Looper.loop(Looper.java:135)\r\n        at android.app.ActivityThread.main(ActivityThread.java:5232)\r\n        at java.lang.reflect.Method.invoke(Native Method)\r\n        at java.lang.reflect.Method.invoke(Method.java:372)\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:904)\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:699)\r\n     Caused by: java.lang.NullPointerException: Can not allocate memory for the interpreter\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.createInterpreter(Native Method)\r\n        at org.tensorflow.lite.NativeInterpreterWrapper.<init>(NativeInterpreterWrapper.java:50)\r\n        at org.tensorflow.lite.Interpreter.<init>(Interpreter.java:77)\r\n        at org.tensorflow.demo.TFLiteObjectDetectionAPIModel.create(TFLiteObjectDetectionAPIModel.java:179)\r\n        at org.tensorflow.demo.DetectorActivity.onPreviewSizeChosen(DetectorActivity.java:109)\u00a0\r\n        at org.tensorflow.demo.CameraActivity.onPreviewFrame(CameraActivity.java:119)\u00a0\r\n        at android.hardware.Camera$EventHandler.handleMessage(Camera.java:1162)\u00a0\r\n        at android.os.Handler.dispatchMessage(Handler.java:102)\u00a0\r\n        at android.os.Looper.loop(Looper.java:135)\u00a0\r\n        at android.app.ActivityThread.main(ActivityThread.java:5232)\u00a0\r\n        at java.lang.reflect.Method.invoke(Native Method)\u00a0\r\n        at java.lang.reflect.Method.invoke(Method.java:372)\u00a0\r\n        at com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:904)\u00a0\r\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:699)\u00a0\r\n\r\nWhen I convert .tflite back to .pb format and check the inference graph in tensorboard,I find some relu6_unfused nodes are not quantized properly.What is the problem?\r\n**Before toco convert** \r\n![before](https://user-images.githubusercontent.com/37242917/38499663-e9870b50-3c3a-11e8-9993-431b392f3104.png)\r\n\r\n**After toco convert** \r\n![relu](https://user-images.githubusercontent.com/37242917/38499487-4c8a5212-3c3a-11e8-99d9-a7ad4910257d.png)\r\n![screenshot from 2018-04-10 10-51-35](https://user-images.githubusercontent.com/37242917/38533796-71355b52-3cad-11e8-91fa-66e0845de5bc.png)\r\n\r\nBy the way,if I use \"dummy-quantization\" to try out quantized inference on a float graph,the tflite model works fine in android,and the structure of Graph in GraphViz Dot format  just looks the same as  my quantized version.\r\n \r\n### Source code / logs\r\n\r\n>  #Strip out problematic nodes before even letting TOCO see the graphdef\r\nbazel run -c opt tensorflow/python/tools/optimize_for_inference -- \\\r\n--input=$DETECT_PB  --output=$STRIPPED_PB --frozen_graph=True \\\r\n--input_names=Preprocessor/sub --output_names=concat,concat_1 \\\r\n--alsologtostderr\r\n\r\n \r\n\r\n> #Run TOCO conversion.\r\nIMAGE_SIZE=300\r\nbazel run tensorflow/contrib/lite/toco:toco -- \\\r\n  --input_file=$STRIPPED_PB \\\r\n  --output_file=$DETECT_FB \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --input_shapes=1,${IMAGE_SIZE},${IMAGE_SIZE},3 \\\r\n  --input_arrays=Preprocessor/sub \\\r\n  --output_arrays=concat,concat_1 \\\r\n  --inference_type=QUANTIZED_UINT8 \\\r\n  --mean_values=128 \\\r\n  --std_values=127 \\\r\n  --dump_graphviz=/tmp\r\n\r\n\r\n"}