{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307800606", "html_url": "https://github.com/tensorflow/tensorflow/issues/7162#issuecomment-307800606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7162", "id": 307800606, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzgwMDYwNg==", "user": {"login": "lissyx", "id": 1645737, "node_id": "MDQ6VXNlcjE2NDU3Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1645737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lissyx", "html_url": "https://github.com/lissyx", "followers_url": "https://api.github.com/users/lissyx/followers", "following_url": "https://api.github.com/users/lissyx/following{/other_user}", "gists_url": "https://api.github.com/users/lissyx/gists{/gist_id}", "starred_url": "https://api.github.com/users/lissyx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lissyx/subscriptions", "organizations_url": "https://api.github.com/users/lissyx/orgs", "repos_url": "https://api.github.com/users/lissyx/repos", "events_url": "https://api.github.com/users/lissyx/events{/privacy}", "received_events_url": "https://api.github.com/users/lissyx/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-12T14:09:02Z", "updated_at": "2017-06-12T14:11:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3124581\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wodesuck\">@wodesuck</a> Experimenting with your fix, I spotted that even models without a RNN layer (actually we do use <code>tf.nn.bidirectional_dynamic_rnn()</code>) do not report the same inference time. From the benchmark I could run locally, I identified, on our mozilla/DeepSpeech model nuking the bidirectionnal dynamic layers:</p>\n<ul>\n<li>model 8-bitized with transform tool without any change shows inference time ~ 0.8secs</li>\n<li>model 8-bitized with fixed transform tools shows inference time ~1.1secs</li>\n</ul>", "body_text": "@wodesuck Experimenting with your fix, I spotted that even models without a RNN layer (actually we do use tf.nn.bidirectional_dynamic_rnn()) do not report the same inference time. From the benchmark I could run locally, I identified, on our mozilla/DeepSpeech model nuking the bidirectionnal dynamic layers:\n\nmodel 8-bitized with transform tool without any change shows inference time ~ 0.8secs\nmodel 8-bitized with fixed transform tools shows inference time ~1.1secs", "body": "@wodesuck Experimenting with your fix, I spotted that even models without a RNN layer (actually we do use `tf.nn.bidirectional_dynamic_rnn()`) do not report the same inference time. From the benchmark I could run locally, I identified, on our mozilla/DeepSpeech model nuking the bidirectionnal dynamic layers:\r\n - model 8-bitized with transform tool without any change shows inference time ~ 0.8secs\r\n - model 8-bitized with fixed transform tools shows inference time ~1.1secs"}