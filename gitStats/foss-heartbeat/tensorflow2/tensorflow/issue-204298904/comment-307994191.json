{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307994191", "html_url": "https://github.com/tensorflow/tensorflow/issues/7162#issuecomment-307994191", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7162", "id": 307994191, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzk5NDE5MQ==", "user": {"login": "wodesuck", "id": 3124581, "node_id": "MDQ6VXNlcjMxMjQ1ODE=", "avatar_url": "https://avatars2.githubusercontent.com/u/3124581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wodesuck", "html_url": "https://github.com/wodesuck", "followers_url": "https://api.github.com/users/wodesuck/followers", "following_url": "https://api.github.com/users/wodesuck/following{/other_user}", "gists_url": "https://api.github.com/users/wodesuck/gists{/gist_id}", "starred_url": "https://api.github.com/users/wodesuck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wodesuck/subscriptions", "organizations_url": "https://api.github.com/users/wodesuck/orgs", "repos_url": "https://api.github.com/users/wodesuck/repos", "events_url": "https://api.github.com/users/wodesuck/events{/privacy}", "received_events_url": "https://api.github.com/users/wodesuck/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-13T03:11:45Z", "updated_at": "2017-06-13T03:11:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1645737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lissyx\">@lissyx</a> That's right.</p>\n<p>Here is the code, it is not suit for general use (maybe someone have a node name \"while\" or a while loop name without \"while\"), so I don't add it in the PR. Besides, there's more works could do to optimize a specific model (run some benchmark &amp; try to optimize the bottleneck), I had customized a lot in my <code>transform_graph</code>.</p>\n<div class=\"highlight highlight-source-c++\"><pre>          <span class=\"pl-c\"><span class=\"pl-c\">//</span> Add some common constants we need for reshaping inputs.</span>\n          NodeDef reshape_dims;\n          reshape_dims.set_op(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Const<span class=\"pl-pds\">\"</span></span>);\n          reshape_dims.set_name(unique_input_name + <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/reshape_dims<span class=\"pl-pds\">\"</span></span>);\n          <span class=\"pl-k\">if</span> (float_node.name().find(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/while/<span class=\"pl-pds\">\"</span></span>) != string::npos) {\n            <span class=\"pl-c1\">AddNodeInput</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>^<span class=\"pl-pds\">\"</span></span> + input_name, &amp;reshape_dims);\n          }\n          <span class=\"pl-en\">SetNodeAttr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dtype<span class=\"pl-pds\">\"</span></span>, DT_INT32, &amp;reshape_dims);\n          Tensor <span class=\"pl-en\">reshape_dims_tensor</span>(DT_INT32, {<span class=\"pl-c1\">1</span>});\n          reshape_dims_tensor.flat&lt;int32&gt;()(<span class=\"pl-c1\">0</span>) = -<span class=\"pl-c1\">1</span>;\n          SetNodeTensorAttr&lt;int32&gt;(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>value<span class=\"pl-pds\">\"</span></span>, reshape_dims_tensor, &amp;reshape_dims);\n          new_nodes-&gt;<span class=\"pl-en\">push_back</span>(reshape_dims);\n\n          NodeDef reduction_dims;\n          reduction_dims.set_op(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Const<span class=\"pl-pds\">\"</span></span>);\n          reduction_dims.set_name(unique_input_name + <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/reduction_dims<span class=\"pl-pds\">\"</span></span>);\n          <span class=\"pl-k\">if</span> (float_node.name().find(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/while/<span class=\"pl-pds\">\"</span></span>) != string::npos) {\n            <span class=\"pl-c1\">AddNodeInput</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>^<span class=\"pl-pds\">\"</span></span> + input_name, &amp;reduction_dims);\n          }\n          <span class=\"pl-en\">SetNodeAttr</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dtype<span class=\"pl-pds\">\"</span></span>, DT_INT32, &amp;reduction_dims);\n          Tensor <span class=\"pl-en\">reduction_dims_tensor</span>(DT_INT32, {<span class=\"pl-c1\">1</span>});\n          reduction_dims_tensor.flat&lt;int32&gt;()(<span class=\"pl-c1\">0</span>) = <span class=\"pl-c1\">0</span>;\n          SetNodeTensorAttr&lt;int32&gt;(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>value<span class=\"pl-pds\">\"</span></span>, reduction_dims_tensor,\n                                   &amp;reduction_dims);\n          new_nodes-&gt;<span class=\"pl-en\">push_back</span>(reduction_dims);</pre></div>", "body_text": "@lissyx That's right.\nHere is the code, it is not suit for general use (maybe someone have a node name \"while\" or a while loop name without \"while\"), so I don't add it in the PR. Besides, there's more works could do to optimize a specific model (run some benchmark & try to optimize the bottleneck), I had customized a lot in my transform_graph.\n          // Add some common constants we need for reshaping inputs.\n          NodeDef reshape_dims;\n          reshape_dims.set_op(\"Const\");\n          reshape_dims.set_name(unique_input_name + \"/reshape_dims\");\n          if (float_node.name().find(\"/while/\") != string::npos) {\n            AddNodeInput(\"^\" + input_name, &reshape_dims);\n          }\n          SetNodeAttr(\"dtype\", DT_INT32, &reshape_dims);\n          Tensor reshape_dims_tensor(DT_INT32, {1});\n          reshape_dims_tensor.flat<int32>()(0) = -1;\n          SetNodeTensorAttr<int32>(\"value\", reshape_dims_tensor, &reshape_dims);\n          new_nodes->push_back(reshape_dims);\n\n          NodeDef reduction_dims;\n          reduction_dims.set_op(\"Const\");\n          reduction_dims.set_name(unique_input_name + \"/reduction_dims\");\n          if (float_node.name().find(\"/while/\") != string::npos) {\n            AddNodeInput(\"^\" + input_name, &reduction_dims);\n          }\n          SetNodeAttr(\"dtype\", DT_INT32, &reduction_dims);\n          Tensor reduction_dims_tensor(DT_INT32, {1});\n          reduction_dims_tensor.flat<int32>()(0) = 0;\n          SetNodeTensorAttr<int32>(\"value\", reduction_dims_tensor,\n                                   &reduction_dims);\n          new_nodes->push_back(reduction_dims);", "body": "@lissyx That's right.\r\n\r\nHere is the code, it is not suit for general use (maybe someone have a node name \"while\" or a while loop name without \"while\"), so I don't add it in the PR. Besides, there's more works could do to optimize a specific model (run some benchmark & try to optimize the bottleneck), I had customized a lot in my `transform_graph`.\r\n```c++\r\n          // Add some common constants we need for reshaping inputs.\r\n          NodeDef reshape_dims;\r\n          reshape_dims.set_op(\"Const\");\r\n          reshape_dims.set_name(unique_input_name + \"/reshape_dims\");\r\n          if (float_node.name().find(\"/while/\") != string::npos) {\r\n            AddNodeInput(\"^\" + input_name, &reshape_dims);\r\n          }\r\n          SetNodeAttr(\"dtype\", DT_INT32, &reshape_dims);\r\n          Tensor reshape_dims_tensor(DT_INT32, {1});\r\n          reshape_dims_tensor.flat<int32>()(0) = -1;\r\n          SetNodeTensorAttr<int32>(\"value\", reshape_dims_tensor, &reshape_dims);\r\n          new_nodes->push_back(reshape_dims);\r\n\r\n          NodeDef reduction_dims;\r\n          reduction_dims.set_op(\"Const\");\r\n          reduction_dims.set_name(unique_input_name + \"/reduction_dims\");\r\n          if (float_node.name().find(\"/while/\") != string::npos) {\r\n            AddNodeInput(\"^\" + input_name, &reduction_dims);\r\n          }\r\n          SetNodeAttr(\"dtype\", DT_INT32, &reduction_dims);\r\n          Tensor reduction_dims_tensor(DT_INT32, {1});\r\n          reduction_dims_tensor.flat<int32>()(0) = 0;\r\n          SetNodeTensorAttr<int32>(\"value\", reduction_dims_tensor,\r\n                                   &reduction_dims);\r\n          new_nodes->push_back(reduction_dims);\r\n```"}