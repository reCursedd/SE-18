{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287583105", "html_url": "https://github.com/tensorflow/tensorflow/pull/8405#issuecomment-287583105", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8405", "id": 287583105, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzU4MzEwNQ==", "user": {"login": "gmbender", "id": 7493680, "node_id": "MDQ6VXNlcjc0OTM2ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/7493680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gmbender", "html_url": "https://github.com/gmbender", "followers_url": "https://api.github.com/users/gmbender/followers", "following_url": "https://api.github.com/users/gmbender/following{/other_user}", "gists_url": "https://api.github.com/users/gmbender/gists{/gist_id}", "starred_url": "https://api.github.com/users/gmbender/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gmbender/subscriptions", "organizations_url": "https://api.github.com/users/gmbender/orgs", "repos_url": "https://api.github.com/users/gmbender/repos", "events_url": "https://api.github.com/users/gmbender/events{/privacy}", "received_events_url": "https://api.github.com/users/gmbender/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-18T23:40:10Z", "updated_at": "2017-03-18T23:42:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=381635\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/louiehelm\">@louiehelm</a> Both AdamOptimizer and LazyAdamOptimizer implement dense updates using C++ op kernels, and implement sparse updates using standard TensorFlow ops like gather() and scatter_update(). Furthermore, the code for handling dense updates is shared between both optimizers. So if you're seeing a slow-down going from Adam to LazyAdam, I suspect that it's for other reasons.</p>\n<p>While LazyAdamOptimizer is several times faster than AdamOptimizer for my use-case (which involves training embedding matrices with sparse updates), I've noticed that my model training jobs get lower throughput with LazyAdam than they do with RMSProp or Adagrad. At peak throughput, my training jobs get around 1600 steps/second with Adagrad, 1550 steps/second with RMSProp, and 1250 steps/second with LazyAdam. Even with lower throughput, LazyAdam consumes more CPU on the parameter servers than Adagrad/RMSProp. It's possible that this is just because Adam is more compute-intensive than the other two algorithms (especially Adagrad). But the fact that Adagrad and RMSProp use C++ op kernels to handle sparse as well as dense updates might also have something to do with it.</p>\n<p>I also suspect that the current implementation of Nadam is likely to get poor throughput for jobs that apply sparse updates to embedding matrices, for exactly the same reason that AdamOptimizer gets poor throughput. This part of the _apply_sparse() function is likely to be a bottleneck:</p>\n<pre><code>m_t = state_ops.assign(m, m * beta1_t,\n                       use_locking=self._use_locking)\nm_t = state_ops.scatter_add(m_t, grad.indices, (tp1 / tp) * m_scaled_g_values,\n                            use_locking=self._use_locking)\n</code></pre>\n<p>The <code>assign</code> op will touch every single row of the embedding matrix, even if not all of the rows appear in the current batch of training examples. It's always possible to add a LazyNadamOptimizer subclass in the future, as we did with LazyAdam, but I wonder if there's a better solution.</p>", "body_text": "@louiehelm Both AdamOptimizer and LazyAdamOptimizer implement dense updates using C++ op kernels, and implement sparse updates using standard TensorFlow ops like gather() and scatter_update(). Furthermore, the code for handling dense updates is shared between both optimizers. So if you're seeing a slow-down going from Adam to LazyAdam, I suspect that it's for other reasons.\nWhile LazyAdamOptimizer is several times faster than AdamOptimizer for my use-case (which involves training embedding matrices with sparse updates), I've noticed that my model training jobs get lower throughput with LazyAdam than they do with RMSProp or Adagrad. At peak throughput, my training jobs get around 1600 steps/second with Adagrad, 1550 steps/second with RMSProp, and 1250 steps/second with LazyAdam. Even with lower throughput, LazyAdam consumes more CPU on the parameter servers than Adagrad/RMSProp. It's possible that this is just because Adam is more compute-intensive than the other two algorithms (especially Adagrad). But the fact that Adagrad and RMSProp use C++ op kernels to handle sparse as well as dense updates might also have something to do with it.\nI also suspect that the current implementation of Nadam is likely to get poor throughput for jobs that apply sparse updates to embedding matrices, for exactly the same reason that AdamOptimizer gets poor throughput. This part of the _apply_sparse() function is likely to be a bottleneck:\nm_t = state_ops.assign(m, m * beta1_t,\n                       use_locking=self._use_locking)\nm_t = state_ops.scatter_add(m_t, grad.indices, (tp1 / tp) * m_scaled_g_values,\n                            use_locking=self._use_locking)\n\nThe assign op will touch every single row of the embedding matrix, even if not all of the rows appear in the current batch of training examples. It's always possible to add a LazyNadamOptimizer subclass in the future, as we did with LazyAdam, but I wonder if there's a better solution.", "body": "@louiehelm Both AdamOptimizer and LazyAdamOptimizer implement dense updates using C++ op kernels, and implement sparse updates using standard TensorFlow ops like gather() and scatter_update(). Furthermore, the code for handling dense updates is shared between both optimizers. So if you're seeing a slow-down going from Adam to LazyAdam, I suspect that it's for other reasons.\r\n\r\nWhile LazyAdamOptimizer is several times faster than AdamOptimizer for my use-case (which involves training embedding matrices with sparse updates), I've noticed that my model training jobs get lower throughput with LazyAdam than they do with RMSProp or Adagrad. At peak throughput, my training jobs get around 1600 steps/second with Adagrad, 1550 steps/second with RMSProp, and 1250 steps/second with LazyAdam. Even with lower throughput, LazyAdam consumes more CPU on the parameter servers than Adagrad/RMSProp. It's possible that this is just because Adam is more compute-intensive than the other two algorithms (especially Adagrad). But the fact that Adagrad and RMSProp use C++ op kernels to handle sparse as well as dense updates might also have something to do with it.\r\n\r\nI also suspect that the current implementation of Nadam is likely to get poor throughput for jobs that apply sparse updates to embedding matrices, for exactly the same reason that AdamOptimizer gets poor throughput. This part of the _apply_sparse() function is likely to be a bottleneck:\r\n```\r\nm_t = state_ops.assign(m, m * beta1_t,\r\n                       use_locking=self._use_locking)\r\nm_t = state_ops.scatter_add(m_t, grad.indices, (tp1 / tp) * m_scaled_g_values,\r\n                            use_locking=self._use_locking)\r\n```\r\nThe `assign` op will touch every single row of the embedding matrix, even if not all of the rows appear in the current batch of training examples. It's always possible to add a LazyNadamOptimizer subclass in the future, as we did with LazyAdam, but I wonder if there's a better solution."}