{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/110971512", "pull_request_review_id": 32178857, "id": 110971512, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMDk3MTUxMg==", "diff_hunk": "@@ -2906,4 +2965,303 @@ REGISTER_KERNELS(double, int64);\n \n #undef REGISTER_KERNELS\n \n+\n+\n+\n+\n+\n+\n+\n+\n+\n+template <typename Device, typename T>\n+class ApplyRadamOp : public OpKernel {\n+ public:\n+  explicit ApplyRadamOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_locking\", &use_exclusive_lock_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    auto locks = MaybeLockMutexesInOrder(ctx, use_exclusive_lock_, {0, 1, 2});\n+\n+    Tensor var;\n+    OP_REQUIRES_OK(ctx, GetInputTensor(ctx, 0, use_exclusive_lock_, &var));\n+    Tensor m;\n+    OP_REQUIRES_OK(ctx, GetInputTensor(ctx, 1, use_exclusive_lock_, &m));\n+    Tensor v;\n+    OP_REQUIRES_OK(ctx, GetInputTensor(ctx, 2, use_exclusive_lock_, &v));\n+    OP_REQUIRES(\n+        ctx, var.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", def().input(0)));\n+    OP_REQUIRES(\n+        ctx, m.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", def().input(1)));\n+    OP_REQUIRES(\n+        ctx, v.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", def().input(2)));\n+\n+    const Tensor& beta1_power = ctx->input(3);\n+    const Tensor& beta2_power = ctx->input(4);\n+    const Tensor& lr = ctx->input(5);\n+    const Tensor& beta1 = ctx->input(6);\n+    const Tensor& beta2 = ctx->input(7);\n+    const Tensor& epsilon = ctx->input(8);\n+    const Tensor& gamma = ctx->input(9);\n+\n+\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(beta1_power.shape()),\n+                errors::InvalidArgument(\"beta1_power is not a scalar: \",\n+                                        beta1_power.shape().DebugString()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(beta2_power.shape()),\n+                errors::InvalidArgument(\"beta2_power is not a scalar: \",\n+                                        beta2_power.shape().DebugString()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(lr.shape()),\n+                errors::InvalidArgument(\"lr is not a scalar : \",\n+                                        lr.shape().DebugString()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(beta1.shape()),\n+                errors::InvalidArgument(\"beta1 is not a scalar: \",\n+                                        beta1.shape().DebugString()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(beta2.shape()),\n+                errors::InvalidArgument(\"beta2 is not a scalar: \",\n+                                        beta2.shape().DebugString()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(epsilon.shape()),\n+                errors::InvalidArgument(\"epsilon is not a scalar: \",\n+                                        epsilon.shape().DebugString()));\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(gamma.shape()),\n+                errors::InvalidArgument(\"gamma is not a scalar: \",\n+                                        gamma.shape().DebugString()));\n+    \n+    const Tensor& grad = ctx->input(10);\n+    OP_REQUIRES(ctx, var.shape().IsSameSize(m.shape()),\n+                errors::InvalidArgument(\"var and m do not have the same shape\",\n+                                        var.shape().DebugString(), \" \",\n+                                        m.shape().DebugString()));\n+    OP_REQUIRES(ctx, var.shape().IsSameSize(v.shape()),\n+                errors::InvalidArgument(\"var and v do not have the same shape\",\n+                                        var.shape().DebugString(), \" \",\n+                                        v.shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, var.shape().IsSameSize(grad.shape()),\n+        errors::InvalidArgument(\"var and grad do not have the same shape\",\n+                                var.shape().DebugString(), \" \",\n+                                grad.shape().DebugString()));\n+\n+    const Device& device = ctx->template eigen_device<Device>();\n+    functor::ApplyRadam<Device, T>()(device, var.flat<T>(), m.flat<T>(),\n+                                    v.flat<T>(), beta1_power.scalar<T>(),\n+                                    beta2_power.scalar<T>(), lr.scalar<T>(),\n+                                    beta1.scalar<T>(), beta2.scalar<T>(),\n+                                    epsilon.scalar<T>(), gamma.scalar<T>(),\n+                                    grad.flat<T>());\n+\n+    MaybeForwardRefInputToRefOutput(ctx, 0, 0);\n+  }\n+\n+ private:\n+  bool use_exclusive_lock_;\n+};\n+\n+using CPUDevice = Eigen::ThreadPoolDevice;\n+using GPUDevice = Eigen::GpuDevice;\n+\n+#define REGISTER_KERNELS(D, T)                                              \\\n+  REGISTER_KERNEL_BUILDER(                                                  \\\n+      Name(\"ApplyRadam\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"),         \\\n+      ApplyRadamOp<D##Device, T>);                                          \\\n+  REGISTER_KERNEL_BUILDER(                                                  \\\n+      Name(\"ResourceApplyRadam\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n+      ApplyRadamOp<D##Device, T>);\n+#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T);\n+\n+TF_CALL_half(REGISTER_CPU_KERNELS);\n+TF_CALL_float(REGISTER_CPU_KERNELS);\n+TF_CALL_double(REGISTER_CPU_KERNELS);\n+\n+#ifdef TENSORFLOW_USE_SYCL\n+#define REGISTER_SYCL_KERNELS(T) REGISTER_KERNELS(SYCL, T);\n+\n+TF_CALL_float(REGISTER_SYCL_KERNELS);\n+#endif", "path": "tensorflow/core/kernels/training_ops.cc", "position": null, "original_position": 189, "commit_id": "ebd35938768ba8b483ed82c2f7233c7b5a608340", "original_commit_id": "484c9e8af2f9a074b0fe873dc9a61640b7e372af", "user": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "body": " // TENSORFLOW_USE_SYCL", "created_at": "2017-04-11T17:56:36Z", "updated_at": "2017-04-12T18:37:13Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8405#discussion_r110971512", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8405", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/110971512"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8405#discussion_r110971512"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8405"}}, "body_html": "<p>// TENSORFLOW_USE_SYCL</p>", "body_text": "// TENSORFLOW_USE_SYCL"}