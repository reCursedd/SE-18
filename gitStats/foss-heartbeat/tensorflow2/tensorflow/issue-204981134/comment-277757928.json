{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/277757928", "html_url": "https://github.com/tensorflow/tensorflow/issues/7225#issuecomment-277757928", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7225", "id": 277757928, "node_id": "MDEyOklzc3VlQ29tbWVudDI3Nzc1NzkyOA==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-06T17:47:57Z", "updated_at": "2017-02-06T17:47:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> if you look at the implementation of clip_by_value, it is exactly doing <code>tf.minimum(upper, tf.maximum(lower, x))</code>, so presumably we could change the documentation to not mention it's only scalar arguments.</p>\n<p>Your gradient argument seems reasonable, I'm going to assume that XLA will help address these issues instead of writing custom kernels.</p>\n<p>However, if someone wants to write a custom CPU and GPU kernel for clip_by_value that fuses it in the short term, that would be a nice intermediate contribution.</p>\n<p>(p.s., I actually didn't write this op).</p>", "body_text": "@yaroslavvb if you look at the implementation of clip_by_value, it is exactly doing tf.minimum(upper, tf.maximum(lower, x)), so presumably we could change the documentation to not mention it's only scalar arguments.\nYour gradient argument seems reasonable, I'm going to assume that XLA will help address these issues instead of writing custom kernels.\nHowever, if someone wants to write a custom CPU and GPU kernel for clip_by_value that fuses it in the short term, that would be a nice intermediate contribution.\n(p.s., I actually didn't write this op).", "body": "@yaroslavvb if you look at the implementation of clip_by_value, it is exactly doing ```tf.minimum(upper, tf.maximum(lower, x))```, so presumably we could change the documentation to not mention it's only scalar arguments.\r\n\r\nYour gradient argument seems reasonable, I'm going to assume that XLA will help address these issues instead of writing custom kernels.\r\n\r\nHowever, if someone wants to write a custom CPU and GPU kernel for clip_by_value that fuses it in the short term, that would be a nice intermediate contribution.\r\n\r\n(p.s., I actually didn't write this op)."}