{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/377780128", "html_url": "https://github.com/tensorflow/tensorflow/issues/11974#issuecomment-377780128", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11974", "id": 377780128, "node_id": "MDEyOklzc3VlQ29tbWVudDM3Nzc4MDEyOA==", "user": {"login": "dhinar1991", "id": 26268279, "node_id": "MDQ6VXNlcjI2MjY4Mjc5", "avatar_url": "https://avatars2.githubusercontent.com/u/26268279?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dhinar1991", "html_url": "https://github.com/dhinar1991", "followers_url": "https://api.github.com/users/dhinar1991/followers", "following_url": "https://api.github.com/users/dhinar1991/following{/other_user}", "gists_url": "https://api.github.com/users/dhinar1991/gists{/gist_id}", "starred_url": "https://api.github.com/users/dhinar1991/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dhinar1991/subscriptions", "organizations_url": "https://api.github.com/users/dhinar1991/orgs", "repos_url": "https://api.github.com/users/dhinar1991/repos", "events_url": "https://api.github.com/users/dhinar1991/events{/privacy}", "received_events_url": "https://api.github.com/users/dhinar1991/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-01T11:24:21Z", "updated_at": "2018-04-01T11:32:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a>  I have a completely different error that says</p>\n<p>Failed to convert object of type &lt;class 'werkzeug.datastructures.File.Storage&gt; to tensor.</p>\n<blockquote>\n<p>TypeError: Failed to convert object of type (class'werkzeug.datastructures.File.Storage') to tensor. Contents: (Filestorage: u'File.txt' ('text/plain')). Consider casting elements to a supported type</p>\n</blockquote>\n<p>below is the client file with flask framework for tensorflow serving running inside a docker.  <a href=\"url\">https://github.com/tensorflow/tensorflow/issues/18157</a></p>\n<pre><code>\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\nFLAGS = tf.app.flags.FLAGS\n\napp = Flask(__name__)\n\nclass mainSessRunning():\n    \n    def __init__(self):\n        host, port = FLAGS.server.split(':')\n        channel = implementations.insecure_channel(host, int(port))\n        self.stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n        self.request = predict_pb2.PredictRequest()\n        self.request.model_spec.name = 'example_model'\n        self.request.model_spec.signature_name = 'prediction'\n\n    def inference(self, val_x):\n        #temp_data = numpy.random.randn(100, 3).astype(numpy.float32)\n        #temp_data = val_x.astype(np.float32).reshape(-1, 3)\n        data = val_x\n        self.request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\n        result = self.stub.Predict(self.request, 5.0)\n        return result\n\nrun = mainSessRunning()\n\nprint(\"Initialization done. \")\n\n# Define a route for the default URL, which loads the form\n@app.route('/inference', methods=['POST'])\ndef inference():\n    request_data = request.files['file']\n    result = run.inference(request_data)\n    r = json_format.MessageToJson(result)\n    return jsonify({'result':r})\n</code></pre>", "body_text": "@yaroslavvb  @mrry  I have a completely different error that says\nFailed to convert object of type <class 'werkzeug.datastructures.File.Storage> to tensor.\n\nTypeError: Failed to convert object of type (class'werkzeug.datastructures.File.Storage') to tensor. Contents: (Filestorage: u'File.txt' ('text/plain')). Consider casting elements to a supported type\n\nbelow is the client file with flask framework for tensorflow serving running inside a docker.  https://github.com/tensorflow/tensorflow/issues/18157\n\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\nFLAGS = tf.app.flags.FLAGS\n\napp = Flask(__name__)\n\nclass mainSessRunning():\n    \n    def __init__(self):\n        host, port = FLAGS.server.split(':')\n        channel = implementations.insecure_channel(host, int(port))\n        self.stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n\n        self.request = predict_pb2.PredictRequest()\n        self.request.model_spec.name = 'example_model'\n        self.request.model_spec.signature_name = 'prediction'\n\n    def inference(self, val_x):\n        #temp_data = numpy.random.randn(100, 3).astype(numpy.float32)\n        #temp_data = val_x.astype(np.float32).reshape(-1, 3)\n        data = val_x\n        self.request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\n        result = self.stub.Predict(self.request, 5.0)\n        return result\n\nrun = mainSessRunning()\n\nprint(\"Initialization done. \")\n\n# Define a route for the default URL, which loads the form\n@app.route('/inference', methods=['POST'])\ndef inference():\n    request_data = request.files['file']\n    result = run.inference(request_data)\n    r = json_format.MessageToJson(result)\n    return jsonify({'result':r})", "body": "@yaroslavvb  @mrry  I have a completely different error that says \r\n\r\nFailed to convert object of type <class 'werkzeug.datastructures.File.Storage> to tensor.\r\n\r\n>TypeError: Failed to convert object of type (class'werkzeug.datastructures.File.Storage') to tensor. Contents: (Filestorage: u'File.txt' ('text/plain')). Consider casting elements to a supported type\r\n\r\nbelow is the client file with flask framework for tensorflow serving running inside a docker.  [https://github.com/tensorflow/tensorflow/issues/18157](url)\r\n```\r\n\r\ntf.app.flags.DEFINE_string('server', 'localhost:9000', 'PredictionService host:port')\r\nFLAGS = tf.app.flags.FLAGS\r\n\r\napp = Flask(__name__)\r\n\r\nclass mainSessRunning():\r\n    \r\n    def __init__(self):\r\n        host, port = FLAGS.server.split(':')\r\n        channel = implementations.insecure_channel(host, int(port))\r\n        self.stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n\r\n        self.request = predict_pb2.PredictRequest()\r\n        self.request.model_spec.name = 'example_model'\r\n        self.request.model_spec.signature_name = 'prediction'\r\n\r\n    def inference(self, val_x):\r\n        #temp_data = numpy.random.randn(100, 3).astype(numpy.float32)\r\n        #temp_data = val_x.astype(np.float32).reshape(-1, 3)\r\n        data = val_x\r\n        self.request.inputs['input'].CopyFrom(tf.contrib.util.make_tensor_proto(data))\r\n        result = self.stub.Predict(self.request, 5.0)\r\n        return result\r\n\r\nrun = mainSessRunning()\r\n\r\nprint(\"Initialization done. \")\r\n\r\n# Define a route for the default URL, which loads the form\r\n@app.route('/inference', methods=['POST'])\r\ndef inference():\r\n    request_data = request.files['file']\r\n    result = run.inference(request_data)\r\n    r = json_format.MessageToJson(result)\r\n    return jsonify({'result':r})\r\n```"}