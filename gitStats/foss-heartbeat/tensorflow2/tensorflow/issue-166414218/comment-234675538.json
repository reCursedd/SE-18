{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234675538", "html_url": "https://github.com/tensorflow/tensorflow/issues/3396#issuecomment-234675538", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3396", "id": 234675538, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDY3NTUzOA==", "user": {"login": "Anjum48", "id": 13783303, "node_id": "MDQ6VXNlcjEzNzgzMzAz", "avatar_url": "https://avatars1.githubusercontent.com/u/13783303?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Anjum48", "html_url": "https://github.com/Anjum48", "followers_url": "https://api.github.com/users/Anjum48/followers", "following_url": "https://api.github.com/users/Anjum48/following{/other_user}", "gists_url": "https://api.github.com/users/Anjum48/gists{/gist_id}", "starred_url": "https://api.github.com/users/Anjum48/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Anjum48/subscriptions", "organizations_url": "https://api.github.com/users/Anjum48/orgs", "repos_url": "https://api.github.com/users/Anjum48/repos", "events_url": "https://api.github.com/users/Anjum48/events{/privacy}", "received_events_url": "https://api.github.com/users/Anjum48/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-22T22:49:53Z", "updated_at": "2016-07-22T22:49:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4269898\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/terrytangyuan\">@terrytangyuan</a>, here's where I've put my prints</p>\n<pre><code>def my_model(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n\n\ndef my_model2(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    features = tf.reshape(features, [-1, 8*8])  # Flatten 8x8 MNIST digits\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n</code></pre>\n<p>And here's the output (i'm using yesterday's build):</p>\n<pre><code>/usr/bin/python3.5 \"/home/anjum/Python/numpy vs tf reshape.py\"\n(1149, 8, 8) (288, 8, 8) (360, 8, 8)\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp68y2znql\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpc8uirsil\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(64)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(8), Dimension(8)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n             precision    recall  f1-score   support\n\n          0       0.97      0.91      0.94        33\n          1       0.97      1.00      0.98        28\n          2       0.94      0.97      0.96        33\n          3       1.00      0.91      0.95        34\n          4       0.96      0.98      0.97        46\n          5       0.94      0.96      0.95        47\n          6       1.00      0.97      0.99        35\n          7       0.92      0.97      0.94        34\n          8       0.97      0.93      0.95        30\n          9       0.93      0.95      0.94        40\n\navg / total       0.96      0.96      0.96       360\n\n             precision    recall  f1-score   support\n\n          0       0.97      0.94      0.95        33\n          1       0.96      0.96      0.96        28\n          2       0.80      0.85      0.82        33\n          3       0.93      0.82      0.87        34\n          4       0.96      1.00      0.98        46\n          5       0.94      0.94      0.94        47\n          6       0.97      0.97      0.97        35\n          7       1.00      0.97      0.99        34\n          8       0.87      0.90      0.89        30\n          9       0.88      0.90      0.89        40\n\navg / total       0.93      0.93      0.93       360\n\n\nProcess finished with exit code 0\n</code></pre>", "body_text": "Hi @terrytangyuan, here's where I've put my prints\ndef my_model(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n\n\ndef my_model2(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    features = tf.reshape(features, [-1, 8*8])  # Flatten 8x8 MNIST digits\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n\nAnd here's the output (i'm using yesterday's build):\n/usr/bin/python3.5 \"/home/anjum/Python/numpy vs tf reshape.py\"\n(1149, 8, 8) (288, 8, 8) (360, 8, 8)\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp68y2znql\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpc8uirsil\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(64)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(8), Dimension(8)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n             precision    recall  f1-score   support\n\n          0       0.97      0.91      0.94        33\n          1       0.97      1.00      0.98        28\n          2       0.94      0.97      0.96        33\n          3       1.00      0.91      0.95        34\n          4       0.96      0.98      0.97        46\n          5       0.94      0.96      0.95        47\n          6       1.00      0.97      0.99        35\n          7       0.92      0.97      0.94        34\n          8       0.97      0.93      0.95        30\n          9       0.93      0.95      0.94        40\n\navg / total       0.96      0.96      0.96       360\n\n             precision    recall  f1-score   support\n\n          0       0.97      0.94      0.95        33\n          1       0.96      0.96      0.96        28\n          2       0.80      0.85      0.82        33\n          3       0.93      0.82      0.87        34\n          4       0.96      1.00      0.98        46\n          5       0.94      0.94      0.94        47\n          6       0.97      0.97      0.97        35\n          7       1.00      0.97      0.99        34\n          8       0.87      0.90      0.89        30\n          9       0.88      0.90      0.89        40\n\navg / total       0.93      0.93      0.93       360\n\n\nProcess finished with exit code 0", "body": "Hi @terrytangyuan, here's where I've put my prints\n\n```\ndef my_model(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n\n\ndef my_model2(features, target):\n    tf.set_random_seed(1618)\n    target = tf.one_hot(target, 10, 1, 0)  # Create 10x1 one-hot vectors\n    features = tf.reshape(features, [-1, 8*8])  # Flatten 8x8 MNIST digits\n    print(features)\n    features = layers.stack(features, layers.fully_connected, [10, 20, 10])\n    prediction, loss = (tf.contrib.learn.models.logistic_regression_zero_init(features, target))\n    train_op = tf.contrib.layers.optimize_loss(\n        loss, tf.contrib.framework.get_global_step(), optimizer='Adagrad',\n        learning_rate=0.1)\n    return {'class': tf.argmax(prediction, 1), 'prob': prediction}, loss, train_op\n```\n\nAnd here's the output (i'm using yesterday's build):\n\n```\n/usr/bin/python3.5 \"/home/anjum/Python/numpy vs tf reshape.py\"\n(1149, 8, 8) (288, 8, 8) (360, 8, 8)\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmp68y2znql\nWARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpc8uirsil\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(64)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nWARNING:tensorflow:Setting feature info to TensorSignature(dtype=tf.float32, shape=TensorShape([Dimension(None), Dimension(8), Dimension(8)]), is_sparse=False)\nWARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(None)]), is_sparse=False)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\nTensor(\"input:0\", shape=(?, 64), dtype=float32)\nTensor(\"Reshape:0\", shape=(?, 64), dtype=float32)\n             precision    recall  f1-score   support\n\n          0       0.97      0.91      0.94        33\n          1       0.97      1.00      0.98        28\n          2       0.94      0.97      0.96        33\n          3       1.00      0.91      0.95        34\n          4       0.96      0.98      0.97        46\n          5       0.94      0.96      0.95        47\n          6       1.00      0.97      0.99        35\n          7       0.92      0.97      0.94        34\n          8       0.97      0.93      0.95        30\n          9       0.93      0.95      0.94        40\n\navg / total       0.96      0.96      0.96       360\n\n             precision    recall  f1-score   support\n\n          0       0.97      0.94      0.95        33\n          1       0.96      0.96      0.96        28\n          2       0.80      0.85      0.82        33\n          3       0.93      0.82      0.87        34\n          4       0.96      1.00      0.98        46\n          5       0.94      0.94      0.94        47\n          6       0.97      0.97      0.97        35\n          7       1.00      0.97      0.99        34\n          8       0.87      0.90      0.89        30\n          9       0.88      0.90      0.89        40\n\navg / total       0.93      0.93      0.93       360\n\n\nProcess finished with exit code 0\n```\n"}