{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10040", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10040/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10040/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10040/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10040", "id": 230050374, "node_id": "MDU6SXNzdWUyMzAwNTAzNzQ=", "number": 10040, "title": "Virtual memory Leak when using gpu", "user": {"login": "Cospel", "id": 615554, "node_id": "MDQ6VXNlcjYxNTU1NA==", "avatar_url": "https://avatars3.githubusercontent.com/u/615554?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Cospel", "html_url": "https://github.com/Cospel", "followers_url": "https://api.github.com/users/Cospel/followers", "following_url": "https://api.github.com/users/Cospel/following{/other_user}", "gists_url": "https://api.github.com/users/Cospel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Cospel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Cospel/subscriptions", "organizations_url": "https://api.github.com/users/Cospel/orgs", "repos_url": "https://api.github.com/users/Cospel/repos", "events_url": "https://api.github.com/users/Cospel/events{/privacy}", "received_events_url": "https://api.github.com/users/Cospel/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-05-19T18:18:15Z", "updated_at": "2017-06-05T17:53:00Z", "closed_at": "2017-06-05T17:52:59Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I have little problem with virtual memory in tensorflow when running on GPU:</p>\n<p>When I run any network/graph, for example squeezenet:</p>\n<pre><code>args = ...\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\n\nwith tf.Session(config=config) as session:\n  model = SqueezeNet(args, session)\n  raw_input()\n</code></pre>\n<p>I get proper allocation when I look on nvidia-smi:</p>\n<pre><code>+-----------------------------------------------------------------------------+\n|  NVIDIA-SMI 367.48                 Driver Version: 367.48\n|  ....\n|   0  Quadro K620         Off  | 0000:11:00.0     Off |                  N/A |\n| 34%   35C    P8     1W /  30W |     65MiB /  1999MiB |      0%      Default |\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|    0     12128    C   python                                          63MiB |\n+-----------------------------------------------------------------------------+\n</code></pre>\n<p>However the top utility on the process looks very dangerous:</p>\n<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n12128 xuser 20   0 64.696g 461928 162552 S   2.3  4.9   0:11.24 python\n\n</code></pre>\n<p>64 gigs in virtual memory is probably not normal, however when i run it on the CPU (just switching CUDA_VISIBLE_DEVICES to some big number):</p>\n<pre><code>  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n12193 xuser 20   0 4468872 236988  74008 S   0.0  0.5   0:07.66 python\n\n</code></pre>\n<p>Then everything looks fine.</p>\n<p>Do you know where can be the problem?</p>\n<p>I am using tensorflow-gpu 1.1.0 on linux ubuntu16, cuda8, python2.7</p>", "body_text": "Hello,\nI have little problem with virtual memory in tensorflow when running on GPU:\nWhen I run any network/graph, for example squeezenet:\nargs = ...\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth=True\n\nwith tf.Session(config=config) as session:\n  model = SqueezeNet(args, session)\n  raw_input()\n\nI get proper allocation when I look on nvidia-smi:\n+-----------------------------------------------------------------------------+\n|  NVIDIA-SMI 367.48                 Driver Version: 367.48\n|  ....\n|   0  Quadro K620         Off  | 0000:11:00.0     Off |                  N/A |\n| 34%   35C    P8     1W /  30W |     65MiB /  1999MiB |      0%      Default |\n| Processes:                                                       GPU Memory |\n|  GPU       PID  Type  Process name                               Usage      |\n|    0     12128    C   python                                          63MiB |\n+-----------------------------------------------------------------------------+\n\nHowever the top utility on the process looks very dangerous:\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n12128 xuser 20   0 64.696g 461928 162552 S   2.3  4.9   0:11.24 python\n\n\n64 gigs in virtual memory is probably not normal, however when i run it on the CPU (just switching CUDA_VISIBLE_DEVICES to some big number):\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n12193 xuser 20   0 4468872 236988  74008 S   0.0  0.5   0:07.66 python\n\n\nThen everything looks fine.\nDo you know where can be the problem?\nI am using tensorflow-gpu 1.1.0 on linux ubuntu16, cuda8, python2.7", "body": "Hello,\r\n\r\nI have little problem with virtual memory in tensorflow when running on GPU:\r\n\r\nWhen I run any network/graph, for example squeezenet:\r\n\r\n```\r\nargs = ...\r\nconfig = tf.ConfigProto()\r\nconfig.gpu_options.allow_growth=True\r\n\r\nwith tf.Session(config=config) as session:\r\n  model = SqueezeNet(args, session)\r\n  raw_input()\r\n```\r\n\r\nI get proper allocation when I look on nvidia-smi:\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n|  NVIDIA-SMI 367.48                 Driver Version: 367.48\r\n|  ....\r\n|   0  Quadro K620         Off  | 0000:11:00.0     Off |                  N/A |\r\n| 34%   35C    P8     1W /  30W |     65MiB /  1999MiB |      0%      Default |\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID  Type  Process name                               Usage      |\r\n|    0     12128    C   python                                          63MiB |\r\n+-----------------------------------------------------------------------------+\r\n```\r\n\r\nHowever the top utility on the process looks very dangerous:\r\n```\r\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n12128 xuser 20   0 64.696g 461928 162552 S   2.3  4.9   0:11.24 python\r\n\r\n```\r\n\r\n64 gigs in virtual memory is probably not normal, however when i run it on the CPU (just switching CUDA_VISIBLE_DEVICES to some big number):\r\n```\r\n  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\r\n12193 xuser 20   0 4468872 236988  74008 S   0.0  0.5   0:07.66 python\r\n\r\n```\r\nThen everything looks fine.\r\n\r\nDo you know where can be the problem?\r\n\r\nI am using tensorflow-gpu 1.1.0 on linux ubuntu16, cuda8, python2.7"}