{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6058", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6058/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6058/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6058/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6058", "id": 193315008, "node_id": "MDU6SXNzdWUxOTMzMTUwMDg=", "number": 6058, "title": "tf.cond() + tf.dynamic_rnn() gives AttributeError: 'NoneType' object has no attribute 'pivot'", "user": {"login": "wengong-jin", "id": 2518060, "node_id": "MDQ6VXNlcjI1MTgwNjA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2518060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wengong-jin", "html_url": "https://github.com/wengong-jin", "followers_url": "https://api.github.com/users/wengong-jin/followers", "following_url": "https://api.github.com/users/wengong-jin/following{/other_user}", "gists_url": "https://api.github.com/users/wengong-jin/gists{/gist_id}", "starred_url": "https://api.github.com/users/wengong-jin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wengong-jin/subscriptions", "organizations_url": "https://api.github.com/users/wengong-jin/orgs", "repos_url": "https://api.github.com/users/wengong-jin/repos", "events_url": "https://api.github.com/users/wengong-jin/events{/privacy}", "received_events_url": "https://api.github.com/users/wengong-jin/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-12-03T21:35:52Z", "updated_at": "2017-06-16T17:46:35Z", "closed_at": "2017-06-16T17:46:35Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: ubuntu 14.04.5<br>\nInstalled version of CUDA and cuDNN: cuda7.5 + cuDNNv5<br>\nInstalled from pip package: python 2.7+0.11.0rc0</p>\n<h3>Code</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">linear</span>(<span class=\"pl-smi\">input_</span>, <span class=\"pl-smi\">output_size</span>, <span class=\"pl-smi\">scope</span>, <span class=\"pl-smi\">init_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>):\n    shape <span class=\"pl-k\">=</span> input_.get_shape().as_list()\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n        W <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Matrix<span class=\"pl-pds\">\"</span></span>, [shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], output_size], tf.float32, tf.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">/</span> math.sqrt(shape[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>])))\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n        b <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bias<span class=\"pl-pds\">\"</span></span>, [output_size], <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(init_bias))\n    <span class=\"pl-k\">return</span> tf.matmul(input_, W) <span class=\"pl-k\">+</span> b\n\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">64</span>\nhidden_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">300</span>\nvocab_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10000</span>\n\nsession <span class=\"pl-k\">=</span> tf.Session()\nsource <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [batch_size, <span class=\"pl-c1\">10</span>, hidden_size])\nsrc_len <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [batch_size])\nenc_holder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [batch_size, hidden_size])\ndecode_mode <span class=\"pl-k\">=</span> tf.placeholder(tf.bool)\nlabel <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [batch_size])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">encoder</span>():\n    src_cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.GRUCell(hidden_size)\n    src_init <span class=\"pl-k\">=</span> src_cell.zero_state(batch_size, tf.float32)\n    src_hiddens, src_final <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(src_cell, source, <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span>src_init, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>src_len, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>encoder<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-k\">return</span> src_final\n\ndecode_state <span class=\"pl-k\">=</span> tf.cond(decode_mode, <span class=\"pl-k\">lambda</span>:enc_holder, <span class=\"pl-k\">lambda</span>:encoder())\noutputs <span class=\"pl-k\">=</span> linear(decode_state, vocab_size, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>)\nloss <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(outputs, label)\noptimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer().minimize(loss)</pre></div>\n<h2>Error Message:</h2>\n<pre><code>Traceback (most recent call last):\n  File \"issue.py\", line 33, in &lt;module&gt;\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 469, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_grad.py\", line 91, in _MergeGrad\n    return control_flow_ops._SwitchRefOrTensor(grad, grad_ctxt.pivot)\nAttributeError: 'NoneType' object has no attribute 'pivot'\n</code></pre>\n<h2>Note</h2>\n<p>If I change the line</p>\n<div class=\"highlight highlight-source-python\"><pre>decode_state <span class=\"pl-k\">=</span> tf.cond(decode_mode, <span class=\"pl-k\">lambda</span>:enc_holder, <span class=\"pl-k\">lambda</span>:encoder())</pre></div>\n<p>to</p>\n<div class=\"highlight highlight-source-python\"><pre>decode_state <span class=\"pl-k\">=</span> tf.cond(decode_mode, <span class=\"pl-k\">lambda</span>:enc_holder, <span class=\"pl-k\">lambda</span>:enc_holder)</pre></div>\n<p>or</p>\n<div class=\"highlight highlight-source-python\"><pre>decode_state <span class=\"pl-k\">=</span> encoder()</pre></div>\n<p>No error thrown. So I guess there must be something wrong when tf.cond() + tf.dynamic_rnn() is combined.</p>", "body_text": "Environment info\nOperating System: ubuntu 14.04.5\nInstalled version of CUDA and cuDNN: cuda7.5 + cuDNNv5\nInstalled from pip package: python 2.7+0.11.0rc0\nCode\nimport tensorflow as tf\n\ndef linear(input_, output_size, scope, init_bias=0.0):\n    shape = input_.get_shape().as_list()\n    with tf.variable_scope(scope):\n        W = tf.get_variable(\"Matrix\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\n    with tf.variable_scope(scope):\n        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\n    return tf.matmul(input_, W) + b\n\nbatch_size = 64\nhidden_size = 300\nvocab_size = 10000\n\nsession = tf.Session()\nsource = tf.placeholder(tf.float32, [batch_size, 10, hidden_size])\nsrc_len = tf.placeholder(tf.int32, [batch_size])\nenc_holder = tf.placeholder(tf.float32, [batch_size, hidden_size])\ndecode_mode = tf.placeholder(tf.bool)\nlabel = tf.placeholder(tf.int32, [batch_size])\n\ndef encoder():\n    src_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\n    src_init = src_cell.zero_state(batch_size, tf.float32)\n    src_hiddens, src_final = tf.nn.dynamic_rnn(src_cell, source, initial_state=src_init, sequence_length=src_len, scope=\"encoder\")\n    return src_final\n\ndecode_state = tf.cond(decode_mode, lambda:enc_holder, lambda:encoder())\noutputs = linear(decode_state, vocab_size, \"output\")\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(outputs, label)\noptimizer = tf.train.AdamOptimizer().minimize(loss)\nError Message:\nTraceback (most recent call last):\n  File \"issue.py\", line 33, in <module>\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\n    grad_loss=grad_loss)\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 469, in gradients\n    in_grads = _AsList(grad_fn(op, *out_grads))\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_grad.py\", line 91, in _MergeGrad\n    return control_flow_ops._SwitchRefOrTensor(grad, grad_ctxt.pivot)\nAttributeError: 'NoneType' object has no attribute 'pivot'\n\nNote\nIf I change the line\ndecode_state = tf.cond(decode_mode, lambda:enc_holder, lambda:encoder())\nto\ndecode_state = tf.cond(decode_mode, lambda:enc_holder, lambda:enc_holder)\nor\ndecode_state = encoder()\nNo error thrown. So I guess there must be something wrong when tf.cond() + tf.dynamic_rnn() is combined.", "body": "### Environment info\r\nOperating System: ubuntu 14.04.5\r\nInstalled version of CUDA and cuDNN: cuda7.5 + cuDNNv5\r\nInstalled from pip package: python 2.7+0.11.0rc0\r\n\r\n### Code\r\n```python\r\nimport tensorflow as tf\r\n\r\ndef linear(input_, output_size, scope, init_bias=0.0):\r\n    shape = input_.get_shape().as_list()\r\n    with tf.variable_scope(scope):\r\n        W = tf.get_variable(\"Matrix\", [shape[-1], output_size], tf.float32, tf.random_normal_initializer(stddev=1.0 / math.sqrt(shape[-1])))\r\n    with tf.variable_scope(scope):\r\n        b = tf.get_variable(\"bias\", [output_size], initializer=tf.constant_initializer(init_bias))\r\n    return tf.matmul(input_, W) + b\r\n\r\nbatch_size = 64\r\nhidden_size = 300\r\nvocab_size = 10000\r\n\r\nsession = tf.Session()\r\nsource = tf.placeholder(tf.float32, [batch_size, 10, hidden_size])\r\nsrc_len = tf.placeholder(tf.int32, [batch_size])\r\nenc_holder = tf.placeholder(tf.float32, [batch_size, hidden_size])\r\ndecode_mode = tf.placeholder(tf.bool)\r\nlabel = tf.placeholder(tf.int32, [batch_size])\r\n\r\ndef encoder():\r\n    src_cell = tf.nn.rnn_cell.GRUCell(hidden_size)\r\n    src_init = src_cell.zero_state(batch_size, tf.float32)\r\n    src_hiddens, src_final = tf.nn.dynamic_rnn(src_cell, source, initial_state=src_init, sequence_length=src_len, scope=\"encoder\")\r\n    return src_final\r\n\r\ndecode_state = tf.cond(decode_mode, lambda:enc_holder, lambda:encoder())\r\noutputs = linear(decode_state, vocab_size, \"output\")\r\nloss = tf.nn.sparse_softmax_cross_entropy_with_logits(outputs, label)\r\noptimizer = tf.train.AdamOptimizer().minimize(loss)\r\n```\r\n\r\n## Error Message:\r\n```\r\nTraceback (most recent call last):\r\n  File \"issue.py\", line 33, in <module>\r\n    optimizer = tf.train.AdamOptimizer().minimize(loss)\r\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 196, in minimize\r\n    grad_loss=grad_loss)\r\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 253, in compute_gradients\r\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\r\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 469, in gradients\r\n    in_grads = _AsList(grad_fn(op, *out_grads))\r\n  File \"~/anaconda2/lib/python2.7/site-packages/tensorflow/python/ops/control_flow_grad.py\", line 91, in _MergeGrad\r\n    return control_flow_ops._SwitchRefOrTensor(grad, grad_ctxt.pivot)\r\nAttributeError: 'NoneType' object has no attribute 'pivot'\r\n```\r\n## Note\r\nIf I change the line \r\n```python\r\ndecode_state = tf.cond(decode_mode, lambda:enc_holder, lambda:encoder())\r\n```\r\nto\r\n```python\r\ndecode_state = tf.cond(decode_mode, lambda:enc_holder, lambda:enc_holder)\r\n```\r\nor\r\n```python\r\ndecode_state = encoder()\r\n```\r\nNo error thrown. So I guess there must be something wrong when tf.cond() + tf.dynamic_rnn() is combined. "}