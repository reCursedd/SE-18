{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276062967", "html_url": "https://github.com/tensorflow/tensorflow/issues/2126#issuecomment-276062967", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2126", "id": 276062967, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjA2Mjk2Nw==", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-30T13:31:37Z", "updated_at": "2017-01-30T13:32:05Z", "author_association": "NONE", "body_html": "<p>Since <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"178861823\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4552\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4552/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4552\">#4552</a> was closed and referenced here, I would just like to point out that there's a separate issue having nothing to do with distributed computing, and that is to prevent TF from scheduling too many concurrent operations at once such that it runs out of memory (on a single GPU). I have run into this problem many times where every single op can fit onto memory but because TF is overly aggressive about concurrency I end up not being able to run the model. This isn't about efficiency, but simply about avoiding unnecessary OOM errors.</p>", "body_text": "Since #4552 was closed and referenced here, I would just like to point out that there's a separate issue having nothing to do with distributed computing, and that is to prevent TF from scheduling too many concurrent operations at once such that it runs out of memory (on a single GPU). I have run into this problem many times where every single op can fit onto memory but because TF is overly aggressive about concurrency I end up not being able to run the model. This isn't about efficiency, but simply about avoiding unnecessary OOM errors.", "body": "Since #4552 was closed and referenced here, I would just like to point out that there's a separate issue having nothing to do with distributed computing, and that is to prevent TF from scheduling too many concurrent operations at once such that it runs out of memory (on a single GPU). I have run into this problem many times where every single op can fit onto memory but because TF is overly aggressive about concurrency I end up not being able to run the model. This isn't about efficiency, but simply about avoiding unnecessary OOM errors."}