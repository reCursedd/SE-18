{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313022206", "html_url": "https://github.com/tensorflow/tensorflow/issues/2126#issuecomment-313022206", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2126", "id": 313022206, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzAyMjIwNg==", "user": {"login": "SanZhiYuan", "id": 12406695, "node_id": "MDQ6VXNlcjEyNDA2Njk1", "avatar_url": "https://avatars1.githubusercontent.com/u/12406695?v=4", "gravatar_id": "", "url": "https://api.github.com/users/SanZhiYuan", "html_url": "https://github.com/SanZhiYuan", "followers_url": "https://api.github.com/users/SanZhiYuan/followers", "following_url": "https://api.github.com/users/SanZhiYuan/following{/other_user}", "gists_url": "https://api.github.com/users/SanZhiYuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/SanZhiYuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/SanZhiYuan/subscriptions", "organizations_url": "https://api.github.com/users/SanZhiYuan/orgs", "repos_url": "https://api.github.com/users/SanZhiYuan/repos", "events_url": "https://api.github.com/users/SanZhiYuan/events{/privacy}", "received_events_url": "https://api.github.com/users/SanZhiYuan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-05T07:18:22Z", "updated_at": "2017-07-05T07:18:22Z", "author_association": "NONE", "body_html": "<p>hi, why not take a little step forward,  like users should explicitly declare where the computation should happens (cpu, gpu, ps), but just with a 'virtual' device id, then tf automatically schedule the sub graphs to any free devices, or a preferred device list would be even better for migration between distrubuted, multi-gpus, single gpu and cpu only. And I think this would be much easier for us to try those training tf models in our own environment.<br>\nPS, I really don't want to try the distributed tf after I checked the tutorials, -,-</p>", "body_text": "hi, why not take a little step forward,  like users should explicitly declare where the computation should happens (cpu, gpu, ps), but just with a 'virtual' device id, then tf automatically schedule the sub graphs to any free devices, or a preferred device list would be even better for migration between distrubuted, multi-gpus, single gpu and cpu only. And I think this would be much easier for us to try those training tf models in our own environment.\nPS, I really don't want to try the distributed tf after I checked the tutorials, -,-", "body": "hi, why not take a little step forward,  like users should explicitly declare where the computation should happens (cpu, gpu, ps), but just with a 'virtual' device id, then tf automatically schedule the sub graphs to any free devices, or a preferred device list would be even better for migration between distrubuted, multi-gpus, single gpu and cpu only. And I think this would be much easier for us to try those training tf models in our own environment.\r\nPS, I really don't want to try the distributed tf after I checked the tutorials, -,-"}