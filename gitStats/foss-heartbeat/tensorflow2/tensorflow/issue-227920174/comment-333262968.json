{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333262968", "html_url": "https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-333262968", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9832", "id": 333262968, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzI2Mjk2OA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-29T23:39:43Z", "updated_at": "2017-09-29T23:39:43Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">Added documentation and an example to `BeamSearchDecoder`,\n`AttentionWrapper.__init__`, and `AttentionWrapper.zero_state`.  Should\nshow up in a day or two.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Fri, Sep 29, 2017 at 1:35 PM, Sumeet Singh ***@***.***&gt; wrote:\n This is pretty poor documentation on part of BeamSearchDecoder. Is it\n really that hard to add one line of documentation (this is as of Tensorflow\n v1.3)?\n It is very important to use tf.contrib.seq2seq.tile_batch and not tf.tile\n - but one wouldn't know that unless one stumbled upon this page. I used\n tf.tile about 2 months back in response to a cryptic error about not having\n created a zero state of size = batch_size*beam_width. And I had been\n struggling to get my model accuracy to become reasonable. Turns out it was\n because I was using tf.tile to tile my init-state-model. In my case the\n init-state is itself a neural-network that is conditioned by a\n sample-specific context. Therefore the initial-state is different for each\n input-sequence - not a straight zero-state. Hence it is very important in\n my case to line-up the init-state with the samples.\n\n BeamSearchDecoder folks - please document this.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"227920174\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9832\" href=\"https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-333232331\">#9832 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim6AlG-lmxyb9FlL-6BFIHaLg0QE8ks5snVSCgaJpZM4NXsUR\">https://github.com/notifications/unsubscribe-auth/ABtim6AlG-lmxyb9FlL-6BFIHaLg0QE8ks5snVSCgaJpZM4NXsUR</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Added documentation and an example to `BeamSearchDecoder`,\n`AttentionWrapper.__init__`, and `AttentionWrapper.zero_state`.  Should\nshow up in a day or two.\n\u2026\nOn Fri, Sep 29, 2017 at 1:35 PM, Sumeet Singh ***@***.***> wrote:\n This is pretty poor documentation on part of BeamSearchDecoder. Is it\n really that hard to add one line of documentation (this is as of Tensorflow\n v1.3)?\n It is very important to use tf.contrib.seq2seq.tile_batch and not tf.tile\n - but one wouldn't know that unless one stumbled upon this page. I used\n tf.tile about 2 months back in response to a cryptic error about not having\n created a zero state of size = batch_size*beam_width. And I had been\n struggling to get my model accuracy to become reasonable. Turns out it was\n because I was using tf.tile to tile my init-state-model. In my case the\n init-state is itself a neural-network that is conditioned by a\n sample-specific context. Therefore the initial-state is different for each\n input-sequence - not a straight zero-state. Hence it is very important in\n my case to line-up the init-state with the samples.\n\n BeamSearchDecoder folks - please document this.\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#9832 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtim6AlG-lmxyb9FlL-6BFIHaLg0QE8ks5snVSCgaJpZM4NXsUR>\n .", "body": "Added documentation and an example to `BeamSearchDecoder`,\n`AttentionWrapper.__init__`, and `AttentionWrapper.zero_state`.  Should\nshow up in a day or two.\n\nOn Fri, Sep 29, 2017 at 1:35 PM, Sumeet Singh <notifications@github.com>\nwrote:\n\n> This is pretty poor documentation on part of BeamSearchDecoder. Is it\n> really that hard to add one line of documentation (this is as of Tensorflow\n> v1.3)?\n> It is very important to use tf.contrib.seq2seq.tile_batch and not tf.tile\n> - but one wouldn't know that unless one stumbled upon this page. I used\n> tf.tile about 2 months back in response to a cryptic error about not having\n> created a zero state of size = batch_size*beam_width. And I had been\n> struggling to get my model accuracy to become reasonable. Turns out it was\n> because I was using tf.tile to tile my init-state-model. In my case the\n> init-state is itself a neural-network that is conditioned by a\n> sample-specific context. Therefore the initial-state is different for each\n> input-sequence - not a straight zero-state. Hence it is very important in\n> my case to line-up the init-state with the samples.\n>\n> BeamSearchDecoder folks - please document this.\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/9832#issuecomment-333232331>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtim6AlG-lmxyb9FlL-6BFIHaLg0QE8ks5snVSCgaJpZM4NXsUR>\n> .\n>\n"}