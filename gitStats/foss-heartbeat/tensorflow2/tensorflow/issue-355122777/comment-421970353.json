{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421970353", "html_url": "https://github.com/tensorflow/tensorflow/pull/21945#issuecomment-421970353", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21945", "id": 421970353, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTk3MDM1Mw==", "user": {"login": "efagerho", "id": 2458944, "node_id": "MDQ6VXNlcjI0NTg5NDQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/2458944?v=4", "gravatar_id": "", "url": "https://api.github.com/users/efagerho", "html_url": "https://github.com/efagerho", "followers_url": "https://api.github.com/users/efagerho/followers", "following_url": "https://api.github.com/users/efagerho/following{/other_user}", "gists_url": "https://api.github.com/users/efagerho/gists{/gist_id}", "starred_url": "https://api.github.com/users/efagerho/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/efagerho/subscriptions", "organizations_url": "https://api.github.com/users/efagerho/orgs", "repos_url": "https://api.github.com/users/efagerho/repos", "events_url": "https://api.github.com/users/efagerho/events{/privacy}", "received_events_url": "https://api.github.com/users/efagerho/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-17T11:05:50Z", "updated_at": "2018-09-17T11:19:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It looks like there are a few tests where this patch causes a test to fail. They all raise an exception in the same place:</p>\n<p>tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1924, in _centered_bias_step</p>\n<p>What's strange is that the code that builds the graph doesn't fail when tf.transpose is called, i.e. the graph node is created just as expected, so its input parameters seem validated. Having gone through every such call with some good old print debugging, the parameters don't look like anything strange. The exception in the test is raised when the optimizer is creating the backprop graph for the bias computation and at this point it looks like some variable and it's grad have different shapes:</p>\n<pre><code>File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn_test.py\", line 1562, in testEnableCenteredBias\n    regressor.fit(input_fn=_input_fn, steps=5)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 525, in fit\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1042, in _train_model\n    model_fn_ops = self._get_train_ops(features, labels)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1265, in _get_train_ops\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1228, in _call_model_fn\n    model_fn_results = self._model_fn(features, labels, **kwargs)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 214, in _dnn_model_fn\n    logits=logits)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 758, in create_model_fn_ops\n    enable_centered_bias=self._enable_centered_bias)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 669, in _create_model_fn_ops\n    batch_size, loss_fn, weight_tensor)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1940, in _train_op\n    weights=weights)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1924, in _centered_bias_step\n    centered_bias_loss, var_list=(centered_bias,), name=name)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 410, in minimize\n    name=name)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 607, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 115, in update_op\n    update_op = optimizer._apply_dense(g, self._v)  # pylint: disable=protected-access\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/adagrad.py\", line 103, in _apply_dense\n    use_locking=self._use_locking)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/gen_training_ops.py\", line 174, in apply_adagrad\n    use_locking=use_locking, update_slots=update_slots, name=name)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): var and grad do not have the same shape[1] []\n         [[node dnn/regression_head/centered_bias_step/update_dnn/regression_head/centered_bias_weight/ApplyAdagrad (defined at /home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py:1924)  = ApplyAdagrad[T=DT_FLOAT, _class=[\"loc:@dnn/r...plyAdagrad\"], update_slots=true, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/regression_head/centered_bias_weight, dnn/regression_head/dnn/regression_head/centered_bias_weight/Adagrad, dnn/regression_head/centered_bias_step/learning_rate, dnn/regression_head/gradients/dnn/regression_head/centered_bias_step/Tile_grad/Sum)]]\n</code></pre>\n<p>I don't quite understand how it would be possible to have a valid forward graph and then have the optimizer end up with different sizes for variables during backprop. I'll look closer into this later this week.</p>", "body_text": "It looks like there are a few tests where this patch causes a test to fail. They all raise an exception in the same place:\ntensorflow/contrib/learn/python/learn/estimators/head.py\", line 1924, in _centered_bias_step\nWhat's strange is that the code that builds the graph doesn't fail when tf.transpose is called, i.e. the graph node is created just as expected, so its input parameters seem validated. Having gone through every such call with some good old print debugging, the parameters don't look like anything strange. The exception in the test is raised when the optimizer is creating the backprop graph for the bias computation and at this point it looks like some variable and it's grad have different shapes:\nFile \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn_test.py\", line 1562, in testEnableCenteredBias\n    regressor.fit(input_fn=_input_fn, steps=5)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 525, in fit\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1042, in _train_model\n    model_fn_ops = self._get_train_ops(features, labels)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1265, in _get_train_ops\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1228, in _call_model_fn\n    model_fn_results = self._model_fn(features, labels, **kwargs)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 214, in _dnn_model_fn\n    logits=logits)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 758, in create_model_fn_ops\n    enable_centered_bias=self._enable_centered_bias)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 669, in _create_model_fn_ops\n    batch_size, loss_fn, weight_tensor)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1940, in _train_op\n    weights=weights)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1924, in _centered_bias_step\n    centered_bias_loss, var_list=(centered_bias,), name=name)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 410, in minimize\n    name=name)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 607, in apply_gradients\n    update_ops.append(processor.update_op(self, grad))\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 115, in update_op\n    update_op = optimizer._apply_dense(g, self._v)  # pylint: disable=protected-access\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/adagrad.py\", line 103, in _apply_dense\n    use_locking=self._use_locking)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/gen_training_ops.py\", line 174, in apply_adagrad\n    use_locking=use_locking, update_slots=update_slots, name=name)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 3274, in create_op\n    op_def=op_def)\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): var and grad do not have the same shape[1] []\n         [[node dnn/regression_head/centered_bias_step/update_dnn/regression_head/centered_bias_weight/ApplyAdagrad (defined at /home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py:1924)  = ApplyAdagrad[T=DT_FLOAT, _class=[\"loc:@dnn/r...plyAdagrad\"], update_slots=true, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/regression_head/centered_bias_weight, dnn/regression_head/dnn/regression_head/centered_bias_weight/Adagrad, dnn/regression_head/centered_bias_step/learning_rate, dnn/regression_head/gradients/dnn/regression_head/centered_bias_step/Tile_grad/Sum)]]\n\nI don't quite understand how it would be possible to have a valid forward graph and then have the optimizer end up with different sizes for variables during backprop. I'll look closer into this later this week.", "body": "It looks like there are a few tests where this patch causes a test to fail. They all raise an exception in the same place:\r\n\r\ntensorflow/contrib/learn/python/learn/estimators/head.py\", line 1924, in _centered_bias_step\r\n\r\nWhat's strange is that the code that builds the graph doesn't fail when tf.transpose is called, i.e. the graph node is created just as expected, so its input parameters seem validated. Having gone through every such call with some good old print debugging, the parameters don't look like anything strange. The exception in the test is raised when the optimizer is creating the backprop graph for the bias computation and at this point it looks like some variable and it's grad have different shapes:\r\n\r\n```\r\nFile \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn_test.py\", line 1562, in testEnableCenteredBias\r\n    regressor.fit(input_fn=_input_fn, steps=5)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 525, in fit\r\n    loss = self._train_model(input_fn=input_fn, hooks=hooks)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1042, in _train_model\r\n    model_fn_ops = self._get_train_ops(features, labels)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1265, in _get_train_ops\r\n    return self._call_model_fn(features, labels, model_fn_lib.ModeKeys.TRAIN)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/estimator.py\", line 1228, in _call_model_fn\r\n    model_fn_results = self._model_fn(features, labels, **kwargs)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/dnn.py\", line 214, in _dnn_model_fn\r\n    logits=logits)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 758, in create_model_fn_ops\r\n    enable_centered_bias=self._enable_centered_bias)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 669, in _create_model_fn_ops\r\n    batch_size, loss_fn, weight_tensor)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1940, in _train_op\r\n    weights=weights)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py\", line 1924, in _centered_bias_step\r\n    centered_bias_loss, var_list=(centered_bias,), name=name)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 410, in minimize\r\n    name=name)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 607, in apply_gradients\r\n    update_ops.append(processor.update_op(self, grad))\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/optimizer.py\", line 115, in update_op\r\n    update_op = optimizer._apply_dense(g, self._v)  # pylint: disable=protected-access\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/adagrad.py\", line 103, in _apply_dense\r\n    use_locking=self._use_locking)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/training/gen_training_ops.py\", line 174, in apply_adagrad\r\n    use_locking=use_locking, update_slots=update_slots, name=name)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 3274, in create_op\r\n    op_def=op_def)\r\n  File \"/home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/python/framework/ops.py\", line 1770, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): var and grad do not have the same shape[1] []\r\n         [[node dnn/regression_head/centered_bias_step/update_dnn/regression_head/centered_bias_weight/ApplyAdagrad (defined at /home/efagerholm/.cache/bazel/_bazel_efagerholm/3bd66cc293ffd5c1e1b6be4e441d09f4/execroot/org_tensorflow/bazel-out/k8-opt/bin/tensorflow/contrib/learn/dnn_test.runfiles/org_tensorflow/tensorflow/contrib/learn/python/learn/estimators/head.py:1924)  = ApplyAdagrad[T=DT_FLOAT, _class=[\"loc:@dnn/r...plyAdagrad\"], update_slots=true, use_locking=false, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](dnn/regression_head/centered_bias_weight, dnn/regression_head/dnn/regression_head/centered_bias_weight/Adagrad, dnn/regression_head/centered_bias_step/learning_rate, dnn/regression_head/gradients/dnn/regression_head/centered_bias_step/Tile_grad/Sum)]]\r\n```\r\n\r\nI don't quite understand how it would be possible to have a valid forward graph and then have the optimizer end up with different sizes for variables during backprop. I'll look closer into this later this week."}