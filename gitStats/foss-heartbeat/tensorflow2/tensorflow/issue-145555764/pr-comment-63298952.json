{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/63298952", "pull_request_review_id": null, "id": 63298952, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYzMjk4OTUy", "diff_hunk": "@@ -0,0 +1,80 @@\n+/* Copyright 2015 Google Inc. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#include \"tensorflow/core/framework/op.h\"\n+\n+namespace tensorflow {\n+\n+REGISTER_OP(\"BatchNormalizeTraining\")\n+    .Input(\"input: T\")\n+    .Input(\"scale: T\")\n+    .Input(\"bias: T\")\n+    .Input(\"running_mean: Ref(T)\")\n+    .Input(\"running_inv_var: Ref(T)\")\n+    .Output(\"out: T\")\n+    .Output(\"save_mean: T\")\n+    .Output(\"save_inv_var: T\")\n+    .Attr(\"T: {float}\")\n+    .Attr(\"epsilon: float\")\n+    .Attr(\"exponential_average_factor: float\")", "path": "tensorflow/core/ops/batchnorm_training_op.cc", "position": null, "original_position": 31, "commit_id": "458ad677e40b083992f338f2cdd2836e06b99896", "original_commit_id": "dd8c1650a8e8e59d5493cdfed7c756b26ef04b9f", "user": {"login": "lukemetz", "id": 1248454, "node_id": "MDQ6VXNlcjEyNDg0NTQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1248454?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukemetz", "html_url": "https://github.com/lukemetz", "followers_url": "https://api.github.com/users/lukemetz/followers", "following_url": "https://api.github.com/users/lukemetz/following{/other_user}", "gists_url": "https://api.github.com/users/lukemetz/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukemetz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukemetz/subscriptions", "organizations_url": "https://api.github.com/users/lukemetz/orgs", "repos_url": "https://api.github.com/users/lukemetz/repos", "events_url": "https://api.github.com/users/lukemetz/events{/privacy}", "received_events_url": "https://api.github.com/users/lukemetz/received_events", "type": "User", "site_admin": false}, "body": "When shifting to T or double, somewhere in TF's code generation fails. Does this make sense? An alternative could be to make epsilon a tensor input and not an attr but that seems a little extreme.\n\n```\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/gen_batchnorm_training_op_py_wrappers_cc BatchNormTrainingGrad 0 > bazel-out/local_linux-opt/genfiles/tensorflow/python/gen_batchnorm_training_op.py').\nF tensorflow/core/framework/op.cc:132] Check failed: ::tensorflow::Status::OK() == (RegisterAlreadyLocked(op_def)) (OK vs. Invalid argument: Unrecognized type '' in attr 'epsilon'; in OpDef: name: \"BatchNormTraining\" input_arg { name: \"input\" description: \"A 4D input Tensor.\" type_attr: \"T\" } input_arg { name: \"scale\" description: \"A 1D Tensor with size equal to the number of channels.\\nThis is the learned scale value multiplied post normalization.\\nAlso known as gamma.\" type_attr: \"T\" } input_arg { name: \"bias\" description: \"A 1D Tensor with size equal to the number of channels.\\nThis is the learned bias value added post normalization.\\nAlso known as beta.\" type_attr: \"T\" } output_arg { name: \"out\" description: \"A 4D output Tensor. The input after applying batch normalization.\" type_attr: \"T\" } output_arg { name: \"save_mean\" description: \"A 1D Tensor. Computed means. To be used in the backward pass.\" type_attr: \"T\" } output_arg { name: \"save_inv_var\" description: \"A 1D Tensor. Computed inverse variance. To be used in the backward pass.\" type_attr: \"T\" } attr { name: \"T\" type: \"type\" allowed_values { list { type: DT_FLOAT type: DT_DOUBLE } } } attr { name: \"epsilon\" description: \"float\" } attr { name: \"data_format\" type: \"string\" } summary: \"Perform batch norm using batch statistics.\")Attempting to register: Op<name=BatchNormTraining; signature=input:T, scale:T, bias:T -> out:T, save_mean:T, save_inv_var:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=epsilon:; attr=data_format:string>\n```\n", "created_at": "2016-05-15T22:20:26Z", "updated_at": "2016-05-30T21:43:27Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/1759#discussion_r63298952", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1759", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/63298952"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/1759#discussion_r63298952"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/1759"}}, "body_html": "<p>When shifting to T or double, somewhere in TF's code generation fails. Does this make sense? An alternative could be to make epsilon a tensor input and not an attr but that seems a little extreme.</p>\n<pre><code>  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/gen_batchnorm_training_op_py_wrappers_cc BatchNormTrainingGrad 0 &gt; bazel-out/local_linux-opt/genfiles/tensorflow/python/gen_batchnorm_training_op.py').\nF tensorflow/core/framework/op.cc:132] Check failed: ::tensorflow::Status::OK() == (RegisterAlreadyLocked(op_def)) (OK vs. Invalid argument: Unrecognized type '' in attr 'epsilon'; in OpDef: name: \"BatchNormTraining\" input_arg { name: \"input\" description: \"A 4D input Tensor.\" type_attr: \"T\" } input_arg { name: \"scale\" description: \"A 1D Tensor with size equal to the number of channels.\\nThis is the learned scale value multiplied post normalization.\\nAlso known as gamma.\" type_attr: \"T\" } input_arg { name: \"bias\" description: \"A 1D Tensor with size equal to the number of channels.\\nThis is the learned bias value added post normalization.\\nAlso known as beta.\" type_attr: \"T\" } output_arg { name: \"out\" description: \"A 4D output Tensor. The input after applying batch normalization.\" type_attr: \"T\" } output_arg { name: \"save_mean\" description: \"A 1D Tensor. Computed means. To be used in the backward pass.\" type_attr: \"T\" } output_arg { name: \"save_inv_var\" description: \"A 1D Tensor. Computed inverse variance. To be used in the backward pass.\" type_attr: \"T\" } attr { name: \"T\" type: \"type\" allowed_values { list { type: DT_FLOAT type: DT_DOUBLE } } } attr { name: \"epsilon\" description: \"float\" } attr { name: \"data_format\" type: \"string\" } summary: \"Perform batch norm using batch statistics.\")Attempting to register: Op&lt;name=BatchNormTraining; signature=input:T, scale:T, bias:T -&gt; out:T, save_mean:T, save_inv_var:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=epsilon:; attr=data_format:string&gt;\n</code></pre>", "body_text": "When shifting to T or double, somewhere in TF's code generation fails. Does this make sense? An alternative could be to make epsilon a tensor input and not an attr but that seems a little extreme.\n  /bin/bash -c 'source external/bazel_tools/tools/genrule/genrule-setup.sh; bazel-out/host/bin/tensorflow/python/gen_batchnorm_training_op_py_wrappers_cc BatchNormTrainingGrad 0 > bazel-out/local_linux-opt/genfiles/tensorflow/python/gen_batchnorm_training_op.py').\nF tensorflow/core/framework/op.cc:132] Check failed: ::tensorflow::Status::OK() == (RegisterAlreadyLocked(op_def)) (OK vs. Invalid argument: Unrecognized type '' in attr 'epsilon'; in OpDef: name: \"BatchNormTraining\" input_arg { name: \"input\" description: \"A 4D input Tensor.\" type_attr: \"T\" } input_arg { name: \"scale\" description: \"A 1D Tensor with size equal to the number of channels.\\nThis is the learned scale value multiplied post normalization.\\nAlso known as gamma.\" type_attr: \"T\" } input_arg { name: \"bias\" description: \"A 1D Tensor with size equal to the number of channels.\\nThis is the learned bias value added post normalization.\\nAlso known as beta.\" type_attr: \"T\" } output_arg { name: \"out\" description: \"A 4D output Tensor. The input after applying batch normalization.\" type_attr: \"T\" } output_arg { name: \"save_mean\" description: \"A 1D Tensor. Computed means. To be used in the backward pass.\" type_attr: \"T\" } output_arg { name: \"save_inv_var\" description: \"A 1D Tensor. Computed inverse variance. To be used in the backward pass.\" type_attr: \"T\" } attr { name: \"T\" type: \"type\" allowed_values { list { type: DT_FLOAT type: DT_DOUBLE } } } attr { name: \"epsilon\" description: \"float\" } attr { name: \"data_format\" type: \"string\" } summary: \"Perform batch norm using batch statistics.\")Attempting to register: Op<name=BatchNormTraining; signature=input:T, scale:T, bias:T -> out:T, save_mean:T, save_inv_var:T; attr=T:type,allowed=[DT_FLOAT, DT_DOUBLE]; attr=epsilon:; attr=data_format:string>"}