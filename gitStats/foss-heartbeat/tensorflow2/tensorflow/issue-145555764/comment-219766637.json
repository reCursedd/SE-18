{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219766637", "html_url": "https://github.com/tensorflow/tensorflow/pull/1759#issuecomment-219766637", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1759", "id": 219766637, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTc2NjYzNw==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-17T16:03:44Z", "updated_at": "2016-05-17T16:03:44Z", "author_association": "NONE", "body_html": "<p>Also, I had to manually normalize the input using the returned <code>mean</code> and <code>inv_var</code> to make the back pass work. Simply using <code>batch_norm_training</code> will lead to the correct <code>output</code> but the gradients aren't actually back propagated past the BN layer. Can anyone confirm this?</p>", "body_text": "Also, I had to manually normalize the input using the returned mean and inv_var to make the back pass work. Simply using batch_norm_training will lead to the correct output but the gradients aren't actually back propagated past the BN layer. Can anyone confirm this?", "body": "Also, I had to manually normalize the input using the returned `mean` and `inv_var` to make the back pass work. Simply using `batch_norm_training` will lead to the correct `output` but the gradients aren't actually back propagated past the BN layer. Can anyone confirm this?\n"}