{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/219881873", "html_url": "https://github.com/tensorflow/tensorflow/pull/1759#issuecomment-219881873", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1759", "id": 219881873, "node_id": "MDEyOklzc3VlQ29tbWVudDIxOTg4MTg3Mw==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-17T23:14:56Z", "updated_at": "2016-05-17T23:14:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1248454\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukemetz\">@lukemetz</a>, thank you very much for the code improvement! It is awesome to hear about the performance gain.</p>\n<p>There are a few comments at the bottom. Some of them are regarding the interface of this op. Because TensorFlow promises backward compatibility on TF core, it is difficult to change the op interface once the code is in TF core.</p>\n<p>On the other hand, we would like to merge in this code as soon as possible so more people can benefit and help improve. How about checking the same code into TF contrib for now? I think we would be able to merge with very little modifications. And after we iron out the details, the TF team will help integrate the change back into the TF core.</p>\n<p>Detailed comments:</p>\n<ol>\n<li>Lack of CPU implementation.</li>\n<li>We do not want a separate training and inference op. The same op should check whether \"save_mean\" and \"saved_inv_var\" are actually used before picking the \"training\" and \"inference\" version.</li>\n<li>We don't want stream-executor to allocate any memory. TensorFlow has a much more efficient memory management system. So this should be done through ScratchAllocator.</li>\n<li>Ongoing discussion whether to expose the running mean and average, and measure the performance impact.</li>\n<li>We need to test more common shapes to verify the results.</li>\n</ol>", "body_text": "@lukemetz, thank you very much for the code improvement! It is awesome to hear about the performance gain.\nThere are a few comments at the bottom. Some of them are regarding the interface of this op. Because TensorFlow promises backward compatibility on TF core, it is difficult to change the op interface once the code is in TF core.\nOn the other hand, we would like to merge in this code as soon as possible so more people can benefit and help improve. How about checking the same code into TF contrib for now? I think we would be able to merge with very little modifications. And after we iron out the details, the TF team will help integrate the change back into the TF core.\nDetailed comments:\n\nLack of CPU implementation.\nWe do not want a separate training and inference op. The same op should check whether \"save_mean\" and \"saved_inv_var\" are actually used before picking the \"training\" and \"inference\" version.\nWe don't want stream-executor to allocate any memory. TensorFlow has a much more efficient memory management system. So this should be done through ScratchAllocator.\nOngoing discussion whether to expose the running mean and average, and measure the performance impact.\nWe need to test more common shapes to verify the results.", "body": "@lukemetz, thank you very much for the code improvement! It is awesome to hear about the performance gain. \n\nThere are a few comments at the bottom. Some of them are regarding the interface of this op. Because TensorFlow promises backward compatibility on TF core, it is difficult to change the op interface once the code is in TF core. \n\nOn the other hand, we would like to merge in this code as soon as possible so more people can benefit and help improve. How about checking the same code into TF contrib for now? I think we would be able to merge with very little modifications. And after we iron out the details, the TF team will help integrate the change back into the TF core. \n\nDetailed comments: \n1. Lack of CPU implementation. \n2. We do not want a separate training and inference op. The same op should check whether \"save_mean\" and \"saved_inv_var\" are actually used before picking the \"training\" and \"inference\" version. \n3. We don't want stream-executor to allocate any memory. TensorFlow has a much more efficient memory management system. So this should be done through ScratchAllocator. \n4. Ongoing discussion whether to expose the running mean and average, and measure the performance impact. \n5. We need to test more common shapes to verify the results. \n"}