{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17379", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17379/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17379/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17379/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17379", "id": 301779208, "node_id": "MDU6SXNzdWUzMDE3NzkyMDg=", "number": 17379, "title": "`tf.case` is not allowing the computation of the `default` tensor when it falls in the default case", "user": {"login": "CharlesJQuarra", "id": 7565570, "node_id": "MDQ6VXNlcjc1NjU1NzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/7565570?v=4", "gravatar_id": "", "url": "https://api.github.com/users/CharlesJQuarra", "html_url": "https://github.com/CharlesJQuarra", "followers_url": "https://api.github.com/users/CharlesJQuarra/followers", "following_url": "https://api.github.com/users/CharlesJQuarra/following{/other_user}", "gists_url": "https://api.github.com/users/CharlesJQuarra/gists{/gist_id}", "starred_url": "https://api.github.com/users/CharlesJQuarra/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/CharlesJQuarra/subscriptions", "organizations_url": "https://api.github.com/users/CharlesJQuarra/orgs", "repos_url": "https://api.github.com/users/CharlesJQuarra/repos", "events_url": "https://api.github.com/users/CharlesJQuarra/events{/privacy}", "received_events_url": "https://api.github.com/users/CharlesJQuarra/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-03-02T14:14:28Z", "updated_at": "2018-03-02T14:18:59Z", "closed_at": "2018-03-02T14:18:59Z", "author_association": "NONE", "body_html": "<pre><code>== cat /etc/issue ===============================================\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nYes\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.0)\nprotobuf (3.5.1)\ntensorflow (1.6.0rc1)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.6.0-rc1\ntf.GIT_VERSION = v1.6.0-rc0-19-gecec1d8\ntf.COMPILER_VERSION = v1.6.0-rc0-19-gecec1d8\nSanity check: array([1], dtype=int32)\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\n\n== cuda libs  ===================================================\n</code></pre>\n<h3>Describe the problem</h3>\n<p><code>tf.case</code> does not seem to be computing the graph tensor related with the <code>default</code> parameter when it should fall in it. It seems to be computing all the case tensors correctly in all the other cases</p>\n<h3>Source code / logs</h3>\n<p>The script optimizes a piecewise rectilinear function defined with <code>tf.case</code> to match the square of the argument as closely as possible in the interval (-10, 10). If the argument is greater or equal than 10, the case should be handled by the <code>default</code> argument tensor, which in the example below is <code>O1</code>, which is also the case tensor when the argument is between [-5, -1). After the function has been optimized, the script outputs the value of the piecewise function in the range (-100, 100), and we see that in the interval [10, 100), O1 seems to have the constant value -1.0 that does not depend on the input, while in the range [-5, -1) it correctly depends on it</p>\n<pre><code>import sys\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\nimport numpy as np\n\ninitia = tf.random_normal_initializer(5e-3, 1e-4)\nbias_init = tf.random_normal_initializer(-1e-3, 5*1e-4)\ncdtype = tf.float32\nDEPTH_1 = 1\nOUT_DEPTH = 1\n\ndef act(inp, **kwargs):\n  return tf.nn.elu(inp, name = kwargs['name'])\n\nI = tf.placeholder(cdtype, shape=[None,1], name='I') # input\n\nW = tf.get_variable('W', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb = tf.get_variable('b', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO = act(tf.matmul(I, W) + b, name='O') # activation / output\n\nW1 = tf.get_variable('W1', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb1 = tf.get_variable('b1', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO1 = act(tf.matmul(I, W1) + b1, name='O1')\n\nW2 = tf.get_variable('W2', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb2 = tf.get_variable('b2', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO2 = act(tf.matmul(I, W2) + b2, name='O2')\n\nW3 = tf.get_variable('W3', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb3 = tf.get_variable('b3', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO3 = act(tf.matmul(I, W3) + b3, name='O3')\n\nW4 = tf.get_variable('W4', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb4 = tf.get_variable('b4', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO4 = act(tf.matmul(I, W4) + b4, name='O4')\n\nW5 = tf.get_variable('W5', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb5 = tf.get_variable('b5', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO5 = act(tf.matmul(I, W5) + b5, name='O5')\n\neval_inp = tf.gather_nd(I,[[0,0]])\n\nc1 = tf.gather_nd(tf.less(I , [[-5.0]]), [[0,0]])[0]\nc2 = tf.gather_nd(tf.less(I , [[-1.0]]), [[0,0]])[0]\nc3 = tf.gather_nd(tf.less(I , [[0.0]]), [[0,0]])[0]\nc4 = tf.gather_nd(tf.less(I , [[1.0]]), [[0,0]])[0]\nc5 = tf.gather_nd(tf.less(I , [[5.0]]), [[0,0]])[0]\nc6 = tf.gather_nd(tf.less(I , [[10.0]]), [[0,0]])[0]\n\nf1 = lambda: O\nf2 = lambda: O1\nf3 = lambda: O2\nf4 = lambda: O3\nf5 = lambda: O4\nf6 = lambda: O5\nfdef = lambda: O1\n\ncaseFun = tf.case([(c1 , f1), (c2 , f2), (c3 , f3), (c4, f4), (c5, f5), (c6,f6)], default=fdef)\n\ndistance = tf.reduce_mean( tf.square( caseFun - tf.square(eval_inp) ) )\n\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(distance)\ninit_op = tf.global_variables_initializer()\n\n\nwith tf.Session() as sess:\n  sess.run(init_op)\n\n  for i in range(10000):\n    s = sess.run([\n      train_op,\n      I, caseFun, distance\n      ], feed_dict={ I: 20.0*np.random.rand(1,1) - 10.0})\n    if i % 1000 == 0:\n      print s\n\n  print \"W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\"\n  print sess.run([W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5], feed_dict={})\n  for i in np.arange(-100.0, 100.0):\n    print sess.run([I, caseFun, distance , O1\n      ], feed_dict={ I: [[i]]})\n</code></pre>\n<p>The output looks like this:</p>\n<pre><code>....\n[array([[-10.]], dtype=float32), array([[43.378143]], dtype=float32), 3206.0347, array([[27.115139]], dtype=float32)]\n[array([[-9.]], dtype=float32), array([[39.456512]], dtype=float32), 1725.8613, array([[24.640066]], dtype=float32)]\n[array([[-8.]], dtype=float32), array([[35.53488]], dtype=float32), 810.26294, array([[22.164993]], dtype=float32)]\n[array([[-7.]], dtype=float32), array([[31.613249]], dtype=float32), 302.2991, array([[19.68992]], dtype=float32)]\n[array([[-6.]], dtype=float32), array([[27.691616]], dtype=float32), 69.02924, array([[17.214848]], dtype=float32)]\n[array([[-5.]], dtype=float32), array([[14.739774]], dtype=float32), 105.27224, array([[14.739774]], dtype=float32)]\n[array([[-4.]], dtype=float32), array([[12.264701]], dtype=float32), 13.952459, array([[12.264701]], dtype=float32)]\n[array([[-3.]], dtype=float32), array([[9.789628]], dtype=float32), 0.62351245, array([[9.789628]], dtype=float32)]\n[array([[-2.]], dtype=float32), array([[7.314555]], dtype=float32), 10.986276, array([[7.314555]], dtype=float32)]\n[array([[-1.]], dtype=float32), array([[0.6768117]], dtype=float32), 0.10445068, array([[4.8394823]], dtype=float32)]\n[array([[0.]], dtype=float32), array([[0.02995399]], dtype=float32), 0.00089724176, array([[2.3644097]], dtype=float32)]\n[array([[1.]], dtype=float32), array([[4.860016]], dtype=float32), 14.899722, array([[-0.10475975]], dtype=float32)]\n[array([[2.]], dtype=float32), array([[7.3534083]], dtype=float32), 11.245347, array([[-0.9246594]], dtype=float32)]\n[array([[3.]], dtype=float32), array([[9.846801]], dtype=float32), 0.7170716, array([[-0.99365956]], dtype=float32)]\n[array([[4.]], dtype=float32), array([[12.340193]], dtype=float32), 13.394189, array([[-0.9994664]], dtype=float32)]\n[array([[5.]], dtype=float32), array([[24.035427]], dtype=float32), 0.9304009, array([[-0.9999551]], dtype=float32)]\n[array([[6.]], dtype=float32), array([[28.001446]], dtype=float32), 63.97687, array([[-0.99999624]], dtype=float32)]\n[array([[7.]], dtype=float32), array([[31.96746]], dtype=float32), 290.1074, array([[-0.9999997]], dtype=float32)]\n[array([[8.]], dtype=float32), array([[35.93348]], dtype=float32), 787.72955, array([[-1.]], dtype=float32)]\n[array([[9.]], dtype=float32), array([[39.899498]], dtype=float32), 1689.2512, array([[-1.]], dtype=float32)]\n[array([[10.]], dtype=float32), array([[-1.]], dtype=float32), 10201.0, array([[-1.]], dtype=float32)]\n[array([[11.]], dtype=float32), array([[-1.]], dtype=float32), 14884.0, array([[-1.]], dtype=float32)]\n.....\n</code></pre>", "body_text": "== cat /etc/issue ===============================================\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nYes\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.0)\nprotobuf (3.5.1)\ntensorflow (1.6.0rc1)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.6.0-rc1\ntf.GIT_VERSION = v1.6.0-rc0-19-gecec1d8\ntf.COMPILER_VERSION = v1.6.0-rc0-19-gecec1d8\nSanity check: array([1], dtype=int32)\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\n\n== cuda libs  ===================================================\n\nDescribe the problem\ntf.case does not seem to be computing the graph tensor related with the default parameter when it should fall in it. It seems to be computing all the case tensors correctly in all the other cases\nSource code / logs\nThe script optimizes a piecewise rectilinear function defined with tf.case to match the square of the argument as closely as possible in the interval (-10, 10). If the argument is greater or equal than 10, the case should be handled by the default argument tensor, which in the example below is O1, which is also the case tensor when the argument is between [-5, -1). After the function has been optimized, the script outputs the value of the piecewise function in the range (-100, 100), and we see that in the interval [10, 100), O1 seems to have the constant value -1.0 that does not depend on the input, while in the range [-5, -1) it correctly depends on it\nimport sys\nimport tensorflow as tf\nfrom tensorflow.python import debug as tf_debug\nimport numpy as np\n\ninitia = tf.random_normal_initializer(5e-3, 1e-4)\nbias_init = tf.random_normal_initializer(-1e-3, 5*1e-4)\ncdtype = tf.float32\nDEPTH_1 = 1\nOUT_DEPTH = 1\n\ndef act(inp, **kwargs):\n  return tf.nn.elu(inp, name = kwargs['name'])\n\nI = tf.placeholder(cdtype, shape=[None,1], name='I') # input\n\nW = tf.get_variable('W', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb = tf.get_variable('b', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO = act(tf.matmul(I, W) + b, name='O') # activation / output\n\nW1 = tf.get_variable('W1', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb1 = tf.get_variable('b1', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO1 = act(tf.matmul(I, W1) + b1, name='O1')\n\nW2 = tf.get_variable('W2', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb2 = tf.get_variable('b2', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO2 = act(tf.matmul(I, W2) + b2, name='O2')\n\nW3 = tf.get_variable('W3', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb3 = tf.get_variable('b3', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO3 = act(tf.matmul(I, W3) + b3, name='O3')\n\nW4 = tf.get_variable('W4', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb4 = tf.get_variable('b4', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO4 = act(tf.matmul(I, W4) + b4, name='O4')\n\nW5 = tf.get_variable('W5', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\nb5 = tf.get_variable('b5', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\nO5 = act(tf.matmul(I, W5) + b5, name='O5')\n\neval_inp = tf.gather_nd(I,[[0,0]])\n\nc1 = tf.gather_nd(tf.less(I , [[-5.0]]), [[0,0]])[0]\nc2 = tf.gather_nd(tf.less(I , [[-1.0]]), [[0,0]])[0]\nc3 = tf.gather_nd(tf.less(I , [[0.0]]), [[0,0]])[0]\nc4 = tf.gather_nd(tf.less(I , [[1.0]]), [[0,0]])[0]\nc5 = tf.gather_nd(tf.less(I , [[5.0]]), [[0,0]])[0]\nc6 = tf.gather_nd(tf.less(I , [[10.0]]), [[0,0]])[0]\n\nf1 = lambda: O\nf2 = lambda: O1\nf3 = lambda: O2\nf4 = lambda: O3\nf5 = lambda: O4\nf6 = lambda: O5\nfdef = lambda: O1\n\ncaseFun = tf.case([(c1 , f1), (c2 , f2), (c3 , f3), (c4, f4), (c5, f5), (c6,f6)], default=fdef)\n\ndistance = tf.reduce_mean( tf.square( caseFun - tf.square(eval_inp) ) )\n\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(distance)\ninit_op = tf.global_variables_initializer()\n\n\nwith tf.Session() as sess:\n  sess.run(init_op)\n\n  for i in range(10000):\n    s = sess.run([\n      train_op,\n      I, caseFun, distance\n      ], feed_dict={ I: 20.0*np.random.rand(1,1) - 10.0})\n    if i % 1000 == 0:\n      print s\n\n  print \"W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\"\n  print sess.run([W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5], feed_dict={})\n  for i in np.arange(-100.0, 100.0):\n    print sess.run([I, caseFun, distance , O1\n      ], feed_dict={ I: [[i]]})\n\nThe output looks like this:\n....\n[array([[-10.]], dtype=float32), array([[43.378143]], dtype=float32), 3206.0347, array([[27.115139]], dtype=float32)]\n[array([[-9.]], dtype=float32), array([[39.456512]], dtype=float32), 1725.8613, array([[24.640066]], dtype=float32)]\n[array([[-8.]], dtype=float32), array([[35.53488]], dtype=float32), 810.26294, array([[22.164993]], dtype=float32)]\n[array([[-7.]], dtype=float32), array([[31.613249]], dtype=float32), 302.2991, array([[19.68992]], dtype=float32)]\n[array([[-6.]], dtype=float32), array([[27.691616]], dtype=float32), 69.02924, array([[17.214848]], dtype=float32)]\n[array([[-5.]], dtype=float32), array([[14.739774]], dtype=float32), 105.27224, array([[14.739774]], dtype=float32)]\n[array([[-4.]], dtype=float32), array([[12.264701]], dtype=float32), 13.952459, array([[12.264701]], dtype=float32)]\n[array([[-3.]], dtype=float32), array([[9.789628]], dtype=float32), 0.62351245, array([[9.789628]], dtype=float32)]\n[array([[-2.]], dtype=float32), array([[7.314555]], dtype=float32), 10.986276, array([[7.314555]], dtype=float32)]\n[array([[-1.]], dtype=float32), array([[0.6768117]], dtype=float32), 0.10445068, array([[4.8394823]], dtype=float32)]\n[array([[0.]], dtype=float32), array([[0.02995399]], dtype=float32), 0.00089724176, array([[2.3644097]], dtype=float32)]\n[array([[1.]], dtype=float32), array([[4.860016]], dtype=float32), 14.899722, array([[-0.10475975]], dtype=float32)]\n[array([[2.]], dtype=float32), array([[7.3534083]], dtype=float32), 11.245347, array([[-0.9246594]], dtype=float32)]\n[array([[3.]], dtype=float32), array([[9.846801]], dtype=float32), 0.7170716, array([[-0.99365956]], dtype=float32)]\n[array([[4.]], dtype=float32), array([[12.340193]], dtype=float32), 13.394189, array([[-0.9994664]], dtype=float32)]\n[array([[5.]], dtype=float32), array([[24.035427]], dtype=float32), 0.9304009, array([[-0.9999551]], dtype=float32)]\n[array([[6.]], dtype=float32), array([[28.001446]], dtype=float32), 63.97687, array([[-0.99999624]], dtype=float32)]\n[array([[7.]], dtype=float32), array([[31.96746]], dtype=float32), 290.1074, array([[-0.9999997]], dtype=float32)]\n[array([[8.]], dtype=float32), array([[35.93348]], dtype=float32), 787.72955, array([[-1.]], dtype=float32)]\n[array([[9.]], dtype=float32), array([[39.899498]], dtype=float32), 1689.2512, array([[-1.]], dtype=float32)]\n[array([[10.]], dtype=float32), array([[-1.]], dtype=float32), 10201.0, array([[-1.]], dtype=float32)]\n[array([[11.]], dtype=float32), array([[-1.]], dtype=float32), 14884.0, array([[-1.]], dtype=float32)]\n.....", "body": "```\r\n== cat /etc/issue ===============================================\r\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.3 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.6) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux tf-service 4.9.31-moby #1 SMP Sat Jun 24 06:29:23 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.0)\r\nprotobuf (3.5.1)\r\ntensorflow (1.6.0rc1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.6.0-rc1\r\ntf.GIT_VERSION = v1.6.0-rc0-19-gecec1d8\r\ntf.COMPILER_VERSION = v1.6.0-rc0-19-gecec1d8\r\nSanity check: array([1], dtype=int32)\r\n/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\n./tf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n```\r\n\r\n### Describe the problem\r\n\r\n`tf.case` does not seem to be computing the graph tensor related with the `default` parameter when it should fall in it. It seems to be computing all the case tensors correctly in all the other cases\r\n\r\n\r\n### Source code / logs\r\n\r\nThe script optimizes a piecewise rectilinear function defined with `tf.case` to match the square of the argument as closely as possible in the interval (-10, 10). If the argument is greater or equal than 10, the case should be handled by the `default` argument tensor, which in the example below is `O1`, which is also the case tensor when the argument is between [-5, -1). After the function has been optimized, the script outputs the value of the piecewise function in the range (-100, 100), and we see that in the interval [10, 100), O1 seems to have the constant value -1.0 that does not depend on the input, while in the range [-5, -1) it correctly depends on it\r\n\r\n\r\n```\r\nimport sys\r\nimport tensorflow as tf\r\nfrom tensorflow.python import debug as tf_debug\r\nimport numpy as np\r\n\r\ninitia = tf.random_normal_initializer(5e-3, 1e-4)\r\nbias_init = tf.random_normal_initializer(-1e-3, 5*1e-4)\r\ncdtype = tf.float32\r\nDEPTH_1 = 1\r\nOUT_DEPTH = 1\r\n\r\ndef act(inp, **kwargs):\r\n  return tf.nn.elu(inp, name = kwargs['name'])\r\n\r\nI = tf.placeholder(cdtype, shape=[None,1], name='I') # input\r\n\r\nW = tf.get_variable('W', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb = tf.get_variable('b', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO = act(tf.matmul(I, W) + b, name='O') # activation / output\r\n\r\nW1 = tf.get_variable('W1', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb1 = tf.get_variable('b1', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO1 = act(tf.matmul(I, W1) + b1, name='O1')\r\n\r\nW2 = tf.get_variable('W2', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb2 = tf.get_variable('b2', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO2 = act(tf.matmul(I, W2) + b2, name='O2')\r\n\r\nW3 = tf.get_variable('W3', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb3 = tf.get_variable('b3', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO3 = act(tf.matmul(I, W3) + b3, name='O3')\r\n\r\nW4 = tf.get_variable('W4', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb4 = tf.get_variable('b4', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO4 = act(tf.matmul(I, W4) + b4, name='O4')\r\n\r\nW5 = tf.get_variable('W5', shape=[1,DEPTH_1], initializer=initia, dtype=cdtype, trainable=True) # weights\r\nb5 = tf.get_variable('b5', shape=[DEPTH_1], initializer=bias_init, dtype=cdtype, trainable=True) # biases\r\nO5 = act(tf.matmul(I, W5) + b5, name='O5')\r\n\r\neval_inp = tf.gather_nd(I,[[0,0]])\r\n\r\nc1 = tf.gather_nd(tf.less(I , [[-5.0]]), [[0,0]])[0]\r\nc2 = tf.gather_nd(tf.less(I , [[-1.0]]), [[0,0]])[0]\r\nc3 = tf.gather_nd(tf.less(I , [[0.0]]), [[0,0]])[0]\r\nc4 = tf.gather_nd(tf.less(I , [[1.0]]), [[0,0]])[0]\r\nc5 = tf.gather_nd(tf.less(I , [[5.0]]), [[0,0]])[0]\r\nc6 = tf.gather_nd(tf.less(I , [[10.0]]), [[0,0]])[0]\r\n\r\nf1 = lambda: O\r\nf2 = lambda: O1\r\nf3 = lambda: O2\r\nf4 = lambda: O3\r\nf5 = lambda: O4\r\nf6 = lambda: O5\r\nfdef = lambda: O1\r\n\r\ncaseFun = tf.case([(c1 , f1), (c2 , f2), (c3 , f3), (c4, f4), (c5, f5), (c6,f6)], default=fdef)\r\n\r\ndistance = tf.reduce_mean( tf.square( caseFun - tf.square(eval_inp) ) )\r\n\r\ntrain_op = tf.train.AdamOptimizer(1e-3).minimize(distance)\r\ninit_op = tf.global_variables_initializer()\r\n\r\n\r\nwith tf.Session() as sess:\r\n  sess.run(init_op)\r\n\r\n  for i in range(10000):\r\n    s = sess.run([\r\n      train_op,\r\n      I, caseFun, distance\r\n      ], feed_dict={ I: 20.0*np.random.rand(1,1) - 10.0})\r\n    if i % 1000 == 0:\r\n      print s\r\n\r\n  print \"W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5\"\r\n  print sess.run([W, b, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5], feed_dict={})\r\n  for i in np.arange(-100.0, 100.0):\r\n    print sess.run([I, caseFun, distance , O1\r\n      ], feed_dict={ I: [[i]]})\r\n```\r\n\r\nThe output looks like this:\r\n\r\n```\r\n....\r\n[array([[-10.]], dtype=float32), array([[43.378143]], dtype=float32), 3206.0347, array([[27.115139]], dtype=float32)]\r\n[array([[-9.]], dtype=float32), array([[39.456512]], dtype=float32), 1725.8613, array([[24.640066]], dtype=float32)]\r\n[array([[-8.]], dtype=float32), array([[35.53488]], dtype=float32), 810.26294, array([[22.164993]], dtype=float32)]\r\n[array([[-7.]], dtype=float32), array([[31.613249]], dtype=float32), 302.2991, array([[19.68992]], dtype=float32)]\r\n[array([[-6.]], dtype=float32), array([[27.691616]], dtype=float32), 69.02924, array([[17.214848]], dtype=float32)]\r\n[array([[-5.]], dtype=float32), array([[14.739774]], dtype=float32), 105.27224, array([[14.739774]], dtype=float32)]\r\n[array([[-4.]], dtype=float32), array([[12.264701]], dtype=float32), 13.952459, array([[12.264701]], dtype=float32)]\r\n[array([[-3.]], dtype=float32), array([[9.789628]], dtype=float32), 0.62351245, array([[9.789628]], dtype=float32)]\r\n[array([[-2.]], dtype=float32), array([[7.314555]], dtype=float32), 10.986276, array([[7.314555]], dtype=float32)]\r\n[array([[-1.]], dtype=float32), array([[0.6768117]], dtype=float32), 0.10445068, array([[4.8394823]], dtype=float32)]\r\n[array([[0.]], dtype=float32), array([[0.02995399]], dtype=float32), 0.00089724176, array([[2.3644097]], dtype=float32)]\r\n[array([[1.]], dtype=float32), array([[4.860016]], dtype=float32), 14.899722, array([[-0.10475975]], dtype=float32)]\r\n[array([[2.]], dtype=float32), array([[7.3534083]], dtype=float32), 11.245347, array([[-0.9246594]], dtype=float32)]\r\n[array([[3.]], dtype=float32), array([[9.846801]], dtype=float32), 0.7170716, array([[-0.99365956]], dtype=float32)]\r\n[array([[4.]], dtype=float32), array([[12.340193]], dtype=float32), 13.394189, array([[-0.9994664]], dtype=float32)]\r\n[array([[5.]], dtype=float32), array([[24.035427]], dtype=float32), 0.9304009, array([[-0.9999551]], dtype=float32)]\r\n[array([[6.]], dtype=float32), array([[28.001446]], dtype=float32), 63.97687, array([[-0.99999624]], dtype=float32)]\r\n[array([[7.]], dtype=float32), array([[31.96746]], dtype=float32), 290.1074, array([[-0.9999997]], dtype=float32)]\r\n[array([[8.]], dtype=float32), array([[35.93348]], dtype=float32), 787.72955, array([[-1.]], dtype=float32)]\r\n[array([[9.]], dtype=float32), array([[39.899498]], dtype=float32), 1689.2512, array([[-1.]], dtype=float32)]\r\n[array([[10.]], dtype=float32), array([[-1.]], dtype=float32), 10201.0, array([[-1.]], dtype=float32)]\r\n[array([[11.]], dtype=float32), array([[-1.]], dtype=float32), 14884.0, array([[-1.]], dtype=float32)]\r\n.....\r\n```"}