{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2476", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2476/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2476/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2476/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2476", "id": 156304283, "node_id": "MDU6SXNzdWUxNTYzMDQyODM=", "number": 2476, "title": "Feature request: RMSPropOptimizer support for sparse gradients", "user": {"login": "333caowei", "id": 4569055, "node_id": "MDQ6VXNlcjQ1NjkwNTU=", "avatar_url": "https://avatars3.githubusercontent.com/u/4569055?v=4", "gravatar_id": "", "url": "https://api.github.com/users/333caowei", "html_url": "https://github.com/333caowei", "followers_url": "https://api.github.com/users/333caowei/followers", "following_url": "https://api.github.com/users/333caowei/following{/other_user}", "gists_url": "https://api.github.com/users/333caowei/gists{/gist_id}", "starred_url": "https://api.github.com/users/333caowei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/333caowei/subscriptions", "organizations_url": "https://api.github.com/users/333caowei/orgs", "repos_url": "https://api.github.com/users/333caowei/repos", "events_url": "https://api.github.com/users/333caowei/events{/privacy}", "received_events_url": "https://api.github.com/users/333caowei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-05-23T15:35:46Z", "updated_at": "2017-02-09T22:37:16Z", "closed_at": "2016-07-19T22:03:53Z", "author_association": "NONE", "body_html": "<p>I use RMSPropOptimizer to optimize parameters, I get NotImplementedError. <strong>But when I change to use AdamOptimizer, it works fine.</strong></p>\n<p>So I try to fix the problem and I find some key point, may be it can help.<br>\nHere is my code:</p>\n<pre><code>self.x = tf.placeholder(tf.int32, [None, sequence_length])  \npoint = tf.get_variable([len(embedding_matrix)])\n............\n............\noutputs, states = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\nindex     =    self.x[:, 0]\nindex  = tf.reshape(index, [-1,1])\nindex_point  =    tf.gather(pointt, index)\noutput   =  tf.mul(outputs[-1] ,   index_point)\n\nscores = tf.nn.xw_plus_b(output, self.W, b)\nlosses = tf.nn.softmax_cross_entropy_with_logits(scores, self.input_y)\nself.loss = tf.reduce_mean(losses) \noptimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.9)\ngrads_and_vars = optimizer.compute_gradients(self.loss)\nself.train_op = optimizer.apply_gradients(grads_and_vars)\n</code></pre>\n<p>When I try to change the line <code>output   =  tf.mul(outputs[-1] ,   index_point)</code> to some others such as <code>output   =  tf.mul(outputs[-1] ,   2)</code>, <strong>the error disappear</strong>. And I try to change to use <code>bidirectional_rnn</code>, <strong>the error also disappear.</strong></p>", "body_text": "I use RMSPropOptimizer to optimize parameters, I get NotImplementedError. But when I change to use AdamOptimizer, it works fine.\nSo I try to fix the problem and I find some key point, may be it can help.\nHere is my code:\nself.x = tf.placeholder(tf.int32, [None, sequence_length])  \npoint = tf.get_variable([len(embedding_matrix)])\n............\n............\noutputs, states = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\nindex     =    self.x[:, 0]\nindex  = tf.reshape(index, [-1,1])\nindex_point  =    tf.gather(pointt, index)\noutput   =  tf.mul(outputs[-1] ,   index_point)\n\nscores = tf.nn.xw_plus_b(output, self.W, b)\nlosses = tf.nn.softmax_cross_entropy_with_logits(scores, self.input_y)\nself.loss = tf.reduce_mean(losses) \noptimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.9)\ngrads_and_vars = optimizer.compute_gradients(self.loss)\nself.train_op = optimizer.apply_gradients(grads_and_vars)\n\nWhen I try to change the line output   =  tf.mul(outputs[-1] ,   index_point) to some others such as output   =  tf.mul(outputs[-1] ,   2), the error disappear. And I try to change to use bidirectional_rnn, the error also disappear.", "body": "I use RMSPropOptimizer to optimize parameters, I get NotImplementedError. **But when I change to use AdamOptimizer, it works fine.**\n\nSo I try to fix the problem and I find some key point, may be it can help.\nHere is my code:\n\n```\nself.x = tf.placeholder(tf.int32, [None, sequence_length])  \npoint = tf.get_variable([len(embedding_matrix)])\n............\n............\noutputs, states = rnn.rnn(lstm_cell, x, initial_state=initial_state, sequence_length=real_length)\nindex     =    self.x[:, 0]\nindex  = tf.reshape(index, [-1,1])\nindex_point  =    tf.gather(pointt, index)\noutput   =  tf.mul(outputs[-1] ,   index_point)\n\nscores = tf.nn.xw_plus_b(output, self.W, b)\nlosses = tf.nn.softmax_cross_entropy_with_logits(scores, self.input_y)\nself.loss = tf.reduce_mean(losses) \noptimizer = tf.train.RMSPropOptimizer(1e-3, decay = 0.9)\ngrads_and_vars = optimizer.compute_gradients(self.loss)\nself.train_op = optimizer.apply_gradients(grads_and_vars)\n```\n\nWhen I try to change the line `output   =  tf.mul(outputs[-1] ,   index_point)` to some others such as `output   =  tf.mul(outputs[-1] ,   2)`, **the error disappear**. And I try to change to use `bidirectional_rnn`, **the error also disappear.**\n"}