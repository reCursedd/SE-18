{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/837", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/837/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/837/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/837/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/837", "id": 128062452, "node_id": "MDU6SXNzdWUxMjgwNjI0NTI=", "number": 837, "title": "reading_data/fully_connected_reader.py VERY slow relative to fully_connected_feed.py", "user": {"login": "nryant", "id": 716377, "node_id": "MDQ6VXNlcjcxNjM3Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/716377?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nryant", "html_url": "https://github.com/nryant", "followers_url": "https://api.github.com/users/nryant/followers", "following_url": "https://api.github.com/users/nryant/following{/other_user}", "gists_url": "https://api.github.com/users/nryant/gists{/gist_id}", "starred_url": "https://api.github.com/users/nryant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nryant/subscriptions", "organizations_url": "https://api.github.com/users/nryant/orgs", "repos_url": "https://api.github.com/users/nryant/repos", "events_url": "https://api.github.com/users/nryant/events{/privacy}", "received_events_url": "https://api.github.com/users/nryant/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-01-22T01:41:40Z", "updated_at": "2017-01-24T02:03:06Z", "closed_at": "2017-01-24T02:03:06Z", "author_association": "NONE", "body_html": "<p>I noticed that when using a data reader to provide minibatches of examples to a model that performance is greatly reduced relative to just supplying the examples via <code>feed_dict</code>. For instance, when running <code>reading_data/fully_connected_reader.py</code> with the following flags::</p>\n<pre><code>--hidden1 512 --hidden2 512 --batch_size 128\n</code></pre>\n<p>it takes 28.7 seconds to process 600 minibatches with a GPU utilization of 13%. If I edit the code so that <code>num_threads=16</code> (instead of <code>num_threads=2</code>) when <code>shuffle_batch</code> is called, these numbers improve to 14.9 seconds and 23% GPU utilization. However, training the same model via <code>fully_connected_feed.py</code> takes only 2.63 seconds and achieves a GPU utilization of 55%. This is hardly rigorous, but it seems that the overhead involved in reading the Example protos from the TFRecords file, putting them into a queue, etc is much higher than I would expect.</p>\n<p>These numbers were compiled using <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/039981f5a382ce9dc1e97dc3bd25aeba7fd82ade/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/039981f5a382ce9dc1e97dc3bd25aeba7fd82ade\"><tt>039981f</tt></a> and running on a Titan X card with no other background processes running.</p>", "body_text": "I noticed that when using a data reader to provide minibatches of examples to a model that performance is greatly reduced relative to just supplying the examples via feed_dict. For instance, when running reading_data/fully_connected_reader.py with the following flags::\n--hidden1 512 --hidden2 512 --batch_size 128\n\nit takes 28.7 seconds to process 600 minibatches with a GPU utilization of 13%. If I edit the code so that num_threads=16 (instead of num_threads=2) when shuffle_batch is called, these numbers improve to 14.9 seconds and 23% GPU utilization. However, training the same model via fully_connected_feed.py takes only 2.63 seconds and achieves a GPU utilization of 55%. This is hardly rigorous, but it seems that the overhead involved in reading the Example protos from the TFRecords file, putting them into a queue, etc is much higher than I would expect.\nThese numbers were compiled using 039981f and running on a Titan X card with no other background processes running.", "body": "I noticed that when using a data reader to provide minibatches of examples to a model that performance is greatly reduced relative to just supplying the examples via `feed_dict`. For instance, when running `reading_data/fully_connected_reader.py` with the following flags::\n\n```\n--hidden1 512 --hidden2 512 --batch_size 128\n```\n\nit takes 28.7 seconds to process 600 minibatches with a GPU utilization of 13%. If I edit the code so that `num_threads=16` (instead of `num_threads=2`) when `shuffle_batch` is called, these numbers improve to 14.9 seconds and 23% GPU utilization. However, training the same model via `fully_connected_feed.py` takes only 2.63 seconds and achieves a GPU utilization of 55%. This is hardly rigorous, but it seems that the overhead involved in reading the Example protos from the TFRecords file, putting them into a queue, etc is much higher than I would expect. \n\nThese numbers were compiled using 039981f5a382ce9dc1e97dc3bd25aeba7fd82ade and running on a Titan X card with no other background processes running.\n"}