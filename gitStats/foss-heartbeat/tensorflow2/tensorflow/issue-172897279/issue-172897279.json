{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4011", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4011/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4011/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4011/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4011", "id": 172897279, "node_id": "MDU6SXNzdWUxNzI4OTcyNzk=", "number": 4011, "title": "Is there anyway to run a tensorflow graph (.pb or .meta) generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes?", "user": {"login": "classicsong", "id": 1751052, "node_id": "MDQ6VXNlcjE3NTEwNTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/1751052?v=4", "gravatar_id": "", "url": "https://api.github.com/users/classicsong", "html_url": "https://github.com/classicsong", "followers_url": "https://api.github.com/users/classicsong/followers", "following_url": "https://api.github.com/users/classicsong/following{/other_user}", "gists_url": "https://api.github.com/users/classicsong/gists{/gist_id}", "starred_url": "https://api.github.com/users/classicsong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/classicsong/subscriptions", "organizations_url": "https://api.github.com/users/classicsong/orgs", "repos_url": "https://api.github.com/users/classicsong/repos", "events_url": "https://api.github.com/users/classicsong/events{/privacy}", "received_events_url": "https://api.github.com/users/classicsong/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-08-24T08:46:45Z", "updated_at": "2016-08-24T16:57:12Z", "closed_at": "2016-08-24T16:57:12Z", "author_association": "NONE", "body_html": "<p>Will future versions of tensorflow provide a way to run the tensorflow graph generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes through python interfaces?<br>\nOr is it supported right now?</p>\n<p>I am trying to build my tf.graph on my notebook (single node) and save then graph into a binary file,<br>\nand then loading the binary graph into a distributed environment (with multiply ps and worker nodes) to train and verify it. It seems it is not supported now.</p>\n<p>I tried it on tensorflow-0.10 and failed.<br>\nBy using tf.train.write_graph(sess.graph_def, path, pb_name) interface: The graph saved is not trainable as loading the .pb file through import_graph_def will only g.create_ops according to the '.bp' file but not add then into ops.collections. So the graph loaded is not trainable<br>\nBy using tf.saver.save to save a \".meta\" file: The loaded graph cannot fit into the distributed environment as devices assignment is messy.<br>\nI tried the tf.train.import_meta_graph('test_model.meta', clear_devices=True) interface to let the load clean the original device assignment and let the \"with tf.device(device_setter)\" reassign the device for each variable, but there is a problem as operations belonging to \"Saver\" and \"Restore\" still can not be assigned correctly. When creating operations for \"Saver\" and \"Restore\" ops through g.create_op inside import_graph_def called by import_meta_graph, the device_setter will not assign ps node to these ops as their name is not \"Variable\".<br>\nIs there any way to do so?</p>", "body_text": "Will future versions of tensorflow provide a way to run the tensorflow graph generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes through python interfaces?\nOr is it supported right now?\nI am trying to build my tf.graph on my notebook (single node) and save then graph into a binary file,\nand then loading the binary graph into a distributed environment (with multiply ps and worker nodes) to train and verify it. It seems it is not supported now.\nI tried it on tensorflow-0.10 and failed.\nBy using tf.train.write_graph(sess.graph_def, path, pb_name) interface: The graph saved is not trainable as loading the .pb file through import_graph_def will only g.create_ops according to the '.bp' file but not add then into ops.collections. So the graph loaded is not trainable\nBy using tf.saver.save to save a \".meta\" file: The loaded graph cannot fit into the distributed environment as devices assignment is messy.\nI tried the tf.train.import_meta_graph('test_model.meta', clear_devices=True) interface to let the load clean the original device assignment and let the \"with tf.device(device_setter)\" reassign the device for each variable, but there is a problem as operations belonging to \"Saver\" and \"Restore\" still can not be assigned correctly. When creating operations for \"Saver\" and \"Restore\" ops through g.create_op inside import_graph_def called by import_meta_graph, the device_setter will not assign ps node to these ops as their name is not \"Variable\".\nIs there any way to do so?", "body": "Will future versions of tensorflow provide a way to run the tensorflow graph generated by single node tf.sess on a distributed environments with multiple ps nodes and worker nodes through python interfaces?\nOr is it supported right now?\n\nI am trying to build my tf.graph on my notebook (single node) and save then graph into a binary file, \nand then loading the binary graph into a distributed environment (with multiply ps and worker nodes) to train and verify it. It seems it is not supported now.\n\nI tried it on tensorflow-0.10 and failed.\nBy using tf.train.write_graph(sess.graph_def, path, pb_name) interface: The graph saved is not trainable as loading the .pb file through import_graph_def will only g.create_ops according to the '.bp' file but not add then into ops.collections. So the graph loaded is not trainable\nBy using tf.saver.save to save a \".meta\" file: The loaded graph cannot fit into the distributed environment as devices assignment is messy. \nI tried the tf.train.import_meta_graph('test_model.meta', clear_devices=True) interface to let the load clean the original device assignment and let the \"with tf.device(device_setter)\" reassign the device for each variable, but there is a problem as operations belonging to \"Saver\" and \"Restore\" still can not be assigned correctly. When creating operations for \"Saver\" and \"Restore\" ops through g.create_op inside import_graph_def called by import_meta_graph, the device_setter will not assign ps node to these ops as their name is not \"Variable\".\nIs there any way to do so?\n"}