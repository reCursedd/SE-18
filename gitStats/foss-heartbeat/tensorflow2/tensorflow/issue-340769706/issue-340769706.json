{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20750", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20750/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20750/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20750/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20750", "id": 340769706, "node_id": "MDU6SXNzdWUzNDA3Njk3MDY=", "number": 20750, "title": "RunOptions.FULL_TRACE produce wrong timestamps", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "smit-hinsu", "id": 1990079, "node_id": "MDQ6VXNlcjE5OTAwNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1990079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smit-hinsu", "html_url": "https://github.com/smit-hinsu", "followers_url": "https://api.github.com/users/smit-hinsu/followers", "following_url": "https://api.github.com/users/smit-hinsu/following{/other_user}", "gists_url": "https://api.github.com/users/smit-hinsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/smit-hinsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smit-hinsu/subscriptions", "organizations_url": "https://api.github.com/users/smit-hinsu/orgs", "repos_url": "https://api.github.com/users/smit-hinsu/repos", "events_url": "https://api.github.com/users/smit-hinsu/events{/privacy}", "received_events_url": "https://api.github.com/users/smit-hinsu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "smit-hinsu", "id": 1990079, "node_id": "MDQ6VXNlcjE5OTAwNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1990079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smit-hinsu", "html_url": "https://github.com/smit-hinsu", "followers_url": "https://api.github.com/users/smit-hinsu/followers", "following_url": "https://api.github.com/users/smit-hinsu/following{/other_user}", "gists_url": "https://api.github.com/users/smit-hinsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/smit-hinsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smit-hinsu/subscriptions", "organizations_url": "https://api.github.com/users/smit-hinsu/orgs", "repos_url": "https://api.github.com/users/smit-hinsu/repos", "events_url": "https://api.github.com/users/smit-hinsu/events{/privacy}", "received_events_url": "https://api.github.com/users/smit-hinsu/received_events", "type": "User", "site_admin": false}, {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-07-12T19:33:31Z", "updated_at": "2018-10-26T22:34:25Z", "closed_at": "2018-10-26T22:33:31Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:v1.9.0-0-g25c197e023 1.9.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:n/a</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:n/a</li>\n<li><strong>CUDA/cuDNN version</strong>:9.0.176/7.1.1</li>\n<li><strong>GPU model and memory</strong>:P100 16G (can reproduce on other GPU model as well)</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span>!/usr/bin/env python</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> -*- coding: utf-8 -*-</span>\n\n<span class=\"pl-k\">import</span> sys\n<span class=\"pl-k\">import</span> os\n<span class=\"pl-k\">import</span> tqdm\n<span class=\"pl-k\">from</span> datetime <span class=\"pl-k\">import</span> datetime\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.python.client <span class=\"pl-k\">import</span> timeline\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build</span>():\n    image <span class=\"pl-k\">=</span> tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">32</span>,<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    label <span class=\"pl-k\">=</span> tf.random_uniform(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">64</span>], <span class=\"pl-v\">maxval</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n    l <span class=\"pl-k\">=</span> tf.transpose(image, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>])\n    <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">100</span>):\n        l <span class=\"pl-k\">=</span> tf.layers.conv2d(l, <span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-v\">data_format</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>channels_first<span class=\"pl-pds\">'</span></span>,\n                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(k), <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n    l <span class=\"pl-k\">=</span> tf.reduce_mean(l, [<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>])\n\n    logits <span class=\"pl-k\">=</span> tf.layers.dense(l, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>linear<span class=\"pl-pds\">'</span></span>)\n    cost <span class=\"pl-k\">=</span> tf.nn.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>label)\n    cost <span class=\"pl-k\">=</span> tf.reduce_mean(cost, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>cross_entropy_loss<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> cost\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n        cost1 <span class=\"pl-k\">=</span> build()\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:1<span class=\"pl-pds\">'</span></span>), tf.variable_scope(tf.get_variable_scope(), <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n        cost2 <span class=\"pl-k\">=</span> build()\n    cost <span class=\"pl-k\">=</span> cost1 <span class=\"pl-k\">+</span> cost2\n    opt <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.1</span>)\n    train_op <span class=\"pl-k\">=</span> opt.minimize(cost)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">write_tracing</span>(<span class=\"pl-smi\">idx</span>, <span class=\"pl-smi\">metadata</span>):\n        tl <span class=\"pl-k\">=</span> timeline.Timeline(<span class=\"pl-v\">step_stats</span><span class=\"pl-k\">=</span>metadata.step_stats)\n        fname <span class=\"pl-k\">=</span> os.path.join(\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>.<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>chrome-trace-<span class=\"pl-c1\">{}</span>.json<span class=\"pl-pds\">'</span></span>.format(idx))\n        <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(fname, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:\n            f.write(tl.generate_chrome_trace_format(\n                <span class=\"pl-v\">show_dataflow</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">show_memory</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n\n    config <span class=\"pl-k\">=</span> tf.ConfigProto()\n    config.allow_soft_placement <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n    <span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>config) <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.global_variables_initializer())\n        opt <span class=\"pl-k\">=</span> tf.RunOptions()\n        opt.trace_level <span class=\"pl-k\">=</span> tf.RunOptions.<span class=\"pl-c1\">FULL_TRACE</span>\n\n        <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> tqdm.trange(<span class=\"pl-c1\">100</span>):\n            meta <span class=\"pl-k\">=</span> tf.RunMetadata()\n            sess.run(train_op, <span class=\"pl-v\">options</span><span class=\"pl-k\">=</span>opt, <span class=\"pl-v\">run_metadata</span><span class=\"pl-k\">=</span>meta)\n\n            write_tracing(k, meta)\n\n            <span class=\"pl-k\">for</span> devst <span class=\"pl-k\">in</span> meta.step_stats.dev_stats:\n                <span class=\"pl-k\">for</span> ns <span class=\"pl-k\">in</span> devst.node_stats:\n                    micro <span class=\"pl-k\">=</span> timestamp <span class=\"pl-k\">=</span> ns.all_start_micros <span class=\"pl-k\">//</span> <span class=\"pl-c1\">1000000</span>\n                    timestamp <span class=\"pl-k\">=</span> datetime.fromtimestamp(timestamp)\n                    diff <span class=\"pl-k\">=</span> timestamp <span class=\"pl-k\">-</span> datetime.now()\n                    <span class=\"pl-k\">if</span> diff.days <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">100</span>:\n                        <span class=\"pl-c1\">print</span>(k, micro, timestamp)\n                        <span class=\"pl-c\"><span class=\"pl-c\">#</span>import IPython as IP; IP.embed()</span>\n                        <span class=\"pl-c\"><span class=\"pl-c\">#</span>sys.exit()</span></pre></div>\n<p>The code above trains a CNN on two GPUs with <code>FULL_TRACE</code> enabled. The returned profiling information contains correct timestamps, but sometimes contains timestamps that are many years in the future. It prints the following output:</p>\n<pre><code>...\n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33\n...\n</code></pre>\n<p>The issue was originally reported at <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"340169545\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorpack/tensorpack/issues/819\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorpack/tensorpack/issues/819/hovercard\" href=\"https://github.com/tensorpack/tensorpack/issues/819\">tensorpack/tensorpack#819</a>.<br>\nThe issue is more likely to happen after running about 50 steps.<br>\nThe issue seems to disappear when training on one GPU, or training a small model.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):v1.9.0-0-g25c197e023 1.9.0\nPython version: 3.6\nBazel version (if compiling from source):n/a\nGCC/Compiler version (if compiling from source):n/a\nCUDA/cuDNN version:9.0.176/7.1.1\nGPU model and memory:P100 16G (can reproduce on other GPU model as well)\nExact command to reproduce: see below\n\n#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nimport sys\nimport os\nimport tqdm\nfrom datetime import datetime\nimport tensorflow as tf\nfrom tensorflow.python.client import timeline\n\ndef build():\n    image = tf.random_normal(shape=[64,32,32,3], dtype=tf.float32)\n    label = tf.random_uniform(shape=[64], maxval=10, dtype=tf.int32)\n    l = tf.transpose(image, [0, 3, 1, 2])\n    for k in range(1, 100):\n        l = tf.layers.conv2d(l, 16, 3, data_format='channels_first',\n                name='conv{}'.format(k), padding='SAME')\n    l = tf.reduce_mean(l, [2, 3])\n\n    logits = tf.layers.dense(l, 10, name='linear')\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\n    return cost\n\nif __name__ == '__main__':\n    with tf.device('/gpu:0'):\n        cost1 = build()\n    with tf.device('/gpu:1'), tf.variable_scope(tf.get_variable_scope(), reuse=True):\n        cost2 = build()\n    cost = cost1 + cost2\n    opt = tf.train.GradientDescentOptimizer(0.1)\n    train_op = opt.minimize(cost)\n\n    def write_tracing(idx, metadata):\n        tl = timeline.Timeline(step_stats=metadata.step_stats)\n        fname = os.path.join(\n            '.', 'chrome-trace-{}.json'.format(idx))\n        with open(fname, 'w') as f:\n            f.write(tl.generate_chrome_trace_format(\n                show_dataflow=True, show_memory=True))\n\n    config = tf.ConfigProto()\n    config.allow_soft_placement = True\n    with tf.Session(config=config) as sess:\n        sess.run(tf.global_variables_initializer())\n        opt = tf.RunOptions()\n        opt.trace_level = tf.RunOptions.FULL_TRACE\n\n        for k in tqdm.trange(100):\n            meta = tf.RunMetadata()\n            sess.run(train_op, options=opt, run_metadata=meta)\n\n            write_tracing(k, meta)\n\n            for devst in meta.step_stats.dev_stats:\n                for ns in devst.node_stats:\n                    micro = timestamp = ns.all_start_micros // 1000000\n                    timestamp = datetime.fromtimestamp(timestamp)\n                    diff = timestamp - datetime.now()\n                    if diff.days > 100:\n                        print(k, micro, timestamp)\n                        #import IPython as IP; IP.embed()\n                        #sys.exit()\nThe code above trains a CNN on two GPUs with FULL_TRACE enabled. The returned profiling information contains correct timestamps, but sometimes contains timestamps that are many years in the future. It prints the following output:\n...\n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33               \n13 18446744073 2554-07-21 16:34:33\n...\n\nThe issue was originally reported at tensorpack/tensorpack#819.\nThe issue is more likely to happen after running about 50 steps.\nThe issue seems to disappear when training on one GPU, or training a small model.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:binary\r\n- **TensorFlow version (use command below)**:v1.9.0-0-g25c197e023 1.9.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:n/a\r\n- **GCC/Compiler version (if compiling from source)**:n/a\r\n- **CUDA/cuDNN version**:9.0.176/7.1.1\r\n- **GPU model and memory**:P100 16G (can reproduce on other GPU model as well)\r\n- **Exact command to reproduce**: see below\r\n\r\n```python\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport sys\r\nimport os\r\nimport tqdm\r\nfrom datetime import datetime\r\nimport tensorflow as tf\r\nfrom tensorflow.python.client import timeline\r\n\r\ndef build():\r\n    image = tf.random_normal(shape=[64,32,32,3], dtype=tf.float32)\r\n    label = tf.random_uniform(shape=[64], maxval=10, dtype=tf.int32)\r\n    l = tf.transpose(image, [0, 3, 1, 2])\r\n    for k in range(1, 100):\r\n        l = tf.layers.conv2d(l, 16, 3, data_format='channels_first',\r\n                name='conv{}'.format(k), padding='SAME')\r\n    l = tf.reduce_mean(l, [2, 3])\r\n\r\n    logits = tf.layers.dense(l, 10, name='linear')\r\n    cost = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=label)\r\n    cost = tf.reduce_mean(cost, name='cross_entropy_loss')\r\n    return cost\r\n\r\nif __name__ == '__main__':\r\n    with tf.device('/gpu:0'):\r\n        cost1 = build()\r\n    with tf.device('/gpu:1'), tf.variable_scope(tf.get_variable_scope(), reuse=True):\r\n        cost2 = build()\r\n    cost = cost1 + cost2\r\n    opt = tf.train.GradientDescentOptimizer(0.1)\r\n    train_op = opt.minimize(cost)\r\n\r\n    def write_tracing(idx, metadata):\r\n        tl = timeline.Timeline(step_stats=metadata.step_stats)\r\n        fname = os.path.join(\r\n            '.', 'chrome-trace-{}.json'.format(idx))\r\n        with open(fname, 'w') as f:\r\n            f.write(tl.generate_chrome_trace_format(\r\n                show_dataflow=True, show_memory=True))\r\n\r\n    config = tf.ConfigProto()\r\n    config.allow_soft_placement = True\r\n    with tf.Session(config=config) as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        opt = tf.RunOptions()\r\n        opt.trace_level = tf.RunOptions.FULL_TRACE\r\n\r\n        for k in tqdm.trange(100):\r\n            meta = tf.RunMetadata()\r\n            sess.run(train_op, options=opt, run_metadata=meta)\r\n\r\n            write_tracing(k, meta)\r\n\r\n            for devst in meta.step_stats.dev_stats:\r\n                for ns in devst.node_stats:\r\n                    micro = timestamp = ns.all_start_micros // 1000000\r\n                    timestamp = datetime.fromtimestamp(timestamp)\r\n                    diff = timestamp - datetime.now()\r\n                    if diff.days > 100:\r\n                        print(k, micro, timestamp)\r\n                        #import IPython as IP; IP.embed()\r\n                        #sys.exit()\r\n```\r\n\r\nThe code above trains a CNN on two GPUs with `FULL_TRACE` enabled. The returned profiling information contains correct timestamps, but sometimes contains timestamps that are many years in the future. It prints the following output:\r\n```\r\n...\r\n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \r\n13 18446744073 2554-07-21 16:34:33               \r\n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \r\n13 18446744073 2554-07-21 16:34:33               \r\n13 18446744073 2554-07-21 16:34:33               \r\n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \r\n13 18446744073 2554-07-21 16:34:33               \r\n13 18446744073 2554-07-21 16:34:33                                                                                                                                                                  \r\n13 18446744073 2554-07-21 16:34:33               \r\n13 18446744073 2554-07-21 16:34:33               \r\n13 18446744073 2554-07-21 16:34:33\r\n...\r\n```\r\n\r\nThe issue was originally reported at https://github.com/tensorpack/tensorpack/issues/819.\r\nThe issue is more likely to happen after running about 50 steps.\r\nThe issue seems to disappear when training on one GPU, or training a small model."}