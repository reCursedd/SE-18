{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/393351339", "html_url": "https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-393351339", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12859", "id": 393351339, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzM1MTMzOQ==", "user": {"login": "duckworthd", "id": 626236, "node_id": "MDQ6VXNlcjYyNjIzNg==", "avatar_url": "https://avatars2.githubusercontent.com/u/626236?v=4", "gravatar_id": "", "url": "https://api.github.com/users/duckworthd", "html_url": "https://github.com/duckworthd", "followers_url": "https://api.github.com/users/duckworthd/followers", "following_url": "https://api.github.com/users/duckworthd/following{/other_user}", "gists_url": "https://api.github.com/users/duckworthd/gists{/gist_id}", "starred_url": "https://api.github.com/users/duckworthd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/duckworthd/subscriptions", "organizations_url": "https://api.github.com/users/duckworthd/orgs", "repos_url": "https://api.github.com/users/duckworthd/repos", "events_url": "https://api.github.com/users/duckworthd/events{/privacy}", "received_events_url": "https://api.github.com/users/duckworthd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-30T23:26:33Z", "updated_at": "2018-05-30T23:28:23Z", "author_association": "MEMBER", "body_html": "<p>A combination of saveable iterator state,</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create dataset with placeholder.</span>\n    placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">5</span>])\n    dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(placeholder)\n    <span class=\"pl-k\">if</span> shuffle:\n      dataset <span class=\"pl-k\">=</span> dataset.apply(tf.contrib.data.shuffle_and_repeat(<span class=\"pl-c1\">100</span>, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>))\n    <span class=\"pl-k\">else</span>:\n      dataset <span class=\"pl-k\">=</span> dataset.repeat()\n    dataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">32</span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Construct initialization function and a minibatch.</span>\n    iterator <span class=\"pl-k\">=</span> dataset.make_initializable_iterator()\n    x <span class=\"pl-k\">=</span> iterator.get_next()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Add dataset serialization ops to graph.</span>\n    saveable <span class=\"pl-k\">=</span> tf.contrib.data.make_saveable_from_iterator(iterator)\n    tf.add_to_collection(tf.GraphKeys.<span class=\"pl-c1\">SAVEABLE_OBJECTS</span>, saveable)</pre></div>\n<p>and <code>init_fn</code></p>\n<div class=\"highlight highlight-source-python\"><pre>  <span class=\"pl-k\">def</span> <span class=\"pl-en\">init_fn</span>(<span class=\"pl-smi\">scaffold</span>, <span class=\"pl-smi\">session</span>):\n    session.run(iterator.initializer, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{placeholder: np.random.randn(<span class=\"pl-c1\">10</span>, <span class=\"pl-c1\">5</span>)})\n\n  <span class=\"pl-k\">with</span> tf.MonitoredTrainingSession(\n      <span class=\"pl-v\">checkpoint_dir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">...</span>,\n      <span class=\"pl-v\">scaffold</span><span class=\"pl-k\">=</span>tf.train.Scaffold(<span class=\"pl-v\">init_fn</span><span class=\"pl-k\">=</span>init_fn)) <span class=\"pl-k\">as</span> sess:\n        sess.run(x)</pre></div>\n<p>works for me. It guarantees the iterator is initialized before the first checkpoint is created, and ensures state is restored from disk afterwards.</p>\n<p>Pay special attention to the use of <code>tf.contrib.data.shuffle_and_repeat()</code> instead of <code>Dataset.shuffle()</code>. Only the former is guaranteed to be serializable.</p>", "body_text": "A combination of saveable iterator state,\n    # Create dataset with placeholder.\n    placeholder = tf.placeholder(tf.float32, [None, 5])\n    dataset = tf.data.Dataset.from_tensor_slices(placeholder)\n    if shuffle:\n      dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(100, seed=0))\n    else:\n      dataset = dataset.repeat()\n    dataset = dataset.batch(32)\n\n    # Construct initialization function and a minibatch.\n    iterator = dataset.make_initializable_iterator()\n    x = iterator.get_next()\n\n    # Add dataset serialization ops to graph.\n    saveable = tf.contrib.data.make_saveable_from_iterator(iterator)\n    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\nand init_fn\n  def init_fn(scaffold, session):\n    session.run(iterator.initializer, feed_dict={placeholder: np.random.randn(10, 5)})\n\n  with tf.MonitoredTrainingSession(\n      checkpoint_dir=...,\n      scaffold=tf.train.Scaffold(init_fn=init_fn)) as sess:\n        sess.run(x)\nworks for me. It guarantees the iterator is initialized before the first checkpoint is created, and ensures state is restored from disk afterwards.\nPay special attention to the use of tf.contrib.data.shuffle_and_repeat() instead of Dataset.shuffle(). Only the former is guaranteed to be serializable.", "body": "A combination of saveable iterator state,\r\n\r\n```python\r\n    # Create dataset with placeholder.\r\n    placeholder = tf.placeholder(tf.float32, [None, 5])\r\n    dataset = tf.data.Dataset.from_tensor_slices(placeholder)\r\n    if shuffle:\r\n      dataset = dataset.apply(tf.contrib.data.shuffle_and_repeat(100, seed=0))\r\n    else:\r\n      dataset = dataset.repeat()\r\n    dataset = dataset.batch(32)\r\n\r\n    # Construct initialization function and a minibatch.\r\n    iterator = dataset.make_initializable_iterator()\r\n    x = iterator.get_next()\r\n\r\n    # Add dataset serialization ops to graph.\r\n    saveable = tf.contrib.data.make_saveable_from_iterator(iterator)\r\n    tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)\r\n```\r\n\r\nand `init_fn`\r\n\r\n```python\r\n  def init_fn(scaffold, session):\r\n    session.run(iterator.initializer, feed_dict={placeholder: np.random.randn(10, 5)})\r\n\r\n  with tf.MonitoredTrainingSession(\r\n      checkpoint_dir=...,\r\n      scaffold=tf.train.Scaffold(init_fn=init_fn)) as sess:\r\n        sess.run(x)\r\n```\r\n\r\nworks for me. It guarantees the iterator is initialized before the first checkpoint is created, and ensures state is restored from disk afterwards. \r\n\r\nPay special attention to the use of `tf.contrib.data.shuffle_and_repeat()` instead of `Dataset.shuffle()`. Only the former is guaranteed to be serializable."}