{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12859", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12859/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12859/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12859/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12859", "id": 255772759, "node_id": "MDU6SXNzdWUyNTU3NzI3NTk=", "number": 12859, "title": "FailedPreconditionError when restoring initializable_iterator with Scaffold in a MonitoredTrainingSession for the second time.", "user": {"login": "MtDersvan", "id": 7069222, "node_id": "MDQ6VXNlcjcwNjkyMjI=", "avatar_url": "https://avatars3.githubusercontent.com/u/7069222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MtDersvan", "html_url": "https://github.com/MtDersvan", "followers_url": "https://api.github.com/users/MtDersvan/followers", "following_url": "https://api.github.com/users/MtDersvan/following{/other_user}", "gists_url": "https://api.github.com/users/MtDersvan/gists{/gist_id}", "starred_url": "https://api.github.com/users/MtDersvan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MtDersvan/subscriptions", "organizations_url": "https://api.github.com/users/MtDersvan/orgs", "repos_url": "https://api.github.com/users/MtDersvan/repos", "events_url": "https://api.github.com/users/MtDersvan/events{/privacy}", "received_events_url": "https://api.github.com/users/MtDersvan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 32, "created_at": "2017-09-06T23:37:27Z", "updated_at": "2018-09-20T07:53:19Z", "closed_at": "2017-09-07T12:25:52Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: +</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.3.0-rc2-20-g0787eee 1.3.0</li>\n<li><strong>Python version</strong>: Python 3.5.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: -</li>\n<li><strong>CUDA/cuDNN version</strong>: -</li>\n<li><strong>GPU model and memory</strong>: -</li>\n<li><strong>Exact command to reproduce</strong>: -</li>\n</ul>\n<h3>Context:</h3>\n<p>Using <code>initializable_iterator</code> with <code>MonitoredTrainingSession</code> because there are stateful <code>lookup_ops.index_table_from_tensor()</code> lookup tables that don't work with <code>one_shot_iterator</code>.</p>\n<p><code>initializable_iterator</code> is initialized with a tf.train.Scaffold():</p>\n<pre><code>Scaffold = tf.train.Scaffold(\n        init_op=control_flow_ops.group(variables.global_variables_initializer(),\n                                       resources.initialize_resources(resources.shared_resources()),\n                                       iter_init_op))\n\nwith tf.train.MonitoredTrainingSession(\n    master=server.target,\n    is_chief=hps.is_chief,\n    scaffold=Scaffold,\n    config=config,\n    checkpoint_dir=hps.checkpoint_dir,\n    hooks=hooks\n) as mon_sess:\n                ...\n</code></pre>\n<p>Where <code>iter_init_op</code> is equivalent to <code>iterator.initializer</code>.</p>\n<h3>Problem</h3>\n<p>Upper mentioned initialization works properly when the model is initialized and created for the <strong>first</strong> time and some initial training can be done without problems.</p>\n<p>If chief worker crashes or is shut down purposefully, after <strong>restarting</strong> <code>MonitoredTrainingSession</code> shows following error as if iterator is not initialized:</p>\n<pre><code>FailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element...\n</code></pre>\n<h3>Workaround</h3>\n<p>Right now the only solution that works for is to run initialization internally using <code>_coordinated_creator.tf_sess.run</code>:</p>\n<pre><code>mon_sess._coordinated_creator.tf_sess.run(iter_init_op)\n</code></pre>\n<p>This doesn't look like an intended use.</p>\n<h3>Statement:</h3>\n<p>This doesn't seem as an intended behaviour.<br>\nWhat is a better way to use <code>initializable_iterator</code> with <code>MonitoredTrainingSession</code> or <code>lookup_ops.index_table_from_tensor</code> with <code>one_shot_iterator</code>?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): +\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.3.0-rc2-20-g0787eee 1.3.0\nPython version: Python 3.5.1\nBazel version (if compiling from source): -\nCUDA/cuDNN version: -\nGPU model and memory: -\nExact command to reproduce: -\n\nContext:\nUsing initializable_iterator with MonitoredTrainingSession because there are stateful lookup_ops.index_table_from_tensor() lookup tables that don't work with one_shot_iterator.\ninitializable_iterator is initialized with a tf.train.Scaffold():\nScaffold = tf.train.Scaffold(\n        init_op=control_flow_ops.group(variables.global_variables_initializer(),\n                                       resources.initialize_resources(resources.shared_resources()),\n                                       iter_init_op))\n\nwith tf.train.MonitoredTrainingSession(\n    master=server.target,\n    is_chief=hps.is_chief,\n    scaffold=Scaffold,\n    config=config,\n    checkpoint_dir=hps.checkpoint_dir,\n    hooks=hooks\n) as mon_sess:\n                ...\n\nWhere iter_init_op is equivalent to iterator.initializer.\nProblem\nUpper mentioned initialization works properly when the model is initialized and created for the first time and some initial training can be done without problems.\nIf chief worker crashes or is shut down purposefully, after restarting MonitoredTrainingSession shows following error as if iterator is not initialized:\nFailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element...\n\nWorkaround\nRight now the only solution that works for is to run initialization internally using _coordinated_creator.tf_sess.run:\nmon_sess._coordinated_creator.tf_sess.run(iter_init_op)\n\nThis doesn't look like an intended use.\nStatement:\nThis doesn't seem as an intended behaviour.\nWhat is a better way to use initializable_iterator with MonitoredTrainingSession or lookup_ops.index_table_from_tensor with one_shot_iterator?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: +\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.3.0-rc2-20-g0787eee 1.3.0\r\n- **Python version**: Python 3.5.1\r\n- **Bazel version (if compiling from source)**: -\r\n- **CUDA/cuDNN version**: - \r\n- **GPU model and memory**: -\r\n- **Exact command to reproduce**: -\r\n\r\n### Context:\r\nUsing `initializable_iterator` with `MonitoredTrainingSession` because there are stateful `lookup_ops.index_table_from_tensor()` lookup tables that don't work with `one_shot_iterator`.\r\n\r\n`initializable_iterator` is initialized with a tf.train.Scaffold():\r\n```\r\nScaffold = tf.train.Scaffold(\r\n        init_op=control_flow_ops.group(variables.global_variables_initializer(),\r\n                                       resources.initialize_resources(resources.shared_resources()),\r\n                                       iter_init_op))\r\n\r\nwith tf.train.MonitoredTrainingSession(\r\n    master=server.target,\r\n    is_chief=hps.is_chief,\r\n    scaffold=Scaffold,\r\n    config=config,\r\n    checkpoint_dir=hps.checkpoint_dir,\r\n    hooks=hooks\r\n) as mon_sess:\r\n                ...\r\n```\r\nWhere `iter_init_op` is equivalent to `iterator.initializer`.\r\n\r\n### Problem\r\nUpper mentioned initialization works properly when the model is initialized and created for the **first** time and some initial training can be done without problems.\r\n\r\nIf chief worker crashes or is shut down purposefully, after **restarting** `MonitoredTrainingSession` shows following error as if iterator is not initialized:\r\n```\r\nFailedPreconditionError (see above for traceback): GetNext() failed because the iterator has not been initialized. Ensure that you have run the initializer operation for this iterator before getting the next element...\r\n```\r\n\r\n### Workaround\r\nRight now the only solution that works for is to run initialization internally using `_coordinated_creator.tf_sess.run`:\r\n```\r\nmon_sess._coordinated_creator.tf_sess.run(iter_init_op)\r\n```\r\nThis doesn't look like an intended use.\r\n\r\n### Statement:\r\nThis doesn't seem as an intended behaviour.\r\nWhat is a better way to use `initializable_iterator` with `MonitoredTrainingSession` or `lookup_ops.index_table_from_tensor` with `one_shot_iterator`?"}