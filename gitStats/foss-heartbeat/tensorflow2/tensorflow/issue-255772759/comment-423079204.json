{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423079204", "html_url": "https://github.com/tensorflow/tensorflow/issues/12859#issuecomment-423079204", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12859", "id": 423079204, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzA3OTIwNA==", "user": {"login": "jiawei6636", "id": 24591769, "node_id": "MDQ6VXNlcjI0NTkxNzY5", "avatar_url": "https://avatars2.githubusercontent.com/u/24591769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiawei6636", "html_url": "https://github.com/jiawei6636", "followers_url": "https://api.github.com/users/jiawei6636/followers", "following_url": "https://api.github.com/users/jiawei6636/following{/other_user}", "gists_url": "https://api.github.com/users/jiawei6636/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiawei6636/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiawei6636/subscriptions", "organizations_url": "https://api.github.com/users/jiawei6636/orgs", "repos_url": "https://api.github.com/users/jiawei6636/repos", "events_url": "https://api.github.com/users/jiawei6636/events{/privacy}", "received_events_url": "https://api.github.com/users/jiawei6636/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-20T07:53:19Z", "updated_at": "2018-09-20T07:53:19Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Related to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1968539\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dongjk\">@dongjk</a>'s question/problem, I have an explicit example of loading tfrecords with the <code>Dataset</code> API on a modified version of the <a href=\"https://github.com/tensorflow/models/tree/master/research/resnet\">tensorflow/models/research/resnet</a> example, with <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19293677\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ispirmustafa\">@ispirmustafa</a>'s suggestion of using a hook (thank you for that suggestion btw, it solved a very long headache <g-emoji class=\"g-emoji\" alias=\"smiley\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f603.png\">\ud83d\ude03</g-emoji>). Hopefully it helps \"connect the dots\" in case anyone like me is having trouble piecing all of this together and making a working example with their own data. I really like the <code>Dataset</code> version of doing things, and am looking forward to see how it develops further. Thanks!</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c1\">...</span>\n\ntrain_tfrecord_path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/path/to/my/train-000.tfrecord<span class=\"pl-pds\">'</span></span>\ntrain_tfrecord_filenames <span class=\"pl-k\">=</span> [train_tfrecord_path] <span class=\"pl-c\"><span class=\"pl-c\">#</span> Add more to the list if you need</span>\nval_tfrecord_path <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/path/to/my/val-000.tfrecord<span class=\"pl-pds\">'</span></span>\nval_tfrecord_filenames <span class=\"pl-k\">=</span> [val_tfrecord_path] <span class=\"pl-c\"><span class=\"pl-c\">#</span> Add more to the list if you need</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Make a placeholder, we will use a feed_dict to fill this with the filenames of the tfrecords</span>\nfilenames_placeholder <span class=\"pl-k\">=</span> tf.placeholder(tf.string, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the `Dataset` and apply some preprocessing</span>\ndataset <span class=\"pl-k\">=</span> tf.data.TFRecordDataset(filenames_placeholder)\ndataset <span class=\"pl-k\">=</span> dataset.map(_my_parse_function, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\ndataset <span class=\"pl-k\">=</span> dataset.repeat()\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> This method of batching assures a fixed batch size and avoids problems</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> of unknown shapes [?, num_classes] (for labels), otherwise can use `dataset.batch(batch_size)`</span>\ndataset <span class=\"pl-k\">=</span> dataset.apply(tf.contrib.data.batch_and_drop_remainder(<span class=\"pl-c1\">FLAGS</span>.batch_size))\ndataset <span class=\"pl-k\">=</span> dataset.prefetch(<span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the `Iterator`, `Initializer` and get the images and labels for building the model</span>\niterator <span class=\"pl-k\">=</span> tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\ninit_train <span class=\"pl-k\">=</span> iterator.make_initializer(dataset)\nimages, labels <span class=\"pl-k\">=</span> iterator.get_next()\n\n<span class=\"pl-c1\">...</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create the hook to initialize the Iterator with the filenames_list, credit to @ispirmustafa</span>\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">_DatasetInitializerHook</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">train</span>.<span class=\"pl-e\">SessionRunHook</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">initializer</span>, <span class=\"pl-smi\">filenames_list</span>):\n        <span class=\"pl-c1\">self</span>._initializer <span class=\"pl-k\">=</span> initializer\n        <span class=\"pl-c1\">self</span>._filenames_list <span class=\"pl-k\">=</span> filenames_list\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">begin</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-k\">pass</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">after_create_session</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">session</span>, <span class=\"pl-smi\">coord</span>):\n        <span class=\"pl-k\">del</span> coord\n        session.run(<span class=\"pl-c1\">self</span>._initializer, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{filenames_placeholder: <span class=\"pl-c1\">self</span>._filenames_list})\n\n<span class=\"pl-c1\">...</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> define the training function. Modified version of wideresnet cifar example from &lt;https://github.com/tensorflow/models/tree/master/research/resnet&gt;</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\">hps</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> `images` and `labels` are the previous output from our call to `iterator.get_next()`</span>\n    model <span class=\"pl-k\">=</span> resnet_model.ResNet(hps, images, labels, <span class=\"pl-c1\">FLAGS</span>.mode, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.batch_size)\n    model.build_graph()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Make instance of `_DatasetInitializerHook`</span>\n    initializer_hook <span class=\"pl-k\">=</span> _DatasetInitializerHook(init_train, train_tfrecord_filenames)\n    <span class=\"pl-c1\">...</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> After everything else is prepared, prepend the `initializer_hook` to the hooks in `MonitoredTrainingSession`</span>\n    <span class=\"pl-k\">with</span> tf.train.MonitoredTrainingSession(\n            <span class=\"pl-v\">checkpoint_dir</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.log_root,\n            <span class=\"pl-v\">hooks</span><span class=\"pl-k\">=</span>[initializer_hook, logging_hook, _LearningRateSetterHook()],\n            <span class=\"pl-v\">chief_only_hooks</span><span class=\"pl-k\">=</span>[summary_hook],\n            <span class=\"pl-v\">save_summaries_steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>,\n            <span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>tf.ConfigProto(<span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)) <span class=\"pl-k\">as</span> mon_sess:\n        <span class=\"pl-k\">while</span> <span class=\"pl-k\">not</span> mon_sess.should_stop():\n            mon_sess.run(model.train_op)\n\n<span class=\"pl-c1\">...</span>.\n\n<span class=\"pl-k\">def</span> <span class=\"pl-c1\">eval</span>(<span class=\"pl-smi\">hps</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Modify `eval` the same way, but use an initializer for the validation dataset, or feed `val_filenames`</span>\n\n    <span class=\"pl-c1\">...</span>\n\n<span class=\"pl-c1\">...</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> For completeness, the rest of the framework from the tf example</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Note: I removed the gpu and batch_size handling from the example, modify the network accordingly.</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">_</span>):\n    hps <span class=\"pl-k\">=</span> resnet_model.HParams(<span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.batch_size,\n                               <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.num_classes,\n                               <span class=\"pl-v\">min_lrn_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0001</span>,\n                               <span class=\"pl-v\">lrn_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>,\n                               <span class=\"pl-v\">num_residual_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>,\n                               <span class=\"pl-v\">use_bottleneck</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n                               <span class=\"pl-v\">weight_decay_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0002</span>,\n                               <span class=\"pl-v\">relu_leakiness</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>,\n                               <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>mom<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">FLAGS</span>.mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>:\n        train(hps)\n    <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">FLAGS</span>.mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>eval<span class=\"pl-pds\">'</span></span>:\n        evaluate(hps)\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    tf.logging.set_verbosity(tf.logging.<span class=\"pl-c1\">INFO</span>)\n    tf.app.run()</pre></div>\n</blockquote>\n<p>you are very good!</p>", "body_text": "Related to @dongjk's question/problem, I have an explicit example of loading tfrecords with the Dataset API on a modified version of the tensorflow/models/research/resnet example, with @ispirmustafa's suggestion of using a hook (thank you for that suggestion btw, it solved a very long headache \ud83d\ude03). Hopefully it helps \"connect the dots\" in case anyone like me is having trouble piecing all of this together and making a working example with their own data. I really like the Dataset version of doing things, and am looking forward to see how it develops further. Thanks!\nimport tensorflow as tf\n\n...\n\ntrain_tfrecord_path = '/path/to/my/train-000.tfrecord'\ntrain_tfrecord_filenames = [train_tfrecord_path] # Add more to the list if you need\nval_tfrecord_path = '/path/to/my/val-000.tfrecord'\nval_tfrecord_filenames = [val_tfrecord_path] # Add more to the list if you need\n\n#Make a placeholder, we will use a feed_dict to fill this with the filenames of the tfrecords\nfilenames_placeholder = tf.placeholder(tf.string, shape=[None])\n\n# Create the `Dataset` and apply some preprocessing\ndataset = tf.data.TFRecordDataset(filenames_placeholder)\ndataset = dataset.map(_my_parse_function, num_parallel_calls=4)\ndataset = dataset.repeat()\n# This method of batching assures a fixed batch size and avoids problems\n# of unknown shapes [?, num_classes] (for labels), otherwise can use `dataset.batch(batch_size)`\ndataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(FLAGS.batch_size))\ndataset = dataset.prefetch(1)\n\n# Create the `Iterator`, `Initializer` and get the images and labels for building the model\niterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\ninit_train = iterator.make_initializer(dataset)\nimages, labels = iterator.get_next()\n\n...\n\n# Create the hook to initialize the Iterator with the filenames_list, credit to @ispirmustafa\nclass _DatasetInitializerHook(tf.train.SessionRunHook):\n    def __init__(self, initializer, filenames_list):\n        self._initializer = initializer\n        self._filenames_list = filenames_list\n    def begin(self):\n        pass\n    def after_create_session(self, session, coord):\n        del coord\n        session.run(self._initializer, feed_dict={filenames_placeholder: self._filenames_list})\n\n...\n\n# define the training function. Modified version of wideresnet cifar example from <https://github.com/tensorflow/models/tree/master/research/resnet>\ndef train(hps):\n    # `images` and `labels` are the previous output from our call to `iterator.get_next()`\n    model = resnet_model.ResNet(hps, images, labels, FLAGS.mode, batch_size=FLAGS.batch_size)\n    model.build_graph()\n\n    # Make instance of `_DatasetInitializerHook`\n    initializer_hook = _DatasetInitializerHook(init_train, train_tfrecord_filenames)\n    ...\n    # After everything else is prepared, prepend the `initializer_hook` to the hooks in `MonitoredTrainingSession`\n    with tf.train.MonitoredTrainingSession(\n            checkpoint_dir=FLAGS.log_root,\n            hooks=[initializer_hook, logging_hook, _LearningRateSetterHook()],\n            chief_only_hooks=[summary_hook],\n            save_summaries_steps=0,\n            config=tf.ConfigProto(allow_soft_placement=True)) as mon_sess:\n        while not mon_sess.should_stop():\n            mon_sess.run(model.train_op)\n\n....\n\ndef eval(hps):\n    # Modify `eval` the same way, but use an initializer for the validation dataset, or feed `val_filenames`\n\n    ...\n\n...\n\n# For completeness, the rest of the framework from the tf example\n# Note: I removed the gpu and batch_size handling from the example, modify the network accordingly.\ndef main(_):\n    hps = resnet_model.HParams(batch_size=FLAGS.batch_size,\n                               num_classes=FLAGS.num_classes,\n                               min_lrn_rate=0.0001,\n                               lrn_rate=0.1,\n                               num_residual_units=5,\n                               use_bottleneck=False,\n                               weight_decay_rate=0.0002,\n                               relu_leakiness=0.1,\n                               optimizer='mom')\n\n    if FLAGS.mode == 'train':\n        train(hps)\n    elif FLAGS.mode == 'eval':\n        evaluate(hps)\n\nif __name__ == '__main__':\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.app.run()\n\nyou are very good!", "body": "> Related to @dongjk's question/problem, I have an explicit example of loading tfrecords with the `Dataset` API on a modified version of the [tensorflow/models/research/resnet](https://github.com/tensorflow/models/tree/master/research/resnet) example, with @ispirmustafa's suggestion of using a hook (thank you for that suggestion btw, it solved a very long headache \ud83d\ude03). Hopefully it helps \"connect the dots\" in case anyone like me is having trouble piecing all of this together and making a working example with their own data. I really like the `Dataset` version of doing things, and am looking forward to see how it develops further. Thanks!\r\n> \r\n> ```python\r\n> import tensorflow as tf\r\n> \r\n> ...\r\n> \r\n> train_tfrecord_path = '/path/to/my/train-000.tfrecord'\r\n> train_tfrecord_filenames = [train_tfrecord_path] # Add more to the list if you need\r\n> val_tfrecord_path = '/path/to/my/val-000.tfrecord'\r\n> val_tfrecord_filenames = [val_tfrecord_path] # Add more to the list if you need\r\n> \r\n> #Make a placeholder, we will use a feed_dict to fill this with the filenames of the tfrecords\r\n> filenames_placeholder = tf.placeholder(tf.string, shape=[None])\r\n> \r\n> # Create the `Dataset` and apply some preprocessing\r\n> dataset = tf.data.TFRecordDataset(filenames_placeholder)\r\n> dataset = dataset.map(_my_parse_function, num_parallel_calls=4)\r\n> dataset = dataset.repeat()\r\n> # This method of batching assures a fixed batch size and avoids problems\r\n> # of unknown shapes [?, num_classes] (for labels), otherwise can use `dataset.batch(batch_size)`\r\n> dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(FLAGS.batch_size))\r\n> dataset = dataset.prefetch(1)\r\n> \r\n> # Create the `Iterator`, `Initializer` and get the images and labels for building the model\r\n> iterator = tf.data.Iterator.from_structure(dataset.output_types, dataset.output_shapes)\r\n> init_train = iterator.make_initializer(dataset)\r\n> images, labels = iterator.get_next()\r\n> \r\n> ...\r\n> \r\n> # Create the hook to initialize the Iterator with the filenames_list, credit to @ispirmustafa\r\n> class _DatasetInitializerHook(tf.train.SessionRunHook):\r\n>     def __init__(self, initializer, filenames_list):\r\n>         self._initializer = initializer\r\n>         self._filenames_list = filenames_list\r\n>     def begin(self):\r\n>         pass\r\n>     def after_create_session(self, session, coord):\r\n>         del coord\r\n>         session.run(self._initializer, feed_dict={filenames_placeholder: self._filenames_list})\r\n> \r\n> ...\r\n> \r\n> # define the training function. Modified version of wideresnet cifar example from <https://github.com/tensorflow/models/tree/master/research/resnet>\r\n> def train(hps):\r\n>     # `images` and `labels` are the previous output from our call to `iterator.get_next()`\r\n>     model = resnet_model.ResNet(hps, images, labels, FLAGS.mode, batch_size=FLAGS.batch_size)\r\n>     model.build_graph()\r\n> \r\n>     # Make instance of `_DatasetInitializerHook`\r\n>     initializer_hook = _DatasetInitializerHook(init_train, train_tfrecord_filenames)\r\n>     ...\r\n>     # After everything else is prepared, prepend the `initializer_hook` to the hooks in `MonitoredTrainingSession`\r\n>     with tf.train.MonitoredTrainingSession(\r\n>             checkpoint_dir=FLAGS.log_root,\r\n>             hooks=[initializer_hook, logging_hook, _LearningRateSetterHook()],\r\n>             chief_only_hooks=[summary_hook],\r\n>             save_summaries_steps=0,\r\n>             config=tf.ConfigProto(allow_soft_placement=True)) as mon_sess:\r\n>         while not mon_sess.should_stop():\r\n>             mon_sess.run(model.train_op)\r\n> \r\n> ....\r\n> \r\n> def eval(hps):\r\n>     # Modify `eval` the same way, but use an initializer for the validation dataset, or feed `val_filenames`\r\n> \r\n>     ...\r\n> \r\n> ...\r\n> \r\n> # For completeness, the rest of the framework from the tf example\r\n> # Note: I removed the gpu and batch_size handling from the example, modify the network accordingly.\r\n> def main(_):\r\n>     hps = resnet_model.HParams(batch_size=FLAGS.batch_size,\r\n>                                num_classes=FLAGS.num_classes,\r\n>                                min_lrn_rate=0.0001,\r\n>                                lrn_rate=0.1,\r\n>                                num_residual_units=5,\r\n>                                use_bottleneck=False,\r\n>                                weight_decay_rate=0.0002,\r\n>                                relu_leakiness=0.1,\r\n>                                optimizer='mom')\r\n> \r\n>     if FLAGS.mode == 'train':\r\n>         train(hps)\r\n>     elif FLAGS.mode == 'eval':\r\n>         evaluate(hps)\r\n> \r\n> if __name__ == '__main__':\r\n>     tf.logging.set_verbosity(tf.logging.INFO)\r\n>     tf.app.run()\r\n> ```\r\n\r\nyou are very good!"}