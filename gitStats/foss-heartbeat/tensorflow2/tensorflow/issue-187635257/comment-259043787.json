{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259043787", "html_url": "https://github.com/tensorflow/tensorflow/issues/5437#issuecomment-259043787", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5437", "id": 259043787, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTA0Mzc4Nw==", "user": {"login": "s4sarath", "id": 10637096, "node_id": "MDQ6VXNlcjEwNjM3MDk2", "avatar_url": "https://avatars0.githubusercontent.com/u/10637096?v=4", "gravatar_id": "", "url": "https://api.github.com/users/s4sarath", "html_url": "https://github.com/s4sarath", "followers_url": "https://api.github.com/users/s4sarath/followers", "following_url": "https://api.github.com/users/s4sarath/following{/other_user}", "gists_url": "https://api.github.com/users/s4sarath/gists{/gist_id}", "starred_url": "https://api.github.com/users/s4sarath/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/s4sarath/subscriptions", "organizations_url": "https://api.github.com/users/s4sarath/orgs", "repos_url": "https://api.github.com/users/s4sarath/repos", "events_url": "https://api.github.com/users/s4sarath/events{/privacy}", "received_events_url": "https://api.github.com/users/s4sarath/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-08T04:36:27Z", "updated_at": "2016-11-08T04:36:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> - I will provide a detailed explanation of what the code is and where I am getting the error .<br>\nWhat I am trying to do is , I will get a 3 x 10 matrix in we_project and I am having 3 x 10 x 5 matrix in attention_input_mod . I need to do a matmul of each row of 3 x 10 matrix to corresponding matrix batch in 3 x 10 x 5 . That is , first row 0f 3 x 10 will take a  matmul with first batch of 3 x 10 x 5 ( [0, : ,:]) and so on . For that , I am making use of tf.while_loop inside the other loop which is mostly a duplication of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=12167999\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alrojo\">@alrojo</a> code . So , I use sub_initial and sub_input to hold the 2d and 3d matrix respectively and inside sub_decoder function I will try to iterate until I reach batch_len ( 3 by default ) and I read each row and matrix from sub_initial and sub_input respectively and then trying to store the result in temp_holder TensorArray . But getting the following error</p>\n<p>AlreadyExistsError: Resource _tensor_arrays/temp_holder/N10tensorflow11TensorArrayE</p>\n<pre><code>target_dims = 8\nattention_dims = 10\ninput_max_len =  5\nattn_len = 3\nmax_sequence_length = 3\nnum_units = 10\nbatch_len = 3\n\nweight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n\nattention_input = tf.convert_to_tensor(np.random.rand(3,5,10).astype(np.float32)) ##### consider it as RNN encoder output\nattention_input_mod = tf.transpose(attention_input , [0,2,1])\ninitial_state = tf.convert_to_tensor(np.random.rand(3,10).astype(np.float32)) ######## Last states of 3 batches of RNN\ntarget_input = tf.convert_to_tensor(np.random.rand(3,3,8).astype(np.float32))   ###### target input\ninputs = tf.transpose(target_input, perm=[1, 0, 2])\n\nvar = tf.get_variable # for ease of use\n\nW_z_x = var('W_z_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_z_h = var('W_z_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_z = var('b_z', shape=[num_units], initializer=weight_initializer)\nW_r_x = var('W_r_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_r_h = var('W_r_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_r = var('b_r', shape=[num_units], initializer=weight_initializer)\nW_c_x = var('W_c_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_c_h = var('W_c_h', shape=[num_units, num_units], initializer = weight_initializer)\nb_c = var('b_c', shape=[num_units], initializer=weight_initializer)\nmiddle_matrix = var('middle', shape=[num_units, num_units], initializer = weight_initializer)\n\n\n###### Reading the first element in target of each batch and creating a new state\ninput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, name = 'input_ta')\ninput_ta = input_ta.unpack(inputs)\n# calculate the GRU\ntime = tf.constant(0)\nx_t = input_ta.read(time)\nz = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(initial_state, W_z_h) + b_z) # update gate\nr = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(initial_state, W_r_h) + b_r) # reset gate\nc = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*initial_state, W_c_h) + b_c) # proposed new state\nnew_state = (1-z)*c + z*initial_state # new state\ninitial_state = new_state\n\ndef decoder_cond(time, state, output_ta_t, attention_tracker):\n    return tf.less(time, max_sequence_length)\n\n\ndef decoder_body_builder(feedback=False):\n    def decoder_body(time, old_state, output_ta_t, attention_tracker):\n        if feedback:\n            def from_previous():\n                prev_1 = tf.matmul(old_state, W_out) + b_out\n                return tf.gather(embeddings, tf.argmax(prev_1, 1))\n            x_t = tf.cond(tf.greater(time, 0), from_previous, lambda: input_ta.read(0))\n        else:\n            x_t = input_ta.read(time)\n\n        # calculate the GRU\n\n\n\n        def sub_decoder_cond(sub_time,temp_holder_):\n                return tf.less(sub_time, 3) \n\n        def sub_decoder_body_builder():\n\n            def sub_decoder_body(sub_time ,temp_holder_t):\n                sub_x_t = tf.reshape(sub_initial_.read(sub_time) , [1,-1])\n                sub_i_t = sub_input.read(sub_time)\n                sub_res = tf.matmul(sub_x_t, sub_i_t)\n                temp_holder_t.write(sub_time, sub_res)\n\n                return(sub_time+1, temp_holder_t )\n            return sub_decoder_body\n\n\n        we_project = tf.tanh(tf.matmul( initial_state , middle_matrix ))\n\n        sub_initial_ = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True ,tensor_array_name='sub_initial')\n        sub_input = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, tensor_array_name = 'sub_input')\n        temp_holder = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True , tensor_array_name = 'temp_holder'  )\n\n        sub_initial_ = sub_initial_.unpack(we_project)\n        sub_input = sub_input.unpack(attention_input_mod)\n        sub_time = tf.constant(0)\n\n        sub_loop_vars = [sub_time, temp_holder]\n\n        _, temp_holder_out = tf.while_loop(sub_decoder_cond,\n                                       sub_decoder_body_builder(),\n                                       sub_loop_vars,\n                                       swap_memory=False)\n\n\n\n        alpha_time = temp_holder_out.pack()\n        # temp_holder.close()\n        alpha = alpha_time\n        alpha_softmax = alpha\n        # alpha_softmax = tf.nn.softmax(alpha)\n        z = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(old_state, W_z_h) + b_z) # update gate\n        r = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(old_state, W_r_h) + b_r) # reset gate\n        c = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*old_state, W_c_h) + b_c) # proposed new state\n        new_state = (1-z)*c + z*old_state # new state\n\n        # writing output\n        output_ta_t = output_ta_t.write(time+1, new_state)\n        attention_tracker = attention_tracker.write(time, alpha_softmax)\n        # context = tf.reduce_sum(tf.expand_dims(alpha_softmax, 2) * attention_input, [1])\n\n\n        return (time + 1, new_state, output_ta_t, attention_tracker)\n    return decoder_body\n\n\noutput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\nattention_tracker = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\ntime = tf.constant(0)\nloop_vars = [time, initial_state, output_ta, attention_tracker]\n\n_, state, output_ta, attention_tracker_holder = tf.while_loop(decoder_cond,\n                                               decoder_body_builder(),\n                                               loop_vars,\n                                               swap_memory=False)\nA = attention_tracker_holder.pack()\nO = output_ta.pack()\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n</code></pre>", "body_text": "@aselle - I will provide a detailed explanation of what the code is and where I am getting the error .\nWhat I am trying to do is , I will get a 3 x 10 matrix in we_project and I am having 3 x 10 x 5 matrix in attention_input_mod . I need to do a matmul of each row of 3 x 10 matrix to corresponding matrix batch in 3 x 10 x 5 . That is , first row 0f 3 x 10 will take a  matmul with first batch of 3 x 10 x 5 ( [0, : ,:]) and so on . For that , I am making use of tf.while_loop inside the other loop which is mostly a duplication of @alrojo code . So , I use sub_initial and sub_input to hold the 2d and 3d matrix respectively and inside sub_decoder function I will try to iterate until I reach batch_len ( 3 by default ) and I read each row and matrix from sub_initial and sub_input respectively and then trying to store the result in temp_holder TensorArray . But getting the following error\nAlreadyExistsError: Resource _tensor_arrays/temp_holder/N10tensorflow11TensorArrayE\ntarget_dims = 8\nattention_dims = 10\ninput_max_len =  5\nattn_len = 3\nmax_sequence_length = 3\nnum_units = 10\nbatch_len = 3\n\nweight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n\nattention_input = tf.convert_to_tensor(np.random.rand(3,5,10).astype(np.float32)) ##### consider it as RNN encoder output\nattention_input_mod = tf.transpose(attention_input , [0,2,1])\ninitial_state = tf.convert_to_tensor(np.random.rand(3,10).astype(np.float32)) ######## Last states of 3 batches of RNN\ntarget_input = tf.convert_to_tensor(np.random.rand(3,3,8).astype(np.float32))   ###### target input\ninputs = tf.transpose(target_input, perm=[1, 0, 2])\n\nvar = tf.get_variable # for ease of use\n\nW_z_x = var('W_z_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_z_h = var('W_z_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_z = var('b_z', shape=[num_units], initializer=weight_initializer)\nW_r_x = var('W_r_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_r_h = var('W_r_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_r = var('b_r', shape=[num_units], initializer=weight_initializer)\nW_c_x = var('W_c_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_c_h = var('W_c_h', shape=[num_units, num_units], initializer = weight_initializer)\nb_c = var('b_c', shape=[num_units], initializer=weight_initializer)\nmiddle_matrix = var('middle', shape=[num_units, num_units], initializer = weight_initializer)\n\n\n###### Reading the first element in target of each batch and creating a new state\ninput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, name = 'input_ta')\ninput_ta = input_ta.unpack(inputs)\n# calculate the GRU\ntime = tf.constant(0)\nx_t = input_ta.read(time)\nz = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(initial_state, W_z_h) + b_z) # update gate\nr = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(initial_state, W_r_h) + b_r) # reset gate\nc = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*initial_state, W_c_h) + b_c) # proposed new state\nnew_state = (1-z)*c + z*initial_state # new state\ninitial_state = new_state\n\ndef decoder_cond(time, state, output_ta_t, attention_tracker):\n    return tf.less(time, max_sequence_length)\n\n\ndef decoder_body_builder(feedback=False):\n    def decoder_body(time, old_state, output_ta_t, attention_tracker):\n        if feedback:\n            def from_previous():\n                prev_1 = tf.matmul(old_state, W_out) + b_out\n                return tf.gather(embeddings, tf.argmax(prev_1, 1))\n            x_t = tf.cond(tf.greater(time, 0), from_previous, lambda: input_ta.read(0))\n        else:\n            x_t = input_ta.read(time)\n\n        # calculate the GRU\n\n\n\n        def sub_decoder_cond(sub_time,temp_holder_):\n                return tf.less(sub_time, 3) \n\n        def sub_decoder_body_builder():\n\n            def sub_decoder_body(sub_time ,temp_holder_t):\n                sub_x_t = tf.reshape(sub_initial_.read(sub_time) , [1,-1])\n                sub_i_t = sub_input.read(sub_time)\n                sub_res = tf.matmul(sub_x_t, sub_i_t)\n                temp_holder_t.write(sub_time, sub_res)\n\n                return(sub_time+1, temp_holder_t )\n            return sub_decoder_body\n\n\n        we_project = tf.tanh(tf.matmul( initial_state , middle_matrix ))\n\n        sub_initial_ = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True ,tensor_array_name='sub_initial')\n        sub_input = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, tensor_array_name = 'sub_input')\n        temp_holder = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True , tensor_array_name = 'temp_holder'  )\n\n        sub_initial_ = sub_initial_.unpack(we_project)\n        sub_input = sub_input.unpack(attention_input_mod)\n        sub_time = tf.constant(0)\n\n        sub_loop_vars = [sub_time, temp_holder]\n\n        _, temp_holder_out = tf.while_loop(sub_decoder_cond,\n                                       sub_decoder_body_builder(),\n                                       sub_loop_vars,\n                                       swap_memory=False)\n\n\n\n        alpha_time = temp_holder_out.pack()\n        # temp_holder.close()\n        alpha = alpha_time\n        alpha_softmax = alpha\n        # alpha_softmax = tf.nn.softmax(alpha)\n        z = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(old_state, W_z_h) + b_z) # update gate\n        r = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(old_state, W_r_h) + b_r) # reset gate\n        c = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*old_state, W_c_h) + b_c) # proposed new state\n        new_state = (1-z)*c + z*old_state # new state\n\n        # writing output\n        output_ta_t = output_ta_t.write(time+1, new_state)\n        attention_tracker = attention_tracker.write(time, alpha_softmax)\n        # context = tf.reduce_sum(tf.expand_dims(alpha_softmax, 2) * attention_input, [1])\n\n\n        return (time + 1, new_state, output_ta_t, attention_tracker)\n    return decoder_body\n\n\noutput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\nattention_tracker = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\ntime = tf.constant(0)\nloop_vars = [time, initial_state, output_ta, attention_tracker]\n\n_, state, output_ta, attention_tracker_holder = tf.while_loop(decoder_cond,\n                                               decoder_body_builder(),\n                                               loop_vars,\n                                               swap_memory=False)\nA = attention_tracker_holder.pack()\nO = output_ta.pack()\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())", "body": "@aselle - I will provide a detailed explanation of what the code is and where I am getting the error . \nWhat I am trying to do is , I will get a 3 x 10 matrix in we_project and I am having 3 x 10 x 5 matrix in attention_input_mod . I need to do a matmul of each row of 3 x 10 matrix to corresponding matrix batch in 3 x 10 x 5 . That is , first row 0f 3 x 10 will take a  matmul with first batch of 3 x 10 x 5 ( [0, : ,:]) and so on . For that , I am making use of tf.while_loop inside the other loop which is mostly a duplication of @alrojo code . So , I use sub_initial and sub_input to hold the 2d and 3d matrix respectively and inside sub_decoder function I will try to iterate until I reach batch_len ( 3 by default ) and I read each row and matrix from sub_initial and sub_input respectively and then trying to store the result in temp_holder TensorArray . But getting the following error\n\nAlreadyExistsError: Resource _tensor_arrays/temp_holder/N10tensorflow11TensorArrayE \n\n```\ntarget_dims = 8\nattention_dims = 10\ninput_max_len =  5\nattn_len = 3\nmax_sequence_length = 3\nnum_units = 10\nbatch_len = 3\n\nweight_initializer = tf.truncated_normal_initializer(stddev=0.1)\n\nattention_input = tf.convert_to_tensor(np.random.rand(3,5,10).astype(np.float32)) ##### consider it as RNN encoder output\nattention_input_mod = tf.transpose(attention_input , [0,2,1])\ninitial_state = tf.convert_to_tensor(np.random.rand(3,10).astype(np.float32)) ######## Last states of 3 batches of RNN\ntarget_input = tf.convert_to_tensor(np.random.rand(3,3,8).astype(np.float32))   ###### target input\ninputs = tf.transpose(target_input, perm=[1, 0, 2])\n\nvar = tf.get_variable # for ease of use\n\nW_z_x = var('W_z_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_z_h = var('W_z_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_z = var('b_z', shape=[num_units], initializer=weight_initializer)\nW_r_x = var('W_r_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_r_h = var('W_r_h', shape=[num_units, num_units], initializer=weight_initializer)\nb_r = var('b_r', shape=[num_units], initializer=weight_initializer)\nW_c_x = var('W_c_x', shape=[target_dims, num_units], initializer=weight_initializer)\nW_c_h = var('W_c_h', shape=[num_units, num_units], initializer = weight_initializer)\nb_c = var('b_c', shape=[num_units], initializer=weight_initializer)\nmiddle_matrix = var('middle', shape=[num_units, num_units], initializer = weight_initializer)\n\n\n###### Reading the first element in target of each batch and creating a new state\ninput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, name = 'input_ta')\ninput_ta = input_ta.unpack(inputs)\n# calculate the GRU\ntime = tf.constant(0)\nx_t = input_ta.read(time)\nz = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(initial_state, W_z_h) + b_z) # update gate\nr = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(initial_state, W_r_h) + b_r) # reset gate\nc = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*initial_state, W_c_h) + b_c) # proposed new state\nnew_state = (1-z)*c + z*initial_state # new state\ninitial_state = new_state\n\ndef decoder_cond(time, state, output_ta_t, attention_tracker):\n    return tf.less(time, max_sequence_length)\n\n\ndef decoder_body_builder(feedback=False):\n    def decoder_body(time, old_state, output_ta_t, attention_tracker):\n        if feedback:\n            def from_previous():\n                prev_1 = tf.matmul(old_state, W_out) + b_out\n                return tf.gather(embeddings, tf.argmax(prev_1, 1))\n            x_t = tf.cond(tf.greater(time, 0), from_previous, lambda: input_ta.read(0))\n        else:\n            x_t = input_ta.read(time)\n\n        # calculate the GRU\n\n\n\n        def sub_decoder_cond(sub_time,temp_holder_):\n                return tf.less(sub_time, 3) \n\n        def sub_decoder_body_builder():\n\n            def sub_decoder_body(sub_time ,temp_holder_t):\n                sub_x_t = tf.reshape(sub_initial_.read(sub_time) , [1,-1])\n                sub_i_t = sub_input.read(sub_time)\n                sub_res = tf.matmul(sub_x_t, sub_i_t)\n                temp_holder_t.write(sub_time, sub_res)\n\n                return(sub_time+1, temp_holder_t )\n            return sub_decoder_body\n\n\n        we_project = tf.tanh(tf.matmul( initial_state , middle_matrix ))\n\n        sub_initial_ = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True ,tensor_array_name='sub_initial')\n        sub_input = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, tensor_array_name = 'sub_input')\n        temp_holder = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True , tensor_array_name = 'temp_holder'  )\n\n        sub_initial_ = sub_initial_.unpack(we_project)\n        sub_input = sub_input.unpack(attention_input_mod)\n        sub_time = tf.constant(0)\n\n        sub_loop_vars = [sub_time, temp_holder]\n\n        _, temp_holder_out = tf.while_loop(sub_decoder_cond,\n                                       sub_decoder_body_builder(),\n                                       sub_loop_vars,\n                                       swap_memory=False)\n\n\n\n        alpha_time = temp_holder_out.pack()\n        # temp_holder.close()\n        alpha = alpha_time\n        alpha_softmax = alpha\n        # alpha_softmax = tf.nn.softmax(alpha)\n        z = tf.sigmoid(tf.matmul(x_t, W_z_x) + tf.matmul(old_state, W_z_h) + b_z) # update gate\n        r = tf.sigmoid(tf.matmul(x_t, W_r_x) + tf.matmul(old_state, W_r_h) + b_r) # reset gate\n        c = tf.tanh(tf.matmul(x_t, W_c_x) + tf.matmul(r*old_state, W_c_h) + b_c) # proposed new state\n        new_state = (1-z)*c + z*old_state # new state\n\n        # writing output\n        output_ta_t = output_ta_t.write(time+1, new_state)\n        attention_tracker = attention_tracker.write(time, alpha_softmax)\n        # context = tf.reduce_sum(tf.expand_dims(alpha_softmax, 2) * attention_input, [1])\n\n\n        return (time + 1, new_state, output_ta_t, attention_tracker)\n    return decoder_body\n\n\noutput_ta = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\nattention_tracker = tensor_array_ops.TensorArray(tf.float32, size=1, dynamic_size=True, infer_shape=False)\ntime = tf.constant(0)\nloop_vars = [time, initial_state, output_ta, attention_tracker]\n\n_, state, output_ta, attention_tracker_holder = tf.while_loop(decoder_cond,\n                                               decoder_body_builder(),\n                                               loop_vars,\n                                               swap_memory=False)\nA = attention_tracker_holder.pack()\nO = output_ta.pack()\n\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\n```\n"}