{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439872015", "html_url": "https://github.com/tensorflow/tensorflow/issues/23743#issuecomment-439872015", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23743", "id": 439872015, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTg3MjAxNQ==", "user": {"login": "lei6315", "id": 17965302, "node_id": "MDQ6VXNlcjE3OTY1MzAy", "avatar_url": "https://avatars1.githubusercontent.com/u/17965302?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lei6315", "html_url": "https://github.com/lei6315", "followers_url": "https://api.github.com/users/lei6315/followers", "following_url": "https://api.github.com/users/lei6315/following{/other_user}", "gists_url": "https://api.github.com/users/lei6315/gists{/gist_id}", "starred_url": "https://api.github.com/users/lei6315/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lei6315/subscriptions", "organizations_url": "https://api.github.com/users/lei6315/orgs", "repos_url": "https://api.github.com/users/lei6315/repos", "events_url": "https://api.github.com/users/lei6315/events{/privacy}", "received_events_url": "https://api.github.com/users/lei6315/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-19T12:08:27Z", "updated_at": "2018-11-21T02:43:04Z", "author_association": "NONE", "body_html": "<p>I found the same code that I run in one machine , the log files are normal. Three kinds of log (.index /.data/.meta) all had. Anyone to help me, thank you!</p>\n<pre><code>def main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.reset_default_graph()\n\n    # 1. data  https://www.tensorflow.org/programmers_guide/datasets\n\n    filenames = get_tfrecord_filenames(FLAGS.dataset_dir)\n    tf.logging.info(\"dataset %s:\" % filenames)\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(parse_single_image)\n    dataset = dataset.shuffle(buffer_size=1000)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(FLAGS.batch_size)\n\n    iterator = dataset.make_initializable_iterator()\n    # iterator = dataset.make_one_shot_iterator()\n    batch = iterator.get_next()\n    image_file_name, img_batch, label_batch = batch\n    print(img_batch, label_batch)\n\n    # 3. model\n    with slim.arg_scope(inception_v3_arg_scope()):\n        logits, _ = inception_v3(img_batch, FLAGS.class_num)\n    # predicts = tf.nn.softmax(logits)\n    # coross_mean = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_batch)\n    # loss = tf.reduce_mean(coross_mean)\n\n    loss = tf.losses.softmax_cross_entropy(logits=logits,\n                                           onehot_labels=tf.one_hot(label_batch, depth=FLAGS.class_num,\n                                                                    on_value=1,\n                                                                    off_value=0))\n\n    global_step = tf.train.get_or_create_global_step()\n    opt = tf.train.AdamOptimizer(FLAGS.lr)\n\n    train_op = opt.minimize(loss, global_step=global_step)\n\n    class _DatasetInitializerHook(tf.train.SessionRunHook):\n        def __init__(self, data_iterator):\n            super(_DatasetInitializerHook, self).__init__()\n            self._iterator = data_iterator\n\n        def begin(self):\n            self._initializer = self._iterator.initializer\n\n        def after_create_session(self, session, coord):\n            del coord\n            session.run(self._initializer)\n\n    class _LogHook(tf.train.SessionRunHook):\n        def __init__(self, global_step, loss, log_frequency, batch_size):\n            super(_LogHook, self).__init__()\n            self.loss = loss\n            self.global_step = global_step\n            self.log_frequency = log_frequency\n            self.batch_size = batch_size\n\n        def begin(self):\n            # self.step = -1\n            self.start_time = time.time()\n\n        def before_run(self, run_context):\n            # self.step += 1\n            return tf.train.SessionRunArgs([global_step, loss])\n\n        def after_run(self,\n                      run_context,  # pylint: disable=unused-argument\n                      run_values):\n            global_step_value, loss_value = run_values.results\n            if global_step_value % self.log_frequency == 0:\n                current_time = time.time()\n                duration = current_time - self.start_time\n                self.start_time = current_time\n                imgs_per_sec = self.log_frequency * self.batch_size / duration\n                sec_per_batch = float(duration / self.log_frequency)\n                format_str = ('%s : global step = %d, loss = %.2f (%.1f imgs/sec; %.3fsec/batch)')\n                tf.logging.info(\n                    format_str % (datetime.now(), global_step_value+1, loss_value, imgs_per_sec, sec_per_batch))\n\n    # 4.train\n    hooks = [tf.train.StopAtStepHook(last_step=FLAGS.steps),\n             tf.train.NanTensorHook(loss),\n             _LogHook(global_step, loss, FLAGS.log_frequency, FLAGS.batch_size),\n             _DatasetInitializerHook(iterator)]\n\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=False,\n                            device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\n    config.gpu_options.allow_growth = True\n\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir,\n                                           # scaffold=scaffold,\n                                           hooks=hooks,\n                                           save_checkpoint_steps=100,\n                                           config=config,\n                                           stop_grace_period_secs=5) as mon_sess:\n\n        try:\n\n            # mon_sess.run(iterator.initializer)\n            # mon_sess = tf_debug.LocalCLIDebugWrapperSession(mon_sess)\n            while not mon_sess.should_stop():\n                try:\n                    mon_sess.run(train_op)\n                except tf.errors.OutOfRangeError:\n                    break\n        except Exception as e:\n            print(e)\n</code></pre>", "body_text": "I found the same code that I run in one machine , the log files are normal. Three kinds of log (.index /.data/.meta) all had. Anyone to help me, thank you!\ndef main(_):\n    tf.logging.set_verbosity(tf.logging.INFO)\n    tf.reset_default_graph()\n\n    # 1. data  https://www.tensorflow.org/programmers_guide/datasets\n\n    filenames = get_tfrecord_filenames(FLAGS.dataset_dir)\n    tf.logging.info(\"dataset %s:\" % filenames)\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(parse_single_image)\n    dataset = dataset.shuffle(buffer_size=1000)\n    dataset = dataset.repeat()\n    dataset = dataset.batch(FLAGS.batch_size)\n\n    iterator = dataset.make_initializable_iterator()\n    # iterator = dataset.make_one_shot_iterator()\n    batch = iterator.get_next()\n    image_file_name, img_batch, label_batch = batch\n    print(img_batch, label_batch)\n\n    # 3. model\n    with slim.arg_scope(inception_v3_arg_scope()):\n        logits, _ = inception_v3(img_batch, FLAGS.class_num)\n    # predicts = tf.nn.softmax(logits)\n    # coross_mean = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_batch)\n    # loss = tf.reduce_mean(coross_mean)\n\n    loss = tf.losses.softmax_cross_entropy(logits=logits,\n                                           onehot_labels=tf.one_hot(label_batch, depth=FLAGS.class_num,\n                                                                    on_value=1,\n                                                                    off_value=0))\n\n    global_step = tf.train.get_or_create_global_step()\n    opt = tf.train.AdamOptimizer(FLAGS.lr)\n\n    train_op = opt.minimize(loss, global_step=global_step)\n\n    class _DatasetInitializerHook(tf.train.SessionRunHook):\n        def __init__(self, data_iterator):\n            super(_DatasetInitializerHook, self).__init__()\n            self._iterator = data_iterator\n\n        def begin(self):\n            self._initializer = self._iterator.initializer\n\n        def after_create_session(self, session, coord):\n            del coord\n            session.run(self._initializer)\n\n    class _LogHook(tf.train.SessionRunHook):\n        def __init__(self, global_step, loss, log_frequency, batch_size):\n            super(_LogHook, self).__init__()\n            self.loss = loss\n            self.global_step = global_step\n            self.log_frequency = log_frequency\n            self.batch_size = batch_size\n\n        def begin(self):\n            # self.step = -1\n            self.start_time = time.time()\n\n        def before_run(self, run_context):\n            # self.step += 1\n            return tf.train.SessionRunArgs([global_step, loss])\n\n        def after_run(self,\n                      run_context,  # pylint: disable=unused-argument\n                      run_values):\n            global_step_value, loss_value = run_values.results\n            if global_step_value % self.log_frequency == 0:\n                current_time = time.time()\n                duration = current_time - self.start_time\n                self.start_time = current_time\n                imgs_per_sec = self.log_frequency * self.batch_size / duration\n                sec_per_batch = float(duration / self.log_frequency)\n                format_str = ('%s : global step = %d, loss = %.2f (%.1f imgs/sec; %.3fsec/batch)')\n                tf.logging.info(\n                    format_str % (datetime.now(), global_step_value+1, loss_value, imgs_per_sec, sec_per_batch))\n\n    # 4.train\n    hooks = [tf.train.StopAtStepHook(last_step=FLAGS.steps),\n             tf.train.NanTensorHook(loss),\n             _LogHook(global_step, loss, FLAGS.log_frequency, FLAGS.batch_size),\n             _DatasetInitializerHook(iterator)]\n\n    config = tf.ConfigProto(allow_soft_placement=True,\n                            log_device_placement=False,\n                            device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\n    config.gpu_options.allow_growth = True\n\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir,\n                                           # scaffold=scaffold,\n                                           hooks=hooks,\n                                           save_checkpoint_steps=100,\n                                           config=config,\n                                           stop_grace_period_secs=5) as mon_sess:\n\n        try:\n\n            # mon_sess.run(iterator.initializer)\n            # mon_sess = tf_debug.LocalCLIDebugWrapperSession(mon_sess)\n            while not mon_sess.should_stop():\n                try:\n                    mon_sess.run(train_op)\n                except tf.errors.OutOfRangeError:\n                    break\n        except Exception as e:\n            print(e)", "body": "I found the same code that I run in one machine , the log files are normal. Three kinds of log (.index /.data/.meta) all had. Anyone to help me, thank you!\r\n```\r\ndef main(_):\r\n    tf.logging.set_verbosity(tf.logging.INFO)\r\n    tf.reset_default_graph()\r\n\r\n    # 1. data  https://www.tensorflow.org/programmers_guide/datasets\r\n\r\n    filenames = get_tfrecord_filenames(FLAGS.dataset_dir)\r\n    tf.logging.info(\"dataset %s:\" % filenames)\r\n    dataset = tf.data.TFRecordDataset(filenames)\r\n    dataset = dataset.map(parse_single_image)\r\n    dataset = dataset.shuffle(buffer_size=1000)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.batch(FLAGS.batch_size)\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n    # iterator = dataset.make_one_shot_iterator()\r\n    batch = iterator.get_next()\r\n    image_file_name, img_batch, label_batch = batch\r\n    print(img_batch, label_batch)\r\n\r\n    # 3. model\r\n    with slim.arg_scope(inception_v3_arg_scope()):\r\n        logits, _ = inception_v3(img_batch, FLAGS.class_num)\r\n    # predicts = tf.nn.softmax(logits)\r\n    # coross_mean = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=label_batch)\r\n    # loss = tf.reduce_mean(coross_mean)\r\n\r\n    loss = tf.losses.softmax_cross_entropy(logits=logits,\r\n                                           onehot_labels=tf.one_hot(label_batch, depth=FLAGS.class_num,\r\n                                                                    on_value=1,\r\n                                                                    off_value=0))\r\n\r\n    global_step = tf.train.get_or_create_global_step()\r\n    opt = tf.train.AdamOptimizer(FLAGS.lr)\r\n\r\n    train_op = opt.minimize(loss, global_step=global_step)\r\n\r\n    class _DatasetInitializerHook(tf.train.SessionRunHook):\r\n        def __init__(self, data_iterator):\r\n            super(_DatasetInitializerHook, self).__init__()\r\n            self._iterator = data_iterator\r\n\r\n        def begin(self):\r\n            self._initializer = self._iterator.initializer\r\n\r\n        def after_create_session(self, session, coord):\r\n            del coord\r\n            session.run(self._initializer)\r\n\r\n    class _LogHook(tf.train.SessionRunHook):\r\n        def __init__(self, global_step, loss, log_frequency, batch_size):\r\n            super(_LogHook, self).__init__()\r\n            self.loss = loss\r\n            self.global_step = global_step\r\n            self.log_frequency = log_frequency\r\n            self.batch_size = batch_size\r\n\r\n        def begin(self):\r\n            # self.step = -1\r\n            self.start_time = time.time()\r\n\r\n        def before_run(self, run_context):\r\n            # self.step += 1\r\n            return tf.train.SessionRunArgs([global_step, loss])\r\n\r\n        def after_run(self,\r\n                      run_context,  # pylint: disable=unused-argument\r\n                      run_values):\r\n            global_step_value, loss_value = run_values.results\r\n            if global_step_value % self.log_frequency == 0:\r\n                current_time = time.time()\r\n                duration = current_time - self.start_time\r\n                self.start_time = current_time\r\n                imgs_per_sec = self.log_frequency * self.batch_size / duration\r\n                sec_per_batch = float(duration / self.log_frequency)\r\n                format_str = ('%s : global step = %d, loss = %.2f (%.1f imgs/sec; %.3fsec/batch)')\r\n                tf.logging.info(\r\n                    format_str % (datetime.now(), global_step_value+1, loss_value, imgs_per_sec, sec_per_batch))\r\n\r\n    # 4.train\r\n    hooks = [tf.train.StopAtStepHook(last_step=FLAGS.steps),\r\n             tf.train.NanTensorHook(loss),\r\n             _LogHook(global_step, loss, FLAGS.log_frequency, FLAGS.batch_size),\r\n             _DatasetInitializerHook(iterator)]\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True,\r\n                            log_device_placement=False,\r\n                            device_filters=[\"/job:ps\", \"/job:worker/task:%d\" % FLAGS.task_index])\r\n    config.gpu_options.allow_growth = True\r\n\r\n    with tf.train.MonitoredTrainingSession(checkpoint_dir=FLAGS.train_dir,\r\n                                           # scaffold=scaffold,\r\n                                           hooks=hooks,\r\n                                           save_checkpoint_steps=100,\r\n                                           config=config,\r\n                                           stop_grace_period_secs=5) as mon_sess:\r\n\r\n        try:\r\n\r\n            # mon_sess.run(iterator.initializer)\r\n            # mon_sess = tf_debug.LocalCLIDebugWrapperSession(mon_sess)\r\n            while not mon_sess.should_stop():\r\n                try:\r\n                    mon_sess.run(train_op)\r\n                except tf.errors.OutOfRangeError:\r\n                    break\r\n        except Exception as e:\r\n            print(e)\r\n```"}