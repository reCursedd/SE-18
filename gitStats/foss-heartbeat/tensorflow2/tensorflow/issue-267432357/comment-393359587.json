{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/393359587", "html_url": "https://github.com/tensorflow/tensorflow/issues/13895#issuecomment-393359587", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13895", "id": 393359587, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MzM1OTU4Nw==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-31T00:12:49Z", "updated_at": "2018-05-31T00:12:49Z", "author_association": "MEMBER", "body_html": "<p>If you're using Estimator in a distributed system, evaluation happens in parallel to training, and not strictly in between epochs.</p>\n<p>In local mode, to evaluate, say, twice per epoch, you can either use a variable in the input_fn, and store state there, or you can store that state in a hook. Either way, this is somewhat complicated.</p>\n<p>Another option (and the one used by most of our production pipelines) is to shuffle the inputs. That way, you will get a different set of inputs every time you run, and you can simply evaluate after a given set of steps and then evaluate, without taking special care of the input state. If you do this, be careful that you don't only shuffle the beginning of the data.</p>", "body_text": "If you're using Estimator in a distributed system, evaluation happens in parallel to training, and not strictly in between epochs.\nIn local mode, to evaluate, say, twice per epoch, you can either use a variable in the input_fn, and store state there, or you can store that state in a hook. Either way, this is somewhat complicated.\nAnother option (and the one used by most of our production pipelines) is to shuffle the inputs. That way, you will get a different set of inputs every time you run, and you can simply evaluate after a given set of steps and then evaluate, without taking special care of the input state. If you do this, be careful that you don't only shuffle the beginning of the data.", "body": "If you're using Estimator in a distributed system, evaluation happens in parallel to training, and not strictly in between epochs. \r\n\r\nIn local mode, to evaluate, say, twice per epoch, you can either use a variable in the input_fn, and store state there, or you can store that state in a hook. Either way, this is somewhat complicated. \r\n\r\nAnother option (and the one used by most of our production pipelines) is to shuffle the inputs. That way, you will get a different set of inputs every time you run, and you can simply evaluate after a given set of steps and then evaluate, without taking special care of the input state. If you do this, be careful that you don't only shuffle the beginning of the data."}