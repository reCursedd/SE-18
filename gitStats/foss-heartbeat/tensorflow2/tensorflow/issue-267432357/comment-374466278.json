{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/374466278", "html_url": "https://github.com/tensorflow/tensorflow/issues/13895#issuecomment-374466278", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13895", "id": 374466278, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDQ2NjI3OA==", "user": {"login": "datlife", "id": 13214917, "node_id": "MDQ6VXNlcjEzMjE0OTE3", "avatar_url": "https://avatars0.githubusercontent.com/u/13214917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/datlife", "html_url": "https://github.com/datlife", "followers_url": "https://api.github.com/users/datlife/followers", "following_url": "https://api.github.com/users/datlife/following{/other_user}", "gists_url": "https://api.github.com/users/datlife/gists{/gist_id}", "starred_url": "https://api.github.com/users/datlife/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/datlife/subscriptions", "organizations_url": "https://api.github.com/users/datlife/orgs", "repos_url": "https://api.github.com/users/datlife/repos", "events_url": "https://api.github.com/users/datlife/events{/privacy}", "received_events_url": "https://api.github.com/users/datlife/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-20T04:01:04Z", "updated_at": "2018-03-20T04:01:04Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a> May you elaborate the argument that <em>\"... but if you train on less than an epoch in each iteration, <strong>such loops are a bad idea.\"</strong></em> .  In many machine learning methods, shouldn't we need to evaluate the model after every epoch?</p>\n<p>AFAIK, Keras has the idea to evaluate model every epoch  in its <code>fit_generator</code> and <code>fit</code>. Particularly, one use case (that been showed in TensorFlow official code <a href=\"https://github.com/tensorflow/models/blob/ce4459762727264f35a00a7ff1f8151380ea681d/official/resnet/resnet.py#L605\">here</a> . Here is my example using K-fold cross validation for <code>Estimator</code>. :</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(training_epochs <span class=\"pl-k\">//</span> epochs_per_eval):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> #####################</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> K-fold cross validation</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> #####################</span>\n  train_features, train_labels, eval_features, eval_labels <span class=\"pl-k\">=</span> data_provider.split_training_data(\n      <span class=\"pl-v\">test_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.2</span>,\n      <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> The pipeline below is similar to `fit_generator` in Keras</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ##############</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Training cycle</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ##############</span>\n  estimator.train(\n      <span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: data_provider.get_input_fn(\n          <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>tf.estimator.ModeKeys.<span class=\"pl-c1\">TRAIN</span>,\n          <span class=\"pl-v\">features</span><span class=\"pl-k\">=</span>train_features,\n          <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>train_labels,\n          <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n          <span class=\"pl-v\">parse_fn</span><span class=\"pl-k\">=</span>parse_sample_fn,\n          <span class=\"pl-v\">preprocess_fn</span><span class=\"pl-k\">=</span>preprocess,\n          <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span>steps_per_epoch <span class=\"pl-k\">*</span> epochs_per_eval,\n          <span class=\"pl-v\">shuffle_buffer</span><span class=\"pl-k\">=</span>shuffle_buffer,\n          <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>cpu_cores),\n      <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span>steps_per_epoch <span class=\"pl-k\">*</span> epochs_per_eval,)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ################</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Evaluation cycle</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ################</span>\n  eval_result <span class=\"pl-k\">=</span> estimator.evaluate(\n      <span class=\"pl-v\">input_fn</span><span class=\"pl-k\">=</span><span class=\"pl-k\">lambda</span>: data_provider.get_input_fn(\n          <span class=\"pl-v\">mode</span><span class=\"pl-k\">=</span>tf.estimator.ModeKeys.<span class=\"pl-c1\">EVAL</span>,\n          <span class=\"pl-v\">features</span><span class=\"pl-k\">=</span>eval_features,\n          <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>eval_labels,\n          <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n          <span class=\"pl-v\">parse_fn</span><span class=\"pl-k\">=</span>parse_sample_fn,\n          <span class=\"pl-v\">preprocess_fn</span><span class=\"pl-k\">=</span>preprocess,\n          <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>,\n          <span class=\"pl-v\">shuffle_buffer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n          <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span>cpu_cores),\n      <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">200</span>)\n  <span class=\"pl-c1\">print</span>(eval_result)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>---- Training Completed ----<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>One of the issues ( I think) with <code>Estimator</code> in this approach is :</p>\n<ul>\n<li>If <code>epochs_per_eval</code> and <code>steps_per_epoch</code> is relative small, the overhead to create the Graph for training and evaluation in each iteration in a loop is huge.</li>\n</ul>\n<p>My question is:</p>\n<ul>\n<li>Is it possible to share the Graph in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L817\"><code>_train_model</code></a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L910\"><code>_evaluate_model</code></a>, instead of creating them everytime ? Thanks for reading.</li>\n</ul>", "body_text": "@martinwicke May you elaborate the argument that \"... but if you train on less than an epoch in each iteration, such loops are a bad idea.\" .  In many machine learning methods, shouldn't we need to evaluate the model after every epoch?\nAFAIK, Keras has the idea to evaluate model every epoch  in its fit_generator and fit. Particularly, one use case (that been showed in TensorFlow official code here . Here is my example using K-fold cross validation for Estimator. :\nfor _ in range(training_epochs // epochs_per_eval):\n  # #####################\n  # K-fold cross validation\n  # #####################\n  train_features, train_labels, eval_features, eval_labels = data_provider.split_training_data(\n      test_size=0.2,\n      shuffle=True)\n\n  # The pipeline below is similar to `fit_generator` in Keras\n  # ##############\n  # Training cycle\n  # ##############\n  estimator.train(\n      input_fn=lambda: data_provider.get_input_fn(\n          mode=tf.estimator.ModeKeys.TRAIN,\n          features=train_features,\n          labels=train_labels,\n          batch_size=batch_size,\n          parse_fn=parse_sample_fn,\n          preprocess_fn=preprocess,\n          steps_per_epoch=steps_per_epoch * epochs_per_eval,\n          shuffle_buffer=shuffle_buffer,\n          num_parallel_calls=cpu_cores),\n      steps=steps_per_epoch * epochs_per_eval,)\n\n  # ################\n  # Evaluation cycle\n  # ################\n  eval_result = estimator.evaluate(\n      input_fn=lambda: data_provider.get_input_fn(\n          mode=tf.estimator.ModeKeys.EVAL,\n          features=eval_features,\n          labels=eval_labels,\n          batch_size=batch_size,\n          parse_fn=parse_sample_fn,\n          preprocess_fn=preprocess,\n          steps_per_epoch=200,\n          shuffle_buffer=None,\n          num_parallel_calls=cpu_cores),\n      steps=200)\n  print(eval_result)\nprint(\"---- Training Completed ----\")\nOne of the issues ( I think) with Estimator in this approach is :\n\nIf epochs_per_eval and steps_per_epoch is relative small, the overhead to create the Graph for training and evaluation in each iteration in a loop is huge.\n\nMy question is:\n\nIs it possible to share the Graph in _train_model and _evaluate_model, instead of creating them everytime ? Thanks for reading.", "body": "@martinwicke May you elaborate the argument that _\"... but if you train on less than an epoch in each iteration, **such loops are a bad idea.\"**_ .  In many machine learning methods, shouldn't we need to evaluate the model after every epoch? \r\n\r\nAFAIK, Keras has the idea to evaluate model every epoch  in its `fit_generator` and `fit`. Particularly, one use case (that been showed in TensorFlow official code [here](https://github.com/tensorflow/models/blob/ce4459762727264f35a00a7ff1f8151380ea681d/official/resnet/resnet.py#L605) . Here is my example using K-fold cross validation for `Estimator`. : \r\n\r\n```python\r\nfor _ in range(training_epochs // epochs_per_eval):\r\n  # #####################\r\n  # K-fold cross validation\r\n  # #####################\r\n  train_features, train_labels, eval_features, eval_labels = data_provider.split_training_data(\r\n      test_size=0.2,\r\n      shuffle=True)\r\n\r\n  # The pipeline below is similar to `fit_generator` in Keras\r\n  # ##############\r\n  # Training cycle\r\n  # ##############\r\n  estimator.train(\r\n      input_fn=lambda: data_provider.get_input_fn(\r\n          mode=tf.estimator.ModeKeys.TRAIN,\r\n          features=train_features,\r\n          labels=train_labels,\r\n          batch_size=batch_size,\r\n          parse_fn=parse_sample_fn,\r\n          preprocess_fn=preprocess,\r\n          steps_per_epoch=steps_per_epoch * epochs_per_eval,\r\n          shuffle_buffer=shuffle_buffer,\r\n          num_parallel_calls=cpu_cores),\r\n      steps=steps_per_epoch * epochs_per_eval,)\r\n\r\n  # ################\r\n  # Evaluation cycle\r\n  # ################\r\n  eval_result = estimator.evaluate(\r\n      input_fn=lambda: data_provider.get_input_fn(\r\n          mode=tf.estimator.ModeKeys.EVAL,\r\n          features=eval_features,\r\n          labels=eval_labels,\r\n          batch_size=batch_size,\r\n          parse_fn=parse_sample_fn,\r\n          preprocess_fn=preprocess,\r\n          steps_per_epoch=200,\r\n          shuffle_buffer=None,\r\n          num_parallel_calls=cpu_cores),\r\n      steps=200)\r\n  print(eval_result)\r\nprint(\"---- Training Completed ----\")\r\n```\r\n\r\nOne of the issues ( I think) with `Estimator` in this approach is : \r\n* If `epochs_per_eval` and `steps_per_epoch` is relative small, the overhead to create the Graph for training and evaluation in each iteration in a loop is huge.\r\n\r\nMy question is:\r\n* Is it possible to share the Graph in [`_train_model`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L817) and [`_evaluate_model`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/estimator.py#L910), instead of creating them everytime ? Thanks for reading."}