{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/374639236", "html_url": "https://github.com/tensorflow/tensorflow/issues/13895#issuecomment-374639236", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13895", "id": 374639236, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NDYzOTIzNg==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-20T15:25:21Z", "updated_at": "2018-03-20T15:25:33Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>My question is:</p>\n<ul>\n<li>Is it possible to share the Graph in _train_model and _evaluate_model, instead of creating them<br>\neverytime ? Thanks for reading.</li>\n</ul>\n</blockquote>\n<p>The reason for this strict separation is that we want to guarantee that the model can be scaled up and trained in a distributed fashion. Therefore we make sure that the behavior of an <code>Estimator</code> is consistent, whether you train it in local mode or on a cluster of machines. If we allowed you to share the graph between training and eval, it would be very easy to introduce subtle mistakes that would break the training once you run it distributed, so we do not allow it.</p>\n<p>I understand that this makes life harder in \"local\" mode. If you are not interested in distributed training, I recommend using <code>tf.keras.Model</code>. You can always go back to an Estimator using <code>model_to_estimator</code> if you change your mind.</p>", "body_text": "My question is:\n\nIs it possible to share the Graph in _train_model and _evaluate_model, instead of creating them\neverytime ? Thanks for reading.\n\n\nThe reason for this strict separation is that we want to guarantee that the model can be scaled up and trained in a distributed fashion. Therefore we make sure that the behavior of an Estimator is consistent, whether you train it in local mode or on a cluster of machines. If we allowed you to share the graph between training and eval, it would be very easy to introduce subtle mistakes that would break the training once you run it distributed, so we do not allow it.\nI understand that this makes life harder in \"local\" mode. If you are not interested in distributed training, I recommend using tf.keras.Model. You can always go back to an Estimator using model_to_estimator if you change your mind.", "body": "> My question is:\r\n>\r\n> - Is it possible to share the Graph in _train_model and _evaluate_model, instead of creating them \r\n>   everytime ? Thanks for reading.\r\n\r\nThe reason for this strict separation is that we want to guarantee that the model can be scaled up and trained in a distributed fashion. Therefore we make sure that the behavior of an `Estimator` is consistent, whether you train it in local mode or on a cluster of machines. If we allowed you to share the graph between training and eval, it would be very easy to introduce subtle mistakes that would break the training once you run it distributed, so we do not allow it.\r\n\r\nI understand that this makes life harder in \"local\" mode. If you are not interested in distributed training, I recommend using `tf.keras.Model`. You can always go back to an Estimator using `model_to_estimator` if you change your mind."}