{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/412327562", "html_url": "https://github.com/tensorflow/tensorflow/issues/13895#issuecomment-412327562", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13895", "id": 412327562, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMjMyNzU2Mg==", "user": {"login": "tavramov", "id": 30814992, "node_id": "MDQ6VXNlcjMwODE0OTky", "avatar_url": "https://avatars2.githubusercontent.com/u/30814992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tavramov", "html_url": "https://github.com/tavramov", "followers_url": "https://api.github.com/users/tavramov/followers", "following_url": "https://api.github.com/users/tavramov/following{/other_user}", "gists_url": "https://api.github.com/users/tavramov/gists{/gist_id}", "starred_url": "https://api.github.com/users/tavramov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tavramov/subscriptions", "organizations_url": "https://api.github.com/users/tavramov/orgs", "repos_url": "https://api.github.com/users/tavramov/repos", "events_url": "https://api.github.com/users/tavramov/events{/privacy}", "received_events_url": "https://api.github.com/users/tavramov/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-12T08:33:03Z", "updated_at": "2018-08-12T08:33:03Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a> ,<br>\n<code>InMemoryEvaluatorHook</code> looks like a great way to evaluate while training but I am running into issue where all eval_metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered.<br>\nHere is a snippet of my output when <code>every_n_iter=50</code>:</p>\n<p>INFO:tensorflow:Saving dict for global step 0: accuracy = 0.158545, global_step = 0, loss = 3.36605<br>\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0<br>\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0<br>\nINFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0</p>\n<p>My suspicion is that it does not work because my  <code>input_fn</code> is defined as <code>tf.estimator.inputs.numpy_input_fn</code> with <code>num_epochs=1</code>.</p>\n<p>The exact definition of the input function that I use to create the <code>InMemoryEvaluatorHook</code> is:</p>\n<pre><code>    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={\"x\": eval_data},\n        y=eval_labels,\n        num_epochs=1,\n        batch_size=25,\n        shuffle=False)\n</code></pre>\n<p>How can I confirm if my input_fn is really to blame here? Any ideas how to get around this? Thanks</p>", "body_text": "Hi @martinwicke ,\nInMemoryEvaluatorHook looks like a great way to evaluate while training but I am running into issue where all eval_metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered.\nHere is a snippet of my output when every_n_iter=50:\nINFO:tensorflow:Saving dict for global step 0: accuracy = 0.158545, global_step = 0, loss = 3.36605\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0\nINFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0\nMy suspicion is that it does not work because my  input_fn is defined as tf.estimator.inputs.numpy_input_fn with num_epochs=1.\nThe exact definition of the input function that I use to create the InMemoryEvaluatorHook is:\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n        x={\"x\": eval_data},\n        y=eval_labels,\n        num_epochs=1,\n        batch_size=25,\n        shuffle=False)\n\nHow can I confirm if my input_fn is really to blame here? Any ideas how to get around this? Thanks", "body": "Hi @martinwicke ,\r\n`InMemoryEvaluatorHook` looks like a great way to evaluate while training but I am running into issue where all eval_metrics are evaluated to 0.0 after the very first time the evaluation hook is triggered. \r\nHere is a snippet of my output when `every_n_iter=50`:\r\n\r\nINFO:tensorflow:Saving dict for global step 0: accuracy = 0.158545, global_step = 0, loss = 3.36605\r\nINFO:tensorflow:Saving dict for global step 50: accuracy = 0.0, global_step = 50, loss = 0.0\r\nINFO:tensorflow:Saving dict for global step 100: accuracy = 0.0, global_step = 100, loss = 0.0\r\nINFO:tensorflow:Saving dict for global step 150: accuracy = 0.0, global_step = 150, loss = 0.0\r\n\r\nMy suspicion is that it does not work because my  `input_fn` is defined as `tf.estimator.inputs.numpy_input_fn` with `num_epochs=1`.\r\n\r\nThe exact definition of the input function that I use to create the `InMemoryEvaluatorHook` is:\r\n```\r\n    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n        x={\"x\": eval_data},\r\n        y=eval_labels,\r\n        num_epochs=1,\r\n        batch_size=25,\r\n        shuffle=False)\r\n```\r\nHow can I confirm if my input_fn is really to blame here? Any ideas how to get around this? Thanks"}