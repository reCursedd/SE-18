{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/383629349", "html_url": "https://github.com/tensorflow/tensorflow/issues/13895#issuecomment-383629349", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13895", "id": 383629349, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MzYyOTM0OQ==", "user": {"login": "fgervais", "id": 5411996, "node_id": "MDQ6VXNlcjU0MTE5OTY=", "avatar_url": "https://avatars1.githubusercontent.com/u/5411996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fgervais", "html_url": "https://github.com/fgervais", "followers_url": "https://api.github.com/users/fgervais/followers", "following_url": "https://api.github.com/users/fgervais/following{/other_user}", "gists_url": "https://api.github.com/users/fgervais/gists{/gist_id}", "starred_url": "https://api.github.com/users/fgervais/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fgervais/subscriptions", "organizations_url": "https://api.github.com/users/fgervais/orgs", "repos_url": "https://api.github.com/users/fgervais/repos", "events_url": "https://api.github.com/users/fgervais/events{/privacy}", "received_events_url": "https://api.github.com/users/fgervais/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-23T16:03:07Z", "updated_at": "2018-04-23T16:03:07Z", "author_association": "NONE", "body_html": "<p>I'd like to get a bit more information on this problem but from the predict() point of view.</p>\n<p>I posted on stackoverflow but really there is not much support there for now. I can see why people would try here.</p>\n<p><a href=\"https://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs\" rel=\"nofollow\">https://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs</a></p>\n<p>It look like when running the estimator live and doing one prediction at a time (as data comes in) most of the prediction time is wasted on reloading the model.</p>\n<p>In my use case I'd like to do the [prediction, action, prediction, action, ...] as fast as possible.</p>\n<p>Is there a way around having the model reloaded in between predictions?</p>", "body_text": "I'd like to get a bit more information on this problem but from the predict() point of view.\nI posted on stackoverflow but really there is not much support there for now. I can see why people would try here.\nhttps://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs\nIt look like when running the estimator live and doing one prediction at a time (as data comes in) most of the prediction time is wasted on reloading the model.\nIn my use case I'd like to do the [prediction, action, prediction, action, ...] as fast as possible.\nIs there a way around having the model reloaded in between predictions?", "body": "I'd like to get a bit more information on this problem but from the predict() point of view.\r\n\r\nI posted on stackoverflow but really there is not much support there for now. I can see why people would try here.\r\n \r\nhttps://stackoverflow.com/questions/49966447/keep-the-tensorflow-estimator-in-memory-while-waiting-for-live-prediction-inputs\r\n\r\nIt look like when running the estimator live and doing one prediction at a time (as data comes in) most of the prediction time is wasted on reloading the model.\r\n\r\nIn my use case I'd like to do the [prediction, action, prediction, action, ...] as fast as possible.\r\n\r\nIs there a way around having the model reloaded in between predictions?"}