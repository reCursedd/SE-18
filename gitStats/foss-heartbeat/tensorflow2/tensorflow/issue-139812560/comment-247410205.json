{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/247410205", "html_url": "https://github.com/tensorflow/tensorflow/issues/1450#issuecomment-247410205", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1450", "id": 247410205, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NzQxMDIwNQ==", "user": {"login": "kbrems", "id": 456665, "node_id": "MDQ6VXNlcjQ1NjY2NQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/456665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kbrems", "html_url": "https://github.com/kbrems", "followers_url": "https://api.github.com/users/kbrems/followers", "following_url": "https://api.github.com/users/kbrems/following{/other_user}", "gists_url": "https://api.github.com/users/kbrems/gists{/gist_id}", "starred_url": "https://api.github.com/users/kbrems/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kbrems/subscriptions", "organizations_url": "https://api.github.com/users/kbrems/orgs", "repos_url": "https://api.github.com/users/kbrems/repos", "events_url": "https://api.github.com/users/kbrems/events{/privacy}", "received_events_url": "https://api.github.com/users/kbrems/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-15T18:25:30Z", "updated_at": "2016-09-15T18:25:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am getting the same error when I create a simple custom operator that operates on a list of input tensors of type int32. My input tensor is 5 elements, so this is clearly not a memory limitation issue.</p>\n<p>Specifics:<br>\nubuntu 14.04<br>\nGeForce GTX TITAN driver version 367.44<br>\ncuda 7.5, cudnn v4<br>\nbinary pip install tensrflow gpu version 0.10.0rc0<br>\npython 2.7</p>\n<p>Build and run the attached source code:<br>\n$ python cuda_op_unittest.py<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\nname: GeForce GTX TITAN<br>\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928<br>\npciBusID 0000:05:00.0<br>\nTotal memory: 5.94GiB<br>\nFree memory: 5.45GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)<br>\nDevice mapping:<br>\n/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0<br>\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:<br>\n/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0</p>\n<p>int32: /job:localhost/replica:0/task:0/gpu:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32: /job:localhost/replica:0/task:0/gpu:0<br>\nint32/input_0: /job:localhost/replica:0/task:0/gpu:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32/input_0: /job:localhost/replica:0/task:0/gpu:0<br>\n*** running on GPU ***<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available<br>\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed<br>\nAborted (core dumped)</p>\n<p>Key info:<br>\nIf I run this with tf.device('/cpu:0') it works.<br>\nIf I make my inputs and outputs a single tensor instead of a list of 1 tensor it also works. (This took a while to figure out!). ie instead of .Input(\"input: in_types\") in the REGISTER_OP use .Input(\"input: int32\")</p>\n<p>Notes:<br>\nbased on the response to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"177028728\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4387\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4387/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4387\">#4387</a>, some research led me here: <a href=\"http://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op\" rel=\"nofollow\">http://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op</a>. It seems that tensorflow does not really support GPU operators on integer tensors and adding that support is difficult. In the interim though, better documentation on integer tensor support and a meaningful error message would be preferable to a core dump :).</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/475304/issue1450.zip\">issue1450.zip</a></p>", "body_text": "I am getting the same error when I create a simple custom operator that operates on a list of input tensors of type int32. My input tensor is 5 elements, so this is clearly not a memory limitation issue.\nSpecifics:\nubuntu 14.04\nGeForce GTX TITAN driver version 367.44\ncuda 7.5, cudnn v4\nbinary pip install tensrflow gpu version 0.10.0rc0\npython 2.7\nBuild and run the attached source code:\n$ python cuda_op_unittest.py\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 5.94GiB\nFree memory: 5.45GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nint32: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32: /job:localhost/replica:0/task:0/gpu:0\nint32/input_0: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32/input_0: /job:localhost/replica:0/task:0/gpu:0\n*** running on GPU ***\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\nKey info:\nIf I run this with tf.device('/cpu:0') it works.\nIf I make my inputs and outputs a single tensor instead of a list of 1 tensor it also works. (This took a while to figure out!). ie instead of .Input(\"input: in_types\") in the REGISTER_OP use .Input(\"input: int32\")\nNotes:\nbased on the response to #4387, some research led me here: http://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op. It seems that tensorflow does not really support GPU operators on integer tensors and adding that support is difficult. In the interim though, better documentation on integer tensor support and a meaningful error message would be preferable to a core dump :).\nissue1450.zip", "body": "I am getting the same error when I create a simple custom operator that operates on a list of input tensors of type int32. My input tensor is 5 elements, so this is clearly not a memory limitation issue. \n\nSpecifics:\nubuntu 14.04\nGeForce GTX TITAN driver version 367.44\ncuda 7.5, cudnn v4\nbinary pip install tensrflow gpu version 0.10.0rc0\npython 2.7\n\nBuild and run the attached source code:\n$ python cuda_op_unittest.py \nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 5.94GiB\nFree memory: 5.45GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:839] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nI tensorflow/core/common_runtime/direct_session.cc:175] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\n\nint32: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32: /job:localhost/replica:0/task:0/gpu:0\nint32/input_0: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:818] int32/input_0: /job:localhost/replica:0/task:0/gpu:0\n**\\* running on GPU ***\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1140] could not synchronize on CUDA context: CUDA_ERROR_ILLEGAL_ADDRESS :: No stack trace available\nF tensorflow/core/common_runtime/gpu/gpu_util.cc:370] GPU sync failed\nAborted (core dumped)\n\nKey info:\nIf I run this with tf.device('/cpu:0') it works. \nIf I make my inputs and outputs a single tensor instead of a list of 1 tensor it also works. (This took a while to figure out!). ie instead of .Input(\"input: in_types\") in the REGISTER_OP use .Input(\"input: int32\")\n\nNotes: \nbased on the response to https://github.com/tensorflow/tensorflow/issues/4387, some research led me here: http://stackoverflow.com/questions/37439299/no-gpu-kernel-for-an-int32-variable-op. It seems that tensorflow does not really support GPU operators on integer tensors and adding that support is difficult. In the interim though, better documentation on integer tensor support and a meaningful error message would be preferable to a core dump :). \n\n[issue1450.zip](https://github.com/tensorflow/tensorflow/files/475304/issue1450.zip)\n"}