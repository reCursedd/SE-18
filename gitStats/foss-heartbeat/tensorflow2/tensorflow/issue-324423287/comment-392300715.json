{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392300715", "html_url": "https://github.com/tensorflow/tensorflow/issues/19385#issuecomment-392300715", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19385", "id": 392300715, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MjMwMDcxNQ==", "user": {"login": "hmorimitsu", "id": 24420973, "node_id": "MDQ6VXNlcjI0NDIwOTcz", "avatar_url": "https://avatars2.githubusercontent.com/u/24420973?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hmorimitsu", "html_url": "https://github.com/hmorimitsu", "followers_url": "https://api.github.com/users/hmorimitsu/followers", "following_url": "https://api.github.com/users/hmorimitsu/following{/other_user}", "gists_url": "https://api.github.com/users/hmorimitsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/hmorimitsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hmorimitsu/subscriptions", "organizations_url": "https://api.github.com/users/hmorimitsu/orgs", "repos_url": "https://api.github.com/users/hmorimitsu/repos", "events_url": "https://api.github.com/users/hmorimitsu/events{/privacy}", "received_events_url": "https://api.github.com/users/hmorimitsu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-27T02:29:34Z", "updated_at": "2018-05-27T02:29:34Z", "author_association": "NONE", "body_html": "<p>Thank you for the fix <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=122911\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/akshaym\">@akshaym</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3731025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/allenlavoie\">@allenlavoie</a> ! It did solve that problem. However, now I am observing another leak when I use tfe.GradientTape(). The code to reproduce the problem, and the memory plot are below:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ntf.enable_eager_execution()\ntfe <span class=\"pl-k\">=</span> tf.contrib.eager\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Net</span>(<span class=\"pl-e\">tf</span>.<span class=\"pl-e\">keras</span>.<span class=\"pl-e\">Model</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n        <span class=\"pl-c1\">super</span>(Net, <span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__init__</span>()\n        <span class=\"pl-c1\">self</span>.fc1 <span class=\"pl-k\">=</span> tf.keras.layers.Dense(<span class=\"pl-c1\">5</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">call</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>):\n        y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.fc1(x)\n        <span class=\"pl-k\">return</span> y\n\ninputs <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">100</span>], tf.float32)\nnet <span class=\"pl-k\">=</span> Net()\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000000</span>):\n    <span class=\"pl-k\">with</span> tfe.GradientTape() <span class=\"pl-k\">as</span> tape:\n        net(inputs)</pre></div>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/24420973/40581921-16b78f98-613c-11e8-887b-47a3ea3d3061.png\"><img src=\"https://user-images.githubusercontent.com/24420973/40581921-16b78f98-613c-11e8-887b-47a3ea3d3061.png\" alt=\"tape\" style=\"max-width:100%;\"></a></p>\n<p>Should I open a new issue for this?</p>", "body_text": "Thank you for the fix @akshaym and @allenlavoie ! It did solve that problem. However, now I am observing another leak when I use tfe.GradientTape(). The code to reproduce the problem, and the memory plot are below:\nimport tensorflow as tf\n\ntf.enable_eager_execution()\ntfe = tf.contrib.eager\n\nclass Net(tf.keras.Model):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = tf.keras.layers.Dense(5)\n\n    def call(self, x):\n        y = self.fc1(x)\n        return y\n\ninputs = tf.zeros([32, 100], tf.float32)\nnet = Net()\nfor i in range(1000000):\n    with tfe.GradientTape() as tape:\n        net(inputs)\n\nShould I open a new issue for this?", "body": "Thank you for the fix @akshaym and @allenlavoie ! It did solve that problem. However, now I am observing another leak when I use tfe.GradientTape(). The code to reproduce the problem, and the memory plot are below:\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\ntf.enable_eager_execution()\r\ntfe = tf.contrib.eager\r\n\r\nclass Net(tf.keras.Model):\r\n    def __init__(self):\r\n        super(Net, self).__init__()\r\n        self.fc1 = tf.keras.layers.Dense(5)\r\n\r\n    def call(self, x):\r\n        y = self.fc1(x)\r\n        return y\r\n\r\ninputs = tf.zeros([32, 100], tf.float32)\r\nnet = Net()\r\nfor i in range(1000000):\r\n    with tfe.GradientTape() as tape:\r\n        net(inputs)\r\n```\r\n![tape](https://user-images.githubusercontent.com/24420973/40581921-16b78f98-613c-11e8-887b-47a3ea3d3061.png)\r\n\r\nShould I open a new issue for this?"}