{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/304812350", "html_url": "https://github.com/tensorflow/tensorflow/issues/1300#issuecomment-304812350", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1300", "id": 304812350, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNDgxMjM1MA==", "user": {"login": "tblaschke", "id": 21105057, "node_id": "MDQ6VXNlcjIxMTA1MDU3", "avatar_url": "https://avatars2.githubusercontent.com/u/21105057?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tblaschke", "html_url": "https://github.com/tblaschke", "followers_url": "https://api.github.com/users/tblaschke/followers", "following_url": "https://api.github.com/users/tblaschke/following{/other_user}", "gists_url": "https://api.github.com/users/tblaschke/gists{/gist_id}", "starred_url": "https://api.github.com/users/tblaschke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tblaschke/subscriptions", "organizations_url": "https://api.github.com/users/tblaschke/orgs", "repos_url": "https://api.github.com/users/tblaschke/repos", "events_url": "https://api.github.com/users/tblaschke/events{/privacy}", "received_events_url": "https://api.github.com/users/tblaschke/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-30T08:38:58Z", "updated_at": "2017-05-30T08:38:58Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3407450\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/icyblade\">@icyblade</a><br>\nNo. FP16 is half precision, not single precision. The stated 5591\u20138736 GFLOPS are single precision (FP32) performance. As sbrodehl mentioned: Up to now only the P100 supports FP16. Other cards have crippled FP16 support/performance</p>", "body_text": "@icyblade\nNo. FP16 is half precision, not single precision. The stated 5591\u20138736 GFLOPS are single precision (FP32) performance. As sbrodehl mentioned: Up to now only the P100 supports FP16. Other cards have crippled FP16 support/performance", "body": "@icyblade \r\nNo. FP16 is half precision, not single precision. The stated 5591\u20138736 GFLOPS are single precision (FP32) performance. As sbrodehl mentioned: Up to now only the P100 supports FP16. Other cards have crippled FP16 support/performance"}