{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/194349719", "html_url": "https://github.com/tensorflow/tensorflow/issues/1300#issuecomment-194349719", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1300", "id": 194349719, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NDM0OTcxOQ==", "user": {"login": "sesse", "id": 238700, "node_id": "MDQ6VXNlcjIzODcwMA==", "avatar_url": "https://avatars0.githubusercontent.com/u/238700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sesse", "html_url": "https://github.com/sesse", "followers_url": "https://api.github.com/users/sesse/followers", "following_url": "https://api.github.com/users/sesse/following{/other_user}", "gists_url": "https://api.github.com/users/sesse/gists{/gist_id}", "starred_url": "https://api.github.com/users/sesse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sesse/subscriptions", "organizations_url": "https://api.github.com/users/sesse/orgs", "repos_url": "https://api.github.com/users/sesse/repos", "events_url": "https://api.github.com/users/sesse/events{/privacy}", "received_events_url": "https://api.github.com/users/sesse/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-09T15:36:56Z", "updated_at": "2016-03-09T15:36:56Z", "author_association": "NONE", "body_html": "<p>An update here; the Eigen::half class (making half look roughly like any other arithmetic type) works. I've added half support to a bunch of ops (with the class in place, it's mostly only about adding the right template instantiations), including some a very few select cuDNN and cuBLAS operations. I can do training of a CNN example with most ops in float16 (I need to cast back to float32 for some operations, like maxpool and dropout, that don't have implementations yet), but if I have too many of them, training starts to go slower, I believe this has something to do with gradients, but it's hard to say without the tests.</p>\n<p>Some operations that are about composing Eigen stuff probably would be faster (and more accurate) by just being a composition of cast to float32, the operation, and then cast back. We'll see about that later.</p>\n<p>Overall I'm going to try to put things piecewise through code reviews, although it is going somewhat slowly, mostly due to boring fiddly issues about build environments.</p>", "body_text": "An update here; the Eigen::half class (making half look roughly like any other arithmetic type) works. I've added half support to a bunch of ops (with the class in place, it's mostly only about adding the right template instantiations), including some a very few select cuDNN and cuBLAS operations. I can do training of a CNN example with most ops in float16 (I need to cast back to float32 for some operations, like maxpool and dropout, that don't have implementations yet), but if I have too many of them, training starts to go slower, I believe this has something to do with gradients, but it's hard to say without the tests.\nSome operations that are about composing Eigen stuff probably would be faster (and more accurate) by just being a composition of cast to float32, the operation, and then cast back. We'll see about that later.\nOverall I'm going to try to put things piecewise through code reviews, although it is going somewhat slowly, mostly due to boring fiddly issues about build environments.", "body": "An update here; the Eigen::half class (making half look roughly like any other arithmetic type) works. I've added half support to a bunch of ops (with the class in place, it's mostly only about adding the right template instantiations), including some a very few select cuDNN and cuBLAS operations. I can do training of a CNN example with most ops in float16 (I need to cast back to float32 for some operations, like maxpool and dropout, that don't have implementations yet), but if I have too many of them, training starts to go slower, I believe this has something to do with gradients, but it's hard to say without the tests.\n\nSome operations that are about composing Eigen stuff probably would be faster (and more accurate) by just being a composition of cast to float32, the operation, and then cast back. We'll see about that later.\n\nOverall I'm going to try to put things piecewise through code reviews, although it is going somewhat slowly, mostly due to boring fiddly issues about build environments.\n"}