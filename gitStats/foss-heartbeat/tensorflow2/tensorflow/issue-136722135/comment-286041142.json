{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286041142", "html_url": "https://github.com/tensorflow/tensorflow/issues/1300#issuecomment-286041142", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1300", "id": 286041142, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjA0MTE0Mg==", "user": {"login": "icyblade", "id": 3407450, "node_id": "MDQ6VXNlcjM0MDc0NTA=", "avatar_url": "https://avatars1.githubusercontent.com/u/3407450?v=4", "gravatar_id": "", "url": "https://api.github.com/users/icyblade", "html_url": "https://github.com/icyblade", "followers_url": "https://api.github.com/users/icyblade/followers", "following_url": "https://api.github.com/users/icyblade/following{/other_user}", "gists_url": "https://api.github.com/users/icyblade/gists{/gist_id}", "starred_url": "https://api.github.com/users/icyblade/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/icyblade/subscriptions", "organizations_url": "https://api.github.com/users/icyblade/orgs", "repos_url": "https://api.github.com/users/icyblade/repos", "events_url": "https://api.github.com/users/icyblade/events{/privacy}", "received_events_url": "https://api.github.com/users/icyblade/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-13T08:18:51Z", "updated_at": "2017-03-13T08:18:51Z", "author_association": "NONE", "body_html": "<p>Confirm a slow speed on fp16 just like <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7113492\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/sbrodehl\">@sbrodehl</a> .</p>\n<h2>Environment</h2>\n<p>Tesla K80 + CUDA 8.0 + cuDNN v5.1</p>\n<p>Tensorflow v0.12.1</p>\n<p><code>nvidia-smi</code> information:</p>\n<pre><code># nvidia-smi\nMon Mar 13 16:14:44 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:00:08.0     Off |                  Off |\n| N/A   52C    P0    57W / 149W |   1543MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:00:09.0     Off |                  Off |\n| N/A   39C    P0    68W / 149W |   1458MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre>\n<h2>FP32 result</h2>\n<pre><code># python cifar10_train.py\n2017-03-13 16:08:27.988583: step 0, loss = 4.68 (7.6 examples/sec; 16.831 sec/batch)\n2017-03-13 16:08:29.203714: step 10, loss = 4.61 (805.1 examples/sec; 0.159 sec/batch)\n2017-03-13 16:08:30.726646: step 20, loss = 4.50 (759.7 examples/sec; 0.168 sec/batch)\n2017-03-13 16:08:32.171580: step 30, loss = 4.43 (965.8 examples/sec; 0.133 sec/batch)\n2017-03-13 16:08:33.639704: step 40, loss = 4.40 (831.2 examples/sec; 0.154 sec/batch)\n2017-03-13 16:08:35.092967: step 50, loss = 4.34 (964.5 examples/sec; 0.133 sec/batch)\n2017-03-13 16:08:36.590003: step 60, loss = 4.28 (865.6 examples/sec; 0.148 sec/batch)\n</code></pre>\n<p><code>nvidia-smi</code> shows that GPU-util is around 50%.</p>\n<h2>FP16 result</h2>\n<pre><code># python cifar10_train.py --use_fp16\n2017-03-13 16:12:18.382315: step 0, loss = 4.67 (7.2 examples/sec; 17.864 sec/batch)\n2017-03-13 16:12:28.586276: step 10, loss = 4.64 (130.9 examples/sec; 0.978 sec/batch)\n2017-03-13 16:12:38.912454: step 20, loss = 4.44 (126.1 examples/sec; 1.015 sec/batch)\n2017-03-13 16:12:49.141316: step 30, loss = 4.52 (130.9 examples/sec; 0.978 sec/batch)\n2017-03-13 16:12:59.328050: step 40, loss = 4.43 (129.4 examples/sec; 0.989 sec/batch)\n2017-03-13 16:13:09.573551: step 50, loss = 4.42 (132.3 examples/sec; 0.967 sec/batch)\n2017-03-13 16:13:20.116336: step 60, loss = 4.49 (106.9 examples/sec; 1.197 sec/batch)\n</code></pre>\n<p><code>nvidia-smi</code> shows that GPU-util is around 10% only.</p>", "body_text": "Confirm a slow speed on fp16 just like @sbrodehl .\nEnvironment\nTesla K80 + CUDA 8.0 + cuDNN v5.1\nTensorflow v0.12.1\nnvidia-smi information:\n# nvidia-smi\nMon Mar 13 16:14:44 2017\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 0000:00:08.0     Off |                  Off |\n| N/A   52C    P0    57W / 149W |   1543MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n|   1  Tesla K80           Off  | 0000:00:09.0     Off |                  Off |\n| N/A   39C    P0    68W / 149W |   1458MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\nFP32 result\n# python cifar10_train.py\n2017-03-13 16:08:27.988583: step 0, loss = 4.68 (7.6 examples/sec; 16.831 sec/batch)\n2017-03-13 16:08:29.203714: step 10, loss = 4.61 (805.1 examples/sec; 0.159 sec/batch)\n2017-03-13 16:08:30.726646: step 20, loss = 4.50 (759.7 examples/sec; 0.168 sec/batch)\n2017-03-13 16:08:32.171580: step 30, loss = 4.43 (965.8 examples/sec; 0.133 sec/batch)\n2017-03-13 16:08:33.639704: step 40, loss = 4.40 (831.2 examples/sec; 0.154 sec/batch)\n2017-03-13 16:08:35.092967: step 50, loss = 4.34 (964.5 examples/sec; 0.133 sec/batch)\n2017-03-13 16:08:36.590003: step 60, loss = 4.28 (865.6 examples/sec; 0.148 sec/batch)\n\nnvidia-smi shows that GPU-util is around 50%.\nFP16 result\n# python cifar10_train.py --use_fp16\n2017-03-13 16:12:18.382315: step 0, loss = 4.67 (7.2 examples/sec; 17.864 sec/batch)\n2017-03-13 16:12:28.586276: step 10, loss = 4.64 (130.9 examples/sec; 0.978 sec/batch)\n2017-03-13 16:12:38.912454: step 20, loss = 4.44 (126.1 examples/sec; 1.015 sec/batch)\n2017-03-13 16:12:49.141316: step 30, loss = 4.52 (130.9 examples/sec; 0.978 sec/batch)\n2017-03-13 16:12:59.328050: step 40, loss = 4.43 (129.4 examples/sec; 0.989 sec/batch)\n2017-03-13 16:13:09.573551: step 50, loss = 4.42 (132.3 examples/sec; 0.967 sec/batch)\n2017-03-13 16:13:20.116336: step 60, loss = 4.49 (106.9 examples/sec; 1.197 sec/batch)\n\nnvidia-smi shows that GPU-util is around 10% only.", "body": "Confirm a slow speed on fp16 just like @sbrodehl .\r\n\r\n## Environment\r\n\r\nTesla K80 + CUDA 8.0 + cuDNN v5.1\r\n\r\nTensorflow v0.12.1\r\n\r\n`nvidia-smi` information:\r\n\r\n```\r\n# nvidia-smi\r\nMon Mar 13 16:14:44 2017\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 367.48                 Driver Version: 367.48                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 0000:00:08.0     Off |                  Off |\r\n| N/A   52C    P0    57W / 149W |   1543MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n|   1  Tesla K80           Off  | 0000:00:09.0     Off |                  Off |\r\n| N/A   39C    P0    68W / 149W |   1458MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```\r\n\r\n## FP32 result\r\n\r\n```\r\n# python cifar10_train.py\r\n2017-03-13 16:08:27.988583: step 0, loss = 4.68 (7.6 examples/sec; 16.831 sec/batch)\r\n2017-03-13 16:08:29.203714: step 10, loss = 4.61 (805.1 examples/sec; 0.159 sec/batch)\r\n2017-03-13 16:08:30.726646: step 20, loss = 4.50 (759.7 examples/sec; 0.168 sec/batch)\r\n2017-03-13 16:08:32.171580: step 30, loss = 4.43 (965.8 examples/sec; 0.133 sec/batch)\r\n2017-03-13 16:08:33.639704: step 40, loss = 4.40 (831.2 examples/sec; 0.154 sec/batch)\r\n2017-03-13 16:08:35.092967: step 50, loss = 4.34 (964.5 examples/sec; 0.133 sec/batch)\r\n2017-03-13 16:08:36.590003: step 60, loss = 4.28 (865.6 examples/sec; 0.148 sec/batch)\r\n```\r\n\r\n`nvidia-smi` shows that GPU-util is around 50%.\r\n\r\n## FP16 result\r\n\r\n```\r\n# python cifar10_train.py --use_fp16\r\n2017-03-13 16:12:18.382315: step 0, loss = 4.67 (7.2 examples/sec; 17.864 sec/batch)\r\n2017-03-13 16:12:28.586276: step 10, loss = 4.64 (130.9 examples/sec; 0.978 sec/batch)\r\n2017-03-13 16:12:38.912454: step 20, loss = 4.44 (126.1 examples/sec; 1.015 sec/batch)\r\n2017-03-13 16:12:49.141316: step 30, loss = 4.52 (130.9 examples/sec; 0.978 sec/batch)\r\n2017-03-13 16:12:59.328050: step 40, loss = 4.43 (129.4 examples/sec; 0.989 sec/batch)\r\n2017-03-13 16:13:09.573551: step 50, loss = 4.42 (132.3 examples/sec; 0.967 sec/batch)\r\n2017-03-13 16:13:20.116336: step 60, loss = 4.49 (106.9 examples/sec; 1.197 sec/batch)\r\n```\r\n\r\n`nvidia-smi` shows that GPU-util is around 10% only."}