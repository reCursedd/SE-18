{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/191694915", "html_url": "https://github.com/tensorflow/tensorflow/issues/1300#issuecomment-191694915", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1300", "id": 191694915, "node_id": "MDEyOklzc3VlQ29tbWVudDE5MTY5NDkxNQ==", "user": {"login": "sesse", "id": 238700, "node_id": "MDQ6VXNlcjIzODcwMA==", "avatar_url": "https://avatars0.githubusercontent.com/u/238700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sesse", "html_url": "https://github.com/sesse", "followers_url": "https://api.github.com/users/sesse/followers", "following_url": "https://api.github.com/users/sesse/following{/other_user}", "gists_url": "https://api.github.com/users/sesse/gists{/gist_id}", "starred_url": "https://api.github.com/users/sesse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sesse/subscriptions", "organizations_url": "https://api.github.com/users/sesse/orgs", "repos_url": "https://api.github.com/users/sesse/repos", "events_url": "https://api.github.com/users/sesse/events{/privacy}", "received_events_url": "https://api.github.com/users/sesse/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-03T10:21:13Z", "updated_at": "2016-03-03T10:21:13Z", "author_association": "NONE", "body_html": "<p>I'm currently working on this. There's a fair amount of work to do\u2014the CUDA \u201chalf\u201d type is just a struct of uint16_t, which doesn't really match what Eigen expects. In particular, Eigen expects all such arithmetic types to have operator overloads, converts to and from other arithmetic types (including even bool), etc. The actual arithmetic intrinsics require CUDA compute 5.3 (which right now means essentially Tegra X1 only), so mostly, we'll have to rely on casting on both sides and then doing the actual calculations in float.</p>\n<p>Also, we're going to need conversions on the CPU because e.g. cuBLAS functions might require the alpha and beta to be in half, we often need a definition of zero, etc.. We don't need these to be fast, though; fp16 is generally not a win on CPU.</p>", "body_text": "I'm currently working on this. There's a fair amount of work to do\u2014the CUDA \u201chalf\u201d type is just a struct of uint16_t, which doesn't really match what Eigen expects. In particular, Eigen expects all such arithmetic types to have operator overloads, converts to and from other arithmetic types (including even bool), etc. The actual arithmetic intrinsics require CUDA compute 5.3 (which right now means essentially Tegra X1 only), so mostly, we'll have to rely on casting on both sides and then doing the actual calculations in float.\nAlso, we're going to need conversions on the CPU because e.g. cuBLAS functions might require the alpha and beta to be in half, we often need a definition of zero, etc.. We don't need these to be fast, though; fp16 is generally not a win on CPU.", "body": "I'm currently working on this. There's a fair amount of work to do\u2014the CUDA \u201chalf\u201d type is just a struct of uint16_t, which doesn't really match what Eigen expects. In particular, Eigen expects all such arithmetic types to have operator overloads, converts to and from other arithmetic types (including even bool), etc. The actual arithmetic intrinsics require CUDA compute 5.3 (which right now means essentially Tegra X1 only), so mostly, we'll have to rely on casting on both sides and then doing the actual calculations in float.\n\nAlso, we're going to need conversions on the CPU because e.g. cuBLAS functions might require the alpha and beta to be in half, we often need a definition of zero, etc.. We don't need these to be fast, though; fp16 is generally not a win on CPU.\n"}