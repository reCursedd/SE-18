{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/222250443", "html_url": "https://github.com/tensorflow/tensorflow/issues/1300#issuecomment-222250443", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1300", "id": 222250443, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMjI1MDQ0Mw==", "user": {"login": "sesse", "id": 238700, "node_id": "MDQ6VXNlcjIzODcwMA==", "avatar_url": "https://avatars0.githubusercontent.com/u/238700?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sesse", "html_url": "https://github.com/sesse", "followers_url": "https://api.github.com/users/sesse/followers", "following_url": "https://api.github.com/users/sesse/following{/other_user}", "gists_url": "https://api.github.com/users/sesse/gists{/gist_id}", "starred_url": "https://api.github.com/users/sesse/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sesse/subscriptions", "organizations_url": "https://api.github.com/users/sesse/orgs", "repos_url": "https://api.github.com/users/sesse/repos", "events_url": "https://api.github.com/users/sesse/events{/privacy}", "received_events_url": "https://api.github.com/users/sesse/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-27T20:52:06Z", "updated_at": "2016-05-27T20:52:06Z", "author_association": "NONE", "body_html": "<p><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/99671f3a705789ef217c7bca92409add3cf529e5/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/99671f3a705789ef217c7bca92409add3cf529e5\"><tt>99671f3</tt></a> fp16-enables ReverseOp. <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/6164d02144239c58a8f19cd12ff2a3ff7b7605d4/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/6164d02144239c58a8f19cd12ff2a3ff7b7605d4\"><tt>6164d02</tt></a> (by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a>) fp16-enables AddN. <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/bb0190f6c26bf11f601102dfe2166a68a7833020/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/bb0190f6c26bf11f601102dfe2166a68a7833020\"><tt>bb0190f</tt></a> (also by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6969686\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benoitsteiner\">@benoitsteiner</a>) fp16-enables the softmax ops. <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/80da0a63200cb7c9c449188620992c7a8d18c8b9/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/80da0a63200cb7c9c449188620992c7a8d18c8b9\"><tt>80da0a6</tt></a> fp16-enables the resize ops, although you should note that some of them always output float no matter the input (this was preserving existing behavior, although I'm not sure it makes sense for half or double).</p>\n<p>And\u2026 <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/4f257a2427ba0414bd7513c9b61fb835870bd3cf/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/4f257a2427ba0414bd7513c9b61fb835870bd3cf\"><tt>4f257a2</tt></a> fp16-enables convolutions on GPU, assuming you have CUDA 7.5 or newer.</p>\n<p>You still can't MaxPool/AvgPool, but that's up for next week. After that, I will declare this feature complete, since you can actually do useful stuff with it. There's always more performance to be had, though (currently it's pretty much performance-neutral on Maxwell, so it's mostly about the memory savings), more ops that need conversion and so on, but that should probably live outside this bug. And that work will be continued by Benoit alone, as I'm leaving Google and thus TensorFlow. :-)</p>", "body_text": "99671f3 fp16-enables ReverseOp. 6164d02 (by @benoitsteiner) fp16-enables AddN. bb0190f (also by @benoitsteiner) fp16-enables the softmax ops. 80da0a6 fp16-enables the resize ops, although you should note that some of them always output float no matter the input (this was preserving existing behavior, although I'm not sure it makes sense for half or double).\nAnd\u2026 4f257a2 fp16-enables convolutions on GPU, assuming you have CUDA 7.5 or newer.\nYou still can't MaxPool/AvgPool, but that's up for next week. After that, I will declare this feature complete, since you can actually do useful stuff with it. There's always more performance to be had, though (currently it's pretty much performance-neutral on Maxwell, so it's mostly about the memory savings), more ops that need conversion and so on, but that should probably live outside this bug. And that work will be continued by Benoit alone, as I'm leaving Google and thus TensorFlow. :-)", "body": "99671f3a705789ef217c7bca92409add3cf529e5 fp16-enables ReverseOp. 6164d02144239c58a8f19cd12ff2a3ff7b7605d4 (by @benoitsteiner) fp16-enables AddN. bb0190f6c26bf11f601102dfe2166a68a7833020 (also by @benoitsteiner) fp16-enables the softmax ops. 80da0a63200cb7c9c449188620992c7a8d18c8b9 fp16-enables the resize ops, although you should note that some of them always output float no matter the input (this was preserving existing behavior, although I'm not sure it makes sense for half or double).\n\nAnd\u2026 4f257a2427ba0414bd7513c9b61fb835870bd3cf fp16-enables convolutions on GPU, assuming you have CUDA 7.5 or newer.\n\nYou still can't MaxPool/AvgPool, but that's up for next week. After that, I will declare this feature complete, since you can actually do useful stuff with it. There's always more performance to be had, though (currently it's pretty much performance-neutral on Maxwell, so it's mostly about the memory savings), more ops that need conversion and so on, but that should probably live outside this bug. And that work will be continued by Benoit alone, as I'm leaving Google and thus TensorFlow. :-)\n"}