{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19200", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19200/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19200/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19200/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19200", "id": 321962058, "node_id": "MDU6SXNzdWUzMjE5NjIwNTg=", "number": 19200, "title": "Difference in output between CPU and GPU ", "user": {"login": "annemenini", "id": 13631130, "node_id": "MDQ6VXNlcjEzNjMxMTMw", "avatar_url": "https://avatars0.githubusercontent.com/u/13631130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/annemenini", "html_url": "https://github.com/annemenini", "followers_url": "https://api.github.com/users/annemenini/followers", "following_url": "https://api.github.com/users/annemenini/following{/other_user}", "gists_url": "https://api.github.com/users/annemenini/gists{/gist_id}", "starred_url": "https://api.github.com/users/annemenini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/annemenini/subscriptions", "organizations_url": "https://api.github.com/users/annemenini/orgs", "repos_url": "https://api.github.com/users/annemenini/repos", "events_url": "https://api.github.com/users/annemenini/events{/privacy}", "received_events_url": "https://api.github.com/users/annemenini/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2018-05-10T14:55:12Z", "updated_at": "2018-11-20T12:28:04Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Custom code</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)</li>\n<li><strong>Python version</strong>: Python 3.5.4</li>\n<li><strong>Bazel version (if compiling from source)</strong>: n/a</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: n/a</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA 9.0, cudnn 7.0.5</li>\n<li><strong>GPU model and memory</strong>: GPU GTX1080</li>\n<li><strong>Exact command to reproduce</strong>: n/a</li>\n</ul>\n<p><strong>Problem</strong><br>\nI see a very different output when executing a training on CPU vs. GPU. The problem does not seem to be related to initialization or numerical precision (see investigation): same code, same data, same machine, just switching between GPU and CPU execution by adding <code>os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"</code> gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).</p>\n<p><strong>Investigation</strong><br>\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a <code>tf.multiply</code> (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.</p>\n<p>Unfortunately, I am not able to reproduce the problem in a unit test (but am still working on it). If I feed the same inputs to <code>tf.multiply</code> in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.</p>\n<p><strong>Seeding and reproducibility</strong><br>\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.</p>\n<p>The problem is reproducible from one run to the other.</p>\n<p><strong>Environment dependency</strong><br>\nThe problem occurs with both TF 1.7 and TF 1.8. However, if I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.</p>\n<p>Any idea or debugging experiment suggestion is very welcome.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Custom code\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)\nPython version: Python 3.5.4\nBazel version (if compiling from source): n/a\nGCC/Compiler version (if compiling from source): n/a\nCUDA/cuDNN version: CUDA 9.0, cudnn 7.0.5\nGPU model and memory: GPU GTX1080\nExact command to reproduce: n/a\n\nProblem\nI see a very different output when executing a training on CPU vs. GPU. The problem does not seem to be related to initialization or numerical precision (see investigation): same code, same data, same machine, just switching between GPU and CPU execution by adding os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).\nInvestigation\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a tf.multiply (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.\nUnfortunately, I am not able to reproduce the problem in a unit test (but am still working on it). If I feed the same inputs to tf.multiply in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.\nSeeding and reproducibility\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.\nThe problem is reproducible from one run to the other.\nEnvironment dependency\nThe problem occurs with both TF 1.7 and TF 1.8. However, if I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.\nAny idea or debugging experiment suggestion is very welcome.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Custom code\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: TF 1.7 and TF 1.8 (v1.8.0-0-g93bc2e2072 1.8.0)\r\n- **Python version**: Python 3.5.4\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: CUDA 9.0, cudnn 7.0.5\r\n- **GPU model and memory**: GPU GTX1080\r\n- **Exact command to reproduce**: n/a\r\n\r\n**Problem**\r\nI see a very different output when executing a training on CPU vs. GPU. The problem does not seem to be related to initialization or numerical precision (see investigation): same code, same data, same machine, just switching between GPU and CPU execution by adding `os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"` gives fundamentally different results (the loss function differs by several orders of magnitude at the end of the first run of the first epoch).\r\n\r\n**Investigation**\r\nI gave names to all the variables and operations in order to track what was the first operation in the graph where the results differ between CPU and GPU execution. I could actually identify it. In my case, it is a `tf.multiply` (pixel-wise multiplication of complex64 Tensors). I can see that the inputs of that operation are identical (+/- numerical precision), while the outputs are really different: the magnitude of the complex output is consistent but the phase is very different (the real and imaginary parts are both very different). I can tell that the CPU output is the right one.\r\n\r\nUnfortunately, I am not able to reproduce the problem in a unit test (but am still working on it). If I feed the same inputs to `tf.multiply` in a unit test, the CPU and GPU outputs are consistent. I am wondering if the CPU/GPU execution may be affecting the order in which the graph is build/executed, and if there may be interferences somehow.\r\n\r\n**Seeding and reproducibility**\r\nRegarding the seeds, they are all set (graph wise and operation wise), but anyway, the diverging operation does not depend on a seed in this case.\r\n\r\nThe problem is reproducible from one run to the other.\r\n\r\n**Environment dependency**\r\nThe problem occurs with both TF 1.7 and TF 1.8. However, if I switch the environment (same machine, same OS, same code, same data, but python 2.7, TF 1.4), the problem disappears, i.e. the CPU and GPU outputs are consistent.\r\n\r\nAny idea or debugging experiment suggestion is very welcome.\r\n"}