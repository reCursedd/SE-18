{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/398448108", "html_url": "https://github.com/tensorflow/tensorflow/issues/19200#issuecomment-398448108", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19200", "id": 398448108, "node_id": "MDEyOklzc3VlQ29tbWVudDM5ODQ0ODEwOA==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-19T15:47:45Z", "updated_at": "2018-06-20T06:44:26Z", "author_association": "NONE", "body_html": "<p>Hi guys, I have exactly the same issue when running the same code on CPU and on GPU ... my guess is that computation on some operations are numerically instable on GPU (but I am not a pro in numerical computation ....).<br>\nHere is first my setup :<br>\n<strong>I am on Ubuntu 16.04.4 LTS with Tensorflow built from source (I have both 1.7 and 1.8). I have both python version (2.7 and 3.5), bazel version is 0.14, the GCC compiler is 5.4, CUDA is 9.2 and cudnn 7.1.4. I have 2 GPUs, both are NVIDIA GTX 1080Ti.</strong><br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a>, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=32465472\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/angersson\">@angersson</a> as requested, I created a snippet that easily reproduces my issue.</p>\n<p><em><strong>ON CPU</strong></em></p>\n<pre><code>import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\nimport numpy as np\nimport tensorflow as tf\ndata = np.array(\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\n     range(1, 33)]).astype(np.float32)\n\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\n\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\n\nweights = tf.ones([8, 8, 16, 1])\nbiases = tf.ones([16])\n\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\n\nsess = tf.Session()\n\nwith sess.as_default():\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\n\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\nprint(transposed)\n</code></pre>\n<p>Just for some clarity issue, I print the end of what the snippet outputs <strong>each time</strong>:</p>\n<pre><code>[[[  2.          2.          2.        ...   2.          2.\n       2.       ]\n    [  4.          4.          4.        ...   4.          4.\n       4.       ]\n    [  7.          7.          7.        ...   7.          7.\n       7.       ]\n    ...\n    [ 22.         22.         22.        ...  22.         22.\n      22.       ]\n    [ 16.         16.         16.        ...  16.         16.\n      16.       ]\n    [  9.          9.          9.        ...   9.          9.\n       9.       ]]\n\n   [[ 11.         11.         11.        ...  11.         11.\n      11.       ]\n    [ 23.         23.         23.        ...  23.         23.\n      23.       ]\n    [ 37.         37.         37.        ...  37.         37.\n      37.       ]\n    ...\n    [ 67.         67.         67.        ...  67.         67.\n      67.       ]\n    [ 47.         47.         47.        ...  47.         47.\n      47.       ]\n    [ 25.         25.         25.        ...  25.         25.\n      25.       ]]\n\n   [[ 28.         28.         28.        ...  28.         28.\n      28.       ]\n    [ 58.         58.         58.        ...  58.         58.\n      58.       ]\n    [ 91.         91.         91.        ...  91.         91.\n      91.       ]\n    ...\n    [136.        136.        136.        ... 136.        136.\n     136.       ]\n    [ 94.         94.         94.        ...  94.         94.\n      94.       ]\n    [ 49.         49.         49.        ...  49.         49.\n      49.       ]]\n\n   ...\n\n   [[268.        268.        268.        ... 268.        268.\n     268.       ]\n    [538.        538.        538.        ... 538.        538.\n     538.       ]\n    [811.        811.        811.        ... 811.        811.\n     811.       ]\n    ...\n    [856.        856.        856.        ... 856.        856.\n     856.       ]\n    [574.        574.        574.        ... 574.        574.\n     574.       ]\n    [289.        289.        289.        ... 289.        289.\n     289.       ]]\n\n   [[187.        187.        187.        ... 187.        187.\n     187.       ]\n    [375.        375.        375.        ... 375.        375.\n     375.       ]\n    [565.        565.        565.        ... 565.        565.\n     565.       ]\n    ...\n    [595.        595.        595.        ... 595.        595.\n     595.       ]\n    [399.        399.        399.        ... 399.        399.\n     399.       ]\n    [201.        201.        201.        ... 201.        201.\n     201.       ]]\n\n   [[ 98.         98.         98.        ...  98.         98.\n      98.       ]\n    [196.        196.        196.        ... 196.        196.\n     196.       ]\n    [295.        295.        295.        ... 295.        295.\n     295.       ]\n    ...\n    [310.        310.        310.        ... 310.        310.\n     310.       ]\n    [208.        208.        208.        ... 208.        208.\n     208.       ]\n    [105.        105.        105.        ... 105.        105.\n     105.       ]]]]]\n</code></pre>\n<p><strong><em>ON GPU</em></strong></p>\n<pre><code>import numpy as np\nimport tensorflow as tf\ndata = np.array(\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\n     range(1, 33)]).astype(np.float32)\n\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\n\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\n\nweights = tf.ones([8, 8, 16, 1])\nbiases = tf.ones([16])\n\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\n\nsess = tf.Session()\n\nwith sess.as_default():\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\n\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\nprint(transposed)\n</code></pre>\n<p>It is exactly the same code (except the cuda visible command), and <strong>sometimes</strong> I have exactly the same aforementioned output, <strong>but I also have these</strong> outputs:</p>\n<pre><code>[[[  1.9999084   1.9999084   1.9999084 ...   1.9999084   1.9999084\n       1.9999084]\n    [  4.          4.          4.        ...   4.          4.\n       4.       ]\n    [  7.          7.          7.        ...   7.          7.\n       7.       ]\n    ...\n    [ 22.         22.         22.        ...  22.         22.\n      22.       ]\n    [ 15.9999695  15.9999695  15.9999695 ...  15.9999695  15.9999695\n      15.9999695]\n    [  8.999954    8.999954    8.999954  ...   8.999954    8.999954\n       8.999954 ]]\n\n   [[ 10.999954   10.999954   10.999954  ...  10.999954   10.999954\n      10.999954 ]\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\n      22.99997  ]\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\n      37.00006  ]\n    ...\n    [ 67.00003    67.00003    67.00003   ...  67.00003    67.00003\n      67.00003  ]\n    [ 46.99997    46.99997    46.99997   ...  46.99997    46.99997\n      46.99997  ]\n    [ 25.         25.         25.        ...  25.         25.\n      25.       ]]\n\n   [[ 27.999954   27.999954   27.999954  ...  27.999954   27.999954\n      27.999954 ]\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\n      57.99997  ]\n    [ 90.999985   90.999985   90.999985  ...  90.999985   90.999985\n      90.999985 ]\n    ...\n    [136.        136.        136.        ... 136.        136.\n     136.       ]\n    [ 93.999985   93.999985   93.999985  ...  93.999985   93.999985\n      93.999985 ]\n    [ 48.999954   48.999954   48.999954  ...  48.999954   48.999954\n      48.999954 ]]\n\n   ...\n\n   [[268.        268.        268.        ... 268.        268.\n     268.       ]\n    [538.        538.        538.        ... 538.        538.\n     538.       ]\n    [810.99994   810.99994   810.99994   ... 810.99994   810.99994\n     810.99994  ]\n    ...\n    [856.        856.        856.        ... 856.        856.\n     856.       ]\n    [574.        574.        574.        ... 574.        574.\n     574.       ]\n    [289.0001    289.0001    289.0001    ... 289.0001    289.0001\n     289.0001   ]]\n\n   [[186.99992   186.99992   186.99992   ... 186.99992   186.99992\n     186.99992  ]\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\n     374.99997  ]\n    [565.        565.        565.        ... 565.        565.\n     565.       ]\n    ...\n    [595.        595.        595.        ... 595.        595.\n     595.       ]\n    [399.00006   399.00006   399.00006   ... 399.00006   399.00006\n     399.00006  ]\n    [201.00008   201.00008   201.00008   ... 201.00008   201.00008\n     201.00008  ]]\n\n   [[ 97.99998    97.99998    97.99998   ...  97.99998    97.99998\n      97.99998  ]\n    [195.99997   195.99997   195.99997   ... 195.99997   195.99997\n     195.99997  ]\n    [295.        295.        295.        ... 295.        295.\n     295.       ]\n    ...\n    [310.00003   310.00003   310.00003   ... 310.00003   310.00003\n     310.00003  ]\n    [208.00012   208.00012   208.00012   ... 208.00012   208.00012\n     208.00012  ]\n    [105.00009   105.00009   105.00009   ... 105.00009   105.00009\n     105.00009  ]]]]]\n</code></pre>\n<p>or</p>\n<pre><code>  [[[  1.9999237   1.9999237   1.9999237 ...   1.9999237   1.9999237\n       1.9999237]\n    [  4.          4.          4.        ...   4.          4.\n       4.       ]\n    [  7.          7.          7.        ...   7.          7.\n       7.       ]\n    ...\n    [ 22.         22.         22.        ...  22.         22.\n      22.       ]\n    [ 16.         16.         16.        ...  16.         16.\n      16.       ]\n    [  9.000046    9.000046    9.000046  ...   9.000046    9.000046\n       9.000046 ]]\n\n   [[ 11.000015   11.000015   11.000015  ...  11.000015   11.000015\n      11.000015 ]\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\n      22.99997  ]\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\n      37.00006  ]\n    ...\n    [ 67.         67.         67.        ...  67.         67.\n      67.       ]\n    [ 47.00006    47.00006    47.00006   ...  47.00006    47.00006\n      47.00006  ]\n    [ 25.000076   25.000076   25.000076  ...  25.000076   25.000076\n      25.000076 ]]\n\n   [[ 27.999962   27.999962   27.999962  ...  27.999962   27.999962\n      27.999962 ]\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\n      57.99997  ]\n    [ 91.00003    91.00003    91.00003   ...  91.00003    91.00003\n      91.00003  ]\n    ...\n    [136.00002   136.00002   136.00002   ... 136.00002   136.00002\n     136.00002  ]\n    [ 94.000046   94.000046   94.000046  ...  94.000046   94.000046\n      94.000046 ]\n    [ 49.00006    49.00006    49.00006   ...  49.00006    49.00006\n      49.00006  ]]\n\n   ...\n\n   [[268.00006   268.00006   268.00006   ... 268.00006   268.00006\n     268.00006  ]\n    [538.        538.        538.        ... 538.        538.\n     538.       ]\n    [810.9999    810.9999    810.9999    ... 810.9999    810.9999\n     810.9999   ]\n    ...\n    [856.        856.        856.        ... 856.        856.\n     856.       ]\n    [573.99994   573.99994   573.99994   ... 573.99994   573.99994\n     573.99994  ]\n    [289.        289.        289.        ... 289.        289.\n     289.       ]]\n\n   [[186.99997   186.99997   186.99997   ... 186.99997   186.99997\n     186.99997  ]\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\n     374.99997  ]\n    [565.        565.        565.        ... 565.        565.\n     565.       ]\n    ...\n    [595.        595.        595.        ... 595.        595.\n     595.       ]\n    [398.99994   398.99994   398.99994   ... 398.99994   398.99994\n     398.99994  ]\n    [200.99997   200.99997   200.99997   ... 200.99997   200.99997\n     200.99997  ]]\n\n   [[ 98.00005    98.00005    98.00005   ...  98.00005    98.00005\n      98.00005  ]\n    [195.99998   195.99998   195.99998   ... 195.99998   195.99998\n     195.99998  ]\n    [294.99994   294.99994   294.99994   ... 294.99994   294.99994\n     294.99994  ]\n    ...\n    [309.99997   309.99997   309.99997   ... 309.99997   309.99997\n     309.99997  ]\n    [207.99997   207.99997   207.99997   ... 207.99997   207.99997\n     207.99997  ]\n    [104.99999   104.99999   104.99999   ... 104.99999   104.99999\n     104.99999  ]]]]]\n</code></pre>\n<p>I also tried to run the command inside a <code>tf.device('/cpu:0')</code> but I still got the same behavior. Same thing if I try with tf 1.8 or 1.7 or using python 2.7 or 3.5.<br>\nI also tried using <code>tf.layers.conv2d_transpose</code> and same thing happens</p>\n<p>From what I have read so far, these differences are <em>kind of expected</em> but as soon as you apply other methods on such small differences, systems are diverging (my case) instead of converging, and that's annoying to have GPUs but can not be able to use them if I want to converge.</p>\n<p>Although the difference here is quite small, I invite you to repeat exactly the same code without the <code>tf.nn.bias_add</code> and the difference between results is significant.</p>", "body_text": "Hi guys, I have exactly the same issue when running the same code on CPU and on GPU ... my guess is that computation on some operations are numerically instable on GPU (but I am not a pro in numerical computation ....).\nHere is first my setup :\nI am on Ubuntu 16.04.4 LTS with Tensorflow built from source (I have both 1.7 and 1.8). I have both python version (2.7 and 3.5), bazel version is 0.14, the GCC compiler is 5.4, CUDA is 9.2 and cudnn 7.1.4. I have 2 GPUs, both are NVIDIA GTX 1080Ti.\n@zheng-xq, @angersson as requested, I created a snippet that easily reproduces my issue.\nON CPU\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\nimport numpy as np\nimport tensorflow as tf\ndata = np.array(\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\n     range(1, 33)]).astype(np.float32)\n\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\n\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\n\nweights = tf.ones([8, 8, 16, 1])\nbiases = tf.ones([16])\n\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\n\nsess = tf.Session()\n\nwith sess.as_default():\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\n\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\nprint(transposed)\n\nJust for some clarity issue, I print the end of what the snippet outputs each time:\n[[[  2.          2.          2.        ...   2.          2.\n       2.       ]\n    [  4.          4.          4.        ...   4.          4.\n       4.       ]\n    [  7.          7.          7.        ...   7.          7.\n       7.       ]\n    ...\n    [ 22.         22.         22.        ...  22.         22.\n      22.       ]\n    [ 16.         16.         16.        ...  16.         16.\n      16.       ]\n    [  9.          9.          9.        ...   9.          9.\n       9.       ]]\n\n   [[ 11.         11.         11.        ...  11.         11.\n      11.       ]\n    [ 23.         23.         23.        ...  23.         23.\n      23.       ]\n    [ 37.         37.         37.        ...  37.         37.\n      37.       ]\n    ...\n    [ 67.         67.         67.        ...  67.         67.\n      67.       ]\n    [ 47.         47.         47.        ...  47.         47.\n      47.       ]\n    [ 25.         25.         25.        ...  25.         25.\n      25.       ]]\n\n   [[ 28.         28.         28.        ...  28.         28.\n      28.       ]\n    [ 58.         58.         58.        ...  58.         58.\n      58.       ]\n    [ 91.         91.         91.        ...  91.         91.\n      91.       ]\n    ...\n    [136.        136.        136.        ... 136.        136.\n     136.       ]\n    [ 94.         94.         94.        ...  94.         94.\n      94.       ]\n    [ 49.         49.         49.        ...  49.         49.\n      49.       ]]\n\n   ...\n\n   [[268.        268.        268.        ... 268.        268.\n     268.       ]\n    [538.        538.        538.        ... 538.        538.\n     538.       ]\n    [811.        811.        811.        ... 811.        811.\n     811.       ]\n    ...\n    [856.        856.        856.        ... 856.        856.\n     856.       ]\n    [574.        574.        574.        ... 574.        574.\n     574.       ]\n    [289.        289.        289.        ... 289.        289.\n     289.       ]]\n\n   [[187.        187.        187.        ... 187.        187.\n     187.       ]\n    [375.        375.        375.        ... 375.        375.\n     375.       ]\n    [565.        565.        565.        ... 565.        565.\n     565.       ]\n    ...\n    [595.        595.        595.        ... 595.        595.\n     595.       ]\n    [399.        399.        399.        ... 399.        399.\n     399.       ]\n    [201.        201.        201.        ... 201.        201.\n     201.       ]]\n\n   [[ 98.         98.         98.        ...  98.         98.\n      98.       ]\n    [196.        196.        196.        ... 196.        196.\n     196.       ]\n    [295.        295.        295.        ... 295.        295.\n     295.       ]\n    ...\n    [310.        310.        310.        ... 310.        310.\n     310.       ]\n    [208.        208.        208.        ... 208.        208.\n     208.       ]\n    [105.        105.        105.        ... 105.        105.\n     105.       ]]]]]\n\nON GPU\nimport numpy as np\nimport tensorflow as tf\ndata = np.array(\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\n     range(1, 33)]).astype(np.float32)\n\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\n\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\n\nweights = tf.ones([8, 8, 16, 1])\nbiases = tf.ones([16])\n\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\n\nsess = tf.Session()\n\nwith sess.as_default():\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\n\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\nprint(transposed)\n\nIt is exactly the same code (except the cuda visible command), and sometimes I have exactly the same aforementioned output, but I also have these outputs:\n[[[  1.9999084   1.9999084   1.9999084 ...   1.9999084   1.9999084\n       1.9999084]\n    [  4.          4.          4.        ...   4.          4.\n       4.       ]\n    [  7.          7.          7.        ...   7.          7.\n       7.       ]\n    ...\n    [ 22.         22.         22.        ...  22.         22.\n      22.       ]\n    [ 15.9999695  15.9999695  15.9999695 ...  15.9999695  15.9999695\n      15.9999695]\n    [  8.999954    8.999954    8.999954  ...   8.999954    8.999954\n       8.999954 ]]\n\n   [[ 10.999954   10.999954   10.999954  ...  10.999954   10.999954\n      10.999954 ]\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\n      22.99997  ]\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\n      37.00006  ]\n    ...\n    [ 67.00003    67.00003    67.00003   ...  67.00003    67.00003\n      67.00003  ]\n    [ 46.99997    46.99997    46.99997   ...  46.99997    46.99997\n      46.99997  ]\n    [ 25.         25.         25.        ...  25.         25.\n      25.       ]]\n\n   [[ 27.999954   27.999954   27.999954  ...  27.999954   27.999954\n      27.999954 ]\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\n      57.99997  ]\n    [ 90.999985   90.999985   90.999985  ...  90.999985   90.999985\n      90.999985 ]\n    ...\n    [136.        136.        136.        ... 136.        136.\n     136.       ]\n    [ 93.999985   93.999985   93.999985  ...  93.999985   93.999985\n      93.999985 ]\n    [ 48.999954   48.999954   48.999954  ...  48.999954   48.999954\n      48.999954 ]]\n\n   ...\n\n   [[268.        268.        268.        ... 268.        268.\n     268.       ]\n    [538.        538.        538.        ... 538.        538.\n     538.       ]\n    [810.99994   810.99994   810.99994   ... 810.99994   810.99994\n     810.99994  ]\n    ...\n    [856.        856.        856.        ... 856.        856.\n     856.       ]\n    [574.        574.        574.        ... 574.        574.\n     574.       ]\n    [289.0001    289.0001    289.0001    ... 289.0001    289.0001\n     289.0001   ]]\n\n   [[186.99992   186.99992   186.99992   ... 186.99992   186.99992\n     186.99992  ]\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\n     374.99997  ]\n    [565.        565.        565.        ... 565.        565.\n     565.       ]\n    ...\n    [595.        595.        595.        ... 595.        595.\n     595.       ]\n    [399.00006   399.00006   399.00006   ... 399.00006   399.00006\n     399.00006  ]\n    [201.00008   201.00008   201.00008   ... 201.00008   201.00008\n     201.00008  ]]\n\n   [[ 97.99998    97.99998    97.99998   ...  97.99998    97.99998\n      97.99998  ]\n    [195.99997   195.99997   195.99997   ... 195.99997   195.99997\n     195.99997  ]\n    [295.        295.        295.        ... 295.        295.\n     295.       ]\n    ...\n    [310.00003   310.00003   310.00003   ... 310.00003   310.00003\n     310.00003  ]\n    [208.00012   208.00012   208.00012   ... 208.00012   208.00012\n     208.00012  ]\n    [105.00009   105.00009   105.00009   ... 105.00009   105.00009\n     105.00009  ]]]]]\n\nor\n  [[[  1.9999237   1.9999237   1.9999237 ...   1.9999237   1.9999237\n       1.9999237]\n    [  4.          4.          4.        ...   4.          4.\n       4.       ]\n    [  7.          7.          7.        ...   7.          7.\n       7.       ]\n    ...\n    [ 22.         22.         22.        ...  22.         22.\n      22.       ]\n    [ 16.         16.         16.        ...  16.         16.\n      16.       ]\n    [  9.000046    9.000046    9.000046  ...   9.000046    9.000046\n       9.000046 ]]\n\n   [[ 11.000015   11.000015   11.000015  ...  11.000015   11.000015\n      11.000015 ]\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\n      22.99997  ]\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\n      37.00006  ]\n    ...\n    [ 67.         67.         67.        ...  67.         67.\n      67.       ]\n    [ 47.00006    47.00006    47.00006   ...  47.00006    47.00006\n      47.00006  ]\n    [ 25.000076   25.000076   25.000076  ...  25.000076   25.000076\n      25.000076 ]]\n\n   [[ 27.999962   27.999962   27.999962  ...  27.999962   27.999962\n      27.999962 ]\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\n      57.99997  ]\n    [ 91.00003    91.00003    91.00003   ...  91.00003    91.00003\n      91.00003  ]\n    ...\n    [136.00002   136.00002   136.00002   ... 136.00002   136.00002\n     136.00002  ]\n    [ 94.000046   94.000046   94.000046  ...  94.000046   94.000046\n      94.000046 ]\n    [ 49.00006    49.00006    49.00006   ...  49.00006    49.00006\n      49.00006  ]]\n\n   ...\n\n   [[268.00006   268.00006   268.00006   ... 268.00006   268.00006\n     268.00006  ]\n    [538.        538.        538.        ... 538.        538.\n     538.       ]\n    [810.9999    810.9999    810.9999    ... 810.9999    810.9999\n     810.9999   ]\n    ...\n    [856.        856.        856.        ... 856.        856.\n     856.       ]\n    [573.99994   573.99994   573.99994   ... 573.99994   573.99994\n     573.99994  ]\n    [289.        289.        289.        ... 289.        289.\n     289.       ]]\n\n   [[186.99997   186.99997   186.99997   ... 186.99997   186.99997\n     186.99997  ]\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\n     374.99997  ]\n    [565.        565.        565.        ... 565.        565.\n     565.       ]\n    ...\n    [595.        595.        595.        ... 595.        595.\n     595.       ]\n    [398.99994   398.99994   398.99994   ... 398.99994   398.99994\n     398.99994  ]\n    [200.99997   200.99997   200.99997   ... 200.99997   200.99997\n     200.99997  ]]\n\n   [[ 98.00005    98.00005    98.00005   ...  98.00005    98.00005\n      98.00005  ]\n    [195.99998   195.99998   195.99998   ... 195.99998   195.99998\n     195.99998  ]\n    [294.99994   294.99994   294.99994   ... 294.99994   294.99994\n     294.99994  ]\n    ...\n    [309.99997   309.99997   309.99997   ... 309.99997   309.99997\n     309.99997  ]\n    [207.99997   207.99997   207.99997   ... 207.99997   207.99997\n     207.99997  ]\n    [104.99999   104.99999   104.99999   ... 104.99999   104.99999\n     104.99999  ]]]]]\n\nI also tried to run the command inside a tf.device('/cpu:0') but I still got the same behavior. Same thing if I try with tf 1.8 or 1.7 or using python 2.7 or 3.5.\nI also tried using tf.layers.conv2d_transpose and same thing happens\nFrom what I have read so far, these differences are kind of expected but as soon as you apply other methods on such small differences, systems are diverging (my case) instead of converging, and that's annoying to have GPUs but can not be able to use them if I want to converge.\nAlthough the difference here is quite small, I invite you to repeat exactly the same code without the tf.nn.bias_add and the difference between results is significant.", "body": "Hi guys, I have exactly the same issue when running the same code on CPU and on GPU ... my guess is that computation on some operations are numerically instable on GPU (but I am not a pro in numerical computation ....). \r\nHere is first my setup :    \r\n**I am on Ubuntu 16.04.4 LTS with Tensorflow built from source (I have both 1.7 and 1.8). I have both python version (2.7 and 3.5), bazel version is 0.14, the GCC compiler is 5.4, CUDA is 9.2 and cudnn 7.1.4. I have 2 GPUs, both are NVIDIA GTX 1080Ti.**\r\n@zheng-xq, @angersson as requested, I created a snippet that easily reproduces my issue.\r\n\r\n_**ON CPU**_\r\n\r\n```\r\nimport os\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\"\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndata = np.array(\r\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\r\n     range(1, 33)]).astype(np.float32)\r\n\r\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\r\n\r\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\r\n\r\nweights = tf.ones([8, 8, 16, 1])\r\nbiases = tf.ones([16])\r\n\r\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\r\n\r\nsess = tf.Session()\r\n\r\nwith sess.as_default():\r\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\r\n\r\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\r\nprint(transposed)\r\n```\r\nJust for some clarity issue, I print the end of what the snippet outputs **each time**: \r\n```\r\n[[[  2.          2.          2.        ...   2.          2.\r\n       2.       ]\r\n    [  4.          4.          4.        ...   4.          4.\r\n       4.       ]\r\n    [  7.          7.          7.        ...   7.          7.\r\n       7.       ]\r\n    ...\r\n    [ 22.         22.         22.        ...  22.         22.\r\n      22.       ]\r\n    [ 16.         16.         16.        ...  16.         16.\r\n      16.       ]\r\n    [  9.          9.          9.        ...   9.          9.\r\n       9.       ]]\r\n\r\n   [[ 11.         11.         11.        ...  11.         11.\r\n      11.       ]\r\n    [ 23.         23.         23.        ...  23.         23.\r\n      23.       ]\r\n    [ 37.         37.         37.        ...  37.         37.\r\n      37.       ]\r\n    ...\r\n    [ 67.         67.         67.        ...  67.         67.\r\n      67.       ]\r\n    [ 47.         47.         47.        ...  47.         47.\r\n      47.       ]\r\n    [ 25.         25.         25.        ...  25.         25.\r\n      25.       ]]\r\n\r\n   [[ 28.         28.         28.        ...  28.         28.\r\n      28.       ]\r\n    [ 58.         58.         58.        ...  58.         58.\r\n      58.       ]\r\n    [ 91.         91.         91.        ...  91.         91.\r\n      91.       ]\r\n    ...\r\n    [136.        136.        136.        ... 136.        136.\r\n     136.       ]\r\n    [ 94.         94.         94.        ...  94.         94.\r\n      94.       ]\r\n    [ 49.         49.         49.        ...  49.         49.\r\n      49.       ]]\r\n\r\n   ...\r\n\r\n   [[268.        268.        268.        ... 268.        268.\r\n     268.       ]\r\n    [538.        538.        538.        ... 538.        538.\r\n     538.       ]\r\n    [811.        811.        811.        ... 811.        811.\r\n     811.       ]\r\n    ...\r\n    [856.        856.        856.        ... 856.        856.\r\n     856.       ]\r\n    [574.        574.        574.        ... 574.        574.\r\n     574.       ]\r\n    [289.        289.        289.        ... 289.        289.\r\n     289.       ]]\r\n\r\n   [[187.        187.        187.        ... 187.        187.\r\n     187.       ]\r\n    [375.        375.        375.        ... 375.        375.\r\n     375.       ]\r\n    [565.        565.        565.        ... 565.        565.\r\n     565.       ]\r\n    ...\r\n    [595.        595.        595.        ... 595.        595.\r\n     595.       ]\r\n    [399.        399.        399.        ... 399.        399.\r\n     399.       ]\r\n    [201.        201.        201.        ... 201.        201.\r\n     201.       ]]\r\n\r\n   [[ 98.         98.         98.        ...  98.         98.\r\n      98.       ]\r\n    [196.        196.        196.        ... 196.        196.\r\n     196.       ]\r\n    [295.        295.        295.        ... 295.        295.\r\n     295.       ]\r\n    ...\r\n    [310.        310.        310.        ... 310.        310.\r\n     310.       ]\r\n    [208.        208.        208.        ... 208.        208.\r\n     208.       ]\r\n    [105.        105.        105.        ... 105.        105.\r\n     105.       ]]]]]\r\n```\r\n\r\n\r\n**_ON GPU_**\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\ndata = np.array(\r\n    [np.array([np.arange(1, 13 * 8 + 1).reshape(13, 8, 1) * i / 12.0 for i in range(1, 13)]) * j / 32.0 for j in\r\n     range(1, 33)]).astype(np.float32)\r\n\r\ninput_placeholder = tf.placeholder(\"float32\", shape=(None, 12, 13, 8, 1))\r\n\r\nres = tf.transpose(input_placeholder, [1, 0, 2, 3, 4])\r\n\r\nweights = tf.ones([8, 8, 16, 1])\r\nbiases = tf.ones([16])\r\n\r\ndeconv = tf.map_fn(lambda x: tf.nn.bias_add(tf.nn.conv2d_transpose(x, weights, strides=[1, 1, 1, 1], padding='VALID', output_shape=[32, 20, 15, 16]), biases), res)\r\n\r\nsess = tf.Session()\r\n\r\nwith sess.as_default():\r\n   output = sess.run(deconv, feed_dict={input_placeholder: data})\r\n\r\ntransposed = np.transpose(fin, [1, 0, 2, 3, 4])\r\nprint(transposed)\r\n```\r\nIt is exactly the same code (except the cuda visible command), and **sometimes** I have exactly the same aforementioned output, **but I also have these** outputs:\r\n\r\n```\r\n[[[  1.9999084   1.9999084   1.9999084 ...   1.9999084   1.9999084\r\n       1.9999084]\r\n    [  4.          4.          4.        ...   4.          4.\r\n       4.       ]\r\n    [  7.          7.          7.        ...   7.          7.\r\n       7.       ]\r\n    ...\r\n    [ 22.         22.         22.        ...  22.         22.\r\n      22.       ]\r\n    [ 15.9999695  15.9999695  15.9999695 ...  15.9999695  15.9999695\r\n      15.9999695]\r\n    [  8.999954    8.999954    8.999954  ...   8.999954    8.999954\r\n       8.999954 ]]\r\n\r\n   [[ 10.999954   10.999954   10.999954  ...  10.999954   10.999954\r\n      10.999954 ]\r\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\r\n      22.99997  ]\r\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\r\n      37.00006  ]\r\n    ...\r\n    [ 67.00003    67.00003    67.00003   ...  67.00003    67.00003\r\n      67.00003  ]\r\n    [ 46.99997    46.99997    46.99997   ...  46.99997    46.99997\r\n      46.99997  ]\r\n    [ 25.         25.         25.        ...  25.         25.\r\n      25.       ]]\r\n\r\n   [[ 27.999954   27.999954   27.999954  ...  27.999954   27.999954\r\n      27.999954 ]\r\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\r\n      57.99997  ]\r\n    [ 90.999985   90.999985   90.999985  ...  90.999985   90.999985\r\n      90.999985 ]\r\n    ...\r\n    [136.        136.        136.        ... 136.        136.\r\n     136.       ]\r\n    [ 93.999985   93.999985   93.999985  ...  93.999985   93.999985\r\n      93.999985 ]\r\n    [ 48.999954   48.999954   48.999954  ...  48.999954   48.999954\r\n      48.999954 ]]\r\n\r\n   ...\r\n\r\n   [[268.        268.        268.        ... 268.        268.\r\n     268.       ]\r\n    [538.        538.        538.        ... 538.        538.\r\n     538.       ]\r\n    [810.99994   810.99994   810.99994   ... 810.99994   810.99994\r\n     810.99994  ]\r\n    ...\r\n    [856.        856.        856.        ... 856.        856.\r\n     856.       ]\r\n    [574.        574.        574.        ... 574.        574.\r\n     574.       ]\r\n    [289.0001    289.0001    289.0001    ... 289.0001    289.0001\r\n     289.0001   ]]\r\n\r\n   [[186.99992   186.99992   186.99992   ... 186.99992   186.99992\r\n     186.99992  ]\r\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\r\n     374.99997  ]\r\n    [565.        565.        565.        ... 565.        565.\r\n     565.       ]\r\n    ...\r\n    [595.        595.        595.        ... 595.        595.\r\n     595.       ]\r\n    [399.00006   399.00006   399.00006   ... 399.00006   399.00006\r\n     399.00006  ]\r\n    [201.00008   201.00008   201.00008   ... 201.00008   201.00008\r\n     201.00008  ]]\r\n\r\n   [[ 97.99998    97.99998    97.99998   ...  97.99998    97.99998\r\n      97.99998  ]\r\n    [195.99997   195.99997   195.99997   ... 195.99997   195.99997\r\n     195.99997  ]\r\n    [295.        295.        295.        ... 295.        295.\r\n     295.       ]\r\n    ...\r\n    [310.00003   310.00003   310.00003   ... 310.00003   310.00003\r\n     310.00003  ]\r\n    [208.00012   208.00012   208.00012   ... 208.00012   208.00012\r\n     208.00012  ]\r\n    [105.00009   105.00009   105.00009   ... 105.00009   105.00009\r\n     105.00009  ]]]]]\r\n```\r\nor \r\n```\r\n  [[[  1.9999237   1.9999237   1.9999237 ...   1.9999237   1.9999237\r\n       1.9999237]\r\n    [  4.          4.          4.        ...   4.          4.\r\n       4.       ]\r\n    [  7.          7.          7.        ...   7.          7.\r\n       7.       ]\r\n    ...\r\n    [ 22.         22.         22.        ...  22.         22.\r\n      22.       ]\r\n    [ 16.         16.         16.        ...  16.         16.\r\n      16.       ]\r\n    [  9.000046    9.000046    9.000046  ...   9.000046    9.000046\r\n       9.000046 ]]\r\n\r\n   [[ 11.000015   11.000015   11.000015  ...  11.000015   11.000015\r\n      11.000015 ]\r\n    [ 22.99997    22.99997    22.99997   ...  22.99997    22.99997\r\n      22.99997  ]\r\n    [ 37.00006    37.00006    37.00006   ...  37.00006    37.00006\r\n      37.00006  ]\r\n    ...\r\n    [ 67.         67.         67.        ...  67.         67.\r\n      67.       ]\r\n    [ 47.00006    47.00006    47.00006   ...  47.00006    47.00006\r\n      47.00006  ]\r\n    [ 25.000076   25.000076   25.000076  ...  25.000076   25.000076\r\n      25.000076 ]]\r\n\r\n   [[ 27.999962   27.999962   27.999962  ...  27.999962   27.999962\r\n      27.999962 ]\r\n    [ 57.99997    57.99997    57.99997   ...  57.99997    57.99997\r\n      57.99997  ]\r\n    [ 91.00003    91.00003    91.00003   ...  91.00003    91.00003\r\n      91.00003  ]\r\n    ...\r\n    [136.00002   136.00002   136.00002   ... 136.00002   136.00002\r\n     136.00002  ]\r\n    [ 94.000046   94.000046   94.000046  ...  94.000046   94.000046\r\n      94.000046 ]\r\n    [ 49.00006    49.00006    49.00006   ...  49.00006    49.00006\r\n      49.00006  ]]\r\n\r\n   ...\r\n\r\n   [[268.00006   268.00006   268.00006   ... 268.00006   268.00006\r\n     268.00006  ]\r\n    [538.        538.        538.        ... 538.        538.\r\n     538.       ]\r\n    [810.9999    810.9999    810.9999    ... 810.9999    810.9999\r\n     810.9999   ]\r\n    ...\r\n    [856.        856.        856.        ... 856.        856.\r\n     856.       ]\r\n    [573.99994   573.99994   573.99994   ... 573.99994   573.99994\r\n     573.99994  ]\r\n    [289.        289.        289.        ... 289.        289.\r\n     289.       ]]\r\n\r\n   [[186.99997   186.99997   186.99997   ... 186.99997   186.99997\r\n     186.99997  ]\r\n    [374.99997   374.99997   374.99997   ... 374.99997   374.99997\r\n     374.99997  ]\r\n    [565.        565.        565.        ... 565.        565.\r\n     565.       ]\r\n    ...\r\n    [595.        595.        595.        ... 595.        595.\r\n     595.       ]\r\n    [398.99994   398.99994   398.99994   ... 398.99994   398.99994\r\n     398.99994  ]\r\n    [200.99997   200.99997   200.99997   ... 200.99997   200.99997\r\n     200.99997  ]]\r\n\r\n   [[ 98.00005    98.00005    98.00005   ...  98.00005    98.00005\r\n      98.00005  ]\r\n    [195.99998   195.99998   195.99998   ... 195.99998   195.99998\r\n     195.99998  ]\r\n    [294.99994   294.99994   294.99994   ... 294.99994   294.99994\r\n     294.99994  ]\r\n    ...\r\n    [309.99997   309.99997   309.99997   ... 309.99997   309.99997\r\n     309.99997  ]\r\n    [207.99997   207.99997   207.99997   ... 207.99997   207.99997\r\n     207.99997  ]\r\n    [104.99999   104.99999   104.99999   ... 104.99999   104.99999\r\n     104.99999  ]]]]]\r\n```\r\n\r\nI also tried to run the command inside a ```tf.device('/cpu:0')``` but I still got the same behavior. Same thing if I try with tf 1.8 or 1.7 or using python 2.7 or 3.5.\r\nI also tried using ```tf.layers.conv2d_transpose``` and same thing happens\r\n\r\nFrom what I have read so far, these differences are *kind of expected* but as soon as you apply other methods on such small differences, systems are diverging (my case) instead of converging, and that's annoying to have GPUs but can not be able to use them if I want to converge.\r\n\r\nAlthough the difference here is quite small, I invite you to repeat exactly the same code without the ```tf.nn.bias_add``` and the difference between results is significant."}