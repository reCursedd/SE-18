{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2458", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2458/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2458/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2458/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2458", "id": 156128956, "node_id": "MDU6SXNzdWUxNTYxMjg5NTY=", "number": 2458, "title": "Graph \"Training/Test\" flag & global_flags", "user": {"login": "Mazecreator", "id": 18412448, "node_id": "MDQ6VXNlcjE4NDEyNDQ4", "avatar_url": "https://avatars2.githubusercontent.com/u/18412448?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Mazecreator", "html_url": "https://github.com/Mazecreator", "followers_url": "https://api.github.com/users/Mazecreator/followers", "following_url": "https://api.github.com/users/Mazecreator/following{/other_user}", "gists_url": "https://api.github.com/users/Mazecreator/gists{/gist_id}", "starred_url": "https://api.github.com/users/Mazecreator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Mazecreator/subscriptions", "organizations_url": "https://api.github.com/users/Mazecreator/orgs", "repos_url": "https://api.github.com/users/Mazecreator/repos", "events_url": "https://api.github.com/users/Mazecreator/events{/privacy}", "received_events_url": "https://api.github.com/users/Mazecreator/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-05-22T00:53:49Z", "updated_at": "2017-02-09T22:37:17Z", "closed_at": "2016-06-08T16:15:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am not sure if this has been proposed yet, but I would like the ability in the .RUN() op to flag \"test\" rather than \"train\".  The motivation is around \"Dropout\".  When I am training a batch I would like the dropout in my graph, but when I am running a \"test\" I would like the Dropout to simply pass the tensor (0% dropout if you will).  I am sure I can pass the \"dropout\" as a param through the feeder but they need to be set and reset each time and could get cumbersome for larger and complex graphs.  It would be nice to have an internal feature in \"Dropout\" and other Training specific features such that they will adjust their behavior if \"Not-Training\".</p>\n<p>Right now I have a graph that has 2 paths, one for training with the Dropouts Ops in-line and one that does note and I call the proper 'Output' based upon my Test/Training desire.  This complicated the graph.</p>\n<p>Maybe this function can be extended to pass \"Flags\" into the graph so other features can be added that are based upon run-time demand.  This \"flag_dict\" could be global to the entire graph so Ops' could check their status upon a run.</p>\n<p><code>sess.run([cost,optimizer,merged],  feed_dict={X: Xbatch, Y: YDesired}, flag_dict={Training: True})</code></p>\n<p>I can think of another request where this would be useful.  As I search solution space, I would like to setup a Genetic Algorithm to assist.  One of the things I would like to change in each graph iteration might be the \"activation\", \"Cost\" and/or \"optimizer\" functions.  So a generic Activation Op or Optimizer op could be written (are maybe a simple \"Case\" Op based upon the flag)?</p>\n<p>eg. <code>flag_dict={Training: True, Activation: Relu, Optimizer: AdagradOptimizer}</code></p>\n<p>Not sure how to deal with the parameters if they differ from Op to Op.  But maybe \"custom global flags\" could be passed as well.</p>\n<p>eg.  <code>layer2out    = tf.nn.genericActivation(layer1out,ActivationFlag=Activation)   #Activation=Relu from above</code></p>\n<p>Just a thought to help explore during training/testing.</p>", "body_text": "I am not sure if this has been proposed yet, but I would like the ability in the .RUN() op to flag \"test\" rather than \"train\".  The motivation is around \"Dropout\".  When I am training a batch I would like the dropout in my graph, but when I am running a \"test\" I would like the Dropout to simply pass the tensor (0% dropout if you will).  I am sure I can pass the \"dropout\" as a param through the feeder but they need to be set and reset each time and could get cumbersome for larger and complex graphs.  It would be nice to have an internal feature in \"Dropout\" and other Training specific features such that they will adjust their behavior if \"Not-Training\".\nRight now I have a graph that has 2 paths, one for training with the Dropouts Ops in-line and one that does note and I call the proper 'Output' based upon my Test/Training desire.  This complicated the graph.\nMaybe this function can be extended to pass \"Flags\" into the graph so other features can be added that are based upon run-time demand.  This \"flag_dict\" could be global to the entire graph so Ops' could check their status upon a run.\nsess.run([cost,optimizer,merged],  feed_dict={X: Xbatch, Y: YDesired}, flag_dict={Training: True})\nI can think of another request where this would be useful.  As I search solution space, I would like to setup a Genetic Algorithm to assist.  One of the things I would like to change in each graph iteration might be the \"activation\", \"Cost\" and/or \"optimizer\" functions.  So a generic Activation Op or Optimizer op could be written (are maybe a simple \"Case\" Op based upon the flag)?\neg. flag_dict={Training: True, Activation: Relu, Optimizer: AdagradOptimizer}\nNot sure how to deal with the parameters if they differ from Op to Op.  But maybe \"custom global flags\" could be passed as well.\neg.  layer2out    = tf.nn.genericActivation(layer1out,ActivationFlag=Activation)   #Activation=Relu from above\nJust a thought to help explore during training/testing.", "body": "I am not sure if this has been proposed yet, but I would like the ability in the .RUN() op to flag \"test\" rather than \"train\".  The motivation is around \"Dropout\".  When I am training a batch I would like the dropout in my graph, but when I am running a \"test\" I would like the Dropout to simply pass the tensor (0% dropout if you will).  I am sure I can pass the \"dropout\" as a param through the feeder but they need to be set and reset each time and could get cumbersome for larger and complex graphs.  It would be nice to have an internal feature in \"Dropout\" and other Training specific features such that they will adjust their behavior if \"Not-Training\".\n\nRight now I have a graph that has 2 paths, one for training with the Dropouts Ops in-line and one that does note and I call the proper 'Output' based upon my Test/Training desire.  This complicated the graph.\n\nMaybe this function can be extended to pass \"Flags\" into the graph so other features can be added that are based upon run-time demand.  This \"flag_dict\" could be global to the entire graph so Ops' could check their status upon a run.\n\n`sess.run([cost,optimizer,merged], \n                                                                 feed_dict={X: Xbatch,\n                                                                 Y: YDesired}, flag_dict={Training: True})`\n\nI can think of another request where this would be useful.  As I search solution space, I would like to setup a Genetic Algorithm to assist.  One of the things I would like to change in each graph iteration might be the \"activation\", \"Cost\" and/or \"optimizer\" functions.  So a generic Activation Op or Optimizer op could be written (are maybe a simple \"Case\" Op based upon the flag)?\n\neg. `flag_dict={Training: True, Activation: Relu, Optimizer: AdagradOptimizer}`\n\nNot sure how to deal with the parameters if they differ from Op to Op.  But maybe \"custom global flags\" could be passed as well. \n\neg.  `layer2out    = tf.nn.genericActivation(layer1out,ActivationFlag=Activation)   #Activation=Relu from above`\n\nJust a thought to help explore during training/testing.\n"}