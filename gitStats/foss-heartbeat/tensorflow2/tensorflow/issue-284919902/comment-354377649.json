{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354377649", "html_url": "https://github.com/tensorflow/tensorflow/issues/15694#issuecomment-354377649", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15694", "id": 354377649, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDM3NzY0OQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-29T00:06:23Z", "updated_at": "2017-12-29T00:06:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The short answer here is that building a single minibatch of 100 elements is too small an operation to benefit from TensorFlow as it's currently implemented. (You'll also find that multiplying two small matrices is faster in NumPy than loading it into TensorFlow.)</p>\n<p>The <code>Dataset</code> implementation is designed to perform non-trivial amount of preprocessing (decoding images, parsing text, etc.); and the <code>Session</code> implementation is geared towards computations that take at least a few milliseconds to perform. In the intended use case, you would pass the output of <code>iterator.get_next()</code> to the input of a neural network, for inference or training.</p>\n<p>Each of the TensorFlow operations in your example takes approximately 140 microseconds. Calling into TensorFlow from Python currently has an overhead of around 60 microseconds, a trivial step takes around 20 microseconds, and invoking an operation such as <code>Iterator.get_next()</code> incurs a few more microseconds of overhead, plus a context switch.</p>\n<p>There are a couple of things you could try:</p>\n<ul>\n<li>\n<p>Try using a <code>tfe.Iterator</code> in Eager mode, which acts more like a Python iterator, and takes approximately 100us per batch in my tests.</p>\n</li>\n<li>\n<p>Creating a callable for the step, using <code>get_next_chunk = sess.make_callable(next_chunk)</code> and then using <code>get_next_chunk()</code> in place of <code>sess.run(next_chunk)</code> shaves about 65us off each invocation (to approximately 92us per batch) in my tests.</p>\n</li>\n<li>\n<p>Rewrite the pipeline to cache the processed data for an epoch:</p>\n<div class=\"highlight highlight-source-python\"><pre>dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(raw_data)\ndataset <span class=\"pl-k\">=</span> dataset.batch(<span class=\"pl-c1\">MINI_BATCH</span>)\ndataset <span class=\"pl-k\">=</span> dataset.cache()\ndataset <span class=\"pl-k\">=</span> dataset.repeat(<span class=\"pl-c1\">EPOCHS</span>)\niterator <span class=\"pl-k\">=</span> dataset.make_one_shot_iterator()</pre></div>\n<p>This change reduces the cost of each mini-batch to about 42us in my tests (and ~52us in Eager mode).</p>\n</li>\n</ul>\n<p>Where does the rest of the time go? In the non-caching versions, the data are currently copied twice, out of the original array in <code>Dataset.from_tensor_slices()</code>, then into the batch in <code>Dataset.batch()</code>. We could potentially use <code>Tensor::Slice()</code> to avoid a deep copy in some cases, although that only works when the slices are aligned to 32-byte boundaries, which might not always be the case. (It isn't for a vector of shape <code>(10000,)</code> <code>int64</code> elements but it is if e.g. you reshape it to <code>(100, 100)</code>.) Alternatively, and at the cost of some memory inflation, <code>Dataset.from_tensor_slices()</code> could eagerly slice up its input ahead of time, where the up-front initialized cost would be amortized over multiple epochs... this is what the explicit <code>Dataset.cache()</code> does for you. There are more context switches than are strictly necessary, especially when running in Eager mode, where the work can happen on a donated Python thread. Finally, profiling suggests that there are some non-trivial overheads when dealing with tensors of a single element (shape calculations, in particular, weigh heavily in the profile when the cost of the actual copy is small).</p>\n<p>There are definitely some overheads worth paring down here, although they are likely to have only a small effect on headline performance numbers, so fixing them won't be top priority. I'll add some benchmarks to track the various versions, and invite contributions that improve things for all workloads.</p>", "body_text": "The short answer here is that building a single minibatch of 100 elements is too small an operation to benefit from TensorFlow as it's currently implemented. (You'll also find that multiplying two small matrices is faster in NumPy than loading it into TensorFlow.)\nThe Dataset implementation is designed to perform non-trivial amount of preprocessing (decoding images, parsing text, etc.); and the Session implementation is geared towards computations that take at least a few milliseconds to perform. In the intended use case, you would pass the output of iterator.get_next() to the input of a neural network, for inference or training.\nEach of the TensorFlow operations in your example takes approximately 140 microseconds. Calling into TensorFlow from Python currently has an overhead of around 60 microseconds, a trivial step takes around 20 microseconds, and invoking an operation such as Iterator.get_next() incurs a few more microseconds of overhead, plus a context switch.\nThere are a couple of things you could try:\n\n\nTry using a tfe.Iterator in Eager mode, which acts more like a Python iterator, and takes approximately 100us per batch in my tests.\n\n\nCreating a callable for the step, using get_next_chunk = sess.make_callable(next_chunk) and then using get_next_chunk() in place of sess.run(next_chunk) shaves about 65us off each invocation (to approximately 92us per batch) in my tests.\n\n\nRewrite the pipeline to cache the processed data for an epoch:\ndataset = tf.data.Dataset.from_tensor_slices(raw_data)\ndataset = dataset.batch(MINI_BATCH)\ndataset = dataset.cache()\ndataset = dataset.repeat(EPOCHS)\niterator = dataset.make_one_shot_iterator()\nThis change reduces the cost of each mini-batch to about 42us in my tests (and ~52us in Eager mode).\n\n\nWhere does the rest of the time go? In the non-caching versions, the data are currently copied twice, out of the original array in Dataset.from_tensor_slices(), then into the batch in Dataset.batch(). We could potentially use Tensor::Slice() to avoid a deep copy in some cases, although that only works when the slices are aligned to 32-byte boundaries, which might not always be the case. (It isn't for a vector of shape (10000,) int64 elements but it is if e.g. you reshape it to (100, 100).) Alternatively, and at the cost of some memory inflation, Dataset.from_tensor_slices() could eagerly slice up its input ahead of time, where the up-front initialized cost would be amortized over multiple epochs... this is what the explicit Dataset.cache() does for you. There are more context switches than are strictly necessary, especially when running in Eager mode, where the work can happen on a donated Python thread. Finally, profiling suggests that there are some non-trivial overheads when dealing with tensors of a single element (shape calculations, in particular, weigh heavily in the profile when the cost of the actual copy is small).\nThere are definitely some overheads worth paring down here, although they are likely to have only a small effect on headline performance numbers, so fixing them won't be top priority. I'll add some benchmarks to track the various versions, and invite contributions that improve things for all workloads.", "body": "The short answer here is that building a single minibatch of 100 elements is too small an operation to benefit from TensorFlow as it's currently implemented. (You'll also find that multiplying two small matrices is faster in NumPy than loading it into TensorFlow.)\r\n\r\nThe `Dataset` implementation is designed to perform non-trivial amount of preprocessing (decoding images, parsing text, etc.); and the `Session` implementation is geared towards computations that take at least a few milliseconds to perform. In the intended use case, you would pass the output of `iterator.get_next()` to the input of a neural network, for inference or training.\r\n\r\nEach of the TensorFlow operations in your example takes approximately 140 microseconds. Calling into TensorFlow from Python currently has an overhead of around 60 microseconds, a trivial step takes around 20 microseconds, and invoking an operation such as `Iterator.get_next()` incurs a few more microseconds of overhead, plus a context switch. \r\n\r\nThere are a couple of things you could try:\r\n\r\n* Try using a `tfe.Iterator` in Eager mode, which acts more like a Python iterator, and takes approximately 100us per batch in my tests.\r\n* Creating a callable for the step, using `get_next_chunk = sess.make_callable(next_chunk)` and then using `get_next_chunk()` in place of `sess.run(next_chunk)` shaves about 65us off each invocation (to approximately 92us per batch) in my tests.\r\n* Rewrite the pipeline to cache the processed data for an epoch:\r\n\r\n    ```python\r\n    dataset = tf.data.Dataset.from_tensor_slices(raw_data)\r\n    dataset = dataset.batch(MINI_BATCH)\r\n    dataset = dataset.cache()\r\n    dataset = dataset.repeat(EPOCHS)\r\n    iterator = dataset.make_one_shot_iterator()\r\n    ```\r\n\r\n  This change reduces the cost of each mini-batch to about 42us in my tests (and ~52us in Eager mode). \r\n\r\nWhere does the rest of the time go? In the non-caching versions, the data are currently copied twice, out of the original array in `Dataset.from_tensor_slices()`, then into the batch in `Dataset.batch()`. We could potentially use `Tensor::Slice()` to avoid a deep copy in some cases, although that only works when the slices are aligned to 32-byte boundaries, which might not always be the case. (It isn't for a vector of shape `(10000,)` `int64` elements but it is if e.g. you reshape it to `(100, 100)`.) Alternatively, and at the cost of some memory inflation, `Dataset.from_tensor_slices()` could eagerly slice up its input ahead of time, where the up-front initialized cost would be amortized over multiple epochs... this is what the explicit `Dataset.cache()` does for you. There are more context switches than are strictly necessary, especially when running in Eager mode, where the work can happen on a donated Python thread. Finally, profiling suggests that there are some non-trivial overheads when dealing with tensors of a single element (shape calculations, in particular, weigh heavily in the profile when the cost of the actual copy is small). \r\n\r\nThere are definitely some overheads worth paring down here, although they are likely to have only a small effect on headline performance numbers, so fixing them won't be top priority. I'll add some benchmarks to track the various versions, and invite contributions that improve things for all workloads."}