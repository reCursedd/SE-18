{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11496", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11496/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11496/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11496/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11496", "id": 242934109, "node_id": "MDU6SXNzdWUyNDI5MzQxMDk=", "number": 11496, "title": "Turning on XLA JIT Compilation during session crashes computer?", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-14T08:42:07Z", "updated_at": "2017-07-14T16:54:34Z", "closed_at": "2017-07-14T16:54:34Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>System information</h2>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.2.1</li>\n<li><strong>Python version</strong>:  2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.2</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/5.1</li>\n<li><strong>GPU model and memory</strong>: Titan X Pascal 12GB</li>\n<li><strong>Exact command to reproduce</strong>: Running any code with the following configuration passed to a supervisor managed session:</li>\n</ul>\n<pre><code># Config to turn on JIT compilation\nconfig = tf.ConfigProto()\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\nwith sv.managed_session(config=config) as sess:\n...\n</code></pre>\n<h3>Describe the problem</h3>\n<p>No idea why but when I tried to turn on the XLA JIT compilation according to this guide:<br>\n<a href=\"https://www.tensorflow.org/performance/xla/jit\" rel=\"nofollow\">https://www.tensorflow.org/performance/xla/jit</a></p>\n<p>My computer crashes immediately and I've no way of diagnosing the problem. Note that I installed TF from source and enabled the XLA JIT option. Does this option consume more power than normal?</p>\n<p>Previously, I encountered a similar problem when I ran the TF-slim VGG model with a pip-installed TensorFlow-gpu r1.2, and the computer crashes simply. I could not find any issue until I ran the exact same code in another computer (a laptop with weaker power supply) with source-compiled tensorflow, which works. I then replaced the pip-installed TensorFlow with a source-compiled one, and everything works fine --- this led me to think there might be some issue with either my installation or the source code (although I can't verify).</p>\n<p>Has anyone faced a similar problem? Also, is XLA activated by default if I configured it to be enabled during the source installation? i.e. it could be because I called the compilation twice when it was already activated by default, causing the system failure. Is there a way to verify whether XLA is activated - so far the only indication I see are the messages \"XLA service 0x62bb180 executing computations on platform Host\" and \"XLA service 0x62a43b0 executing computations on platform CUDA\". Is this sufficient, i.e. to say I do not have to manually activate XLA?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.2.1\nPython version:  2.7.12\nBazel version (if compiling from source): 0.5.2\nCUDA/cuDNN version: 8.0/5.1\nGPU model and memory: Titan X Pascal 12GB\nExact command to reproduce: Running any code with the following configuration passed to a supervisor managed session:\n\n# Config to turn on JIT compilation\nconfig = tf.ConfigProto()\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\nwith sv.managed_session(config=config) as sess:\n...\n\nDescribe the problem\nNo idea why but when I tried to turn on the XLA JIT compilation according to this guide:\nhttps://www.tensorflow.org/performance/xla/jit\nMy computer crashes immediately and I've no way of diagnosing the problem. Note that I installed TF from source and enabled the XLA JIT option. Does this option consume more power than normal?\nPreviously, I encountered a similar problem when I ran the TF-slim VGG model with a pip-installed TensorFlow-gpu r1.2, and the computer crashes simply. I could not find any issue until I ran the exact same code in another computer (a laptop with weaker power supply) with source-compiled tensorflow, which works. I then replaced the pip-installed TensorFlow with a source-compiled one, and everything works fine --- this led me to think there might be some issue with either my installation or the source code (although I can't verify).\nHas anyone faced a similar problem? Also, is XLA activated by default if I configured it to be enabled during the source installation? i.e. it could be because I called the compilation twice when it was already activated by default, causing the system failure. Is there a way to verify whether XLA is activated - so far the only indication I see are the messages \"XLA service 0x62bb180 executing computations on platform Host\" and \"XLA service 0x62a43b0 executing computations on platform CUDA\". Is this sufficient, i.e. to say I do not have to manually activate XLA?", "body": "## System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:No \r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.2.1\r\n- **Python version**:  2.7.12\r\n- **Bazel version (if compiling from source)**: 0.5.2\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: Titan X Pascal 12GB\r\n- **Exact command to reproduce**: Running any code with the following configuration passed to a supervisor managed session:\r\n\r\n```\r\n# Config to turn on JIT compilation\r\nconfig = tf.ConfigProto()\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\nwith sv.managed_session(config=config) as sess:\r\n...\r\n```\r\n\r\n### Describe the problem\r\nNo idea why but when I tried to turn on the XLA JIT compilation according to this guide:\r\nhttps://www.tensorflow.org/performance/xla/jit\r\n\r\nMy computer crashes immediately and I've no way of diagnosing the problem. Note that I installed TF from source and enabled the XLA JIT option. Does this option consume more power than normal? \r\n\r\nPreviously, I encountered a similar problem when I ran the TF-slim VGG model with a pip-installed TensorFlow-gpu r1.2, and the computer crashes simply. I could not find any issue until I ran the exact same code in another computer (a laptop with weaker power supply) with source-compiled tensorflow, which works. I then replaced the pip-installed TensorFlow with a source-compiled one, and everything works fine --- this led me to think there might be some issue with either my installation or the source code (although I can't verify). \r\n\r\nHas anyone faced a similar problem? Also, is XLA activated by default if I configured it to be enabled during the source installation? i.e. it could be because I called the compilation twice when it was already activated by default, causing the system failure. Is there a way to verify whether XLA is activated - so far the only indication I see are the messages \"XLA service 0x62bb180 executing computations on platform Host\" and \"XLA service 0x62a43b0 executing computations on platform CUDA\". Is this sufficient, i.e. to say I do not have to manually activate XLA?\r\n"}