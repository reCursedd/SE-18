{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/230378816", "html_url": "https://github.com/tensorflow/tensorflow/issues/3178#issuecomment-230378816", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3178", "id": 230378816, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMDM3ODgxNg==", "user": {"login": "Carreau", "id": 335567, "node_id": "MDQ6VXNlcjMzNTU2Nw==", "avatar_url": "https://avatars3.githubusercontent.com/u/335567?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Carreau", "html_url": "https://github.com/Carreau", "followers_url": "https://api.github.com/users/Carreau/followers", "following_url": "https://api.github.com/users/Carreau/following{/other_user}", "gists_url": "https://api.github.com/users/Carreau/gists{/gist_id}", "starred_url": "https://api.github.com/users/Carreau/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Carreau/subscriptions", "organizations_url": "https://api.github.com/users/Carreau/orgs", "repos_url": "https://api.github.com/users/Carreau/repos", "events_url": "https://api.github.com/users/Carreau/events{/privacy}", "received_events_url": "https://api.github.com/users/Carreau/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-05T03:04:04Z", "updated_at": "2016-07-05T03:04:04Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Actually I'm not 100% sure whether I'm using a virtual environment or virtual machine or Docker</p>\n</blockquote>\n<p>If you are not sure, then there is 99% chance that you are not using either Docker or a VM. If you are you would explicitly start these and you would thus be able to pass an option at startup to limit the RAM, in te case of Docker <code>--memory=\"&lt;memory&gt;\"</code>. In case of OS X it's worse as you run Docker in a VM.</p>\n<p>Os it's unlikely that your python is memory limited already:</p>\n<pre><code>In [15]: import numpy as np\nIn [18]: z = np.random.rand(*((1000,)*3)\nIn [17]: %whos\nVariable   Type       Data/Info\n-------------------------------\nnp         module     &lt;module 'numpy' from '/Us&lt;...&gt;kages/numpy/__init__.py'&gt;\nz          ndarray    1000x1000x1000: 1000000000 elems, type `float64`, 8000000000 bytes (7629.39453125 Mb)\n</code></pre>\n<p>(note we might want to update <code>whos</code>, sic) but I'm actually using 7.5Gb Ram.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/335567/16572830/e1681c26-4221-11e6-800b-057a03f7640b.png\"><img width=\"540\" alt=\"screen shot 2016-07-04 at 19 59 36\" src=\"https://cloud.githubusercontent.com/assets/335567/16572830/e1681c26-4221-11e6-800b-057a03f7640b.png\" style=\"max-width:100%;\"></a></p>\n<p>The \"limited to 2 Gb\" would make sens on 32bits OS, especialy windows, which you are extremely unlikely to get, you had to boot with <code>/LARGEADDRESSAWARE</code> flag, or upgrade your windows version (of vista) which was limited <em>on purpose</em>.</p>\n<p>My guess is that tensorflow is ment to be used on much more powerful machines (eg the machines I have seen it use have 100 of GB of memory of 10s of GB of GPUs). It is slow because your machine is just not up to the task.</p>", "body_text": "Actually I'm not 100% sure whether I'm using a virtual environment or virtual machine or Docker\n\nIf you are not sure, then there is 99% chance that you are not using either Docker or a VM. If you are you would explicitly start these and you would thus be able to pass an option at startup to limit the RAM, in te case of Docker --memory=\"<memory>\". In case of OS X it's worse as you run Docker in a VM.\nOs it's unlikely that your python is memory limited already:\nIn [15]: import numpy as np\nIn [18]: z = np.random.rand(*((1000,)*3)\nIn [17]: %whos\nVariable   Type       Data/Info\n-------------------------------\nnp         module     <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\nz          ndarray    1000x1000x1000: 1000000000 elems, type `float64`, 8000000000 bytes (7629.39453125 Mb)\n\n(note we might want to update whos, sic) but I'm actually using 7.5Gb Ram.\n\nThe \"limited to 2 Gb\" would make sens on 32bits OS, especialy windows, which you are extremely unlikely to get, you had to boot with /LARGEADDRESSAWARE flag, or upgrade your windows version (of vista) which was limited on purpose.\nMy guess is that tensorflow is ment to be used on much more powerful machines (eg the machines I have seen it use have 100 of GB of memory of 10s of GB of GPUs). It is slow because your machine is just not up to the task.", "body": "> Actually I'm not 100% sure whether I'm using a virtual environment or virtual machine or Docker\n\nIf you are not sure, then there is 99% chance that you are not using either Docker or a VM. If you are you would explicitly start these and you would thus be able to pass an option at startup to limit the RAM, in te case of Docker `--memory=\"<memory>\"`. In case of OS X it's worse as you run Docker in a VM. \n\nOs it's unlikely that your python is memory limited already:\n\n```\nIn [15]: import numpy as np\nIn [18]: z = np.random.rand(*((1000,)*3)\nIn [17]: %whos\nVariable   Type       Data/Info\n-------------------------------\nnp         module     <module 'numpy' from '/Us<...>kages/numpy/__init__.py'>\nz          ndarray    1000x1000x1000: 1000000000 elems, type `float64`, 8000000000 bytes (7629.39453125 Mb)\n```\n\n(note we might want to update `whos`, sic) but I'm actually using 7.5Gb Ram.\n\n<img width=\"540\" alt=\"screen shot 2016-07-04 at 19 59 36\" src=\"https://cloud.githubusercontent.com/assets/335567/16572830/e1681c26-4221-11e6-800b-057a03f7640b.png\">\n\nThe \"limited to 2 Gb\" would make sens on 32bits OS, especialy windows, which you are extremely unlikely to get, you had to boot with `/LARGEADDRESSAWARE` flag, or upgrade your windows version (of vista) which was limited _on purpose_.\n\nMy guess is that tensorflow is ment to be used on much more powerful machines (eg the machines I have seen it use have 100 of GB of memory of 10s of GB of GPUs). It is slow because your machine is just not up to the task. \n"}