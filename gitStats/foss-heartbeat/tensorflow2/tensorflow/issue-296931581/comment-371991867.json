{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/371991867", "html_url": "https://github.com/tensorflow/tensorflow/issues/16995#issuecomment-371991867", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16995", "id": 371991867, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTk5MTg2Nw==", "user": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-10T01:41:07Z", "updated_at": "2018-03-10T01:41:07Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=34459978\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/navsuda\">@navsuda</a>  Yes this is indeed due to the nudging that happens. Nudging is very important because the float zero has to be represented exactly as a fixed point value when converted for hardware.</p>\n<p>That being said, the particular implementation of nudging in FakeQuant may not be ideal because of saturation cases similar to what you describe. In practice it seems that as long as the nudging technique that the model is trained with is the same as what happens during inference on hardware, things seem to work out, but we are exploring different ways of nudging to avoid these cases.</p>\n<p>I will close this since this isn't really a true 'error' and is how to op is expected to work. We may be introduce new techniques in the future though.</p>\n<p>Thanks!</p>", "body_text": "@navsuda  Yes this is indeed due to the nudging that happens. Nudging is very important because the float zero has to be represented exactly as a fixed point value when converted for hardware.\nThat being said, the particular implementation of nudging in FakeQuant may not be ideal because of saturation cases similar to what you describe. In practice it seems that as long as the nudging technique that the model is trained with is the same as what happens during inference on hardware, things seem to work out, but we are exploring different ways of nudging to avoid these cases.\nI will close this since this isn't really a true 'error' and is how to op is expected to work. We may be introduce new techniques in the future though.\nThanks!", "body": "@navsuda  Yes this is indeed due to the nudging that happens. Nudging is very important because the float zero has to be represented exactly as a fixed point value when converted for hardware.\r\n\r\nThat being said, the particular implementation of nudging in FakeQuant may not be ideal because of saturation cases similar to what you describe. In practice it seems that as long as the nudging technique that the model is trained with is the same as what happens during inference on hardware, things seem to work out, but we are exploring different ways of nudging to avoid these cases.\r\n\r\nI will close this since this isn't really a true 'error' and is how to op is expected to work. We may be introduce new techniques in the future though.\r\n\r\nThanks!"}