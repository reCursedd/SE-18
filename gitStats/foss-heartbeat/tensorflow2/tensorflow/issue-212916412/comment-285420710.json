{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/285420710", "html_url": "https://github.com/tensorflow/tensorflow/issues/8227#issuecomment-285420710", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8227", "id": 285420710, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NTQyMDcxMA==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-09T17:28:22Z", "updated_at": "2017-03-09T17:30:00Z", "author_association": "MEMBER", "body_html": "<p>I agree.</p>\n<p>It is actually the case that internally XLA is free to permute the physical layout order of Tensor dimensions to improve speed.  However, when tensors flow in and out of XLA code from regular TensorFlow ops the data needs to be in row-major layout.</p>\n<p>The visible <code>data_layout</code> fields of Conv ops, etc. are an unfortunate artifact of the NVidia cuDNN library interface, and the way it is supported in TensorFlow.  Theoretically it ought to be possible to write a TensorFlow <code>GraphOptimizationPass</code> to analyze the user model (written with canonical layouts) and make the appropriate transformations.</p>", "body_text": "I agree.\nIt is actually the case that internally XLA is free to permute the physical layout order of Tensor dimensions to improve speed.  However, when tensors flow in and out of XLA code from regular TensorFlow ops the data needs to be in row-major layout.\nThe visible data_layout fields of Conv ops, etc. are an unfortunate artifact of the NVidia cuDNN library interface, and the way it is supported in TensorFlow.  Theoretically it ought to be possible to write a TensorFlow GraphOptimizationPass to analyze the user model (written with canonical layouts) and make the appropriate transformations.", "body": "I agree.  \r\n\r\nIt is actually the case that internally XLA is free to permute the physical layout order of Tensor dimensions to improve speed.  However, when tensors flow in and out of XLA code from regular TensorFlow ops the data needs to be in row-major layout.\r\n\r\nThe visible `data_layout` fields of Conv ops, etc. are an unfortunate artifact of the NVidia cuDNN library interface, and the way it is supported in TensorFlow.  Theoretically it ought to be possible to write a TensorFlow `GraphOptimizationPass` to analyze the user model (written with canonical layouts) and make the appropriate transformations.  "}