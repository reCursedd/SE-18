{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19825", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19825/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19825/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19825/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19825", "id": 330099107, "node_id": "MDU6SXNzdWUzMzAwOTkxMDc=", "number": 19825, "title": "Feature request: Concurrently serving models with optimizers", "user": {"login": "rothn", "id": 4665783, "node_id": "MDQ6VXNlcjQ2NjU3ODM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4665783?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rothn", "html_url": "https://github.com/rothn", "followers_url": "https://api.github.com/users/rothn/followers", "following_url": "https://api.github.com/users/rothn/following{/other_user}", "gists_url": "https://api.github.com/users/rothn/gists{/gist_id}", "starred_url": "https://api.github.com/users/rothn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rothn/subscriptions", "organizations_url": "https://api.github.com/users/rothn/orgs", "repos_url": "https://api.github.com/users/rothn/repos", "events_url": "https://api.github.com/users/rothn/events{/privacy}", "received_events_url": "https://api.github.com/users/rothn/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-06-07T02:56:39Z", "updated_at": "2018-11-20T13:28:14Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>==Problem==</p>\n<p>I want to use TensorFlow Serving to serve an output given certain placeholder inputs with a single run. The graph looks like this:</p>\n<p>Input vector -&gt; embedding lookup -&gt; iterative inference with optimizer and tf.while_loop -&gt; matrix math using inferred vector -&gt; top K -&gt; output</p>\n<p>The problem is that I can't simply use a Tensor in the loop because tf.Optimizer requires a Variable. This in turn forces me to modify global state in each prediction, effectively prohibiting concurrency.</p>\n<p>==Feature Request==</p>\n<p>This could be solved by implementing:</p>\n<ul>\n<li>a version of Optimizer.minimize() that returns a list of Tensors and takes Tensors instead of Variables as input and</li>\n<li>an optimization to make Optimizer.minimize() recycle Tensors that are loop variables re-assigned to the return of Optimizer.minimize()</li>\n</ul>\n<p>==For tensorflowbutler==</p>\n<p>Since this is a feature request, the following fields are irrelevant:<br>\nHave I written custom code: NA<br>\nOS Platform and Distribution: NA<br>\nTensorFlow installed from: NA<br>\nTensorFlow version: NA<br>\nBazel version: NA<br>\nCUDA/cuDNN version: NA<br>\nGPU model and memory: NA<br>\nExact command to reproduce: NA</p>", "body_text": "==Problem==\nI want to use TensorFlow Serving to serve an output given certain placeholder inputs with a single run. The graph looks like this:\nInput vector -> embedding lookup -> iterative inference with optimizer and tf.while_loop -> matrix math using inferred vector -> top K -> output\nThe problem is that I can't simply use a Tensor in the loop because tf.Optimizer requires a Variable. This in turn forces me to modify global state in each prediction, effectively prohibiting concurrency.\n==Feature Request==\nThis could be solved by implementing:\n\na version of Optimizer.minimize() that returns a list of Tensors and takes Tensors instead of Variables as input and\nan optimization to make Optimizer.minimize() recycle Tensors that are loop variables re-assigned to the return of Optimizer.minimize()\n\n==For tensorflowbutler==\nSince this is a feature request, the following fields are irrelevant:\nHave I written custom code: NA\nOS Platform and Distribution: NA\nTensorFlow installed from: NA\nTensorFlow version: NA\nBazel version: NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: NA", "body": "==Problem==\r\n\r\nI want to use TensorFlow Serving to serve an output given certain placeholder inputs with a single run. The graph looks like this:\r\n\r\nInput vector -> embedding lookup -> iterative inference with optimizer and tf.while_loop -> matrix math using inferred vector -> top K -> output\r\n\r\nThe problem is that I can't simply use a Tensor in the loop because tf.Optimizer requires a Variable. This in turn forces me to modify global state in each prediction, effectively prohibiting concurrency.\r\n\r\n==Feature Request==\r\n\r\nThis could be solved by implementing:\r\n* a version of Optimizer.minimize() that returns a list of Tensors and takes Tensors instead of Variables as input and\r\n* an optimization to make Optimizer.minimize() recycle Tensors that are loop variables re-assigned to the return of Optimizer.minimize()\r\n\r\n==For tensorflowbutler==\r\n\r\nSince this is a feature request, the following fields are irrelevant:\r\nHave I written custom code: NA\r\nOS Platform and Distribution: NA\r\nTensorFlow installed from: NA\r\nTensorFlow version: NA\r\nBazel version: NA\r\nCUDA/cuDNN version: NA\r\nGPU model and memory: NA\r\nExact command to reproduce: NA"}