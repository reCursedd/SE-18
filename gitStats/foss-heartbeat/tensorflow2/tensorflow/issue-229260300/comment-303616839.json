{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303616839", "html_url": "https://github.com/tensorflow/tensorflow/issues/9958#issuecomment-303616839", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9958", "id": 303616839, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzYxNjgzOQ==", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-24T04:48:59Z", "updated_at": "2017-05-24T05:07:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3376817\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/andrewharp\">@andrewharp</a> After several days of testing, I have managed to locate the source of the error, which is actually coming from <code>quantize_nodes</code>. As the ClassifierActivity.java file requires an input and output nodes of:</p>\n<pre><code>private static final String INPUT_NAME = \"Placeholder_only\";\nprivate static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\n</code></pre>\n<p>I realized the quantized file should have such an input output node as well. After confirming that the frozen model has these input and output nodes, I checked whether the quantized version has these nodes as well, and I realized it doesn't. Instead, it has these nodes after quantization:</p>\n<pre><code>(&lt;tf.Tensor 'import/MobileNet/conv_ds_6/dw_batch_norm/batchnorm/sub/_7__cf__7_quantized_const:0' shape=(256,) dtype=quint8&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_1/batch_norm/batchnorm/sub/_25__cf__25_quantized_min:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_1/weights_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_10/pw_batch_norm/batchnorm/sub/_29__cf__29_quantized_const:0' shape=(512,) dtype=quint8&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_12/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_1/convolution_eightbit/Placeholder_only/reduction_dims:0' shape=(1,) dtype=int32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_quantized_min:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_9/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_5/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_9/dw_batch_norm/batchnorm/Rsqrt/_34__cf__34_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_quantized_const:0' shape=(3, 3, 32, 1) dtype=quint8&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/biases_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_const:0' shape=(1, 1, 1024, 1024) dtype=quint8&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_14/depthwise_conv/biases_quantized_const:0' shape=(1024,) dtype=quint8&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/fc_16/weights_quantized_min:0' shape=() dtype=float32&gt;,)\n</code></pre>\n<p>After removing each graph transform one by one, I hoped to see if there is a specific transformation which after I removed, will give me the correct input node called 'Placeholder_only', which I included in the graph when I first froze the model. After checking all transformations, I found <code>quantize_nodes</code> is the transformation that removed the input_node. After removing <code>quantize_nodes</code> from the transformation, I have these nodes instead:</p>\n<pre><code>(&lt;tf.Tensor 'import/Placeholder_only:0' shape=&lt;unknown&gt; dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_min:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_const:0' shape=(512,) dtype=quint8&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53:0' shape=(512,) dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_max:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_min:0' shape=() dtype=float32&gt;,)\n(&lt;tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_const:0' shape=(512,) dtype=quint8&gt;,)\n</code></pre>\n<p>And now my quantized model works on the app. Is there a reason why <code>quantize_nodes</code> would cause the model to not work? <strong>However</strong>, my accuracy drops dramatically after removing this operation, till an extent where quantization won't bring any benefit to the application.</p>\n<p>One possible reason I could think of is that the input node is missing or at least changed to a Tensor which I can't identify. As I scanned through the tensors available after importing the quantized pb file, there seem to be only weight tensors, but not any input tensor. Is this supposed to be the case?</p>\n<p>Also, even after I've read the documentation here: <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#quantize_nodes\">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#quantize_nodes</a><br>\nI don't quite understand what <code>quantize_nodes</code> does and the benefits that may be gained. What are the purpose and benefits of this tool?</p>\n<p>Also, strangely for the logcat, the error log is all I could see when I run the app and after it crashes immediately. There is nothing produced after the 'fatal signal' output. My current logcat status is at 'Verbose' - is this the correct status to use?</p>", "body_text": "@andrewharp After several days of testing, I have managed to locate the source of the error, which is actually coming from quantize_nodes. As the ClassifierActivity.java file requires an input and output nodes of:\nprivate static final String INPUT_NAME = \"Placeholder_only\";\nprivate static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\n\nI realized the quantized file should have such an input output node as well. After confirming that the frozen model has these input and output nodes, I checked whether the quantized version has these nodes as well, and I realized it doesn't. Instead, it has these nodes after quantization:\n(<tf.Tensor 'import/MobileNet/conv_ds_6/dw_batch_norm/batchnorm/sub/_7__cf__7_quantized_const:0' shape=(256,) dtype=quint8>,)\n(<tf.Tensor 'import/MobileNet/conv_1/batch_norm/batchnorm/sub/_25__cf__25_quantized_min:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_1/weights_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_10/pw_batch_norm/batchnorm/sub/_29__cf__29_quantized_const:0' shape=(512,) dtype=quint8>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_12/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_1/convolution_eightbit/Placeholder_only/reduction_dims:0' shape=(1,) dtype=int32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_quantized_min:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_9/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_5/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_9/dw_batch_norm/batchnorm/Rsqrt/_34__cf__34_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_quantized_const:0' shape=(3, 3, 32, 1) dtype=quint8>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/biases_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_const:0' shape=(1, 1, 1024, 1024) dtype=quint8>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_14/depthwise_conv/biases_quantized_const:0' shape=(1024,) dtype=quint8>,)\n(<tf.Tensor 'import/MobileNet/fc_16/weights_quantized_min:0' shape=() dtype=float32>,)\n\nAfter removing each graph transform one by one, I hoped to see if there is a specific transformation which after I removed, will give me the correct input node called 'Placeholder_only', which I included in the graph when I first froze the model. After checking all transformations, I found quantize_nodes is the transformation that removed the input_node. After removing quantize_nodes from the transformation, I have these nodes instead:\n(<tf.Tensor 'import/Placeholder_only:0' shape=<unknown> dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_min:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_const:0' shape=(512,) dtype=quint8>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53:0' shape=(512,) dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_max:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_min:0' shape=() dtype=float32>,)\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_const:0' shape=(512,) dtype=quint8>,)\n\nAnd now my quantized model works on the app. Is there a reason why quantize_nodes would cause the model to not work? However, my accuracy drops dramatically after removing this operation, till an extent where quantization won't bring any benefit to the application.\nOne possible reason I could think of is that the input node is missing or at least changed to a Tensor which I can't identify. As I scanned through the tensors available after importing the quantized pb file, there seem to be only weight tensors, but not any input tensor. Is this supposed to be the case?\nAlso, even after I've read the documentation here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#quantize_nodes\nI don't quite understand what quantize_nodes does and the benefits that may be gained. What are the purpose and benefits of this tool?\nAlso, strangely for the logcat, the error log is all I could see when I run the app and after it crashes immediately. There is nothing produced after the 'fatal signal' output. My current logcat status is at 'Verbose' - is this the correct status to use?", "body": "@andrewharp After several days of testing, I have managed to locate the source of the error, which is actually coming from `quantize_nodes`. As the ClassifierActivity.java file requires an input and output nodes of:\r\n\r\n```\r\nprivate static final String INPUT_NAME = \"Placeholder_only\";\r\nprivate static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\r\n```\r\nI realized the quantized file should have such an input output node as well. After confirming that the frozen model has these input and output nodes, I checked whether the quantized version has these nodes as well, and I realized it doesn't. Instead, it has these nodes after quantization:\r\n\r\n```\r\n(<tf.Tensor 'import/MobileNet/conv_ds_6/dw_batch_norm/batchnorm/sub/_7__cf__7_quantized_const:0' shape=(256,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_1/batch_norm/batchnorm/sub/_25__cf__25_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_6/depthwise_conv/depthwise_weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_1/weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_10/pw_batch_norm/batchnorm/sub/_29__cf__29_quantized_const:0' shape=(512,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_12/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_1/convolution_eightbit/Placeholder_only/reduction_dims:0' shape=(1,) dtype=int32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_3/depthwise_conv/depthwise_weights_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_9/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_5/pointwise_conv/biases_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_9/dw_batch_norm/batchnorm/Rsqrt/_34__cf__34_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/depthwise_weights_quantized_const:0' shape=(3, 3, 32, 1) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_2/depthwise_conv/biases_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_14/pointwise_conv/weights_quantized_const:0' shape=(1, 1, 1024, 1024) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_14/depthwise_conv/biases_quantized_const:0' shape=(1024,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/fc_16/weights_quantized_min:0' shape=() dtype=float32>,)\r\n```\r\n\r\nAfter removing each graph transform one by one, I hoped to see if there is a specific transformation which after I removed, will give me the correct input node called 'Placeholder_only', which I included in the graph when I first froze the model. After checking all transformations, I found `quantize_nodes` is the transformation that removed the input_node. After removing `quantize_nodes` from the transformation, I have these nodes instead:\r\n\r\n```\r\n(<tf.Tensor 'import/Placeholder_only:0' shape=<unknown> dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53_quantized_const:0' shape=(512,) dtype=quint8>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/sub/_53__cf__53:0' shape=(512,) dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_max:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_min:0' shape=() dtype=float32>,)\r\n(<tf.Tensor 'import/MobileNet/conv_ds_8/dw_batch_norm/batchnorm/Rsqrt/_52__cf__52_quantized_const:0' shape=(512,) dtype=quint8>,)\r\n```\r\n\r\nAnd now my quantized model works on the app. Is there a reason why `quantize_nodes` would cause the model to not work? **However**, my accuracy drops dramatically after removing this operation, till an extent where quantization won't bring any benefit to the application. \r\n\r\nOne possible reason I could think of is that the input node is missing or at least changed to a Tensor which I can't identify. As I scanned through the tensors available after importing the quantized pb file, there seem to be only weight tensors, but not any input tensor. Is this supposed to be the case?\r\n\r\nAlso, even after I've read the documentation here: https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#quantize_nodes \r\nI don't quite understand what `quantize_nodes` does and the benefits that may be gained. What are the purpose and benefits of this tool?\r\n\r\nAlso, strangely for the logcat, the error log is all I could see when I run the app and after it crashes immediately. There is nothing produced after the 'fatal signal' output. My current logcat status is at 'Verbose' - is this the correct status to use?\r\n\r\n\r\n"}