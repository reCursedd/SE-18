{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336824781", "html_url": "https://github.com/tensorflow/tensorflow/issues/9958#issuecomment-336824781", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9958", "id": 336824781, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjgyNDc4MQ==", "user": {"login": "ruppeshnalwaya1993", "id": 2301185, "node_id": "MDQ6VXNlcjIzMDExODU=", "avatar_url": "https://avatars2.githubusercontent.com/u/2301185?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ruppeshnalwaya1993", "html_url": "https://github.com/ruppeshnalwaya1993", "followers_url": "https://api.github.com/users/ruppeshnalwaya1993/followers", "following_url": "https://api.github.com/users/ruppeshnalwaya1993/following{/other_user}", "gists_url": "https://api.github.com/users/ruppeshnalwaya1993/gists{/gist_id}", "starred_url": "https://api.github.com/users/ruppeshnalwaya1993/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ruppeshnalwaya1993/subscriptions", "organizations_url": "https://api.github.com/users/ruppeshnalwaya1993/orgs", "repos_url": "https://api.github.com/users/ruppeshnalwaya1993/repos", "events_url": "https://api.github.com/users/ruppeshnalwaya1993/events{/privacy}", "received_events_url": "https://api.github.com/users/ruppeshnalwaya1993/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-16T09:01:49Z", "updated_at": "2017-10-16T09:02:09Z", "author_association": "NONE", "body_html": "<p>I fine-tuned a mobilenet model for binary classification. The problem is when I use Quantised model on Android, it produces exactly same prediction values for all images after retraining. The same mobilenet model when non-quantised produces reasonable values.<br>\nThis problem is only on Android. On iOS it works nicely for both non-quantised and quantised model. What could be reason for this and solution for Android? I tried</p>\n<ol>\n<li>pre-compiled .so of tensorflow inference</li>\n<li>compiling .a file with selective registration.</li>\n</ol>", "body_text": "I fine-tuned a mobilenet model for binary classification. The problem is when I use Quantised model on Android, it produces exactly same prediction values for all images after retraining. The same mobilenet model when non-quantised produces reasonable values.\nThis problem is only on Android. On iOS it works nicely for both non-quantised and quantised model. What could be reason for this and solution for Android? I tried\n\npre-compiled .so of tensorflow inference\ncompiling .a file with selective registration.", "body": "I fine-tuned a mobilenet model for binary classification. The problem is when I use Quantised model on Android, it produces exactly same prediction values for all images after retraining. The same mobilenet model when non-quantised produces reasonable values. \r\nThis problem is only on Android. On iOS it works nicely for both non-quantised and quantised model. What could be reason for this and solution for Android? I tried \r\n1. pre-compiled .so of tensorflow inference\r\n2. compiling .a file with selective registration. \r\n\r\n"}