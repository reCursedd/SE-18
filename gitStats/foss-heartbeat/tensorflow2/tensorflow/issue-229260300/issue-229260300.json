{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9958", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9958/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9958/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9958/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9958", "id": 229260300, "node_id": "MDU6SXNzdWUyMjkyNjAzMDA=", "number": 9958, "title": "Android demo app crashes when using quantized model obtained from graph_transform tool?", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 28, "created_at": "2017-05-17T07:26:14Z", "updated_at": "2018-03-23T06:06:59Z", "closed_at": "2018-01-29T22:47:27Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nOnly edited <code>ClassifierActivity.java</code> to suit a custom model</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nAndroid smartphone / host machine: Ubuntu 16.04 LTS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/5.1</li>\n<li><strong>GPU model and memory</strong>: GTX 860M</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am currently following the guide in: <a href=\"http://nilhcem.com/android/custom-tensorflow-classifier\" rel=\"nofollow\">http://nilhcem.com/android/custom-tensorflow-classifier</a></p>\n<p>to train a custom classifier. However, I am using my own frozen graph that I obtained from my own training. What I noticed was when I used the quantized graph obtained through the <code>graph_transform</code> method, the app simply crashes without even running. In the guide, it is recommended to run this command on the inference graph:</p>\n<pre><code>bazel-bin/tensorflow/python/tools/optimize_for_inference \\\n  --input=/tf_files/retrained_graph.pb \\\n  --output=/tf_files/retrained_graph_optimized.pb \\\n  --input_names=Mul \\\n  --output_names=final_result\n</code></pre>\n<p>While I think it may be in conflict with the quantized graph transformations, I ran this command to test, and the app did crash. Here are the observations for all 4 permutations I tested:</p>\n<ol>\n<li>quantization + optimize_for_inference = <strong>app crash</strong></li>\n<li>quantization only = <strong>app crash</strong></li>\n<li>optimize_for_inference on frozen graph = <strong>app works</strong></li>\n<li>frozen_graph only = <strong>app works</strong></li>\n</ol>\n<p>So my conclusion is the quantization operations within the quantized graph caused the app to fail.</p>\n<p>Here is the quantization command I ran:</p>\n<pre><code>/home/kwotsin/tensorflow-android/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=./frozen_model_mobilenet.pb \\\n--out_graph=./quantized_model_mobilenet.pb \\\n--inputs='Placeholder_only' \\\n--outputs='MobileNet/Predictions/Softmax' \\\n--transforms='\n  add_default_attributes\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n  remove_nodes(op=Identity, op=CheckNumerics)\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms\n  quantize_weights\n  quantize_nodes\n  strip_unused_nodes\n  sort_by_execution_order'\n</code></pre>\n<p>And the java file I edited to build my APK:</p>\n<pre><code>  private static final int INPUT_SIZE = 224;\n  private static final int IMAGE_MEAN = 128;\n  private static final float IMAGE_STD = 128;\n  private static final String INPUT_NAME = \"Placeholder_only\";\n  private static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\n\n  private static final String MODEL_FILE = \"file:///android_asset/quantized_model_mobilenet.pb\";\n  private static final String LABEL_FILE =\n      \"file:///android_asset/mobilenet_labels.txt\";\n</code></pre>\n<p>Other than these edit, every other file is the same. I then imported this project in Android Studio, and ran the <code>build apk</code> option located on the top toolbar of Android Studio.</p>\n<p>So far, existing tutorials I've seen (e.g. those from Pete Warden) use the <code>quantize_graph</code> method to run on mobile devices. Is <code>graph_transform</code> compatible for quantizing models for mobile devices yet?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOnly edited ClassifierActivity.java to suit a custom model\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nAndroid smartphone / host machine: Ubuntu 16.04 LTS\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.1\nBazel version (if compiling from source): 0.4.5\nCUDA/cuDNN version: 8.0/5.1\nGPU model and memory: GTX 860M\n\nDescribe the problem\nI am currently following the guide in: http://nilhcem.com/android/custom-tensorflow-classifier\nto train a custom classifier. However, I am using my own frozen graph that I obtained from my own training. What I noticed was when I used the quantized graph obtained through the graph_transform method, the app simply crashes without even running. In the guide, it is recommended to run this command on the inference graph:\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\\n  --input=/tf_files/retrained_graph.pb \\\n  --output=/tf_files/retrained_graph_optimized.pb \\\n  --input_names=Mul \\\n  --output_names=final_result\n\nWhile I think it may be in conflict with the quantized graph transformations, I ran this command to test, and the app did crash. Here are the observations for all 4 permutations I tested:\n\nquantization + optimize_for_inference = app crash\nquantization only = app crash\noptimize_for_inference on frozen graph = app works\nfrozen_graph only = app works\n\nSo my conclusion is the quantization operations within the quantized graph caused the app to fail.\nHere is the quantization command I ran:\n/home/kwotsin/tensorflow-android/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\n--in_graph=./frozen_model_mobilenet.pb \\\n--out_graph=./quantized_model_mobilenet.pb \\\n--inputs='Placeholder_only' \\\n--outputs='MobileNet/Predictions/Softmax' \\\n--transforms='\n  add_default_attributes\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\n  remove_nodes(op=Identity, op=CheckNumerics)\n  fold_constants(ignore_errors=true)\n  fold_batch_norms\n  fold_old_batch_norms\n  quantize_weights\n  quantize_nodes\n  strip_unused_nodes\n  sort_by_execution_order'\n\nAnd the java file I edited to build my APK:\n  private static final int INPUT_SIZE = 224;\n  private static final int IMAGE_MEAN = 128;\n  private static final float IMAGE_STD = 128;\n  private static final String INPUT_NAME = \"Placeholder_only\";\n  private static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\n\n  private static final String MODEL_FILE = \"file:///android_asset/quantized_model_mobilenet.pb\";\n  private static final String LABEL_FILE =\n      \"file:///android_asset/mobilenet_labels.txt\";\n\nOther than these edit, every other file is the same. I then imported this project in Android Studio, and ran the build apk option located on the top toolbar of Android Studio.\nSo far, existing tutorials I've seen (e.g. those from Pete Warden) use the quantize_graph method to run on mobile devices. Is graph_transform compatible for quantizing models for mobile devices yet?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: \r\nOnly edited `ClassifierActivity.java` to suit a custom model\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nAndroid smartphone / host machine: Ubuntu 16.04 LTS\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.1\r\n- **Bazel version (if compiling from source)**: 0.4.5\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX 860M\r\n\r\n### Describe the problem\r\nI am currently following the guide in: http://nilhcem.com/android/custom-tensorflow-classifier\r\n\r\nto train a custom classifier. However, I am using my own frozen graph that I obtained from my own training. What I noticed was when I used the quantized graph obtained through the `graph_transform` method, the app simply crashes without even running. In the guide, it is recommended to run this command on the inference graph:\r\n\r\n```\r\nbazel-bin/tensorflow/python/tools/optimize_for_inference \\\r\n  --input=/tf_files/retrained_graph.pb \\\r\n  --output=/tf_files/retrained_graph_optimized.pb \\\r\n  --input_names=Mul \\\r\n  --output_names=final_result\r\n```\r\n\r\nWhile I think it may be in conflict with the quantized graph transformations, I ran this command to test, and the app did crash. Here are the observations for all 4 permutations I tested:\r\n\r\n1. quantization + optimize_for_inference = **app crash**\r\n2. quantization only = **app crash**\r\n3. optimize_for_inference on frozen graph = **app works**\r\n4. frozen_graph only = **app works**\r\n\r\nSo my conclusion is the quantization operations within the quantized graph caused the app to fail. \r\n\r\nHere is the quantization command I ran:\r\n\r\n```\r\n/home/kwotsin/tensorflow-android/tensorflow/bazel-bin/tensorflow/tools/graph_transforms/transform_graph \\\r\n--in_graph=./frozen_model_mobilenet.pb \\\r\n--out_graph=./quantized_model_mobilenet.pb \\\r\n--inputs='Placeholder_only' \\\r\n--outputs='MobileNet/Predictions/Softmax' \\\r\n--transforms='\r\n  add_default_attributes\r\n  strip_unused_nodes(type=float, shape=\"1,299,299,3\")\r\n  remove_nodes(op=Identity, op=CheckNumerics)\r\n  fold_constants(ignore_errors=true)\r\n  fold_batch_norms\r\n  fold_old_batch_norms\r\n  quantize_weights\r\n  quantize_nodes\r\n  strip_unused_nodes\r\n  sort_by_execution_order'\r\n```\r\n\r\nAnd the java file I edited to build my APK:\r\n\r\n```\r\n  private static final int INPUT_SIZE = 224;\r\n  private static final int IMAGE_MEAN = 128;\r\n  private static final float IMAGE_STD = 128;\r\n  private static final String INPUT_NAME = \"Placeholder_only\";\r\n  private static final String OUTPUT_NAME = \"MobileNet/Predictions/Softmax\";\r\n\r\n  private static final String MODEL_FILE = \"file:///android_asset/quantized_model_mobilenet.pb\";\r\n  private static final String LABEL_FILE =\r\n      \"file:///android_asset/mobilenet_labels.txt\";\r\n```\r\n\r\nOther than these edit, every other file is the same. I then imported this project in Android Studio, and ran the `build apk` option located on the top toolbar of Android Studio.\r\n\r\nSo far, existing tutorials I've seen (e.g. those from Pete Warden) use the `quantize_graph` method to run on mobile devices. Is `graph_transform` compatible for quantizing models for mobile devices yet?"}