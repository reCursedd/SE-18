{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11720", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11720/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11720/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11720/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11720", "id": 245131035, "node_id": "MDU6SXNzdWUyNDUxMzEwMzU=", "number": 11720, "title": "how to use the trained model(build in OOP schema) to predict new samples?", "user": {"login": "zishuaiz", "id": 9448376, "node_id": "MDQ6VXNlcjk0NDgzNzY=", "avatar_url": "https://avatars0.githubusercontent.com/u/9448376?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zishuaiz", "html_url": "https://github.com/zishuaiz", "followers_url": "https://api.github.com/users/zishuaiz/followers", "following_url": "https://api.github.com/users/zishuaiz/following{/other_user}", "gists_url": "https://api.github.com/users/zishuaiz/gists{/gist_id}", "starred_url": "https://api.github.com/users/zishuaiz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zishuaiz/subscriptions", "organizations_url": "https://api.github.com/users/zishuaiz/orgs", "repos_url": "https://api.github.com/users/zishuaiz/repos", "events_url": "https://api.github.com/users/zishuaiz/events{/privacy}", "received_events_url": "https://api.github.com/users/zishuaiz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-24T16:09:15Z", "updated_at": "2017-07-24T16:41:58Z", "closed_at": "2017-07-24T16:41:58Z", "author_association": "NONE", "body_html": "<p>Hi, there<br>\nI write a OOP schema cnn model, but there is some error when predict. I know that if define Variable and write the code like <code>c</code> style in one file, it is easy to restore the value, but in OOP schema, it is something wrong. Here is my class:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">TextCNN</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    A CNN for text classification.</span>\n<span class=\"pl-s\">    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>,\n                 <span class=\"pl-smi\">is_training</span>,\n                 <span class=\"pl-smi\">sequence_length</span>,\n                 <span class=\"pl-smi\">num_classes</span>,\n                 <span class=\"pl-smi\">vocab_size</span>,\n                 <span class=\"pl-smi\">embedding_size</span>,\n                 <span class=\"pl-smi\">filter_sizes</span>,\n                 <span class=\"pl-smi\">num_filters</span>,\n                 <span class=\"pl-smi\">l2_reg_lambda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>,\n                 <span class=\"pl-smi\">drop_out</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>):\n        <span class=\"pl-c1\">self</span>.is_training <span class=\"pl-k\">=</span> is_training\n        <span class=\"pl-c1\">self</span>.sequence_length <span class=\"pl-k\">=</span> sequence_length\n        <span class=\"pl-c1\">self</span>.num_classes <span class=\"pl-k\">=</span> num_classes\n        <span class=\"pl-c1\">self</span>.vocab_size <span class=\"pl-k\">=</span> vocab_size\n        <span class=\"pl-c1\">self</span>.embedding_size <span class=\"pl-k\">=</span> embedding_size\n        <span class=\"pl-c1\">self</span>.filter_sizes <span class=\"pl-k\">=</span> filter_sizes\n        <span class=\"pl-c1\">self</span>.num_filters <span class=\"pl-k\">=</span> num_filters\n        <span class=\"pl-c1\">self</span>.l2_reg_lambda <span class=\"pl-k\">=</span> l2_reg_lambda\n        <span class=\"pl-c1\">self</span>.input_x_test <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">self</span>.sequence_length], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x_text<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-c1\">self</span>.input_y_test <span class=\"pl-k\">=</span> tf.placeholder(tf.int32, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">self</span>.num_classes], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y_text<span class=\"pl-pds\">'</span></span>)\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.is_training:\n            <span class=\"pl-c1\">self</span>.dropout_keep_prob <span class=\"pl-k\">=</span> drop_out\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">self</span>.dropout_keep_prob <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.0</span>\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Embedding layer</span>\n        <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>), tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding<span class=\"pl-pds\">\"</span></span>):\n            <span class=\"pl-c1\">self</span>.W <span class=\"pl-k\">=</span> tf.Variable(\n                tf.random_uniform([vocab_size, embedding_size], <span class=\"pl-k\">-</span><span class=\"pl-c1\">1.0</span>, <span class=\"pl-c1\">1.0</span>),\n                <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W<span class=\"pl-pds\">\"</span></span>)\n        <span class=\"pl-c1\">self</span>.l2_loss <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0.0</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">inference</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">input_x</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>.is_training:\n            input_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.input_x_test\n        embedded_chars <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(<span class=\"pl-c1\">self</span>.W, input_x)\n        embedded_chars_expanded <span class=\"pl-k\">=</span> tf.expand_dims(embedded_chars, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create a convolution + maxpool layer for each filter size</span>\n        pooled_outputs <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i, filter_size <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(<span class=\"pl-c1\">self</span>.filter_sizes):\n            <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>conv-maxpool-<span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> filter_size):\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Convolution Layer</span>\n                filter_shape <span class=\"pl-k\">=</span> [filter_size, <span class=\"pl-c1\">self</span>.embedding_size, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.num_filters]\n                W <span class=\"pl-k\">=</span> tf.Variable(tf.truncated_normal(filter_shape, <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W<span class=\"pl-pds\">\"</span></span>)\n                b <span class=\"pl-k\">=</span> tf.Variable(tf.constant(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">self</span>.num_filters]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>b<span class=\"pl-pds\">\"</span></span>)\n                conv <span class=\"pl-k\">=</span> tf.nn.conv2d(\n                    embedded_chars_expanded,\n                    W,\n                    <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                    <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>VALID<span class=\"pl-pds\">\"</span></span>,\n                    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>conv<span class=\"pl-pds\">\"</span></span>)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Apply nonlinearity</span>\n                h <span class=\"pl-k\">=</span> tf.nn.relu(tf.nn.bias_add(conv, b), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>relu<span class=\"pl-pds\">\"</span></span>)\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Maxpooling over the outputs</span>\n                pooled <span class=\"pl-k\">=</span> tf.nn.max_pool(\n                    h,\n                    <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>.sequence_length <span class=\"pl-k\">-</span> filter_size <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                    <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>],\n                    <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>VALID<span class=\"pl-pds\">'</span></span>,\n                    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pool<span class=\"pl-pds\">\"</span></span>)\n                pooled_outputs.append(pooled)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Combine all the pooled features</span>\n        num_filters_total <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.num_filters <span class=\"pl-k\">*</span> <span class=\"pl-c1\">len</span>(<span class=\"pl-c1\">self</span>.filter_sizes)\n        h_pool <span class=\"pl-k\">=</span> tf.concat(pooled_outputs, <span class=\"pl-c1\">3</span>)\n        h_pool_flat <span class=\"pl-k\">=</span> tf.reshape(h_pool, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, num_filters_total])\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Add dropout</span>\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>dropout<span class=\"pl-pds\">\"</span></span>):\n            h_drop <span class=\"pl-k\">=</span> tf.nn.dropout(h_pool_flat, <span class=\"pl-c1\">self</span>.dropout_keep_prob)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Final (unnormalized) scores and predictions</span>\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>output<span class=\"pl-pds\">\"</span></span>):\n            W <span class=\"pl-k\">=</span> tf.get_variable(\n                <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W<span class=\"pl-pds\">\"</span></span>,\n                <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[num_filters_total, <span class=\"pl-c1\">self</span>.num_classes],\n                <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.contrib.layers.xavier_initializer())\n            b <span class=\"pl-k\">=</span> tf.Variable(tf.constant(<span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">self</span>.num_classes]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>b<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-c1\">self</span>.l2_loss <span class=\"pl-k\">+=</span> tf.nn.l2_loss(W)\n            <span class=\"pl-c1\">self</span>.l2_loss <span class=\"pl-k\">+=</span> tf.nn.l2_loss(b)\n            logits <span class=\"pl-k\">=</span> tf.nn.xw_plus_b(h_drop, W, b, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>scores<span class=\"pl-pds\">\"</span></span>)\n            <span class=\"pl-k\">return</span> logits\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">loss</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">input_y</span>):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> CalculateMean cross-entropy loss</span>\n        <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>loss<span class=\"pl-pds\">\"</span></span>):\n            <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>.is_training:\n                input_y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.input_y_test\n            losses <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>input_y)\n            loss <span class=\"pl-k\">=</span> tf.reduce_mean(losses) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>.l2_reg_lambda <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.l2_loss\n            <span class=\"pl-k\">return</span> loss\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">training</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">loss</span>, <span class=\"pl-smi\">learning_rate</span>):\n        tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>, loss)\n        optimizer <span class=\"pl-k\">=</span> tf.train.AdamOptimizer(learning_rate)\n        global_step <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>global_step<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        train_op <span class=\"pl-k\">=</span> optimizer.minimize(loss, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step)\n        <span class=\"pl-k\">return</span> train_op\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">evaluation</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">logits</span>, <span class=\"pl-smi\">input_y</span>):\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">self</span>.is_training:\n            input_y <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.input_y_test\n        predictions <span class=\"pl-k\">=</span> tf.argmax(logits, <span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>predictions<span class=\"pl-pds\">\"</span></span>)\n        correct_predictions <span class=\"pl-k\">=</span> tf.equal(predictions, tf.argmax(input_y, <span class=\"pl-c1\">1</span>))\n        accuracy <span class=\"pl-k\">=</span> tf.reduce_mean(tf.cast(correct_predictions, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>)\n        acc_summary <span class=\"pl-k\">=</span> tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>accuracy<span class=\"pl-pds\">\"</span></span>, accuracy)\n        <span class=\"pl-k\">return</span> accuracy</pre></div>\n<h2>and this is my train file:</h2>\n<div class=\"highlight highlight-source-python\"><pre>cnn <span class=\"pl-k\">=</span> TextCNN(<span class=\"pl-c1\">...</span>)\nlogits <span class=\"pl-k\">=</span> cnn.inference(x_batch)\nloss <span class=\"pl-k\">=</span> cnn.loss(logits, y_batch)\ntrain_op <span class=\"pl-k\">=</span> cnn.training(loss, <span class=\"pl-c1\">FLAGS</span>.learning_rate)\neval_correct <span class=\"pl-k\">=</span> cnn.evaluation(logits, y_batch)\n_, loss_value, accuracy <span class=\"pl-k\">=</span> sess.run([train_op, loss, eval_correct])\n<span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> <span class=\"pl-c1\">FLAGS</span>.checkpoint_every <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span> <span class=\"pl-k\">and</span> step <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">0</span>:\n    path <span class=\"pl-k\">=</span> saver.save(sess, checkpoint_prefix, <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>step)</pre></div>\n<h2>and this is my predict file:</h2>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> graph.as_default():\n    session_conf <span class=\"pl-k\">=</span> tf.ConfigProto(\n        <span class=\"pl-v\">allow_soft_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.allow_soft_placement,\n        <span class=\"pl-v\">log_device_placement</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.log_device_placement)\n    sess <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">config</span><span class=\"pl-k\">=</span>session_conf)\n\n    data_set_test <span class=\"pl-k\">=</span> DataSet(<span class=\"pl-c1\">...</span>)\n\n    <span class=\"pl-k\">with</span> sess.as_default():\n        cnn <span class=\"pl-k\">=</span> TextCNN(\n            <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n            <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>data_set_test.max_seq_len,\n            <span class=\"pl-v\">num_classes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">len</span>(data_set_test.label_dict),\n            <span class=\"pl-v\">vocab_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">len</span>(data_set_test.vocab),\n            <span class=\"pl-v\">embedding_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.embedding_dim,\n            <span class=\"pl-v\">filter_sizes</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">int</span>, <span class=\"pl-c1\">FLAGS</span>.filter_sizes.split(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>,<span class=\"pl-pds\">\"</span></span>))),\n            <span class=\"pl-v\">num_filters</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.num_filters,\n            <span class=\"pl-v\">l2_reg_lambda</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.l2_reg_lambda,\n            <span class=\"pl-v\">drop_out</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">FLAGS</span>.dropout_keep_prob\n        )\n\n        saver <span class=\"pl-k\">=</span> tf.train.Saver()\n        saver.restore(sess, checkpoint_file)\n        x_batch, y_batch <span class=\"pl-k\">=</span> data_set_test.next_batch(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">True</span>)\n        logits <span class=\"pl-k\">=</span> cnn.inference(x_batch)\n        results <span class=\"pl-k\">=</span> sess.run(logits, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{cnn.input_x_test: x_batch})\n        <span class=\"pl-k\">pass</span></pre></div>\n<h2>error is:</h2>\n<pre><code>tensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value output/b_1\n\t [[Node: output/b_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@output/b_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](output/b_1)]]\n</code></pre>\n<p>Do you know how to load the Variable like OOP schema, is there a demo projects?<br>\nThanks!</p>", "body_text": "Hi, there\nI write a OOP schema cnn model, but there is some error when predict. I know that if define Variable and write the code like c style in one file, it is easy to restore the value, but in OOP schema, it is something wrong. Here is my class:\nclass TextCNN(object):\n    \"\"\"\n    A CNN for text classification.\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n    \"\"\"\n\n    def __init__(self,\n                 is_training,\n                 sequence_length,\n                 num_classes,\n                 vocab_size,\n                 embedding_size,\n                 filter_sizes,\n                 num_filters,\n                 l2_reg_lambda=0.0,\n                 drop_out=1.0):\n        self.is_training = is_training\n        self.sequence_length = sequence_length\n        self.num_classes = num_classes\n        self.vocab_size = vocab_size\n        self.embedding_size = embedding_size\n        self.filter_sizes = filter_sizes\n        self.num_filters = num_filters\n        self.l2_reg_lambda = l2_reg_lambda\n        self.input_x_test = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x_text')\n        self.input_y_test = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y_text')\n        if self.is_training:\n            self.dropout_keep_prob = drop_out\n        else:\n            self.dropout_keep_prob = 1.0\n\n        # Embedding layer\n        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n            self.W = tf.Variable(\n                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n                name=\"W\")\n        self.l2_loss = tf.constant(0.0)\n\n    def inference(self, input_x):\n        if not self.is_training:\n            input_x = self.input_x_test\n        embedded_chars = tf.nn.embedding_lookup(self.W, input_x)\n        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\n\n        # Create a convolution + maxpool layer for each filter size\n        pooled_outputs = []\n        for i, filter_size in enumerate(self.filter_sizes):\n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n                # Convolution Layer\n                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\n                conv = tf.nn.conv2d(\n                    embedded_chars_expanded,\n                    W,\n                    strides=[1, 1, 1, 1],\n                    padding=\"VALID\",\n                    name=\"conv\")\n                # Apply nonlinearity\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n                # Maxpooling over the outputs\n                pooled = tf.nn.max_pool(\n                    h,\n                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\n                    strides=[1, 1, 1, 1],\n                    padding='VALID',\n                    name=\"pool\")\n                pooled_outputs.append(pooled)\n\n        # Combine all the pooled features\n        num_filters_total = self.num_filters * len(self.filter_sizes)\n        h_pool = tf.concat(pooled_outputs, 3)\n        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\n\n        # Add dropout\n        with tf.name_scope(\"dropout\"):\n            h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\n\n        # Final (unnormalized) scores and predictions\n        with tf.name_scope(\"output\"):\n            W = tf.get_variable(\n                \"W\",\n                shape=[num_filters_total, self.num_classes],\n                initializer=tf.contrib.layers.xavier_initializer())\n            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\n            self.l2_loss += tf.nn.l2_loss(W)\n            self.l2_loss += tf.nn.l2_loss(b)\n            logits = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\n            return logits\n\n    def loss(self, logits, input_y):\n        # CalculateMean cross-entropy loss\n        with tf.name_scope(\"loss\"):\n            if not self.is_training:\n                input_y = self.input_y_test\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\n            loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\n            return loss\n\n    def training(self, loss, learning_rate):\n        tf.summary.scalar('loss', loss)\n        optimizer = tf.train.AdamOptimizer(learning_rate)\n        global_step = tf.Variable(0, name='global_step', trainable=False)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n        return train_op\n\n    def evaluation(self, logits, input_y):\n        if not self.is_training:\n            input_y = self.input_y_test\n        predictions = tf.argmax(logits, 1, name=\"predictions\")\n        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\n        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\n        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\n        return accuracy\nand this is my train file:\ncnn = TextCNN(...)\nlogits = cnn.inference(x_batch)\nloss = cnn.loss(logits, y_batch)\ntrain_op = cnn.training(loss, FLAGS.learning_rate)\neval_correct = cnn.evaluation(logits, y_batch)\n_, loss_value, accuracy = sess.run([train_op, loss, eval_correct])\nif step % FLAGS.checkpoint_every == 0 and step != 0:\n    path = saver.save(sess, checkpoint_prefix, global_step=step)\nand this is my predict file:\nwith graph.as_default():\n    session_conf = tf.ConfigProto(\n        allow_soft_placement=FLAGS.allow_soft_placement,\n        log_device_placement=FLAGS.log_device_placement)\n    sess = tf.Session(config=session_conf)\n\n    data_set_test = DataSet(...)\n\n    with sess.as_default():\n        cnn = TextCNN(\n            is_training=False,\n            sequence_length=data_set_test.max_seq_len,\n            num_classes=len(data_set_test.label_dict),\n            vocab_size=len(data_set_test.vocab),\n            embedding_size=FLAGS.embedding_dim,\n            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n            num_filters=FLAGS.num_filters,\n            l2_reg_lambda=FLAGS.l2_reg_lambda,\n            drop_out=FLAGS.dropout_keep_prob\n        )\n\n        saver = tf.train.Saver()\n        saver.restore(sess, checkpoint_file)\n        x_batch, y_batch = data_set_test.next_batch(-1, True, True)\n        logits = cnn.inference(x_batch)\n        results = sess.run(logits, feed_dict={cnn.input_x_test: x_batch})\n        pass\nerror is:\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value output/b_1\n\t [[Node: output/b_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@output/b_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](output/b_1)]]\n\nDo you know how to load the Variable like OOP schema, is there a demo projects?\nThanks!", "body": "Hi, there\r\n   I write a OOP schema cnn model, but there is some error when predict. I know that if define Variable and write the code like `c` style in one file, it is easy to restore the value, but in OOP schema, it is something wrong. Here is my class:\r\n```python\r\nclass TextCNN(object):\r\n    \"\"\"\r\n    A CNN for text classification.\r\n    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 is_training,\r\n                 sequence_length,\r\n                 num_classes,\r\n                 vocab_size,\r\n                 embedding_size,\r\n                 filter_sizes,\r\n                 num_filters,\r\n                 l2_reg_lambda=0.0,\r\n                 drop_out=1.0):\r\n        self.is_training = is_training\r\n        self.sequence_length = sequence_length\r\n        self.num_classes = num_classes\r\n        self.vocab_size = vocab_size\r\n        self.embedding_size = embedding_size\r\n        self.filter_sizes = filter_sizes\r\n        self.num_filters = num_filters\r\n        self.l2_reg_lambda = l2_reg_lambda\r\n        self.input_x_test = tf.placeholder(tf.int32, [None, self.sequence_length], name='input_x_text')\r\n        self.input_y_test = tf.placeholder(tf.int32, [None, self.num_classes], name='input_y_text')\r\n        if self.is_training:\r\n            self.dropout_keep_prob = drop_out\r\n        else:\r\n            self.dropout_keep_prob = 1.0\r\n\r\n        # Embedding layer\r\n        with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\r\n            self.W = tf.Variable(\r\n                tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\r\n                name=\"W\")\r\n        self.l2_loss = tf.constant(0.0)\r\n\r\n    def inference(self, input_x):\r\n        if not self.is_training:\r\n            input_x = self.input_x_test\r\n        embedded_chars = tf.nn.embedding_lookup(self.W, input_x)\r\n        embedded_chars_expanded = tf.expand_dims(embedded_chars, -1)\r\n\r\n        # Create a convolution + maxpool layer for each filter size\r\n        pooled_outputs = []\r\n        for i, filter_size in enumerate(self.filter_sizes):\r\n            with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\r\n                # Convolution Layer\r\n                filter_shape = [filter_size, self.embedding_size, 1, self.num_filters]\r\n                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\r\n                b = tf.Variable(tf.constant(0.1, shape=[self.num_filters]), name=\"b\")\r\n                conv = tf.nn.conv2d(\r\n                    embedded_chars_expanded,\r\n                    W,\r\n                    strides=[1, 1, 1, 1],\r\n                    padding=\"VALID\",\r\n                    name=\"conv\")\r\n                # Apply nonlinearity\r\n                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\r\n                # Maxpooling over the outputs\r\n                pooled = tf.nn.max_pool(\r\n                    h,\r\n                    ksize=[1, self.sequence_length - filter_size + 1, 1, 1],\r\n                    strides=[1, 1, 1, 1],\r\n                    padding='VALID',\r\n                    name=\"pool\")\r\n                pooled_outputs.append(pooled)\r\n\r\n        # Combine all the pooled features\r\n        num_filters_total = self.num_filters * len(self.filter_sizes)\r\n        h_pool = tf.concat(pooled_outputs, 3)\r\n        h_pool_flat = tf.reshape(h_pool, [-1, num_filters_total])\r\n\r\n        # Add dropout\r\n        with tf.name_scope(\"dropout\"):\r\n            h_drop = tf.nn.dropout(h_pool_flat, self.dropout_keep_prob)\r\n\r\n        # Final (unnormalized) scores and predictions\r\n        with tf.name_scope(\"output\"):\r\n            W = tf.get_variable(\r\n                \"W\",\r\n                shape=[num_filters_total, self.num_classes],\r\n                initializer=tf.contrib.layers.xavier_initializer())\r\n            b = tf.Variable(tf.constant(0.1, shape=[self.num_classes]), name=\"b\")\r\n            self.l2_loss += tf.nn.l2_loss(W)\r\n            self.l2_loss += tf.nn.l2_loss(b)\r\n            logits = tf.nn.xw_plus_b(h_drop, W, b, name=\"scores\")\r\n            return logits\r\n\r\n    def loss(self, logits, input_y):\r\n        # CalculateMean cross-entropy loss\r\n        with tf.name_scope(\"loss\"):\r\n            if not self.is_training:\r\n                input_y = self.input_y_test\r\n            losses = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=input_y)\r\n            loss = tf.reduce_mean(losses) + self.l2_reg_lambda * self.l2_loss\r\n            return loss\r\n\r\n    def training(self, loss, learning_rate):\r\n        tf.summary.scalar('loss', loss)\r\n        optimizer = tf.train.AdamOptimizer(learning_rate)\r\n        global_step = tf.Variable(0, name='global_step', trainable=False)\r\n        train_op = optimizer.minimize(loss, global_step=global_step)\r\n        return train_op\r\n\r\n    def evaluation(self, logits, input_y):\r\n        if not self.is_training:\r\n            input_y = self.input_y_test\r\n        predictions = tf.argmax(logits, 1, name=\"predictions\")\r\n        correct_predictions = tf.equal(predictions, tf.argmax(input_y, 1))\r\n        accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")\r\n        acc_summary = tf.summary.scalar(\"accuracy\", accuracy)\r\n        return accuracy\r\n```\r\n\r\n## and this is my train file:\r\n```python\r\ncnn = TextCNN(...)\r\nlogits = cnn.inference(x_batch)\r\nloss = cnn.loss(logits, y_batch)\r\ntrain_op = cnn.training(loss, FLAGS.learning_rate)\r\neval_correct = cnn.evaluation(logits, y_batch)\r\n_, loss_value, accuracy = sess.run([train_op, loss, eval_correct])\r\nif step % FLAGS.checkpoint_every == 0 and step != 0:\r\n    path = saver.save(sess, checkpoint_prefix, global_step=step)\r\n```\r\n\r\n\r\n## and this is my predict file:\r\n```python\r\nwith graph.as_default():\r\n    session_conf = tf.ConfigProto(\r\n        allow_soft_placement=FLAGS.allow_soft_placement,\r\n        log_device_placement=FLAGS.log_device_placement)\r\n    sess = tf.Session(config=session_conf)\r\n\r\n    data_set_test = DataSet(...)\r\n\r\n    with sess.as_default():\r\n        cnn = TextCNN(\r\n            is_training=False,\r\n            sequence_length=data_set_test.max_seq_len,\r\n            num_classes=len(data_set_test.label_dict),\r\n            vocab_size=len(data_set_test.vocab),\r\n            embedding_size=FLAGS.embedding_dim,\r\n            filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\r\n            num_filters=FLAGS.num_filters,\r\n            l2_reg_lambda=FLAGS.l2_reg_lambda,\r\n            drop_out=FLAGS.dropout_keep_prob\r\n        )\r\n\r\n        saver = tf.train.Saver()\r\n        saver.restore(sess, checkpoint_file)\r\n        x_batch, y_batch = data_set_test.next_batch(-1, True, True)\r\n        logits = cnn.inference(x_batch)\r\n        results = sess.run(logits, feed_dict={cnn.input_x_test: x_batch})\r\n        pass\r\n```\r\n\r\n\r\n## error is:\r\n```\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: Attempting to use uninitialized value output/b_1\r\n\t [[Node: output/b_1/read = Identity[T=DT_FLOAT, _class=[\"loc:@output/b_1\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](output/b_1)]]\r\n```\r\n\r\nDo you know how to load the Variable like OOP schema, is there a demo projects?\r\nThanks!\r\n\r\n"}