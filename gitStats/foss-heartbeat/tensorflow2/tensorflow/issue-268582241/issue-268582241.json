{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13985", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13985/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13985/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13985/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/13985", "id": 268582241, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQ4ODA0ODIx", "number": 13985, "title": "Update adding_an_op.md", "user": {"login": "ecpoppenheimer", "id": 31865490, "node_id": "MDQ6VXNlcjMxODY1NDkw", "avatar_url": "https://avatars1.githubusercontent.com/u/31865490?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ecpoppenheimer", "html_url": "https://github.com/ecpoppenheimer", "followers_url": "https://api.github.com/users/ecpoppenheimer/followers", "following_url": "https://api.github.com/users/ecpoppenheimer/following{/other_user}", "gists_url": "https://api.github.com/users/ecpoppenheimer/gists{/gist_id}", "starred_url": "https://api.github.com/users/ecpoppenheimer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ecpoppenheimer/subscriptions", "organizations_url": "https://api.github.com/users/ecpoppenheimer/orgs", "repos_url": "https://api.github.com/users/ecpoppenheimer/repos", "events_url": "https://api.github.com/users/ecpoppenheimer/events{/privacy}", "received_events_url": "https://api.github.com/users/ecpoppenheimer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 474725938, "node_id": "MDU6TGFiZWw0NzQ3MjU5Mzg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stalled", "name": "stalled", "color": "d4c5f9", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2017-10-25T23:25:05Z", "updated_at": "2018-05-17T18:54:21Z", "closed_at": "2018-05-17T18:48:14Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13985", "html_url": "https://github.com/tensorflow/tensorflow/pull/13985", "diff_url": "https://github.com/tensorflow/tensorflow/pull/13985.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/13985.patch"}, "body_html": "<p>I am proposing several changes to this file.  I am a new user to TF, and used this documentation extensively while trying to put a together my custom activation function, since I wanted to explore how different activation functions might affect NN training and performace.  I eventually figured out everything I needed to know, but it took weeks because this documentation is lacking in several areas, so I have a good feel for how this documentation can be improved to make life easier for future new users.  Changes 1 and 2 are pretty clear and I expect to not be controversial.  Change 3 you will probably need to think about, and might want to do further work to make things more clear.</p>\n<ol>\n<li>\n<p>correct example.cc (221-228):<br>\nI added a REGISTER_OP macro to the \"example\" code.  The previous absence of these lines was clearly an omission in the documentaion - the example code does not work without this line.  This is the least controversial change I am making.</p>\n</li>\n<li>\n<p>explain how to use Bazel to compile a CUDA op (1256-1272:<br>\nI eventually figured out how to do this on my own, but it sure would have been useful had this information been in this documentation.</p>\n</li>\n<li>\n<p>explaining how the gradient actually works in TF (1293-1301):<br>\nNow that I know the answer, I can read the documentation and sort of parse what it means, but I found this extremely confusing when I was reading it for the first (and tenth) time.  I have a masters in physics, so I understand calculus, but it wasn't until I had implemented backpropagation twice by hand and spent a bunch of time trying to imagine how TF must be working under the hood before I got this.  So I was tempted to delete this entire section and start over.  But I am not sure I can win this battle, because a major portion of what makes this confusing for me is the way TF uses the word gradient.  The thing that all of the TF documentation refers to as \"gradient\" I would much prefer to refer to as \"error\".  But I doubt I am going to get everybody to change everything.</p>\n</li>\n</ol>\n<p>So I have opted to just append another paragraph to this section, which I hope will explain things better.  It might be nice to update the mathematical notation a little bit, but since I am currently making these edits in a simple text editor I don't want to wade too deeply into trying to figure out the notation.</p>\n<ol start=\"4\">\n<li>Putting it all together (1467-1778):<br>\nI was able to find pieces of example code while googling, but somehow I wasn't really able to find a complete tutorial that documented all of the steps start to finish of designing and using a custom activation function.  So I think it might be useful to post a complete example here which is just complicated enough to demonstrate all the parts needed to write a custom activation function working together.  But you might decide that this belongs elsewhere.</li>\n</ol>", "body_text": "I am proposing several changes to this file.  I am a new user to TF, and used this documentation extensively while trying to put a together my custom activation function, since I wanted to explore how different activation functions might affect NN training and performace.  I eventually figured out everything I needed to know, but it took weeks because this documentation is lacking in several areas, so I have a good feel for how this documentation can be improved to make life easier for future new users.  Changes 1 and 2 are pretty clear and I expect to not be controversial.  Change 3 you will probably need to think about, and might want to do further work to make things more clear.\n\n\ncorrect example.cc (221-228):\nI added a REGISTER_OP macro to the \"example\" code.  The previous absence of these lines was clearly an omission in the documentaion - the example code does not work without this line.  This is the least controversial change I am making.\n\n\nexplain how to use Bazel to compile a CUDA op (1256-1272:\nI eventually figured out how to do this on my own, but it sure would have been useful had this information been in this documentation.\n\n\nexplaining how the gradient actually works in TF (1293-1301):\nNow that I know the answer, I can read the documentation and sort of parse what it means, but I found this extremely confusing when I was reading it for the first (and tenth) time.  I have a masters in physics, so I understand calculus, but it wasn't until I had implemented backpropagation twice by hand and spent a bunch of time trying to imagine how TF must be working under the hood before I got this.  So I was tempted to delete this entire section and start over.  But I am not sure I can win this battle, because a major portion of what makes this confusing for me is the way TF uses the word gradient.  The thing that all of the TF documentation refers to as \"gradient\" I would much prefer to refer to as \"error\".  But I doubt I am going to get everybody to change everything.\n\n\nSo I have opted to just append another paragraph to this section, which I hope will explain things better.  It might be nice to update the mathematical notation a little bit, but since I am currently making these edits in a simple text editor I don't want to wade too deeply into trying to figure out the notation.\n\nPutting it all together (1467-1778):\nI was able to find pieces of example code while googling, but somehow I wasn't really able to find a complete tutorial that documented all of the steps start to finish of designing and using a custom activation function.  So I think it might be useful to post a complete example here which is just complicated enough to demonstrate all the parts needed to write a custom activation function working together.  But you might decide that this belongs elsewhere.", "body": "I am proposing several changes to this file.  I am a new user to TF, and used this documentation extensively while trying to put a together my custom activation function, since I wanted to explore how different activation functions might affect NN training and performace.  I eventually figured out everything I needed to know, but it took weeks because this documentation is lacking in several areas, so I have a good feel for how this documentation can be improved to make life easier for future new users.  Changes 1 and 2 are pretty clear and I expect to not be controversial.  Change 3 you will probably need to think about, and might want to do further work to make things more clear.\r\n\r\n1) correct example.cc (221-228):\r\nI added a REGISTER_OP macro to the \"example\" code.  The previous absence of these lines was clearly an omission in the documentaion - the example code does not work without this line.  This is the least controversial change I am making.\r\n\r\n2) explain how to use Bazel to compile a CUDA op (1256-1272:\r\nI eventually figured out how to do this on my own, but it sure would have been useful had this information been in this documentation.\r\n\r\n3) explaining how the gradient actually works in TF (1293-1301):\r\nNow that I know the answer, I can read the documentation and sort of parse what it means, but I found this extremely confusing when I was reading it for the first (and tenth) time.  I have a masters in physics, so I understand calculus, but it wasn't until I had implemented backpropagation twice by hand and spent a bunch of time trying to imagine how TF must be working under the hood before I got this.  So I was tempted to delete this entire section and start over.  But I am not sure I can win this battle, because a major portion of what makes this confusing for me is the way TF uses the word gradient.  The thing that all of the TF documentation refers to as \"gradient\" I would much prefer to refer to as \"error\".  But I doubt I am going to get everybody to change everything.\r\n\r\nSo I have opted to just append another paragraph to this section, which I hope will explain things better.  It might be nice to update the mathematical notation a little bit, but since I am currently making these edits in a simple text editor I don't want to wade too deeply into trying to figure out the notation.\r\n\r\n4) Putting it all together (1467-1778):\r\nI was able to find pieces of example code while googling, but somehow I wasn't really able to find a complete tutorial that documented all of the steps start to finish of designing and using a custom activation function.  So I think it might be useful to post a complete example here which is just complicated enough to demonstrate all the parts needed to write a custom activation function working together.  But you might decide that this belongs elsewhere."}