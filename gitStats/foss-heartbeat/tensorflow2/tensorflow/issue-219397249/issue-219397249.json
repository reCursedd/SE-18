{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8964", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8964/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8964/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8964/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8964", "id": 219397249, "node_id": "MDU6SXNzdWUyMTkzOTcyNDk=", "number": 8964, "title": "Is there a bug in tf.layers.batch_normalization?", "user": {"login": "yliu120", "id": 9438093, "node_id": "MDQ6VXNlcjk0MzgwOTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/9438093?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yliu120", "html_url": "https://github.com/yliu120", "followers_url": "https://api.github.com/users/yliu120/followers", "following_url": "https://api.github.com/users/yliu120/following{/other_user}", "gists_url": "https://api.github.com/users/yliu120/gists{/gist_id}", "starred_url": "https://api.github.com/users/yliu120/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yliu120/subscriptions", "organizations_url": "https://api.github.com/users/yliu120/orgs", "repos_url": "https://api.github.com/users/yliu120/repos", "events_url": "https://api.github.com/users/yliu120/events{/privacy}", "received_events_url": "https://api.github.com/users/yliu120/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-04T21:58:57Z", "updated_at": "2017-04-04T22:14:00Z", "closed_at": "2017-04-04T22:12:55Z", "author_association": "NONE", "body_html": "<p>Since TF 1.0 API came out, I have been trying to use <code>tf.layers.batch_normalization</code> instead of the version in tf.contrib.layers. However, I found this layer works abnormally in my case.</p>\n<p>Here is my simple code that uses <code>tf.layers.batch_normalization</code>.</p>\n<pre><code>output = tf.nn.bias_add(tf.matmul(input_tensor, self._weight), self._bias)\n\n  if self._use_batch_norm:\n       output = tf.layers.batch_normalization(\n             output,\n             momentum=0.9,\n             training=training,\n             name=self._name + \"_bn\",\n             reuse=reuse\n        )\n\n output = tf.nn.relu(output)\n</code></pre>\n<p>However, when I enabled the batch_normalization on this layer. I found that my model maps all raw data to a single point in the hidden representation space. When I disabled batch_normalization, this cannot happen at all.</p>\n<p>Here is a final view of my data in the latent space:</p>\n<p>array([[ 0.46338093,  0.53661913],<br>\n[ 0.46339276,  0.53660733],<br>\n[ 0.46329296,  0.53670704],<br>\n...,<br>\n[ 0.4633435 ,  0.53665644],<br>\n[ 0.46335611,  0.53664398],<br>\n[ 0.4633027 ,  0.53669727]], dtype=float32)</p>\n<p>The input data are generated from a multivariate gaussian distribution so that their hidden representations should be different as well. I searched the usage of this function on stackoverflow but the example is so trivial and it doesn't help. I think there may be a bug in this layer object or I may misuse it somehow.</p>\n<p>In the contrib.layers's version, the batch_norm layer should be linked with update_ops collection. But I read the source code of this implementation and it seems that it is not necessary. Is there anyone has some thoughts on this?</p>", "body_text": "Since TF 1.0 API came out, I have been trying to use tf.layers.batch_normalization instead of the version in tf.contrib.layers. However, I found this layer works abnormally in my case.\nHere is my simple code that uses tf.layers.batch_normalization.\noutput = tf.nn.bias_add(tf.matmul(input_tensor, self._weight), self._bias)\n\n  if self._use_batch_norm:\n       output = tf.layers.batch_normalization(\n             output,\n             momentum=0.9,\n             training=training,\n             name=self._name + \"_bn\",\n             reuse=reuse\n        )\n\n output = tf.nn.relu(output)\n\nHowever, when I enabled the batch_normalization on this layer. I found that my model maps all raw data to a single point in the hidden representation space. When I disabled batch_normalization, this cannot happen at all.\nHere is a final view of my data in the latent space:\narray([[ 0.46338093,  0.53661913],\n[ 0.46339276,  0.53660733],\n[ 0.46329296,  0.53670704],\n...,\n[ 0.4633435 ,  0.53665644],\n[ 0.46335611,  0.53664398],\n[ 0.4633027 ,  0.53669727]], dtype=float32)\nThe input data are generated from a multivariate gaussian distribution so that their hidden representations should be different as well. I searched the usage of this function on stackoverflow but the example is so trivial and it doesn't help. I think there may be a bug in this layer object or I may misuse it somehow.\nIn the contrib.layers's version, the batch_norm layer should be linked with update_ops collection. But I read the source code of this implementation and it seems that it is not necessary. Is there anyone has some thoughts on this?", "body": "Since TF 1.0 API came out, I have been trying to use `tf.layers.batch_normalization` instead of the version in tf.contrib.layers. However, I found this layer works abnormally in my case.\r\n\r\nHere is my simple code that uses `tf.layers.batch_normalization`.\r\n\r\n```\r\noutput = tf.nn.bias_add(tf.matmul(input_tensor, self._weight), self._bias)\r\n\r\n  if self._use_batch_norm:\r\n       output = tf.layers.batch_normalization(\r\n             output,\r\n             momentum=0.9,\r\n             training=training,\r\n             name=self._name + \"_bn\",\r\n             reuse=reuse\r\n        )\r\n\r\n output = tf.nn.relu(output)\r\n```\r\n\r\nHowever, when I enabled the batch_normalization on this layer. I found that my model maps all raw data to a single point in the hidden representation space. When I disabled batch_normalization, this cannot happen at all.\r\n\r\nHere is a final view of my data in the latent space:\r\n\r\narray([[ 0.46338093,  0.53661913],\r\n       [ 0.46339276,  0.53660733],\r\n       [ 0.46329296,  0.53670704],\r\n       ...,\r\n       [ 0.4633435 ,  0.53665644],\r\n       [ 0.46335611,  0.53664398],\r\n       [ 0.4633027 ,  0.53669727]], dtype=float32)\r\n\r\nThe input data are generated from a multivariate gaussian distribution so that their hidden representations should be different as well. I searched the usage of this function on stackoverflow but the example is so trivial and it doesn't help. I think there may be a bug in this layer object or I may misuse it somehow.\r\n\r\nIn the contrib.layers's version, the batch_norm layer should be linked with update_ops collection. But I read the source code of this implementation and it seems that it is not necessary. Is there anyone has some thoughts on this?\r\n"}