{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10291", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10291/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10291/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10291/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10291", "id": 232301314, "node_id": "MDU6SXNzdWUyMzIzMDEzMTQ=", "number": 10291, "title": "Memory violation when adding an new op", "user": {"login": "sj6077", "id": 2465713, "node_id": "MDQ6VXNlcjI0NjU3MTM=", "avatar_url": "https://avatars1.githubusercontent.com/u/2465713?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sj6077", "html_url": "https://github.com/sj6077", "followers_url": "https://api.github.com/users/sj6077/followers", "following_url": "https://api.github.com/users/sj6077/following{/other_user}", "gists_url": "https://api.github.com/users/sj6077/gists{/gist_id}", "starred_url": "https://api.github.com/users/sj6077/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sj6077/subscriptions", "organizations_url": "https://api.github.com/users/sj6077/orgs", "repos_url": "https://api.github.com/users/sj6077/repos", "events_url": "https://api.github.com/users/sj6077/events{/privacy}", "received_events_url": "https://api.github.com/users/sj6077/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-05-30T15:58:14Z", "updated_at": "2017-06-02T00:53:17Z", "closed_at": "2017-06-02T00:53:16Z", "author_association": "CONTRIBUTOR", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\ncommit <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/3bee923c93f9624ce3abf8d55173be66a7755545/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/3bee923c93f9624ce3abf8d55173be66a7755545\"><tt>3bee923</tt></a></li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.4.5</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\ncuda 8.0/cudnn 5.1.5</li>\n<li><strong>GPU model and memory</strong>:<br>\nTesla P40</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I created a custom operation, it works well with bazel test  --run_under=valgrind. However, when I uses the custom operation using python api, memory violation is happend. Even though no modification of the input tensor, input tensor error comes out.</p>\n<h3>Source code / logs</h3>\n<pre><code>namespace tensorflow {\n\nclass InGraphAutoParallelOp : public OpKernel {\n public:\n  explicit InGraphAutoParallelOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    // Get graph definition\n    const Tensor* meta_graph_proto_tensor;\n    OP_REQUIRES_OK(ctx, ctx-&gt;input(\"meta_graph_def_str\", &amp;meta_graph_proto_tensor));\n    MetaGraphDef meta_graph_def;\n    meta_graph_def.ParseFromString(meta_graph_proto_tensor-&gt;scalar&lt;string&gt;()());\n\n    // Get num replicas\n    const Tensor* num_replicas_tensor;\n    OP_REQUIRES_OK(ctx, ctx-&gt;input(\"num_replicas\", &amp;num_replicas_tensor));\n    int num_replicas = num_replicas_tensor-&gt;flat&lt;int&gt;()(0);\n\n    MetaGraphDef out_meta_graph_def;\n    if (num_replicas == 1) {\n      out_meta_graph_def = meta_graph_def;\n    } else {\n      rdag::grappler::AutoParallel auto_parallel(num_replicas);\n      auto_parallel.BuildGraph(meta_graph_def, out_meta_graph_def.mutable_graph_def());\n    }\n\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(ctx,\n        ctx-&gt;allocate_output(0, TensorShape({}), &amp;output_tensor));\n    CHECK(out_meta_graph_def.SerializeToString(&amp;output_tensor-&gt;scalar&lt;string&gt;()()));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"InGraphAutoParallel\").Device(DEVICE_CPU), InGraphAutoParallelOp);\n\n \n</code></pre>\n<p>Below is the modification of auto_parallel code in TensorFlow.</p>\n<pre><code>#include \"tensorflow_rdag/grappler/auto_parallel.h\"\n\n#include \"tensorflow_rdag/grappler/grappler_item_builder.h\"\n#include \"tensorflow_rdag/grappler/op_types.h\"\n#include \"tensorflow_rdag/grappler/utils.h\"\n\nnamespace tensorflow {\nnamespace rdag {\nnamespace grappler {\n\nstatic const std::set&lt;std::string&gt; APPLY_GRADIENT_OPS = {\"ApplyGradientDescent\",\n                                                     \"ApplyProximalGradientDescent\",\n                                                     \"ApplyAdadelta\",\n                                                     \"ApplyAdagrad\",\n                                                     \"ApplyProximalAdagrad\",\n                                                     \"ApplyAdagradDA\",\n                                                     \"ApplyFtrl\",\n                                                     \"ApplyMomentum\",\n                                                     \"ApplyAdam\",\n                                                     \"ApplyRMSProp\",\n                                                     \"ApplyCenteredRMSProp\"};\nstatic std::map&lt;std::string, int&gt; GRADIENT_POS = {{\"ApplyGradientDescent\", 2},\n                                             {\"ApplyProximalGradientDescent\", 4},\n                                             {\"ApplyAdadelta\", 6},\n                                             {\"ApplyAdagrad\", 3},\n                                             {\"ApplyProximalAdagrad\", 5},\n                                             {\"ApplyAdagradDA\", 3},\n                                             {\"ApplyFtrl\", 3},\n                                             {\"ApplyMomentum\", 3},\n                                             {\"ApplyAdam\", 9},\n                                             {\"ApplyRMSProp\", 7},\n                                             {\"ApplyCenteredRMSProp\", 8}};\nconst char kAutoParallelPrefix[] = \"AutoParallel\";\n\nstd::string AutoParallel::GetGradientNodeName(const std::string&amp; apply_gradient_node_name) {\n  auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\n  return apply_gradients_node-&gt;input(GRADIENT_POS[apply_gradients_node-&gt;op()]);\n}\n\nStatus AutoParallel::Initialize(const GrapplerItem&amp; item) {\n  LOG(INFO) &lt;&lt; \"Number of replicas: \" &lt;&lt; num_replicas_;\n  item_ = &amp;item;\n  graph_ = item.graph;\n  LOG(INFO) &lt;&lt; \"Original graph size: \" &lt;&lt; item.graph.node_size();\n  if (item.fetch.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\n  }\n\n  if (item.MainVariables().empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\n  }\n\n  for (const auto&amp; init : item.init_ops) {\n    VLOG(1) &lt;&lt; \"Init node: \" &lt;&lt; init;\n  }\n\n  for (const auto&amp; fetch : item.fetch) {\n    VLOG(1) &lt;&lt; \"Fetch node: \" &lt;&lt; fetch;\n  }\n\n  for (const auto&amp; var : item.MainVariables()) {\n    VLOG(2) &lt;&lt; \"Variable: \" &lt;&lt; var-&gt;name();\n  }\n\n  for (QueueRunnerDef def : item.queue_runners) {\n    queue_runners_.insert(std::make_pair(def.queue_name(), def));\n  }\n\n  for (VariableDef def : item.variables) {\n    variables_.insert(std::make_pair(def.variable_name(), def));\n  }\n\n  std::vector&lt;std::string&gt; queue_nodes;\n  for (int i = 0; i &lt; graph_.node_size(); i++) {\n    all_nodes_.insert(\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\n    if (APPLY_GRADIENT_OPS.find(graph_.node(i).op()) !=\n      APPLY_GRADIENT_OPS.end()) {\n      apply_gradients_nodes_.insert(graph_.node(i).name());\n      VLOG(2) &lt;&lt; \"Apply gradients node: \" &lt;&lt; graph_.node(i).name();\n    }\n  }\n\n  std::set&lt;std::string&gt; dont_replicate_nodes;\n  for (const auto&amp; variable : item.MainVariables()) {\n    dont_replicate_nodes.insert(variable-&gt;name());\n    VariableDef def = variables_[variable-&gt;name()];\n    dont_replicate_nodes.insert(def.initializer_name());\n    dont_replicate_nodes.insert(def.snapshot_name());\n  }\n\n  std::vector&lt;std::string&gt; gradient_nodes;\n  for (const auto&amp; apply_gradient_node_name : apply_gradients_nodes_) {\n    gradient_nodes.push_back(GetGradientNodeName(apply_gradient_node_name));\n  }\n\n  auto train_nodes = ComputeTransitiveFanin(graph_, gradient_nodes);\n  LOG(INFO) &lt;&lt; \"Number of training nodes: \" &lt;&lt; train_nodes.size();\n\n  for (const auto&amp; fetch_node_name : item.fetch) {\n    dont_replicate_nodes.insert(fetch_node_name);\n  }\n\n  std::vector&lt;const NodeDef*&gt; enqueue_dequeue_nodes;\n  std::vector&lt;const NodeDef*&gt; visitied;\n  for (const auto&amp; node : train_nodes) {\n    if (dont_replicate_nodes.find(node-&gt;name()) == dont_replicate_nodes.end()) {\n      replica_nodes_.insert(node-&gt;name());\n      if (IsDequeueOp(*node)) {\n        enqueue_dequeue_nodes.push_back(node);\n      }\n    }\n  }\n  LOG(INFO) &lt;&lt; \"Number of replica nodes: \" &lt;&lt; replica_nodes_.size();\n\n  std::vector&lt;const NodeDef*&gt; input_nodes;\n  while (!enqueue_dequeue_nodes.empty()) {\n    // Pop first node in eneque_dequeue_nodes.\n    const NodeDef* enqueue_dequeue_node = *enqueue_dequeue_nodes.begin();\n    enqueue_dequeue_nodes.erase(enqueue_dequeue_nodes.begin());\n    if(std::find(visitied.begin(), visitied.end(), enqueue_dequeue_node) != visitied.end())\n      continue;\n    visitied.push_back(enqueue_dequeue_node);\n\n    auto temp_input_nodes = ComputeTransitiveFanin(graph_, {enqueue_dequeue_node-&gt;name()});\n\n    for (const NodeDef* input_node : temp_input_nodes) {\n      input_nodes.push_back(input_node);\n\n      if (IsQueueOp(*input_node)) {\n        QueueRunnerDef def = queue_runners_[input_node-&gt;name()];\n        for (int i = 0; i &lt; def.enqueue_op_name_size(); i++) {\n          const auto&amp; enqueue_op = all_nodes_[def.enqueue_op_name(i)];\n          input_nodes.push_back(enqueue_op);\n          enqueue_dequeue_nodes.push_back(enqueue_op);\n        }\n        input_nodes.push_back(all_nodes_[def.close_op_name()]);\n        input_nodes.push_back(all_nodes_[def.cancel_op_name()]);\n      } else if (IsDequeueOp(*input_node)) {\n        enqueue_dequeue_nodes.push_back(input_node);\n      }\n    }\n  }\n\n  LOG(INFO) &lt;&lt; \"Number of input nodes: \" &lt;&lt; input_nodes.size();\n\n  // Replicate all input pipeline nodes\n  for (const auto&amp; input_node : input_nodes) {\n    replica_nodes_.insert(input_node-&gt;name());\n  }\n\n  for (const auto&amp; node : all_nodes_) {\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\n      shared_nodes_.insert(node.first);\n    }\n  }\n  LOG(INFO) &lt;&lt; \"Number of shared nodes: \" &lt;&lt; shared_nodes_.size();\n  return Status::OK();\n}\n\nbool AutoParallel::NotSharedNode(const std::string&amp; name) {\n  return shared_nodes_.find(name) == shared_nodes_.end();\n}\n\nvoid AutoParallel::AddSharedNodes(GraphDef* graph) {\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", 0);\n  for (const auto&amp; node : shared_nodes_) {\n    auto new_node = graph-&gt;add_node();\n    *new_node = *all_nodes_[node];\n    for (int i = 0; i &lt; new_node-&gt;input_size(); i++) {\n      if (NotSharedNode(NodeName(new_node-&gt;input(i)))) {\n        std::string new_name = AddPrefixToNodeName(new_node-&gt;input(i), prefix);\n        *new_node-&gt;mutable_input(i) = new_name;\n      }\n    }\n  }\n}\n\nvoid AutoParallel::AddOneReplica(GraphDef* graph, int number) {\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", number);\n  for (const auto&amp; node : replica_nodes_) {\n    auto new_node = graph-&gt;add_node();\n    *new_node = *all_nodes_[node];\n    assert(new_node != all_nodes_[node]);\n    if (NotSharedNode(new_node-&gt;name())) {\n      new_node-&gt;set_name(AddPrefixToNodeName(new_node-&gt;name(), prefix));\n      if (num_replicas_ &gt; 0) {\n        // TODO : keep previous device setting except gpu device\n        // new_node-&gt;set_device(std::strings::StrCat(\"/gpu:\", number % num_replicas_));\n      }\n      for (int i = 0; i &lt; new_node-&gt;input_size(); i++) {\n        if (NotSharedNode(NodeName(new_node-&gt;input(i)))) {\n          std::string new_name = AddPrefixToNodeName(new_node-&gt;input(i), prefix);\n          *new_node-&gt;mutable_input(i) = new_name;\n        }\n      }\n    }\n  }\n}\n\nNodeDef* AutoParallel::AddNodeDivConst(GraphDef* graph) {\n  NodeDef* node = graph-&gt;add_node();\n  node-&gt;set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-Const\"));\n  node-&gt;set_op(\"Const\");\n\n  AttrValue attr_data_type;\n  attr_data_type.set_type(DT_FLOAT);\n  node-&gt;mutable_attr()-&gt;insert({\"dtype\", attr_data_type});\n\n  AttrValue attr_tensor;\n  auto tensor = attr_tensor.mutable_tensor();\n  tensor-&gt;add_float_val(static_cast&lt;float&gt;(num_replicas_));\n  tensor-&gt;set_dtype(DT_FLOAT);\n  node-&gt;mutable_attr()-&gt;insert({\"value\", attr_tensor});\n  return node;\n}\n\nNodeDef* AutoParallel::AddNodeDiv(GraphDef* graph, const std::string&amp; name, const std::string&amp; input_a,\n                                  const std::string&amp; input_b) {\n  NodeDef* node = graph-&gt;add_node();\n  node-&gt;set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-\", name));\n  node-&gt;set_op(\"RealDiv\");\n  node-&gt;add_input(input_a);\n  node-&gt;add_input(input_b);\n  AttrValue attr_type;\n  attr_type.set_type(DT_FLOAT);\n  node-&gt;mutable_attr()-&gt;insert({\"T\", attr_type});\n  return node;\n}\n\nvoid AutoParallel::AddApplyGradientDescent(GraphDef* graph) {\n  std::map&lt;std::string, NodeDef*&gt; apply_gradients_nodes;\n  for (int i = 0; i &lt; graph-&gt;node_size(); i++) {\n    if (APPLY_GRADIENT_OPS.find(graph-&gt;node(i).op()) !=\n        APPLY_GRADIENT_OPS.end()) {\n        apply_gradients_nodes.insert(\n          std::make_pair(graph-&gt;node(i).name(), graph-&gt;mutable_node(i)));\n    }\n  }\n\n  auto div_const_node = AddNodeDivConst(graph);\n\n  for (const auto&amp; apply_gradient_node_name : apply_gradients_nodes_) {\n    auto apply_gradients_node = apply_gradients_nodes[apply_gradient_node_name];\n\n    // Add all gradients\n    NodeDef* add_node = graph-&gt;add_node();\n    add_node-&gt;set_name(strings::StrCat(kAutoParallelPrefix, \"-Add-\", apply_gradient_node_name));\n    add_node-&gt;set_op(\"AddN\");\n    const auto&amp; input_name = GetGradientNodeName(apply_gradient_node_name);\n    for (int i = 0; i &lt; num_replicas_; i++) {\n      add_node-&gt;add_input(strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name));\n      LOG(INFO) &lt;&lt; \"Gradient Node Name: \" &lt;&lt; strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name);\n    }\n    AttrValue attr_type;\n    attr_type.set_type(DT_FLOAT);\n    add_node-&gt;mutable_attr()-&gt;insert({\"T\", attr_type});\n    AttrValue attr_type2;\n    attr_type2.set_type(DT_INT32);\n    attr_type2.set_i(num_replicas_);\n    add_node-&gt;mutable_attr()-&gt;insert({\"N\", attr_type2});\n\n    // Divide by number of GPUs\n    auto div_node = AddNodeDiv(\n        graph,\n        apply_gradient_node_name,\n        add_node-&gt;name(),\n        div_const_node-&gt;name());\n\n    LOG(INFO) &lt;&lt; \"Change gradient node as : \" &lt;&lt; div_node-&gt;name();\n    *apply_gradients_node-&gt;mutable_input(GRADIENT_POS[apply_gradients_node-&gt;op()]) =\n        div_node-&gt;name();\n  }\n}\n\nvoid AutoParallel::BuildGraph(const MetaGraphDef&amp; meta_graph, GraphDef* graph) {\n  ItemConfig cfg;\n  std::unique_ptr&lt;GrapplerItem&gt; grappler_item = GrapplerItemFromMetaGraphDef(\n      \"graph_to_optimize\", meta_graph, cfg);\n  Initialize(*grappler_item);\n\n  //GraphDef* out_graph_def = out_graph.mutable_graph_def();\n  AddSharedNodes(graph);\n  for (int i = 0; i &lt; num_replicas_; i++) {\n    AddOneReplica(graph, i);\n  }\n  AddApplyGradientDescent(graph);\n  //\n  *(graph-&gt;mutable_library()) = meta_graph.graph_def().library();\n  // LOG(INFO) &lt;&lt; \"Parallelized graph size: \" &lt;&lt; out_graph_def-&gt;node_size();\n\n  // SetMetaGraphForQueueRunners(out_graph, replicated_queues);\n  // SetMetaGraphForVariables(out_graph, variables);\n  //return graph;\n}\n\n}  // end namespace grappler\n}  // end namespace rdag\n}  // end namespace tensorflow\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 14.04\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\ncommit 3bee923\nBazel version (if compiling from source):\n0.4.5\nCUDA/cuDNN version:\ncuda 8.0/cudnn 5.1.5\nGPU model and memory:\nTesla P40\nExact command to reproduce:\n\nDescribe the problem\nI created a custom operation, it works well with bazel test  --run_under=valgrind. However, when I uses the custom operation using python api, memory violation is happend. Even though no modification of the input tensor, input tensor error comes out.\nSource code / logs\nnamespace tensorflow {\n\nclass InGraphAutoParallelOp : public OpKernel {\n public:\n  explicit InGraphAutoParallelOp(OpKernelConstruction* context) : OpKernel(context) {}\n\n  void Compute(OpKernelContext* ctx) override {\n    // Get graph definition\n    const Tensor* meta_graph_proto_tensor;\n    OP_REQUIRES_OK(ctx, ctx->input(\"meta_graph_def_str\", &meta_graph_proto_tensor));\n    MetaGraphDef meta_graph_def;\n    meta_graph_def.ParseFromString(meta_graph_proto_tensor->scalar<string>()());\n\n    // Get num replicas\n    const Tensor* num_replicas_tensor;\n    OP_REQUIRES_OK(ctx, ctx->input(\"num_replicas\", &num_replicas_tensor));\n    int num_replicas = num_replicas_tensor->flat<int>()(0);\n\n    MetaGraphDef out_meta_graph_def;\n    if (num_replicas == 1) {\n      out_meta_graph_def = meta_graph_def;\n    } else {\n      rdag::grappler::AutoParallel auto_parallel(num_replicas);\n      auto_parallel.BuildGraph(meta_graph_def, out_meta_graph_def.mutable_graph_def());\n    }\n\n    Tensor* output_tensor = NULL;\n    OP_REQUIRES_OK(ctx,\n        ctx->allocate_output(0, TensorShape({}), &output_tensor));\n    CHECK(out_meta_graph_def.SerializeToString(&output_tensor->scalar<string>()()));\n  }\n};\n\nREGISTER_KERNEL_BUILDER(Name(\"InGraphAutoParallel\").Device(DEVICE_CPU), InGraphAutoParallelOp);\n\n \n\nBelow is the modification of auto_parallel code in TensorFlow.\n#include \"tensorflow_rdag/grappler/auto_parallel.h\"\n\n#include \"tensorflow_rdag/grappler/grappler_item_builder.h\"\n#include \"tensorflow_rdag/grappler/op_types.h\"\n#include \"tensorflow_rdag/grappler/utils.h\"\n\nnamespace tensorflow {\nnamespace rdag {\nnamespace grappler {\n\nstatic const std::set<std::string> APPLY_GRADIENT_OPS = {\"ApplyGradientDescent\",\n                                                     \"ApplyProximalGradientDescent\",\n                                                     \"ApplyAdadelta\",\n                                                     \"ApplyAdagrad\",\n                                                     \"ApplyProximalAdagrad\",\n                                                     \"ApplyAdagradDA\",\n                                                     \"ApplyFtrl\",\n                                                     \"ApplyMomentum\",\n                                                     \"ApplyAdam\",\n                                                     \"ApplyRMSProp\",\n                                                     \"ApplyCenteredRMSProp\"};\nstatic std::map<std::string, int> GRADIENT_POS = {{\"ApplyGradientDescent\", 2},\n                                             {\"ApplyProximalGradientDescent\", 4},\n                                             {\"ApplyAdadelta\", 6},\n                                             {\"ApplyAdagrad\", 3},\n                                             {\"ApplyProximalAdagrad\", 5},\n                                             {\"ApplyAdagradDA\", 3},\n                                             {\"ApplyFtrl\", 3},\n                                             {\"ApplyMomentum\", 3},\n                                             {\"ApplyAdam\", 9},\n                                             {\"ApplyRMSProp\", 7},\n                                             {\"ApplyCenteredRMSProp\", 8}};\nconst char kAutoParallelPrefix[] = \"AutoParallel\";\n\nstd::string AutoParallel::GetGradientNodeName(const std::string& apply_gradient_node_name) {\n  auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\n  return apply_gradients_node->input(GRADIENT_POS[apply_gradients_node->op()]);\n}\n\nStatus AutoParallel::Initialize(const GrapplerItem& item) {\n  LOG(INFO) << \"Number of replicas: \" << num_replicas_;\n  item_ = &item;\n  graph_ = item.graph;\n  LOG(INFO) << \"Original graph size: \" << item.graph.node_size();\n  if (item.fetch.empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\n  }\n\n  if (item.MainVariables().empty()) {\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\n  }\n\n  for (const auto& init : item.init_ops) {\n    VLOG(1) << \"Init node: \" << init;\n  }\n\n  for (const auto& fetch : item.fetch) {\n    VLOG(1) << \"Fetch node: \" << fetch;\n  }\n\n  for (const auto& var : item.MainVariables()) {\n    VLOG(2) << \"Variable: \" << var->name();\n  }\n\n  for (QueueRunnerDef def : item.queue_runners) {\n    queue_runners_.insert(std::make_pair(def.queue_name(), def));\n  }\n\n  for (VariableDef def : item.variables) {\n    variables_.insert(std::make_pair(def.variable_name(), def));\n  }\n\n  std::vector<std::string> queue_nodes;\n  for (int i = 0; i < graph_.node_size(); i++) {\n    all_nodes_.insert(\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\n    if (APPLY_GRADIENT_OPS.find(graph_.node(i).op()) !=\n      APPLY_GRADIENT_OPS.end()) {\n      apply_gradients_nodes_.insert(graph_.node(i).name());\n      VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();\n    }\n  }\n\n  std::set<std::string> dont_replicate_nodes;\n  for (const auto& variable : item.MainVariables()) {\n    dont_replicate_nodes.insert(variable->name());\n    VariableDef def = variables_[variable->name()];\n    dont_replicate_nodes.insert(def.initializer_name());\n    dont_replicate_nodes.insert(def.snapshot_name());\n  }\n\n  std::vector<std::string> gradient_nodes;\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\n    gradient_nodes.push_back(GetGradientNodeName(apply_gradient_node_name));\n  }\n\n  auto train_nodes = ComputeTransitiveFanin(graph_, gradient_nodes);\n  LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\n\n  for (const auto& fetch_node_name : item.fetch) {\n    dont_replicate_nodes.insert(fetch_node_name);\n  }\n\n  std::vector<const NodeDef*> enqueue_dequeue_nodes;\n  std::vector<const NodeDef*> visitied;\n  for (const auto& node : train_nodes) {\n    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {\n      replica_nodes_.insert(node->name());\n      if (IsDequeueOp(*node)) {\n        enqueue_dequeue_nodes.push_back(node);\n      }\n    }\n  }\n  LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();\n\n  std::vector<const NodeDef*> input_nodes;\n  while (!enqueue_dequeue_nodes.empty()) {\n    // Pop first node in eneque_dequeue_nodes.\n    const NodeDef* enqueue_dequeue_node = *enqueue_dequeue_nodes.begin();\n    enqueue_dequeue_nodes.erase(enqueue_dequeue_nodes.begin());\n    if(std::find(visitied.begin(), visitied.end(), enqueue_dequeue_node) != visitied.end())\n      continue;\n    visitied.push_back(enqueue_dequeue_node);\n\n    auto temp_input_nodes = ComputeTransitiveFanin(graph_, {enqueue_dequeue_node->name()});\n\n    for (const NodeDef* input_node : temp_input_nodes) {\n      input_nodes.push_back(input_node);\n\n      if (IsQueueOp(*input_node)) {\n        QueueRunnerDef def = queue_runners_[input_node->name()];\n        for (int i = 0; i < def.enqueue_op_name_size(); i++) {\n          const auto& enqueue_op = all_nodes_[def.enqueue_op_name(i)];\n          input_nodes.push_back(enqueue_op);\n          enqueue_dequeue_nodes.push_back(enqueue_op);\n        }\n        input_nodes.push_back(all_nodes_[def.close_op_name()]);\n        input_nodes.push_back(all_nodes_[def.cancel_op_name()]);\n      } else if (IsDequeueOp(*input_node)) {\n        enqueue_dequeue_nodes.push_back(input_node);\n      }\n    }\n  }\n\n  LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();\n\n  // Replicate all input pipeline nodes\n  for (const auto& input_node : input_nodes) {\n    replica_nodes_.insert(input_node->name());\n  }\n\n  for (const auto& node : all_nodes_) {\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\n      shared_nodes_.insert(node.first);\n    }\n  }\n  LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();\n  return Status::OK();\n}\n\nbool AutoParallel::NotSharedNode(const std::string& name) {\n  return shared_nodes_.find(name) == shared_nodes_.end();\n}\n\nvoid AutoParallel::AddSharedNodes(GraphDef* graph) {\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", 0);\n  for (const auto& node : shared_nodes_) {\n    auto new_node = graph->add_node();\n    *new_node = *all_nodes_[node];\n    for (int i = 0; i < new_node->input_size(); i++) {\n      if (NotSharedNode(NodeName(new_node->input(i)))) {\n        std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);\n        *new_node->mutable_input(i) = new_name;\n      }\n    }\n  }\n}\n\nvoid AutoParallel::AddOneReplica(GraphDef* graph, int number) {\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", number);\n  for (const auto& node : replica_nodes_) {\n    auto new_node = graph->add_node();\n    *new_node = *all_nodes_[node];\n    assert(new_node != all_nodes_[node]);\n    if (NotSharedNode(new_node->name())) {\n      new_node->set_name(AddPrefixToNodeName(new_node->name(), prefix));\n      if (num_replicas_ > 0) {\n        // TODO : keep previous device setting except gpu device\n        // new_node->set_device(std::strings::StrCat(\"/gpu:\", number % num_replicas_));\n      }\n      for (int i = 0; i < new_node->input_size(); i++) {\n        if (NotSharedNode(NodeName(new_node->input(i)))) {\n          std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);\n          *new_node->mutable_input(i) = new_name;\n        }\n      }\n    }\n  }\n}\n\nNodeDef* AutoParallel::AddNodeDivConst(GraphDef* graph) {\n  NodeDef* node = graph->add_node();\n  node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-Const\"));\n  node->set_op(\"Const\");\n\n  AttrValue attr_data_type;\n  attr_data_type.set_type(DT_FLOAT);\n  node->mutable_attr()->insert({\"dtype\", attr_data_type});\n\n  AttrValue attr_tensor;\n  auto tensor = attr_tensor.mutable_tensor();\n  tensor->add_float_val(static_cast<float>(num_replicas_));\n  tensor->set_dtype(DT_FLOAT);\n  node->mutable_attr()->insert({\"value\", attr_tensor});\n  return node;\n}\n\nNodeDef* AutoParallel::AddNodeDiv(GraphDef* graph, const std::string& name, const std::string& input_a,\n                                  const std::string& input_b) {\n  NodeDef* node = graph->add_node();\n  node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-\", name));\n  node->set_op(\"RealDiv\");\n  node->add_input(input_a);\n  node->add_input(input_b);\n  AttrValue attr_type;\n  attr_type.set_type(DT_FLOAT);\n  node->mutable_attr()->insert({\"T\", attr_type});\n  return node;\n}\n\nvoid AutoParallel::AddApplyGradientDescent(GraphDef* graph) {\n  std::map<std::string, NodeDef*> apply_gradients_nodes;\n  for (int i = 0; i < graph->node_size(); i++) {\n    if (APPLY_GRADIENT_OPS.find(graph->node(i).op()) !=\n        APPLY_GRADIENT_OPS.end()) {\n        apply_gradients_nodes.insert(\n          std::make_pair(graph->node(i).name(), graph->mutable_node(i)));\n    }\n  }\n\n  auto div_const_node = AddNodeDivConst(graph);\n\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\n    auto apply_gradients_node = apply_gradients_nodes[apply_gradient_node_name];\n\n    // Add all gradients\n    NodeDef* add_node = graph->add_node();\n    add_node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Add-\", apply_gradient_node_name));\n    add_node->set_op(\"AddN\");\n    const auto& input_name = GetGradientNodeName(apply_gradient_node_name);\n    for (int i = 0; i < num_replicas_; i++) {\n      add_node->add_input(strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name));\n      LOG(INFO) << \"Gradient Node Name: \" << strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name);\n    }\n    AttrValue attr_type;\n    attr_type.set_type(DT_FLOAT);\n    add_node->mutable_attr()->insert({\"T\", attr_type});\n    AttrValue attr_type2;\n    attr_type2.set_type(DT_INT32);\n    attr_type2.set_i(num_replicas_);\n    add_node->mutable_attr()->insert({\"N\", attr_type2});\n\n    // Divide by number of GPUs\n    auto div_node = AddNodeDiv(\n        graph,\n        apply_gradient_node_name,\n        add_node->name(),\n        div_const_node->name());\n\n    LOG(INFO) << \"Change gradient node as : \" << div_node->name();\n    *apply_gradients_node->mutable_input(GRADIENT_POS[apply_gradients_node->op()]) =\n        div_node->name();\n  }\n}\n\nvoid AutoParallel::BuildGraph(const MetaGraphDef& meta_graph, GraphDef* graph) {\n  ItemConfig cfg;\n  std::unique_ptr<GrapplerItem> grappler_item = GrapplerItemFromMetaGraphDef(\n      \"graph_to_optimize\", meta_graph, cfg);\n  Initialize(*grappler_item);\n\n  //GraphDef* out_graph_def = out_graph.mutable_graph_def();\n  AddSharedNodes(graph);\n  for (int i = 0; i < num_replicas_; i++) {\n    AddOneReplica(graph, i);\n  }\n  AddApplyGradientDescent(graph);\n  //\n  *(graph->mutable_library()) = meta_graph.graph_def().library();\n  // LOG(INFO) << \"Parallelized graph size: \" << out_graph_def->node_size();\n\n  // SetMetaGraphForQueueRunners(out_graph, replicated_queues);\n  // SetMetaGraphForVariables(out_graph, variables);\n  //return graph;\n}\n\n}  // end namespace grappler\n}  // end namespace rdag\n}  // end namespace tensorflow", "body": "-----------------------\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\ncommit 3bee923c9\r\n- **Bazel version (if compiling from source)**:\r\n0.4.5\r\n- **CUDA/cuDNN version**:\r\ncuda 8.0/cudnn 5.1.5\r\n- **GPU model and memory**:\r\nTesla P40 \r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI created a custom operation, it works well with bazel test  --run_under=valgrind. However, when I uses the custom operation using python api, memory violation is happend. Even though no modification of the input tensor, input tensor error comes out.\r\n\r\n### Source code / logs\r\n```\r\nnamespace tensorflow {\r\n\r\nclass InGraphAutoParallelOp : public OpKernel {\r\n public:\r\n  explicit InGraphAutoParallelOp(OpKernelConstruction* context) : OpKernel(context) {}\r\n\r\n  void Compute(OpKernelContext* ctx) override {\r\n    // Get graph definition\r\n    const Tensor* meta_graph_proto_tensor;\r\n    OP_REQUIRES_OK(ctx, ctx->input(\"meta_graph_def_str\", &meta_graph_proto_tensor));\r\n    MetaGraphDef meta_graph_def;\r\n    meta_graph_def.ParseFromString(meta_graph_proto_tensor->scalar<string>()());\r\n\r\n    // Get num replicas\r\n    const Tensor* num_replicas_tensor;\r\n    OP_REQUIRES_OK(ctx, ctx->input(\"num_replicas\", &num_replicas_tensor));\r\n    int num_replicas = num_replicas_tensor->flat<int>()(0);\r\n\r\n    MetaGraphDef out_meta_graph_def;\r\n    if (num_replicas == 1) {\r\n      out_meta_graph_def = meta_graph_def;\r\n    } else {\r\n      rdag::grappler::AutoParallel auto_parallel(num_replicas);\r\n      auto_parallel.BuildGraph(meta_graph_def, out_meta_graph_def.mutable_graph_def());\r\n    }\r\n\r\n    Tensor* output_tensor = NULL;\r\n    OP_REQUIRES_OK(ctx,\r\n        ctx->allocate_output(0, TensorShape({}), &output_tensor));\r\n    CHECK(out_meta_graph_def.SerializeToString(&output_tensor->scalar<string>()()));\r\n  }\r\n};\r\n\r\nREGISTER_KERNEL_BUILDER(Name(\"InGraphAutoParallel\").Device(DEVICE_CPU), InGraphAutoParallelOp);\r\n\r\n \r\n```\r\n\r\nBelow is the modification of auto_parallel code in TensorFlow.\r\n\r\n```\r\n#include \"tensorflow_rdag/grappler/auto_parallel.h\"\r\n\r\n#include \"tensorflow_rdag/grappler/grappler_item_builder.h\"\r\n#include \"tensorflow_rdag/grappler/op_types.h\"\r\n#include \"tensorflow_rdag/grappler/utils.h\"\r\n\r\nnamespace tensorflow {\r\nnamespace rdag {\r\nnamespace grappler {\r\n\r\nstatic const std::set<std::string> APPLY_GRADIENT_OPS = {\"ApplyGradientDescent\",\r\n                                                     \"ApplyProximalGradientDescent\",\r\n                                                     \"ApplyAdadelta\",\r\n                                                     \"ApplyAdagrad\",\r\n                                                     \"ApplyProximalAdagrad\",\r\n                                                     \"ApplyAdagradDA\",\r\n                                                     \"ApplyFtrl\",\r\n                                                     \"ApplyMomentum\",\r\n                                                     \"ApplyAdam\",\r\n                                                     \"ApplyRMSProp\",\r\n                                                     \"ApplyCenteredRMSProp\"};\r\nstatic std::map<std::string, int> GRADIENT_POS = {{\"ApplyGradientDescent\", 2},\r\n                                             {\"ApplyProximalGradientDescent\", 4},\r\n                                             {\"ApplyAdadelta\", 6},\r\n                                             {\"ApplyAdagrad\", 3},\r\n                                             {\"ApplyProximalAdagrad\", 5},\r\n                                             {\"ApplyAdagradDA\", 3},\r\n                                             {\"ApplyFtrl\", 3},\r\n                                             {\"ApplyMomentum\", 3},\r\n                                             {\"ApplyAdam\", 9},\r\n                                             {\"ApplyRMSProp\", 7},\r\n                                             {\"ApplyCenteredRMSProp\", 8}};\r\nconst char kAutoParallelPrefix[] = \"AutoParallel\";\r\n\r\nstd::string AutoParallel::GetGradientNodeName(const std::string& apply_gradient_node_name) {\r\n  auto apply_gradients_node = all_nodes_[apply_gradient_node_name];\r\n  return apply_gradients_node->input(GRADIENT_POS[apply_gradients_node->op()]);\r\n}\r\n\r\nStatus AutoParallel::Initialize(const GrapplerItem& item) {\r\n  LOG(INFO) << \"Number of replicas: \" << num_replicas_;\r\n  item_ = &item;\r\n  graph_ = item.graph;\r\n  LOG(INFO) << \"Original graph size: \" << item.graph.node_size();\r\n  if (item.fetch.empty()) {\r\n    return Status(error::INVALID_ARGUMENT, \"No fetch nodes provided.\");\r\n  }\r\n\r\n  if (item.MainVariables().empty()) {\r\n    return Status(error::INVALID_ARGUMENT, \"No variables provided.\");\r\n  }\r\n\r\n  for (const auto& init : item.init_ops) {\r\n    VLOG(1) << \"Init node: \" << init;\r\n  }\r\n\r\n  for (const auto& fetch : item.fetch) {\r\n    VLOG(1) << \"Fetch node: \" << fetch;\r\n  }\r\n\r\n  for (const auto& var : item.MainVariables()) {\r\n    VLOG(2) << \"Variable: \" << var->name();\r\n  }\r\n\r\n  for (QueueRunnerDef def : item.queue_runners) {\r\n    queue_runners_.insert(std::make_pair(def.queue_name(), def));\r\n  }\r\n\r\n  for (VariableDef def : item.variables) {\r\n    variables_.insert(std::make_pair(def.variable_name(), def));\r\n  }\r\n\r\n  std::vector<std::string> queue_nodes;\r\n  for (int i = 0; i < graph_.node_size(); i++) {\r\n    all_nodes_.insert(\r\n        std::make_pair(graph_.node(i).name(), graph_.mutable_node(i)));\r\n    if (APPLY_GRADIENT_OPS.find(graph_.node(i).op()) !=\r\n      APPLY_GRADIENT_OPS.end()) {\r\n      apply_gradients_nodes_.insert(graph_.node(i).name());\r\n      VLOG(2) << \"Apply gradients node: \" << graph_.node(i).name();\r\n    }\r\n  }\r\n\r\n  std::set<std::string> dont_replicate_nodes;\r\n  for (const auto& variable : item.MainVariables()) {\r\n    dont_replicate_nodes.insert(variable->name());\r\n    VariableDef def = variables_[variable->name()];\r\n    dont_replicate_nodes.insert(def.initializer_name());\r\n    dont_replicate_nodes.insert(def.snapshot_name());\r\n  }\r\n\r\n  std::vector<std::string> gradient_nodes;\r\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\r\n    gradient_nodes.push_back(GetGradientNodeName(apply_gradient_node_name));\r\n  }\r\n\r\n  auto train_nodes = ComputeTransitiveFanin(graph_, gradient_nodes);\r\n  LOG(INFO) << \"Number of training nodes: \" << train_nodes.size();\r\n\r\n  for (const auto& fetch_node_name : item.fetch) {\r\n    dont_replicate_nodes.insert(fetch_node_name);\r\n  }\r\n\r\n  std::vector<const NodeDef*> enqueue_dequeue_nodes;\r\n  std::vector<const NodeDef*> visitied;\r\n  for (const auto& node : train_nodes) {\r\n    if (dont_replicate_nodes.find(node->name()) == dont_replicate_nodes.end()) {\r\n      replica_nodes_.insert(node->name());\r\n      if (IsDequeueOp(*node)) {\r\n        enqueue_dequeue_nodes.push_back(node);\r\n      }\r\n    }\r\n  }\r\n  LOG(INFO) << \"Number of replica nodes: \" << replica_nodes_.size();\r\n\r\n  std::vector<const NodeDef*> input_nodes;\r\n  while (!enqueue_dequeue_nodes.empty()) {\r\n    // Pop first node in eneque_dequeue_nodes.\r\n    const NodeDef* enqueue_dequeue_node = *enqueue_dequeue_nodes.begin();\r\n    enqueue_dequeue_nodes.erase(enqueue_dequeue_nodes.begin());\r\n    if(std::find(visitied.begin(), visitied.end(), enqueue_dequeue_node) != visitied.end())\r\n      continue;\r\n    visitied.push_back(enqueue_dequeue_node);\r\n\r\n    auto temp_input_nodes = ComputeTransitiveFanin(graph_, {enqueue_dequeue_node->name()});\r\n\r\n    for (const NodeDef* input_node : temp_input_nodes) {\r\n      input_nodes.push_back(input_node);\r\n\r\n      if (IsQueueOp(*input_node)) {\r\n        QueueRunnerDef def = queue_runners_[input_node->name()];\r\n        for (int i = 0; i < def.enqueue_op_name_size(); i++) {\r\n          const auto& enqueue_op = all_nodes_[def.enqueue_op_name(i)];\r\n          input_nodes.push_back(enqueue_op);\r\n          enqueue_dequeue_nodes.push_back(enqueue_op);\r\n        }\r\n        input_nodes.push_back(all_nodes_[def.close_op_name()]);\r\n        input_nodes.push_back(all_nodes_[def.cancel_op_name()]);\r\n      } else if (IsDequeueOp(*input_node)) {\r\n        enqueue_dequeue_nodes.push_back(input_node);\r\n      }\r\n    }\r\n  }\r\n\r\n  LOG(INFO) << \"Number of input nodes: \" << input_nodes.size();\r\n\r\n  // Replicate all input pipeline nodes\r\n  for (const auto& input_node : input_nodes) {\r\n    replica_nodes_.insert(input_node->name());\r\n  }\r\n\r\n  for (const auto& node : all_nodes_) {\r\n    if (replica_nodes_.find(node.first) == replica_nodes_.end()) {\r\n      shared_nodes_.insert(node.first);\r\n    }\r\n  }\r\n  LOG(INFO) << \"Number of shared nodes: \" << shared_nodes_.size();\r\n  return Status::OK();\r\n}\r\n\r\nbool AutoParallel::NotSharedNode(const std::string& name) {\r\n  return shared_nodes_.find(name) == shared_nodes_.end();\r\n}\r\n\r\nvoid AutoParallel::AddSharedNodes(GraphDef* graph) {\r\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", 0);\r\n  for (const auto& node : shared_nodes_) {\r\n    auto new_node = graph->add_node();\r\n    *new_node = *all_nodes_[node];\r\n    for (int i = 0; i < new_node->input_size(); i++) {\r\n      if (NotSharedNode(NodeName(new_node->input(i)))) {\r\n        std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);\r\n        *new_node->mutable_input(i) = new_name;\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nvoid AutoParallel::AddOneReplica(GraphDef* graph, int number) {\r\n  std::string prefix = strings::StrCat(kAutoParallelPrefix, \"-Replica-\", number);\r\n  for (const auto& node : replica_nodes_) {\r\n    auto new_node = graph->add_node();\r\n    *new_node = *all_nodes_[node];\r\n    assert(new_node != all_nodes_[node]);\r\n    if (NotSharedNode(new_node->name())) {\r\n      new_node->set_name(AddPrefixToNodeName(new_node->name(), prefix));\r\n      if (num_replicas_ > 0) {\r\n        // TODO : keep previous device setting except gpu device\r\n        // new_node->set_device(std::strings::StrCat(\"/gpu:\", number % num_replicas_));\r\n      }\r\n      for (int i = 0; i < new_node->input_size(); i++) {\r\n        if (NotSharedNode(NodeName(new_node->input(i)))) {\r\n          std::string new_name = AddPrefixToNodeName(new_node->input(i), prefix);\r\n          *new_node->mutable_input(i) = new_name;\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n\r\nNodeDef* AutoParallel::AddNodeDivConst(GraphDef* graph) {\r\n  NodeDef* node = graph->add_node();\r\n  node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-Const\"));\r\n  node->set_op(\"Const\");\r\n\r\n  AttrValue attr_data_type;\r\n  attr_data_type.set_type(DT_FLOAT);\r\n  node->mutable_attr()->insert({\"dtype\", attr_data_type});\r\n\r\n  AttrValue attr_tensor;\r\n  auto tensor = attr_tensor.mutable_tensor();\r\n  tensor->add_float_val(static_cast<float>(num_replicas_));\r\n  tensor->set_dtype(DT_FLOAT);\r\n  node->mutable_attr()->insert({\"value\", attr_tensor});\r\n  return node;\r\n}\r\n\r\nNodeDef* AutoParallel::AddNodeDiv(GraphDef* graph, const std::string& name, const std::string& input_a,\r\n                                  const std::string& input_b) {\r\n  NodeDef* node = graph->add_node();\r\n  node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Div-\", name));\r\n  node->set_op(\"RealDiv\");\r\n  node->add_input(input_a);\r\n  node->add_input(input_b);\r\n  AttrValue attr_type;\r\n  attr_type.set_type(DT_FLOAT);\r\n  node->mutable_attr()->insert({\"T\", attr_type});\r\n  return node;\r\n}\r\n\r\nvoid AutoParallel::AddApplyGradientDescent(GraphDef* graph) {\r\n  std::map<std::string, NodeDef*> apply_gradients_nodes;\r\n  for (int i = 0; i < graph->node_size(); i++) {\r\n    if (APPLY_GRADIENT_OPS.find(graph->node(i).op()) !=\r\n        APPLY_GRADIENT_OPS.end()) {\r\n        apply_gradients_nodes.insert(\r\n          std::make_pair(graph->node(i).name(), graph->mutable_node(i)));\r\n    }\r\n  }\r\n\r\n  auto div_const_node = AddNodeDivConst(graph);\r\n\r\n  for (const auto& apply_gradient_node_name : apply_gradients_nodes_) {\r\n    auto apply_gradients_node = apply_gradients_nodes[apply_gradient_node_name];\r\n\r\n    // Add all gradients\r\n    NodeDef* add_node = graph->add_node();\r\n    add_node->set_name(strings::StrCat(kAutoParallelPrefix, \"-Add-\", apply_gradient_node_name));\r\n    add_node->set_op(\"AddN\");\r\n    const auto& input_name = GetGradientNodeName(apply_gradient_node_name);\r\n    for (int i = 0; i < num_replicas_; i++) {\r\n      add_node->add_input(strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name));\r\n      LOG(INFO) << \"Gradient Node Name: \" << strings::StrCat(kAutoParallelPrefix, \"-Replica-\", i, \"/\", input_name);\r\n    }\r\n    AttrValue attr_type;\r\n    attr_type.set_type(DT_FLOAT);\r\n    add_node->mutable_attr()->insert({\"T\", attr_type});\r\n    AttrValue attr_type2;\r\n    attr_type2.set_type(DT_INT32);\r\n    attr_type2.set_i(num_replicas_);\r\n    add_node->mutable_attr()->insert({\"N\", attr_type2});\r\n\r\n    // Divide by number of GPUs\r\n    auto div_node = AddNodeDiv(\r\n        graph,\r\n        apply_gradient_node_name,\r\n        add_node->name(),\r\n        div_const_node->name());\r\n\r\n    LOG(INFO) << \"Change gradient node as : \" << div_node->name();\r\n    *apply_gradients_node->mutable_input(GRADIENT_POS[apply_gradients_node->op()]) =\r\n        div_node->name();\r\n  }\r\n}\r\n\r\nvoid AutoParallel::BuildGraph(const MetaGraphDef& meta_graph, GraphDef* graph) {\r\n  ItemConfig cfg;\r\n  std::unique_ptr<GrapplerItem> grappler_item = GrapplerItemFromMetaGraphDef(\r\n      \"graph_to_optimize\", meta_graph, cfg);\r\n  Initialize(*grappler_item);\r\n\r\n  //GraphDef* out_graph_def = out_graph.mutable_graph_def();\r\n  AddSharedNodes(graph);\r\n  for (int i = 0; i < num_replicas_; i++) {\r\n    AddOneReplica(graph, i);\r\n  }\r\n  AddApplyGradientDescent(graph);\r\n  //\r\n  *(graph->mutable_library()) = meta_graph.graph_def().library();\r\n  // LOG(INFO) << \"Parallelized graph size: \" << out_graph_def->node_size();\r\n\r\n  // SetMetaGraphForQueueRunners(out_graph, replicated_queues);\r\n  // SetMetaGraphForVariables(out_graph, variables);\r\n  //return graph;\r\n}\r\n\r\n}  // end namespace grappler\r\n}  // end namespace rdag\r\n}  // end namespace tensorflow\r\n\r\n```"}