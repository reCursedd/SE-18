{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10396", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10396/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10396/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10396/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10396", "id": 233179158, "node_id": "MDU6SXNzdWUyMzMxNzkxNTg=", "number": 10396, "title": "inference require the old training datasets exist", "user": {"login": "KangHsi", "id": 17063920, "node_id": "MDQ6VXNlcjE3MDYzOTIw", "avatar_url": "https://avatars1.githubusercontent.com/u/17063920?v=4", "gravatar_id": "", "url": "https://api.github.com/users/KangHsi", "html_url": "https://github.com/KangHsi", "followers_url": "https://api.github.com/users/KangHsi/followers", "following_url": "https://api.github.com/users/KangHsi/following{/other_user}", "gists_url": "https://api.github.com/users/KangHsi/gists{/gist_id}", "starred_url": "https://api.github.com/users/KangHsi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/KangHsi/subscriptions", "organizations_url": "https://api.github.com/users/KangHsi/orgs", "repos_url": "https://api.github.com/users/KangHsi/repos", "events_url": "https://api.github.com/users/KangHsi/events{/privacy}", "received_events_url": "https://api.github.com/users/KangHsi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-06-02T13:13:26Z", "updated_at": "2018-04-28T04:32:04Z", "closed_at": "2017-12-21T18:21:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu 1604</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.1</li>\n<li><strong>Bazel version (if compiling from source)</strong>: /</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/5.1</li>\n<li><strong>GPU model and memory</strong>: GTX1080/8G</li>\n</ul>\n<p><strong>Problem description</strong><br>\nThe exact problem is Inference requires training dataset<br>\nThis may because some of the stuff in import_meta_graph(). After you create the training graph and op train_input/ReaderRead would save the 'file names' as const in the graph. And then use saver to save the model. The next time you retrain your model, the import_meta_graph() would requires training dataset existing, or you may found the error below.</p>\n<p>For example, such a case:</p>\n<p>when I build my graph, I use such code to generate input tensor:</p>\n<pre><code>logging.info(\"Using batch size of \" + str(batch_size) + \" for training.\")\nwith tf.name_scope(\"train_input\"):\n  files = gfile.Glob(data_pattern)\n  if not files:\n    raise IOError(\"Unable to find training files. data_pattern='\" +\n                  data_pattern + \"'.\")\n  logging.info(\"Number of training files: %s.\", str(len(files)))\n  filename_queue = tf.train.string_input_producer(\n      files, num_epochs=num_epochs, shuffle=True)\n  training_data = [\n      reader.prepare_reader(filename_queue) for _ in range(num_readers)\n  ]\n\n  return tf.train.shuffle_batch_join(\n      training_data,\n      batch_size=batch_size,\n      capacity=batch_size * 5,\n      min_after_dequeue=batch_size,\n      allow_smaller_final_batch=True,\n      enqueue_many=True)\n</code></pre>\n<p>After trained the model , I want to inference the model then I rebuild my input tensor and recover my model like this:</p>\n<pre><code>saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\n\n   saver.restore(sess, latest_checkpoint)\n</code></pre>\n<p><strong>Everything is ok when the training datasets could be found</strong>, However, after I train the model I remove the training data,  inference.py got error like this:</p>\n<pre><code>Traceback (most recent call last):\n  File \"inference.py\", line 197, in &lt;module&gt;\n    app.run()\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"inference.py\", line 193, in main\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\n  File \"inference.py\", line 166, in inference\n    coord.join(threads)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\n    run_metadata_ptr)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\n    target_list, options, run_metadata)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: /data_tmp/train1Z.tfrecord\n\t [[Node: train_input/ReaderReadV2_5 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_5, train_input/input_producer)]]\n\t [[Node: train_input/DecodeRaw_11/_143 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_18_train_input/DecodeRaw_11\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op u'train_input/ReaderReadV2_1', defined at:\n  File \"inference.py\", line 197, in &lt;module&gt;\n    app.run()\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"inference.py\", line 193, in main\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\n  File \"inference.py\", line 122, in inference\n    saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1595, in import_meta_graph\n    **kwargs)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 499, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 308, in import_graph_def\n    op_def=op_def)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): \n/data_tmp/traintY.tfrecord\n\t [[Node: train_input/ReaderReadV2_1 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_1, train_input/input_producer)]]\n\t [[Node: train_input/DecodeRaw_2/_127 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_train_input/DecodeRaw_2\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n</code></pre>\n<p>I wonder why train.import_meta_graph() does such check? Because after restore the weights I will run the graph by feeding a new input tensor, for example:</p>\n<pre><code>video_id_batch_val, video_batch_val,num_frames_batch_val = sess.run([video_id_batch, video_batch, num_frames_batch])\n          predictions_val, = sess.run([predictions_tensor], feed_dict={input_tensor: video_batch_val, num_frames_tensor: num_frames_batch_val})\n</code></pre>\n<p>when it is inferencing, nothing concerns about the 'old input producer' in training section, however , when the 'old files for training' changed in the environment, the error arise and I have to rebuild the whole graph again instead of using import_meta_graph().  It's really confused.</p>", "body_text": "System information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 1604\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below): r1.1\nBazel version (if compiling from source): /\nCUDA/cuDNN version: 8.0/5.1\nGPU model and memory: GTX1080/8G\n\nProblem description\nThe exact problem is Inference requires training dataset\nThis may because some of the stuff in import_meta_graph(). After you create the training graph and op train_input/ReaderRead would save the 'file names' as const in the graph. And then use saver to save the model. The next time you retrain your model, the import_meta_graph() would requires training dataset existing, or you may found the error below.\nFor example, such a case:\nwhen I build my graph, I use such code to generate input tensor:\nlogging.info(\"Using batch size of \" + str(batch_size) + \" for training.\")\nwith tf.name_scope(\"train_input\"):\n  files = gfile.Glob(data_pattern)\n  if not files:\n    raise IOError(\"Unable to find training files. data_pattern='\" +\n                  data_pattern + \"'.\")\n  logging.info(\"Number of training files: %s.\", str(len(files)))\n  filename_queue = tf.train.string_input_producer(\n      files, num_epochs=num_epochs, shuffle=True)\n  training_data = [\n      reader.prepare_reader(filename_queue) for _ in range(num_readers)\n  ]\n\n  return tf.train.shuffle_batch_join(\n      training_data,\n      batch_size=batch_size,\n      capacity=batch_size * 5,\n      min_after_dequeue=batch_size,\n      allow_smaller_final_batch=True,\n      enqueue_many=True)\n\nAfter trained the model , I want to inference the model then I rebuild my input tensor and recover my model like this:\nsaver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\n\n   saver.restore(sess, latest_checkpoint)\n\nEverything is ok when the training datasets could be found, However, after I train the model I remove the training data,  inference.py got error like this:\nTraceback (most recent call last):\n  File \"inference.py\", line 197, in <module>\n    app.run()\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"inference.py\", line 193, in main\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\n  File \"inference.py\", line 166, in inference\n    coord.join(threads)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\n    run_metadata_ptr)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\n    target_list, options, run_metadata)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.NotFoundError: /data_tmp/train1Z.tfrecord\n\t [[Node: train_input/ReaderReadV2_5 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_5, train_input/input_producer)]]\n\t [[Node: train_input/DecodeRaw_11/_143 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_18_train_input/DecodeRaw_11\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op u'train_input/ReaderReadV2_1', defined at:\n  File \"inference.py\", line 197, in <module>\n    app.run()\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"inference.py\", line 193, in main\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\n  File \"inference.py\", line 122, in inference\n    saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1595, in import_meta_graph\n    **kwargs)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 499, in import_scoped_meta_graph\n    producer_op_list=producer_op_list)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 308, in import_graph_def\n    op_def=op_def)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): \n/data_tmp/traintY.tfrecord\n\t [[Node: train_input/ReaderReadV2_1 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_1, train_input/input_producer)]]\n\t [[Node: train_input/DecodeRaw_2/_127 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_train_input/DecodeRaw_2\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nI wonder why train.import_meta_graph() does such check? Because after restore the weights I will run the graph by feeding a new input tensor, for example:\nvideo_id_batch_val, video_batch_val,num_frames_batch_val = sess.run([video_id_batch, video_batch, num_frames_batch])\n          predictions_val, = sess.run([predictions_tensor], feed_dict={input_tensor: video_batch_val, num_frames_tensor: num_frames_batch_val})\n\nwhen it is inferencing, nothing concerns about the 'old input producer' in training section, however , when the 'old files for training' changed in the environment, the error arise and I have to rebuild the whole graph again instead of using import_meta_graph().  It's really confused.", "body": "### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 1604\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**: r1.1\r\n- **Bazel version (if compiling from source)**: /\r\n- **CUDA/cuDNN version**: 8.0/5.1\r\n- **GPU model and memory**: GTX1080/8G\r\n \r\n**Problem description**\r\nThe exact problem is Inference requires training dataset \r\nThis may because some of the stuff in import_meta_graph(). After you create the training graph and op train_input/ReaderRead would save the 'file names' as const in the graph. And then use saver to save the model. The next time you retrain your model, the import_meta_graph() would requires training dataset existing, or you may found the error below.\r\n\r\nFor example, such a case:\r\n\r\nwhen I build my graph, I use such code to generate input tensor:\r\n\r\n  ```\r\nlogging.info(\"Using batch size of \" + str(batch_size) + \" for training.\")\r\n  with tf.name_scope(\"train_input\"):\r\n    files = gfile.Glob(data_pattern)\r\n    if not files:\r\n      raise IOError(\"Unable to find training files. data_pattern='\" +\r\n                    data_pattern + \"'.\")\r\n    logging.info(\"Number of training files: %s.\", str(len(files)))\r\n    filename_queue = tf.train.string_input_producer(\r\n        files, num_epochs=num_epochs, shuffle=True)\r\n    training_data = [\r\n        reader.prepare_reader(filename_queue) for _ in range(num_readers)\r\n    ]\r\n\r\n    return tf.train.shuffle_batch_join(\r\n        training_data,\r\n        batch_size=batch_size,\r\n        capacity=batch_size * 5,\r\n        min_after_dequeue=batch_size,\r\n        allow_smaller_final_batch=True,\r\n        enqueue_many=True)\r\n```\r\n\r\nAfter trained the model , I want to inference the model then I rebuild my input tensor and recover my model like this:\r\n\r\n```\r\nsaver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\r\n\r\n   saver.restore(sess, latest_checkpoint)\r\n```\r\n\r\n**Everything is ok when the training datasets could be found**, However, after I train the model I remove the training data,  inference.py got error like this: \r\n```\r\nTraceback (most recent call last):\r\n  File \"inference.py\", line 197, in <module>\r\n    app.run()\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"inference.py\", line 193, in main\r\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\r\n  File \"inference.py\", line 166, in inference\r\n    coord.join(threads)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 778, in run\r\n    run_metadata_ptr)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 982, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1032, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1052, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.NotFoundError: /data_tmp/train1Z.tfrecord\r\n\t [[Node: train_input/ReaderReadV2_5 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_5, train_input/input_producer)]]\r\n\t [[Node: train_input/DecodeRaw_11/_143 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_18_train_input/DecodeRaw_11\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n\r\nCaused by op u'train_input/ReaderReadV2_1', defined at:\r\n  File \"inference.py\", line 197, in <module>\r\n    app.run()\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"inference.py\", line 193, in main\r\n    FLAGS.output_file, FLAGS.batch_size, FLAGS.top_k)\r\n  File \"inference.py\", line 122, in inference\r\n    saver = tf.train.import_meta_graph(meta_graph_location, clear_devices=True)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/training/saver.py\", line 1595, in import_meta_graph\r\n    **kwargs)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/meta_graph.py\", line 499, in import_scoped_meta_graph\r\n    producer_op_list=producer_op_list)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/importer.py\", line 308, in import_graph_def\r\n    op_def=op_def)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/opt/tiger/keras2/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nNotFoundError (see above for traceback): \r\n/data_tmp/traintY.tfrecord\r\n\t [[Node: train_input/ReaderReadV2_1 = ReaderReadV2[_device=\"/job:localhost/replica:0/task:0/cpu:0\"](train_input/TFRecordReaderV2_1, train_input/input_producer)]]\r\n\t [[Node: train_input/DecodeRaw_2/_127 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=1, tensor_name=\"edge_47_train_input/DecodeRaw_2\", tensor_type=DT_UINT8, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\r\n```\r\n\r\n \r\n\r\nI wonder why train.import_meta_graph() does such check? Because after restore the weights I will run the graph by feeding a new input tensor, for example:\r\n```\r\nvideo_id_batch_val, video_batch_val,num_frames_batch_val = sess.run([video_id_batch, video_batch, num_frames_batch])\r\n          predictions_val, = sess.run([predictions_tensor], feed_dict={input_tensor: video_batch_val, num_frames_tensor: num_frames_batch_val})\r\n```\r\nwhen it is inferencing, nothing concerns about the 'old input producer' in training section, however , when the 'old files for training' changed in the environment, the error arise and I have to rebuild the whole graph again instead of using import_meta_graph().  It's really confused.\r\n\r\n \r\n"}