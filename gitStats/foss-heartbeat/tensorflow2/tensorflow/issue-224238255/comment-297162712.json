{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/297162712", "html_url": "https://github.com/tensorflow/tensorflow/issues/9446#issuecomment-297162712", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9446", "id": 297162712, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NzE2MjcxMg==", "user": {"login": "srkunze", "id": 1389648, "node_id": "MDQ6VXNlcjEzODk2NDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1389648?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srkunze", "html_url": "https://github.com/srkunze", "followers_url": "https://api.github.com/users/srkunze/followers", "following_url": "https://api.github.com/users/srkunze/following{/other_user}", "gists_url": "https://api.github.com/users/srkunze/gists{/gist_id}", "starred_url": "https://api.github.com/users/srkunze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srkunze/subscriptions", "organizations_url": "https://api.github.com/users/srkunze/orgs", "repos_url": "https://api.github.com/users/srkunze/repos", "events_url": "https://api.github.com/users/srkunze/events{/privacy}", "received_events_url": "https://api.github.com/users/srkunze/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-25T20:57:29Z", "updated_at": "2017-04-25T20:57:29Z", "author_association": "NONE", "body_html": "<p>Here you are. It could be possible if there's something wrong with my code but when it's non-obvious from reading the docs (at least not to me).</p>\n<pre><code>from itertools import islice\n\nfrom io import BytesIO\nfrom tensorflow import float32, placeholder, Variable, zeros, matmul, reduce_mean, reduce_sum, train, \\\n    InteractiveSession, global_variables_initializer, argmax, constant, truncated_normal, reshape\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom tensorflow.contrib.rnn import LSTMStateTuple\nfrom tensorflow.python.ops.nn_ops import sparse_softmax_cross_entropy_with_logits\nfrom tensorflow.python.ops.rnn import dynamic_rnn\n\n\ndef get_one_hot(i, num_units, num_chars, on=1.0):\n    from numpy import zeros\n    one_hot = zeros([num_chars * num_units])\n    one_hot[[i+j*num_chars for j in range(num_units)]] = on\n    return one_hot\n\n\ndef gen_data(batch_size, file, max_length=None, forward_size=None):\n    forward_size = forward_size or batch_size\n    def get_one_hot_stream(file):\n        while True:\n            byte = file.read(1)\n            if not byte or max_length and file.tell() &gt; max_length:\n                file.seek(0)\n                continue\n            yield get_one_hot(ord(byte), num_units, num_chars)\n    one_hots_stream = get_one_hot_stream(file)\n    one_hots = [one_hot for one_hot in islice(one_hots_stream, 0, num_steps+batch_size+num_steps//2)]\n    while True:\n        yield [one_hots[i:num_steps+i] for i in range(batch_size)], [one_hots[i+num_steps//2:num_steps+i+num_steps//2] for i in range(batch_size)]\n        for i in range(forward_size):\n            one_hots = one_hots[1:] + [next(one_hots_stream)]\n\ndef get_one_hot_string(x):\n    return ''.join(chr(a) if chr(a) not in ['\\n', '\\t', '\\r'] else ' ' for a in x)\n\n\nnum_chars = 256\nnum_units = 100\nnum_steps = 200\n\nchars = placeholder(float32, [None, None, num_units * num_chars], 'chars')\ntargets = placeholder(float32, [None, None, num_units * num_chars], 'targets')\n\ncell = LSTMCell(num_units)\nc_state = placeholder(float32, [None, num_units], 'c_state')\nh_state = placeholder(float32, [None, num_units], 'h_state')\ninitial_state = LSTMStateTuple(c_state, h_state)\noutputs, final_state = dynamic_rnn(cell, chars, initial_state=initial_state)\n\nsoftmax_w = Variable(truncated_normal([num_units, num_chars], stddev=0.1))\nsoftmax_b = Variable(constant(0.1, shape=[num_chars]))\noutput = reshape(outputs, [-1, num_units])\nlogits = matmul(output, softmax_w) + softmax_b\nloss = sparse_softmax_cross_entropy_with_logits(logits=logits, labels=reshape(argmax(targets[:, :, 0:255], 2), [-1]))\ncost = reduce_mean(loss)\n\ntrain_step = train.AdamOptimizer(0.01).minimize(cost)\n\nsess = InteractiveSession()\nsess.run(global_variables_initializer())\n\nbatch_size = 10\nsource = gen_data(batch_size, BytesIO(b'test'))\ncurrent_state = LSTMStateTuple(c=zeros([batch_size, cell.state_size.c]).eval(), h=zeros([batch_size, cell.state_size.h]).eval())\nfor i, batch in enumerate(source):\n    feed_dict = {c_state: current_state.c, h_state: current_state.h, chars: batch[0], targets: batch[1]}\n    loss_sum, current_state, _ = sess.run([reduce_sum(loss), final_state, train_step], feed_dict=feed_dict)\n</code></pre>", "body_text": "Here you are. It could be possible if there's something wrong with my code but when it's non-obvious from reading the docs (at least not to me).\nfrom itertools import islice\n\nfrom io import BytesIO\nfrom tensorflow import float32, placeholder, Variable, zeros, matmul, reduce_mean, reduce_sum, train, \\\n    InteractiveSession, global_variables_initializer, argmax, constant, truncated_normal, reshape\nfrom tensorflow.contrib.rnn import LSTMCell\nfrom tensorflow.contrib.rnn import LSTMStateTuple\nfrom tensorflow.python.ops.nn_ops import sparse_softmax_cross_entropy_with_logits\nfrom tensorflow.python.ops.rnn import dynamic_rnn\n\n\ndef get_one_hot(i, num_units, num_chars, on=1.0):\n    from numpy import zeros\n    one_hot = zeros([num_chars * num_units])\n    one_hot[[i+j*num_chars for j in range(num_units)]] = on\n    return one_hot\n\n\ndef gen_data(batch_size, file, max_length=None, forward_size=None):\n    forward_size = forward_size or batch_size\n    def get_one_hot_stream(file):\n        while True:\n            byte = file.read(1)\n            if not byte or max_length and file.tell() > max_length:\n                file.seek(0)\n                continue\n            yield get_one_hot(ord(byte), num_units, num_chars)\n    one_hots_stream = get_one_hot_stream(file)\n    one_hots = [one_hot for one_hot in islice(one_hots_stream, 0, num_steps+batch_size+num_steps//2)]\n    while True:\n        yield [one_hots[i:num_steps+i] for i in range(batch_size)], [one_hots[i+num_steps//2:num_steps+i+num_steps//2] for i in range(batch_size)]\n        for i in range(forward_size):\n            one_hots = one_hots[1:] + [next(one_hots_stream)]\n\ndef get_one_hot_string(x):\n    return ''.join(chr(a) if chr(a) not in ['\\n', '\\t', '\\r'] else ' ' for a in x)\n\n\nnum_chars = 256\nnum_units = 100\nnum_steps = 200\n\nchars = placeholder(float32, [None, None, num_units * num_chars], 'chars')\ntargets = placeholder(float32, [None, None, num_units * num_chars], 'targets')\n\ncell = LSTMCell(num_units)\nc_state = placeholder(float32, [None, num_units], 'c_state')\nh_state = placeholder(float32, [None, num_units], 'h_state')\ninitial_state = LSTMStateTuple(c_state, h_state)\noutputs, final_state = dynamic_rnn(cell, chars, initial_state=initial_state)\n\nsoftmax_w = Variable(truncated_normal([num_units, num_chars], stddev=0.1))\nsoftmax_b = Variable(constant(0.1, shape=[num_chars]))\noutput = reshape(outputs, [-1, num_units])\nlogits = matmul(output, softmax_w) + softmax_b\nloss = sparse_softmax_cross_entropy_with_logits(logits=logits, labels=reshape(argmax(targets[:, :, 0:255], 2), [-1]))\ncost = reduce_mean(loss)\n\ntrain_step = train.AdamOptimizer(0.01).minimize(cost)\n\nsess = InteractiveSession()\nsess.run(global_variables_initializer())\n\nbatch_size = 10\nsource = gen_data(batch_size, BytesIO(b'test'))\ncurrent_state = LSTMStateTuple(c=zeros([batch_size, cell.state_size.c]).eval(), h=zeros([batch_size, cell.state_size.h]).eval())\nfor i, batch in enumerate(source):\n    feed_dict = {c_state: current_state.c, h_state: current_state.h, chars: batch[0], targets: batch[1]}\n    loss_sum, current_state, _ = sess.run([reduce_sum(loss), final_state, train_step], feed_dict=feed_dict)", "body": "Here you are. It could be possible if there's something wrong with my code but when it's non-obvious from reading the docs (at least not to me).\r\n\r\n    from itertools import islice\r\n    \r\n    from io import BytesIO\r\n    from tensorflow import float32, placeholder, Variable, zeros, matmul, reduce_mean, reduce_sum, train, \\\r\n        InteractiveSession, global_variables_initializer, argmax, constant, truncated_normal, reshape\r\n    from tensorflow.contrib.rnn import LSTMCell\r\n    from tensorflow.contrib.rnn import LSTMStateTuple\r\n    from tensorflow.python.ops.nn_ops import sparse_softmax_cross_entropy_with_logits\r\n    from tensorflow.python.ops.rnn import dynamic_rnn\r\n    \r\n    \r\n    def get_one_hot(i, num_units, num_chars, on=1.0):\r\n        from numpy import zeros\r\n        one_hot = zeros([num_chars * num_units])\r\n        one_hot[[i+j*num_chars for j in range(num_units)]] = on\r\n        return one_hot\r\n    \r\n    \r\n    def gen_data(batch_size, file, max_length=None, forward_size=None):\r\n        forward_size = forward_size or batch_size\r\n        def get_one_hot_stream(file):\r\n            while True:\r\n                byte = file.read(1)\r\n                if not byte or max_length and file.tell() > max_length:\r\n                    file.seek(0)\r\n                    continue\r\n                yield get_one_hot(ord(byte), num_units, num_chars)\r\n        one_hots_stream = get_one_hot_stream(file)\r\n        one_hots = [one_hot for one_hot in islice(one_hots_stream, 0, num_steps+batch_size+num_steps//2)]\r\n        while True:\r\n            yield [one_hots[i:num_steps+i] for i in range(batch_size)], [one_hots[i+num_steps//2:num_steps+i+num_steps//2] for i in range(batch_size)]\r\n            for i in range(forward_size):\r\n                one_hots = one_hots[1:] + [next(one_hots_stream)]\r\n    \r\n    def get_one_hot_string(x):\r\n        return ''.join(chr(a) if chr(a) not in ['\\n', '\\t', '\\r'] else ' ' for a in x)\r\n    \r\n    \r\n    num_chars = 256\r\n    num_units = 100\r\n    num_steps = 200\r\n    \r\n    chars = placeholder(float32, [None, None, num_units * num_chars], 'chars')\r\n    targets = placeholder(float32, [None, None, num_units * num_chars], 'targets')\r\n    \r\n    cell = LSTMCell(num_units)\r\n    c_state = placeholder(float32, [None, num_units], 'c_state')\r\n    h_state = placeholder(float32, [None, num_units], 'h_state')\r\n    initial_state = LSTMStateTuple(c_state, h_state)\r\n    outputs, final_state = dynamic_rnn(cell, chars, initial_state=initial_state)\r\n    \r\n    softmax_w = Variable(truncated_normal([num_units, num_chars], stddev=0.1))\r\n    softmax_b = Variable(constant(0.1, shape=[num_chars]))\r\n    output = reshape(outputs, [-1, num_units])\r\n    logits = matmul(output, softmax_w) + softmax_b\r\n    loss = sparse_softmax_cross_entropy_with_logits(logits=logits, labels=reshape(argmax(targets[:, :, 0:255], 2), [-1]))\r\n    cost = reduce_mean(loss)\r\n    \r\n    train_step = train.AdamOptimizer(0.01).minimize(cost)\r\n    \r\n    sess = InteractiveSession()\r\n    sess.run(global_variables_initializer())\r\n    \r\n    batch_size = 10\r\n    source = gen_data(batch_size, BytesIO(b'test'))\r\n    current_state = LSTMStateTuple(c=zeros([batch_size, cell.state_size.c]).eval(), h=zeros([batch_size, cell.state_size.h]).eval())\r\n    for i, batch in enumerate(source):\r\n        feed_dict = {c_state: current_state.c, h_state: current_state.h, chars: batch[0], targets: batch[1]}\r\n        loss_sum, current_state, _ = sess.run([reduce_sum(loss), final_state, train_step], feed_dict=feed_dict)\r\n    \r\n"}