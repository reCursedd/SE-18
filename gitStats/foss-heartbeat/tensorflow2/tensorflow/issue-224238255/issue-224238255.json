{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9446", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9446/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9446/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9446/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9446", "id": 224238255, "node_id": "MDU6SXNzdWUyMjQyMzgyNTU=", "number": 9446, "title": "dynamic_rnn: session.run with train_step behaves differently than train_step run alone", "user": {"login": "srkunze", "id": 1389648, "node_id": "MDQ6VXNlcjEzODk2NDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1389648?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srkunze", "html_url": "https://github.com/srkunze", "followers_url": "https://api.github.com/users/srkunze/followers", "following_url": "https://api.github.com/users/srkunze/following{/other_user}", "gists_url": "https://api.github.com/users/srkunze/gists{/gist_id}", "starred_url": "https://api.github.com/users/srkunze/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srkunze/subscriptions", "organizations_url": "https://api.github.com/users/srkunze/orgs", "repos_url": "https://api.github.com/users/srkunze/repos", "events_url": "https://api.github.com/users/srkunze/events{/privacy}", "received_events_url": "https://api.github.com/users/srkunze/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-04-25T18:54:56Z", "updated_at": "2017-05-09T15:03:46Z", "closed_at": "2017-04-25T20:10:51Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>4.4.0-72-generic <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116118515\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/93\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/93/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/93\">#93</a>-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux</li>\n<li>via pip</li>\n<li>1.0.0-65-g4763edf-dirty 1.0.1</li>\n<li>cuda-8.0 + cudnn-5.1.10</li>\n<li>GeForce GTX 760 + 1996MiB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Consider the following pieces of code:</p>\n<h4>Variant 1</h4>\n<pre><code>loop:\n    feed_dict = {c_state: current_state.c, h_state: current_state.h, ...}\n    loss_sum, current_state, _ = sess.run([reduce_sum(loss), final_state, train_step], feed_dict=feed_dict)\n</code></pre>\n<h4>Variant 2</h4>\n<pre><code>loop:\n    feed_dict = {...}\n    loss_sum, current_state = sess.run([reduce_sum(loss), final_state], feed_dict=feed_dict)\n    train_step.run(feed_dict)\n</code></pre>\n<h4>Variant 3</h4>\n<pre><code>loop:\n    feed_dict = {...}\n    loss_sum, current_state = sess.run([reduce_sum(loss), final_state], feed_dict=feed_dict)\n    sess.run([train_step], feed_dict=feed_dict)\n</code></pre>\n<p><strong>Variant 1</strong> breaks after a couple of iterations (15 in my case) by raising a \"ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[200,10,25600]\" exception whereas <strong>Variant 2</strong> and <strong>Variant 3</strong> do not.</p>\n<p>train_step is a minimize operation (optimizer doesn't matter) on a dynamic_rnn with a given initial_state and a single layer of LSTM cells.</p>\n<p>I cannot imagine the reason why <strong>Variant 1</strong> needs more and more memory whereas the other both don't. Seems like something's wrong here.</p>\n<p>SO question: <a href=\"http://stackoverflow.com/questions/43620353/resourceexhaustederror-when-using-session-run-instead-of-train-step-run-in-a-loo\" rel=\"nofollow\">http://stackoverflow.com/questions/43620353/resourceexhaustederror-when-using-session-run-instead-of-train-step-run-in-a-loo</a></p>", "body_text": "System information\n\n4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\nvia pip\n1.0.0-65-g4763edf-dirty 1.0.1\ncuda-8.0 + cudnn-5.1.10\nGeForce GTX 760 + 1996MiB\n\nDescribe the problem\nConsider the following pieces of code:\nVariant 1\nloop:\n    feed_dict = {c_state: current_state.c, h_state: current_state.h, ...}\n    loss_sum, current_state, _ = sess.run([reduce_sum(loss), final_state, train_step], feed_dict=feed_dict)\n\nVariant 2\nloop:\n    feed_dict = {...}\n    loss_sum, current_state = sess.run([reduce_sum(loss), final_state], feed_dict=feed_dict)\n    train_step.run(feed_dict)\n\nVariant 3\nloop:\n    feed_dict = {...}\n    loss_sum, current_state = sess.run([reduce_sum(loss), final_state], feed_dict=feed_dict)\n    sess.run([train_step], feed_dict=feed_dict)\n\nVariant 1 breaks after a couple of iterations (15 in my case) by raising a \"ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[200,10,25600]\" exception whereas Variant 2 and Variant 3 do not.\ntrain_step is a minimize operation (optimizer doesn't matter) on a dynamic_rnn with a given initial_state and a single layer of LSTM cells.\nI cannot imagine the reason why Variant 1 needs more and more memory whereas the other both don't. Seems like something's wrong here.\nSO question: http://stackoverflow.com/questions/43620353/resourceexhaustederror-when-using-session-run-instead-of-train-step-run-in-a-loo", "body": "### System information\r\n- 4.4.0-72-generic #93-Ubuntu SMP Fri Mar 31 14:07:41 UTC 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n- via pip\r\n- 1.0.0-65-g4763edf-dirty 1.0.1\r\n- cuda-8.0 + cudnn-5.1.10\r\n- GeForce GTX 760 + 1996MiB\r\n\r\n### Describe the problem\r\n\r\nConsider the following pieces of code:\r\n\r\n#### Variant 1\r\n    loop:\r\n        feed_dict = {c_state: current_state.c, h_state: current_state.h, ...}\r\n        loss_sum, current_state, _ = sess.run([reduce_sum(loss), final_state, train_step], feed_dict=feed_dict)\r\n\r\n#### Variant 2\r\n    loop:\r\n        feed_dict = {...}\r\n        loss_sum, current_state = sess.run([reduce_sum(loss), final_state], feed_dict=feed_dict)\r\n        train_step.run(feed_dict)\r\n\r\n#### Variant 3\r\n    loop:\r\n        feed_dict = {...}\r\n        loss_sum, current_state = sess.run([reduce_sum(loss), final_state], feed_dict=feed_dict)\r\n        sess.run([train_step], feed_dict=feed_dict)\r\n\r\n\r\n**Variant 1** breaks after a couple of iterations (15 in my case) by raising a \"ResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[200,10,25600]\" exception whereas **Variant 2** and **Variant 3** do not.\r\n\r\ntrain_step is a minimize operation (optimizer doesn't matter) on a dynamic_rnn with a given initial_state and a single layer of LSTM cells.\r\n\r\n\r\nI cannot imagine the reason why **Variant 1** needs more and more memory whereas the other both don't. Seems like something's wrong here.\r\n\r\nSO question: http://stackoverflow.com/questions/43620353/resourceexhaustederror-when-using-session-run-instead-of-train-step-run-in-a-loo"}