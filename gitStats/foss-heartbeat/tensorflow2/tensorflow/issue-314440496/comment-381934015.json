{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/381934015", "html_url": "https://github.com/tensorflow/tensorflow/issues/18537#issuecomment-381934015", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18537", "id": 381934015, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTkzNDAxNQ==", "user": {"login": "Benyuel", "id": 5361725, "node_id": "MDQ6VXNlcjUzNjE3MjU=", "avatar_url": "https://avatars3.githubusercontent.com/u/5361725?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Benyuel", "html_url": "https://github.com/Benyuel", "followers_url": "https://api.github.com/users/Benyuel/followers", "following_url": "https://api.github.com/users/Benyuel/following{/other_user}", "gists_url": "https://api.github.com/users/Benyuel/gists{/gist_id}", "starred_url": "https://api.github.com/users/Benyuel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Benyuel/subscriptions", "organizations_url": "https://api.github.com/users/Benyuel/orgs", "repos_url": "https://api.github.com/users/Benyuel/repos", "events_url": "https://api.github.com/users/Benyuel/events{/privacy}", "received_events_url": "https://api.github.com/users/Benyuel/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-17T10:11:34Z", "updated_at": "2018-04-17T10:11:34Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=4792573\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gskulkarni\">@gskulkarni</a> : Here's how you would generate on CPU but perform the matmul on GPU if you want to test:</p>\n<pre><code>with tf.device('/device:CPU:0'):\n    x = tf.random_uniform...\n    y = ...\n\nwith tf.device('/device:GPU:0'):\n    z = tf.matmul(x, y)\n\nwith tf.Session() as sess:\n    sess.run(z)\n</code></pre>\n<p>I think you may be getting the CUDA_ERROR_LAUNCH_FAILED because you ran out of memory copying <code>np.random.random</code> matrices on your GPU.  The <code>np.random.random</code> matrix is by default <code>np.float64</code> which creates a subsequent graph bigger than 2GB and errors out for me.  If I do <code>np.random.random((10000, 10000)).astype(np.float32)</code>, I can fit it in memory and compute.  Typically, it is better to use tf.float32 for speed with the price of a possible negligible loss of precision.</p>", "body_text": "@gskulkarni : Here's how you would generate on CPU but perform the matmul on GPU if you want to test:\nwith tf.device('/device:CPU:0'):\n    x = tf.random_uniform...\n    y = ...\n\nwith tf.device('/device:GPU:0'):\n    z = tf.matmul(x, y)\n\nwith tf.Session() as sess:\n    sess.run(z)\n\nI think you may be getting the CUDA_ERROR_LAUNCH_FAILED because you ran out of memory copying np.random.random matrices on your GPU.  The np.random.random matrix is by default np.float64 which creates a subsequent graph bigger than 2GB and errors out for me.  If I do np.random.random((10000, 10000)).astype(np.float32), I can fit it in memory and compute.  Typically, it is better to use tf.float32 for speed with the price of a possible negligible loss of precision.", "body": "@gskulkarni : Here's how you would generate on CPU but perform the matmul on GPU if you want to test:\r\n```\r\nwith tf.device('/device:CPU:0'):\r\n    x = tf.random_uniform...\r\n    y = ...\r\n\r\nwith tf.device('/device:GPU:0'):\r\n    z = tf.matmul(x, y)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(z)\r\n```\r\n\r\nI think you may be getting the CUDA_ERROR_LAUNCH_FAILED because you ran out of memory copying `np.random.random` matrices on your GPU.  The `np.random.random` matrix is by default `np.float64` which creates a subsequent graph bigger than 2GB and errors out for me.  If I do `np.random.random((10000, 10000)).astype(np.float32)`, I can fit it in memory and compute.  Typically, it is better to use tf.float32 for speed with the price of a possible negligible loss of precision."}