{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242884690", "html_url": "https://github.com/tensorflow/tensorflow/issues/1686#issuecomment-242884690", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1686", "id": 242884690, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mjg4NDY5MA==", "user": {"login": "jhollowayj", "id": 7515474, "node_id": "MDQ6VXNlcjc1MTU0NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7515474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhollowayj", "html_url": "https://github.com/jhollowayj", "followers_url": "https://api.github.com/users/jhollowayj/followers", "following_url": "https://api.github.com/users/jhollowayj/following{/other_user}", "gists_url": "https://api.github.com/users/jhollowayj/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhollowayj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhollowayj/subscriptions", "organizations_url": "https://api.github.com/users/jhollowayj/orgs", "repos_url": "https://api.github.com/users/jhollowayj/repos", "events_url": "https://api.github.com/users/jhollowayj/events{/privacy}", "received_events_url": "https://api.github.com/users/jhollowayj/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-27T00:47:47Z", "updated_at": "2016-08-27T00:47:47Z", "author_association": "NONE", "body_html": "<p>I've spent some time reading up on slurm and it's allocation stuff.  What I have so far is <a href=\"https://github.com/jhollowayj/tensorflow_slurm_manager/blob/master/slurm_manager.py\">here</a> (still a work in progress).</p>\n<p>I've been trying to figure out how to best support all possible slurm clusters (slurm adds environment variables, but not everyone runs the same version of slurm) as well as possible run configurations (x Nodes running y processes, each process on it's own exclusive node, etc.).</p>\n<p>Some Questions I have are:</p>\n<ul>\n<li>Do other cluster managers force each process on it's own physical node, or do multiple processes occasionally get put on the same node?\n<ul>\n<li>Note: our setup here will probably be the later as we only have a few GPU nodes.  However, I wanted to try and be consistent across other cluster managers like Kubernetes.</li>\n</ul>\n</li>\n<li>Are there any benefits to having multiple PS on the same physical node?  I've set it up to limit PS to one per node if the user has multiple processes on the same physical node.\n<ul>\n<li>Do PS use multiple cores?  If so, I feel like network bandwidth will be the bottle neck over CPU usage.  If not, I can try to write a better assigner.</li>\n</ul>\n</li>\n<li>Are there any rules for importing pip packages once the manager is included in TF?  (I used python-hostlist to help parse slurm's list of hostnames)</li>\n<li>Where would this file go in the Tensorflow repo once it's accepted?  (i.e. /tf/tf/contrib/cluster_managers/slurm.py ?)</li>\n</ul>", "body_text": "I've spent some time reading up on slurm and it's allocation stuff.  What I have so far is here (still a work in progress).\nI've been trying to figure out how to best support all possible slurm clusters (slurm adds environment variables, but not everyone runs the same version of slurm) as well as possible run configurations (x Nodes running y processes, each process on it's own exclusive node, etc.).\nSome Questions I have are:\n\nDo other cluster managers force each process on it's own physical node, or do multiple processes occasionally get put on the same node?\n\nNote: our setup here will probably be the later as we only have a few GPU nodes.  However, I wanted to try and be consistent across other cluster managers like Kubernetes.\n\n\nAre there any benefits to having multiple PS on the same physical node?  I've set it up to limit PS to one per node if the user has multiple processes on the same physical node.\n\nDo PS use multiple cores?  If so, I feel like network bandwidth will be the bottle neck over CPU usage.  If not, I can try to write a better assigner.\n\n\nAre there any rules for importing pip packages once the manager is included in TF?  (I used python-hostlist to help parse slurm's list of hostnames)\nWhere would this file go in the Tensorflow repo once it's accepted?  (i.e. /tf/tf/contrib/cluster_managers/slurm.py ?)", "body": "I've spent some time reading up on slurm and it's allocation stuff.  What I have so far is [here](https://github.com/jhollowayj/tensorflow_slurm_manager/blob/master/slurm_manager.py) (still a work in progress).  \n\nI've been trying to figure out how to best support all possible slurm clusters (slurm adds environment variables, but not everyone runs the same version of slurm) as well as possible run configurations (x Nodes running y processes, each process on it's own exclusive node, etc.). \n\nSome Questions I have are:\n- Do other cluster managers force each process on it's own physical node, or do multiple processes occasionally get put on the same node?\n  - Note: our setup here will probably be the later as we only have a few GPU nodes.  However, I wanted to try and be consistent across other cluster managers like Kubernetes.\n- Are there any benefits to having multiple PS on the same physical node?  I've set it up to limit PS to one per node if the user has multiple processes on the same physical node.\n  - Do PS use multiple cores?  If so, I feel like network bandwidth will be the bottle neck over CPU usage.  If not, I can try to write a better assigner.\n- Are there any rules for importing pip packages once the manager is included in TF?  (I used python-hostlist to help parse slurm's list of hostnames)\n- Where would this file go in the Tensorflow repo once it's accepted?  (i.e. /tf/tf/contrib/cluster_managers/slurm.py ?)\n"}