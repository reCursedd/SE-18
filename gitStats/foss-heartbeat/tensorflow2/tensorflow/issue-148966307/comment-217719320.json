{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217719320", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217719320", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002", "id": 217719320, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzcxOTMyMA==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-08T13:42:47Z", "updated_at": "2016-05-08T13:46:08Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1131892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wchan\">@wchan</a>  -- really appreciate your work here. I think its going to benefit many.</p>\n<p>Just to voice <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8100\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/kashif\">@kashif</a> concerns, would it make sense to integrate batch norm into the lstm as a flag?</p>\n<p>If the goal is to increase the network's speed to convergence, this paper shows that batch normalization can make a remarkable difference especially for multiple stacked layers.</p>\n<p><a href=\"https://arxiv.org/abs/1510.01378\" rel=\"nofollow\">https://arxiv.org/abs/1510.01378</a></p>\n<p>torch implementation here:<br>\n<a href=\"https://github.com/iassael/torch-bnlstm\">https://github.com/iassael/torch-bnlstm</a></p>", "body_text": "@wchan  -- really appreciate your work here. I think its going to benefit many.\nJust to voice @kashif concerns, would it make sense to integrate batch norm into the lstm as a flag?\nIf the goal is to increase the network's speed to convergence, this paper shows that batch normalization can make a remarkable difference especially for multiple stacked layers.\nhttps://arxiv.org/abs/1510.01378\ntorch implementation here:\nhttps://github.com/iassael/torch-bnlstm", "body": "@wchan  -- really appreciate your work here. I think its going to benefit many.\n\nJust to voice @kashif concerns, would it make sense to integrate batch norm into the lstm as a flag? \n\nIf the goal is to increase the network's speed to convergence, this paper shows that batch normalization can make a remarkable difference especially for multiple stacked layers. \n\nhttps://arxiv.org/abs/1510.01378\n\ntorch implementation here:\nhttps://github.com/iassael/torch-bnlstm\n"}