{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62399943", "pull_request_review_id": null, "id": 62399943, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzk5OTQz", "diff_hunk": "@@ -19,16 +19,90 @@\n from __future__ import print_function\n \n import math\n+import os\n+import threading\n \n+from tensorflow.contrib.rnn.ops import gen_lstm_ops\n+from tensorflow.python.framework import tensor_shape\n from tensorflow.python.framework import ops\n+from tensorflow.python.framework.load_library import load_op_library\n from tensorflow.python.ops import array_ops\n from tensorflow.python.ops import clip_ops\n+from tensorflow.python.ops import init_ops\n from tensorflow.python.ops import math_ops\n from tensorflow.python.ops import nn_ops\n from tensorflow.python.ops import rnn_cell\n from tensorflow.python.ops import variable_scope as vs\n from tensorflow.python.ops.math_ops import sigmoid\n from tensorflow.python.ops.math_ops import tanh\n+from tensorflow.python.platform import resource_loader\n+\n+\n+LSTM_OPS_FILE = \"_lstm_ops.so\"\n+\n+_lstm_ops = None\n+_ops_lock = threading.Lock()\n+\n+\n+@ops.RegisterShape(\"LSTMCellBlock\")\n+def _LSTMCellBlockShape(op):\n+  batch_size = op.inputs[0].get_shape().with_rank(2)[0].value\n+  cell_size = op.get_attr(\"cell_size\")\n+\n+  return (tensor_shape.TensorShape([batch_size, cell_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size * 2]),\n+          tensor_shape.TensorShape([batch_size, cell_size]))\n+\n+\n+@ops.RegisterGradient(\"LSTMCellBlock\")\n+def _LSTMCellBlockGrad(op, *grad):\n+  x = op.inputs[0]\n+  states_prev = op.inputs[1]\n+  w = op.inputs[2]\n+  b = op.inputs[3]\n+\n+  i = op.outputs[0]\n+  cs = op.outputs[1]\n+  f = op.outputs[2]\n+  o = op.outputs[3]\n+  ci = op.outputs[4]\n+  co = op.outputs[5]\n+  # states = op.outputs[6]\n+  h = op.outputs[7]\n+\n+  states_grad = grad[6]\n+  h_grad = grad[7]\n+\n+  parallel_dw = op.get_attr(\"parallel_dw\")\n+\n+  (x_grad, states_prev_grad, w_grad, b_grad, dicfo, xh) = gen_lstm_ops._lstm_cell_block_grad(\n+      x, states_prev, w, b, i, cs, f, o, ci, co, h, states_grad, h_grad,\n+      cell_size=op.get_attr(\"cell_size\"), bprop_dx=op.get_attr(\"bprop_dx\"),\n+      parallel_dw=parallel_dw)\n+\n+  if parallel_dw:\n+    w_grad = math_ops.matmul(xh, dicfo, transpose_a=True)\n+    b_grad = nn_ops.bias_add_grad(dicfo)\n+  return (x_grad, states_prev_grad, w_grad, b_grad)\n+\n+\n+@ops.RegisterShape(\"LSTMCellBlockGrad\")\n+def _LSTMCellBlockGradShape(op):\n+  batch_size = op.inputs[0].get_shape().with_rank(2)[0].value\n+  input_size = op.inputs[0].get_shape().with_rank(2)[1].value\n+  cell_size = op.get_attr(\"cell_size\")\n+\n+  return [tensor_shape.TensorShape([batch_size, input_size]),\n+          tensor_shape.TensorShape([batch_size, cell_size * 2]),\n+          tensor_shape.TensorShape([input_size + cell_size, cell_size * 4]),\n+          tensor_shape.TensorShape([cell_size * 4]),\n+          tensor_shape.TensorShape([batch_size, cell_size * 4]),\n+          tensor_shape.TensorShape([batch_size, input_size + cell_size])]\n \n \n def _get_concat_variable(name, shape, dtype, num_shards):", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": 73, "original_position": 90, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "727e0c76bed212fdf240a221e577e1b1a86848d5", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "body": "that code isn't mine, it was from the existing rnn_cell.py which I presume\nwas for TimeFreqLSTMCell which I presume is from Tara?\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Fri, May 6, 2016 at 3:07 PM, ebrevdo notifications@github.com wrote:\n\n> In tensorflow/contrib/rnn/python/ops/rnn_cell.py\n> https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62397359:\n> \n> > -    b_grad = nn_ops.bias_add_grad(dicfo)\n> > -  return (x_grad, states_prev_grad, w_grad, b_grad)\n> >   +\n> >   +\n> >   +@ops.RegisterShape(\"LSTMCellBlockGrad\")\n> >   +def _LSTMCellBlockGradShape(op):\n> > -  batch_size = op.inputs[0].get_shape().with_rank(2)[0].value\n> > -  input_size = op.inputs[0].get_shape().with_rank(2)[1].value\n> > -  cell_size = op.get_attr(\"cell_size\")\n> >   +\n> > -  return [tensor_shape.TensorShape([batch_size, input_size]),\n> > -          tensor_shape.TensorShape([batch_size, cell_size \\* 2]),\n> > -          tensor_shape.TensorShape([input_size + cell_size, cell_size \\* 4]),\n> > -          tensor_shape.TensorShape([cell_size \\* 4]),\n> > -          tensor_shape.TensorShape([batch_size, cell_size \\* 4]),\n> > - ```\n> >        tensor_shape.TensorShape([batch_size, input_size + cell_size])]\n> >   ```\n> >   \n> >   def _get_concat_variable(name, shape, dtype, num_shards):\n> \n> fyi see the new get_variable partitioner behavior. you no longer need to\n> worry about sharding yourself.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002/files/727e0c76bed212fdf240a221e577e1b1a86848d5#r62397359\n", "created_at": "2016-05-06T22:33:37Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62399943", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62399943"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62399943"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>that code isn't mine, it was from the existing rnn_cell.py which I presume<br>\nwas for TimeFreqLSTMCell which I presume is from Tara?</p>\n<h2></h2>\n<p>William Chan<br>\nCarnegie Mellon University<br>\n(650) 450-9455<br>\nwilliamchan.ca</p>\n<p>On Fri, May 6, 2016 at 3:07 PM, ebrevdo <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>In tensorflow/contrib/rnn/python/ops/rnn_cell.py<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148966307\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2002\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/2002/hovercard?comment_id=62397359&amp;comment_type=review_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62397359\">#2002 (comment)</a>:</p>\n<blockquote>\n<ul>\n<li>b_grad = nn_ops.bias_add_grad(dicfo)</li>\n<li>return (x_grad, states_prev_grad, w_grad, b_grad)</li>\n</ul>\n<ul>\n<li></li>\n<li></li>\n</ul>\n<p><a href=\"mailto:+@ops.RegisterShape\">+@ops.RegisterShape</a>(\"LSTMCellBlockGrad\")<br>\n+def _LSTMCellBlockGradShape(op):</p>\n<ul>\n<li>batch_size = op.inputs[0].get_shape().with_rank(2)[0].value</li>\n<li>input_size = op.inputs[0].get_shape().with_rank(2)[1].value</li>\n<li>cell_size = op.get_attr(\"cell_size\")</li>\n</ul>\n<ul>\n<li></li>\n</ul>\n<ul>\n<li>\n<p>return [tensor_shape.TensorShape([batch_size, input_size]),</p>\n</li>\n<li>\n<pre><code>     tensor_shape.TensorShape([batch_size, cell_size \\* 2]),\n</code></pre>\n</li>\n<li>\n<pre><code>     tensor_shape.TensorShape([input_size + cell_size, cell_size \\* 4]),\n</code></pre>\n</li>\n<li>\n<pre><code>     tensor_shape.TensorShape([cell_size \\* 4]),\n</code></pre>\n</li>\n<li>\n<pre><code>     tensor_shape.TensorShape([batch_size, cell_size \\* 4]),\n</code></pre>\n</li>\n<li>\n<pre><code>     tensor_shape.TensorShape([batch_size, input_size + cell_size])]\n</code></pre>\n<p>def _get_concat_variable(name, shape, dtype, num_shards):</p>\n</li>\n</ul>\n</blockquote>\n<p>fyi see the new get_variable partitioner behavior. you no longer need to<br>\nworry about sharding yourself.</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub<br>\n<a href=\"https://github.com/tensorflow/tensorflow/pull/2002/files/727e0c76bed212fdf240a221e577e1b1a86848d5#r62397359\">https://github.com/tensorflow/tensorflow/pull/2002/files/727e0c76bed212fdf240a221e577e1b1a86848d5#r62397359</a></p>\n</blockquote>", "body_text": "that code isn't mine, it was from the existing rnn_cell.py which I presume\nwas for TimeFreqLSTMCell which I presume is from Tara?\n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\nOn Fri, May 6, 2016 at 3:07 PM, ebrevdo notifications@github.com wrote:\n\nIn tensorflow/contrib/rnn/python/ops/rnn_cell.py\n#2002 (comment):\n\n\nb_grad = nn_ops.bias_add_grad(dicfo)\nreturn (x_grad, states_prev_grad, w_grad, b_grad)\n\n\n\n\n\n+@ops.RegisterShape(\"LSTMCellBlockGrad\")\n+def _LSTMCellBlockGradShape(op):\n\nbatch_size = op.inputs[0].get_shape().with_rank(2)[0].value\ninput_size = op.inputs[0].get_shape().with_rank(2)[1].value\ncell_size = op.get_attr(\"cell_size\")\n\n\n\n\n\n\nreturn [tensor_shape.TensorShape([batch_size, input_size]),\n\n\n     tensor_shape.TensorShape([batch_size, cell_size \\* 2]),\n\n\n\n     tensor_shape.TensorShape([input_size + cell_size, cell_size \\* 4]),\n\n\n\n     tensor_shape.TensorShape([cell_size \\* 4]),\n\n\n\n     tensor_shape.TensorShape([batch_size, cell_size \\* 4]),\n\n\n\n     tensor_shape.TensorShape([batch_size, input_size + cell_size])]\n\ndef _get_concat_variable(name, shape, dtype, num_shards):\n\n\n\nfyi see the new get_variable partitioner behavior. you no longer need to\nworry about sharding yourself.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/2002/files/727e0c76bed212fdf240a221e577e1b1a86848d5#r62397359"}