{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62930096", "pull_request_review_id": null, "id": 62930096, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyOTMwMDk2", "diff_hunk": "@@ -0,0 +1,310 @@\n+#ifndef TENSORFLOW_KERNELS_LSTM_OPS_H_\n+#define TENSORFLOW_KERNELS_LSTM_OPS_H_\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/lib/core/blocking_counter.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace perftools {\n+namespace gputools {\n+class Stream;\n+}  // end namespace gputools\n+}  // end namespace perftools\n+\n+namespace tensorflow {\n+class OpKernelContext;\n+\n+namespace functor {\n+\n+template <typename Device, typename T>\n+struct TensorMemZero {\n+  void operator()(const Device& d, typename TTypes<T>::Vec x) {\n+    x.device(d) = x.constant(0);\n+  }\n+};\n+\n+template <typename Device, typename T>\n+struct TensorMemCopy {\n+  void operator()(const Device& d, typename TTypes<T>::ConstVec in,\n+                  typename TTypes<T>::Vec out) {\n+    out.device(d) = in;\n+  }\n+};\n+\n+template <typename T>\n+struct TensorCuBlasGemm {\n+  void operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      bool transa, bool transb, uint64 m, uint64 n, uint64 k, T alpha,\n+      const T* a, int lda, const T* b, int ldb, T beta, T *c, int ldc);\n+};\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+struct TensorBlasGemm;\n+\n+template <typename Device, typename T>\n+struct TensorBlasGemm<Device, T, true /* USE_CUBLAS */> {\n+  static void compute(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, bool transa, bool transb, T alpha,\n+      typename TTypes<T>::ConstMatrix a, typename TTypes<T>::ConstMatrix b,\n+      T beta, typename TTypes<T>::Matrix c) {\n+    int64 m = c.dimensions()[0];\n+    int64 n = c.dimensions()[1];\n+    int64 k = transa ? a.dimensions()[0] : a.dimensions()[1];\n+\n+    TensorCuBlasGemm<T>()(\n+        ctx, stream, transb, transa, n, m, k, alpha, b.data(),\n+        transb ? k : n, a.data(), transa ? m : k, beta, c.data(), n);\n+  }\n+};\n+\n+template <typename Device, typename T>\n+struct TensorBlasGemm<Device, T, false /* USE_CUBLAS */ > {\n+  static void compute(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, bool transa, bool transb, T alpha,\n+      typename TTypes<T>::ConstMatrix a, typename TTypes<T>::ConstMatrix b,\n+      T beta, typename TTypes<T>::Matrix c) {\n+    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_pairs;\n+    contract_pairs[0] = Eigen::IndexPair<Eigen::DenseIndex>(\n+        transa == false, transb == true);\n+    if (alpha == T(1) && beta == T(0)) {\n+      c.device(d) = a.contract(b, contract_pairs);\n+    } else if (alpha == T(1) && beta == T(1)) {\n+      c.device(d) += a.contract(b, contract_pairs);\n+    } else {\n+      c.device(d) = c.constant(alpha) * a.contract(b, contract_pairs) +\n+                    c.constant(beta) * c;\n+    }\n+  }\n+};\n+\n+struct LSTMCellBlock {\n+  LSTMCellBlock(const int batch_size, const int input_size, const int cell_size)\n+    : batch_size_(batch_size), input_size_(input_size), cell_size_(cell_size) {}\n+\n+  Eigen::array<int, 2> icfo_i_offsets() const {\n+    return {0, cell_size_ * 0};\n+  }\n+\n+  Eigen::array<int, 2> icfo_c_offsets() const {\n+    return {0, cell_size_ * 1};\n+  }\n+\n+  Eigen::array<int, 2> icfo_f_offsets() const {\n+    return {0, cell_size_ * 2};\n+  }\n+\n+  Eigen::array<int, 2> icfo_o_offsets() const {\n+    return {0, cell_size_ * 3};\n+  }\n+\n+  Eigen::array<int, 2> states_cs_offsets() const {\n+    return {0, 0};\n+  }\n+\n+  Eigen::array<int, 2> states_h_offsets() const {\n+    return {0, cell_size_};\n+  }\n+\n+  Eigen::array<int, 2> cell_extents() const {\n+    return {batch_size_, cell_size_};\n+  }\n+\n+  Eigen::array<int, 2> xh_x_offsets() const {\n+    return {0, 0};\n+  }\n+\n+  Eigen::array<int, 2> xh_x_extents() const {\n+    return {batch_size_, input_size_};\n+  }\n+\n+  Eigen::array<int, 2> xh_h_offsets() const {\n+    return {0, input_size_};\n+  }\n+\n+  Eigen::array<int, 2> xh_h_extents() const {\n+    return {batch_size_, cell_size_};\n+  }\n+\n+ protected:\n+  const int batch_size_;\n+  const int input_size_;\n+  const int cell_size_;\n+};\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+struct LSTMCellBlockFprop : public LSTMCellBlock {\n+  LSTMCellBlockFprop(const int batch_size, const int input_size,\n+                     const int cell_size)\n+    : LSTMCellBlock(batch_size, input_size, cell_size) {}\n+\n+  void operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, const T forget_bias, typename TTypes<T>::ConstMatrix x,\n+      typename TTypes<T>::ConstMatrix states_prev,\n+      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec b,\n+      typename TTypes<T>::Matrix cs_prev, typename TTypes<T>::Matrix h_prev,\n+      typename TTypes<T>::Matrix xh, typename TTypes<T>::Matrix i,\n+      typename TTypes<T>::Matrix cs, typename TTypes<T>::Matrix f,\n+      typename TTypes<T>::Matrix o, typename TTypes<T>::Matrix ci,\n+      typename TTypes<T>::Matrix co, typename TTypes<T>::Matrix icfo,\n+      typename TTypes<T>::Matrix states, typename TTypes<T>::Matrix h) {\n+    // [cs, h] = states_prev\n+    cs_prev.device(d) =\n+        states_prev.slice(states_cs_offsets(), cell_extents());\n+\n+    h_prev.device(d) =\n+        states_prev.slice(states_h_offsets(), cell_extents());\n+\n+    // Concat xh = [x, h].\n+    xh.slice(xh_x_offsets(), xh_x_extents()).device(d) = x;\n+    xh.slice(xh_h_offsets(), xh_h_extents()).device(d) = h_prev;\n+\n+    // states1 = xh * w + b\n+    typename TTypes<T>::ConstMatrix const_xh(xh.data(), xh.dimensions());\n+    TensorBlasGemm<Device, T, USE_CUBLAS>::compute(\n+        ctx, stream, d, false, false, T(1), const_xh, w, T(0), icfo);\n+    icfo.device(d) +=\n+        b.broadcast(Eigen::array<int, 2>({batch_size_, 1}));\n+\n+    // Input gate.\n+    i.device(d) = icfo.slice(icfo_i_offsets(), cell_extents()).sigmoid();\n+\n+    // Cell input.\n+    ci.device(d) = icfo.slice(icfo_c_offsets(), cell_extents()).tanh();\n+\n+    // Forget gate (w/ bias).\n+    f.device(d) =\n+        (icfo.slice(icfo_f_offsets(), cell_extents()) + f.constant(forget_bias))\n+        .sigmoid();\n+\n+    // cs = ci .* i + f .* cs_prev\n+    cs.device(d) = i * ci + f * cs_prev;\n+\n+    // co = tanh(cs)\n+    co.device(d) = cs.tanh();\n+\n+    // Output gate.\n+    o.device(d) = icfo.slice(icfo_o_offsets(), cell_extents()).sigmoid();\n+\n+    // h = o .* co\n+    h.device(d) = o * co;\n+\n+    states.slice(states_cs_offsets(), cell_extents()).device(d) = cs;\n+    states.slice(states_h_offsets(), cell_extents()).device(d) = h;\n+  }\n+};\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+struct LSTMCellBlockBprop : public LSTMCellBlock {\n+  LSTMCellBlockBprop(const int batch_size, const int input_size,\n+                     const int cell_size)\n+    : LSTMCellBlock(batch_size, input_size, cell_size) {}\n+\n+  void operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, bool parallel_dw, typename TTypes<T>::ConstMatrix x,\n+      typename TTypes<T>::ConstMatrix states_prev,\n+      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec b,\n+      typename TTypes<T>::ConstMatrix i, typename TTypes<T>::ConstMatrix cs,\n+      typename TTypes<T>::ConstMatrix f, typename TTypes<T>::ConstMatrix o,\n+      typename TTypes<T>::ConstMatrix ci, typename TTypes<T>::ConstMatrix co,\n+      typename TTypes<T>::ConstMatrix h,\n+      typename TTypes<T>::ConstMatrix states_grad,\n+      typename TTypes<T>::ConstMatrix h_grad,\n+      typename TTypes<T>::Matrix cs_prev,\n+      typename TTypes<T>::Matrix states_c_grad,\n+      typename TTypes<T>::Matrix states_h_grad,\n+      typename TTypes<T>::Matrix xh, typename TTypes<T>::Matrix xh_grad,\n+      typename TTypes<T>::Matrix x_grad, typename TTypes<T>::Matrix dh,\n+      typename TTypes<T>::Matrix do_, typename TTypes<T>::Matrix dcs,\n+      typename TTypes<T>::Matrix dci, typename TTypes<T>::Matrix df,\n+      typename TTypes<T>::Matrix di, typename TTypes<T>::Matrix dicfo,\n+      typename TTypes<T>::Matrix states_prev_grad,\n+      typename TTypes<T>::Matrix w_grad, typename TTypes<T>::Vec b_grad) {\n+    // [c_grad, h_grad] = states_grad.\n+    states_c_grad.device(d) =\n+        states_grad.slice(states_cs_offsets(), cell_extents());\n+    states_h_grad.device(d) =\n+        states_grad.slice(states_h_offsets(), cell_extents());\n+\n+    // dh.\n+    dh.device(d) = h_grad + states_h_grad;\n+\n+    // do[t] = sigm'(o[t]) .* dh[t] .* co[t]\n+    do_.device(d) = o * (o.constant(T(1)) - o) * dh * co;\n+\n+    // dcs[t] += tanh'(cs[t]) .* dh[t] .* o[t] + dcs[t + 1] .* f[t + 1]\n+    dcs.device(d) = (co.constant(T(1)) - co * co) * dh * o + states_c_grad;\n+\n+    // dci[t] = tanh'(ci[t]) dcs[t] i[t]\n+    dci.device(d) = (ci.constant(T(1)) - ci * ci) * dcs * i;\n+\n+    // df[t] = sigm'(f[t]) dcs[t] cs[t - 1]\n+    cs_prev.device(d) = states_prev.slice(states_cs_offsets(), cell_extents());\n+    df.device(d) = f * (f.constant(T(1)) - f) * dcs * cs_prev;\n+\n+    // di[t] = sigm'(i[t]) dcs[t] ci[t]\n+    di.device(d) = i * (i.constant(T(1)) - i) * dcs * ci;\n+\n+    dicfo.slice(icfo_i_offsets(), cell_extents()).device(d) = di;\n+    dicfo.slice(icfo_c_offsets(), cell_extents()).device(d) = dci;\n+    dicfo.slice(icfo_f_offsets(), cell_extents()).device(d) = df;\n+    dicfo.slice(icfo_o_offsets(), cell_extents()).device(d) = do_;\n+\n+    // We can parallelize the bprop GEMMs on the CPU (on GPU doesn't make any\n+    // difference).\n+    BlockingCounter counter(parallel_dw ? 1 : 2);\n+    auto workers_threads = *(ctx->device()->tensorflow_cpu_worker_threads());\n+    auto workers = workers_threads.workers;\n+\n+    // xh_grad = dstate4 * w^T\n+    typename TTypes<T>::ConstMatrix const_dicfo(\n+          dicfo.data(), dicfo.dimensions());\n+    workers->Schedule([ctx, stream, d, const_dicfo, w, xh_grad, &counter]() {", "path": "tensorflow/contrib/rnn/kernels/lstm_ops.h", "position": null, "original_position": 269, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "fc40971bba82762fe413b5e7d6fe12a09722876b", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "perhaps &d instead of d?\nalso &const_dicfo, &w, &xh_grad?\n", "created_at": "2016-05-11T21:27:16Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62930096", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62930096"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62930096"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>perhaps &amp;d instead of d?<br>\nalso &amp;const_dicfo, &amp;w, &amp;xh_grad?</p>", "body_text": "perhaps &d instead of d?\nalso &const_dicfo, &w, &xh_grad?"}