{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62934521", "pull_request_review_id": null, "id": 62934521, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyOTM0NTIx", "diff_hunk": "@@ -0,0 +1,554 @@\n+#define EIGEN_USE_THREADS\n+\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+#endif  // GOOGLE_CUDA\n+\n+#include \"tensorflow/contrib/rnn/kernels/lstm_ops.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+#if GOOGLE_CUDA\n+#include \"tensorflow/stream_executor/stream.h\"\n+#endif  // GOOGLE_CUDA\n+\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+#if GOOGLE_CUDA\n+\n+namespace {\n+template <typename T>\n+perftools::gputools::DeviceMemory<T> AsDeviceMemory(const T* cuda_memory) {\n+  perftools::gputools::DeviceMemoryBase wrapped(const_cast<T*>(cuda_memory));\n+  perftools::gputools::DeviceMemory<T> typed(wrapped);\n+  return typed;\n+}\n+}  // namespace\n+\n+#endif  // GOOGLE_CUDA\n+\n+namespace functor {\n+template <typename T>\n+void TensorCuBlasGemm<T>::operator()(\n+    OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+    bool transa, bool transb, uint64 m, uint64 n, uint64 k, T alpha,\n+    const T* a, int lda, const T* b, int ldb, T beta, T *c, int ldc) {\n+#if GOOGLE_CUDA\n+  perftools::gputools::blas::Transpose trans[] = {\n+      perftools::gputools::blas::Transpose::kNoTranspose,\n+      perftools::gputools::blas::Transpose::kTranspose};\n+\n+  auto a_ptr = AsDeviceMemory(a);\n+  auto b_ptr = AsDeviceMemory(b);\n+  auto c_ptr = AsDeviceMemory(c);\n+\n+  bool blas_launch_status = stream->ThenBlasGemm(\n+      trans[transa], trans[transb], m, n, k, alpha, a_ptr, lda, b_ptr, ldb,\n+      beta, &c_ptr, ldc).ok();\n+  OP_REQUIRES(ctx, blas_launch_status, errors::Aborted(\"CuBlasGemm failed!\"));\n+#else\n+  ctx->SetStatus(errors::InvalidArgument(\"CuBlasGemm needs CUDA.\"));\n+#endif\n+}\n+\n+template struct TensorCuBlasGemm<float>;\n+template struct TensorCuBlasGemm<double>;\n+}  // end namespace functor\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+class LSTMCellBlockOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+    const int64 states_size = cell_size_ * 2;\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_prev.dims(0) != batch_size: \",\n+                                        states_prev_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(1) == states_size,\n+                errors::InvalidArgument(\"states_prev.dims(1) != cell_size * 2: \",\n+                                        states_prev_tensor->dim_size(1),\n+                                        \" vs. \", states_size));\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+        errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                w_tensor->dim_size(0), \" vs. \",\n+                                input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    // Allocate our output tensors.\n+    Tensor* i_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"i\",\n+          TensorShape({batch_size, cell_size_}), &i_tensor));\n+\n+    Tensor* cs_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"cs\",\n+          TensorShape({batch_size, cell_size_}), &cs_tensor));\n+\n+    Tensor* f_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"f\",\n+          TensorShape({batch_size, cell_size_}), &f_tensor));\n+\n+    Tensor* o_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"o\",\n+          TensorShape({batch_size, cell_size_}), &o_tensor));\n+\n+    Tensor* ci_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"ci\",\n+          TensorShape({batch_size, cell_size_}), &ci_tensor));\n+\n+    Tensor* co_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"co\",\n+          TensorShape({batch_size, cell_size_}), &co_tensor));\n+\n+    Tensor* states_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"states\",\n+        TensorShape({batch_size, states_size}), &states_tensor));\n+\n+    Tensor* h_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"h\",\n+          TensorShape({batch_size, cell_size_}), &h_tensor));\n+\n+    // Allocate our temp tensors.\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_tensor));\n+\n+    Tensor cs_prev_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n+        TensorShape({batch_size, cell_size_}), &cs_prev_tensor));\n+\n+    Tensor h_prev_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n+        TensorShape({batch_size, cell_size_}), &h_prev_tensor));\n+\n+    Tensor icfo_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DataTypeToEnum<T>::v(),\n+        TensorShape({batch_size, cell_size_ * 4}), &icfo_tensor));\n+\n+    functor::LSTMCellBlockFprop<Device, T, USE_CUBLAS>(\n+        batch_size, input_size, cell_size_)(\n+        ctx, stream, ctx->eigen_device<Device>(), forget_bias_,\n+        x_tensor->matrix<T>(), states_prev_tensor->matrix<T>(),\n+        w_tensor->matrix<T>(), b_tensor->vec<T>(), cs_prev_tensor.matrix<T>(),\n+        h_prev_tensor.matrix<T>(), xh_tensor.matrix<T>(),\n+        i_tensor->matrix<T>(), cs_tensor->matrix<T>(), f_tensor->matrix<T>(),\n+        o_tensor->matrix<T>(), ci_tensor->matrix<T>(), co_tensor->matrix<T>(),\n+        icfo_tensor.matrix<T>(), states_tensor->matrix<T>(),\n+        h_tensor->matrix<T>());\n+  }\n+\n+ private:\n+  int64 cell_size_;\n+  float forget_bias_;\n+};\n+\n+#define REGISTER_KERNEL(T)                              \\\n+  REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")         \\\n+                              .Device(DEVICE_CPU)       \\\n+                              .TypeConstraint<T>(\"T\"),  \\\n+                          LSTMCellBlockOp<CPUDevice, T, false>);\n+REGISTER_KERNEL(float);\n+REGISTER_KERNEL(double);\n+#undef REGISTER_KERNEL\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+#define DECLARE_GPU_SPEC(T)                                       \\\n+  template <>                                                     \\\n+  void TensorMemZero<GPUDevice, T>::operator()(                   \\\n+      const GPUDevice& d, typename TTypes<T>::Vec x);             \\", "path": "tensorflow/contrib/rnn/kernels/lstm_ops.cc", "position": null, "original_position": 206, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "fc40971bba82762fe413b5e7d6fe12a09722876b", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "body": "I removed the memcpy / memzero cause we actually don't use them anymore.\nHowever, I don't think we can use references and pointers in those\nfunctions? (Correct me if I am wrong?). because when we call the functions\nwe do something like:  func(..., w_tensor->matrix<T>(), ...) unless we do\nsomething like:\n\nauto w_matrix = w_tensor->matrix<T>();\n\nfunc(.., w_matrix, ...)\n\ndo we want to do that instead w/ the extra aliasing? tradeoff of a few more\nlines of code for a bit of CPU cycles saved?\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Wed, May 11, 2016 at 2:19 PM, ebrevdo notifications@github.com wrote:\n\n> In tensorflow/contrib/rnn/kernels/lstm_ops.cc\n> https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62928537:\n> \n> > +\n> > +#define REGISTER_KERNEL(T)                              \\\n> > -  REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")         \\\n> > -                              .Device(DEVICE_CPU)       \\\n> > -                              .TypeConstraint<T>(\"T\"),  \\\n> > -                          LSTMCellBlockOp<CPUDevice, T, false>);\n> >   +REGISTER_KERNEL(float);\n> >   +REGISTER_KERNEL(double);\n> >   +#undef REGISTER_KERNEL\n> >   +\n> >   +#if GOOGLE_CUDA\n> >   +namespace functor {\n> >   +#define DECLARE_GPU_SPEC(T)                                       \\\n> > -  template <>                                                     \\\n> > -  void TensorMemZero<GPUDevice, T>::operator()(                   \\\n> > -      const GPUDevice& d, typename TTypes<T>::Vec x);             \\\n> \n> you're going to be modifying x, so you should be passing in a pointer.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002/files/fc40971bba82762fe413b5e7d6fe12a09722876b#r62928537\n", "created_at": "2016-05-11T21:59:45Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62934521", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62934521"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62934521"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>I removed the memcpy / memzero cause we actually don't use them anymore.<br>\nHowever, I don't think we can use references and pointers in those<br>\nfunctions? (Correct me if I am wrong?). because when we call the functions<br>\nwe do something like:  func(..., w_tensor-&gt;matrix(), ...) unless we do<br>\nsomething like:</p>\n<p>auto w_matrix = w_tensor-&gt;matrix();</p>\n<p>func(.., w_matrix, ...)</p>\n<p>do we want to do that instead w/ the extra aliasing? tradeoff of a few more<br>\nlines of code for a bit of CPU cycles saved?</p>\n<h2></h2>\n<p>William Chan<br>\nCarnegie Mellon University<br>\n(650) 450-9455<br>\nwilliamchan.ca</p>\n<p>On Wed, May 11, 2016 at 2:19 PM, ebrevdo <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>In tensorflow/contrib/rnn/kernels/lstm_ops.cc<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148966307\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2002\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/2002/hovercard?comment_id=62928537&amp;comment_type=review_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62928537\">#2002 (comment)</a>:</p>\n<blockquote>\n<ul>\n<li></li>\n</ul>\n<p>+#define REGISTER_KERNEL(T)                              \\</p>\n<ul>\n<li>REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")         \\</li>\n<li>\n<pre><code>                         .Device(DEVICE_CPU)       \\\n</code></pre>\n</li>\n<li>\n<pre><code>                         .TypeConstraint&lt;T&gt;(\"T\"),  \\\n</code></pre>\n</li>\n<li>\n<pre><code>                     LSTMCellBlockOp&lt;CPUDevice, T, false&gt;);\n</code></pre>\n+REGISTER_KERNEL(float);<br>\n+REGISTER_KERNEL(double);<br>\n+#undef REGISTER_KERNEL<br>\n+<br>\n+#if GOOGLE_CUDA<br>\n+namespace functor {<br>\n+#define DECLARE_GPU_SPEC(T)                                       \\</li>\n<li>template &lt;&gt;                                                     \\</li>\n<li>void TensorMemZero&lt;GPUDevice, T&gt;::operator()(                   \\</li>\n<li>\n<pre><code> const GPUDevice&amp; d, typename TTypes&lt;T&gt;::Vec x);             \\\n</code></pre>\n</li>\n</ul>\n</blockquote>\n<p>you're going to be modifying x, so you should be passing in a pointer.</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub<br>\n<a href=\"https://github.com/tensorflow/tensorflow/pull/2002/files/fc40971bba82762fe413b5e7d6fe12a09722876b#r62928537\">https://github.com/tensorflow/tensorflow/pull/2002/files/fc40971bba82762fe413b5e7d6fe12a09722876b#r62928537</a></p>\n</blockquote>", "body_text": "I removed the memcpy / memzero cause we actually don't use them anymore.\nHowever, I don't think we can use references and pointers in those\nfunctions? (Correct me if I am wrong?). because when we call the functions\nwe do something like:  func(..., w_tensor->matrix(), ...) unless we do\nsomething like:\nauto w_matrix = w_tensor->matrix();\nfunc(.., w_matrix, ...)\ndo we want to do that instead w/ the extra aliasing? tradeoff of a few more\nlines of code for a bit of CPU cycles saved?\n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\nOn Wed, May 11, 2016 at 2:19 PM, ebrevdo notifications@github.com wrote:\n\nIn tensorflow/contrib/rnn/kernels/lstm_ops.cc\n#2002 (comment):\n\n\n\n\n+#define REGISTER_KERNEL(T)                              \\\n\nREGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")         \\\n\n                         .Device(DEVICE_CPU)       \\\n\n\n\n                         .TypeConstraint<T>(\"T\"),  \\\n\n\n\n                     LSTMCellBlockOp<CPUDevice, T, false>);\n\n+REGISTER_KERNEL(float);\n+REGISTER_KERNEL(double);\n+#undef REGISTER_KERNEL\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+#define DECLARE_GPU_SPEC(T)                                       \\\ntemplate <>                                                     \\\nvoid TensorMemZero<GPUDevice, T>::operator()(                   \\\n\n const GPUDevice& d, typename TTypes<T>::Vec x);             \\\n\n\n\n\nyou're going to be modifying x, so you should be passing in a pointer.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/2002/files/fc40971bba82762fe413b5e7d6fe12a09722876b#r62928537"}