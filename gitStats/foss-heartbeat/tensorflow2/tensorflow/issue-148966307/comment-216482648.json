{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/216482648", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-216482648", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002", "id": 216482648, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNjQ4MjY0OA==", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-03T09:48:37Z", "updated_at": "2016-05-03T09:48:37Z", "author_association": "NONE", "body_html": "<p>Is there a way to detect in the python autodiff graph which input vars need to be differentiated? i.e., in the monolithic op, even if we only care about dw/db, we still always compute dx even though it may not be needed (i.e., 1 layer RNN, only need dw dont need to bprop the dx or when x is a constant).</p>\n<p>I've created an attr that can control this behaviour, but ideally, this can be detected by the python graph to pass to the op to avoid computing the dx when its not necessary.</p>", "body_text": "Is there a way to detect in the python autodiff graph which input vars need to be differentiated? i.e., in the monolithic op, even if we only care about dw/db, we still always compute dx even though it may not be needed (i.e., 1 layer RNN, only need dw dont need to bprop the dx or when x is a constant).\nI've created an attr that can control this behaviour, but ideally, this can be detected by the python graph to pass to the op to avoid computing the dx when its not necessary.", "body": "Is there a way to detect in the python autodiff graph which input vars need to be differentiated? i.e., in the monolithic op, even if we only care about dw/db, we still always compute dx even though it may not be needed (i.e., 1 layer RNN, only need dw dont need to bprop the dx or when x is a constant).\n\nI've created an attr that can control this behaviour, but ideally, this can be detected by the python graph to pass to the op to avoid computing the dx when its not necessary.\n"}