{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/61457781", "pull_request_review_id": null, "id": 61457781, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDU3Nzgx", "diff_hunk": "@@ -0,0 +1,272 @@\n+#ifndef TENSORFLOW_KERNELS_LSTM_OPS_H_\n+#define TENSORFLOW_KERNELS_LSTM_OPS_H_\n+\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/stream_executor/stream.h\"\n+\n+namespace tensorflow {\n+\n+#if GOOGLE_CUDA\n+\n+namespace {\n+template <typename T>\n+perftools::gputools::DeviceMemory<T> AsDeviceMemory(const T* cuda_memory) {\n+  perftools::gputools::DeviceMemoryBase wrapped(const_cast<T*>(cuda_memory));\n+  perftools::gputools::DeviceMemory<T> typed(wrapped);\n+  return typed;\n+}\n+}  // namespace\n+\n+#endif  // GOOGLE_CUDA\n+\n+namespace functor {\n+\n+template <typename Device, bool USE_CUBLAS>\n+struct LSTMCellBlockFprop {\n+  void operator()(\n+      perftools::gputools::Stream* stream, const Device& d,\n+      const int batch_size, const int input_size, const int cell_size, const float forget_bias,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::Matrix xh,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w,\n+      typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::Matrix h,\n+      typename TTypes<float>::Matrix states) {\n+    // Pointer offsets.\n+    Eigen::array<int, 2> i_offsets  = {0, cell_size * 0};\n+    Eigen::array<int, 2> cs_offsets = {0, cell_size * 1};\n+    Eigen::array<int, 2> f_offsets  = {0, cell_size * 2};\n+    Eigen::array<int, 2> o_offsets  = {0, cell_size * 3};\n+    Eigen::array<int, 2> ci_offsets = {0, cell_size * 4};\n+    Eigen::array<int, 2> co_offsets = {0, cell_size * 5};\n+    Eigen::array<int, 2> h_offsets  = {0, cell_size * 6};\n+\n+    Eigen::array<int, 2> cell_extents = {batch_size, cell_size};\n+\n+    // xh = [x, h_prev].\n+    Eigen::array<int, 2> xh_x_offsets = {0, 0};\n+    Eigen::array<int, 2> xh_x_extents = {batch_size, input_size};\n+    Eigen::array<int, 2> xh_h_offsets = {0, input_size};\n+    Eigen::array<int, 2> xh_h_extents = {batch_size, cell_size};\n+\n+    auto h_prev =\n+        states_prev.slice(h_offsets, cell_extents);\n+\n+    xh.slice(xh_x_offsets, xh_x_extents).device(d) = x;\n+    xh.slice(xh_h_offsets, xh_h_extents).device(d) = h_prev;\n+\n+    Eigen::array<int, 2> states_offsets = {0, 0};\n+    Eigen::array<int, 2> states_extents = {batch_size, cell_size * 4};\n+\n+    // states = xh * w + b\n+    if (USE_CUBLAS) {\n+#if GOOGLE_CUDA\n+      states.slice(states_offsets, states_extents).device(d) =\n+          b.broadcast(Eigen::array<int, 2>({batch_size, 1}));\n+\n+      auto kNoTranspose =\n+          perftools::gputools::blas::Transpose::kNoTranspose;\n+\n+      const uint64 m = batch_size;\n+      const uint64 k = input_size + cell_size;\n+      const uint64 n = cell_size * 4;\n+\n+      auto a_ptr = AsDeviceMemory(xh.data());\n+      auto b_ptr = AsDeviceMemory(w.data());\n+      auto c_ptr = AsDeviceMemory(states.data());\n+\n+      bool blas_launch_status = stream->ThenBlasGemm(\n+          kNoTranspose, kNoTranspose, n, m, k, 1.0f, b_ptr, n, a_ptr, k, 1.0f,\n+          &c_ptr, cell_size * 7).ok();\n+      CHECK(blas_launch_status);\n+#endif  // GOOGLE_CUDA\n+    } else {\n+      Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_pairs;\n+      contract_pairs[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 0);\n+\n+      states.slice(states_offsets, states_extents).device(d) =\n+          xh.contract(w, contract_pairs) +\n+          b.broadcast(Eigen::array<int, 2>({batch_size, 1}));\n+    }\n+\n+    // Input gate.\n+    auto i = states.slice(i_offsets, cell_extents);\n+    i.device(d) = i.sigmoid();\n+\n+    // Cell input.\n+    auto ci = states.slice(ci_offsets, cell_extents);\n+    ci.device(d) =\n+        states.slice(cs_offsets, cell_extents).tanh();\n+\n+    // Forget gate (w/ bias).\n+    auto f = states.slice(f_offsets, cell_extents);\n+    f.device(d) = (f + f.constant(forget_bias)).sigmoid();\n+\n+    // cs = ci .* i + f .* cs_prev\n+    auto cs_prev = states_prev.slice(cs_offsets, cell_extents);\n+    auto cs = states.slice(cs_offsets, cell_extents);\n+    cs.device(d) = i * ci + f * cs_prev;\n+\n+    // co = tanh(cs)\n+    auto co = states.slice(co_offsets, cell_extents);\n+    co.device(d) = cs.tanh();\n+\n+    // h = o .* co\n+    auto o = states.slice(o_offsets, cell_extents);\n+    o.device(d) = o.sigmoid();\n+    states.slice(h_offsets, cell_extents).device(d) = o * co;\n+\n+    h.device(d) = states.slice(h_offsets, cell_extents);\n+  }\n+};\n+\n+template <typename Device, bool USE_CUBLAS>\n+struct LSTMCellBlockBprop {\n+  void operator()(\n+      perftools::gputools::Stream* stream, const Device& d,\n+      const int batch_size, const int input_size, const int cell_size,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::Matrix xh,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w,\n+      typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::ConstMatrix states,\n+      typename TTypes<float>::ConstMatrix h_grad,\n+      typename TTypes<float>::ConstMatrix states_grad,\n+      typename TTypes<float>::Matrix xh_grad,\n+      typename TTypes<float>::Matrix x_grad,\n+      typename TTypes<float>::Matrix states_prev_grad,\n+      typename TTypes<float>::Matrix w_grad,\n+      typename TTypes<float>::Vec b_grad) {\n+    // Pointer offsets.\n+    Eigen::array<int, 2> i_offsets  = {0, cell_size * 0};\n+    Eigen::array<int, 2> cs_offsets = {0, cell_size * 1};\n+    Eigen::array<int, 2> f_offsets  = {0, cell_size * 2};\n+    Eigen::array<int, 2> o_offsets  = {0, cell_size * 3};\n+    Eigen::array<int, 2> ci_offsets = {0, cell_size * 4};\n+    Eigen::array<int, 2> co_offsets = {0, cell_size * 5};\n+    Eigen::array<int, 2> h_offsets  = {0, cell_size * 6};\n+\n+    Eigen::array<int, 2> cell_extents = {batch_size, cell_size};\n+\n+    // xh = [x, h_prev]\n+    Eigen::array<int, 2> xh_x_offsets = {0, 0};\n+    Eigen::array<int, 2> xh_x_extents = {batch_size, input_size};\n+    Eigen::array<int, 2> xh_h_offsets = {0, input_size};\n+    Eigen::array<int, 2> xh_h_extents = {batch_size, cell_size};\n+    xh.slice(xh_x_offsets, xh_x_extents).device(d) = x;\n+    auto h_prev = states_prev.slice(h_offsets, cell_extents);\n+    xh.slice(xh_h_offsets, xh_h_extents).device(d) = h_prev;\n+\n+    // dh.\n+    auto dh = states_prev_grad.slice(h_offsets, cell_extents);\n+    dh.device(d) =\n+        h_grad + states_grad.slice(h_offsets, cell_extents);\n+\n+    // do[t] = sigm'(o[t]) .* dh[t] .* co[t]\n+    auto co = states.slice(co_offsets, cell_extents);\n+    auto o = states.slice(o_offsets, cell_extents);\n+    states_prev_grad.slice(o_offsets, cell_extents).device(d) =\n+        o * (o.constant(1.0f) - o) * dh * co;\n+\n+    // dcs[t] += tanh'(cs[t]) .* dh[t] .* o[t] + dcs[t + 1] .* f[t + 1]\n+    auto dcs = states_prev_grad.slice(co_offsets, cell_extents);\n+    dcs.device(d) =\n+        (co.constant(1.0f) - co * co) * dh * o +\n+        states_grad.slice(ci_offsets, cell_extents);\n+\n+    // dci[t] = tanh'(ci[t]) dcs[t] i[t]\n+    auto ci = states.slice(ci_offsets, cell_extents);\n+    auto i = states.slice(i_offsets, cell_extents);\n+    states_prev_grad.slice(cs_offsets, cell_extents).device(d) =\n+        (ci.constant(1.0f) - ci * ci) * dcs * i;\n+\n+    // df[t] = sigm'(f[t]) dcs[t] cs[t - 1]\n+    auto f = states.slice(f_offsets, cell_extents);\n+    auto cs_prev = states_prev.slice(cs_offsets, cell_extents);\n+    states_prev_grad.slice(f_offsets, cell_extents).device(d) =\n+        f * (f.constant(1.0f) - f) * dcs * cs_prev;\n+\n+    // di[t] = sigm'(i[t]) dcs[t] ci[t]\n+    states_prev_grad.slice(i_offsets, cell_extents).device(d) =\n+        i * (i.constant(1.0f) - i) * dcs * ci;\n+\n+    // xh_grad.\n+    Eigen::array<int, 2> states_offsets = {0, 0};\n+    Eigen::array<int, 2> states_extents = {batch_size, cell_size * 4};\n+\n+    auto dstate4 = states_prev_grad.slice(states_offsets, states_extents);\n+\n+    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> xh_grad_contract_pairs;\n+    xh_grad_contract_pairs[0] = Eigen::IndexPair<Eigen::DenseIndex>(1, 1);\n+\n+    // xh_grad = dstate4 * w^T\n+    if (USE_CUBLAS) {\n+#if GOOGLE_CUDA\n+      auto kTranspose =\n+          perftools::gputools::blas::Transpose::kTranspose;\n+      auto kNoTranspose =\n+          perftools::gputools::blas::Transpose::kNoTranspose;\n+\n+      const uint64 m = batch_size;\n+      const uint64 k = cell_size * 4;\n+      const uint64 n = input_size + cell_size;\n+\n+      auto a_ptr = AsDeviceMemory(states_prev_grad.data());\n+      auto b_ptr = AsDeviceMemory(w.data());\n+      auto c_ptr = AsDeviceMemory(xh_grad.data());\n+\n+      CHECK(CHECK_NOTNULL(stream)->ThenBlasGemm(", "path": "tensorflow/contrib/rnn/kernels/lstm_ops.h", "position": null, "original_position": 222, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "36fb73b93c217d59a868524383b347104ce431dd", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "can you pass context into this function and then set the status and return ?  Killing the whole program here is probably a bad idea when the error message can be sent back to the client.\n", "created_at": "2016-04-28T16:21:13Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r61457781", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/61457781"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r61457781"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>can you pass context into this function and then set the status and return ?  Killing the whole program here is probably a bad idea when the error message can be sent back to the client.</p>", "body_text": "can you pass context into this function and then set the status and return ?  Killing the whole program here is probably a bad idea when the error message can be sent back to the client."}