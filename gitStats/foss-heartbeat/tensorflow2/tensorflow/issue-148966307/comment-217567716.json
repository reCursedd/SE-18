{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217567716", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217567716", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002", "id": 217567716, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzU2NzcxNg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-06T21:47:34Z", "updated_at": "2016-05-06T21:47:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Perhaps it's worthwhile to not have the parallel_dw option?  Is it much<br>\nhigher memory consumption?</p>\n<p>On Fri, May 6, 2016 at 2:43 PM, William Chan <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>OK... New tests+benchmarks added! Rewrote the op to allow more<br>\nparallelism. Compared earlier version, it now uses more memory but is<br>\nfaster. Simplified the kernel code quite a bit too.</p>\n<p>When \"parallel_dw == True\" (option in the op), its pretty much faster<br>\nacross the board on an variety of minibatches / cell_size configurations.<br>\nOn certain small (but common) minibatch sizes (i.e., 32/64), we can get<br>\ndouble the speed. WOOT!</p>\n<p>See below for benchmarks (benchmarks shamelessly mirrored from existing<br>\npython/kernel_tests/rnn_test.py).</p>\n<p>Calculation: Static Unroll with Basic LSTM vs. Block LSTM<br>\nbatch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)<br>\n512 50 512 False True 1.608925 1.595009 0.991351<br>\n512 50 512 False False 1.622124 1.652539 1.018750<br>\n512 50 512 True True 0.105598 0.100758 0.954168<br>\n512 50 512 True False 0.105723 0.110407 1.044307<br>\n512 50 256 False True 0.570323 0.557588 0.977670<br>\n512 50 256 False False 0.548141 0.565626 1.031899<br>\n512 50 256 True True 0.051169 0.041841 0.817712<br>\n512 50 256 True False 0.049043 0.049948 1.018455<br>\n512 50 128 False True 0.255528 0.223692 0.875412<br>\n512 50 128 False False 0.265746 0.230062 0.865724<br>\n512 50 128 True True 0.035591 0.025530 0.717325<br>\n512 50 128 True False 0.039220 0.030153 0.768819<br>\n256 50 512 False True 0.914745 0.912568 0.997620<br>\n256 50 512 False False 0.898728 0.957362 1.065242<br>\n256 50 512 True True 0.065084 0.061613 0.946667<br>\n256 50 512 True False 0.065379 0.066195 1.012489<br>\n256 50 256 False True 0.345520 0.321437 0.930302<br>\n256 50 256 False False 0.339179 0.331827 0.978325<br>\n256 50 256 True True 0.045273 0.030890 0.682309<br>\n256 50 256 True False 0.043981 0.035227 0.800952<br>\n256 50 128 False True 0.189186 0.148082 0.782732<br>\n256 50 128 False False 0.193971 0.150826 0.777567<br>\n256 50 128 True True 0.035081 0.022858 0.651583<br>\n256 50 128 True False 0.039035 0.024608 0.630408<br>\n128 50 512 False True 0.571939 0.545305 0.953431<br>\n128 50 512 False False 0.554707 0.597802 1.077690<br>\n128 50 512 True True 0.052550 0.046426 0.883464<br>\n128 50 512 True False 0.053724 0.049175 0.915331<br>\n128 50 256 False True 0.235361 0.207240 0.880520<br>\n128 50 256 False False 0.231161 0.224550 0.971402<br>\n128 50 256 True True 0.037826 0.027088 0.716106<br>\n128 50 256 True False 0.034500 0.028032 0.812499<br>\n128 50 128 False True 0.153420 0.112691 0.734525<br>\n128 50 128 False False 0.139047 0.134652 0.968391<br>\n128 50 128 True True 0.042392 0.023966 0.565348<br>\n128 50 128 True False 0.041555 0.021308 0.512771<br>\n64 50 512 False True 0.404943 0.384559 0.949663<br>\n64 50 512 False False 0.403185 0.450789 1.118070<br>\n64 50 512 True True 0.048875 0.037370 0.764596<br>\n64 50 512 True False 0.048246 0.038403 0.795976<br>\n64 50 256 False True 0.179140 0.155737 0.869363<br>\n64 50 256 False False 0.172691 0.176327 1.021057<br>\n64 50 256 True True 0.036864 0.024936 0.676441<br>\n64 50 256 True False 0.036052 0.024993 0.693251<br>\n64 50 128 False True 0.147388 0.122999 0.834523<br>\n64 50 128 False False 0.123700 0.122942 0.993868<br>\n64 50 128 True True 0.039172 0.020994 0.535938<br>\n64 50 128 True False 0.037731 0.018184 0.481955<br>\n32 50 512 False True 0.321763 0.303199 0.942308<br>\n32 50 512 False False 0.313588 0.384228 1.225262<br>\n32 50 512 True True 0.039348 0.032741 0.832085<br>\n32 50 512 True False 0.041097 0.033916 0.825274<br>\n32 50 256 False True 0.156073 0.135813 0.870190<br>\n32 50 256 False False 0.155621 0.171671 1.103136<br>\n32 50 256 True True 0.040236 0.025395 0.631153<br>\n32 50 256 True False 0.049391 0.024777 0.501641<br>\n32 50 128 False True 0.144160 0.111829 0.775730<br>\n32 50 128 False False 0.136285 0.118303 0.868056<br>\n32 50 128 True True 0.045720 0.024474 0.535300<br>\n32 50 128 True False 0.046344 0.023173 0.500020<br>\n16 50 512 False True 0.264227 0.244503 0.925353<br>\n16 50 512 False False 0.267688 0.349579 1.305921<br>\n16 50 512 True True 0.045465 0.031925 0.702196<br>\n16 50 512 True False 0.041305 0.032435 0.785265<br>\n16 50 256 False True 0.155288 0.120286 0.774599<br>\n16 50 256 False False 0.142004 0.181369 1.277210<br>\n16 50 256 True True 0.035678 0.025734 0.721294<br>\n16 50 256 True False 0.041158 0.024548 0.596429<br>\n16 50 128 False True 0.146916 0.117762 0.801561<br>\n16 50 128 False False 0.108035 0.122011 1.129364<br>\n16 50 128 True True 0.040217 0.024973 0.620957<br>\n16 50 128 True False 0.036523 0.022020 0.602924</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148966307\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2002\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/2002/hovercard?comment_id=217565864&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217565864\">#2002 (comment)</a></p>\n</blockquote>", "body_text": "Perhaps it's worthwhile to not have the parallel_dw option?  Is it much\nhigher memory consumption?\nOn Fri, May 6, 2016 at 2:43 PM, William Chan notifications@github.com\nwrote:\n\nOK... New tests+benchmarks added! Rewrote the op to allow more\nparallelism. Compared earlier version, it now uses more memory but is\nfaster. Simplified the kernel code quite a bit too.\nWhen \"parallel_dw == True\" (option in the op), its pretty much faster\nacross the board on an variety of minibatches / cell_size configurations.\nOn certain small (but common) minibatch sizes (i.e., 32/64), we can get\ndouble the speed. WOOT!\nSee below for benchmarks (benchmarks shamelessly mirrored from existing\npython/kernel_tests/rnn_test.py).\nCalculation: Static Unroll with Basic LSTM vs. Block LSTM\nbatch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)\n512 50 512 False True 1.608925 1.595009 0.991351\n512 50 512 False False 1.622124 1.652539 1.018750\n512 50 512 True True 0.105598 0.100758 0.954168\n512 50 512 True False 0.105723 0.110407 1.044307\n512 50 256 False True 0.570323 0.557588 0.977670\n512 50 256 False False 0.548141 0.565626 1.031899\n512 50 256 True True 0.051169 0.041841 0.817712\n512 50 256 True False 0.049043 0.049948 1.018455\n512 50 128 False True 0.255528 0.223692 0.875412\n512 50 128 False False 0.265746 0.230062 0.865724\n512 50 128 True True 0.035591 0.025530 0.717325\n512 50 128 True False 0.039220 0.030153 0.768819\n256 50 512 False True 0.914745 0.912568 0.997620\n256 50 512 False False 0.898728 0.957362 1.065242\n256 50 512 True True 0.065084 0.061613 0.946667\n256 50 512 True False 0.065379 0.066195 1.012489\n256 50 256 False True 0.345520 0.321437 0.930302\n256 50 256 False False 0.339179 0.331827 0.978325\n256 50 256 True True 0.045273 0.030890 0.682309\n256 50 256 True False 0.043981 0.035227 0.800952\n256 50 128 False True 0.189186 0.148082 0.782732\n256 50 128 False False 0.193971 0.150826 0.777567\n256 50 128 True True 0.035081 0.022858 0.651583\n256 50 128 True False 0.039035 0.024608 0.630408\n128 50 512 False True 0.571939 0.545305 0.953431\n128 50 512 False False 0.554707 0.597802 1.077690\n128 50 512 True True 0.052550 0.046426 0.883464\n128 50 512 True False 0.053724 0.049175 0.915331\n128 50 256 False True 0.235361 0.207240 0.880520\n128 50 256 False False 0.231161 0.224550 0.971402\n128 50 256 True True 0.037826 0.027088 0.716106\n128 50 256 True False 0.034500 0.028032 0.812499\n128 50 128 False True 0.153420 0.112691 0.734525\n128 50 128 False False 0.139047 0.134652 0.968391\n128 50 128 True True 0.042392 0.023966 0.565348\n128 50 128 True False 0.041555 0.021308 0.512771\n64 50 512 False True 0.404943 0.384559 0.949663\n64 50 512 False False 0.403185 0.450789 1.118070\n64 50 512 True True 0.048875 0.037370 0.764596\n64 50 512 True False 0.048246 0.038403 0.795976\n64 50 256 False True 0.179140 0.155737 0.869363\n64 50 256 False False 0.172691 0.176327 1.021057\n64 50 256 True True 0.036864 0.024936 0.676441\n64 50 256 True False 0.036052 0.024993 0.693251\n64 50 128 False True 0.147388 0.122999 0.834523\n64 50 128 False False 0.123700 0.122942 0.993868\n64 50 128 True True 0.039172 0.020994 0.535938\n64 50 128 True False 0.037731 0.018184 0.481955\n32 50 512 False True 0.321763 0.303199 0.942308\n32 50 512 False False 0.313588 0.384228 1.225262\n32 50 512 True True 0.039348 0.032741 0.832085\n32 50 512 True False 0.041097 0.033916 0.825274\n32 50 256 False True 0.156073 0.135813 0.870190\n32 50 256 False False 0.155621 0.171671 1.103136\n32 50 256 True True 0.040236 0.025395 0.631153\n32 50 256 True False 0.049391 0.024777 0.501641\n32 50 128 False True 0.144160 0.111829 0.775730\n32 50 128 False False 0.136285 0.118303 0.868056\n32 50 128 True True 0.045720 0.024474 0.535300\n32 50 128 True False 0.046344 0.023173 0.500020\n16 50 512 False True 0.264227 0.244503 0.925353\n16 50 512 False False 0.267688 0.349579 1.305921\n16 50 512 True True 0.045465 0.031925 0.702196\n16 50 512 True False 0.041305 0.032435 0.785265\n16 50 256 False True 0.155288 0.120286 0.774599\n16 50 256 False False 0.142004 0.181369 1.277210\n16 50 256 True True 0.035678 0.025734 0.721294\n16 50 256 True False 0.041158 0.024548 0.596429\n16 50 128 False True 0.146916 0.117762 0.801561\n16 50 128 False False 0.108035 0.122011 1.129364\n16 50 128 True True 0.040217 0.024973 0.620957\n16 50 128 True False 0.036523 0.022020 0.602924\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n#2002 (comment)", "body": "Perhaps it's worthwhile to not have the parallel_dw option?  Is it much\nhigher memory consumption?\n\nOn Fri, May 6, 2016 at 2:43 PM, William Chan notifications@github.com\nwrote:\n\n> OK... New tests+benchmarks added! Rewrote the op to allow more\n> parallelism. Compared earlier version, it now uses more memory but is\n> faster. Simplified the kernel code quite a bit too.\n> \n> When \"parallel_dw == True\" (option in the op), its pretty much faster\n> across the board on an variety of minibatches / cell_size configurations.\n> On certain small (but common) minibatch sizes (i.e., 32/64), we can get\n> double the speed. WOOT!\n> \n> See below for benchmarks (benchmarks shamelessly mirrored from existing\n> python/kernel_tests/rnn_test.py).\n> \n> Calculation: Static Unroll with Basic LSTM vs. Block LSTM\n> batch max_t units gpu parallel_dw dt(basic) dt(block) dt(basic)/dt(block)\n> 512 50 512 False True 1.608925 1.595009 0.991351\n> 512 50 512 False False 1.622124 1.652539 1.018750\n> 512 50 512 True True 0.105598 0.100758 0.954168\n> 512 50 512 True False 0.105723 0.110407 1.044307\n> 512 50 256 False True 0.570323 0.557588 0.977670\n> 512 50 256 False False 0.548141 0.565626 1.031899\n> 512 50 256 True True 0.051169 0.041841 0.817712\n> 512 50 256 True False 0.049043 0.049948 1.018455\n> 512 50 128 False True 0.255528 0.223692 0.875412\n> 512 50 128 False False 0.265746 0.230062 0.865724\n> 512 50 128 True True 0.035591 0.025530 0.717325\n> 512 50 128 True False 0.039220 0.030153 0.768819\n> 256 50 512 False True 0.914745 0.912568 0.997620\n> 256 50 512 False False 0.898728 0.957362 1.065242\n> 256 50 512 True True 0.065084 0.061613 0.946667\n> 256 50 512 True False 0.065379 0.066195 1.012489\n> 256 50 256 False True 0.345520 0.321437 0.930302\n> 256 50 256 False False 0.339179 0.331827 0.978325\n> 256 50 256 True True 0.045273 0.030890 0.682309\n> 256 50 256 True False 0.043981 0.035227 0.800952\n> 256 50 128 False True 0.189186 0.148082 0.782732\n> 256 50 128 False False 0.193971 0.150826 0.777567\n> 256 50 128 True True 0.035081 0.022858 0.651583\n> 256 50 128 True False 0.039035 0.024608 0.630408\n> 128 50 512 False True 0.571939 0.545305 0.953431\n> 128 50 512 False False 0.554707 0.597802 1.077690\n> 128 50 512 True True 0.052550 0.046426 0.883464\n> 128 50 512 True False 0.053724 0.049175 0.915331\n> 128 50 256 False True 0.235361 0.207240 0.880520\n> 128 50 256 False False 0.231161 0.224550 0.971402\n> 128 50 256 True True 0.037826 0.027088 0.716106\n> 128 50 256 True False 0.034500 0.028032 0.812499\n> 128 50 128 False True 0.153420 0.112691 0.734525\n> 128 50 128 False False 0.139047 0.134652 0.968391\n> 128 50 128 True True 0.042392 0.023966 0.565348\n> 128 50 128 True False 0.041555 0.021308 0.512771\n> 64 50 512 False True 0.404943 0.384559 0.949663\n> 64 50 512 False False 0.403185 0.450789 1.118070\n> 64 50 512 True True 0.048875 0.037370 0.764596\n> 64 50 512 True False 0.048246 0.038403 0.795976\n> 64 50 256 False True 0.179140 0.155737 0.869363\n> 64 50 256 False False 0.172691 0.176327 1.021057\n> 64 50 256 True True 0.036864 0.024936 0.676441\n> 64 50 256 True False 0.036052 0.024993 0.693251\n> 64 50 128 False True 0.147388 0.122999 0.834523\n> 64 50 128 False False 0.123700 0.122942 0.993868\n> 64 50 128 True True 0.039172 0.020994 0.535938\n> 64 50 128 True False 0.037731 0.018184 0.481955\n> 32 50 512 False True 0.321763 0.303199 0.942308\n> 32 50 512 False False 0.313588 0.384228 1.225262\n> 32 50 512 True True 0.039348 0.032741 0.832085\n> 32 50 512 True False 0.041097 0.033916 0.825274\n> 32 50 256 False True 0.156073 0.135813 0.870190\n> 32 50 256 False False 0.155621 0.171671 1.103136\n> 32 50 256 True True 0.040236 0.025395 0.631153\n> 32 50 256 True False 0.049391 0.024777 0.501641\n> 32 50 128 False True 0.144160 0.111829 0.775730\n> 32 50 128 False False 0.136285 0.118303 0.868056\n> 32 50 128 True True 0.045720 0.024474 0.535300\n> 32 50 128 True False 0.046344 0.023173 0.500020\n> 16 50 512 False True 0.264227 0.244503 0.925353\n> 16 50 512 False False 0.267688 0.349579 1.305921\n> 16 50 512 True True 0.045465 0.031925 0.702196\n> 16 50 512 True False 0.041305 0.032435 0.785265\n> 16 50 256 False True 0.155288 0.120286 0.774599\n> 16 50 256 False False 0.142004 0.181369 1.277210\n> 16 50 256 True True 0.035678 0.025734 0.721294\n> 16 50 256 True False 0.041158 0.024548 0.596429\n> 16 50 128 False True 0.146916 0.117762 0.801561\n> 16 50 128 False False 0.108035 0.122011 1.129364\n> 16 50 128 True True 0.040217 0.024973 0.620957\n> 16 50 128 True False 0.036523 0.022020 0.602924\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002#issuecomment-217565864\n"}