{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2002/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002", "id": 148966307, "node_id": "MDExOlB1bGxSZXF1ZXN0NjY3NzI1NzU=", "number": 2002, "title": "LSTM*BlockOp: Monolithic kernels for the LSTM, approx 40-50% faster.", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 86, "created_at": "2016-04-17T16:08:28Z", "updated_at": "2016-08-20T04:47:53Z", "closed_at": "2016-06-08T00:58:34Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002", "diff_url": "https://github.com/tensorflow/tensorflow/pull/2002.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/2002.patch"}, "body_html": "<p>These are monolithic OpKernel for LSTMs. Compared to python BasicLSTMCell impl on 1 layer PTB this is a 40-50% performance difference on Titan X. Also, this is much more efficient if you want to create extremely large graphs with thousands of timesteps (i.e., replace many simple ops with 1 op in the graph).</p>\n<p>New kernels:<br>\nLSTMCellBlock, LSTMCellBlockGrad, LSTMBlock, LSTMBlockGrad</p>\n<p>Notes:<br>\nOn the GPU, performance delta varies between Eigen::Contraction and CUBLAS as a function of batch_size (and to a lesser extent cell_size). Recommend use power of two as usual for cell_size and batch_size.</p>\n<p>I did not add the peephole (diagonal) connections, this is trivial and only a few extra lines of C++ code if really desired.</p>\n<p>Benchmark results on Titan X... I changed the PTB script to use 1 layer LSTM and batch size of 32, otherwise everything same as the original ptb_word_lm.py<br>\nBasicLSTMCell 1layer PTB:<br>\n0.007 perplexity: 61.504 speed: 18630 wps<br>\n0.107 perplexity: 44.461 speed: 22189 wps<br>\n0.207 perplexity: 42.393 speed: 22336 wps<br>\n0.306 perplexity: 41.897 speed: 22280 wps<br>\n0.406 perplexity: 41.164 speed: 22235 wps<br>\n0.506 perplexity: 40.266 speed: 22133 wps<br>\n0.606 perplexity: 39.321 speed: 22157 wps<br>\n0.706 perplexity: 37.898 speed: 22178 wps<br>\n0.806 perplexity: 36.951 speed: 22151 wps<br>\n0.906 perplexity: 36.175 speed: 22141 wps<br>\nEpoch: 13 Train Perplexity: 35.804<br>\nEpoch: 13 Valid Perplexity: 122.880<br>\nTest Perplexity: 117.428</p>\n<p>LSTMCellBlock 1layer PTB:<br>\n0.007 perplexity: 61.092 speed: 26158 wps<br>\n0.107 perplexity: 44.842 speed: 33217 wps<br>\n0.207 perplexity: 42.963 speed: 33537 wps<br>\n0.306 perplexity: 42.496 speed: 33671 wps<br>\n0.406 perplexity: 41.753 speed: 33727 wps<br>\n0.506 perplexity: 40.867 speed: 33763 wps<br>\n0.606 perplexity: 39.931 speed: 33787 wps<br>\n0.706 perplexity: 38.443 speed: 33805 wps<br>\n0.806 perplexity: 37.481 speed: 33817 wps<br>\n0.906 perplexity: 36.692 speed: 33820 wps<br>\nEpoch: 13 Train Perplexity: 36.290<br>\nEpoch: 13 Valid Perplexity: 123.468<br>\nTest Perplexity: 117.972</p>\n<p>Acknowledgements: lots of discussions w/ ebrevdo!</p>", "body_text": "These are monolithic OpKernel for LSTMs. Compared to python BasicLSTMCell impl on 1 layer PTB this is a 40-50% performance difference on Titan X. Also, this is much more efficient if you want to create extremely large graphs with thousands of timesteps (i.e., replace many simple ops with 1 op in the graph).\nNew kernels:\nLSTMCellBlock, LSTMCellBlockGrad, LSTMBlock, LSTMBlockGrad\nNotes:\nOn the GPU, performance delta varies between Eigen::Contraction and CUBLAS as a function of batch_size (and to a lesser extent cell_size). Recommend use power of two as usual for cell_size and batch_size.\nI did not add the peephole (diagonal) connections, this is trivial and only a few extra lines of C++ code if really desired.\nBenchmark results on Titan X... I changed the PTB script to use 1 layer LSTM and batch size of 32, otherwise everything same as the original ptb_word_lm.py\nBasicLSTMCell 1layer PTB:\n0.007 perplexity: 61.504 speed: 18630 wps\n0.107 perplexity: 44.461 speed: 22189 wps\n0.207 perplexity: 42.393 speed: 22336 wps\n0.306 perplexity: 41.897 speed: 22280 wps\n0.406 perplexity: 41.164 speed: 22235 wps\n0.506 perplexity: 40.266 speed: 22133 wps\n0.606 perplexity: 39.321 speed: 22157 wps\n0.706 perplexity: 37.898 speed: 22178 wps\n0.806 perplexity: 36.951 speed: 22151 wps\n0.906 perplexity: 36.175 speed: 22141 wps\nEpoch: 13 Train Perplexity: 35.804\nEpoch: 13 Valid Perplexity: 122.880\nTest Perplexity: 117.428\nLSTMCellBlock 1layer PTB:\n0.007 perplexity: 61.092 speed: 26158 wps\n0.107 perplexity: 44.842 speed: 33217 wps\n0.207 perplexity: 42.963 speed: 33537 wps\n0.306 perplexity: 42.496 speed: 33671 wps\n0.406 perplexity: 41.753 speed: 33727 wps\n0.506 perplexity: 40.867 speed: 33763 wps\n0.606 perplexity: 39.931 speed: 33787 wps\n0.706 perplexity: 38.443 speed: 33805 wps\n0.806 perplexity: 37.481 speed: 33817 wps\n0.906 perplexity: 36.692 speed: 33820 wps\nEpoch: 13 Train Perplexity: 36.290\nEpoch: 13 Valid Perplexity: 123.468\nTest Perplexity: 117.972\nAcknowledgements: lots of discussions w/ ebrevdo!", "body": "These are monolithic OpKernel for LSTMs. Compared to python BasicLSTMCell impl on 1 layer PTB this is a 40-50% performance difference on Titan X. Also, this is much more efficient if you want to create extremely large graphs with thousands of timesteps (i.e., replace many simple ops with 1 op in the graph).\n\nNew kernels:\nLSTMCellBlock, LSTMCellBlockGrad, LSTMBlock, LSTMBlockGrad\n\nNotes:\nOn the GPU, performance delta varies between Eigen::Contraction and CUBLAS as a function of batch_size (and to a lesser extent cell_size). Recommend use power of two as usual for cell_size and batch_size.\n\nI did not add the peephole (diagonal) connections, this is trivial and only a few extra lines of C++ code if really desired.\n\nBenchmark results on Titan X... I changed the PTB script to use 1 layer LSTM and batch size of 32, otherwise everything same as the original ptb_word_lm.py\nBasicLSTMCell 1layer PTB:\n0.007 perplexity: 61.504 speed: 18630 wps\n0.107 perplexity: 44.461 speed: 22189 wps\n0.207 perplexity: 42.393 speed: 22336 wps\n0.306 perplexity: 41.897 speed: 22280 wps\n0.406 perplexity: 41.164 speed: 22235 wps\n0.506 perplexity: 40.266 speed: 22133 wps\n0.606 perplexity: 39.321 speed: 22157 wps\n0.706 perplexity: 37.898 speed: 22178 wps\n0.806 perplexity: 36.951 speed: 22151 wps\n0.906 perplexity: 36.175 speed: 22141 wps\nEpoch: 13 Train Perplexity: 35.804\nEpoch: 13 Valid Perplexity: 122.880\nTest Perplexity: 117.428\n\nLSTMCellBlock 1layer PTB:\n0.007 perplexity: 61.092 speed: 26158 wps\n0.107 perplexity: 44.842 speed: 33217 wps\n0.207 perplexity: 42.963 speed: 33537 wps\n0.306 perplexity: 42.496 speed: 33671 wps\n0.406 perplexity: 41.753 speed: 33727 wps\n0.506 perplexity: 40.867 speed: 33763 wps\n0.606 perplexity: 39.931 speed: 33787 wps\n0.706 perplexity: 38.443 speed: 33805 wps\n0.806 perplexity: 37.481 speed: 33817 wps\n0.906 perplexity: 36.692 speed: 33820 wps\nEpoch: 13 Train Perplexity: 36.290\nEpoch: 13 Valid Perplexity: 123.468\nTest Perplexity: 117.972\n\nAcknowledgements: lots of discussions w/ ebrevdo!\n"}