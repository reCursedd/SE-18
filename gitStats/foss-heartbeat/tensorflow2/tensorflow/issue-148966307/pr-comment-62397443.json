{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62397443", "pull_request_review_id": null, "id": 62397443, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzk3NDQz", "diff_hunk": "@@ -478,3 +552,71 @@ def _make_tf_features(self, input_feat):\n                                   [-1, self._feature_size])\n       freq_inputs.append(cur_input)\n     return freq_inputs\n+\n+\n+class LSTMCellBlock(rnn_cell.RNNCell):\n+  \"\"\"Basic LSTM recurrent network cell.\n+\n+  The implementation is based on: http://arxiv.org/abs/1409.2329.\n+\n+  We add forget_bias (default: 1) to the biases of the forget gate in order to\n+  reduce the scale of forgetting in the beginning of the training.\n+\n+  It does not allow cell clipping, a projection layer, and does not\n+  use peep-hole connections: it is the basic baseline.\n+\n+  Unlike BasicLSTMCell, this is a monolithic op and should be much faster. The\n+  weight and bias matrixes should be compatible as long as the variabel scope\n+  matches.\n+  \"\"\"\n+\n+  def __init__(self, num_units, forget_bias=1.0, input_size=None, bprop_dx=True,\n+               parallel_dw=True):\n+    \"\"\"Initialize the basic LSTM cell.\n+\n+    Args:\n+      num_units: int, The number of units in the LSTM cell.\n+      forget_bias: float, The bias added to forget gates (see above).\n+    \"\"\"\n+    if input_size is not None:\n+      logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n+    self._num_units = num_units\n+    self._forget_bias = forget_bias\n+    self._bprop_dx = bprop_dx\n+    self._parallel_dw = parallel_dw\n+\n+  @property\n+  def state_size(self):\n+    return self._num_units * 2\n+\n+  @property\n+  def output_size(self):\n+    return self._num_units\n+\n+  def __call__(self, x, states_prev, scope=None):\n+    \"\"\"Long short-term memory cell (LSTM).\"\"\"\n+    with vs.variable_scope(scope or type(self).__name__):\n+      w = vs.get_variable(\"W\", [x.get_shape()[1] + self._num_units,", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 139, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "727e0c76bed212fdf240a221e577e1b1a86848d5", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "x_shape = x.get_shape().with_rank(2)\ncheck that x_shape[1] is not None or raise a ValueError.  same with w below.\n", "created_at": "2016-05-06T22:06:28Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62397443", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62397443"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62397443"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>x_shape = x.get_shape().with_rank(2)<br>\ncheck that x_shape[1] is not None or raise a ValueError.  same with w below.</p>", "body_text": "x_shape = x.get_shape().with_rank(2)\ncheck that x_shape[1] is not None or raise a ValueError.  same with w below."}