{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62925685", "pull_request_review_id": null, "id": 62925685, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyOTI1Njg1", "diff_hunk": "@@ -0,0 +1,187 @@\n+#include \"tensorflow/core/framework/op.h\"\n+\n+namespace tensorflow {\n+\n+REGISTER_OP(\"LSTMCellBlock\")\n+    .Attr(\"cell_size: int\")\n+    .Attr(\"forget_bias: float = 1.0\")\n+    .Attr(\"T: {float, double}\")\n+    .Input(\"x: T\")\n+    .Input(\"states_prev: T\")\n+    .Input(\"w: T\")\n+    .Input(\"b: T\")\n+    .Output(\"i: T\")\n+    .Output(\"cs: T\")\n+    .Output(\"f: T\")\n+    .Output(\"o: T\")\n+    .Output(\"ci: T\")\n+    .Output(\"co: T\")\n+    .Output(\"states: T\")\n+    .Output(\"h: T\")\n+    .Doc(R\"doc(\n+Computes the LSTM cell forward propagation for 1 time step.\n+\n+This implementation uses 1 weight matrix and 1 bias vector, there is no\n+diagonal peephole connection.\n+\n+This kernel op implements the following mathematical equations:\n+\n+```python\n+[cs_prev, h_prev] = states_prev\n+\n+xh = [x, h_prev]\n+[i, f, ci, o] = xh * w + b\n+f = f + forget_bias\n+\n+i = sigmoid(i)\n+f = sigmoid(f)\n+ci = tanh(ci)\n+o = sigmoid(o)\n+\n+cs = ci .* i + cs_prev .* f\n+co = tanh(cs)\n+\n+h = co .* o\n+states = [cs, h]\n+```\n+\n+cell_size: The LSTM cell size.\n+forget_bias: The forget gate bias.\n+x: The input to the LSTM cell.\n+states_prev: The previous LSTM state of [cs, h].\n+w: The weight matrix.\n+b: The bias vector.\n+i: The input gate.\n+cs: The cell state before the tanh.\n+f: The forget gate.\n+o: The output gate.\n+ci: The cell input.\n+co: The cell after the tanh.\n+states: The concatenation of [cs, h].\n+h: The output h vector.\n+)doc\");\n+\n+REGISTER_OP(\"LSTMCellBlockGrad\")\n+    .Attr(\"cell_size: int\")\n+    .Attr(\"T: {float, double}\")\n+    .Input(\"x: T\")\n+    .Input(\"states_prev: T\")\n+    .Input(\"w: T\")\n+    .Input(\"b: T\")\n+    .Input(\"i: T\")\n+    .Input(\"cs: T\")\n+    .Input(\"f: T\")\n+    .Input(\"o: T\")\n+    .Input(\"ci: T\")\n+    .Input(\"co: T\")\n+    .Input(\"h: T\")\n+    .Input(\"states_grad: T\")\n+    .Input(\"h_grad: T\")\n+    .Output(\"x_grad: T\")\n+    .Output(\"states_prev_grad: T\")\n+    .Output(\"dicfo: T\")\n+    .Output(\"xh: T\")\n+    .Doc(R\"doc(\n+Computes the LSTM cell backward propagation for 1 timestep.\n+\n+This implementation is to be used inconjunction of LSTMCellBlock.\n+\n+cell_size: The LSTM cell size.\n+x: The input to the LSTM cell.\n+states_prev: The previous LSTM state (it is a concatenated vector of c[t - 1]\n+  and h[t - 1].\n+w: The weight matrix.\n+b: The bias vector.\n+i: The input gate.\n+cs: The cell state before the tanh.\n+f: The forget gate.\n+o: The output gate.\n+ci: The cell input.\n+co: The cell after the tanh.\n+states: The concatenation of [cs, h].\n+h: The output h vector.\n+states_grad: The gradient of states vector.\n+h_grad: THe gradient of h vector.\n+x_grad: The gradient of x.\n+states_prev_grad: The gradient of states_prev.\n+dicfo: The derivative wrt to [i, cs, f, o].\n+xh: The concatenated vector of [x, h].\n+)doc\");\n+\n+REGISTER_OP(\"LSTMBlock\")\n+    .Attr(\"cell_size: int\")\n+    .Attr(\"forget_bias: float = 1.0\")\n+    .Attr(\"sequence_len_max: int\")\n+    .Attr(\"T: {float, double}\")\n+    .Input(\"sequence_len: int64\")\n+    .Input(\"initial_state: T\")\n+    .Input(\"x: sequence_len_max * T\")\n+    .Input(\"w: T\")\n+    .Input(\"b: T\")\n+    .Output(\"i: sequence_len_max * T\")\n+    .Output(\"cs: sequence_len_max * T\")\n+    .Output(\"f: sequence_len_max * T\")\n+    .Output(\"o: sequence_len_max * T\")\n+    .Output(\"ci: sequence_len_max * T\")\n+    .Output(\"co: sequence_len_max * T\")\n+    .Output(\"states: sequence_len_max * T\")\n+    .Output(\"h: sequence_len_max * T\")\n+    .Doc(R\"doc(\n+Computes the LSTM forward propagation for N time steps.\n+\n+This implementation uses 1 weight matrix and 1 bias vector, there is no\n+diagonal peephole connection. The computation of this op is dynamic as a\n+function of sequence_len. We compute N = max(sequence_len) timesteps.\n+\n+cell_size: The LSTM cell size.\n+forget_bias: The forget gate bias.\n+sequence_len: A vector of batch_size containing the sequence length.\n+initial_state: Initial state of the LSTM.\n+x: The list of inputs to the LSTM.\n+w: The weight matrix.\n+b: The bias vector.\n+h: The list of outputs h of the LSTM.\n+states: The list of states (it is the concatenated vector of [c, h]).\n+)doc\");\n+\n+REGISTER_OP(\"LSTMBlockGrad\")\n+    .Attr(\"cell_size: int\")\n+    .Attr(\"sequence_len_max: int\")\n+    .Attr(\"T: {float, double}\")\n+    .Input(\"sequence_len: int64\")\n+    .Input(\"initial_state: T\")\n+    .Input(\"x: sequence_len_max * T\")\n+    .Input(\"w: T\")\n+    .Input(\"b: T\")\n+    .Input(\"i: sequence_len_max * T\")\n+    .Input(\"cs: sequence_len_max * T\")\n+    .Input(\"f: sequence_len_max * T\")\n+    .Input(\"o: sequence_len_max * T\")\n+    .Input(\"ci: sequence_len_max * T\")\n+    .Input(\"co: sequence_len_max * T\")\n+    .Input(\"states: sequence_len_max * T\")\n+    .Input(\"h: sequence_len_max * T\")\n+    .Input(\"h_grad: sequence_len_max * T\")\n+    .Output(\"x_grad: sequence_len_max * T\")\n+    .Output(\"w_grad: T\")\n+    .Output(\"b_grad: T\")\n+    .Doc(R\"doc(\n+Computes the LSTM backward propagation for N time steps.\n+\n+This implementation is to be used inconjunction of LSTMBlock.", "path": "tensorflow/contrib/rnn/ops/lstm_ops.cc", "position": null, "original_position": 171, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "18163fc2a0428e15079a8399ed1622b3547471c9", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "body": "typo fixed, i also think ur not on the head of the branch.\n\n## \n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\n\nOn Wed, May 11, 2016 at 1:56 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\n> In tensorflow/contrib/rnn/ops/lstm_ops.cc\n> https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62924882:\n> \n> > -    .Input(\"i: sequence_len_max \\* T\")\n> > -    .Input(\"cs: sequence_len_max \\* T\")\n> > -    .Input(\"f: sequence_len_max \\* T\")\n> > -    .Input(\"o: sequence_len_max \\* T\")\n> > -    .Input(\"ci: sequence_len_max \\* T\")\n> > -    .Input(\"co: sequence_len_max \\* T\")\n> > -    .Input(\"states: sequence_len_max \\* T\")\n> > -    .Input(\"h: sequence_len_max \\* T\")\n> > -    .Input(\"h_grad: sequence_len_max \\* T\")\n> > -    .Output(\"x_grad: sequence_len_max \\* T\")\n> > -    .Output(\"w_grad: T\")\n> > -    .Output(\"b_grad: T\")\n> > -    .Doc(R\"doc(\n> >   +Computes the LSTM backward propagation for N time steps.\n> >   +\n> >   +This implementation is to be used inconjunction of LSTMBlock.\n> \n> in conjunction\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/pull/2002/files/18163fc2a0428e15079a8399ed1622b3547471c9#r62924882\n", "created_at": "2016-05-11T20:58:52Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62925685", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62925685"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62925685"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>typo fixed, i also think ur not on the head of the branch.</p>\n<h2></h2>\n<p>William Chan<br>\nCarnegie Mellon University<br>\n(650) 450-9455<br>\nwilliamchan.ca</p>\n<p>On Wed, May 11, 2016 at 1:56 PM, Vijay Vasudevan <a href=\"mailto:notifications@github.com\">notifications@github.com</a><br>\nwrote:</p>\n<blockquote>\n<p>In tensorflow/contrib/rnn/ops/lstm_ops.cc<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148966307\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2002\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/2002/hovercard?comment_id=62924882&amp;comment_type=review_comment\" href=\"https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62924882\">#2002 (comment)</a>:</p>\n<blockquote>\n<ul>\n<li>.Input(\"i: sequence_len_max * T\")</li>\n<li>.Input(\"cs: sequence_len_max * T\")</li>\n<li>.Input(\"f: sequence_len_max * T\")</li>\n<li>.Input(\"o: sequence_len_max * T\")</li>\n<li>.Input(\"ci: sequence_len_max * T\")</li>\n<li>.Input(\"co: sequence_len_max * T\")</li>\n<li>.Input(\"states: sequence_len_max * T\")</li>\n<li>.Input(\"h: sequence_len_max * T\")</li>\n<li>.Input(\"h_grad: sequence_len_max * T\")</li>\n<li>.Output(\"x_grad: sequence_len_max * T\")</li>\n<li>.Output(\"w_grad: T\")</li>\n<li>.Output(\"b_grad: T\")</li>\n<li>.Doc(R\"doc(<br>\n+Computes the LSTM backward propagation for N time steps.</li>\n</ul>\n<ul>\n<li></li>\n</ul>\n<p>+This implementation is to be used inconjunction of LSTMBlock.</p>\n</blockquote>\n<p>in conjunction</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub<br>\n<a href=\"https://github.com/tensorflow/tensorflow/pull/2002/files/18163fc2a0428e15079a8399ed1622b3547471c9#r62924882\">https://github.com/tensorflow/tensorflow/pull/2002/files/18163fc2a0428e15079a8399ed1622b3547471c9#r62924882</a></p>\n</blockquote>", "body_text": "typo fixed, i also think ur not on the head of the branch.\n\nWilliam Chan\nCarnegie Mellon University\n(650) 450-9455\nwilliamchan.ca\nOn Wed, May 11, 2016 at 1:56 PM, Vijay Vasudevan notifications@github.com\nwrote:\n\nIn tensorflow/contrib/rnn/ops/lstm_ops.cc\n#2002 (comment):\n\n\n.Input(\"i: sequence_len_max * T\")\n.Input(\"cs: sequence_len_max * T\")\n.Input(\"f: sequence_len_max * T\")\n.Input(\"o: sequence_len_max * T\")\n.Input(\"ci: sequence_len_max * T\")\n.Input(\"co: sequence_len_max * T\")\n.Input(\"states: sequence_len_max * T\")\n.Input(\"h: sequence_len_max * T\")\n.Input(\"h_grad: sequence_len_max * T\")\n.Output(\"x_grad: sequence_len_max * T\")\n.Output(\"w_grad: T\")\n.Output(\"b_grad: T\")\n.Doc(R\"doc(\n+Computes the LSTM backward propagation for N time steps.\n\n\n\n\n+This implementation is to be used inconjunction of LSTMBlock.\n\nin conjunction\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\nhttps://github.com/tensorflow/tensorflow/pull/2002/files/18163fc2a0428e15079a8399ed1622b3547471c9#r62924882"}