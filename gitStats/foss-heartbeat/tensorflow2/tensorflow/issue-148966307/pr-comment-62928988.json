{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62928988", "pull_request_review_id": null, "id": 62928988, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyOTI4OTg4", "diff_hunk": "@@ -0,0 +1,310 @@\n+#ifndef TENSORFLOW_KERNELS_LSTM_OPS_H_\n+#define TENSORFLOW_KERNELS_LSTM_OPS_H_\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/lib/core/blocking_counter.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+namespace perftools {\n+namespace gputools {\n+class Stream;\n+}  // end namespace gputools\n+}  // end namespace perftools\n+\n+namespace tensorflow {\n+class OpKernelContext;\n+\n+namespace functor {\n+\n+template <typename Device, typename T>\n+struct TensorMemZero {\n+  void operator()(const Device& d, typename TTypes<T>::Vec x) {\n+    x.device(d) = x.constant(0);\n+  }\n+};\n+\n+template <typename Device, typename T>\n+struct TensorMemCopy {\n+  void operator()(const Device& d, typename TTypes<T>::ConstVec in,\n+                  typename TTypes<T>::Vec out) {\n+    out.device(d) = in;\n+  }\n+};\n+\n+template <typename T>\n+struct TensorCuBlasGemm {\n+  void operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      bool transa, bool transb, uint64 m, uint64 n, uint64 k, T alpha,\n+      const T* a, int lda, const T* b, int ldb, T beta, T *c, int ldc);\n+};\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+struct TensorBlasGemm;\n+\n+template <typename Device, typename T>\n+struct TensorBlasGemm<Device, T, true /* USE_CUBLAS */> {\n+  static void compute(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, bool transa, bool transb, T alpha,\n+      typename TTypes<T>::ConstMatrix a, typename TTypes<T>::ConstMatrix b,\n+      T beta, typename TTypes<T>::Matrix c) {\n+    int64 m = c.dimensions()[0];\n+    int64 n = c.dimensions()[1];\n+    int64 k = transa ? a.dimensions()[0] : a.dimensions()[1];\n+\n+    TensorCuBlasGemm<T>()(\n+        ctx, stream, transb, transa, n, m, k, alpha, b.data(),\n+        transb ? k : n, a.data(), transa ? m : k, beta, c.data(), n);\n+  }\n+};\n+\n+template <typename Device, typename T>\n+struct TensorBlasGemm<Device, T, false /* USE_CUBLAS */ > {\n+  static void compute(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, bool transa, bool transb, T alpha,\n+      typename TTypes<T>::ConstMatrix a, typename TTypes<T>::ConstMatrix b,\n+      T beta, typename TTypes<T>::Matrix c) {\n+    Eigen::array<Eigen::IndexPair<Eigen::DenseIndex>, 1> contract_pairs;\n+    contract_pairs[0] = Eigen::IndexPair<Eigen::DenseIndex>(\n+        transa == false, transb == true);\n+    if (alpha == T(1) && beta == T(0)) {\n+      c.device(d) = a.contract(b, contract_pairs);\n+    } else if (alpha == T(1) && beta == T(1)) {\n+      c.device(d) += a.contract(b, contract_pairs);\n+    } else {\n+      c.device(d) = c.constant(alpha) * a.contract(b, contract_pairs) +\n+                    c.constant(beta) * c;\n+    }\n+  }\n+};\n+\n+struct LSTMCellBlock {\n+  LSTMCellBlock(const int batch_size, const int input_size, const int cell_size)\n+    : batch_size_(batch_size), input_size_(input_size), cell_size_(cell_size) {}\n+\n+  Eigen::array<int, 2> icfo_i_offsets() const {\n+    return {0, cell_size_ * 0};\n+  }\n+\n+  Eigen::array<int, 2> icfo_c_offsets() const {\n+    return {0, cell_size_ * 1};\n+  }\n+\n+  Eigen::array<int, 2> icfo_f_offsets() const {\n+    return {0, cell_size_ * 2};\n+  }\n+\n+  Eigen::array<int, 2> icfo_o_offsets() const {\n+    return {0, cell_size_ * 3};\n+  }\n+\n+  Eigen::array<int, 2> states_cs_offsets() const {\n+    return {0, 0};\n+  }\n+\n+  Eigen::array<int, 2> states_h_offsets() const {\n+    return {0, cell_size_};\n+  }\n+\n+  Eigen::array<int, 2> cell_extents() const {\n+    return {batch_size_, cell_size_};\n+  }\n+\n+  Eigen::array<int, 2> xh_x_offsets() const {\n+    return {0, 0};\n+  }\n+\n+  Eigen::array<int, 2> xh_x_extents() const {\n+    return {batch_size_, input_size_};\n+  }\n+\n+  Eigen::array<int, 2> xh_h_offsets() const {\n+    return {0, input_size_};\n+  }\n+\n+  Eigen::array<int, 2> xh_h_extents() const {\n+    return {batch_size_, cell_size_};\n+  }\n+\n+ protected:\n+  const int batch_size_;\n+  const int input_size_;\n+  const int cell_size_;\n+};\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+struct LSTMCellBlockFprop : public LSTMCellBlock {\n+  LSTMCellBlockFprop(const int batch_size, const int input_size,\n+                     const int cell_size)\n+    : LSTMCellBlock(batch_size, input_size, cell_size) {}\n+\n+  void operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const Device& d, const T forget_bias, typename TTypes<T>::ConstMatrix x,\n+      typename TTypes<T>::ConstMatrix states_prev,\n+      typename TTypes<T>::ConstMatrix w, typename TTypes<T>::ConstVec b,\n+      typename TTypes<T>::Matrix cs_prev, typename TTypes<T>::Matrix h_prev,\n+      typename TTypes<T>::Matrix xh, typename TTypes<T>::Matrix i,\n+      typename TTypes<T>::Matrix cs, typename TTypes<T>::Matrix f,\n+      typename TTypes<T>::Matrix o, typename TTypes<T>::Matrix ci,\n+      typename TTypes<T>::Matrix co, typename TTypes<T>::Matrix icfo,\n+      typename TTypes<T>::Matrix states, typename TTypes<T>::Matrix h) {\n+    // [cs, h] = states_prev\n+    cs_prev.device(d) =\n+        states_prev.slice(states_cs_offsets(), cell_extents());\n+\n+    h_prev.device(d) =\n+        states_prev.slice(states_h_offsets(), cell_extents());\n+\n+    // Concat xh = [x, h].\n+    xh.slice(xh_x_offsets(), xh_x_extents()).device(d) = x;\n+    xh.slice(xh_h_offsets(), xh_h_extents()).device(d) = h_prev;\n+\n+    // states1 = xh * w + b\n+    typename TTypes<T>::ConstMatrix const_xh(xh.data(), xh.dimensions());\n+    TensorBlasGemm<Device, T, USE_CUBLAS>::compute(\n+        ctx, stream, d, false, false, T(1), const_xh, w, T(0), icfo);\n+    icfo.device(d) +=\n+        b.broadcast(Eigen::array<int, 2>({batch_size_, 1}));\n+\n+    // Input gate.\n+    i.device(d) = icfo.slice(icfo_i_offsets(), cell_extents()).sigmoid();\n+\n+    // Cell input.\n+    ci.device(d) = icfo.slice(icfo_c_offsets(), cell_extents()).tanh();\n+\n+    // Forget gate (w/ bias).\n+    f.device(d) =\n+        (icfo.slice(icfo_f_offsets(), cell_extents()) + f.constant(forget_bias))\n+        .sigmoid();\n+\n+    // cs = ci .* i + f .* cs_prev\n+    cs.device(d) = i * ci + f * cs_prev;\n+\n+    // co = tanh(cs)\n+    co.device(d) = cs.tanh();\n+\n+    // Output gate.\n+    o.device(d) = icfo.slice(icfo_o_offsets(), cell_extents()).sigmoid();\n+\n+    // h = o .* co\n+    h.device(d) = o * co;\n+\n+    states.slice(states_cs_offsets(), cell_extents()).device(d) = cs;\n+    states.slice(states_h_offsets(), cell_extents()).device(d) = h;\n+  }\n+};\n+\n+template <typename Device, typename T, bool USE_CUBLAS>\n+struct LSTMCellBlockBprop : public LSTMCellBlock {\n+  LSTMCellBlockBprop(const int batch_size, const int input_size,\n+                     const int cell_size)\n+    : LSTMCellBlock(batch_size, input_size, cell_size) {}\n+\n+  void operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,", "path": "tensorflow/contrib/rnn/kernels/lstm_ops.h", "position": 124, "original_position": 210, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "fc40971bba82762fe413b5e7d6fe12a09722876b", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "may want to pass the ConstMatrix as `const... &`  (even though they're lightweight, Matrices are still not POD objects)\n", "created_at": "2016-05-11T21:19:36Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62928988", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62928988"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62928988"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>may want to pass the ConstMatrix as <code>const... &amp;</code>  (even though they're lightweight, Matrices are still not POD objects)</p>", "body_text": "may want to pass the ConstMatrix as const... &  (even though they're lightweight, Matrices are still not POD objects)"}