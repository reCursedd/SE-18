{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/60257110", "pull_request_review_id": null, "id": 60257110, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYwMjU3MTEw", "diff_hunk": "@@ -0,0 +1,522 @@\n+#define EIGEN_USE_THREADS\n+\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+#endif  // GOOGLE_CUDA\n+\n+#include \"tensorflow/core/kernels/lstm_ops.h\"\n+\n+#include <memory>\n+#include <vector>\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+\n+\n+namespace tensorflow {\n+\n+#if GOOGLE_CUDA\n+\n+namespace {\n+perftools::gputools::DeviceMemory<float> AsDeviceMemory(const float* cuda_memory) {\n+  perftools::gputools::DeviceMemoryBase wrapped(const_cast<float*>(cuda_memory));\n+  perftools::gputools::DeviceMemory<float> typed(wrapped);\n+  return typed;\n+}\n+\n+void TensorMemZero(Tensor* tensor, perftools::gputools::Stream* stream) {\n+  auto ptr = AsDeviceMemory(tensor->flat<float>().data());\n+  if (stream) {\n+    CHECK(stream->ThenMemZero(&ptr, tensor->TotalBytes()).ok());\n+  } else {\n+    std::memset(tensor->flat<float>().data(), 0, tensor->TotalBytes());\n+  }\n+}\n+}  // namespace\n+\n+#endif  // GOOGLE_CUDA\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMCellBlockOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    CHECK_EQ(states_prev_tensor->dim_size(0), batch_size);\n+    CHECK_EQ(states_prev_tensor->dim_size(1), cell_size_ * 7);\n+\n+    // CHECK_EQ(w_tensor->dim_size(0), input_size + cell_size_);\n+    // CHECK_EQ(w_tensor->dim_size(1), cell_size_ * 4);\n+\n+    CHECK_EQ(b_tensor->dim_size(0), cell_size_ * 4);\n+\n+    Tensor* h_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"h\",\n+          TensorShape({batch_size, cell_size_}), &h_tensor));\n+\n+    // Allocate our output matrices.\n+    Tensor* states_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"states\",\n+        TensorShape({batch_size, cell_size_ * 7}), &states_tensor));\n+\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_tensor));\n+\n+    functor::LSTMCellBlockFprop<Device, USE_CUBLAS>()(\n+        stream, ctx->eigen_device<Device>(),\n+        batch_size, input_size, cell_size_, forget_bias_,\n+        x_tensor->matrix<float>(),\n+        xh_tensor.matrix<float>(), states_prev_tensor->matrix<float>(),\n+        w_tensor->matrix<float>(), b_tensor->vec<float>(),\n+        h_tensor->matrix<float>(), states_tensor->matrix<float>());\n+  }\n+\n+ private:\n+  int64 cell_size_;\n+  float forget_bias_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")    \\\n+                            .Device(DEVICE_CPU),\n+                        LSTMCellBlockOp<CPUDevice, false>);\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+  template <>\n+  void LSTMCellBlockFprop<GPUDevice, true>::operator()(\n+      perftools::gputools::Stream* stream, const GPUDevice& d, const int batch_size, const int input_size,\n+      const int cell_size, const float forget_bias,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::Matrix xh,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w,\n+      typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::Matrix h,\n+      typename TTypes<float>::Matrix states);\n+  extern template struct LSTMCellBlockFprop<GPUDevice, true>;\n+}  // end namespace functor\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")     \\\n+                            .Device(DEVICE_GPU),  \\\n+                        LSTMCellBlockOp<GPUDevice, true>);\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMCellBlockGradOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const Tensor* states_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states\", &states_tensor));\n+\n+    const Tensor* h_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad_tensor));\n+\n+    const Tensor* states_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_grad\", &states_grad_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    CHECK_EQ(states_prev_tensor->dim_size(0), batch_size);\n+    CHECK_EQ(states_prev_tensor->dim_size(1), cell_size_ * 7);\n+\n+    // CHECK_EQ(w_tensor->dim_size(0), input_size + cell_size_);\n+    // CHECK_EQ(w_tensor->dim_size(1), cell_size_ * 4);\n+\n+    CHECK_EQ(b_tensor->dim_size(0), cell_size_ * 4);", "path": "tensorflow/core/kernels/lstm_ops.cc", "position": null, "original_position": 178, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "8f3abddb142c42d3d46b94904ef72f4215a96659", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "These should be soft errors, no?  OP_REQUIRES(..., etc)?\n", "created_at": "2016-04-19T15:48:13Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r60257110", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/60257110"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r60257110"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>These should be soft errors, no?  OP_REQUIRES(..., etc)?</p>", "body_text": "These should be soft errors, no?  OP_REQUIRES(..., etc)?"}