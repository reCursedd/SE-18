{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62398885", "pull_request_review_id": null, "id": 62398885, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYyMzk4ODg1", "diff_hunk": "@@ -0,0 +1,950 @@\n+#define EIGEN_USE_THREADS\n+\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+#endif  // GOOGLE_CUDA\n+\n+#include \"tensorflow/contrib/rnn/kernels/lstm_ops.h\"\n+\n+#include <memory>\n+#include <vector>\n+\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+#if GOOGLE_CUDA\n+#include \"tensorflow/stream_executor/stream.h\"\n+#endif  // GOOGLE_CUDA\n+\n+\n+namespace tensorflow {\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+#if GOOGLE_CUDA\n+\n+namespace {\n+template <typename T>\n+perftools::gputools::DeviceMemory<T> AsDeviceMemory(const T* cuda_memory) {\n+  perftools::gputools::DeviceMemoryBase wrapped(const_cast<T*>(cuda_memory));\n+  perftools::gputools::DeviceMemory<T> typed(wrapped);\n+  return typed;\n+}\n+}  // namespace\n+\n+#endif  // GOOGLE_CUDA\n+\n+void CuBlasGemm(\n+    OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+    bool transa, bool transb, uint64 m, uint64 n, uint64 k, float alpha,\n+    const float* a, int lda, const float* b, int ldb, float beta, float *c,\n+    int ldc) {\n+#if GOOGLE_CUDA\n+  perftools::gputools::blas::Transpose trans[] = {\n+      perftools::gputools::blas::Transpose::kNoTranspose,\n+      perftools::gputools::blas::Transpose::kTranspose};\n+\n+  auto a_ptr = AsDeviceMemory(a);\n+  auto b_ptr = AsDeviceMemory(b);\n+  auto c_ptr = AsDeviceMemory(c);\n+\n+  bool blas_launch_status = stream->ThenBlasGemm(\n+      trans[transa], trans[transb], m, n, k, alpha, a_ptr, lda, b_ptr, ldb,\n+      beta, &c_ptr, ldc).ok();\n+  OP_REQUIRES(ctx, blas_launch_status, errors::Aborted(\"CuBlasGemm failed!\"));\n+#else\n+  ctx->SetStatus(errors::InvalidArgument(\"CuBlasGemm needs CUDA.\"));\n+#endif\n+}\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMCellBlockOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+    const int64 states_size = cell_size_ * 2;\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_prev.dims(0) != batch_size: \",\n+                                        states_prev_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(1) == states_size,\n+                errors::InvalidArgument(\"states_prev.dims(1) != cell_size * 7: \",\n+                                        states_prev_tensor->dim_size(1),\n+                                        \" vs. \", states_size));\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+        errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                w_tensor->dim_size(0), \" vs. \",\n+                                input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    // Allocate our output tensors.\n+    Tensor* i_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"i\",\n+          TensorShape({batch_size, cell_size_}), &i_tensor));\n+\n+    Tensor* cs_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"cs\",\n+          TensorShape({batch_size, cell_size_}), &cs_tensor));\n+\n+    Tensor* f_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"f\",\n+          TensorShape({batch_size, cell_size_}), &f_tensor));\n+\n+    Tensor* o_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"o\",\n+          TensorShape({batch_size, cell_size_}), &o_tensor));\n+\n+    Tensor* ci_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"ci\",\n+          TensorShape({batch_size, cell_size_}), &ci_tensor));\n+\n+    Tensor* co_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"co\",\n+          TensorShape({batch_size, cell_size_}), &co_tensor));\n+\n+    Tensor* states_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"states\",\n+        TensorShape({batch_size, states_size}), &states_tensor));\n+\n+    Tensor* h_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"h\",\n+          TensorShape({batch_size, cell_size_}), &h_tensor));\n+\n+    // Allocate our temp tensors.\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_tensor));\n+\n+    Tensor cs_prev_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &cs_prev_tensor));\n+\n+    Tensor h_prev_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &h_prev_tensor));\n+\n+    Tensor icfo_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_ * 4}), &icfo_tensor));\n+\n+    functor::LSTMCellBlockFprop<Device, USE_CUBLAS>(\n+        batch_size, input_size, cell_size_)(\n+        ctx, stream, ctx->eigen_device<Device>(), forget_bias_,\n+        x_tensor->matrix<float>(), states_prev_tensor->matrix<float>(),\n+        w_tensor->matrix<float>(), b_tensor->vec<float>(),\n+        cs_prev_tensor.matrix<float>(), h_prev_tensor.matrix<float>(),\n+        xh_tensor.matrix<float>(),\n+        i_tensor->matrix<float>(), cs_tensor->matrix<float>(),\n+        f_tensor->matrix<float>(), o_tensor->matrix<float>(),\n+        ci_tensor->matrix<float>(), co_tensor->matrix<float>(),\n+        icfo_tensor.matrix<float>(),\n+        states_tensor->matrix<float>(), h_tensor->matrix<float>());\n+  }\n+\n+ private:\n+  int64 cell_size_;\n+  float forget_bias_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")     \\\n+                            .Device(DEVICE_CPU),  \\\n+                        LSTMCellBlockOp<CPUDevice, false>);\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+  template <>\n+  void TensorMemZero<GPUDevice, float>::operator()(\n+      const GPUDevice& d, typename TTypes<float>::Vec x);\n+\n+  template <>\n+  void TensorMemCopy<GPUDevice, float>::operator()(\n+      const GPUDevice& d, typename TTypes<float>::ConstVec in,\n+      typename TTypes<float>::Vec out);\n+\n+  template <>\n+  void LSTMCellBlockFprop<GPUDevice, true>::operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const GPUDevice& d, const float forget_bias,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w,\n+      typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::Matrix cs_prev,\n+      typename TTypes<float>::Matrix h_prev,\n+      typename TTypes<float>::Matrix xh,\n+      typename TTypes<float>::Matrix i,\n+      typename TTypes<float>::Matrix cs,\n+      typename TTypes<float>::Matrix f,\n+      typename TTypes<float>::Matrix o,\n+      typename TTypes<float>::Matrix ci,\n+      typename TTypes<float>::Matrix co,\n+      typename TTypes<float>::Matrix icfo,\n+      typename TTypes<float>::Matrix states,\n+      typename TTypes<float>::Matrix h);\n+\n+  extern template struct TensorMemZero<GPUDevice, float>;\n+  extern template struct TensorMemCopy<GPUDevice, float>;\n+  extern template struct LSTMCellBlockFprop<GPUDevice, true>;\n+}  // end namespace functor\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")     \\\n+                            .Device(DEVICE_GPU),  \\\n+                        LSTMCellBlockOp<GPUDevice, true>);\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMCellBlockGradOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"bprop_dx\", &bprop_dx_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"parallel_dw\", &parallel_dw_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const Tensor* i_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"i\", &i_tensor));\n+\n+    const Tensor* cs_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"cs\", &cs_tensor));\n+\n+    const Tensor* f_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"f\", &f_tensor));\n+\n+    const Tensor* o_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"o\", &o_tensor));\n+\n+    const Tensor* ci_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"ci\", &ci_tensor));\n+\n+    const Tensor* co_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"co\", &co_tensor));\n+\n+    const Tensor* h_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"h\", &h_tensor));\n+\n+    const Tensor* h_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad_tensor));\n+\n+    const Tensor* states_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_grad\", &states_grad_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+    const int64 states_size = cell_size_ * 2;\n+\n+    const Device& device = ctx->eigen_device<Device>();\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_prev.dims(0) != batch_size: \",\n+                                        states_prev_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(1) == states_size,\n+        errors::InvalidArgument(\"states_prev.dims(1) != cell_size * 2: \",\n+                                states_prev_tensor->dim_size(1),\n+                                \" vs. \", states_size));\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+        errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                w_tensor->dim_size(0), \" vs. \",\n+                                input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, i_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"i.dim_size(0) != batch_size: \",\n+                                        i_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, i_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"i.dim_size(1) != cell_size_: \",\n+                                        i_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, cs_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"cs.dim_size(0) != batch_size: \",\n+                                        cs_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, cs_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"cs.dim_size(1) != cell_size_: \",\n+                                        cs_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, f_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"f.dim_size(0) != batch_size: \",\n+                                        f_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, f_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"i.dim_size(1) != cell_size_: \",\n+                                        f_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, o_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"o.dim_size(0) != batch_size: \",\n+                                        o_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, o_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"o.dim_size(1) != cell_size_: \",\n+                                        o_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, ci_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"ci.dim_size(0) != batch_size: \",\n+                                        ci_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, ci_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"ci.dim_size(1) != cell_size_: \",\n+                                        ci_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, co_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"co.dim_size(0) != batch_size: \",\n+                                        co_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, co_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"co.dim_size(1) != cell_size_: \",\n+                                        co_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, h_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"h.dim_size(0) != batch_size: \",\n+                                        h_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, h_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"h.dim_size(1) != cell_size_: \",\n+                                        h_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, h_grad_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"h_grad_tensor.dims(0) != batch_size: \",\n+                                        h_grad_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, h_grad_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"h_grad_tensor.dims(1) != state_size: \",\n+                                        h_grad_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, states_grad_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_grad.dims(0) != batch_size: \",\n+                                        states_grad_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_grad_tensor->dim_size(1) == states_size,\n+        errors::InvalidArgument(\"states_grad.dims(1) != cell_size * 2: \",\n+                                states_grad_tensor->dim_size(1),\n+                                \" vs. \", states_size));\n+\n+    // Allocate our output tensors.\n+    Tensor* x_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"x_grad\",\n+        TensorShape({batch_size, input_size}), &x_grad_tensor));\n+\n+    Tensor* states_prev_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"states_prev_grad\",\n+        TensorShape({batch_size, states_size}), &states_prev_grad_tensor));\n+\n+    Tensor* w_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"w_grad\",\n+        TensorShape({input_size + cell_size_, cell_size_ * 4}),\n+        &w_grad_tensor));\n+    functor::TensorMemZero<Device, float>()(\n+        device, w_grad_tensor->flat<float>());\n+\n+    Tensor* b_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"b_grad\",\n+          TensorShape({cell_size_ * 4}), &b_grad_tensor));\n+    functor::TensorMemZero<Device, float>()(\n+        device, b_grad_tensor->flat<float>());\n+\n+    Tensor* dicfo_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"dicfo\",\n+        TensorShape({batch_size, cell_size_ * 4}), &dicfo_tensor));\n+\n+    Tensor* xh_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"xh\",\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_tensor));\n+\n+    // Allocate our temp tensors.\n+    Tensor cs_prev_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &cs_prev_tensor));\n+\n+    Tensor states_c_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &states_c_grad_tensor));\n+\n+    Tensor states_h_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &states_h_grad_tensor));\n+\n+    Tensor xh_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_grad_tensor));\n+\n+    Tensor dh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &dh_tensor));\n+\n+    Tensor do_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &do_tensor));\n+\n+    Tensor dcs_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &dcs_tensor));\n+\n+    Tensor dci_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &dci_tensor));\n+\n+    Tensor df_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &df_tensor));\n+\n+    Tensor di_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, cell_size_}), &di_tensor));\n+\n+    functor::LSTMCellBlockBprop<Device, USE_CUBLAS>(\n+        batch_size, input_size, cell_size_)(\n+        ctx, stream, device, bprop_dx_, parallel_dw_, x_tensor->matrix<float>(),\n+        states_prev_tensor->matrix<float>(), w_tensor->matrix<float>(),\n+        b_tensor->vec<float>(), i_tensor->matrix<float>(),\n+        cs_tensor->matrix<float>(), f_tensor->matrix<float>(),\n+        o_tensor->matrix<float>(), ci_tensor->matrix<float>(),\n+        co_tensor->matrix<float>(), h_tensor->matrix<float>(),\n+        states_grad_tensor->matrix<float>(), h_grad_tensor->matrix<float>(),\n+        cs_prev_tensor.matrix<float>(), states_c_grad_tensor.matrix<float>(),\n+        states_h_grad_tensor.matrix<float>(), xh_tensor->matrix<float>(),\n+        xh_grad_tensor.matrix<float>(), x_grad_tensor->matrix<float>(),\n+        dh_tensor.matrix<float>(), do_tensor.matrix<float>(),\n+        dcs_tensor.matrix<float>(), dci_tensor.matrix<float>(),\n+        df_tensor.matrix<float>(), di_tensor.matrix<float>(),\n+        dicfo_tensor->matrix<float>(), states_prev_grad_tensor->matrix<float>(),\n+        w_grad_tensor->matrix<float>(), b_grad_tensor->vec<float>());\n+  }\n+\n+ protected:\n+  int64 cell_size_;\n+  bool bprop_dx_;\n+  bool parallel_dw_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlockGrad\")    \\\n+                            .Device(DEVICE_CPU),\n+                        LSTMCellBlockGradOp<CPUDevice, false>);\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+  template <>\n+  void LSTMCellBlockBprop<GPUDevice, true>::operator()(\n+      OpKernelContext* ctx, perftools::gputools::Stream* stream,\n+      const GPUDevice& d, bool bprop_dx, bool parallel_dw,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w, typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::ConstMatrix i,\n+      typename TTypes<float>::ConstMatrix cs,\n+      typename TTypes<float>::ConstMatrix f,\n+      typename TTypes<float>::ConstMatrix o,\n+      typename TTypes<float>::ConstMatrix ci,\n+      typename TTypes<float>::ConstMatrix co,\n+      typename TTypes<float>::ConstMatrix h,\n+      typename TTypes<float>::ConstMatrix states_grad,\n+      typename TTypes<float>::ConstMatrix h_grad,\n+      typename TTypes<float>::Matrix cs_prev,\n+      typename TTypes<float>::Matrix states_c_grad,\n+      typename TTypes<float>::Matrix states_h_grad,\n+      typename TTypes<float>::Matrix xh, typename TTypes<float>::Matrix xh_grad,\n+      typename TTypes<float>::Matrix x_grad, typename TTypes<float>::Matrix dh,\n+      typename TTypes<float>::Matrix do_, typename TTypes<float>::Matrix dcs,\n+      typename TTypes<float>::Matrix dci, typename TTypes<float>::Matrix df,\n+      typename TTypes<float>::Matrix di, typename TTypes<float>::Matrix dicfo,\n+      typename TTypes<float>::Matrix states_prev_grad,\n+      typename TTypes<float>::Matrix w_grad,\n+      typename TTypes<float>::Vec b_grad);\n+\n+  extern template struct LSTMCellBlockBprop<GPUDevice, true>;\n+}  // namespace functor\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlockGrad\")  \\\n+                            .Device(DEVICE_GPU),   \\\n+                        LSTMCellBlockGradOp<GPUDevice, true>);\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMBlockOp : public OpKernel {", "path": "tensorflow/contrib/rnn/kernels/lstm_ops.cc", "position": null, "original_position": 536, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "727e0c76bed212fdf240a221e577e1b1a86848d5", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "oh i see what you're doing here.  this actually could be made more general by accepting a 3-tensor for inputs and emitting a 3-tensor for outputs and a 2-tensor for final_state.\n\nby accepting a list of length sequence_length, you're not allowing the per-step input lengths to change; so minibatches that are shorter still have to allocate a ton of space.\n\ninstead, accept a 3-tensor for x with dimensions `[batch, max_length, depth]` or `[max_length, batch, depth]` (whichever leads to fastest execution).  this is how `dynamic_rnn` works.  Now your code will work well with the PaddingFIFOQueue (and tf.input.batch, etc)\n", "created_at": "2016-05-06T22:21:11Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62398885", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/62398885"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r62398885"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>oh i see what you're doing here.  this actually could be made more general by accepting a 3-tensor for inputs and emitting a 3-tensor for outputs and a 2-tensor for final_state.</p>\n<p>by accepting a list of length sequence_length, you're not allowing the per-step input lengths to change; so minibatches that are shorter still have to allocate a ton of space.</p>\n<p>instead, accept a 3-tensor for x with dimensions <code>[batch, max_length, depth]</code> or <code>[max_length, batch, depth]</code> (whichever leads to fastest execution).  this is how <code>dynamic_rnn</code> works.  Now your code will work well with the PaddingFIFOQueue (and tf.input.batch, etc)</p>", "body_text": "oh i see what you're doing here.  this actually could be made more general by accepting a 3-tensor for inputs and emitting a 3-tensor for outputs and a 2-tensor for final_state.\nby accepting a list of length sequence_length, you're not allowing the per-step input lengths to change; so minibatches that are shorter still have to allocate a ton of space.\ninstead, accept a 3-tensor for x with dimensions [batch, max_length, depth] or [max_length, batch, depth] (whichever leads to fastest execution).  this is how dynamic_rnn works.  Now your code will work well with the PaddingFIFOQueue (and tf.input.batch, etc)"}