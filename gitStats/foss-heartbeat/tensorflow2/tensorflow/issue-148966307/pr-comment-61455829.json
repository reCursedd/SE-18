{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/61455829", "pull_request_review_id": null, "id": 61455829, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDYxNDU1ODI5", "diff_hunk": "@@ -0,0 +1,599 @@\n+#define EIGEN_USE_THREADS\n+\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+#endif  // GOOGLE_CUDA\n+\n+#include \"tensorflow/contrib/rnn/kernels/lstm_ops.h\"\n+\n+#include <memory>\n+#include <vector>\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/macros.h\"\n+\n+\n+namespace tensorflow {\n+\n+namespace {\n+#if GOOGLE_CUDA\n+perftools::gputools::DeviceMemory<float> AsDeviceMemory(const float* cuda_memory) {\n+  perftools::gputools::DeviceMemoryBase wrapped(const_cast<float*>(cuda_memory));\n+  perftools::gputools::DeviceMemory<float> typed(wrapped);\n+  return typed;\n+}\n+#endif  // GOOGLE_CUDA\n+\n+void TensorMemZero(Tensor* tensor, perftools::gputools::Stream* stream) {\n+#if GOOGLE_CUDA\n+  auto ptr = AsDeviceMemory(tensor->flat<float>().data());\n+  if (stream) {\n+    CHECK(stream->ThenMemZero(&ptr, tensor->TotalBytes()).ok());\n+  } else {\n+#endif  // GOOGLE_CUDA\n+    std::memset(tensor->flat<float>().data(), 0, tensor->TotalBytes());\n+#if GOOGLE_CUDA\n+  }\n+#endif  // GOOGLE_CUDA\n+}\n+}  // namespace\n+\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMCellBlockOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+    const int64 state_size = cell_size_ * 7;\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_prev.dims(0) != batch_size: \",\n+                                        states_prev_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(1) == state_size,\n+                errors::InvalidArgument(\"states_prev.dims(1) != state_size: \",\n+                                        states_prev_tensor->dim_size(1),\n+                                        \" vs. \", state_size));\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+                errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                        w_tensor->dim_size(0),\n+                                        \" vs. \", input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    Tensor* h_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"h\",\n+          TensorShape({batch_size, cell_size_}), &h_tensor));\n+\n+    // Allocate our output matrices.\n+    Tensor* states_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"states\",\n+        TensorShape({batch_size, state_size}), &states_tensor));\n+\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_tensor));\n+\n+    functor::LSTMCellBlockFprop<Device, USE_CUBLAS>()(\n+        stream, ctx->eigen_device<Device>(),\n+        batch_size, input_size, cell_size_, forget_bias_,\n+        x_tensor->matrix<float>(),\n+        xh_tensor.matrix<float>(), states_prev_tensor->matrix<float>(),\n+        w_tensor->matrix<float>(), b_tensor->vec<float>(),\n+        h_tensor->matrix<float>(), states_tensor->matrix<float>());\n+  }\n+\n+ private:\n+  int64 cell_size_;\n+  float forget_bias_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")    \\\n+                            .Device(DEVICE_CPU),\n+                        LSTMCellBlockOp<CPUDevice, false>);\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+  template <>\n+  void LSTMCellBlockFprop<GPUDevice, true>::operator()(\n+      perftools::gputools::Stream* stream, const GPUDevice& d, const int batch_size, const int input_size,\n+      const int cell_size, const float forget_bias,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::Matrix xh,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w,\n+      typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::Matrix h,\n+      typename TTypes<float>::Matrix states);\n+  extern template struct LSTMCellBlockFprop<GPUDevice, true>;\n+}  // end namespace functor\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlock\")     \\\n+                            .Device(DEVICE_GPU),  \\\n+                        LSTMCellBlockOp<GPUDevice, true>);\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMCellBlockGradOp : public OpKernel {\n+ public:\n+  explicit LSTMCellBlockGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* x_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"x\", &x_tensor));\n+\n+    const Tensor* states_prev_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_prev\", &states_prev_tensor));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    const Tensor* states_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states\", &states_tensor));\n+\n+    const Tensor* h_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"h_grad\", &h_grad_tensor));\n+\n+    const Tensor* states_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"states_grad\", &states_grad_tensor));\n+\n+    const int64 batch_size = x_tensor->dim_size(0);\n+    const int64 input_size = x_tensor->dim_size(1);\n+    const int64 state_size = cell_size_ * 7;\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    // Sanity checks for our input shapes.\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_prev.dims(0) != batch_size: \",\n+                                        states_prev_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_prev_tensor->dim_size(1) == state_size,\n+                errors::InvalidArgument(\"states_prev.dims(1) != state_size: \",\n+                                        states_prev_tensor->dim_size(1),\n+                                        \" vs. \", state_size));\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+                errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                        w_tensor->dim_size(0),\n+                                        \" vs. \", input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, states_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states.dims(0) != batch_size: \",\n+                                        states_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_tensor->dim_size(1) == state_size,\n+                errors::InvalidArgument(\"states.dims(1) != state_size: \",\n+                                        states_tensor->dim_size(1),\n+                                        \" vs. \", state_size));\n+\n+    OP_REQUIRES(ctx, h_grad_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"h_grad_tensor.dims(0) != batch_size: \",\n+                                        h_grad_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, h_grad_tensor->dim_size(1) == cell_size_,\n+                errors::InvalidArgument(\"h_grad_tensor.dims(1) != state_size: \",\n+                                        h_grad_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_));\n+\n+    OP_REQUIRES(ctx, states_grad_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"states_grad.dims(0) != batch_size: \",\n+                                        states_grad_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, states_grad_tensor->dim_size(1) == state_size,\n+                errors::InvalidArgument(\"states_grad.dims(1) != state_size: \",\n+                                        states_grad_tensor->dim_size(1),\n+                                        \" vs. \", state_size));\n+\n+    Tensor* x_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"x_grad\",\n+          TensorShape({batch_size, input_size}), &x_grad_tensor));\n+\n+    Tensor* states_prev_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"states_prev_grad\",\n+          TensorShape({batch_size, cell_size_ * 7}), &states_prev_grad_tensor));\n+\n+    Tensor* w_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"w_grad\",\n+          TensorShape({input_size + cell_size_, cell_size_ * 4}),\n+          &w_grad_tensor));\n+    TensorMemZero(w_grad_tensor, stream);\n+\n+    Tensor* b_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"b_grad\",\n+          TensorShape({cell_size_ * 4}), &b_grad_tensor));\n+    TensorMemZero(b_grad_tensor, stream);\n+\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_tensor));\n+\n+    Tensor xh_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_grad_tensor));\n+\n+    functor::LSTMCellBlockBprop<Device, USE_CUBLAS>()(\n+        stream, ctx->eigen_device<Device>(),\n+        batch_size, input_size, cell_size_,\n+        x_tensor->matrix<float>(), xh_tensor.matrix<float>(),\n+        states_prev_tensor->matrix<float>(), w_tensor->matrix<float>(),\n+        b_tensor->vec<float>(),\n+        states_tensor->matrix<float>(), h_grad_tensor->matrix<float>(),\n+        states_grad_tensor->matrix<float>(), xh_grad_tensor.matrix<float>(),\n+        x_grad_tensor->matrix<float>(), states_prev_grad_tensor->matrix<float>(),\n+        w_grad_tensor->matrix<float>(), b_grad_tensor->vec<float>());\n+  }\n+\n+ protected:\n+  int64 cell_size_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlockGrad\")    \\\n+                            .Device(DEVICE_CPU),\n+                        LSTMCellBlockGradOp<CPUDevice, false>);\n+\n+#if GOOGLE_CUDA\n+namespace functor {\n+  template <>\n+  void LSTMCellBlockBprop<GPUDevice, true>::operator()(\n+      perftools::gputools::Stream* stream, const GPUDevice& d,\n+      const int batch_size, const int input_size, const int cell_size,\n+      typename TTypes<float>::ConstMatrix x,\n+      typename TTypes<float>::Matrix xh,\n+      typename TTypes<float>::ConstMatrix states_prev,\n+      typename TTypes<float>::ConstMatrix w,\n+      typename TTypes<float>::ConstVec b,\n+      typename TTypes<float>::ConstMatrix states,\n+      typename TTypes<float>::ConstMatrix h_grad,\n+      typename TTypes<float>::ConstMatrix states_grad,\n+      typename TTypes<float>::Matrix xh_grad,\n+      typename TTypes<float>::Matrix x_grad,\n+      typename TTypes<float>::Matrix states_prev_grad,\n+      typename TTypes<float>::Matrix w_grad,\n+      typename TTypes<float>::Vec b_grad);\n+  extern template struct LSTMCellBlockBprop<GPUDevice, true>;\n+}  // namespace functor\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMCellBlockGrad\")  \\\n+                            .Device(DEVICE_GPU),   \\\n+                        LSTMCellBlockGradOp<GPUDevice, true>);\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMBlockOp : public OpKernel {\n+ public:\n+  explicit LSTMBlockOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"sequence_len_max\", &sequence_len_max_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"forget_bias\", &forget_bias_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* sequence_len_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"sequence_len\", &sequence_len_tensor));\n+\n+    const Tensor* initial_state_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"initial_state\", &initial_state_tensor));\n+\n+    OpInputList x_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"x\", &x_tensors));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    OpOutputList h_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"h\", &h_tensors));\n+\n+    OpOutputList states_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"states\", &states_tensors));\n+\n+    auto sequence_len_t = sequence_len_tensor->vec<int64>();\n+    std::vector<int64> seq_lens_vector(sequence_len_t.size());\n+    ctx->eigen_device<Device>().memcpyDeviceToHost(\n+        seq_lens_vector.data(), sequence_len_t.data(),\n+        sizeof(int64) * sequence_len_t.size());\n+\n+    const int64 batch_size = x_tensors[0].dim_size(0);\n+    const int64 input_size = x_tensors[0].dim_size(1);\n+    const int64 state_size = cell_size_ * 7;\n+\n+    const int64 sequence_len_max =\n+        *std::max_element(seq_lens_vector.begin(), seq_lens_vector.end());\n+    CHECK_LE(sequence_len_max, sequence_len_max_);\n+\n+    OP_REQUIRES(ctx, initial_state_tensor->dim_size(0) == batch_size,\n+                errors::InvalidArgument(\"initial_state_tensor.dims(0) == batch_size: \",\n+                                        initial_state_tensor->dim_size(0),\n+                                        \" vs. \", batch_size));\n+    OP_REQUIRES(ctx, initial_state_tensor->dim_size(1) == state_size,\n+                errors::InvalidArgument(\"initial_state_tensor.dims(1) == state_size: \",\n+                                        initial_state_tensor->dim_size(1),\n+                                        \" vs. \", state_size));\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+                errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                        w_tensor->dim_size(0),\n+                                        \" vs. \", input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    for (int64 t = 0; t < sequence_len_max_; ++t ) {\n+      Tensor* h_tensor = nullptr;\n+      h_tensors.allocate(\n+          t, TensorShape({batch_size, cell_size_}), &h_tensor);\n+      TensorMemZero(h_tensor, stream);\n+\n+      Tensor* states_tensor = nullptr;\n+      states_tensors.allocate(\n+          t, TensorShape({batch_size, state_size}), &states_tensor);\n+      TensorMemZero(states_tensor, stream);\n+    }\n+\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n+          DT_FLOAT, TensorShape({batch_size, input_size + cell_size_}),\n+          &xh_tensor));\n+    \n+    for (int64 t = 0; t < sequence_len_max; ++t) {\n+      const Tensor x_tensor = x_tensors[t];\n+      const Tensor* states_prev_tensor =\n+          t <= 0 ? initial_state_tensor : states_tensors[t - 1];\n+\n+      Tensor* states_tensor = states_tensors[t];\n+      Tensor* h_tensor = h_tensors[t];\n+\n+      functor::LSTMCellBlockFprop<Device, USE_CUBLAS>()(\n+          stream, ctx->eigen_device<Device>(),\n+          batch_size, input_size, cell_size_, forget_bias_,\n+          x_tensor.matrix<float>(),\n+          xh_tensor.matrix<float>(),\n+          states_prev_tensor->matrix<float>(),\n+          w_tensor->matrix<float>(),\n+          b_tensor->vec<float>(),\n+          h_tensor->matrix<float>(),\n+          states_tensor->matrix<float>());\n+    }\n+  }\n+\n+ private:\n+  int64 sequence_len_max_;\n+  int64 cell_size_;\n+  float forget_bias_;\n+};\n+\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMBlock\")         \\\n+                            .Device(DEVICE_CPU),  \\\n+                        LSTMBlockOp<CPUDevice, false>);\n+\n+#ifdef GOOGLE_CUDA\n+REGISTER_KERNEL_BUILDER(Name(\"LSTMBlock\")         \\\n+                            .Device(DEVICE_GPU),  \\\n+                        LSTMBlockOp<GPUDevice, true>);\n+#endif  // GOOGLE_CUDA\n+\n+template <typename Device, bool USE_CUBLAS>\n+class LSTMBlockGradOp : public OpKernel {\n+ public:\n+  explicit LSTMBlockGradOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"sequence_len_max\", &sequence_len_max_));\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"cell_size\", &cell_size_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    const Tensor* sequence_len_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"sequence_len\", &sequence_len_tensor));\n+\n+    const Tensor* initial_state_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"initial_state\", &initial_state_tensor));\n+\n+    OpInputList x_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"x\", &x_tensors));\n+\n+    const Tensor* w_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"w\", &w_tensor));\n+\n+    const Tensor* b_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"b\", &b_tensor));\n+\n+    OpInputList states_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"states\", &states_tensors));\n+\n+    OpInputList h_grad_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->input_list(\"h_grad\", &h_grad_tensors));\n+\n+    auto sequence_len_t = sequence_len_tensor->vec<int64>();\n+    std::vector<int64> seq_lens_vector(sequence_len_t.size());\n+    ctx->eigen_device<Device>().memcpyDeviceToHost(\n+        seq_lens_vector.data(), sequence_len_t.data(),\n+        sizeof(int64) * sequence_len_t.size());\n+\n+    const int64 batch_size = x_tensors[0].dim_size(0);\n+    const int64 input_size = x_tensors[0].dim_size(1);\n+    const int64 state_size = cell_size_ * 7;\n+\n+    const int64 sequence_len_max =\n+        *std::max_element(seq_lens_vector.begin(), seq_lens_vector.end());\n+    CHECK_LE(sequence_len_max, sequence_len_max_);\n+\n+    OP_REQUIRES(ctx, w_tensor->dim_size(0) == input_size + cell_size_,\n+                errors::InvalidArgument(\"w.dim_size(0) != input_size + cell_size: \",\n+                                        w_tensor->dim_size(0),\n+                                        \" vs. \", input_size + cell_size_));\n+    OP_REQUIRES(ctx, w_tensor->dim_size(1) == cell_size_ * 4,\n+                errors::InvalidArgument(\"w.dim_size(1) != cell_size * 4: \",\n+                                        w_tensor->dim_size(1),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    OP_REQUIRES(ctx, b_tensor->dim_size(0) == cell_size_ * 4,\n+                errors::InvalidArgument(\"b.dim_size(0) != cell_size * 4: \",\n+                                        b_tensor->dim_size(0),\n+                                        \" vs. \", cell_size_ * 4));\n+\n+    perftools::gputools::Stream* stream =\n+        ctx->op_device_context() ? ctx->op_device_context()->stream() : nullptr;\n+\n+    OpOutputList x_grad_tensors;\n+    OP_REQUIRES_OK(ctx, ctx->output_list(\"x_grad\", &x_grad_tensors));\n+\n+    Tensor* w_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"w_grad\",\n+          TensorShape({input_size + cell_size_, cell_size_ * 4}),\n+          &w_grad_tensor));\n+    TensorMemZero(w_grad_tensor, stream);\n+\n+    Tensor* b_grad_tensor = nullptr;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_output(\"b_grad\",\n+          TensorShape({cell_size_ * 4}), &b_grad_tensor));\n+    TensorMemZero(b_grad_tensor, stream);\n+\n+    for (int64 t = 0; t < sequence_len_max_; ++t) {\n+      Tensor* x_grad_tensor = nullptr;\n+      x_grad_tensors.allocate(\n+          t, TensorShape({batch_size, input_size}), &x_grad_tensor);\n+      TensorMemZero(x_grad_tensor, stream);\n+    }\n+\n+    Tensor xh_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(\n+          DT_FLOAT, TensorShape({batch_size, input_size + cell_size_}),\n+          &xh_tensor));\n+\n+    Tensor xh_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, input_size + cell_size_}), &xh_grad_tensor));\n+\n+    Tensor states_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, state_size}), &states_grad_tensor));\n+    TensorMemZero(&states_grad_tensor, stream);\n+\n+    Tensor states_prev_grad_tensor;\n+    OP_REQUIRES_OK(ctx, ctx->allocate_temp(DT_FLOAT,\n+        TensorShape({batch_size, state_size}), &states_prev_grad_tensor));\n+\n+    for (int64 t = sequence_len_max - 1; t >= 0; --t) {\n+      const Tensor& x_tensor = x_tensors[t];\n+      const Tensor& states_prev_tensor =\n+          t <= 0 ? *initial_state_tensor : states_tensors[t - 1];\n+      const Tensor& states_tensor = states_tensors[t];\n+      const Tensor& h_grad_tensor = h_grad_tensors[t];\n+\n+      Tensor* x_grad_tensor = x_grad_tensors[t];\n+      const Tensor& states_grad_const_tensor = states_grad_tensor;\n+\n+      functor::LSTMCellBlockBprop<Device, USE_CUBLAS>()(\n+          stream, ctx->eigen_device<Device>(),\n+          batch_size, input_size, cell_size_,\n+          x_tensor.matrix<float>(),\n+          xh_tensor.matrix<float>(),\n+          states_prev_tensor.matrix<float>(),\n+          w_tensor->matrix<float>(),\n+          b_tensor->vec<float>(),\n+          states_tensor.matrix<float>(),\n+          h_grad_tensor.matrix<float>(),\n+          states_grad_const_tensor.matrix<float>(),\n+          xh_grad_tensor.matrix<float>(),\n+          x_grad_tensor->matrix<float>(),\n+          states_prev_grad_tensor.matrix<float>(),\n+          w_grad_tensor->matrix<float>(),\n+          b_grad_tensor->vec<float>());\n+\n+      if (stream) {\n+#if GOOGLE_CUDA\n+        auto states_grad_ptr =\n+            AsDeviceMemory(states_grad_tensor.flat<float>().data());\n+        auto states_prev_grad_ptr =\n+            AsDeviceMemory(states_prev_grad_tensor.flat<float>().data());\n+        CHECK(stream->ThenMemcpy(\n+            &states_grad_ptr, states_prev_grad_ptr,\n+            states_grad_tensor.TotalBytes()).ok());\n+#endif  // GOOGLE_CUDA\n+      } else {\n+        std::memcpy(states_grad_tensor.flat<float>().data(),", "path": "tensorflow/contrib/rnn/kernels/lstm_ops.cc", "position": null, "original_position": 578, "commit_id": "258144b98730f99489437f6963737480be4f5a43", "original_commit_id": "36fb73b93c217d59a868524383b347104ce431dd", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "body": "FYI this might be slow since it is not a multi-threaded copy.  maybe consider using Eigen to do the copy inside the functor?\n", "created_at": "2016-04-28T16:09:45Z", "updated_at": "2016-05-18T21:31:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r61455829", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/61455829"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/2002#discussion_r61455829"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/2002"}}, "body_html": "<p>FYI this might be slow since it is not a multi-threaded copy.  maybe consider using Eigen to do the copy inside the functor?</p>", "body_text": "FYI this might be slow since it is not a multi-threaded copy.  maybe consider using Eigen to do the copy inside the functor?"}