{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261457223", "html_url": "https://github.com/tensorflow/tensorflow/issues/5663#issuecomment-261457223", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663", "id": 261457223, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTQ1NzIyMw==", "user": {"login": "mikowals", "id": 1331470, "node_id": "MDQ6VXNlcjEzMzE0NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1331470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mikowals", "html_url": "https://github.com/mikowals", "followers_url": "https://api.github.com/users/mikowals/followers", "following_url": "https://api.github.com/users/mikowals/following{/other_user}", "gists_url": "https://api.github.com/users/mikowals/gists{/gist_id}", "starred_url": "https://api.github.com/users/mikowals/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mikowals/subscriptions", "organizations_url": "https://api.github.com/users/mikowals/orgs", "repos_url": "https://api.github.com/users/mikowals/repos", "events_url": "https://api.github.com/users/mikowals/events{/privacy}", "received_events_url": "https://api.github.com/users/mikowals/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-18T06:15:29Z", "updated_at": "2016-11-18T06:16:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I made a simpler reproduction and it shows both examples are equivalent.  There could still be a problem with variable reuse if you are not placing example 1 above into a variable_scope with reuse set to True in validation.</p>\n<pre><code>import tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndata = tf.random_normal(shape=(64, 16, 16, 3), seed=10)\n\nconv_bn = slim.conv2d(data, 16, (3,3), activation_fn=None,\n                        normalizer_fn=slim.batch_norm, normalizer_params={'is_training': True},\n                        scope='conv')\n\nconv_no_bn = slim.conv2d(data, 16, (3,3), activation_fn=None, biases_initializer=None,\n                        reuse=True, scope='conv')\nalt_conv_bn = slim.batch_norm(conv_no_bn, is_training=True)\n\nis_equal = tf.reduce_all(tf.equal(conv_bn, alt_conv_bn))\n\nmoving_variances = tf.contrib.framework.get_variables_by_suffix('moving_variance')\nvariance_is_equal = tf.reduce_all(tf.equal(*moving_variances))\n\nmoving_means = tf.contrib.framework.get_variables_by_suffix('moving_mean')\nmean_is_equal = tf.reduce_all(tf.equal(*moving_means))\n\nwith tf.Session() as sess:\n  sess.run(tf.initialize_all_variables())\n  print(sess.run(is_equal)) # True\n  print(sess.run(is_equal)) # True\n  print([v.name for v in moving_variances]) # ['conv/BatchNorm/moving_variance:0', 'BatchNorm/moving_variance:0']\n  print([v.name for v in moving_means]) # ['conv/BatchNorm/moving_mean:0', 'BatchNorm/moving_mean:0']\n  print(sess.run([variance_is_equal, mean_is_equal])) # [True, True]\n</code></pre>", "body_text": "I made a simpler reproduction and it shows both examples are equivalent.  There could still be a problem with variable reuse if you are not placing example 1 above into a variable_scope with reuse set to True in validation.\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndata = tf.random_normal(shape=(64, 16, 16, 3), seed=10)\n\nconv_bn = slim.conv2d(data, 16, (3,3), activation_fn=None,\n                        normalizer_fn=slim.batch_norm, normalizer_params={'is_training': True},\n                        scope='conv')\n\nconv_no_bn = slim.conv2d(data, 16, (3,3), activation_fn=None, biases_initializer=None,\n                        reuse=True, scope='conv')\nalt_conv_bn = slim.batch_norm(conv_no_bn, is_training=True)\n\nis_equal = tf.reduce_all(tf.equal(conv_bn, alt_conv_bn))\n\nmoving_variances = tf.contrib.framework.get_variables_by_suffix('moving_variance')\nvariance_is_equal = tf.reduce_all(tf.equal(*moving_variances))\n\nmoving_means = tf.contrib.framework.get_variables_by_suffix('moving_mean')\nmean_is_equal = tf.reduce_all(tf.equal(*moving_means))\n\nwith tf.Session() as sess:\n  sess.run(tf.initialize_all_variables())\n  print(sess.run(is_equal)) # True\n  print(sess.run(is_equal)) # True\n  print([v.name for v in moving_variances]) # ['conv/BatchNorm/moving_variance:0', 'BatchNorm/moving_variance:0']\n  print([v.name for v in moving_means]) # ['conv/BatchNorm/moving_mean:0', 'BatchNorm/moving_mean:0']\n  print(sess.run([variance_is_equal, mean_is_equal])) # [True, True]", "body": "I made a simpler reproduction and it shows both examples are equivalent.  There could still be a problem with variable reuse if you are not placing example 1 above into a variable_scope with reuse set to True in validation.  \n\n```\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\n\ndata = tf.random_normal(shape=(64, 16, 16, 3), seed=10)\n\nconv_bn = slim.conv2d(data, 16, (3,3), activation_fn=None,\n                        normalizer_fn=slim.batch_norm, normalizer_params={'is_training': True},\n                        scope='conv')\n\nconv_no_bn = slim.conv2d(data, 16, (3,3), activation_fn=None, biases_initializer=None,\n                        reuse=True, scope='conv')\nalt_conv_bn = slim.batch_norm(conv_no_bn, is_training=True)\n\nis_equal = tf.reduce_all(tf.equal(conv_bn, alt_conv_bn))\n\nmoving_variances = tf.contrib.framework.get_variables_by_suffix('moving_variance')\nvariance_is_equal = tf.reduce_all(tf.equal(*moving_variances))\n\nmoving_means = tf.contrib.framework.get_variables_by_suffix('moving_mean')\nmean_is_equal = tf.reduce_all(tf.equal(*moving_means))\n\nwith tf.Session() as sess:\n  sess.run(tf.initialize_all_variables())\n  print(sess.run(is_equal)) # True\n  print(sess.run(is_equal)) # True\n  print([v.name for v in moving_variances]) # ['conv/BatchNorm/moving_variance:0', 'BatchNorm/moving_variance:0']\n  print([v.name for v in moving_means]) # ['conv/BatchNorm/moving_mean:0', 'BatchNorm/moving_mean:0']\n  print(sess.run([variance_is_equal, mean_is_equal])) # [True, True]\n```\n"}