{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261484486", "html_url": "https://github.com/tensorflow/tensorflow/issues/5663#issuecomment-261484486", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663", "id": 261484486, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTQ4NDQ4Ng==", "user": {"login": "ColaWithIce", "id": 18020562, "node_id": "MDQ6VXNlcjE4MDIwNTYy", "avatar_url": "https://avatars2.githubusercontent.com/u/18020562?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ColaWithIce", "html_url": "https://github.com/ColaWithIce", "followers_url": "https://api.github.com/users/ColaWithIce/followers", "following_url": "https://api.github.com/users/ColaWithIce/following{/other_user}", "gists_url": "https://api.github.com/users/ColaWithIce/gists{/gist_id}", "starred_url": "https://api.github.com/users/ColaWithIce/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ColaWithIce/subscriptions", "organizations_url": "https://api.github.com/users/ColaWithIce/orgs", "repos_url": "https://api.github.com/users/ColaWithIce/repos", "events_url": "https://api.github.com/users/ColaWithIce/events{/privacy}", "received_events_url": "https://api.github.com/users/ColaWithIce/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-18T09:17:44Z", "updated_at": "2016-11-18T09:17:44Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1331470\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mikowals\">@mikowals</a>  Thank you for your comment. I think I find the problem. The trainable variable numbers are different between different ways. That means if I use <code>reuse</code> in <code>conv2d</code> layer, all the variable created in this layer INCLUDE the variables created in <code>batch_norm</code> layer will be reused when <code>reuse</code> is <code>True</code><br>\nWhat I test is shown below.<br>\nFor example 1:</p>\n<pre><code>for v in tf.trainable_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.trainable_variables())\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>name = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\nname = Tensor(\"BatchNorm_1/beta/read:0\", shape=(64,), dtype=float32)\n17\n</code></pre>\n<p>For example 2:</p>\n<pre><code>for v in tf.all_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.all_variables())\n\n</code></pre>\n<p>OUTPUT:</p>\n<pre><code>name = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"d_conv3/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\n16\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1331470\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mikowals\">@mikowals</a> By the way, when I use <code>slim.batch_norm</code> alone. Is it the right way to set parameters of normalization {is_training=True} in Training phase and {is_training=False, reuse=True} in Testing and validation phase? My point is do I need to set <code>reuse</code> to True in the Testing phase. Or just set the <code>reuse</code> as default <code>None.</code> Thank you in advance.</p>", "body_text": "@mikowals  Thank you for your comment. I think I find the problem. The trainable variable numbers are different between different ways. That means if I use reuse in conv2d layer, all the variable created in this layer INCLUDE the variables created in batch_norm layer will be reused when reuse is True\nWhat I test is shown below.\nFor example 1:\nfor v in tf.trainable_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.trainable_variables())\n\nOUTPUT:\nname = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\nname = Tensor(\"BatchNorm_1/beta/read:0\", shape=(64,), dtype=float32)\n17\n\nFor example 2:\nfor v in tf.all_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.all_variables())\n\n\nOUTPUT:\nname = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"d_conv3/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\n16\n\n@mikowals By the way, when I use slim.batch_norm alone. Is it the right way to set parameters of normalization {is_training=True} in Training phase and {is_training=False, reuse=True} in Testing and validation phase? My point is do I need to set reuse to True in the Testing phase. Or just set the reuse as default None. Thank you in advance.", "body": "@mikowals  Thank you for your comment. I think I find the problem. The trainable variable numbers are different between different ways. That means if I use `reuse` in `conv2d` layer, all the variable created in this layer INCLUDE the variables created in `batch_norm` layer will be reused when `reuse` is `True`\nWhat I test is shown below.\nFor example 1:\n\n```\nfor v in tf.trainable_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.trainable_variables())\n```\n\nOUTPUT:\n\n```\nname = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\nname = Tensor(\"BatchNorm_1/beta/read:0\", shape=(64,), dtype=float32)\n17\n```\n\nFor example 2:\n\n```\nfor v in tf.all_variables():\n    print 'name = {}'.format(v.value())\n    print len(tf.all_variables())\n\n```\n\nOUTPUT:\n\n```\nname = Tensor(\"g_project/weights/read:0\", shape=(100, 4096), dtype=float32)\nname = Tensor(\"g_project/BatchNorm/beta/read:0\", shape=(4096,), dtype=float32)\nname = Tensor(\"g_conv1/weights/read:0\", shape=(5, 5, 64, 256), dtype=float32)\nname = Tensor(\"g_conv1/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"g_conv2/weights/read:0\", shape=(5, 5, 32, 64), dtype=float32)\nname = Tensor(\"g_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"g_conv3/weights/read:0\", shape=(5, 5, 16, 32), dtype=float32)\nname = Tensor(\"g_conv3/BatchNorm/beta/read:0\", shape=(16,), dtype=float32)\nname = Tensor(\"g_out/weights/read:0\", shape=(32, 32, 1, 16), dtype=float32)\nname = Tensor(\"d_conv1/weights/read:0\", shape=(4, 4, 1, 16), dtype=float32)\nname = Tensor(\"d_conv2/weights/read:0\", shape=(4, 4, 16, 32), dtype=float32)\nname = Tensor(\"d_conv2/BatchNorm/beta/read:0\", shape=(32,), dtype=float32)\nname = Tensor(\"d_conv3/weights/read:0\", shape=(4, 4, 32, 64), dtype=float32)\nname = Tensor(\"d_conv3/BatchNorm/beta/read:0\", shape=(64,), dtype=float32)\nname = Tensor(\"d_out/weights/read:0\", shape=(1024, 1), dtype=float32)\nname = Tensor(\"d_out/biases/read:0\", shape=(1,), dtype=float32)\n16\n```\n\n@mikowals By the way, when I use `slim.batch_norm` alone. Is it the right way to set parameters of normalization {is_training=True} in Training phase and {is_training=False, reuse=True} in Testing and validation phase? My point is do I need to set `reuse` to True in the Testing phase. Or just set the `reuse` as default `None.` Thank you in advance.\n"}