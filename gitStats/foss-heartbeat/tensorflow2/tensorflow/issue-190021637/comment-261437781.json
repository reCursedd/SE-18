{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/261437781", "html_url": "https://github.com/tensorflow/tensorflow/issues/5663#issuecomment-261437781", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663", "id": 261437781, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MTQzNzc4MQ==", "user": {"login": "ColaWithIce", "id": 18020562, "node_id": "MDQ6VXNlcjE4MDIwNTYy", "avatar_url": "https://avatars2.githubusercontent.com/u/18020562?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ColaWithIce", "html_url": "https://github.com/ColaWithIce", "followers_url": "https://api.github.com/users/ColaWithIce/followers", "following_url": "https://api.github.com/users/ColaWithIce/following{/other_user}", "gists_url": "https://api.github.com/users/ColaWithIce/gists{/gist_id}", "starred_url": "https://api.github.com/users/ColaWithIce/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ColaWithIce/subscriptions", "organizations_url": "https://api.github.com/users/ColaWithIce/orgs", "repos_url": "https://api.github.com/users/ColaWithIce/repos", "events_url": "https://api.github.com/users/ColaWithIce/events{/privacy}", "received_events_url": "https://api.github.com/users/ColaWithIce/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-18T03:09:13Z", "updated_at": "2016-11-18T03:09:13Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1331470\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mikowals\">@mikowals</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Actually, I read the <a href=\"https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/contrib/layers/python/layers/layers.py\">code</a>, from line 439 to 455, it describes how the conv2d layer uses the batch_norm and activation. What I understand from this is that the <code>reuse</code> parameter only influnence the <code>conv2d</code> layer but not the function used inside.<br>\nTo make the two ways same, I also set <code>biases_initializer</code> None.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Sorry, I don't know what is the exact problem. But I guess it ways to use <code>batch_norm</code>.  I just try one <a href=\"https://github.com/awjuliani/TF-Tutorials/blob/master/DCGAN.ipynb\">tutorial</a> written by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=9065325\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/awjuliani\">@awjuliani</a>. Here are the two ways I used in the code which I use Ipython notebook to test. What I just change is the <code>dis3</code> of the <code>discriminator</code>. The first way ipython notebook is shown <a href=\"https://github.com/ColaWithIce/TF-test/blob/master/DCGAN-Copy1.ipynb\">here.</a> The second is <a href=\"https://github.com/ColaWithIce/TF-test/blob/master/DCGAN.ipynb\">here.</a> The output of the training message is very different. The two way's result should be similar but now as you can see the Disc Loss are not in same scale. I just wonder why this will happen. Is there something wrong with the variable updating or the way I use the code is wrong.</p>", "body_text": "@mikowals @girving Actually, I read the code, from line 439 to 455, it describes how the conv2d layer uses the batch_norm and activation. What I understand from this is that the reuse parameter only influnence the conv2d layer but not the function used inside.\nTo make the two ways same, I also set biases_initializer None.\n@girving Sorry, I don't know what is the exact problem. But I guess it ways to use batch_norm.  I just try one tutorial written by @awjuliani. Here are the two ways I used in the code which I use Ipython notebook to test. What I just change is the dis3 of the discriminator. The first way ipython notebook is shown here. The second is here. The output of the training message is very different. The two way's result should be similar but now as you can see the Disc Loss are not in same scale. I just wonder why this will happen. Is there something wrong with the variable updating or the way I use the code is wrong.", "body": "@mikowals @girving Actually, I read the [code](https://github.com/tensorflow/tensorflow/blob/r0.11/tensorflow/contrib/layers/python/layers/layers.py), from line 439 to 455, it describes how the conv2d layer uses the batch_norm and activation. What I understand from this is that the `reuse` parameter only influnence the `conv2d` layer but not the function used inside.\nTo make the two ways same, I also set `biases_initializer` None.\n\n@girving Sorry, I don't know what is the exact problem. But I guess it ways to use `batch_norm`.  I just try one [tutorial](https://github.com/awjuliani/TF-Tutorials/blob/master/DCGAN.ipynb) written by @awjuliani. Here are the two ways I used in the code which I use Ipython notebook to test. What I just change is the `dis3` of the `discriminator`. The first way ipython notebook is shown [here.](https://github.com/ColaWithIce/TF-test/blob/master/DCGAN-Copy1.ipynb) The second is [here.](https://github.com/ColaWithIce/TF-test/blob/master/DCGAN.ipynb) The output of the training message is very different. The two way's result should be similar but now as you can see the Disc Loss are not in same scale. I just wonder why this will happen. Is there something wrong with the variable updating or the way I use the code is wrong. \n"}