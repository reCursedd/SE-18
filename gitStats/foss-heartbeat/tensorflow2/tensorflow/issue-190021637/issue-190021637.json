{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5663/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5663", "id": 190021637, "node_id": "MDU6SXNzdWUxOTAwMjE2Mzc=", "number": 5663, "title": "slim.batch_norm used with slim.conv2d problem", "user": {"login": "ColaWithIce", "id": 18020562, "node_id": "MDQ6VXNlcjE4MDIwNTYy", "avatar_url": "https://avatars2.githubusercontent.com/u/18020562?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ColaWithIce", "html_url": "https://github.com/ColaWithIce", "followers_url": "https://api.github.com/users/ColaWithIce/followers", "following_url": "https://api.github.com/users/ColaWithIce/following{/other_user}", "gists_url": "https://api.github.com/users/ColaWithIce/gists{/gist_id}", "starred_url": "https://api.github.com/users/ColaWithIce/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ColaWithIce/subscriptions", "organizations_url": "https://api.github.com/users/ColaWithIce/orgs", "repos_url": "https://api.github.com/users/ColaWithIce/repos", "events_url": "https://api.github.com/users/ColaWithIce/events{/privacy}", "received_events_url": "https://api.github.com/users/ColaWithIce/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-11-17T11:35:10Z", "updated_at": "2016-11-21T02:33:27Z", "closed_at": "2016-11-21T02:33:27Z", "author_association": "NONE", "body_html": "<p>Dear All,<br>\nI have met some problems when using the batch norm layer of slim. I trained the same model structure but with different ways to use batch_norm layer, shown below. It seems the output of the two ways to use batch_norm is different to me. I have look inside the code to see. It seems the two way no different at all. But the result is actually not the same. Anyone met it before?</p>\n<ol>\n<li>\n<p>net = slim.conv2d(net, 64, [4, 4], 2, normalizer_fn=None, activation_fn=None, biases_initializer=None, reuse=reuse)<br>\noutput1 = slim.batch_norm(net)</p>\n</li>\n<li>\n<p>output2 = slim.conv2d(net, 64 ,[4, 4], 2, normalizer_fn=slim.batch_norm, activation_fn=None, reuse=reuse)</p>\n</li>\n</ol>", "body_text": "Dear All,\nI have met some problems when using the batch norm layer of slim. I trained the same model structure but with different ways to use batch_norm layer, shown below. It seems the output of the two ways to use batch_norm is different to me. I have look inside the code to see. It seems the two way no different at all. But the result is actually not the same. Anyone met it before?\n\n\nnet = slim.conv2d(net, 64, [4, 4], 2, normalizer_fn=None, activation_fn=None, biases_initializer=None, reuse=reuse)\noutput1 = slim.batch_norm(net)\n\n\noutput2 = slim.conv2d(net, 64 ,[4, 4], 2, normalizer_fn=slim.batch_norm, activation_fn=None, reuse=reuse)", "body": "Dear All,\r\nI have met some problems when using the batch norm layer of slim. I trained the same model structure but with different ways to use batch_norm layer, shown below. It seems the output of the two ways to use batch_norm is different to me. I have look inside the code to see. It seems the two way no different at all. But the result is actually not the same. Anyone met it before?\r\n\r\n1. net = slim.conv2d(net, 64, [4, 4], 2, normalizer_fn=None, activation_fn=None, biases_initializer=None, reuse=reuse)\r\n   output1 = slim.batch_norm(net)\r\n\r\n2. output2 = slim.conv2d(net, 64 ,[4, 4], 2, normalizer_fn=slim.batch_norm, activation_fn=None, reuse=reuse)\r\n\r\n\r\n"}