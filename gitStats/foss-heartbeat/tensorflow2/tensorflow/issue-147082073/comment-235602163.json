{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235602163", "html_url": "https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235602163", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1830", "id": 235602163, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTYwMjE2Mw==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-27T14:27:33Z", "updated_at": "2016-07-27T14:27:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Yes, but that would not be backwards compatible and would result in<br>\nincorrect gradients if you apply losses on every output.</p>\n<p>On Jul 27, 2016 7:25 AM, \"Zhuang Ma\" <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a> THANKS! Correct me if I miss<br>\nsomething, the value of the state in currently rnn function will inherit<br>\nthe value from previous state if the the time step t is larger than the<br>\nsentence length. Conceptually, the same trick can be applied to output<br>\nwithout any further difficulty. It seems that we can simply replace<br>\nzero_output=zero_output in the following code with zero_output = output to<br>\nachieve this and set the initial value of output to be zero_output.<br>\n` for time, input_ in enumerate(inputs):<br>\nif time &gt; 0: varscope.reuse_variables()</p>\n<h1>pylint: disable=cell-var-from-loop</h1>\n<p>call_cell = lambda: cell(input_, state)</p>\n<h1>pylint: enable=cell-var-from-loop</h1>\n<p>if sequence_length is not None:<br>\n(output, state) = _rnn_step(<br>\ntime=time,<br>\nsequence_length=sequence_length,<br>\nmin_sequence_length=min_sequence_length,<br>\nmax_sequence_length=max_sequence_length,<br>\nzero_output=zero_output,<br>\nstate=state,<br>\ncall_cell=call_cell,<br>\nstate_size=cell.state_size)<br>\nelse:<br>\n(output, state) = call_cell()</p>\n<p>outputs.append(output)`</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"147082073\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1830\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1830/hovercard?comment_id=235601341&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341\">#1830 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\">https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS</a><br>\n.</p>\n</blockquote>", "body_text": "Yes, but that would not be backwards compatible and would result in\nincorrect gradients if you apply losses on every output.\nOn Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n\n@ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\nsomething, the value of the state in currently rnn function will inherit\nthe value from previous state if the the time step t is larger than the\nsentence length. Conceptually, the same trick can be applied to output\nwithout any further difficulty. It seems that we can simply replace\nzero_output=zero_output in the following code with zero_output = output to\nachieve this and set the initial value of output to be zero_output.\n` for time, input_ in enumerate(inputs):\nif time > 0: varscope.reuse_variables()\npylint: disable=cell-var-from-loop\ncall_cell = lambda: cell(input_, state)\npylint: enable=cell-var-from-loop\nif sequence_length is not None:\n(output, state) = _rnn_step(\ntime=time,\nsequence_length=sequence_length,\nmin_sequence_length=min_sequence_length,\nmax_sequence_length=max_sequence_length,\nzero_output=zero_output,\nstate=state,\ncall_cell=call_cell,\nstate_size=cell.state_size)\nelse:\n(output, state) = call_cell()\noutputs.append(output)`\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#1830 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n.", "body": "Yes, but that would not be backwards compatible and would result in\nincorrect gradients if you apply losses on every output.\n\nOn Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\n> something, the value of the state in currently rnn function will inherit\n> the value from previous state if the the time step t is larger than the\n> sentence length. Conceptually, the same trick can be applied to output\n> without any further difficulty. It seems that we can simply replace\n> zero_output=zero_output in the following code with zero_output = output to\n> achieve this and set the initial value of output to be zero_output.\n> ` for time, input_ in enumerate(inputs):\n> if time > 0: varscope.reuse_variables()\n> \n> # pylint: disable=cell-var-from-loop\n> \n> call_cell = lambda: cell(input_, state)\n> \n> # pylint: enable=cell-var-from-loop\n> \n> if sequence_length is not None:\n> (output, state) = _rnn_step(\n> time=time,\n> sequence_length=sequence_length,\n> min_sequence_length=min_sequence_length,\n> max_sequence_length=max_sequence_length,\n> zero_output=zero_output,\n> state=state,\n> call_cell=call_cell,\n> state_size=cell.state_size)\n> else:\n> (output, state) = call_cell()\n> \n>   outputs.append(output)`\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n> .\n"}