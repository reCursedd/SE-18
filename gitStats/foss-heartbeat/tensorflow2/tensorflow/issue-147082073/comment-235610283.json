{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/235610283", "html_url": "https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235610283", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1830", "id": 235610283, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNTYxMDI4Mw==", "user": {"login": "MaZhuang", "id": 6137461, "node_id": "MDQ6VXNlcjYxMzc0NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/6137461?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MaZhuang", "html_url": "https://github.com/MaZhuang", "followers_url": "https://api.github.com/users/MaZhuang/followers", "following_url": "https://api.github.com/users/MaZhuang/following{/other_user}", "gists_url": "https://api.github.com/users/MaZhuang/gists{/gist_id}", "starred_url": "https://api.github.com/users/MaZhuang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MaZhuang/subscriptions", "organizations_url": "https://api.github.com/users/MaZhuang/orgs", "repos_url": "https://api.github.com/users/MaZhuang/repos", "events_url": "https://api.github.com/users/MaZhuang/events{/privacy}", "received_events_url": "https://api.github.com/users/MaZhuang/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-27T14:52:10Z", "updated_at": "2016-07-27T14:52:10Z", "author_association": "NONE", "body_html": "<p>Why will there be an issue with the backward step since the function seems<br>\nto be completely based on current tensorflow ops?  Will the issue still<br>\nexist if I only use the final output once instead of using all of them from<br>\nthe sentence length to the maximum length?</p>\n<p>On Wed, Jul 27, 2016 at 10:29 AM, ebrevdo <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>Yes, but that would not be backwards compatible and would result in<br>\nincorrect gradients if you apply losses on every output.</p>\n<p>On Jul 27, 2016 7:25 AM, \"Zhuang Ma\" <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a> THANKS! Correct me if I miss<br>\nsomething, the value of the state in currently rnn function will inherit<br>\nthe value from previous state if the the time step t is larger than the<br>\nsentence length. Conceptually, the same trick can be applied to output<br>\nwithout any further difficulty. It seems that we can simply replace<br>\nzero_output=zero_output in the following code with zero_output = output<br>\nto<br>\nachieve this and set the initial value of output to be zero_output.<br>\n` for time, input_ in enumerate(inputs):<br>\nif time &gt; 0: varscope.reuse_variables()</p>\n<h1>pylint: disable=cell-var-from-loop</h1>\n<p>call_cell = lambda: cell(input_, state)</p>\n<h1>pylint: enable=cell-var-from-loop</h1>\n<p>if sequence_length is not None:<br>\n(output, state) = _rnn_step(<br>\ntime=time,<br>\nsequence_length=sequence_length,<br>\nmin_sequence_length=min_sequence_length,<br>\nmax_sequence_length=max_sequence_length,<br>\nzero_output=zero_output,<br>\nstate=state,<br>\ncall_cell=call_cell,<br>\nstate_size=cell.state_size)<br>\nelse:<br>\n(output, state) = call_cell()</p>\n<p>outputs.append(output)`</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n&lt;<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"147082073\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1830\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1830/hovercard?comment_id=235601341&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341\">#1830 (comment)</a><br>\n,<br>\nor mute the thread<br>\n&lt;<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\">https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS</a></p>\n<p>.</p>\n</blockquote>\n<p>\u2014<br>\nYou are receiving this because you commented.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"147082073\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1830\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1830/hovercard?comment_id=235602163&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235602163\">#1830 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/AF2mdb86wpS8zseD5E9nlrLwTghh36_Hks5qZ2s_gaJpZM4IDhJS\">https://github.com/notifications/unsubscribe-auth/AF2mdb86wpS8zseD5E9nlrLwTghh36_Hks5qZ2s_gaJpZM4IDhJS</a><br>\n.</p>\n</blockquote>\n<h2></h2>\n<p>Zhuang Ma<br>\nDepartment of Statistics<br>\nThe Wharton School<br>\nUniversity of Pennsylvania</p>", "body_text": "Why will there be an issue with the backward step since the function seems\nto be completely based on current tensorflow ops?  Will the issue still\nexist if I only use the final output once instead of using all of them from\nthe sentence length to the maximum length?\nOn Wed, Jul 27, 2016 at 10:29 AM, ebrevdo notifications@github.com wrote:\n\nYes, but that would not be backwards compatible and would result in\nincorrect gradients if you apply losses on every output.\nOn Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n\n@ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\nsomething, the value of the state in currently rnn function will inherit\nthe value from previous state if the the time step t is larger than the\nsentence length. Conceptually, the same trick can be applied to output\nwithout any further difficulty. It seems that we can simply replace\nzero_output=zero_output in the following code with zero_output = output\nto\nachieve this and set the initial value of output to be zero_output.\n` for time, input_ in enumerate(inputs):\nif time > 0: varscope.reuse_variables()\npylint: disable=cell-var-from-loop\ncall_cell = lambda: cell(input_, state)\npylint: enable=cell-var-from-loop\nif sequence_length is not None:\n(output, state) = _rnn_step(\ntime=time,\nsequence_length=sequence_length,\nmin_sequence_length=min_sequence_length,\nmax_sequence_length=max_sequence_length,\nzero_output=zero_output,\nstate=state,\ncall_cell=call_cell,\nstate_size=cell.state_size)\nelse:\n(output, state) = call_cell()\noutputs.append(output)`\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n<\n#1830 (comment)\n,\nor mute the thread\n<\nhttps://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n.\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub\n#1830 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/AF2mdb86wpS8zseD5E9nlrLwTghh36_Hks5qZ2s_gaJpZM4IDhJS\n.\n\n\nZhuang Ma\nDepartment of Statistics\nThe Wharton School\nUniversity of Pennsylvania", "body": "Why will there be an issue with the backward step since the function seems\nto be completely based on current tensorflow ops?  Will the issue still\nexist if I only use the final output once instead of using all of them from\nthe sentence length to the maximum length?\n\nOn Wed, Jul 27, 2016 at 10:29 AM, ebrevdo notifications@github.com wrote:\n\n> Yes, but that would not be backwards compatible and would result in\n> incorrect gradients if you apply losses on every output.\n> \n> On Jul 27, 2016 7:25 AM, \"Zhuang Ma\" notifications@github.com wrote:\n> \n> > @ebrevdo https://github.com/ebrevdo THANKS! Correct me if I miss\n> > something, the value of the state in currently rnn function will inherit\n> > the value from previous state if the the time step t is larger than the\n> > sentence length. Conceptually, the same trick can be applied to output\n> > without any further difficulty. It seems that we can simply replace\n> > zero_output=zero_output in the following code with zero_output = output\n> > to\n> > achieve this and set the initial value of output to be zero_output.\n> > ` for time, input_ in enumerate(inputs):\n> > if time > 0: varscope.reuse_variables()\n> > \n> > # pylint: disable=cell-var-from-loop\n> > \n> > call_cell = lambda: cell(input_, state)\n> > \n> > # pylint: enable=cell-var-from-loop\n> > \n> > if sequence_length is not None:\n> > (output, state) = _rnn_step(\n> > time=time,\n> > sequence_length=sequence_length,\n> > min_sequence_length=min_sequence_length,\n> > max_sequence_length=max_sequence_length,\n> > zero_output=zero_output,\n> > state=state,\n> > call_cell=call_cell,\n> > state_size=cell.state_size)\n> > else:\n> > (output, state) = call_cell()\n> > \n> > outputs.append(output)`\n> > \n> > \u2014\n> > You are receiving this because you were mentioned.\n> > Reply to this email directly, view it on GitHub\n> > <\n> > https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235601341\n> > ,\n> > or mute the thread\n> > <\n> > https://github.com/notifications/unsubscribe-auth/ABtim2WFFKPCVavxxrCWq_40jaRkDZ9Qks5qZ2pdgaJpZM4IDhJS\n> > \n> > .\n> \n> \u2014\n> You are receiving this because you commented.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1830#issuecomment-235602163,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/AF2mdb86wpS8zseD5E9nlrLwTghh36_Hks5qZ2s_gaJpZM4IDhJS\n> .\n\n## \n\nZhuang Ma\nDepartment of Statistics\nThe Wharton School\nUniversity of Pennsylvania\n"}