{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20403", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20403/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20403/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20403/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20403", "id": 336893026, "node_id": "MDU6SXNzdWUzMzY4OTMwMjY=", "number": 20403, "title": "[tflite][operator: cast] How to quantize MobileNetV2 for deeplabV3+?", "user": {"login": "kanul", "id": 15260298, "node_id": "MDQ6VXNlcjE1MjYwMjk4", "avatar_url": "https://avatars2.githubusercontent.com/u/15260298?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kanul", "html_url": "https://github.com/kanul", "followers_url": "https://api.github.com/users/kanul/followers", "following_url": "https://api.github.com/users/kanul/following{/other_user}", "gists_url": "https://api.github.com/users/kanul/gists{/gist_id}", "starred_url": "https://api.github.com/users/kanul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kanul/subscriptions", "organizations_url": "https://api.github.com/users/kanul/orgs", "repos_url": "https://api.github.com/users/kanul/repos", "events_url": "https://api.github.com/users/kanul/events{/privacy}", "received_events_url": "https://api.github.com/users/kanul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, {"login": "liyunlu0618", "id": 9705880, "node_id": "MDQ6VXNlcjk3MDU4ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/9705880?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liyunlu0618", "html_url": "https://github.com/liyunlu0618", "followers_url": "https://api.github.com/users/liyunlu0618/followers", "following_url": "https://api.github.com/users/liyunlu0618/following{/other_user}", "gists_url": "https://api.github.com/users/liyunlu0618/gists{/gist_id}", "starred_url": "https://api.github.com/users/liyunlu0618/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liyunlu0618/subscriptions", "organizations_url": "https://api.github.com/users/liyunlu0618/orgs", "repos_url": "https://api.github.com/users/liyunlu0618/repos", "events_url": "https://api.github.com/users/liyunlu0618/events{/privacy}", "received_events_url": "https://api.github.com/users/liyunlu0618/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-06-29T07:29:12Z", "updated_at": "2018-11-19T15:15:35Z", "closed_at": "2018-09-20T14:52:18Z", "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:Source</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.9.0</li>\n<li><strong>Python version</strong>:2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>:0.12.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>:cuda-9.0/7.0</li>\n<li><strong>GPU model and memory</strong>:GeForce GTX 1080/8105MiB</li>\n<li><strong>Phone</strong>:xiaomi5 (Snapdragon 820)</li>\n<li><strong>Exact command to reproduce</strong>:<br>\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --<br>\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb<br>\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite<br>\n--inference_type=QUANTIZED_UINT8<br>\n--input_shape=1,513,513,3<br>\n--input_array=sub_7<br>\n--output_array=logits/semantic/BiasAdd</li>\n</ul>\n<p><strong>Describe the problem</strong><br>\nI have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.<br>\nFrom the following issue, I saw that the operations were not supported for the option of quantization.</p>\n<p><a href=\"https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\">https://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md</a><br>\nCheckpoint name: mobilenetv2_coco_voc_trainaug</p>\n<p>Who can explain and support to resolve the issue?</p>\n<p><strong>Source code / logs</strong><br>\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --<br>\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb<br>\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite<br>\n--inference_type=QUANTIZED_UINT8<br>\n--input_shape=1,513,513,3<br>\n--input_array=sub_7<br>\n--output_array=logits/semantic/BiasAdd<br>\n--default_ranges_min=0<br>\n--default_ranges_max=6</p>\n<p>Unimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).</p>\n<p><strong>Source code / logs</strong><br>\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --<br>\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb<br>\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite<br>\n--inference_type=QUANTIZED_UINT8<br>\n--input_shape=1,513,513,3<br>\n--input_array=sub_7<br>\n--output_array=logits/semantic/BiasAdd</p>\n<p>tensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.</p>\n<p>tensorflow/contrib/lite/toco/tflite/export.cc:367] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\nTensorFlow installed from (source or binary):Source\nTensorFlow version (use command below):1.9.0\nPython version:2.7.12\nBazel version (if compiling from source):0.12.0\nGCC/Compiler version (if compiling from source):5.4.0\nCUDA/cuDNN version:cuda-9.0/7.0\nGPU model and memory:GeForce GTX 1080/8105MiB\nPhone:xiaomi5 (Snapdragon 820)\nExact command to reproduce:\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\n--inference_type=QUANTIZED_UINT8\n--input_shape=1,513,513,3\n--input_array=sub_7\n--output_array=logits/semantic/BiasAdd\n\nDescribe the problem\nI have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.\nFrom the following issue, I saw that the operations were not supported for the option of quantization.\nhttps://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\nCheckpoint name: mobilenetv2_coco_voc_trainaug\nWho can explain and support to resolve the issue?\nSource code / logs\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\n--inference_type=QUANTIZED_UINT8\n--input_shape=1,513,513,3\n--input_array=sub_7\n--output_array=logits/semantic/BiasAdd\n--default_ranges_min=0\n--default_ranges_max=6\nUnimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\nSource code / logs\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\n--inference_type=QUANTIZED_UINT8\n--input_shape=1,513,513,3\n--input_array=sub_7\n--output_array=logits/semantic/BiasAdd\ntensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\ntensorflow/contrib/lite/toco/tflite/export.cc:367] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack.", "body": "**System information**\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.9.0\r\n- **Python version**:2.7.12\r\n- **Bazel version (if compiling from source)**:0.12.0\r\n- **GCC/Compiler version (if compiling from source)**:5.4.0\r\n- **CUDA/cuDNN version**:cuda-9.0/7.0\r\n- **GPU model and memory**:GeForce GTX 1080/8105MiB\r\n- **Phone**:xiaomi5 (Snapdragon 820)\r\n- **Exact command to reproduce**:\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n\r\n**Describe the problem**\r\nI have tried to quantize MobileNetV2 for deeplabV3+ with TFlite. But I fail to convert the model.\r\nFrom the following issue, I saw that the operations were not supported for the option of quantization.\r\n\r\nhttps://github.com/tensorflow/models/blob/master/research/deeplab/g3doc/model_zoo.md\r\nCheckpoint name: mobilenetv2_coco_voc_trainaug\r\n\r\nWho can explain and support to resolve the issue?\r\n\r\n**Source code / logs**\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n--default_ranges_min=0  \r\n--default_ranges_max=6\r\n\r\nUnimplemented: this graph contains an operator of type Cast for which the quantized form is not yet implemented. Sorry, and patches welcome (that's a relatively fun patch to write, mostly providing the actual quantized arithmetic code for this op).\r\n\r\n\r\n**Source code / logs**\r\nbazel run --config=opt //tensorflow/contrib/lite/toco:toco --\r\n--input_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb\r\n--output_file=/external_home/data/model/deeplabv3_mnv2_pascal_train_aug/kanul.tflite\r\n--inference_type=QUANTIZED_UINT8\r\n--input_shape=1,513,513,3\r\n--input_array=sub_7\r\n--output_array=logits/semantic/BiasAdd\r\n\r\ntensorflow/contrib/lite/toco/graph_transformations/hardcode_min_max.cc:105] Tweaking the MinMax of array ResizeBilinear_1, which is an input to {Concatenation operator with output concat}, because we want all inputs and outputs of a Concatenation operator to have the same MinMax so that it can be implemented as a pure byte-copy, no arithmetic.\r\n\r\ntensorflow/contrib/lite/toco/tflite/export.cc:367] Some of the operators in the model are not supported by the standard TensorFlow Lite runtime. If you have a custom implementation for them you can disable this error with --allow_custom_ops, or by setting allow_custom_ops=True when calling tf.contrib.lite.toco_convert(). Here is a list of operators for which  you will need custom implementations: Stack."}