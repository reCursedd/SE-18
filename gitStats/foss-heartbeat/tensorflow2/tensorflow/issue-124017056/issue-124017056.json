{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/634", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/634/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/634/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/634/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/634", "id": 124017056, "node_id": "MDU6SXNzdWUxMjQwMTcwNTY=", "number": 634, "title": "AdamOptimizer + checkpoints", "user": {"login": "wchan", "id": 1131892, "node_id": "MDQ6VXNlcjExMzE4OTI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1131892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wchan", "html_url": "https://github.com/wchan", "followers_url": "https://api.github.com/users/wchan/followers", "following_url": "https://api.github.com/users/wchan/following{/other_user}", "gists_url": "https://api.github.com/users/wchan/gists{/gist_id}", "starred_url": "https://api.github.com/users/wchan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wchan/subscriptions", "organizations_url": "https://api.github.com/users/wchan/orgs", "repos_url": "https://api.github.com/users/wchan/repos", "events_url": "https://api.github.com/users/wchan/events{/privacy}", "received_events_url": "https://api.github.com/users/wchan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2015-12-28T04:35:33Z", "updated_at": "2018-01-11T23:34:48Z", "closed_at": "2015-12-28T16:56:20Z", "author_association": "NONE", "body_html": "<p>When using the AdamOptimizer, does the checkpoint save the state of the AdamOptimizer? If yes, how do you clear the state (i.e., restart the AdamOptimizer but keep the model weights from the checkpoint); if no, how do you maintain the Adam state. Question can be applied to AdaGrad as well.</p>", "body_text": "When using the AdamOptimizer, does the checkpoint save the state of the AdamOptimizer? If yes, how do you clear the state (i.e., restart the AdamOptimizer but keep the model weights from the checkpoint); if no, how do you maintain the Adam state. Question can be applied to AdaGrad as well.", "body": "When using the AdamOptimizer, does the checkpoint save the state of the AdamOptimizer? If yes, how do you clear the state (i.e., restart the AdamOptimizer but keep the model weights from the checkpoint); if no, how do you maintain the Adam state. Question can be applied to AdaGrad as well.\n"}