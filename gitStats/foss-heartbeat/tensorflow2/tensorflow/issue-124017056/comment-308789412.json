{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308789412", "html_url": "https://github.com/tensorflow/tensorflow/issues/634#issuecomment-308789412", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/634", "id": 308789412, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODc4OTQxMg==", "user": {"login": "xksteven", "id": 5234084, "node_id": "MDQ6VXNlcjUyMzQwODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5234084?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xksteven", "html_url": "https://github.com/xksteven", "followers_url": "https://api.github.com/users/xksteven/followers", "following_url": "https://api.github.com/users/xksteven/following{/other_user}", "gists_url": "https://api.github.com/users/xksteven/gists{/gist_id}", "starred_url": "https://api.github.com/users/xksteven/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xksteven/subscriptions", "organizations_url": "https://api.github.com/users/xksteven/orgs", "repos_url": "https://api.github.com/users/xksteven/repos", "events_url": "https://api.github.com/users/xksteven/events{/privacy}", "received_events_url": "https://api.github.com/users/xksteven/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-15T16:10:47Z", "updated_at": "2017-06-15T16:10:47Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7076445\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dylanbfox\">@dylanbfox</a></p>\n<p>If you're using the tensorflow built in saver then if you \"reset\" the adam optimizer by simply creating a new adam optimizer then it will create an entire extra set of parameters for each variable in the graph while maintaining the old unused ones.  This process will not work as a long term solution as you're almost doubling the size of graph every time you reset it in this way.</p>\n<p>It is not a viable solution.</p>\n<p>I honestly still don't have a good way to do this as one would need to iterate through all of the variables and reset all of the \"slot\" variables.  I do not know of a way that works to accomplish this.</p>\n<p>I would suggest keeping this issue open.</p>", "body_text": "@dylanbfox\nIf you're using the tensorflow built in saver then if you \"reset\" the adam optimizer by simply creating a new adam optimizer then it will create an entire extra set of parameters for each variable in the graph while maintaining the old unused ones.  This process will not work as a long term solution as you're almost doubling the size of graph every time you reset it in this way.\nIt is not a viable solution.\nI honestly still don't have a good way to do this as one would need to iterate through all of the variables and reset all of the \"slot\" variables.  I do not know of a way that works to accomplish this.\nI would suggest keeping this issue open.", "body": "@dylanbfox \r\n\r\nIf you're using the tensorflow built in saver then if you \"reset\" the adam optimizer by simply creating a new adam optimizer then it will create an entire extra set of parameters for each variable in the graph while maintaining the old unused ones.  This process will not work as a long term solution as you're almost doubling the size of graph every time you reset it in this way.  \r\n\r\nIt is not a viable solution.  \r\n\r\nI honestly still don't have a good way to do this as one would need to iterate through all of the variables and reset all of the \"slot\" variables.  I do not know of a way that works to accomplish this.\r\n\r\nI would suggest keeping this issue open."}