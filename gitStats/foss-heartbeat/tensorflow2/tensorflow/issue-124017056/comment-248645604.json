{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/248645604", "html_url": "https://github.com/tensorflow/tensorflow/issues/634#issuecomment-248645604", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/634", "id": 248645604, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODY0NTYwNA==", "user": {"login": "xksteven", "id": 5234084, "node_id": "MDQ6VXNlcjUyMzQwODQ=", "avatar_url": "https://avatars2.githubusercontent.com/u/5234084?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xksteven", "html_url": "https://github.com/xksteven", "followers_url": "https://api.github.com/users/xksteven/followers", "following_url": "https://api.github.com/users/xksteven/following{/other_user}", "gists_url": "https://api.github.com/users/xksteven/gists{/gist_id}", "starred_url": "https://api.github.com/users/xksteven/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xksteven/subscriptions", "organizations_url": "https://api.github.com/users/xksteven/orgs", "repos_url": "https://api.github.com/users/xksteven/repos", "events_url": "https://api.github.com/users/xksteven/events{/privacy}", "received_events_url": "https://api.github.com/users/xksteven/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-21T15:20:56Z", "updated_at": "2016-09-21T15:26:52Z", "author_association": "NONE", "body_html": "<p>That seems more like a \"hack\" in the sense that when I save my new model now I'll need to find a way to find and exclude the old optimizer variables and only save the old variables except the old optimizer and new optimizer. This starts to break the code.    I was interested in experimenting with this technique <a href=\"http://arxiv.org/abs/1608.03983\" rel=\"nofollow\">here</a> but other optimization schemes. However I don't see a nice way to do this once the model has been saved and I need to restart it.</p>\n<p>Unless I'm misunderstanding how you intend to code it up?</p>\n<p>I was imagining I have my graph:</p>\n<pre><code>def train():\n  with tf.Session() as sess:\n    inputs = get_inputs()\n    model_out = infer(inputs)\n    loss = (model_out, targets)\n    train_op = (loss)\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    saver.restore(sess, checkpoint)\n</code></pre>\n<p>That's the old model then I imagine the change your suggesting is to add the one below?</p>\n<pre><code>    .... (same as above)\n    saver.restore(sess, checkpoint)\n    train_op = (loss)\n    sess.run(tf.initialize_variables(tf.report_uninitialized_variables(tf.all_variables)))\n</code></pre>", "body_text": "That seems more like a \"hack\" in the sense that when I save my new model now I'll need to find a way to find and exclude the old optimizer variables and only save the old variables except the old optimizer and new optimizer. This starts to break the code.    I was interested in experimenting with this technique here but other optimization schemes. However I don't see a nice way to do this once the model has been saved and I need to restart it.\nUnless I'm misunderstanding how you intend to code it up?\nI was imagining I have my graph:\ndef train():\n  with tf.Session() as sess:\n    inputs = get_inputs()\n    model_out = infer(inputs)\n    loss = (model_out, targets)\n    train_op = (loss)\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    saver.restore(sess, checkpoint)\n\nThat's the old model then I imagine the change your suggesting is to add the one below?\n    .... (same as above)\n    saver.restore(sess, checkpoint)\n    train_op = (loss)\n    sess.run(tf.initialize_variables(tf.report_uninitialized_variables(tf.all_variables)))", "body": "That seems more like a \"hack\" in the sense that when I save my new model now I'll need to find a way to find and exclude the old optimizer variables and only save the old variables except the old optimizer and new optimizer. This starts to break the code.    I was interested in experimenting with this technique [here](http://arxiv.org/abs/1608.03983) but other optimization schemes. However I don't see a nice way to do this once the model has been saved and I need to restart it.  \n\nUnless I'm misunderstanding how you intend to code it up?\n\nI was imagining I have my graph:\n\n```\ndef train():\n  with tf.Session() as sess:\n    inputs = get_inputs()\n    model_out = infer(inputs)\n    loss = (model_out, targets)\n    train_op = (loss)\n    saver = tf.train.Saver()\n    sess.run(tf.initialize_all_variables())\n    saver.restore(sess, checkpoint)\n```\n\nThat's the old model then I imagine the change your suggesting is to add the one below?\n\n```\n    .... (same as above)\n    saver.restore(sess, checkpoint)\n    train_op = (loss)\n    sess.run(tf.initialize_variables(tf.report_uninitialized_variables(tf.all_variables)))\n```\n"}