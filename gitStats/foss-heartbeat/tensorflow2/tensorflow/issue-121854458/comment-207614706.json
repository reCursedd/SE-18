{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/207614706", "html_url": "https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207614706", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/491", "id": 207614706, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzYxNDcwNg==", "user": {"login": "elbamos", "id": 10103420, "node_id": "MDQ6VXNlcjEwMTAzNDIw", "avatar_url": "https://avatars2.githubusercontent.com/u/10103420?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elbamos", "html_url": "https://github.com/elbamos", "followers_url": "https://api.github.com/users/elbamos/followers", "following_url": "https://api.github.com/users/elbamos/following{/other_user}", "gists_url": "https://api.github.com/users/elbamos/gists{/gist_id}", "starred_url": "https://api.github.com/users/elbamos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elbamos/subscriptions", "organizations_url": "https://api.github.com/users/elbamos/orgs", "repos_url": "https://api.github.com/users/elbamos/repos", "events_url": "https://api.github.com/users/elbamos/events{/privacy}", "received_events_url": "https://api.github.com/users/elbamos/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T21:34:38Z", "updated_at": "2016-04-08T21:34:38Z", "author_association": "NONE", "body_html": "<p>You may be right. I will say, I observed this with torch, R, and tensorflow, all of them.</p>\n<p>My point is, the folks who are very eager for Mac-tf-gpu support so they can run tf on the gpus in apple laptops, which aren't remotely cuda-optimized anyway, I think they're going to find that the book isn't worth the candle, and there are easier ways to accomplish what they want.</p>\n<blockquote>\n<p>On Apr 8, 2016, at 5:27 PM, Anatolii Kmetiuk <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<p>Very interesting, indeed, but it doesn\u2019t proof that the problem is fundamental to OS X\u2019s architecture. We should try to isolate the problem and study it once we get a reliable way to compile TF under Mac. Maybe it was specific only to your particular configuration. I\u2019ll try the instructions above later.</p>\n<blockquote>\n<p>On Apr 9, 2016, at 00:19, elbamos <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<p>I don't have any links to provide or anything like that. It's just my personal experience.</p>\n<p>But I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw.</p>\n<p>An example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4.</p>\n<p>Processing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.</p>\n<p>I should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux.</p>\n<p>I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X.</p>\n<blockquote>\n<p>On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10103420\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/elbamos\">@elbamos</a> can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub</p>\n<p>\u2014<br>\nYou are receiving this because you are subscribed to this thread.<br>\nReply to this email directly or view it on GitHub <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"121854458\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/491\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/491/hovercard?comment_id=207609941&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207609941\">#491 (comment)</a></p>\n</blockquote>\n</blockquote>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub</p>\n</blockquote>", "body_text": "You may be right. I will say, I observed this with torch, R, and tensorflow, all of them.\nMy point is, the folks who are very eager for Mac-tf-gpu support so they can run tf on the gpus in apple laptops, which aren't remotely cuda-optimized anyway, I think they're going to find that the book isn't worth the candle, and there are easier ways to accomplish what they want.\n\nOn Apr 8, 2016, at 5:27 PM, Anatolii Kmetiuk notifications@github.com wrote:\nVery interesting, indeed, but it doesn\u2019t proof that the problem is fundamental to OS X\u2019s architecture. We should try to isolate the problem and study it once we get a reliable way to compile TF under Mac. Maybe it was specific only to your particular configuration. I\u2019ll try the instructions above later.\n\nOn Apr 9, 2016, at 00:19, elbamos notifications@github.com wrote:\nI don't have any links to provide or anything like that. It's just my personal experience.\nBut I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw.\nAn example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4.\nProcessing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\nI should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux.\nI was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X.\n\nOn Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n@elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub #491 (comment)\n\n\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub", "body": "You may be right. I will say, I observed this with torch, R, and tensorflow, all of them.\n\nMy point is, the folks who are very eager for Mac-tf-gpu support so they can run tf on the gpus in apple laptops, which aren't remotely cuda-optimized anyway, I think they're going to find that the book isn't worth the candle, and there are easier ways to accomplish what they want.\n\n> On Apr 8, 2016, at 5:27 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> \n> Very interesting, indeed, but it doesn\u2019t proof that the problem is fundamental to OS X\u2019s architecture. We should try to isolate the problem and study it once we get a reliable way to compile TF under Mac. Maybe it was specific only to your particular configuration. I\u2019ll try the instructions above later.\n> \n> > On Apr 9, 2016, at 00:19, elbamos notifications@github.com wrote:\n> > \n> > I don't have any links to provide or anything like that. It's just my personal experience. \n> > \n> > But I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw. \n> > \n> > An example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4. \n> > \n> > Processing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\n> > \n> > I should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux. \n> > \n> > I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X. \n> > \n> > > On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> > > \n> > > @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n> > > \n> > > \u2014\n> > > You are receiving this because you were mentioned.\n> > > Reply to this email directly or view it on GitHub\n> > > \n> > > \u2014\n> > > You are receiving this because you are subscribed to this thread.\n> > > Reply to this email directly or view it on GitHub https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207609941\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n"}