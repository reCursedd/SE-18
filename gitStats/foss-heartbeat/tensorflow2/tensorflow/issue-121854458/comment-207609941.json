{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/207609941", "html_url": "https://github.com/tensorflow/tensorflow/issues/491#issuecomment-207609941", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/491", "id": 207609941, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNzYwOTk0MQ==", "user": {"login": "elbamos", "id": 10103420, "node_id": "MDQ6VXNlcjEwMTAzNDIw", "avatar_url": "https://avatars2.githubusercontent.com/u/10103420?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elbamos", "html_url": "https://github.com/elbamos", "followers_url": "https://api.github.com/users/elbamos/followers", "following_url": "https://api.github.com/users/elbamos/following{/other_user}", "gists_url": "https://api.github.com/users/elbamos/gists{/gist_id}", "starred_url": "https://api.github.com/users/elbamos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elbamos/subscriptions", "organizations_url": "https://api.github.com/users/elbamos/orgs", "repos_url": "https://api.github.com/users/elbamos/repos", "events_url": "https://api.github.com/users/elbamos/events{/privacy}", "received_events_url": "https://api.github.com/users/elbamos/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-08T21:17:57Z", "updated_at": "2016-04-08T21:17:57Z", "author_association": "NONE", "body_html": "<p>I don't have any links to provide or anything like that. It's just my personal experience.</p>\n<p>But I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw.</p>\n<p>An example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4.</p>\n<p>Processing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.</p>\n<p>I should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux.</p>\n<p>I was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X.</p>\n<blockquote>\n<p>On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=10103420\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/elbamos\">@elbamos</a> can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly or view it on GitHub</p>\n</blockquote>", "body_text": "I don't have any links to provide or anything like that. It's just my personal experience.\nBut I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw.\nAn example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4.\nProcessing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\nI should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux.\nI was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X.\n\nOn Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n@elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly or view it on GitHub", "body": "I don't have any links to provide or anything like that. It's just my personal experience. \n\nBut I'm someone who was running tensorflow on OS X with gpu early on (look at the thread), and I switched to Linux specifically because I do a lot of net training with gpus, so take what I have to say fwiw. \n\nAn example: a few weeks ago I was on a deadline to finish training a deep net with torch when, having done something stupid, Linux decided that booting was no longer necessary. So I rebooted the machine into OS X. Using exactly the same code on exactly the same hardware with exactly the same data, on a gpu-centric application, with a dedicated gpu that wasn't driving any monitors, each epoch took roughly twice as long on OS X. That was with torch and cuda 7.5 and cudnn v 4. \n\nProcessing a single image for demonstration purposes, including all preprocessing and reconstruction steps, took 1 min 15 seconds on OS X, and 20 seconds when I was ultimately able to get back into Linux.\n\nI should add, the threading point is partly my speculation based on what I observed in performance running xgboost and other applications that use a multithreaded blas through R. For some reason applications seem to have an easier time using more cores on Linux. \n\nI was very surprised that gpu performance differed. My guess is that maybe the libraries, cuda and torch and such, aren't as optimized for OS X. \n\n> On Apr 8, 2016, at 4:26 PM, Anatolii Kmetiuk notifications@github.com wrote:\n> \n> @elbamos can you please give a link with more info on the performance? Or any way to reproduce the behaviour. Does Mac manage also the GPU threads? Aren't they supposed to be managed by the GPU controls and the drivers?\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly or view it on GitHub\n"}