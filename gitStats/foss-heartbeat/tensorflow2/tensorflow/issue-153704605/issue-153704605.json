{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2280", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2280/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2280/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2280/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2280", "id": 153704605, "node_id": "MDU6SXNzdWUxNTM3MDQ2MDU=", "number": 2280, "title": "Can we optimize redundant computation?", "user": {"login": "myme5261314", "id": 1814831, "node_id": "MDQ6VXNlcjE4MTQ4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1814831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myme5261314", "html_url": "https://github.com/myme5261314", "followers_url": "https://api.github.com/users/myme5261314/followers", "following_url": "https://api.github.com/users/myme5261314/following{/other_user}", "gists_url": "https://api.github.com/users/myme5261314/gists{/gist_id}", "starred_url": "https://api.github.com/users/myme5261314/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myme5261314/subscriptions", "organizations_url": "https://api.github.com/users/myme5261314/orgs", "repos_url": "https://api.github.com/users/myme5261314/repos", "events_url": "https://api.github.com/users/myme5261314/events{/privacy}", "received_events_url": "https://api.github.com/users/myme5261314/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-05-09T05:15:46Z", "updated_at": "2016-06-22T17:09:53Z", "closed_at": "2016-06-22T17:09:53Z", "author_association": "NONE", "body_html": "<p>I just find that for now, tensorflow haven't optimize the computation and leads to very slow performance. This issue I think might be very severe while compute the gradients to all trainable variables of a DNN network via <strong>tf.gradients()</strong>. To simplify the issue demonstration, I wrote a toy code snippets and run a small experiment.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-en\">@profile</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    a <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>]))\n    b <span class=\"pl-k\">=</span> a <span class=\"pl-k\">*</span> <span class=\"pl-c1\">2</span>\n    c <span class=\"pl-k\">=</span> b <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">1000</span>):\n        d <span class=\"pl-k\">=</span> sess.run(b)\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">1000</span>):\n        e <span class=\"pl-k\">=</span> sess.run(c)\n    <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">1000</span>):\n        f <span class=\"pl-k\">=</span> sess.run([b, c])\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>And here's the profile result using line_profiler.</p>\n<pre><code>Timer unit: 1e-06 s\n\nTotal time: 7.54528 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1         9650   9650.0      0.1      a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n     7         1         1611   1611.0      0.0      b = a * 2\n     8         1         1501   1501.0      0.0      c = b * 3\n     9         1       375872 375872.0      5.0      sess = tf.Session()\n    10         1       232079 232079.0      3.1      sess.run(tf.initialize_all_variables())\n    11                                           \n    12      1001         1663      1.7      0.0      for _ in xrange(1000):\n    13      1000      1321475   1321.5     17.5          d = sess.run(b)\n    14      1001         2179      2.2      0.0      for _ in xrange(1000):\n    15      1000      1570674   1570.7     20.8          e = sess.run(c)\n    16      1001         2835      2.8      0.0      for _ in xrange(1000):\n    17      1000      4025739   4025.7     53.4          f = sess.run([b, c])\n</code></pre>\n<p>You see what I mean? If the graph is like below:</p>\n<pre><code>a-------b--------c\n     *         *\n     |          |\n    2         3\n</code></pre>\n<p>And if calculating <code>b</code> and <code>c</code> will consume comparable 20% time, then when I want to get [b, c], it should be optimized to consume around 20% time, right? Since I already have result of \"b\" in the middle of the procedure if I want to get result of \"c\".</p>\n<p>Let's think about the gradient computing as I stated in the head of this issue. You have, say 10 layers DNN.<br>\nYou want to get the gradients to all the 10 \"W\" variables, then will the computation being very unoptimized? Since you treat the 10 gradients separately, you will compute gradients to the 10th layer first, and get the result, and compute gradients to the 9th layer while not using the result of 10th layer, and so on.</p>", "body_text": "I just find that for now, tensorflow haven't optimize the computation and leads to very slow performance. This issue I think might be very severe while compute the gradients to all trainable variables of a DNN network via tf.gradients(). To simplify the issue demonstration, I wrote a toy code snippets and run a small experiment.\nimport tensorflow as tf\nimport numpy as np\n\n@profile\ndef main():\n    a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n    b = a * 2\n    c = b * 3\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n    for _ in xrange(1000):\n        d = sess.run(b)\n    for _ in xrange(1000):\n        e = sess.run(c)\n    for _ in xrange(1000):\n        f = sess.run([b, c])\n\nif __name__ == '__main__':\n    main()\nAnd here's the profile result using line_profiler.\nTimer unit: 1e-06 s\n\nTotal time: 7.54528 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1         9650   9650.0      0.1      a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n     7         1         1611   1611.0      0.0      b = a * 2\n     8         1         1501   1501.0      0.0      c = b * 3\n     9         1       375872 375872.0      5.0      sess = tf.Session()\n    10         1       232079 232079.0      3.1      sess.run(tf.initialize_all_variables())\n    11                                           \n    12      1001         1663      1.7      0.0      for _ in xrange(1000):\n    13      1000      1321475   1321.5     17.5          d = sess.run(b)\n    14      1001         2179      2.2      0.0      for _ in xrange(1000):\n    15      1000      1570674   1570.7     20.8          e = sess.run(c)\n    16      1001         2835      2.8      0.0      for _ in xrange(1000):\n    17      1000      4025739   4025.7     53.4          f = sess.run([b, c])\n\nYou see what I mean? If the graph is like below:\na-------b--------c\n     *         *\n     |          |\n    2         3\n\nAnd if calculating b and c will consume comparable 20% time, then when I want to get [b, c], it should be optimized to consume around 20% time, right? Since I already have result of \"b\" in the middle of the procedure if I want to get result of \"c\".\nLet's think about the gradient computing as I stated in the head of this issue. You have, say 10 layers DNN.\nYou want to get the gradients to all the 10 \"W\" variables, then will the computation being very unoptimized? Since you treat the 10 gradients separately, you will compute gradients to the 10th layer first, and get the result, and compute gradients to the 9th layer while not using the result of 10th layer, and so on.", "body": "I just find that for now, tensorflow haven't optimize the computation and leads to very slow performance. This issue I think might be very severe while compute the gradients to all trainable variables of a DNN network via **tf.gradients()**. To simplify the issue demonstration, I wrote a toy code snippets and run a small experiment.\n\n``` python\nimport tensorflow as tf\nimport numpy as np\n\n@profile\ndef main():\n    a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n    b = a * 2\n    c = b * 3\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n\n    for _ in xrange(1000):\n        d = sess.run(b)\n    for _ in xrange(1000):\n        e = sess.run(c)\n    for _ in xrange(1000):\n        f = sess.run([b, c])\n\nif __name__ == '__main__':\n    main()\n```\n\nAnd here's the profile result using line_profiler.\n\n```\nTimer unit: 1e-06 s\n\nTotal time: 7.54528 s\nFile: test_tf_time.py\nFunction: main at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           @profile\n     5                                           def main():\n     6         1         9650   9650.0      0.1      a = tf.Variable(tf.random_normal(shape=[1000, 1000]))\n     7         1         1611   1611.0      0.0      b = a * 2\n     8         1         1501   1501.0      0.0      c = b * 3\n     9         1       375872 375872.0      5.0      sess = tf.Session()\n    10         1       232079 232079.0      3.1      sess.run(tf.initialize_all_variables())\n    11                                           \n    12      1001         1663      1.7      0.0      for _ in xrange(1000):\n    13      1000      1321475   1321.5     17.5          d = sess.run(b)\n    14      1001         2179      2.2      0.0      for _ in xrange(1000):\n    15      1000      1570674   1570.7     20.8          e = sess.run(c)\n    16      1001         2835      2.8      0.0      for _ in xrange(1000):\n    17      1000      4025739   4025.7     53.4          f = sess.run([b, c])\n```\n\nYou see what I mean? If the graph is like below:\n\n```\na-------b--------c\n     *         *\n     |          |\n    2         3\n```\n\nAnd if calculating `b` and `c` will consume comparable 20% time, then when I want to get [b, c], it should be optimized to consume around 20% time, right? Since I already have result of \"b\" in the middle of the procedure if I want to get result of \"c\".\n\nLet's think about the gradient computing as I stated in the head of this issue. You have, say 10 layers DNN.\nYou want to get the gradients to all the 10 \"W\" variables, then will the computation being very unoptimized? Since you treat the 10 gradients separately, you will compute gradients to the 10th layer first, and get the result, and compute gradients to the 9th layer while not using the result of 10th layer, and so on.\n"}