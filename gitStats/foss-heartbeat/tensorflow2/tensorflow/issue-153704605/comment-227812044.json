{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/227812044", "html_url": "https://github.com/tensorflow/tensorflow/issues/2280#issuecomment-227812044", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2280", "id": 227812044, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNzgxMjA0NA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-22T17:09:53Z", "updated_at": "2016-06-22T17:09:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The redundant computation <em>is</em> eliminated: <code>b.op</code> only executes once when you call <code>sess.run([b, c])</code>. Running this locally, it seems that almost all of the overhead in this microbenchmark is due to the additional copying of another 4MB tensor back to Python. To check this, you can see that <code>sess.run([b.op, c])</code> (which explicitly runs both ops but only copies back the output of <code>c</code>) takes the same time as <code>sess.run(c)</code>.</p>\n<p>The moral of the story is that you shouldn't copy tensors back to Python if you don't need them. Some of the immediate computation work is trying to make this more efficient, by returning handles to computed tensors back to Python rather than the entire values.</p>", "body_text": "The redundant computation is eliminated: b.op only executes once when you call sess.run([b, c]). Running this locally, it seems that almost all of the overhead in this microbenchmark is due to the additional copying of another 4MB tensor back to Python. To check this, you can see that sess.run([b.op, c]) (which explicitly runs both ops but only copies back the output of c) takes the same time as sess.run(c).\nThe moral of the story is that you shouldn't copy tensors back to Python if you don't need them. Some of the immediate computation work is trying to make this more efficient, by returning handles to computed tensors back to Python rather than the entire values.", "body": "The redundant computation _is_ eliminated: `b.op` only executes once when you call `sess.run([b, c])`. Running this locally, it seems that almost all of the overhead in this microbenchmark is due to the additional copying of another 4MB tensor back to Python. To check this, you can see that `sess.run([b.op, c])` (which explicitly runs both ops but only copies back the output of `c`) takes the same time as `sess.run(c)`.\n\nThe moral of the story is that you shouldn't copy tensors back to Python if you don't need them. Some of the immediate computation work is trying to make this more efficient, by returning handles to computed tensors back to Python rather than the entire values.\n"}