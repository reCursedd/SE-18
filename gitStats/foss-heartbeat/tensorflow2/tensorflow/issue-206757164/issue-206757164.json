{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7410", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7410/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7410/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7410/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7410", "id": 206757164, "node_id": "MDU6SXNzdWUyMDY3NTcxNjQ=", "number": 7410, "title": "How to update model parameters with accumulated gradients?", "user": {"login": "weixsong", "id": 7642350, "node_id": "MDQ6VXNlcjc2NDIzNTA=", "avatar_url": "https://avatars0.githubusercontent.com/u/7642350?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weixsong", "html_url": "https://github.com/weixsong", "followers_url": "https://api.github.com/users/weixsong/followers", "following_url": "https://api.github.com/users/weixsong/following{/other_user}", "gists_url": "https://api.github.com/users/weixsong/gists{/gist_id}", "starred_url": "https://api.github.com/users/weixsong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weixsong/subscriptions", "organizations_url": "https://api.github.com/users/weixsong/orgs", "repos_url": "https://api.github.com/users/weixsong/repos", "events_url": "https://api.github.com/users/weixsong/events{/privacy}", "received_events_url": "https://api.github.com/users/weixsong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-02-10T10:15:12Z", "updated_at": "2017-02-10T16:48:50Z", "closed_at": "2017-02-10T16:48:44Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>Due to some reason, my model has limited batch size, then this limited batch-size will make the model has a high variance.</p>\n<p>So, I want to use some trick to make the batch size larger. My idea is to store the gradients of each mini-batch, for example 64 mini-batches, and then sum the gradients together, use the mean gradients of this 64 mini batches of training data to update the model's parameters.</p>\n<p>This means that for the first 63 mini-batches, do not update the parameters, and after the 64 mini batch, update the model's parameters only once.</p>\n<p>But as TensorFlow is graph based, do anyone know how to implement this wanted feature?</p>\n<p>Thanks very much.</p>", "body_text": "Hi,\nDue to some reason, my model has limited batch size, then this limited batch-size will make the model has a high variance.\nSo, I want to use some trick to make the batch size larger. My idea is to store the gradients of each mini-batch, for example 64 mini-batches, and then sum the gradients together, use the mean gradients of this 64 mini batches of training data to update the model's parameters.\nThis means that for the first 63 mini-batches, do not update the parameters, and after the 64 mini batch, update the model's parameters only once.\nBut as TensorFlow is graph based, do anyone know how to implement this wanted feature?\nThanks very much.", "body": "Hi,\r\n\r\nDue to some reason, my model has limited batch size, then this limited batch-size will make the model has a high variance.\r\n\r\nSo, I want to use some trick to make the batch size larger. My idea is to store the gradients of each mini-batch, for example 64 mini-batches, and then sum the gradients together, use the mean gradients of this 64 mini batches of training data to update the model's parameters. \r\n\r\nThis means that for the first 63 mini-batches, do not update the parameters, and after the 64 mini batch, update the model's parameters only once.\r\n\r\nBut as TensorFlow is graph based, do anyone know how to implement this wanted feature?\r\n\r\nThanks very much."}