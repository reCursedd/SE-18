{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9090", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9090/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9090/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9090/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9090", "id": 220498093, "node_id": "MDU6SXNzdWUyMjA0OTgwOTM=", "number": 9090, "title": "Saver/Summary: The process cannot access the file because it is being used by another process.", "user": {"login": "krisjobs", "id": 10547611, "node_id": "MDQ6VXNlcjEwNTQ3NjEx", "avatar_url": "https://avatars1.githubusercontent.com/u/10547611?v=4", "gravatar_id": "", "url": "https://api.github.com/users/krisjobs", "html_url": "https://github.com/krisjobs", "followers_url": "https://api.github.com/users/krisjobs/followers", "following_url": "https://api.github.com/users/krisjobs/following{/other_user}", "gists_url": "https://api.github.com/users/krisjobs/gists{/gist_id}", "starred_url": "https://api.github.com/users/krisjobs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/krisjobs/subscriptions", "organizations_url": "https://api.github.com/users/krisjobs/orgs", "repos_url": "https://api.github.com/users/krisjobs/repos", "events_url": "https://api.github.com/users/krisjobs/events{/privacy}", "received_events_url": "https://api.github.com/users/krisjobs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2017-04-09T20:50:55Z", "updated_at": "2018-11-14T06:11:38Z", "closed_at": "2017-04-09T21:27:03Z", "author_association": "NONE", "body_html": "<p>Hey guys!</p>\n<p>I have been banging my head for a couple of days with the following:</p>\n<p>I am using a MonitoredTrainingSession with a single local server (but bug is identical even if using Supervisor and/or distributed architecture).</p>\n<p><code>#... sess  = tf.train.MonitoredTrainingSession(...)\\ while True:\\ sess.run(train_op)</code></p>\n<p>Assuming sequential runs of the python client without changing a single line of code,  the problem is that I sometimes get the error:<br>\n<code>Tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to rename: .../graph.pbtxt.tmp23e44d8fdce844e6822a56dc886588e6 to: .../graph.pbtxt : The process cannot access the file because it is being used by another process.</code></p>\n<p>... and the interesting thing is that sometimes this does not occur and training begins, but it then happens just as the first before_run call of the checkpoint hook tries to save the initial state (as seen in the MonitoredSession source code):</p>\n<p><code>Failed to rename: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001.tempstate17529973146728747180 to: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001 : The process cannot access the file because it is being used by another process.</code></p>\n<p>SOLVED: don't ever store log data and checkpoints in dropbox (:</p>", "body_text": "Hey guys!\nI have been banging my head for a couple of days with the following:\nI am using a MonitoredTrainingSession with a single local server (but bug is identical even if using Supervisor and/or distributed architecture).\n#... sess  = tf.train.MonitoredTrainingSession(...)\\ while True:\\ sess.run(train_op)\nAssuming sequential runs of the python client without changing a single line of code,  the problem is that I sometimes get the error:\nTensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to rename: .../graph.pbtxt.tmp23e44d8fdce844e6822a56dc886588e6 to: .../graph.pbtxt : The process cannot access the file because it is being used by another process.\n... and the interesting thing is that sometimes this does not occur and training begins, but it then happens just as the first before_run call of the checkpoint hook tries to save the initial state (as seen in the MonitoredSession source code):\nFailed to rename: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001.tempstate17529973146728747180 to: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001 : The process cannot access the file because it is being used by another process.\nSOLVED: don't ever store log data and checkpoints in dropbox (:", "body": "Hey guys!\r\n\r\nI have been banging my head for a couple of days with the following:\r\n\r\nI am using a MonitoredTrainingSession with a single local server (but bug is identical even if using Supervisor and/or distributed architecture).\r\n\r\n`#...\r\nsess  = tf.train.MonitoredTrainingSession(...)\\\r\nwhile True:\\\r\n   sess.run(train_op)`\r\n\r\nAssuming sequential runs of the python client without changing a single line of code,  the problem is that I sometimes get the error: \r\n`Tensorflow.python.framework.errors_impl.FailedPreconditionError: Failed to rename: .../graph.pbtxt.tmp23e44d8fdce844e6822a56dc886588e6 to: .../graph.pbtxt : The process cannot access the file because it is being used by another process.`\r\n\r\n... and the interesting thing is that sometimes this does not occur and training begins, but it then happens just as the first before_run call of the checkpoint hook tries to save the initial state (as seen in the MonitoredSession source code): \r\n\r\n`Failed to rename: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001.tempstate17529973146728747180 to: .../model.ckpt-1_temp_4fcb775402bc44568b86836d94747b27/part-00000-of-00001.data-00000-of-00001 : The process cannot access the file because it is being used by another process.`\r\n\r\nSOLVED: don't ever store log data and checkpoints in dropbox (:\r\n"}