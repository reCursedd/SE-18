{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11753", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11753/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11753/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11753/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11753", "id": 245447318, "node_id": "MDU6SXNzdWUyNDU0NDczMTg=", "number": 11753, "title": "tf.train.SyncReplicasOptimizer no synchronization among workers", "user": {"login": "smodlich", "id": 17433691, "node_id": "MDQ6VXNlcjE3NDMzNjkx", "avatar_url": "https://avatars2.githubusercontent.com/u/17433691?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smodlich", "html_url": "https://github.com/smodlich", "followers_url": "https://api.github.com/users/smodlich/followers", "following_url": "https://api.github.com/users/smodlich/following{/other_user}", "gists_url": "https://api.github.com/users/smodlich/gists{/gist_id}", "starred_url": "https://api.github.com/users/smodlich/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smodlich/subscriptions", "organizations_url": "https://api.github.com/users/smodlich/orgs", "repos_url": "https://api.github.com/users/smodlich/repos", "events_url": "https://api.github.com/users/smodlich/events{/privacy}", "received_events_url": "https://api.github.com/users/smodlich/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2017-07-25T15:47:28Z", "updated_at": "2018-11-04T13:22:45Z", "closed_at": "2017-08-01T07:53:12Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Have I written custom code : Yes</li>\n<li>OS Platform and Distribution : Linux</li>\n<li>TensorFlow installed from (source or binary)**: Binary</li>\n<li>TensorFlow version (use command below)**: 1.2.1</li>\n<li>Python version**: 3.5.2</li>\n</ul>\n<h3>Problem Description</h3>\n<p>I'm trying to train an rnn model with distributed synchronized training and between graph replication. I'm using tf.train.replica_device_setter. Asynchronous Training works perfectly fine. As written in the <a href=\"https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/training/sync_replicas_optimizer.py\">documentation</a> I'm wrapping my optimizer and creating the hook:</p>\n<pre><code>def training(loss,learning_rate,global_step,num_workers,is_chief):\n    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n    optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\n                                       total_num_replicas=num_workers)\n    gvs = optimizer.compute_gradients(loss)\n    capped_gvs = [(tf.clip_by_value(grad, -CLIPPING_THRESHOLD, CLIPPING_THRESHOLD), var) for       grad, var in gvs]\n    train_op = optimizer.apply_gradients(capped_gvs,global_step=global_step)\n    print('Is Chief?: ' + str(is_chief))\n    hook=optimizer.make_session_run_hook(is_chief)\n    return train_op,hook\n</code></pre>\n<p>For creating and running the Session I'm using exactly as told in the documentation:</p>\n<pre><code>sess = tf.train.MonitoredTrainingSession(master=server.target, is_chief=(task_index == 0),hooks=[hook])\nsess.run([train_op],feed_dict=...)\n</code></pre>\n<p>However as already noticed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"225714873\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9596\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9596/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9596\">#9596</a> and several other issues[<a href=\"https://stackoverflow.com/questions/41293576/distributed-tensorflow-good-example-for-synchronous-training-on-cpus\" rel=\"nofollow\">1</a>,<a href=\"https://github.com/tensorflow/tensorflow/issues/8978\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8978/hovercard\">2</a>] the training does not seem to synchronize among workers. So is there a bug in SyncReplicasOptimizer? I'm seeing several hints for this hypothesis:</p>\n<ol>\n<li>One worker is constantly ahead by several steps in my logs.</li>\n<li>When stopping one worker the other just continues with the training as if nothing happened. In a synchronized setting training should stop or crash.</li>\n<li>The training steps take approximately the same time as asynchronous training. Synchronous Training should be slower because of the synchronization.</li>\n</ol>\n<p>Questions:</p>\n<ol>\n<li>Is there any test with which one can confirm, that sync_replicas_optimizer.py really does synchronize?</li>\n<li>Is the API-Documentation regarding sync_replicas_optimizer.py up to date?</li>\n<li>Is this somehow related to tf.train.replica_device_setter as mentioned by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15792374\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jmchen-g\">@jmchen-g</a> in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"225714873\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9596\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9596/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9596\">#9596</a>?</li>\n<li>Are there any workarounds for this?</li>\n</ol>", "body_text": "System information\n\nHave I written custom code : Yes\nOS Platform and Distribution : Linux\nTensorFlow installed from (source or binary)**: Binary\nTensorFlow version (use command below)**: 1.2.1\nPython version**: 3.5.2\n\nProblem Description\nI'm trying to train an rnn model with distributed synchronized training and between graph replication. I'm using tf.train.replica_device_setter. Asynchronous Training works perfectly fine. As written in the documentation I'm wrapping my optimizer and creating the hook:\ndef training(loss,learning_rate,global_step,num_workers,is_chief):\n    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\n    optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\n                                       total_num_replicas=num_workers)\n    gvs = optimizer.compute_gradients(loss)\n    capped_gvs = [(tf.clip_by_value(grad, -CLIPPING_THRESHOLD, CLIPPING_THRESHOLD), var) for       grad, var in gvs]\n    train_op = optimizer.apply_gradients(capped_gvs,global_step=global_step)\n    print('Is Chief?: ' + str(is_chief))\n    hook=optimizer.make_session_run_hook(is_chief)\n    return train_op,hook\n\nFor creating and running the Session I'm using exactly as told in the documentation:\nsess = tf.train.MonitoredTrainingSession(master=server.target, is_chief=(task_index == 0),hooks=[hook])\nsess.run([train_op],feed_dict=...)\n\nHowever as already noticed in #9596 and several other issues[1,2] the training does not seem to synchronize among workers. So is there a bug in SyncReplicasOptimizer? I'm seeing several hints for this hypothesis:\n\nOne worker is constantly ahead by several steps in my logs.\nWhen stopping one worker the other just continues with the training as if nothing happened. In a synchronized setting training should stop or crash.\nThe training steps take approximately the same time as asynchronous training. Synchronous Training should be slower because of the synchronization.\n\nQuestions:\n\nIs there any test with which one can confirm, that sync_replicas_optimizer.py really does synchronize?\nIs the API-Documentation regarding sync_replicas_optimizer.py up to date?\nIs this somehow related to tf.train.replica_device_setter as mentioned by @jmchen-g in #9596?\nAre there any workarounds for this?", "body": "\r\n### System information\r\n- Have I written custom code : Yes\r\n- OS Platform and Distribution : Linux\r\n- TensorFlow installed from (source or binary)**: Binary\r\n- TensorFlow version (use command below)**: 1.2.1\r\n- Python version**: 3.5.2\r\n\r\n### Problem Description\r\nI'm trying to train an rnn model with distributed synchronized training and between graph replication. I'm using tf.train.replica_device_setter. Asynchronous Training works perfectly fine. As written in the [documentation](https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/training/sync_replicas_optimizer.py) I'm wrapping my optimizer and creating the hook:\r\n\r\n```\r\ndef training(loss,learning_rate,global_step,num_workers,is_chief):\r\n    optimizer=tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n    optimizer = tf.train.SyncReplicasOptimizer(optimizer, replicas_to_aggregate=num_workers,\r\n                                       total_num_replicas=num_workers)\r\n    gvs = optimizer.compute_gradients(loss)\r\n    capped_gvs = [(tf.clip_by_value(grad, -CLIPPING_THRESHOLD, CLIPPING_THRESHOLD), var) for       grad, var in gvs]\r\n    train_op = optimizer.apply_gradients(capped_gvs,global_step=global_step)\r\n    print('Is Chief?: ' + str(is_chief))\r\n    hook=optimizer.make_session_run_hook(is_chief)\r\n    return train_op,hook\r\n```\r\nFor creating and running the Session I'm using exactly as told in the documentation:\r\n```\r\nsess = tf.train.MonitoredTrainingSession(master=server.target, is_chief=(task_index == 0),hooks=[hook])\r\nsess.run([train_op],feed_dict=...)\r\n```\r\n\r\nHowever as already noticed in #9596 and several other issues[[1](https://stackoverflow.com/questions/41293576/distributed-tensorflow-good-example-for-synchronous-training-on-cpus),[2](https://github.com/tensorflow/tensorflow/issues/8978)] the training does not seem to synchronize among workers. So is there a bug in SyncReplicasOptimizer? I'm seeing several hints for this hypothesis: \r\n\r\n1. One worker is constantly ahead by several steps in my logs.\r\n2. When stopping one worker the other just continues with the training as if nothing happened. In a synchronized setting training should stop or crash.\r\n3. The training steps take approximately the same time as asynchronous training. Synchronous Training should be slower because of the synchronization.\r\n\r\nQuestions:\r\n1. Is there any test with which one can confirm, that sync_replicas_optimizer.py really does synchronize?\r\n2. Is the API-Documentation regarding sync_replicas_optimizer.py up to date?\r\n2. Is this somehow related to tf.train.replica_device_setter as mentioned by @jmchen-g in #9596?\r\n3. Are there any workarounds for this?\r\n"}