{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317912751", "html_url": "https://github.com/tensorflow/tensorflow/issues/11753#issuecomment-317912751", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11753", "id": 317912751, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzkxMjc1MQ==", "user": {"login": "utkrist", "id": 3055617, "node_id": "MDQ6VXNlcjMwNTU2MTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/3055617?v=4", "gravatar_id": "", "url": "https://api.github.com/users/utkrist", "html_url": "https://github.com/utkrist", "followers_url": "https://api.github.com/users/utkrist/followers", "following_url": "https://api.github.com/users/utkrist/following{/other_user}", "gists_url": "https://api.github.com/users/utkrist/gists{/gist_id}", "starred_url": "https://api.github.com/users/utkrist/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/utkrist/subscriptions", "organizations_url": "https://api.github.com/users/utkrist/orgs", "repos_url": "https://api.github.com/users/utkrist/repos", "events_url": "https://api.github.com/users/utkrist/events{/privacy}", "received_events_url": "https://api.github.com/users/utkrist/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T00:37:06Z", "updated_at": "2017-07-26T12:47:45Z", "author_association": "NONE", "body_html": "<p>I haven't looked at synchronizing with tf.train.MonitoredTrainingSession yet. However, I have some experience related to using tf.train.Supervisor instead. I was having the same problem as yours:<br>\n* that one of the replicas (chief) was far ahead than others, while other seemed to be waiting for some time</p>\n<p>I noticed the default value (30 second) for parameter 'recovery_wait_secs' that tf.train.Supervisor takes. Basically, every replica checks every 30 second to see if the model is ready. So, the chief starts immediately and the rest simply wait for 30 sec. After I set this value to 1, the replicas started training almost at the same time (except the first few steps). So, I suggest you to look at which input parameter of tf.train.MonitoredTrainingSession this time is set. This might be a direction to look at.</p>\n<p>(This following discussion also refers to the use of tf.train.Supervisor so please check for yourself if it holds): Another point I have observed is that it seems like SyncReplicaOptimizer does not really care if the 'replicas_to_aggregate' gradients come from the different workers or not. Even if other workers are waiting or not initialized, the chief starts training immediately. And if you print the global_steps you will see same global_steps for 'replicas_to_aggregate' times. This means that the chief pushes enough gradients for tf.train.SyncReplicaOptimizer to average and apply the gradients. So, start the chief worker process only after starting all other workers.</p>", "body_text": "I haven't looked at synchronizing with tf.train.MonitoredTrainingSession yet. However, I have some experience related to using tf.train.Supervisor instead. I was having the same problem as yours:\n* that one of the replicas (chief) was far ahead than others, while other seemed to be waiting for some time\nI noticed the default value (30 second) for parameter 'recovery_wait_secs' that tf.train.Supervisor takes. Basically, every replica checks every 30 second to see if the model is ready. So, the chief starts immediately and the rest simply wait for 30 sec. After I set this value to 1, the replicas started training almost at the same time (except the first few steps). So, I suggest you to look at which input parameter of tf.train.MonitoredTrainingSession this time is set. This might be a direction to look at.\n(This following discussion also refers to the use of tf.train.Supervisor so please check for yourself if it holds): Another point I have observed is that it seems like SyncReplicaOptimizer does not really care if the 'replicas_to_aggregate' gradients come from the different workers or not. Even if other workers are waiting or not initialized, the chief starts training immediately. And if you print the global_steps you will see same global_steps for 'replicas_to_aggregate' times. This means that the chief pushes enough gradients for tf.train.SyncReplicaOptimizer to average and apply the gradients. So, start the chief worker process only after starting all other workers.", "body": "I haven't looked at synchronizing with tf.train.MonitoredTrainingSession yet. However, I have some experience related to using tf.train.Supervisor instead. I was having the same problem as yours:\r\n    * that one of the replicas (chief) was far ahead than others, while other seemed to be waiting for some time\r\n\r\nI noticed the default value (30 second) for parameter 'recovery_wait_secs' that tf.train.Supervisor takes. Basically, every replica checks every 30 second to see if the model is ready. So, the chief starts immediately and the rest simply wait for 30 sec. After I set this value to 1, the replicas started training almost at the same time (except the first few steps). So, I suggest you to look at which input parameter of tf.train.MonitoredTrainingSession this time is set. This might be a direction to look at.\r\n\r\n(This following discussion also refers to the use of tf.train.Supervisor so please check for yourself if it holds): Another point I have observed is that it seems like SyncReplicaOptimizer does not really care if the 'replicas_to_aggregate' gradients come from the different workers or not. Even if other workers are waiting or not initialized, the chief starts training immediately. And if you print the global_steps you will see same global_steps for 'replicas_to_aggregate' times. This means that the chief pushes enough gradients for tf.train.SyncReplicaOptimizer to average and apply the gradients. So, start the chief worker process only after starting all other workers.\r\n"}