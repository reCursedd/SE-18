{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2298", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2298/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2298/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2298/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2298", "id": 153879633, "node_id": "MDU6SXNzdWUxNTM4Nzk2MzM=", "number": 2298, "title": "opt.compute_gradients() returns values different from the weight difference of opt.apply_gradients()", "user": {"login": "jhollowayj", "id": 7515474, "node_id": "MDQ6VXNlcjc1MTU0NzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/7515474?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jhollowayj", "html_url": "https://github.com/jhollowayj", "followers_url": "https://api.github.com/users/jhollowayj/followers", "following_url": "https://api.github.com/users/jhollowayj/following{/other_user}", "gists_url": "https://api.github.com/users/jhollowayj/gists{/gist_id}", "starred_url": "https://api.github.com/users/jhollowayj/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jhollowayj/subscriptions", "organizations_url": "https://api.github.com/users/jhollowayj/orgs", "repos_url": "https://api.github.com/users/jhollowayj/repos", "events_url": "https://api.github.com/users/jhollowayj/events{/privacy}", "received_events_url": "https://api.github.com/users/jhollowayj/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-05-09T21:58:41Z", "updated_at": "2016-05-17T00:46:45Z", "closed_at": "2016-05-17T00:46:45Z", "author_association": "NONE", "body_html": "<p>This isn't a bug, bug I wasn't sure where to ask for help on this one, so I'm sorry to just throw in another issue here on github.  I've tried searching, but haven't found answers to my questions yet.</p>\n<p>My question is what is the most efficient way to get the delta of my weights when training?  I assume it's related to gradients, but the optimizer (I think) does a lot more than just apply the raw gradients returned from <code>opt.compute_gradients()</code>.</p>\n<p><strong>Where I'm at</strong>: I've got the operators hooked up as follows (thanks to this <a href=\"http://stackoverflow.com/questions/34687761/efficiently-grab-gradients-from-tensorflow\" rel=\"nofollow\">SO question</a>):</p>\n<pre>self.cost = `the rest of the network`\nself.rmsprop = tf.train.RMSPropOptimizer(lr,rms_decay,0.0,rms_eps)\nself.comp_grads = self.rmsprop.compute_gradients(self.cost)\nself.grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape(), name=\"grad_placeholder\"), grad[1]) for grad in self.comp_grads]\nself.apply_grads = self.rmsprop.apply_gradients(self.grad_placeholder)\n</pre>\n<p>Now, to feed in information, I run the following:</p>\n<pre>feed_dict = `training variables`\nstart_weights = self.sess.run([self.w1,self.b1, etc])\ngrad_vals = self.sess.run([grad[0] for grad in self.comp_grads], feed_dict=feed_dict)\n\nfeed_dict2 = `feed_dict plus gradient values added to self.grad_placeholder`\nself.sess.run(self.apply_grads, feed_dict=feed_dict2) # Updates the weights\nend_weights = self.sess.run([self.w1,self.b1, etc])\ndelta_weights = [end_weights[i]-start_weights[i] for i in range(len(start_weights))]\n# Note: delta_weights != grad_vals...\n</pre>\n<p>The command of <code>run(self.apply_grads)</code> will update the network weights, but when I compute the differences in the starting and ending weights (<code>run(self.w1)</code>), those numbers are different than what is stored in <code>grad_vals[0]</code>.  I figure this is because the RMSPropOptimizer does more to the raw gradients, but I'm not sure what, or where to find out what it does.  I could go to the paper and write it myself, but I don't want to hardcode things in like that where TF already does it somewhere else.  I'll admit I'm still new to TF, but I tried to figure out what happens by looking at the source, but it's still confusing for me.</p>\n<p>So back to the question: Is there a way to get the delta's for the weights in a more efficient way than calculating the difference?  Am I stuck running <code>self.w1.eval(sess)</code> multiple times to get the weights and calculate the difference?  Is there something that I'm missing with the <code>tf.RMSPropOptimizer</code> (or any other Optimizer for that matter) function that I haven't seen or heard about</p>\n<p>Thanks!  And sorry again for adding to the issues list.</p>", "body_text": "This isn't a bug, bug I wasn't sure where to ask for help on this one, so I'm sorry to just throw in another issue here on github.  I've tried searching, but haven't found answers to my questions yet.\nMy question is what is the most efficient way to get the delta of my weights when training?  I assume it's related to gradients, but the optimizer (I think) does a lot more than just apply the raw gradients returned from opt.compute_gradients().\nWhere I'm at: I've got the operators hooked up as follows (thanks to this SO question):\nself.cost = `the rest of the network`\nself.rmsprop = tf.train.RMSPropOptimizer(lr,rms_decay,0.0,rms_eps)\nself.comp_grads = self.rmsprop.compute_gradients(self.cost)\nself.grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape(), name=\"grad_placeholder\"), grad[1]) for grad in self.comp_grads]\nself.apply_grads = self.rmsprop.apply_gradients(self.grad_placeholder)\n\nNow, to feed in information, I run the following:\nfeed_dict = `training variables`\nstart_weights = self.sess.run([self.w1,self.b1, etc])\ngrad_vals = self.sess.run([grad[0] for grad in self.comp_grads], feed_dict=feed_dict)\n\nfeed_dict2 = `feed_dict plus gradient values added to self.grad_placeholder`\nself.sess.run(self.apply_grads, feed_dict=feed_dict2) # Updates the weights\nend_weights = self.sess.run([self.w1,self.b1, etc])\ndelta_weights = [end_weights[i]-start_weights[i] for i in range(len(start_weights))]\n# Note: delta_weights != grad_vals...\n\nThe command of run(self.apply_grads) will update the network weights, but when I compute the differences in the starting and ending weights (run(self.w1)), those numbers are different than what is stored in grad_vals[0].  I figure this is because the RMSPropOptimizer does more to the raw gradients, but I'm not sure what, or where to find out what it does.  I could go to the paper and write it myself, but I don't want to hardcode things in like that where TF already does it somewhere else.  I'll admit I'm still new to TF, but I tried to figure out what happens by looking at the source, but it's still confusing for me.\nSo back to the question: Is there a way to get the delta's for the weights in a more efficient way than calculating the difference?  Am I stuck running self.w1.eval(sess) multiple times to get the weights and calculate the difference?  Is there something that I'm missing with the tf.RMSPropOptimizer (or any other Optimizer for that matter) function that I haven't seen or heard about\nThanks!  And sorry again for adding to the issues list.", "body": "This isn't a bug, bug I wasn't sure where to ask for help on this one, so I'm sorry to just throw in another issue here on github.  I've tried searching, but haven't found answers to my questions yet.\n\nMy question is what is the most efficient way to get the delta of my weights when training?  I assume it's related to gradients, but the optimizer (I think) does a lot more than just apply the raw gradients returned from `opt.compute_gradients()`.\n\n**Where I'm at**: I've got the operators hooked up as follows (thanks to this [SO question](http://stackoverflow.com/questions/34687761/efficiently-grab-gradients-from-tensorflow)):\n\n<pre>\nself.cost = `the rest of the network`\nself.rmsprop = tf.train.RMSPropOptimizer(lr,rms_decay,0.0,rms_eps)\nself.comp_grads = self.rmsprop.compute_gradients(self.cost)\nself.grad_placeholder = [(tf.placeholder(\"float\", shape=grad[1].get_shape(), name=\"grad_placeholder\"), grad[1]) for grad in self.comp_grads]\nself.apply_grads = self.rmsprop.apply_gradients(self.grad_placeholder)\n</pre>\n\n\nNow, to feed in information, I run the following:\n\n<pre>\nfeed_dict = `training variables`\nstart_weights = self.sess.run([self.w1,self.b1, etc])\ngrad_vals = self.sess.run([grad[0] for grad in self.comp_grads], feed_dict=feed_dict)\n\nfeed_dict2 = `feed_dict plus gradient values added to self.grad_placeholder`\nself.sess.run(self.apply_grads, feed_dict=feed_dict2) # Updates the weights\nend_weights = self.sess.run([self.w1,self.b1, etc])\ndelta_weights = [end_weights[i]-start_weights[i] for i in range(len(start_weights))]\n# Note: delta_weights != grad_vals...\n</pre>\n\n\nThe command of `run(self.apply_grads)` will update the network weights, but when I compute the differences in the starting and ending weights (`run(self.w1)`), those numbers are different than what is stored in `grad_vals[0]`.  I figure this is because the RMSPropOptimizer does more to the raw gradients, but I'm not sure what, or where to find out what it does.  I could go to the paper and write it myself, but I don't want to hardcode things in like that where TF already does it somewhere else.  I'll admit I'm still new to TF, but I tried to figure out what happens by looking at the source, but it's still confusing for me.\n\nSo back to the question: Is there a way to get the delta's for the weights in a more efficient way than calculating the difference?  Am I stuck running `self.w1.eval(sess)` multiple times to get the weights and calculate the difference?  Is there something that I'm missing with the `tf.RMSPropOptimizer` (or any other Optimizer for that matter) function that I haven't seen or heard about\n\nThanks!  And sorry again for adding to the issues list.\n"}