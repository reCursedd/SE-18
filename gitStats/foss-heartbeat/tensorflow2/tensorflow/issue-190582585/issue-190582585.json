{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5736", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5736/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5736/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5736/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5736", "id": 190582585, "node_id": "MDU6SXNzdWUxOTA1ODI1ODU=", "number": 5736, "title": "Training Cifar10 on two GPUs crashed.", "user": {"login": "Hwhitetooth", "id": 12207675, "node_id": "MDQ6VXNlcjEyMjA3Njc1", "avatar_url": "https://avatars0.githubusercontent.com/u/12207675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Hwhitetooth", "html_url": "https://github.com/Hwhitetooth", "followers_url": "https://api.github.com/users/Hwhitetooth/followers", "following_url": "https://api.github.com/users/Hwhitetooth/following{/other_user}", "gists_url": "https://api.github.com/users/Hwhitetooth/gists{/gist_id}", "starred_url": "https://api.github.com/users/Hwhitetooth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Hwhitetooth/subscriptions", "organizations_url": "https://api.github.com/users/Hwhitetooth/orgs", "repos_url": "https://api.github.com/users/Hwhitetooth/repos", "events_url": "https://api.github.com/users/Hwhitetooth/events{/privacy}", "received_events_url": "https://api.github.com/users/Hwhitetooth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "sherrym", "id": 12770037, "node_id": "MDQ6VXNlcjEyNzcwMDM3", "avatar_url": "https://avatars0.githubusercontent.com/u/12770037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sherrym", "html_url": "https://github.com/sherrym", "followers_url": "https://api.github.com/users/sherrym/followers", "following_url": "https://api.github.com/users/sherrym/following{/other_user}", "gists_url": "https://api.github.com/users/sherrym/gists{/gist_id}", "starred_url": "https://api.github.com/users/sherrym/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sherrym/subscriptions", "organizations_url": "https://api.github.com/users/sherrym/orgs", "repos_url": "https://api.github.com/users/sherrym/repos", "events_url": "https://api.github.com/users/sherrym/events{/privacy}", "received_events_url": "https://api.github.com/users/sherrym/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-11-20T20:48:52Z", "updated_at": "2017-06-16T16:00:25Z", "closed_at": "2017-06-16T16:00:25Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System:16.04.1 LTS (Xenial Xerus)</p>\n<p>Installed version of CUDA and cuDNN: 8.0 and 5.0.5<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n-rw-r--r-- 1 root root   558720 Nov 19 10:33 /usr/local/cuda/lib64/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root       16 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.8.0<br>\nlrwxrwxrwx 1 root root       19 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.44<br>\n-rwxr-xr-x 1 root root   415432 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0.44<br>\n-rw-r--r-- 1 root root   775162 Nov 19 10:33 /usr/local/cuda/lib64/libcudart_static.a<br>\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so<br>\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5<br>\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5.1.5<br>\n-rw-r--r-- 1 root root 69756172 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn_static.a</p>\n<p>Installed from source:</p>\n<ol>\n<li>The commit hash: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/8157ae2552f4ec031e9f3183e1dede66444320fd/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/8157ae2552f4ec031e9f3183e1dede66444320fd\"><tt>8157ae2</tt></a></li>\n<li>The output of <code>bazel version</code>:<br>\nBuild label: 0.3.1<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)<br>\nBuild timestamp: 1469783392<br>\nBuild timestamp as int: 1469783392</li>\n</ol>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>python cifar10_multi_gpu_train.py --num_gpus=2<br>\n(Both 'python cifar10_train.py' and 'python cifar10_multi_gpu_train.py --num_gpus=1' work well.)</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>I tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)<br>\nERROR:tensorflow:Exception in QueueRunner: Expected begin[0] in [0, 32], but got -6<br>\n[[Node: tower_1/random_crop = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/Cast_1, tower_1/random_crop/mod, tower_1/random_crop/size)]]<br>\n[[Node: tower_1/Div/_82 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_131_tower_1/Div\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>]]<br>\nCaused by op u'tower_1/random_crop', defined at:<br>\nFile \"cifar10_multi_gpu_train.py\", line 289, in <br>\ntf.app.run()<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/platform/app.py\", line 30, in run<br>\nsys.exit(main(sys.argv))<br>\nFile \"cifar10_multi_gpu_train.py\", line 285, in main<br>\ntrain()<br>\nFile \"cifar10_multi_gpu_train.py\", line 189, in train<br>\nloss = tower_loss(scope)<br>\nFile \"cifar10_multi_gpu_train.py\", line 76, in tower_loss<br>\nimages, labels = cifar10.distorted_inputs()<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10.py\", line 156, in distorted_inputs<br>\nbatch_size=FLAGS.batch_size)<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10_input.py\", line 172, in distorted_inputs<br>\ndistorted_image = tf.random_crop(reshaped_image, [height, width, 3])<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/random_ops.py\", line 326, in random_crop<br>\nreturn array_ops.slice(value, offset, size, name=name)<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/array_ops.py\", line 328, in slice<br>\nreturn gen_array_ops.<em>slice(input</em>, begin, size, name=name)<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/gen_array_ops.py\", line 2009, in _slice<br>\nname=name)<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op<br>\nop_def=op_def)<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 2322, in create_op<br>\noriginal_op=self._default_original_op, op_def=op_def)<br>\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 1244, in <strong>init</strong><br>\nself._traceback = _extract_stack()</p>\n<p>Other threads reported similar errors.</p>\n<p>Please help! I'm looking forward to your reply.<br>\nSincerely</p>", "body_text": "Environment info\nOperating System:16.04.1 LTS (Xenial Xerus)\nInstalled version of CUDA and cuDNN: 8.0 and 5.0.5\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root   558720 Nov 19 10:33 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\n-rwxr-xr-x 1 root root   415432 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0.44\n-rw-r--r-- 1 root root   775162 Nov 19 10:33 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5.1.5\n-rw-r--r-- 1 root root 69756172 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn_static.a\nInstalled from source:\n\nThe commit hash: 8157ae2\nThe output of bazel version:\nBuild label: 0.3.1\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\nBuild timestamp: 1469783392\nBuild timestamp as int: 1469783392\n\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\npython cifar10_multi_gpu_train.py --num_gpus=2\n(Both 'python cifar10_train.py' and 'python cifar10_multi_gpu_train.py --num_gpus=1' work well.)\nLogs or other output that would be helpful\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)\nERROR:tensorflow:Exception in QueueRunner: Expected begin[0] in [0, 32], but got -6\n[[Node: tower_1/random_crop = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/Cast_1, tower_1/random_crop/mod, tower_1/random_crop/size)]]\n[[Node: tower_1/Div/_82 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_131_tower_1/Div\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\nCaused by op u'tower_1/random_crop', defined at:\nFile \"cifar10_multi_gpu_train.py\", line 289, in \ntf.app.run()\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/platform/app.py\", line 30, in run\nsys.exit(main(sys.argv))\nFile \"cifar10_multi_gpu_train.py\", line 285, in main\ntrain()\nFile \"cifar10_multi_gpu_train.py\", line 189, in train\nloss = tower_loss(scope)\nFile \"cifar10_multi_gpu_train.py\", line 76, in tower_loss\nimages, labels = cifar10.distorted_inputs()\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10.py\", line 156, in distorted_inputs\nbatch_size=FLAGS.batch_size)\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10_input.py\", line 172, in distorted_inputs\ndistorted_image = tf.random_crop(reshaped_image, [height, width, 3])\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/random_ops.py\", line 326, in random_crop\nreturn array_ops.slice(value, offset, size, name=name)\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/array_ops.py\", line 328, in slice\nreturn gen_array_ops.slice(input, begin, size, name=name)\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/gen_array_ops.py\", line 2009, in _slice\nname=name)\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\nop_def=op_def)\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 2322, in create_op\noriginal_op=self._default_original_op, op_def=op_def)\nFile \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 1244, in init\nself._traceback = _extract_stack()\nOther threads reported similar errors.\nPlease help! I'm looking forward to your reply.\nSincerely", "body": "### Environment info\r\nOperating System:16.04.1 LTS (Xenial Xerus)\r\n\r\nInstalled version of CUDA and cuDNN: 8.0 and 5.0.5\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n-rw-r--r-- 1 root root   558720 Nov 19 10:33 /usr/local/cuda/lib64/libcudadevrt.a\r\nlrwxrwxrwx 1 root root       16 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.8.0\r\nlrwxrwxrwx 1 root root       19 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0 -> libcudart.so.8.0.44\r\n-rwxr-xr-x 1 root root   415432 Nov 19 10:33 /usr/local/cuda/lib64/libcudart.so.8.0.44\r\n-rw-r--r-- 1 root root   775162 Nov 19 10:33 /usr/local/cuda/lib64/libcudart_static.a\r\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so\r\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5\r\n-rwxr-xr-x 1 root root 79337624 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn.so.5.1.5\r\n-rw-r--r-- 1 root root 69756172 Nov 19 10:36 /usr/local/cuda/lib64/libcudnn_static.a\r\n\r\nInstalled from source:\r\n1. The commit hash: 8157ae2552f4ec031e9f3183e1dede66444320fd\r\n2. The output of `bazel version`:\r\nBuild label: 0.3.1\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Fri Jul 29 09:09:52 2016 (1469783392)\r\nBuild timestamp: 1469783392\r\nBuild timestamp as int: 1469783392\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\npython cifar10_multi_gpu_train.py --num_gpus=2\r\n(Both 'python cifar10_train.py' and 'python cifar10_multi_gpu_train.py --num_gpus=1' work well.)\r\n\r\n### Logs or other output that would be helpful\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:03:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: TITAN X (Pascal), pci bus id: 0000:04:00.0)\r\nERROR:tensorflow:Exception in QueueRunner: Expected begin[0] in [0, 32], but got -6\r\n\t [[Node: tower_1/random_crop = Slice[Index=DT_INT32, T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/gpu:1\"](tower_1/Cast_1, tower_1/random_crop/mod, tower_1/random_crop/size)]]\r\n\t [[Node: tower_1/Div/_82 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:1\", send_device_incarnation=1, tensor_name=\"edge_131_tower_1/Div\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\nCaused by op u'tower_1/random_crop', defined at:\r\n  File \"cifar10_multi_gpu_train.py\", line 289, in <module>\r\n    tf.app.run()\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/platform/app.py\", line 30, in run\r\n    sys.exit(main(sys.argv))\r\n  File \"cifar10_multi_gpu_train.py\", line 285, in main\r\n    train()\r\n  File \"cifar10_multi_gpu_train.py\", line 189, in train\r\n    loss = tower_loss(scope)\r\n  File \"cifar10_multi_gpu_train.py\", line 76, in tower_loss\r\n    images, labels = cifar10.distorted_inputs()\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10.py\", line 156, in distorted_inputs\r\n    batch_size=FLAGS.batch_size)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/models/image/cifar10/cifar10_input.py\", line 172, in distorted_inputs\r\n    distorted_image = tf.random_crop(reshaped_image, [height, width, 3])\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/random_ops.py\", line 326, in random_crop\r\n    return array_ops.slice(value, offset, size, name=name)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/array_ops.py\", line 328, in slice\r\n    return gen_array_ops._slice(input_, begin, size, name=name)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/ops/gen_array_ops.py\", line 2009, in _slice\r\n    name=name)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\r\n    op_def=op_def)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 2322, in create_op\r\n    original_op=self._default_original_op, op_def=op_def)\r\n  File \"/home/petuum/projects/Poseidon-TensorFlow/_python_build/tensorflow/python/framework/ops.py\", line 1244, in __init__\r\n    self._traceback = _extract_stack()\r\n\r\nOther threads reported similar errors.\r\n\r\nPlease help! I'm looking forward to your reply.\r\nSincerely"}