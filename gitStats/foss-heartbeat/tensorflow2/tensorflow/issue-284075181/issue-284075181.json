{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15572", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15572/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15572/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15572/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15572", "id": 284075181, "node_id": "MDU6SXNzdWUyODQwNzUxODE=", "number": 15572, "title": "layers.Conv3DTranspose doesn't work with unspecified DWH dimensions", "user": {"login": "robb-brown", "id": 3769177, "node_id": "MDQ6VXNlcjM3NjkxNzc=", "avatar_url": "https://avatars2.githubusercontent.com/u/3769177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/robb-brown", "html_url": "https://github.com/robb-brown", "followers_url": "https://api.github.com/users/robb-brown/followers", "following_url": "https://api.github.com/users/robb-brown/following{/other_user}", "gists_url": "https://api.github.com/users/robb-brown/gists{/gist_id}", "starred_url": "https://api.github.com/users/robb-brown/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/robb-brown/subscriptions", "organizations_url": "https://api.github.com/users/robb-brown/orgs", "repos_url": "https://api.github.com/users/robb-brown/repos", "events_url": "https://api.github.com/users/robb-brown/events{/privacy}", "received_events_url": "https://api.github.com/users/robb-brown/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-12-22T05:25:02Z", "updated_at": "2018-09-12T15:04:00Z", "closed_at": "2018-09-12T15:04:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes (n/a)</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04 (All affected)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: n/a</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: n/a</li>\n<li><strong>CUDA/cuDNN version</strong>: n/a</li>\n<li><strong>GPU model and memory</strong>: CPU</li>\n<li><strong>Exact command to reproduce</strong>: n/a</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>tensorflow.layers.Conv3DTranspose doesn't work when the input dimensions are not specified and bias is added.  This occurs because the layer output is reshaped from five dimensions to 4 by combining the depth and height dimensions:  None * None throws an exception.  The code for this is in tensorflow/python/layers/convolutional.py, line 1608.</p>\n<p>It appears that this reshaping is done because nn.bias.add can't handle 5 dimensional inputs.  This seems like it should be an easy fix since nn.bias.add just broadcasts across all but the channel dimension anyway.  The *Transpose layers should really all use the same general _ConvTranspose style that the non-transposed convolution layers use, to avoid code duplication.</p>\n<p>In the meantime, tensorflow.layers.Conv3D DOES work with unspecified input dimensions and channel-last ordering.  It turns out that Conv3D and Conv3DTranspose use different code for adding the bias.  In lieu of a nn.bias.add fix, this problem can be alleviated by using the Conv3D bias code for Conv3DTranspose.</p>\n<h3>Suggested fix (tested)</h3>\n<p>In tensorflow/python/layers/convolutional.py, replace lines 1609-1625 with (modified from lines 169-189):</p>\n<pre><code>    if self.data_format == 'channels_first':\n          outputs_shape = outputs.shape.as_list()\n          outputs_4d = array_ops.reshape(outputs,\n                                         [outputs_shape[0], outputs_shape[1],\n                                          outputs_shape[2] * outputs_shape[3],\n                                          outputs_shape[4]])\n          outputs_4d = nn.bias_add(outputs_4d, self.bias, data_format='NCHW')\n          outputs = array_ops.reshape(outputs_4d, outputs_shape)\n      else:\n        outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes (n/a)\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04 (All affected)\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4.0\nPython version: 3.6\nBazel version (if compiling from source): n/a\nGCC/Compiler version (if compiling from source): n/a\nCUDA/cuDNN version: n/a\nGPU model and memory: CPU\nExact command to reproduce: n/a\n\nDescribe the problem\ntensorflow.layers.Conv3DTranspose doesn't work when the input dimensions are not specified and bias is added.  This occurs because the layer output is reshaped from five dimensions to 4 by combining the depth and height dimensions:  None * None throws an exception.  The code for this is in tensorflow/python/layers/convolutional.py, line 1608.\nIt appears that this reshaping is done because nn.bias.add can't handle 5 dimensional inputs.  This seems like it should be an easy fix since nn.bias.add just broadcasts across all but the channel dimension anyway.  The *Transpose layers should really all use the same general _ConvTranspose style that the non-transposed convolution layers use, to avoid code duplication.\nIn the meantime, tensorflow.layers.Conv3D DOES work with unspecified input dimensions and channel-last ordering.  It turns out that Conv3D and Conv3DTranspose use different code for adding the bias.  In lieu of a nn.bias.add fix, this problem can be alleviated by using the Conv3D bias code for Conv3DTranspose.\nSuggested fix (tested)\nIn tensorflow/python/layers/convolutional.py, replace lines 1609-1625 with (modified from lines 169-189):\n    if self.data_format == 'channels_first':\n          outputs_shape = outputs.shape.as_list()\n          outputs_4d = array_ops.reshape(outputs,\n                                         [outputs_shape[0], outputs_shape[1],\n                                          outputs_shape[2] * outputs_shape[3],\n                                          outputs_shape[4]])\n          outputs_4d = nn.bias_add(outputs_4d, self.bias, data_format='NCHW')\n          outputs = array_ops.reshape(outputs_4d, outputs_shape)\n      else:\n        outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes (n/a)\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04 (All affected)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: n/a\r\n- **GCC/Compiler version (if compiling from source)**: n/a\r\n- **CUDA/cuDNN version**: n/a\r\n- **GPU model and memory**: CPU\r\n- **Exact command to reproduce**: n/a\r\n\r\n### Describe the problem\r\n\r\ntensorflow.layers.Conv3DTranspose doesn't work when the input dimensions are not specified and bias is added.  This occurs because the layer output is reshaped from five dimensions to 4 by combining the depth and height dimensions:  None * None throws an exception.  The code for this is in tensorflow/python/layers/convolutional.py, line 1608.\r\n\r\nIt appears that this reshaping is done because nn.bias.add can't handle 5 dimensional inputs.  This seems like it should be an easy fix since nn.bias.add just broadcasts across all but the channel dimension anyway.  The *Transpose layers should really all use the same general _ConvTranspose style that the non-transposed convolution layers use, to avoid code duplication.\r\n\r\nIn the meantime, tensorflow.layers.Conv3D DOES work with unspecified input dimensions and channel-last ordering.  It turns out that Conv3D and Conv3DTranspose use different code for adding the bias.  In lieu of a nn.bias.add fix, this problem can be alleviated by using the Conv3D bias code for Conv3DTranspose.\r\n\r\n### Suggested fix (tested)\r\n\r\nIn tensorflow/python/layers/convolutional.py, replace lines 1609-1625 with (modified from lines 169-189): \r\n\r\n        if self.data_format == 'channels_first':\r\n              outputs_shape = outputs.shape.as_list()\r\n              outputs_4d = array_ops.reshape(outputs,\r\n                                             [outputs_shape[0], outputs_shape[1],\r\n                                              outputs_shape[2] * outputs_shape[3],\r\n                                              outputs_shape[4]])\r\n              outputs_4d = nn.bias_add(outputs_4d, self.bias, data_format='NCHW')\r\n              outputs = array_ops.reshape(outputs_4d, outputs_shape)\r\n          else:\r\n            outputs = nn.bias_add(outputs, self.bias, data_format='NHWC')\r\n"}