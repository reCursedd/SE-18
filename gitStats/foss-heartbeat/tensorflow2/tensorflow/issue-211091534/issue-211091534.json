{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7976", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7976/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7976/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7976/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7976", "id": 211091534, "node_id": "MDU6SXNzdWUyMTEwOTE1MzQ=", "number": 7976, "title": "distributed tensorflow data Data in parallel\uff0cps server can only use cpu? use gpu is faster?", "user": {"login": "guoying1030", "id": 17998982, "node_id": "MDQ6VXNlcjE3OTk4OTgy", "avatar_url": "https://avatars2.githubusercontent.com/u/17998982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guoying1030", "html_url": "https://github.com/guoying1030", "followers_url": "https://api.github.com/users/guoying1030/followers", "following_url": "https://api.github.com/users/guoying1030/following{/other_user}", "gists_url": "https://api.github.com/users/guoying1030/gists{/gist_id}", "starred_url": "https://api.github.com/users/guoying1030/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guoying1030/subscriptions", "organizations_url": "https://api.github.com/users/guoying1030/orgs", "repos_url": "https://api.github.com/users/guoying1030/repos", "events_url": "https://api.github.com/users/guoying1030/events{/privacy}", "received_events_url": "https://api.github.com/users/guoying1030/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-03-01T14:09:26Z", "updated_at": "2017-03-02T22:55:38Z", "closed_at": "2017-03-02T22:55:38Z", "author_association": "NONE", "body_html": "<p>When I use kears and tensorflow to do the parallel,use sgd sync optimizer I have 10 machines,<br>\neach machine has a gpu, so I do</p>\n<p>1.Each gpu corresponds to a worker hosts\uff0cuse cuda_visible_devices<br>\n2.every ps server only use cpu<br>\n3.use Gigabit Ethernet<br>\nwhen i run it,i find it is so lower,,</p>\n<p>who can tell me ?what is wrong??</p>\n<p>flags.DEFINE_string(\"train_desc_file\",'train.json','train config')<br>\nflags.DEFINE_string(\"val_desc_file\",'test.json','dev config')<br>\nflags.DEFINE_string(\"save_dir\",'model','save model dir')<br>\nflags.DEFINE_integer('epochs',1000,'epoch number')<br>\nflags.DEFINE_integer('batch_size',64,'batch_size')<br>\nflags.DEFINE_string(\"ps_hosts\",\"172.16.186.86:2221\",<br>\n\"Comma-separated list of hostname:port pairs\")<br>\nflags.DEFINE_string(\"worker_hosts\", \"172.16.186.86:2222,172.16.186.86:2223\",\"Comma-separated list of hostname:port pairs\")<br>\nflags.DEFINE_string(\"job_name\", None,\"job name: worker or ps\")<br>\nflags.DEFINE_integer(\"task_index\", None,<br>\n\"Worker task index, should be &gt;= 0. task_index=0 is \"<br>\n\"the master worker task the performs the variable \"<br>\n\"initialization \")</p>\n<p>if FLAGS.job_name is None or FLAGS.job_name == \"\":<br>\nraise ValueError(\"Must specify an explicit <code>job_name</code>\")<br>\nif FLAGS.task_index is None or FLAGS.task_index == \"\":<br>\nraise ValueError(\"Must specify an explicit <code>task_index</code>\")<br>\nprint(\"job name = %s\" % FLAGS.job_name)<br>\nprint(\"task index = %d\" % FLAGS.task_index)<br>\nps_spec = FLAGS.ps_hosts.split(\",\")<br>\nworker_spec = FLAGS.worker_hosts.split(\",\")<br>\nnum_workers = len(worker_spec)</p>\n<pre><code>cluster = tf.train.ClusterSpec({\n    \"ps\": ps_spec,\n    \"worker\": worker_spec})\nserver = tf.train.Server(\n    cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\n    server.join()\n\nwith tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n    is_chief = (FLAGS.task_index == 0)\n    model=....\n    acoustic_input = model.inputs[0]\n    network_output = model.outputs[0]\n    network_output = tf.transpose(network_output, [1, 0, 2])\n    ctc_cost = warpctc_tensorflow.ctc(network_output, sess_label, sess_label_lens, sess_output_lens)\n    ctc_cost = tf.reduce_mean(ctc_cost)\n    trainable_vars = model.trainable_weights\n    learning_rate = 2e-4\n\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    # optimizer = SGD(lr=learning_rate, momentum=0.9, nesterov=True,clipnorm=100)\n    optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.99,use_nesterov=True)\n    optimizer = tf.train.SyncReplicasOptimizer(\n        optimizer,\n        replicas_to_aggregate=num_workers,\n        total_num_replicas=num_workers,\n        use_locking=True,\n        name=\"sync_replicas\")\n\n    grads = optimizer.compute_gradients(ctc_cost, trainable_vars)\n    grads, _ = tf.clip_by_global_norm(tf.gradients(ctc_cost, trainable_vars), 100)\n    train_op = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)\n    time_begin = time.time()\n\n    chief_queue_runner = optimizer.get_chief_queue_runner()\n    sync_init_op = optimizer.get_init_tokens_op()\n    ready_for_local_init_op = optimizer.ready_for_local_init_op\n    K.manual_variable_initialization(True)\n    local_init_op = optimizer.local_step_init_op\n    if is_chief:\n        local_init_op = optimizer.chief_init_op\n    sv = tf.train.Supervisor(\n        is_chief=is_chief,\n        logdir='train_log',\n        local_init_op=local_init_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        recovery_wait_secs=1,\n        global_step=global_step)\n\n    sess = sv.prepare_or_wait_for_session(server.target)\n    K.set_session(sess)\n    K.get_session().run(sync_init_op)\n    sv.start_queue_runners(sess, [chief_queue_runner])\n    print(\"Training begins @ %f\" % time_begin)\n    main(FLAGS.train_desc_file, FLAGS.val_desc_file, FLAGS.epochs, FLAGS.save_dir,\n         FLAGS.sortgrad, model=model, run_require_op=[ctc_cost, train_op], val_fn=val_fn)\n</code></pre>\n<p>but when i run it,</p>", "body_text": "When I use kears and tensorflow to do the parallel,use sgd sync optimizer I have 10 machines,\neach machine has a gpu, so I do\n1.Each gpu corresponds to a worker hosts\uff0cuse cuda_visible_devices\n2.every ps server only use cpu\n3.use Gigabit Ethernet\nwhen i run it,i find it is so lower,,\nwho can tell me ?what is wrong??\nflags.DEFINE_string(\"train_desc_file\",'train.json','train config')\nflags.DEFINE_string(\"val_desc_file\",'test.json','dev config')\nflags.DEFINE_string(\"save_dir\",'model','save model dir')\nflags.DEFINE_integer('epochs',1000,'epoch number')\nflags.DEFINE_integer('batch_size',64,'batch_size')\nflags.DEFINE_string(\"ps_hosts\",\"172.16.186.86:2221\",\n\"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"worker_hosts\", \"172.16.186.86:2222,172.16.186.86:2223\",\"Comma-separated list of hostname:port pairs\")\nflags.DEFINE_string(\"job_name\", None,\"job name: worker or ps\")\nflags.DEFINE_integer(\"task_index\", None,\n\"Worker task index, should be >= 0. task_index=0 is \"\n\"the master worker task the performs the variable \"\n\"initialization \")\nif FLAGS.job_name is None or FLAGS.job_name == \"\":\nraise ValueError(\"Must specify an explicit job_name\")\nif FLAGS.task_index is None or FLAGS.task_index == \"\":\nraise ValueError(\"Must specify an explicit task_index\")\nprint(\"job name = %s\" % FLAGS.job_name)\nprint(\"task index = %d\" % FLAGS.task_index)\nps_spec = FLAGS.ps_hosts.split(\",\")\nworker_spec = FLAGS.worker_hosts.split(\",\")\nnum_workers = len(worker_spec)\ncluster = tf.train.ClusterSpec({\n    \"ps\": ps_spec,\n    \"worker\": worker_spec})\nserver = tf.train.Server(\n    cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\nif FLAGS.job_name == \"ps\":\n    server.join()\n\nwith tf.device(tf.train.replica_device_setter(\n        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n        cluster=cluster)):\n    is_chief = (FLAGS.task_index == 0)\n    model=....\n    acoustic_input = model.inputs[0]\n    network_output = model.outputs[0]\n    network_output = tf.transpose(network_output, [1, 0, 2])\n    ctc_cost = warpctc_tensorflow.ctc(network_output, sess_label, sess_label_lens, sess_output_lens)\n    ctc_cost = tf.reduce_mean(ctc_cost)\n    trainable_vars = model.trainable_weights\n    learning_rate = 2e-4\n\n    global_step = tf.contrib.framework.get_or_create_global_step()\n    # optimizer = SGD(lr=learning_rate, momentum=0.9, nesterov=True,clipnorm=100)\n    optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.99,use_nesterov=True)\n    optimizer = tf.train.SyncReplicasOptimizer(\n        optimizer,\n        replicas_to_aggregate=num_workers,\n        total_num_replicas=num_workers,\n        use_locking=True,\n        name=\"sync_replicas\")\n\n    grads = optimizer.compute_gradients(ctc_cost, trainable_vars)\n    grads, _ = tf.clip_by_global_norm(tf.gradients(ctc_cost, trainable_vars), 100)\n    train_op = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)\n    time_begin = time.time()\n\n    chief_queue_runner = optimizer.get_chief_queue_runner()\n    sync_init_op = optimizer.get_init_tokens_op()\n    ready_for_local_init_op = optimizer.ready_for_local_init_op\n    K.manual_variable_initialization(True)\n    local_init_op = optimizer.local_step_init_op\n    if is_chief:\n        local_init_op = optimizer.chief_init_op\n    sv = tf.train.Supervisor(\n        is_chief=is_chief,\n        logdir='train_log',\n        local_init_op=local_init_op,\n        ready_for_local_init_op=ready_for_local_init_op,\n        recovery_wait_secs=1,\n        global_step=global_step)\n\n    sess = sv.prepare_or_wait_for_session(server.target)\n    K.set_session(sess)\n    K.get_session().run(sync_init_op)\n    sv.start_queue_runners(sess, [chief_queue_runner])\n    print(\"Training begins @ %f\" % time_begin)\n    main(FLAGS.train_desc_file, FLAGS.val_desc_file, FLAGS.epochs, FLAGS.save_dir,\n         FLAGS.sortgrad, model=model, run_require_op=[ctc_cost, train_op], val_fn=val_fn)\n\nbut when i run it,", "body": "\r\nWhen I use kears and tensorflow to do the parallel,use sgd sync optimizer I have 10 machines,\r\n each machine has a gpu, so I do\r\n\r\n\r\n\r\n1.Each gpu corresponds to a worker hosts\uff0cuse cuda_visible_devices \r\n2.every ps server only use cpu\r\n3.use Gigabit Ethernet \r\nwhen i run it,i find it is so lower,,\r\n\r\nwho can tell me ?what is wrong??\r\n\r\n\r\n\r\n\r\n\r\n\r\nflags.DEFINE_string(\"train_desc_file\",'train.json','train config')\r\nflags.DEFINE_string(\"val_desc_file\",'test.json','dev config')\r\nflags.DEFINE_string(\"save_dir\",'model','save model dir')\r\nflags.DEFINE_integer('epochs',1000,'epoch number')\r\nflags.DEFINE_integer('batch_size',64,'batch_size')\r\nflags.DEFINE_string(\"ps_hosts\",\"172.16.186.86:2221\",\r\n                    \"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"worker_hosts\", \"172.16.186.86:2222,172.16.186.86:2223\",\"Comma-separated list of hostname:port pairs\")\r\nflags.DEFINE_string(\"job_name\", None,\"job name: worker or ps\")\r\nflags.DEFINE_integer(\"task_index\", None,\r\n                     \"Worker task index, should be >= 0. task_index=0 is \"\r\n                     \"the master worker task the performs the variable \"\r\n                     \"initialization \")\r\n\r\n\r\nif FLAGS.job_name is None or FLAGS.job_name == \"\":\r\n        raise ValueError(\"Must specify an explicit `job_name`\")\r\n    if FLAGS.task_index is None or FLAGS.task_index == \"\":\r\n        raise ValueError(\"Must specify an explicit `task_index`\")\r\n    print(\"job name = %s\" % FLAGS.job_name)\r\n    print(\"task index = %d\" % FLAGS.task_index)\r\n    ps_spec = FLAGS.ps_hosts.split(\",\")\r\n    worker_spec = FLAGS.worker_hosts.split(\",\")\r\n    num_workers = len(worker_spec)\r\n\r\n    cluster = tf.train.ClusterSpec({\r\n        \"ps\": ps_spec,\r\n        \"worker\": worker_spec})\r\n    server = tf.train.Server(\r\n        cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\r\n    if FLAGS.job_name == \"ps\":\r\n        server.join()\r\n\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n            cluster=cluster)):\r\n        is_chief = (FLAGS.task_index == 0)\r\n        model=....\r\n        acoustic_input = model.inputs[0]\r\n        network_output = model.outputs[0]\r\n        network_output = tf.transpose(network_output, [1, 0, 2])\r\n        ctc_cost = warpctc_tensorflow.ctc(network_output, sess_label, sess_label_lens, sess_output_lens)\r\n        ctc_cost = tf.reduce_mean(ctc_cost)\r\n        trainable_vars = model.trainable_weights\r\n        learning_rate = 2e-4\r\n\r\n        global_step = tf.contrib.framework.get_or_create_global_step()\r\n        # optimizer = SGD(lr=learning_rate, momentum=0.9, nesterov=True,clipnorm=100)\r\n        optimizer=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.99,use_nesterov=True)\r\n        optimizer = tf.train.SyncReplicasOptimizer(\r\n            optimizer,\r\n            replicas_to_aggregate=num_workers,\r\n            total_num_replicas=num_workers,\r\n            use_locking=True,\r\n            name=\"sync_replicas\")\r\n\r\n        grads = optimizer.compute_gradients(ctc_cost, trainable_vars)\r\n        grads, _ = tf.clip_by_global_norm(tf.gradients(ctc_cost, trainable_vars), 100)\r\n        train_op = optimizer.apply_gradients(zip(grads, trainable_vars), global_step=global_step)\r\n        time_begin = time.time()\r\n\r\n        chief_queue_runner = optimizer.get_chief_queue_runner()\r\n        sync_init_op = optimizer.get_init_tokens_op()\r\n        ready_for_local_init_op = optimizer.ready_for_local_init_op\r\n        K.manual_variable_initialization(True)\r\n        local_init_op = optimizer.local_step_init_op\r\n        if is_chief:\r\n            local_init_op = optimizer.chief_init_op\r\n        sv = tf.train.Supervisor(\r\n            is_chief=is_chief,\r\n            logdir='train_log',\r\n            local_init_op=local_init_op,\r\n            ready_for_local_init_op=ready_for_local_init_op,\r\n            recovery_wait_secs=1,\r\n            global_step=global_step)\r\n\r\n        sess = sv.prepare_or_wait_for_session(server.target)\r\n        K.set_session(sess)\r\n        K.get_session().run(sync_init_op)\r\n        sv.start_queue_runners(sess, [chief_queue_runner])\r\n        print(\"Training begins @ %f\" % time_begin)\r\n        main(FLAGS.train_desc_file, FLAGS.val_desc_file, FLAGS.epochs, FLAGS.save_dir,\r\n             FLAGS.sortgrad, model=model, run_require_op=[ctc_cost, train_op], val_fn=val_fn)\r\n\r\n\r\n\r\n\r\nbut when i run it,"}