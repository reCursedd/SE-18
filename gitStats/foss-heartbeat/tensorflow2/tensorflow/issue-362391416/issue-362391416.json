{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22429", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22429/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22429/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22429/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/22429", "id": 362391416, "node_id": "MDExOlB1bGxSZXF1ZXN0MjE3MTI1MDMy", "number": 22429, "title": "Add MatchingFilesDatasetOp to improve performance of Dataset.list_files", "user": {"login": "feihugis", "id": 5057740, "node_id": "MDQ6VXNlcjUwNTc3NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/5057740?v=4", "gravatar_id": "", "url": "https://api.github.com/users/feihugis", "html_url": "https://github.com/feihugis", "followers_url": "https://api.github.com/users/feihugis/followers", "following_url": "https://api.github.com/users/feihugis/following{/other_user}", "gists_url": "https://api.github.com/users/feihugis/gists{/gist_id}", "starred_url": "https://api.github.com/users/feihugis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/feihugis/subscriptions", "organizations_url": "https://api.github.com/users/feihugis/orgs", "repos_url": "https://api.github.com/users/feihugis/repos", "events_url": "https://api.github.com/users/feihugis/events{/privacy}", "received_events_url": "https://api.github.com/users/feihugis/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 987666414, "node_id": "MDU6TGFiZWw5ODc2NjY0MTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/ready%20to%20pull", "name": "ready to pull", "color": "2cd643", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "qlzh727", "id": 5118881, "node_id": "MDQ6VXNlcjUxMTg4ODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5118881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qlzh727", "html_url": "https://github.com/qlzh727", "followers_url": "https://api.github.com/users/qlzh727/followers", "following_url": "https://api.github.com/users/qlzh727/following{/other_user}", "gists_url": "https://api.github.com/users/qlzh727/gists{/gist_id}", "starred_url": "https://api.github.com/users/qlzh727/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qlzh727/subscriptions", "organizations_url": "https://api.github.com/users/qlzh727/orgs", "repos_url": "https://api.github.com/users/qlzh727/repos", "events_url": "https://api.github.com/users/qlzh727/events{/privacy}", "received_events_url": "https://api.github.com/users/qlzh727/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "qlzh727", "id": 5118881, "node_id": "MDQ6VXNlcjUxMTg4ODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/5118881?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qlzh727", "html_url": "https://github.com/qlzh727", "followers_url": "https://api.github.com/users/qlzh727/followers", "following_url": "https://api.github.com/users/qlzh727/following{/other_user}", "gists_url": "https://api.github.com/users/qlzh727/gists{/gist_id}", "starred_url": "https://api.github.com/users/qlzh727/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qlzh727/subscriptions", "organizations_url": "https://api.github.com/users/qlzh727/orgs", "repos_url": "https://api.github.com/users/qlzh727/repos", "events_url": "https://api.github.com/users/qlzh727/events{/privacy}", "received_events_url": "https://api.github.com/users/qlzh727/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 61, "created_at": "2018-09-20T22:30:53Z", "updated_at": "2018-10-16T18:04:37Z", "closed_at": "2018-10-16T17:32:09Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429", "html_url": "https://github.com/tensorflow/tensorflow/pull/22429", "diff_url": "https://github.com/tensorflow/tensorflow/pull/22429.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/22429.patch"}, "body_html": "<p>This PR aims to add <code>MatchingFilesDatasetOp</code> to improve the performance of <code>Dataset.list_files</code>, which needs to scan the entire directory tree and load the filenames into memory before the next operation starts. More details can be found at this issue (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"306245750\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/17810\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/17810/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/17810\">#17810</a>).</p>\n<p><code>MatchingFilesDatasetOp</code> could yield the matching files in a sorted order.</p>\n<p>The performance experiments run using <a href=\"https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff\">the script</a> posted by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=260360\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/darrengarvey\">@darrengarvey</a> , where <code>width</code> is the number of dirs at each level, and <code>depth</code> is the number of nested dirs.</p>\n<p>Here are the initial experiment results:</p>\n<p>when <code>width</code> = 1000 and <code>depth</code> = 2:<br>\n-- MatchingFilesDataset:</p>\n<pre><code>read first filename: time: 16.06 ms\nread second filename: time: 0.52 ms\nread 998 more filenames: time: 336.51 ms (0.34 ms per iteration)\n</code></pre>\n<p>-- Dataset.list_files():</p>\n<pre><code>read first filename: time: 253.91 ms\nread second filename: time: 0.27 ms\nread 998 more filenames: time: 135.77 ms (0.14 ms per iteration)\n</code></pre>\n<p>when <code>width</code> = 1000 and <code>depth</code> = 20:</p>\n<p>-- MatchingFilesDataset:</p>\n<pre><code>read first filename: time: 22.16 ms\nread second filename: time: 3.09 ms\nread 998 more filenames: time: 2269.14 ms (2.27 ms per iteration)\n</code></pre>\n<p>-- Dataset.list_files():</p>\n<pre><code>read first filename: time: 1860.96 ms\nread second filename: time: 0.38 ms\nread 998 more filenames: time: 127.23 ms (0.13 ms per iteration)\n</code></pre>\n<p>The performance test-related functions (e.g., <code>testPerformance</code>) in <code>tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py</code> will be removed later.</p>", "body_text": "This PR aims to add MatchingFilesDatasetOp to improve the performance of Dataset.list_files, which needs to scan the entire directory tree and load the filenames into memory before the next operation starts. More details can be found at this issue (#17810).\nMatchingFilesDatasetOp could yield the matching files in a sorted order.\nThe performance experiments run using the script posted by @darrengarvey , where width is the number of dirs at each level, and depth is the number of nested dirs.\nHere are the initial experiment results:\nwhen width = 1000 and depth = 2:\n-- MatchingFilesDataset:\nread first filename: time: 16.06 ms\nread second filename: time: 0.52 ms\nread 998 more filenames: time: 336.51 ms (0.34 ms per iteration)\n\n-- Dataset.list_files():\nread first filename: time: 253.91 ms\nread second filename: time: 0.27 ms\nread 998 more filenames: time: 135.77 ms (0.14 ms per iteration)\n\nwhen width = 1000 and depth = 20:\n-- MatchingFilesDataset:\nread first filename: time: 22.16 ms\nread second filename: time: 3.09 ms\nread 998 more filenames: time: 2269.14 ms (2.27 ms per iteration)\n\n-- Dataset.list_files():\nread first filename: time: 1860.96 ms\nread second filename: time: 0.38 ms\nread 998 more filenames: time: 127.23 ms (0.13 ms per iteration)\n\nThe performance test-related functions (e.g., testPerformance) in tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py will be removed later.", "body": "This PR aims to add `MatchingFilesDatasetOp` to improve the performance of `Dataset.list_files`, which needs to scan the entire directory tree and load the filenames into memory before the next operation starts. More details can be found at this issue (#17810).\r\n\r\n`MatchingFilesDatasetOp` could yield the matching files in a sorted order.\r\n\r\nThe performance experiments run using [the script](https://gist.github.com/darrengarvey/ff05fbe28ab2061c101fe64353b467ff) posted by @darrengarvey , where `width` is the number of dirs at each level, and `depth` is the number of nested dirs.\r\n\r\nHere are the initial experiment results:\r\n\r\nwhen `width` = 1000 and `depth` = 2:\r\n-- MatchingFilesDataset:\r\n```\r\nread first filename: time: 16.06 ms\r\nread second filename: time: 0.52 ms\r\nread 998 more filenames: time: 336.51 ms (0.34 ms per iteration)\r\n```\r\n-- Dataset.list_files():\r\n```\r\nread first filename: time: 253.91 ms\r\nread second filename: time: 0.27 ms\r\nread 998 more filenames: time: 135.77 ms (0.14 ms per iteration)\r\n```\r\nwhen `width` = 1000 and `depth` = 20:\r\n\r\n-- MatchingFilesDataset:\r\n```\r\nread first filename: time: 22.16 ms\r\nread second filename: time: 3.09 ms\r\nread 998 more filenames: time: 2269.14 ms (2.27 ms per iteration)\r\n```\r\n-- Dataset.list_files():\r\n```\r\nread first filename: time: 1860.96 ms\r\nread second filename: time: 0.38 ms\r\nread 998 more filenames: time: 127.23 ms (0.13 ms per iteration)\r\n```\r\n\r\nThe performance test-related functions (e.g., `testPerformance`) in `tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py` will be removed later."}