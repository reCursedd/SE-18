{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/221327510", "pull_request_review_id": 159951898, "id": 221327510, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMTMyNzUxMA==", "diff_hunk": "@@ -0,0 +1,305 @@\n+/* Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include <queue>\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/partial_tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/kernels/data/dataset.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+#include \"tensorflow/core/lib/io/buffered_inputstream.h\"\n+#include \"tensorflow/core/lib/io/inputbuffer.h\"\n+#include \"tensorflow/core/lib/io/path.h\"\n+#include \"tensorflow/core/lib/io/random_inputstream.h\"\n+#include \"tensorflow/core/lib/io/record_reader.h\"\n+#include \"tensorflow/core/lib/io/zlib_compression_options.h\"\n+#include \"tensorflow/core/lib/io/zlib_inputstream.h\"\n+#include \"tensorflow/core/platform/env.h\"\n+\n+namespace tensorflow {\n+namespace data {\n+namespace {\n+\n+// See documentation in ../../ops/dataset_ops.cc for a high-level\n+// description of the following op.\n+\n+class MatchingFilesDatasetOp : public DatasetOpKernel {\n+ public:\n+  using DatasetOpKernel::DatasetOpKernel;\n+\n+  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n+    const Tensor* patterns_t;\n+    OP_REQUIRES_OK(ctx, ctx->input(\"patterns\", &patterns_t));\n+    const auto patterns = patterns_t->flat<string>();\n+    size_t num_patterns = static_cast<size_t>(patterns.size());\n+    std::vector<string> pattern_strs;\n+    pattern_strs.reserve(num_patterns);\n+\n+    for (int i = 0; i < num_patterns; i++) {\n+      pattern_strs.push_back(patterns(i));\n+    }\n+\n+    // keep the elements in the ascending order\n+    std::sort(pattern_strs.begin(), pattern_strs.end());\n+    *output = new Dataset(ctx, std::move(pattern_strs));\n+  }\n+\n+ private:\n+  class Dataset : public DatasetBase {\n+   public:\n+    Dataset(OpKernelContext* ctx, std::vector<string> patterns)\n+        : DatasetBase(DatasetContext(ctx)), patterns_(std::move(patterns)) {}\n+\n+    std::unique_ptr<IteratorBase> MakeIteratorInternal(\n+        const string& prefix) const override {\n+      return std::unique_ptr<IteratorBase>(\n+          new Iterator({this, strings::StrCat(prefix, \"::MatchingFiles\")}));\n+    }\n+\n+    const DataTypeVector& output_dtypes() const override {\n+      static DataTypeVector* dtypes = new DataTypeVector({DT_STRING});\n+      return *dtypes;\n+    }\n+\n+    const std::vector<PartialTensorShape>& output_shapes() const override {\n+      static std::vector<PartialTensorShape>* shapes =\n+          new std::vector<PartialTensorShape>({{}});\n+      return *shapes;\n+    }\n+\n+    string DebugString() const override {\n+      return \"MatchingFilesDatasetOp::Dataset\";\n+    }\n+\n+   protected:\n+    Status AsGraphDefInternal(SerializationContext* ctx,\n+                              DatasetGraphDefBuilder* b,\n+                              Node** output) const override {\n+      Node* patterns_node = nullptr;\n+      TF_RETURN_IF_ERROR(b->AddVector(patterns_, &patterns_node));\n+      TF_RETURN_IF_ERROR(b->AddDataset(this, {patterns_node}, output));\n+      return Status::OK();\n+    }\n+\n+   private:\n+    class Iterator : public DatasetIterator<Dataset> {\n+     public:\n+      explicit Iterator(const Params& params)\n+          : DatasetIterator<Dataset>(params) {}\n+\n+      Status GetNextInternal(IteratorContext* ctx,\n+                             std::vector<Tensor>* out_tensors,\n+                             bool* end_of_sequence) override {\n+        mutex_lock l(mu_);\n+        Status ret;\n+\n+        while (!filepath_queue_.empty() ||\n+               current_pattern_index_ < dataset()->patterns_.size()) {\n+          // All the elements in the heap will be the matched filename or the\n+          // potential directory.\n+          if (!filepath_queue_.empty()) {\n+            string cur_file = filepath_queue_.top();\n+            filepath_queue_.pop();\n+\n+            // We can also use isDectory() here. But IsDirectory call can be\n+            // expensive for some FS.\n+            if (ctx->env()->MatchPath(cur_file, current_pattern_)) {\n+              Tensor filepath_tensor(ctx->allocator({}), DT_STRING, {});\n+              filepath_tensor.scalar<string>()() = cur_file;\n+              out_tensors->emplace_back(std::move(filepath_tensor));\n+              *end_of_sequence = false;\n+              return Status::OK();\n+            }\n+\n+            // In this case, cur_file is a directory. Then create a sub-pattern\n+            // to continue the search.\n+            size_t pos = current_pattern_.find_first_of(\"*?[\\\\\");\n+            size_t len = current_pattern_.size() - pos;\n+            string cur_pattern_suffix = current_pattern_.substr(pos, len);\n+            string sub_pattern =\n+                strings::StrCat(cur_file, \"/\", cur_pattern_suffix);\n+            Status s = UpdateIterator(ctx, sub_pattern);\n+            ret.Update(s);\n+          } else {\n+            // search a new pattern\n+            current_pattern_ = dataset()->patterns_[current_pattern_index_];\n+            Status s = UpdateIterator(ctx, current_pattern_);\n+            ret.Update(s);\n+            ++current_pattern_index_;\n+          }\n+        }\n+\n+        *end_of_sequence = true;\n+        return Status::OK();\n+      }\n+\n+     protected:\n+      Status SaveInternal(IteratorStateWriter* writer) override {\n+        mutex_lock l(mu_);\n+        TF_RETURN_IF_ERROR(writer->WriteScalar(\n+            full_name(\"current_pattern_index\"), current_pattern_index_));\n+\n+        TF_RETURN_IF_ERROR(writer->WriteScalar(full_name(\"current_pattern\"),\n+                                               current_pattern_));\n+\n+        if (!filepath_queue_.empty()) {\n+          TF_RETURN_IF_ERROR(writer->WriteScalar(full_name(\"queue_size\"),\n+                                                 filepath_queue_.size()));\n+          for (int i = 0; i < filepath_queue_.size(); ++i) {\n+            TF_RETURN_IF_ERROR(writer->WriteScalar(\n+                full_name(strings::StrCat(\"queue_element_\", i)),\n+                filepath_queue_.top()));\n+            filepath_queue_.pop();\n+          }\n+        }\n+      }\n+\n+      Status RestoreInternal(IteratorContext* ctx,\n+                             IteratorStateReader* reader) override {\n+        mutex_lock l(mu_);\n+        int64 current_pattern_index;\n+        TF_RETURN_IF_ERROR(reader->ReadScalar(\n+            full_name(\"current_pattern_index\"), &current_pattern_index));\n+        current_pattern_index_ = size_t(current_pattern_index);\n+\n+        TF_RETURN_IF_ERROR(reader->ReadScalar(full_name(\"current_pattern\"),\n+                                              &current_pattern_));\n+\n+        int64 queue_size;\n+        TF_RETURN_IF_ERROR(\n+            reader->ReadScalar(full_name(\"queue_size\"), &queue_size));\n+        for (int i = 0; i < queue_size; i++) {\n+          string element;\n+          TF_RETURN_IF_ERROR(reader->ReadScalar(\n+              full_name(strings::StrCat(\"queue_element_\", i)), &element));\n+          filepath_queue_.push(element);\n+        }\n+        return Status::OK();\n+      }\n+\n+     private:\n+      Status UpdateIterator(IteratorContext* ctx, const string& pattern)\n+          EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n+        string fixed_prefix = pattern.substr(0, pattern.find_first_of(\"*?[\\\\\"));\n+        string eval_pattern = pattern;\n+        string dir(io::Dirname(fixed_prefix));\n+\n+        // If dir is empty then we need to fix up fixed_prefix and eval_pattern\n+        // to include . as the top level directory.\n+        if (dir.empty()) {\n+          dir = \".\";\n+          fixed_prefix = io::JoinPath(dir, fixed_prefix);\n+          eval_pattern = io::JoinPath(dir, pattern);\n+        }\n+\n+        FileSystem* fs;\n+        TF_RETURN_IF_ERROR(ctx->env()->GetFileSystemForFile(dir, &fs));\n+\n+        filepath_queue_.push(dir);\n+        Status ret;  // Status to return\n+        // children_dir_status holds is_dir status for children. It can have\n+        // three possible values: OK for true; FAILED_PRECONDITION for false;\n+        // CANCELLED if we don't calculate IsDirectory (we might do that because\n+        // there isn't any point in exploring that child path).\n+\n+        // DFS to find the first element in the iterator.\n+        while (!filepath_queue_.empty()) {\n+          string cur_dir = filepath_queue_.top();\n+          filepath_queue_.pop();\n+          std::vector<string> children;\n+          Status s = fs->GetChildren(cur_dir, &children);\n+          ret.Update(s);\n+\n+          // If cur_dir has no children, there will two possible situations: 1)\n+          // the cur_dir is an empty dir; 2) the cur_dir is actual a file\n+          // instead of a director. For the first one, continue to search the\n+          // heap. For the second one, if the file matches the pattern, add\n+          // it to the heap and finish the search; otherwise, continue the next\n+          // search.\n+          if (children.empty()) {\n+            if (ctx->env()->MatchPath(cur_dir, eval_pattern)) {\n+              filepath_queue_.push(cur_dir);\n+              return ret;\n+            } else {\n+              continue;\n+            }\n+          }\n+\n+          std::map<string, Status> children_dir_status;\n+          // This IsDirectory call can be expensive for some FS. Parallelizing\n+          // it.\n+          ForEach(\n+              ctx, 0, children.size(),\n+              [fs, &cur_dir, &children, &fixed_prefix,\n+               &children_dir_status](int i) {\n+                const string child_path = io::JoinPath(cur_dir, children[i]);\n+                // In case the child_path doesn't start with the fixed_prefix,\n+                // then we don't need to explore this path.\n+                if (!str_util::StartsWith(child_path, fixed_prefix)) {\n+                  children_dir_status[child_path] = Status(\n+                      tensorflow::error::CANCELLED, \"Operation not needed\");\n+                } else {\n+                  children_dir_status[child_path] = fs->IsDirectory(child_path);\n+                }\n+              });\n+\n+          for (const auto& child : children) {\n+            const string child_dir_path = io::JoinPath(cur_dir, child);\n+            const Status child_dir_status = children_dir_status[child];\n+            // If the IsDirectory call was cancelled we bail.\n+            if (child_dir_status.code() == tensorflow::error::CANCELLED) {\n+              continue;\n+            }\n+\n+            if (child_dir_status.ok()) {\n+              // push the child dir for next search\n+              filepath_queue_.push(child_dir_path);\n+            } else {\n+              // This case will be a file: if the file matches the pattern, push\n+              // it to the heap; otherwise, ignore it.\n+              if (ctx->env()->MatchPath(child_dir_path, eval_pattern)) {\n+                filepath_queue_.push(child_dir_path);\n+              }\n+            }\n+          }\n+        }\n+        return ret;\n+      }\n+\n+      static void ForEach(IteratorContext* ctx, int first, int last,\n+                          const std::function<void(int)>& f) {\n+        for (int i = first; i < last; i++) {\n+          (*ctx->runner())([f, i] { std::bind(f, i); });", "path": "tensorflow/core/kernels/data/matching_files_dataset_op.cc", "position": null, "original_position": 285, "commit_id": "0d5b9d20cc3e3062aa4d443bc772bb3aed698d15", "original_commit_id": "3c3f6205ab7ae733487e30916b1e2c493e16fb6c", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "body": "should not this be `(*ctx->runner())([f, i] { f(i); });`?", "created_at": "2018-09-28T17:33:11Z", "updated_at": "2018-10-15T21:38:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r221327510", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/221327510"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r221327510"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429"}}, "body_html": "<p>should not this be <code>(*ctx-&gt;runner())([f, i] { f(i); });</code>?</p>", "body_text": "should not this be (*ctx->runner())([f, i] { f(i); });?"}