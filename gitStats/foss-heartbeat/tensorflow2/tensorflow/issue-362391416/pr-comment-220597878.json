{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/220597878", "pull_request_review_id": 159040949, "id": 220597878, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMDU5Nzg3OA==", "diff_hunk": "@@ -0,0 +1,267 @@\n+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for the experimental input pipeline ops.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from os import path\n+import shutil\n+import tempfile\n+\n+from tensorflow.python.data.ops import iterator_ops\n+from tensorflow.python.data.ops.dataset_ops import MatchingFilesDataset\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.platform import test\n+from tensorflow.python.util import compat\n+from tensorflow.python.data.ops import dataset_ops\n+from tensorflow.python.ops.gen_io_ops import matching_files\n+from tensorflow.python.framework import errors\n+\n+import os\n+import time\n+from functools import partial\n+\n+\n+try:\n+  import psutil  # pylint: disable=g-import-not-at-top\n+\n+  psutil_import_succeeded = True\n+except ImportError:\n+  psutil_import_succeeded = False\n+\n+\n+def timeit(fn, msg, N=0):\n+  start = time.time()\n+  res = fn()\n+  end = time.time()\n+  runtime = (end - start) * 1000\n+  msg = '{}: time: {:.2f} ms'.format(msg, runtime)\n+  if N:\n+    msg += ' ({:.2f} ms per iteration)'.format(runtime / N)\n+  print(msg)\n+  return res\n+\n+\n+width = 1000\n+depth = 20\n+\n+\n+class MatchingFilesDatasetTest(test.TestCase):\n+\n+  def setUp(self):\n+    self.tmp_dir = tempfile.mkdtemp()\n+\n+  def tearDown(self):\n+    shutil.rmtree(self.tmp_dir, ignore_errors=True)\n+\n+  def _touchTempFiles(self, filenames):\n+    for filename in filenames:\n+      open(path.join(self.tmp_dir, filename), 'a').close()\n+\n+  def testEmptyDirectory(self):\n+    dataset = MatchingFilesDataset(path.join(self.tmp_dir, '*'))\n+    with self.cached_session() as sess:\n+      itr = iterator_ops.Iterator.from_structure(dataset.output_types)\n+      init_op = itr.make_initializer(dataset)\n+      next_element = itr.get_next()\n+      sess.run(init_op)\n+      with self.assertRaises(errors.OutOfRangeError):\n+        sess.run(next_element)\n+\n+  def testSimpleDirectory(self):\n+    filenames = ['a', 'b', 'c']\n+    self._touchTempFiles(filenames)\n+\n+    dataset = MatchingFilesDataset(path.join(self.tmp_dir, '*'))\n+    with self.cached_session() as sess:\n+      itr = iterator_ops.Iterator.from_structure(dataset.output_types)\n+      init_op = itr.make_initializer(dataset)\n+      next_element = itr.get_next()\n+      sess.run(init_op)\n+\n+      full_filenames = []\n+      produced_filenames = []\n+      for filename in filenames:\n+        full_filenames.append(\n+          compat.as_bytes(path.join(self.tmp_dir, filename)))\n+        produced_filenames.append(compat.as_bytes(sess.run(next_element)))\n+      self.assertItemsEqual(full_filenames, produced_filenames)\n+      with self.assertRaises(errors.OutOfRangeError):\n+        sess.run(itr.get_next())\n+\n+  def testSimpleDirectoryInitializer(self):\n+    filenames = ['a', 'b', 'c']\n+    self._touchTempFiles(filenames)\n+\n+    filename_placeholder = array_ops.placeholder(dtypes.string, shape=[])\n+    dataset = MatchingFilesDataset(filename_placeholder)\n+\n+    with self.cached_session() as sess:\n+      itr = iterator_ops.Iterator.from_structure(dataset.output_types)\n+      init_op = itr.make_initializer(dataset)\n+      next_element = itr.get_next()\n+      sess.run(\n+        init_op,\n+        feed_dict={filename_placeholder: path.join(self.tmp_dir, '*')})\n+\n+      full_filenames = []\n+      produced_filenames = []\n+      for filename in filenames:\n+        full_filenames.append(\n+          compat.as_bytes(path.join(self.tmp_dir, filename)))\n+        produced_filenames.append(compat.as_bytes(sess.run(next_element)))\n+\n+      self.assertItemsEqual(full_filenames, produced_filenames)\n+\n+      with self.assertRaises(errors.OutOfRangeError):\n+        sess.run(itr.get_next())\n+\n+  def testFileSuffixes(self):\n+    filenames = ['a.txt', 'b.py', 'c.py', 'd.pyc']\n+    self._touchTempFiles(filenames)\n+\n+    filename_placeholder = array_ops.placeholder(dtypes.string, shape=[])\n+    dataset = MatchingFilesDataset(filename_placeholder)\n+\n+    with self.cached_session() as sess:\n+      itr = iterator_ops.Iterator.from_structure(dataset.output_types)\n+      init_op = itr.make_initializer(dataset)\n+      next_element = itr.get_next()\n+      sess.run(\n+        init_op,\n+        feed_dict={filename_placeholder: path.join(self.tmp_dir, '*.py')})\n+\n+      full_filenames = []\n+      produced_filenames = []\n+      for filename in filenames[1:-1]:\n+        full_filenames.append(\n+          compat.as_bytes(path.join(self.tmp_dir, filename)))\n+        produced_filenames.append(compat.as_bytes(sess.run(next_element)))\n+      self.assertItemsEqual(full_filenames, produced_filenames)\n+\n+      with self.assertRaises(errors.OutOfRangeError):\n+        sess.run(itr.get_next())\n+\n+  def testFileMiddles(self):\n+    filenames = ['a.txt', 'b.py', 'c.pyc']\n+    self._touchTempFiles(filenames)\n+\n+    filename_placeholder = array_ops.placeholder(dtypes.string, shape=[])\n+    dataset = MatchingFilesDataset(filename_placeholder)\n+\n+    with self.cached_session() as sess:\n+      itr = iterator_ops.Iterator.from_structure(dataset.output_types)\n+      init_op = itr.make_initializer(dataset)\n+      next_element = itr.get_next()\n+      sess.run(\n+        init_op,\n+        feed_dict={filename_placeholder: path.join(self.tmp_dir, '*.py*')})\n+\n+      full_filenames = []\n+      produced_filenames = []\n+      for filename in filenames[1:]:\n+        full_filenames.append(\n+          compat.as_bytes(path.join(self.tmp_dir, filename)))\n+        produced_filenames.append(compat.as_bytes(sess.run(next_element)))\n+\n+      self.assertItemsEqual(full_filenames, produced_filenames)\n+\n+      with self.assertRaises(errors.OutOfRangeError):\n+        sess.run(itr.get_next())\n+\n+  def _load_data(self):\n+    new_files = []\n+    dir = \"/tmp/test/\"\n+    if not os.path.exists(dir):\n+      os.makedirs(dir)\n+    base = tempfile.mkdtemp(prefix=dir)\n+    print('saving files to dir: {}'.format(base))\n+    for i in range(width):\n+      new_base = os.path.join(base, str(i), *[str(j) for j in range(depth - 1)])\n+      if not os.path.exists(new_base):\n+        os.makedirs(new_base)\n+      f = os.path.join(new_base, 'stuff.txt')\n+      new_files.append(compat.as_bytes(f))\n+      open(f, 'w').close()\n+    return base, new_files\n+\n+  def _read_data(self, data, sess, N=1):\n+    for _ in range(N):\n+      sess.run(data)\n+\n+  def _read_data_with_result(self, data, sess, N=1):\n+    result = []\n+    for _ in range(N):\n+      result.append(sess.run(data))\n+    return result\n+\n+  def testPerformance(self):\n+    base, test_filenames = self._load_data()\n+    test_filenames.sort(reverse=True)\n+    patterns = array_ops.placeholder(dtypes.string, shape=[None])\n+    dataset = MatchingFilesDataset(patterns)\n+    iterator = iterator_ops.Iterator.from_structure(dataset.output_types)\n+    init_op = iterator.make_initializer(dataset)\n+    get_next = iterator.get_next()\n+    result = []\n+    with self.cached_session() as sess:\n+      pattern = '{}/{}/*.txt'\\\n+        .format(base, os.path.join(*['**' for _ in range(depth)]))\n+      search_patterns = [pattern]\n+      sess.run(init_op, feed_dict={patterns: search_patterns})\n+      result.extend(timeit(partial(self._read_data_with_result, get_next, sess),\n+        \"read first filename\"))\n+      result.extend(timeit(partial(self._read_data_with_result, get_next, sess),\n+        \"read second filename\"))\n+      N = width * len(search_patterns) - 2\n+      filename = timeit(partial(self._read_data_with_result, get_next, sess, N),\n+        'read {} more filenames'.format(N), N)\n+      result.extend(filename)\n+\n+    matched_filenames = [compat.as_bytes(x) for x in result]\n+    self.assertEqual(matched_filenames, test_filenames)\n+\n+  def testShuffle(self):", "path": "tensorflow/python/data/kernel_tests/matching_files_dataset_op_test.py", "position": null, "original_position": 238, "commit_id": "0d5b9d20cc3e3062aa4d443bc772bb3aed698d15", "original_commit_id": "de453a41d3886afbcb6c1b51caa3bfa61c2d36ef", "user": {"login": "feihugis", "id": 5057740, "node_id": "MDQ6VXNlcjUwNTc3NDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/5057740?v=4", "gravatar_id": "", "url": "https://api.github.com/users/feihugis", "html_url": "https://github.com/feihugis", "followers_url": "https://api.github.com/users/feihugis/followers", "following_url": "https://api.github.com/users/feihugis/following{/other_user}", "gists_url": "https://api.github.com/users/feihugis/gists{/gist_id}", "starred_url": "https://api.github.com/users/feihugis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/feihugis/subscriptions", "organizations_url": "https://api.github.com/users/feihugis/orgs", "repos_url": "https://api.github.com/users/feihugis/repos", "events_url": "https://api.github.com/users/feihugis/events{/privacy}", "received_events_url": "https://api.github.com/users/feihugis/received_events", "type": "User", "site_admin": false}, "body": "This is not related to the logic introduced in this CL. Have removed this testing.", "created_at": "2018-09-26T14:53:51Z", "updated_at": "2018-10-15T21:38:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r220597878", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/220597878"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r220597878"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429"}}, "body_html": "<p>This is not related to the logic introduced in this CL. Have removed this testing.</p>", "body_text": "This is not related to the logic introduced in this CL. Have removed this testing.", "in_reply_to_id": 219971866}