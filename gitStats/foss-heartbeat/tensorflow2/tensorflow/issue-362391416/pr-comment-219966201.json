{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/219966201", "pull_request_review_id": 158258029, "id": 219966201, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxOTk2NjIwMQ==", "diff_hunk": "@@ -0,0 +1,330 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+#include <queue>\n+#include \"tensorflow/core/framework/partial_tensor_shape.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/kernels/data/dataset.h\"\n+#include \"tensorflow/core/lib/io/buffered_inputstream.h\"\n+#include \"tensorflow/core/lib/io/inputbuffer.h\"\n+#include \"tensorflow/core/lib/io/random_inputstream.h\"\n+#include \"tensorflow/core/lib/io/record_reader.h\"\n+#include \"tensorflow/core/lib/io/zlib_compression_options.h\"\n+#include \"tensorflow/core/lib/io/zlib_inputstream.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/env.h\"\n+#include \"tensorflow/core/lib/io/path.h\"\n+#include \"tensorflow/core/lib/core/threadpool.h\"\n+\n+namespace tensorflow {\n+namespace data {\n+\n+namespace {\n+\n+constexpr int kNumThreads = 8;\n+\n+// Run a function in parallel using a ThreadPool, but skip the ThreadPool\n+// on the iOS platform due to its problems with more than a few threads.\n+void ForEach(int first, int last, const std::function<void(int)>& f) {\n+#if TARGET_OS_IPHONE\n+  for (int i = first; i < last; i++) {\n+    f(i);\n+  }\n+#else\n+  int num_threads = std::min(kNumThreads, last - first);\n+  thread::ThreadPool threads(Env::Default(), \"ForEach\", num_threads);\n+  for (int i = first; i < last; i++) {\n+    threads.Schedule([f, i] { f(i); });\n+  }\n+#endif\n+}\n+\n+}  // namespace\n+\n+namespace {\n+\n+class MatchingFilesDatasetOp : public DatasetOpKernel {\n+ public:\n+  using DatasetOpKernel::DatasetOpKernel;\n+\n+  void MakeDataset(OpKernelContext* ctx, DatasetBase** output) override {\n+    const Tensor* patterns_t;\n+    // NOTE(originally from ringwalt): Changing the input name \"pattern\" to\n+    // \"patterns\" would break existing graphs.\n+    OP_REQUIRES_OK(ctx, ctx->input(\"pattern\", &patterns_t));\n+    OP_REQUIRES(\n+        ctx,\n+        TensorShapeUtils::IsScalar(patterns_t->shape()) ||\n+            TensorShapeUtils::IsVector(patterns_t->shape()),\n+        errors::InvalidArgument(\n+            \"Input patterns tensor must be scalar or vector, but had shape: \",\n+            patterns_t->shape().DebugString()));\n+    const auto patterns = patterns_t->flat<string>();\n+    size_t num_patterns = static_cast<size_t >(patterns.size());\n+    std::vector<string> pattern_strs;\n+    pattern_strs.reserve(num_patterns);\n+\n+    for (int i = 0; i < num_patterns; ++i) {\n+      pattern_strs.push_back(patterns(i));\n+    }\n+\n+    // keep the elements in the descending order\n+    std::sort(pattern_strs.begin(), pattern_strs.end(), std::greater<string>());\n+    *output = new Dataset(ctx, std::move(pattern_strs));\n+  }\n+\n+ private:\n+  class Dataset : public DatasetBase {\n+   public:\n+    Dataset(OpKernelContext* ctx, std::vector<string> patterns)\n+        : DatasetBase(DatasetContext(ctx)),\n+          pattern_(std::move(patterns)) {}\n+\n+    std::unique_ptr<IteratorBase> MakeIteratorInternal(\n+        const string& prefix) const override {\n+      return std::unique_ptr<IteratorBase>(\n+          new Iterator({this, strings::StrCat(prefix, \"::FileName\")}));\n+    }\n+\n+    const DataTypeVector& output_dtypes() const override {\n+      static DataTypeVector* dtypes = new DataTypeVector({DT_STRING});\n+      return *dtypes;\n+    }\n+\n+    const std::vector<PartialTensorShape>& output_shapes() const override {\n+      static std::vector<PartialTensorShape>* shapes =\n+          new std::vector<PartialTensorShape>({{}});\n+      return *shapes;\n+    }\n+\n+    string DebugString() const override {\n+      return \"MatchingFilesDatasetOp::Dataset\";\n+    }\n+\n+   protected:\n+    Status AsGraphDefInternal(SerializationContext* ctx,\n+                              DatasetGraphDefBuilder* b,\n+                              Node** output) const override {\n+      Node* pattern = nullptr;\n+      TF_RETURN_IF_ERROR(b->AddVector(pattern_, &pattern));\n+      TF_RETURN_IF_ERROR(b->AddDataset(this, {pattern}, output));\n+      return Status::OK();\n+    }\n+\n+   private:\n+    class Iterator : public DatasetIterator<Dataset> {\n+     public:\n+      explicit Iterator(const Params& params)\n+          : DatasetIterator<Dataset>(params) {}\n+\n+      Status GetNextInternal(IteratorContext* ctx,\n+                             std::vector<Tensor>* out_tensors,\n+                             bool* end_of_sequence) override {\n+        mutex_lock l(mu_);\n+        Status ret;\n+\n+        while (!filepath_queue_.empty() ||\n+            current_pattern_index_ < dataset()->pattern_.size()) {\n+          // all the elements in the heap will be the matched file name or the\n+          // potential directory\n+          if (!filepath_queue_.empty()) {\n+            string cur_file = filepath_queue_.top();\n+            filepath_queue_.pop();\n+\n+            // we can also use isDectory() here. But IsDirectory call can be\n+            // expensive for some FS\n+            if (ctx->env()->MatchPath(cur_file, current_pattern_)){\n+              Tensor filepath_tensor(ctx->allocator({}), DT_STRING, {});\n+              filepath_tensor.scalar<string>()() = cur_file;\n+              out_tensors->emplace_back(std::move(filepath_tensor));\n+              *end_of_sequence = false;\n+              return Status::OK();\n+            }\n+\n+            // in this case, cur_file is a directory. Then create a sub-pattern\n+            // to continue the search\n+            size_t pos = current_pattern_.find_first_of(\"*?[\\\\\");\n+            size_t len = current_pattern_.size() - pos;\n+            string cur_pattern_suffix = current_pattern_.substr(pos, len);\n+            string sub_pattern = strings::StrCat(cur_file,\n+                                                 \"/\",\n+                                                 cur_pattern_suffix);\n+            Status s = UpdateIterator(ctx->env(), sub_pattern);\n+            ret.Update(s);\n+          } else {\n+            // search a new pattern\n+            current_pattern_ = dataset()->pattern_[current_pattern_index_];\n+            Status s = UpdateIterator(ctx->env(), current_pattern_);\n+            ret.Update(s);\n+            ++current_pattern_index_;\n+          }\n+        }\n+\n+        *end_of_sequence = true;\n+        return Status::OK();\n+      }\n+\n+     protected:\n+      Status SaveInternal(IteratorStateWriter* writer) override {\n+        mutex_lock l(mu_);\n+        TF_RETURN_IF_ERROR(writer->WriteScalar(\n+            full_name(\"current_pattern_index\"),\n+            current_pattern_index_));\n+\n+        TF_RETURN_IF_ERROR(writer->WriteScalar(\n+            full_name(\"current_pattern\"),\n+            current_pattern_));\n+\n+        if (!filepath_queue_.empty()) {\n+          TF_RETURN_IF_ERROR(writer->WriteScalar(\n+              full_name(\"queue_size\"), filepath_queue_.size()));\n+          for (int i = 0; i < filepath_queue_.size(); ++i) {\n+            TF_RETURN_IF_ERROR(writer->WriteScalar(\n+                full_name(strings::StrCat(\"queue_element_\", i)),\n+                filepath_queue_.top()));\n+            filepath_queue_.pop();\n+          }\n+        }\n+      }\n+\n+      Status RestoreInternal(IteratorContext* ctx,\n+                             IteratorStateReader* reader) override {\n+        mutex_lock l(mu_);\n+        int64 current_pattern_index;\n+        TF_RETURN_IF_ERROR(reader->ReadScalar(full_name(\"current_pattern_index\"),\n+                                              &current_pattern_index));\n+        current_pattern_index_ = size_t(current_pattern_index);\n+\n+        TF_RETURN_IF_ERROR(reader->ReadScalar(full_name(\"current_pattern\"),\n+                                              &current_pattern_));\n+\n+        int64 queue_size;\n+        TF_RETURN_IF_ERROR(reader->ReadScalar(full_name(\"queue_size\"),\n+                                              &queue_size));\n+        for (int i = static_cast<int>(queue_size - 1); i >= 0; --i) {\n+          string element;\n+          TF_RETURN_IF_ERROR(reader->ReadScalar(\n+              full_name(strings::StrCat(\"queue_element_\", i)), &element));\n+          filepath_queue_.push(element);\n+        }\n+        return Status::OK();\n+      }\n+\n+     private:\n+      Status UpdateIterator(Env *env, const string &pattern)\n+      EXCLUSIVE_LOCKS_REQUIRED(mu_) {\n+        string fixed_prefix = pattern.substr(0, pattern.find_first_of(\"*?[\\\\\"));\n+        string eval_pattern = pattern;\n+        string dir(io::Dirname(fixed_prefix));\n+\n+        // If dir is empty then we need to fix up fixed_prefix and eval_pattern to\n+        // include . as the top level directory.\n+        if (dir.empty()) {\n+          dir = \".\";\n+          fixed_prefix = io::JoinPath(dir, fixed_prefix);\n+          eval_pattern = io::JoinPath(dir, pattern);\n+        }\n+\n+        FileSystem* fs;\n+        TF_RETURN_IF_ERROR(env->GetFileSystemForFile(dir, &fs));\n+\n+        filepath_queue_.push(dir);\n+        Status ret;  //Status to return\n+        // children_dir_status holds is_dir status for children. It can have three\n+        // possible values: OK for true; FAILED_PRECONDITION for false; CANCELLED\n+        // if we don't calculate IsDirectory (we might do that because there isn't\n+        // any point in exploring that child path).\n+\n+        // DFS to find the first element in the iterator\n+        while (!filepath_queue_.empty()) {\n+          string cur_dir = filepath_queue_.top();\n+          filepath_queue_.pop();\n+          std::vector<string> children;\n+          Status s = fs->GetChildren(cur_dir, &children);\n+          ret.Update(s);\n+\n+          // if cur_dir has no children, there will two possible situations: 1)\n+          // the cur_dir is an empty dir; 2) the cur_dir is actual a file\n+          // instead of a director. For the first one, continue to search the\n+          // heap; For the second one, if the file matches the pattern, add\n+          // it to the heap and finish the search; otherwise, continue the next\n+          // search\n+          if (children.empty()) {\n+            if (env->MatchPath(cur_dir, current_pattern_)) {\n+              filepath_queue_.push(cur_dir);\n+              return ret;\n+            } else {\n+              continue;\n+            }\n+          }\n+\n+          std::map<string, Status> children_dir_status;\n+          // This IsDirectory call can be expensive for some FS. Parallelizing it.\n+          ForEach(0, children.size(),\n+                  [fs, &cur_dir, &children, &fixed_prefix,\n+                      &children_dir_status] (int i) {\n+                    const string child_path = io::JoinPath(cur_dir, children[i]);\n+                    // In case the child_path doesn't start with the fixed_prefix then\n+                    // we don't need to explore this path.\n+                    if (!str_util::StartsWith(child_path, fixed_prefix)) {\n+                      children_dir_status[child_path] =\n+                          Status(tensorflow::error::CANCELLED,\n+                                 \"Operation not needed\");\n+                    } else {\n+                      children_dir_status[child_path] = fs->IsDirectory(child_path);\n+                    }\n+                  });\n+\n+          for (const auto &child : children) {\n+            const string child_dir_path = io::JoinPath(cur_dir, child);\n+            const Status child_dir_status = children_dir_status[child];\n+            // If the IsDirectory call was cancelled we bail.\n+            if (child_dir_status.code() == tensorflow::error::CANCELLED) {\n+              continue;\n+            }\n+\n+            if (child_dir_status.ok()) {\n+              //push the child dir for next search\n+              filepath_queue_.push(child_dir_path);\n+            } else {\n+              // this case will be a file; if the file match the pattern, push\n+              // it to the heap; otherwise, ignore it\n+              if (env->MatchPath(child_dir_path, eval_pattern)) {\n+                filepath_queue_.push(child_dir_path);\n+              }\n+            }\n+          }\n+        }\n+        return ret;\n+      }\n+\n+      mutex mu_;\n+      //std::unique_ptr<std::priority_queue<string>> filepath_queue_ GUARDED_BY(mu_);\n+      std::priority_queue<string> filepath_queue_ GUARDED_BY(mu_); // = new std::priority_queue<string>;", "path": "tensorflow/core/kernels/data/matching_files_dataset_op.cc", "position": null, "original_position": 316, "commit_id": "0d5b9d20cc3e3062aa4d443bc772bb3aed698d15", "original_commit_id": "de453a41d3886afbcb6c1b51caa3bfa61c2d36ef", "user": {"login": "jsimsa", "id": 1072079, "node_id": "MDQ6VXNlcjEwNzIwNzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1072079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jsimsa", "html_url": "https://github.com/jsimsa", "followers_url": "https://api.github.com/users/jsimsa/followers", "following_url": "https://api.github.com/users/jsimsa/following{/other_user}", "gists_url": "https://api.github.com/users/jsimsa/gists{/gist_id}", "starred_url": "https://api.github.com/users/jsimsa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jsimsa/subscriptions", "organizations_url": "https://api.github.com/users/jsimsa/orgs", "repos_url": "https://api.github.com/users/jsimsa/repos", "events_url": "https://api.github.com/users/jsimsa/events{/privacy}", "received_events_url": "https://api.github.com/users/jsimsa/received_events", "type": "User", "site_admin": false}, "body": "Remove the trailing comment.", "created_at": "2018-09-24T19:49:38Z", "updated_at": "2018-10-15T21:38:29Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r219966201", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/219966201"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22429#discussion_r219966201"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22429"}}, "body_html": "<p>Remove the trailing comment.</p>", "body_text": "Remove the trailing comment."}