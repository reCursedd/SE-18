{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2788", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2788/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2788/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2788/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2788", "id": 159687812, "node_id": "MDU6SXNzdWUxNTk2ODc4MTI=", "number": 2788, "title": "Tensorflow hangs without explanation, some threads stay busy.", "user": {"login": "alexatknit", "id": 15474222, "node_id": "MDQ6VXNlcjE1NDc0MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/15474222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexatknit", "html_url": "https://github.com/alexatknit", "followers_url": "https://api.github.com/users/alexatknit/followers", "following_url": "https://api.github.com/users/alexatknit/following{/other_user}", "gists_url": "https://api.github.com/users/alexatknit/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexatknit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexatknit/subscriptions", "organizations_url": "https://api.github.com/users/alexatknit/orgs", "repos_url": "https://api.github.com/users/alexatknit/repos", "events_url": "https://api.github.com/users/alexatknit/events{/privacy}", "received_events_url": "https://api.github.com/users/alexatknit/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 41, "created_at": "2016-06-10T17:53:20Z", "updated_at": "2017-05-10T10:25:17Z", "closed_at": "2016-08-24T23:00:35Z", "author_association": "NONE", "body_html": "<p>I've got a machine running a GTX 980 Ti with 32 gb of ram (of which about half is used by the data loading pipeline) and a i7-5930K cpu driving a network. I'm running Ubuntu 14.04, I've got the drivers x86_64-361.45.11 installed, I've installed CUDA 7.5.18_linux with CUDNN linux-x64-v4.0-prod. Every 30 or so iterations the run method hangs and 2 to 5 virtual cores are busy, with the gpu not doing anything more than idling. I thought it hanged indefinitely but when I ran it overnight I saw that it actually completed what it was doing after some time. Pausing and continuing the process using gdb will make it continue immediately. I saw something like this with the initial release of 0.6.0 but it was fixed by 0.6.1 so I just pulled from master and assumed I would never see it again. Inspecting the root process just shows that its waiting on the run method:</p>\n<pre><code>(gdb) bt full\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\nNo locals.\n#1  0x00007f29299964dc in std::condition_variable::wait(std::unique_lock&lt;std::mutex&gt;&amp;) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\nNo symbol table info available.\n#2  0x00007f292b46a883 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#3  0x00007f292b474800 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&amp;, std::vector&lt;std::pair&lt;std::string, tensorflow::Tensor&gt;, std::allocator&lt;std::pair&lt;std::string, tensorflow::Tensor&gt; &gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;std::string, std::allocator&lt;std::string&gt; &gt; const&amp;, std::vector&lt;tensorflow::Tensor, std::allocator&lt;tensorflow::Tensor&gt; &gt;*, tensorflow::RunMetadata*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#4  0x00007f292b549c81 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#5  0x00007f292b54a0f1 in TF_Run () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#6  0x00007f292a5787b2 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector&lt;std::pair&lt;char const*, tagPyArrayObject*&gt;, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#7  0x00007f292a578de1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector&lt;std::pair&lt;char const*, tagPyArrayObject*&gt;, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, tensorflow::gtl::InlinedVector&lt;char const*, 8&gt; const&amp;, TF_Status*, tensorflow::gtl::InlinedVector&lt;_object*, 8&gt;*, TF_Buffer*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#8  0x00007f292a5666c8 in _wrap_TF_Run () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n...\n</code></pre>\n<p>Inspection of one of the busy threads is also equally uninteresting:</p>\n<pre><code>(gdb) bt full\n#0  0x00007f293767e3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81\nNo locals.\n#1  0x00007f292b76aa60 in Eigen::NonBlockingThreadPoolTempl&lt;tensorflow::thread::EigenEnvironment&gt;::WorkerLoop(unsigned int) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#2  0x00007f292b769f52 in std::_Function_handler&lt;void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function&lt;void ()&gt;)::{lambda()#1}&gt;::_M_invoke(std::_Any_data const&amp;) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#3  0x00007f2929999a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\nNo symbol table info available.\n#4  0x00007f2937980182 in start_thread (arg=0x7f28657fa700) at pthread_create.c:312\n        __res = &lt;optimized out&gt;\n        pd = 0x7f28657fa700\n        now = &lt;optimized out&gt;\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {139811478284032, -825179003405916230, 1, 0, 139811478284736, 139811478284032, 782688377649571770, 783080021823268794}, mask_was_saved = 0}}, priv = {\n            pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\n        not_first_call = &lt;optimized out&gt;\n        pagesize_m1 = &lt;optimized out&gt;\n        sp = &lt;optimized out&gt;\n        freesize = &lt;optimized out&gt;\n        __PRETTY_FUNCTION__ = \"start_thread\"\n#5  0x00007f29376ad47d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\nNo locals.\n</code></pre>\n<p>Finally the cpu usage revealed by htop doesn't seem to be fully accounted for in the displayed processes, 5 virtual cores might be busy but the processes are only credited with 0-300% cpu usage.</p>", "body_text": "I've got a machine running a GTX 980 Ti with 32 gb of ram (of which about half is used by the data loading pipeline) and a i7-5930K cpu driving a network. I'm running Ubuntu 14.04, I've got the drivers x86_64-361.45.11 installed, I've installed CUDA 7.5.18_linux with CUDNN linux-x64-v4.0-prod. Every 30 or so iterations the run method hangs and 2 to 5 virtual cores are busy, with the gpu not doing anything more than idling. I thought it hanged indefinitely but when I ran it overnight I saw that it actually completed what it was doing after some time. Pausing and continuing the process using gdb will make it continue immediately. I saw something like this with the initial release of 0.6.0 but it was fixed by 0.6.1 so I just pulled from master and assumed I would never see it again. Inspecting the root process just shows that its waiting on the run method:\n(gdb) bt full\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\nNo locals.\n#1  0x00007f29299964dc in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\nNo symbol table info available.\n#2  0x00007f292b46a883 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#3  0x00007f292b474800 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#4  0x00007f292b549c81 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#5  0x00007f292b54a0f1 in TF_Run () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#6  0x00007f292a5787b2 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#7  0x00007f292a578de1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#8  0x00007f292a5666c8 in _wrap_TF_Run () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n...\n\nInspection of one of the busy threads is also equally uninteresting:\n(gdb) bt full\n#0  0x00007f293767e3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81\nNo locals.\n#1  0x00007f292b76aa60 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(unsigned int) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#2  0x00007f292b769f52 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#3  0x00007f2929999a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\nNo symbol table info available.\n#4  0x00007f2937980182 in start_thread (arg=0x7f28657fa700) at pthread_create.c:312\n        __res = <optimized out>\n        pd = 0x7f28657fa700\n        now = <optimized out>\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {139811478284032, -825179003405916230, 1, 0, 139811478284736, 139811478284032, 782688377649571770, 783080021823268794}, mask_was_saved = 0}}, priv = {\n            pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\n        not_first_call = <optimized out>\n        pagesize_m1 = <optimized out>\n        sp = <optimized out>\n        freesize = <optimized out>\n        __PRETTY_FUNCTION__ = \"start_thread\"\n#5  0x00007f29376ad47d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\nNo locals.\n\nFinally the cpu usage revealed by htop doesn't seem to be fully accounted for in the displayed processes, 5 virtual cores might be busy but the processes are only credited with 0-300% cpu usage.", "body": "I've got a machine running a GTX 980 Ti with 32 gb of ram (of which about half is used by the data loading pipeline) and a i7-5930K cpu driving a network. I'm running Ubuntu 14.04, I've got the drivers x86_64-361.45.11 installed, I've installed CUDA 7.5.18_linux with CUDNN linux-x64-v4.0-prod. Every 30 or so iterations the run method hangs and 2 to 5 virtual cores are busy, with the gpu not doing anything more than idling. I thought it hanged indefinitely but when I ran it overnight I saw that it actually completed what it was doing after some time. Pausing and continuing the process using gdb will make it continue immediately. I saw something like this with the initial release of 0.6.0 but it was fixed by 0.6.1 so I just pulled from master and assumed I would never see it again. Inspecting the root process just shows that its waiting on the run method:\n\n```\n(gdb) bt full\n#0  pthread_cond_wait@@GLIBC_2.3.2 () at ../nptl/sysdeps/unix/sysv/linux/x86_64/pthread_cond_wait.S:185\nNo locals.\n#1  0x00007f29299964dc in std::condition_variable::wait(std::unique_lock<std::mutex>&) () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\nNo symbol table info available.\n#2  0x00007f292b46a883 in tensorflow::DirectSession::WaitForNotification(tensorflow::DirectSession::RunState*, long long) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#3  0x00007f292b474800 in tensorflow::DirectSession::Run(tensorflow::RunOptions const&, std::vector<std::pair<std::string, tensorflow::Tensor>, std::allocator<std::pair<std::string, tensorflow::Tensor> > > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<std::string, std::allocator<std::string> > const&, std::vector<tensorflow::Tensor, std::allocator<tensorflow::Tensor> >*, tensorflow::RunMetadata*) () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#4  0x00007f292b549c81 in TF_Run_Helper(TF_Session*, char const*, TF_Buffer const*, char const**, TF_Tensor**, int, char const**, TF_Tensor**, int, char const**, int, TF_Buffer*, TF_Status*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#5  0x00007f292b54a0f1 in TF_Run () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#6  0x00007f292a5787b2 in tensorflow::TF_Run_wrapper_helper(TF_Session*, char const*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#7  0x00007f292a578de1 in tensorflow::TF_Run_wrapper(TF_Session*, TF_Buffer const*, tensorflow::gtl::InlinedVector<std::pair<char const*, tagPyArrayObject*>, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, tensorflow::gtl::InlinedVector<char const*, 8> const&, TF_Status*, tensorflow::gtl::InlinedVector<_object*, 8>*, TF_Buffer*) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#8  0x00007f292a5666c8 in _wrap_TF_Run () from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n...\n```\n\nInspection of one of the busy threads is also equally uninteresting:\n\n```\n(gdb) bt full\n#0  0x00007f293767e3f7 in sched_yield () at ../sysdeps/unix/syscall-template.S:81\nNo locals.\n#1  0x00007f292b76aa60 in Eigen::NonBlockingThreadPoolTempl<tensorflow::thread::EigenEnvironment>::WorkerLoop(unsigned int) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#2  0x00007f292b769f52 in std::_Function_handler<void (), tensorflow::thread::EigenEnvironment::CreateThread(std::function<void ()>)::{lambda()#1}>::_M_invoke(std::_Any_data const&) ()\n   from /usr/local/lib/python3.4/dist-packages/tensorflow/python/_pywrap_tensorflow.so\nNo symbol table info available.\n#3  0x00007f2929999a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\nNo symbol table info available.\n#4  0x00007f2937980182 in start_thread (arg=0x7f28657fa700) at pthread_create.c:312\n        __res = <optimized out>\n        pd = 0x7f28657fa700\n        now = <optimized out>\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {139811478284032, -825179003405916230, 1, 0, 139811478284736, 139811478284032, 782688377649571770, 783080021823268794}, mask_was_saved = 0}}, priv = {\n            pad = {0x0, 0x0, 0x0, 0x0}, data = {prev = 0x0, cleanup = 0x0, canceltype = 0}}}\n        not_first_call = <optimized out>\n        pagesize_m1 = <optimized out>\n        sp = <optimized out>\n        freesize = <optimized out>\n        __PRETTY_FUNCTION__ = \"start_thread\"\n#5  0x00007f29376ad47d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111\nNo locals.\n```\n\nFinally the cpu usage revealed by htop doesn't seem to be fully accounted for in the displayed processes, 5 virtual cores might be busy but the processes are only credited with 0-300% cpu usage.\n"}