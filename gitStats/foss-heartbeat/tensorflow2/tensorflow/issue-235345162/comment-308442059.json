{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/308442059", "html_url": "https://github.com/tensorflow/tensorflow/issues/10659#issuecomment-308442059", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10659", "id": 308442059, "node_id": "MDEyOklzc3VlQ29tbWVudDMwODQ0MjA1OQ==", "user": {"login": "adakum", "id": 26038604, "node_id": "MDQ6VXNlcjI2MDM4NjA0", "avatar_url": "https://avatars2.githubusercontent.com/u/26038604?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adakum", "html_url": "https://github.com/adakum", "followers_url": "https://api.github.com/users/adakum/followers", "following_url": "https://api.github.com/users/adakum/following{/other_user}", "gists_url": "https://api.github.com/users/adakum/gists{/gist_id}", "starred_url": "https://api.github.com/users/adakum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adakum/subscriptions", "organizations_url": "https://api.github.com/users/adakum/orgs", "repos_url": "https://api.github.com/users/adakum/repos", "events_url": "https://api.github.com/users/adakum/events{/privacy}", "received_events_url": "https://api.github.com/users/adakum/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-14T14:06:21Z", "updated_at": "2017-06-14T14:07:07Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7606451\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Scitator\">@Scitator</a>  Your implementation is only for single layer encoder right ?<br>\nBut yeah u are right. The problem was with concatenation. Right way to concatenate is as follows:</p>\n<pre><code>self.encoder_state = []\n\n    for i in range(self.num_layers):\n        if isinstance(encoder_fw_state[i], LSTMStateTuple):\n            encoder_state_c = tf.concat((encoder_fw_state[i].c, encoder_bw_state[i].c), 1, name='bidirectional_concat_c')\n            encoder_state_h = tf.concat((encoder_fw_state[i].h, encoder_bw_state[i].h), 1, name='bidirectional_concat_h')\n            encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n        elif isinstance(encoder_fw_state[i], tf.Tensor):\n            encoder_state = tf.concat((encoder_fw_state[i], encoder_bw_state[i]), 1, name='bidirectional_concat')\n        self.encoder_state.append(encoder_state)\n\n    self.encoder_state = tuple(self.encoder_state)\n\n</code></pre>\n<p>Can you suggest me any way to bypass manually looping ? Thanks</p>", "body_text": "@Scitator  Your implementation is only for single layer encoder right ?\nBut yeah u are right. The problem was with concatenation. Right way to concatenate is as follows:\nself.encoder_state = []\n\n    for i in range(self.num_layers):\n        if isinstance(encoder_fw_state[i], LSTMStateTuple):\n            encoder_state_c = tf.concat((encoder_fw_state[i].c, encoder_bw_state[i].c), 1, name='bidirectional_concat_c')\n            encoder_state_h = tf.concat((encoder_fw_state[i].h, encoder_bw_state[i].h), 1, name='bidirectional_concat_h')\n            encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n        elif isinstance(encoder_fw_state[i], tf.Tensor):\n            encoder_state = tf.concat((encoder_fw_state[i], encoder_bw_state[i]), 1, name='bidirectional_concat')\n        self.encoder_state.append(encoder_state)\n\n    self.encoder_state = tuple(self.encoder_state)\n\n\nCan you suggest me any way to bypass manually looping ? Thanks", "body": "@Scitator  Your implementation is only for single layer encoder right ? \r\nBut yeah u are right. The problem was with concatenation. Right way to concatenate is as follows:\r\n```\r\nself.encoder_state = []\r\n\r\n    for i in range(self.num_layers):\r\n        if isinstance(encoder_fw_state[i], LSTMStateTuple):\r\n            encoder_state_c = tf.concat((encoder_fw_state[i].c, encoder_bw_state[i].c), 1, name='bidirectional_concat_c')\r\n            encoder_state_h = tf.concat((encoder_fw_state[i].h, encoder_bw_state[i].h), 1, name='bidirectional_concat_h')\r\n            encoder_state = LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\r\n        elif isinstance(encoder_fw_state[i], tf.Tensor):\r\n            encoder_state = tf.concat((encoder_fw_state[i], encoder_bw_state[i]), 1, name='bidirectional_concat')\r\n        self.encoder_state.append(encoder_state)\r\n\r\n    self.encoder_state = tuple(self.encoder_state)\r\n\r\n```\r\n\r\nCan you suggest me any way to bypass manually looping ? Thanks "}