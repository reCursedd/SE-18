{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20263", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20263/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20263/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20263/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20263", "id": 335189743, "node_id": "MDU6SXNzdWUzMzUxODk3NDM=", "number": 20263, "title": "Computing gradients in extracted subgraph which contains a 'while_loop'", "user": {"login": "Tal-Golan", "id": 10431558, "node_id": "MDQ6VXNlcjEwNDMxNTU4", "avatar_url": "https://avatars3.githubusercontent.com/u/10431558?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Tal-Golan", "html_url": "https://github.com/Tal-Golan", "followers_url": "https://api.github.com/users/Tal-Golan/followers", "following_url": "https://api.github.com/users/Tal-Golan/following{/other_user}", "gists_url": "https://api.github.com/users/Tal-Golan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Tal-Golan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Tal-Golan/subscriptions", "organizations_url": "https://api.github.com/users/Tal-Golan/orgs", "repos_url": "https://api.github.com/users/Tal-Golan/repos", "events_url": "https://api.github.com/users/Tal-Golan/events{/privacy}", "received_events_url": "https://api.github.com/users/Tal-Golan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-06-24T15:49:31Z", "updated_at": "2018-11-21T19:00:02Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 17.10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nv1.8.0-0-g93bc2e2072</li>\n<li><strong>Python version</strong>:<br>\n3.6.5</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n9.0/7.0.5</li>\n<li><strong>GPU model and memory</strong>:<br>\nTitan XP</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>import tensorflow as tf\ng1=tf.Graph()\nsess1=tf.Session(graph=g1)\nwith g1.as_default():\n    with sess1.as_default():\n        i=tf.constant(0, name=\"input\")\n        out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\n        loss=tf.square(out,name='loss')\n        graph_def = tf.graph_util.convert_variables_to_constants(sess1,g1.as_graph_def(),['output/Exit'])\n\ng2 = tf.Graph()\nwith g2.as_default():\n    tf.import_graph_def(graph_def,name='')\n    i_imported = g2.get_tensor_by_name(\"input:0\")\n    out_imported = g2.get_tensor_by_name(\"output/Exit:0\")\n    tf.gradients(out_imported, i_imported)\n\n</code></pre>\n<p>output:</p>\n<pre><code>INFO:tensorflow:Froze 0 variables.\nConverted 0 variables to const ops.\nTraceback (most recent call last):\n\n  File \"&lt;ipython-input-1-908dc1dee750&gt;\", line 23, in &lt;module&gt;\n    tf.gradients(out_imported, i_imported)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 543, in _GradientsHelper\n    ops.get_default_graph(), to_ops, from_ops, colocate_gradients_with_ops)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 195, in _PendingCount\n    between_op_list, between_ops, colocate_gradients_with_ops)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1456, in MaybeCreateControlFlowState\n    loop_state.AddWhileContext(op, between_op_list, between_ops)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1262, in AddWhileContext\n    outer_forward_ctxt = forward_ctxt.outer_context\n\nAttributeError: 'NoneType' object has no attribute 'outer_context'\n</code></pre>\n<h3>Describe the problem</h3>\n<p>TF  <a href=\"https://github.com/tensorflow/tensorflow/issues/7404\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/7404/hovercard\">issue #7404</a> describes that when trying to form a gradient op in an imported (sub)graph, a 'No attribute 'outer_context'' error occurs. This issue was closed with the recommendation to use <code>tf.train.import_meta_graph</code> instead, so the outer context related to the while op is included.</p>\n<p>However, this does not fully solve the problem. In some deep-learning related settings, one might want to train a model, extract a subgraph (i.e., remove training related ops), connect the extracted subgraph into a larger graph to serve as part of an ensemble, GAN and so on, and then retrain the larger graph. When there is no dependence on outer context, one can easily use graph editing tools such as <code>tf.graph_util.convert_variables_to_constants</code> or <code>tf.graph_util.extract_sub_graph</code> to achieve that, exporting and importing subgraphs and then forming new tf.gradients operations.</p>\n<p>The minimal example above, adapted from issue 7404, shows how this approach fails when a tf.while is used and the outer context is missing. Importing and exporting the metagraph instead would leave the tf.square op in the graph. While for this minimal example being forced to save also the loss tensor looks like a very minor limitation it is easy to conceive actual applications in which there are large parts of the graph which we might really want to exclude (e.g., a decoder network in Capsule-network training).</p>\n<p>Right now, the dependence on outer context for computing gradients for tf.while is incompatible with tf.graph_util.extract_sub_graph and similar operations that operate on graphdefs. I believe that this is not a negligible functionality limitation.  A <a href=\"https://stackoverflow.com/questions/50663594/computing-gradients-in-extracted-tensorflow-subgraph-which-contains-a-while-loo\" rel=\"nofollow\">related StackOverflow question</a> was upvoted but left unanswered.</p>\n<p>In general, from the perspective of an API user who is ignorant of the internal implementation of TF, any dependence of operations on information stored out of the graphdef is not expected, hinders graph editing (as I try to convey above) and seems patchy.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 17.10\nTensorFlow installed from (source or binary):\nBinary\nTensorFlow version (use command below):\nv1.8.0-0-g93bc2e2072\nPython version:\n3.6.5\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\n9.0/7.0.5\nGPU model and memory:\nTitan XP\nExact command to reproduce:\n\nimport tensorflow as tf\ng1=tf.Graph()\nsess1=tf.Session(graph=g1)\nwith g1.as_default():\n    with sess1.as_default():\n        i=tf.constant(0, name=\"input\")\n        out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\n        loss=tf.square(out,name='loss')\n        graph_def = tf.graph_util.convert_variables_to_constants(sess1,g1.as_graph_def(),['output/Exit'])\n\ng2 = tf.Graph()\nwith g2.as_default():\n    tf.import_graph_def(graph_def,name='')\n    i_imported = g2.get_tensor_by_name(\"input:0\")\n    out_imported = g2.get_tensor_by_name(\"output/Exit:0\")\n    tf.gradients(out_imported, i_imported)\n\n\noutput:\nINFO:tensorflow:Froze 0 variables.\nConverted 0 variables to const ops.\nTraceback (most recent call last):\n\n  File \"<ipython-input-1-908dc1dee750>\", line 23, in <module>\n    tf.gradients(out_imported, i_imported)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\n    gate_gradients, aggregation_method, stop_gradients)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 543, in _GradientsHelper\n    ops.get_default_graph(), to_ops, from_ops, colocate_gradients_with_ops)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 195, in _PendingCount\n    between_op_list, between_ops, colocate_gradients_with_ops)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1456, in MaybeCreateControlFlowState\n    loop_state.AddWhileContext(op, between_op_list, between_ops)\n\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1262, in AddWhileContext\n    outer_forward_ctxt = forward_ctxt.outer_context\n\nAttributeError: 'NoneType' object has no attribute 'outer_context'\n\nDescribe the problem\nTF  issue #7404 describes that when trying to form a gradient op in an imported (sub)graph, a 'No attribute 'outer_context'' error occurs. This issue was closed with the recommendation to use tf.train.import_meta_graph instead, so the outer context related to the while op is included.\nHowever, this does not fully solve the problem. In some deep-learning related settings, one might want to train a model, extract a subgraph (i.e., remove training related ops), connect the extracted subgraph into a larger graph to serve as part of an ensemble, GAN and so on, and then retrain the larger graph. When there is no dependence on outer context, one can easily use graph editing tools such as tf.graph_util.convert_variables_to_constants or tf.graph_util.extract_sub_graph to achieve that, exporting and importing subgraphs and then forming new tf.gradients operations.\nThe minimal example above, adapted from issue 7404, shows how this approach fails when a tf.while is used and the outer context is missing. Importing and exporting the metagraph instead would leave the tf.square op in the graph. While for this minimal example being forced to save also the loss tensor looks like a very minor limitation it is easy to conceive actual applications in which there are large parts of the graph which we might really want to exclude (e.g., a decoder network in Capsule-network training).\nRight now, the dependence on outer context for computing gradients for tf.while is incompatible with tf.graph_util.extract_sub_graph and similar operations that operate on graphdefs. I believe that this is not a negligible functionality limitation.  A related StackOverflow question was upvoted but left unanswered.\nIn general, from the perspective of an API user who is ignorant of the internal implementation of TF, any dependence of operations on information stored out of the graphdef is not expected, hinders graph editing (as I try to convey above) and seems patchy.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**:\r\nBinary\r\n- **TensorFlow version (use command below)**:\r\nv1.8.0-0-g93bc2e2072 \r\n- **Python version**: \r\n3.6.5\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\n9.0/7.0.5\r\n- **GPU model and memory**:\r\nTitan XP\r\n- **Exact command to reproduce**:\r\n```\r\nimport tensorflow as tf\r\ng1=tf.Graph()\r\nsess1=tf.Session(graph=g1)\r\nwith g1.as_default():\r\n    with sess1.as_default():\r\n        i=tf.constant(0, name=\"input\")\r\n        out=tf.while_loop(lambda i: tf.less(i,5), lambda i: [tf.add(i,1)], [i], name=\"output\")\r\n        loss=tf.square(out,name='loss')\r\n        graph_def = tf.graph_util.convert_variables_to_constants(sess1,g1.as_graph_def(),['output/Exit'])\r\n\r\ng2 = tf.Graph()\r\nwith g2.as_default():\r\n    tf.import_graph_def(graph_def,name='')\r\n    i_imported = g2.get_tensor_by_name(\"input:0\")\r\n    out_imported = g2.get_tensor_by_name(\"output/Exit:0\")\r\n    tf.gradients(out_imported, i_imported)\r\n\r\n```\r\noutput:\r\n```\r\nINFO:tensorflow:Froze 0 variables.\r\nConverted 0 variables to const ops.\r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-1-908dc1dee750>\", line 23, in <module>\r\n    tf.gradients(out_imported, i_imported)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 494, in gradients\r\n    gate_gradients, aggregation_method, stop_gradients)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 543, in _GradientsHelper\r\n    ops.get_default_graph(), to_ops, from_ops, colocate_gradients_with_ops)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py\", line 195, in _PendingCount\r\n    between_op_list, between_ops, colocate_gradients_with_ops)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1456, in MaybeCreateControlFlowState\r\n    loop_state.AddWhileContext(op, between_op_list, between_ops)\r\n\r\n  File \"/home/user/anaconda3/envs/tf1.8_gpu/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 1262, in AddWhileContext\r\n    outer_forward_ctxt = forward_ctxt.outer_context\r\n\r\nAttributeError: 'NoneType' object has no attribute 'outer_context'\r\n```\r\n### Describe the problem\r\nTF  [issue #7404](https://github.com/tensorflow/tensorflow/issues/7404) describes that when trying to form a gradient op in an imported (sub)graph, a 'No attribute 'outer_context'' error occurs. This issue was closed with the recommendation to use `tf.train.import_meta_graph` instead, so the outer context related to the while op is included.\r\n\r\nHowever, this does not fully solve the problem. In some deep-learning related settings, one might want to train a model, extract a subgraph (i.e., remove training related ops), connect the extracted subgraph into a larger graph to serve as part of an ensemble, GAN and so on, and then retrain the larger graph. When there is no dependence on outer context, one can easily use graph editing tools such as `tf.graph_util.convert_variables_to_constants` or `tf.graph_util.extract_sub_graph` to achieve that, exporting and importing subgraphs and then forming new tf.gradients operations. \r\n\r\nThe minimal example above, adapted from issue 7404, shows how this approach fails when a tf.while is used and the outer context is missing. Importing and exporting the metagraph instead would leave the tf.square op in the graph. While for this minimal example being forced to save also the loss tensor looks like a very minor limitation it is easy to conceive actual applications in which there are large parts of the graph which we might really want to exclude (e.g., a decoder network in Capsule-network training). \r\n\r\nRight now, the dependence on outer context for computing gradients for tf.while is incompatible with tf.graph_util.extract_sub_graph and similar operations that operate on graphdefs. I believe that this is not a negligible functionality limitation.  A [related StackOverflow question](https://stackoverflow.com/questions/50663594/computing-gradients-in-extracted-tensorflow-subgraph-which-contains-a-while-loo) was upvoted but left unanswered.\r\n\r\nIn general, from the perspective of an API user who is ignorant of the internal implementation of TF, any dependence of operations on information stored out of the graphdef is not expected, hinders graph editing (as I try to convey above) and seems patchy."}