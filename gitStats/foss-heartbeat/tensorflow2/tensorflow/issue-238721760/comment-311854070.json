{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/311854070", "html_url": "https://github.com/tensorflow/tensorflow/issues/11071#issuecomment-311854070", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11071", "id": 311854070, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMTg1NDA3MA==", "user": {"login": "hellolovetiger", "id": 18715195, "node_id": "MDQ6VXNlcjE4NzE1MTk1", "avatar_url": "https://avatars0.githubusercontent.com/u/18715195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hellolovetiger", "html_url": "https://github.com/hellolovetiger", "followers_url": "https://api.github.com/users/hellolovetiger/followers", "following_url": "https://api.github.com/users/hellolovetiger/following{/other_user}", "gists_url": "https://api.github.com/users/hellolovetiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/hellolovetiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hellolovetiger/subscriptions", "organizations_url": "https://api.github.com/users/hellolovetiger/orgs", "repos_url": "https://api.github.com/users/hellolovetiger/repos", "events_url": "https://api.github.com/users/hellolovetiger/events{/privacy}", "received_events_url": "https://api.github.com/users/hellolovetiger/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-29T03:40:23Z", "updated_at": "2017-06-29T04:00:16Z", "author_association": "NONE", "body_html": "<p>No, there is no \"Waiting for model to be ready.\" info printed. Just \"Start master session *** with config:\" hang there over and over again.<br>\nAnd, the problem fixed after I remove the local optimizer below and write my own by following <a href=\"https://stackoverflow.com/questions/39167070/implementing-gradient-descent-in-tensorflow-instead-of-using-the-one-provided-wi\" rel=\"nofollow\">this</a>.<br>\nold:</p>\n<pre><code>            print '-- local update'\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\n            updates = worker_tparams\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\n</code></pre>\n<p>new:</p>\n<pre><code>            print '-- local update'\n            local_train_ops = []\n            vars = worker_tparams\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\n            grads = tf.gradients(cost, vars, colocate_gradients_with_ops=True)\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\n            for var, grad in zip(vars, clipped_grads):\n                local_train_ops.append(var.assign_sub(grad * tf.convert_to_tensor(config.lr)))  # var -= lr * grad, GradientDescentOptimizer instead of MomentumOptimizer\n            train_op = tf.group(*local_train_ops)\n</code></pre>\n<p>But new problem occurs, the \"sync\" mode seems not stable. The loss will increase dramatically suddenly after several global updates.</p>", "body_text": "No, there is no \"Waiting for model to be ready.\" info printed. Just \"Start master session *** with config:\" hang there over and over again.\nAnd, the problem fixed after I remove the local optimizer below and write my own by following this.\nold:\n            print '-- local update'\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\n            updates = worker_tparams\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\n\nnew:\n            print '-- local update'\n            local_train_ops = []\n            vars = worker_tparams\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\n            grads = tf.gradients(cost, vars, colocate_gradients_with_ops=True)\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\n            for var, grad in zip(vars, clipped_grads):\n                local_train_ops.append(var.assign_sub(grad * tf.convert_to_tensor(config.lr)))  # var -= lr * grad, GradientDescentOptimizer instead of MomentumOptimizer\n            train_op = tf.group(*local_train_ops)\n\nBut new problem occurs, the \"sync\" mode seems not stable. The loss will increase dramatically suddenly after several global updates.", "body": "No, there is no \"Waiting for model to be ready.\" info printed. Just \"Start master session *** with config:\" hang there over and over again.\r\nAnd, the problem fixed after I remove the local optimizer below and write my own by following [this](https://stackoverflow.com/questions/39167070/implementing-gradient-descent-in-tensorflow-instead-of-using-the-one-provided-wi).\r\nold:\r\n```\r\n            print '-- local update'\r\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\r\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\r\n            updates = worker_tparams\r\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\r\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\r\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\r\n```\r\nnew:\r\n```\r\n            print '-- local update'\r\n            local_train_ops = []\r\n            vars = worker_tparams\r\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\r\n            grads = tf.gradients(cost, vars, colocate_gradients_with_ops=True)\r\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\r\n            for var, grad in zip(vars, clipped_grads):\r\n                local_train_ops.append(var.assign_sub(grad * tf.convert_to_tensor(config.lr)))  # var -= lr * grad, GradientDescentOptimizer instead of MomentumOptimizer\r\n            train_op = tf.group(*local_train_ops)\r\n```\r\nBut new problem occurs, the \"sync\" mode seems not stable. The loss will increase dramatically suddenly after several global updates."}