{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11071", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11071/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11071/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11071/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11071", "id": 238721760, "node_id": "MDU6SXNzdWUyMzg3MjE3NjA=", "number": 11071, "title": "non-chief worker stuck in distributed SYNC mode for graph with two optimizers", "user": {"login": "hellolovetiger", "id": 18715195, "node_id": "MDQ6VXNlcjE4NzE1MTk1", "avatar_url": "https://avatars0.githubusercontent.com/u/18715195?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hellolovetiger", "html_url": "https://github.com/hellolovetiger", "followers_url": "https://api.github.com/users/hellolovetiger/followers", "following_url": "https://api.github.com/users/hellolovetiger/following{/other_user}", "gists_url": "https://api.github.com/users/hellolovetiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/hellolovetiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hellolovetiger/subscriptions", "organizations_url": "https://api.github.com/users/hellolovetiger/orgs", "repos_url": "https://api.github.com/users/hellolovetiger/repos", "events_url": "https://api.github.com/users/hellolovetiger/events{/privacy}", "received_events_url": "https://api.github.com/users/hellolovetiger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 18, "created_at": "2017-06-27T02:36:57Z", "updated_at": "2018-04-09T17:11:10Z", "closed_at": "2018-04-09T17:11:10Z", "author_association": "NONE", "body_html": "<p>Seems distributed tensorflow cannot train graph with two optimizers in sync mode (one for local update, the other for ps update)</p>\n<p>There are three parts in my graph:</p>\n<ol>\n<li>each worker has its own copy of vars as the ps server, but are defined with local variable <code>collections=[tf.GraphKeys.LOCAL_VARIABLES]</code>, each worker has its own forward-backward loop based on the local vars and its own optimizer</li>\n<li>(local variable - ps variable) as the gradient and apply to ps variable with SyncReplicasOptimizer.apply_gradients</li>\n<li>broadcast the ps variable to the local variable</li>\n</ol>\n<p>The three parts are run in this way: run subgraph 1 several times, then run subgraph 2 in distributed sync mode to update ps params and then run subgraph 3</p>\n<h3>Source code / logs</h3>\n<pre><code>    if args.job_name == 'ps':\n        server.join()\n    elif args.job_name == 'worker':\n        is_chief = (args.task_index == 0)\n        num_gpus = len(worker_spec)\n\n        ps_device = '/job:ps/cpu:0'\n        worker_device = '/job:worker/task:%d/gpu:0' % args.task_index\n        with tf.device(\n                tf.train.replica_device_setter(cluster=cluster, ps_device=ps_device, worker_device=worker_device)):\n            global_step = tf.Variable(args.start_step, name='global_step', trainable=False)\n\n            print 'building ps params'\n            ps_tparams = init_tparams()\n\n            print 'building local params'\n            with tf.device(worker_device):\n                worker_tparams = init_tparams(is_local=True)  # define variable in collection tf.GraphKeys.LOCAL_VARIABLES\n\n            print 'building graph'\n\n            print '-- local update'\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\n            updates = worker_tparams\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\n\n            print '-- reduce average'\n            ps_updates = ps_tparams\n            avg_grads = [tf.sub(var, ps_var) for var, ps_var in zip(updates, ps_updates)]\n            bopt = tf.train.MomentumOptimizer(config.blr, config.bmr, use_nesterov=True)\n            bopt = tf.train.SyncReplicasOptimizerV2(bopt, replicas_to_aggregate=num_gpus, total_num_replicas=num_gpus)\n            update_op = bopt.apply_gradients(zip(avg_grads, ps_updates), global_step=global_step)\n\n            print '-- broadcast'\n            broadcast_ops = []\n            for kk, pp in ps_tparams.items():\n                broadcast_ops.append(worker_tparams[kk].assign(pp).op)\n\n            # Others related to sync mode\n            chief_queue_runner = bopt.get_chief_queue_runner()\n            sync_init_op = bopt.get_init_tokens_op()\n\n            sv = tf.train.Supervisor(\n                is_chief=is_chief,\n                logdir=config.ckp,\n                init_op=tf.global_variables_initializer(),\n                local_init_op=tf.local_variables_initializer(),\n                global_step=global_step)\n\n            if is_chief:\n                print('Worker %d: Initializing session' % args.task_index)\n            else:\n                print('Worker %d: Waiting for session to be initialized' % args.task_index)\n\n            sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                         device_filters=['/job:ps', '/job:worker/task:%d' % args.task_index])\n            with sv.managed_session(server.target, config=sess_config) as sess:\n                print('Worker %d: Session initialization completed' % args.task_index)\n\n                if is_chief:\n                    # Chief worker will start the chief queue runner and call the init op\n                    sess.run(sync_init_op)\n                    sv.start_queue_runners(sess, [chief_queue_runner])\n</code></pre>\n<p>The non-chief works stuck at <code>sv.managed_session</code> and showing below message again and again:<br>\n<code>I tensorflow/core/distributed_runtime/master_session.cc:993] Start master session a4de1cec7011a62d with config:</code><br>\nThe code can run successfully when there is no local optimizer.</p>\n<h3>System information</h3>\n<ul>\n<li>Linux Ubuntu 14.04</li>\n<li>CUDA 8.0</li>\n<li>tf 0.12.0-rc1</li>\n<li>GPU: GeForce GTX 1080</li>\n</ul>", "body_text": "Seems distributed tensorflow cannot train graph with two optimizers in sync mode (one for local update, the other for ps update)\nThere are three parts in my graph:\n\neach worker has its own copy of vars as the ps server, but are defined with local variable collections=[tf.GraphKeys.LOCAL_VARIABLES], each worker has its own forward-backward loop based on the local vars and its own optimizer\n(local variable - ps variable) as the gradient and apply to ps variable with SyncReplicasOptimizer.apply_gradients\nbroadcast the ps variable to the local variable\n\nThe three parts are run in this way: run subgraph 1 several times, then run subgraph 2 in distributed sync mode to update ps params and then run subgraph 3\nSource code / logs\n    if args.job_name == 'ps':\n        server.join()\n    elif args.job_name == 'worker':\n        is_chief = (args.task_index == 0)\n        num_gpus = len(worker_spec)\n\n        ps_device = '/job:ps/cpu:0'\n        worker_device = '/job:worker/task:%d/gpu:0' % args.task_index\n        with tf.device(\n                tf.train.replica_device_setter(cluster=cluster, ps_device=ps_device, worker_device=worker_device)):\n            global_step = tf.Variable(args.start_step, name='global_step', trainable=False)\n\n            print 'building ps params'\n            ps_tparams = init_tparams()\n\n            print 'building local params'\n            with tf.device(worker_device):\n                worker_tparams = init_tparams(is_local=True)  # define variable in collection tf.GraphKeys.LOCAL_VARIABLES\n\n            print 'building graph'\n\n            print '-- local update'\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\n            updates = worker_tparams\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\n\n            print '-- reduce average'\n            ps_updates = ps_tparams\n            avg_grads = [tf.sub(var, ps_var) for var, ps_var in zip(updates, ps_updates)]\n            bopt = tf.train.MomentumOptimizer(config.blr, config.bmr, use_nesterov=True)\n            bopt = tf.train.SyncReplicasOptimizerV2(bopt, replicas_to_aggregate=num_gpus, total_num_replicas=num_gpus)\n            update_op = bopt.apply_gradients(zip(avg_grads, ps_updates), global_step=global_step)\n\n            print '-- broadcast'\n            broadcast_ops = []\n            for kk, pp in ps_tparams.items():\n                broadcast_ops.append(worker_tparams[kk].assign(pp).op)\n\n            # Others related to sync mode\n            chief_queue_runner = bopt.get_chief_queue_runner()\n            sync_init_op = bopt.get_init_tokens_op()\n\n            sv = tf.train.Supervisor(\n                is_chief=is_chief,\n                logdir=config.ckp,\n                init_op=tf.global_variables_initializer(),\n                local_init_op=tf.local_variables_initializer(),\n                global_step=global_step)\n\n            if is_chief:\n                print('Worker %d: Initializing session' % args.task_index)\n            else:\n                print('Worker %d: Waiting for session to be initialized' % args.task_index)\n\n            sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\n                                         device_filters=['/job:ps', '/job:worker/task:%d' % args.task_index])\n            with sv.managed_session(server.target, config=sess_config) as sess:\n                print('Worker %d: Session initialization completed' % args.task_index)\n\n                if is_chief:\n                    # Chief worker will start the chief queue runner and call the init op\n                    sess.run(sync_init_op)\n                    sv.start_queue_runners(sess, [chief_queue_runner])\n\nThe non-chief works stuck at sv.managed_session and showing below message again and again:\nI tensorflow/core/distributed_runtime/master_session.cc:993] Start master session a4de1cec7011a62d with config:\nThe code can run successfully when there is no local optimizer.\nSystem information\n\nLinux Ubuntu 14.04\nCUDA 8.0\ntf 0.12.0-rc1\nGPU: GeForce GTX 1080", "body": "Seems distributed tensorflow cannot train graph with two optimizers in sync mode (one for local update, the other for ps update)\r\n\r\nThere are three parts in my graph: \r\n1. each worker has its own copy of vars as the ps server, but are defined with local variable `collections=[tf.GraphKeys.LOCAL_VARIABLES]`, each worker has its own forward-backward loop based on the local vars and its own optimizer\r\n2. (local variable - ps variable) as the gradient and apply to ps variable with SyncReplicasOptimizer.apply_gradients\r\n3. broadcast the ps variable to the local variable\r\n\r\nThe three parts are run in this way: run subgraph 1 several times, then run subgraph 2 in distributed sync mode to update ps params and then run subgraph 3\r\n\r\n### Source code / logs\r\n```\r\n    if args.job_name == 'ps':\r\n        server.join()\r\n    elif args.job_name == 'worker':\r\n        is_chief = (args.task_index == 0)\r\n        num_gpus = len(worker_spec)\r\n\r\n        ps_device = '/job:ps/cpu:0'\r\n        worker_device = '/job:worker/task:%d/gpu:0' % args.task_index\r\n        with tf.device(\r\n                tf.train.replica_device_setter(cluster=cluster, ps_device=ps_device, worker_device=worker_device)):\r\n            global_step = tf.Variable(args.start_step, name='global_step', trainable=False)\r\n\r\n            print 'building ps params'\r\n            ps_tparams = init_tparams()\r\n\r\n            print 'building local params'\r\n            with tf.device(worker_device):\r\n                worker_tparams = init_tparams(is_local=True)  # define variable in collection tf.GraphKeys.LOCAL_VARIABLES\r\n\r\n            print 'building graph'\r\n\r\n            print '-- local update'\r\n            x, x_mask, y, y_mask, cost = build_graph(worker_tparams, config)\r\n            opt = tf.train.MomentumOptimizer(config.lr, config.mr)\r\n            updates = worker_tparams\r\n            grads = tf.gradients(cost, updates, colocate_gradients_with_ops=True)\r\n            clipped_grads, _ = tf.clip_by_global_norm(grads, config.clip_grads)\r\n            train_op = opt.apply_gradients(zip(clipped_grads, updates))\r\n\r\n            print '-- reduce average'\r\n            ps_updates = ps_tparams\r\n            avg_grads = [tf.sub(var, ps_var) for var, ps_var in zip(updates, ps_updates)]\r\n            bopt = tf.train.MomentumOptimizer(config.blr, config.bmr, use_nesterov=True)\r\n            bopt = tf.train.SyncReplicasOptimizerV2(bopt, replicas_to_aggregate=num_gpus, total_num_replicas=num_gpus)\r\n            update_op = bopt.apply_gradients(zip(avg_grads, ps_updates), global_step=global_step)\r\n\r\n            print '-- broadcast'\r\n            broadcast_ops = []\r\n            for kk, pp in ps_tparams.items():\r\n                broadcast_ops.append(worker_tparams[kk].assign(pp).op)\r\n\r\n            # Others related to sync mode\r\n            chief_queue_runner = bopt.get_chief_queue_runner()\r\n            sync_init_op = bopt.get_init_tokens_op()\r\n\r\n            sv = tf.train.Supervisor(\r\n                is_chief=is_chief,\r\n                logdir=config.ckp,\r\n                init_op=tf.global_variables_initializer(),\r\n                local_init_op=tf.local_variables_initializer(),\r\n                global_step=global_step)\r\n\r\n            if is_chief:\r\n                print('Worker %d: Initializing session' % args.task_index)\r\n            else:\r\n                print('Worker %d: Waiting for session to be initialized' % args.task_index)\r\n\r\n            sess_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False,\r\n                                         device_filters=['/job:ps', '/job:worker/task:%d' % args.task_index])\r\n            with sv.managed_session(server.target, config=sess_config) as sess:\r\n                print('Worker %d: Session initialization completed' % args.task_index)\r\n\r\n                if is_chief:\r\n                    # Chief worker will start the chief queue runner and call the init op\r\n                    sess.run(sync_init_op)\r\n                    sv.start_queue_runners(sess, [chief_queue_runner])\r\n```\r\nThe non-chief works stuck at `sv.managed_session` and showing below message again and again:\r\n`I tensorflow/core/distributed_runtime/master_session.cc:993] Start master session a4de1cec7011a62d with config:`\r\nThe code can run successfully when there is no local optimizer.\r\n\r\n### System information\r\n- Linux Ubuntu 14.04\r\n- CUDA 8.0\r\n- tf 0.12.0-rc1\r\n- GPU: GeForce GTX 1080"}