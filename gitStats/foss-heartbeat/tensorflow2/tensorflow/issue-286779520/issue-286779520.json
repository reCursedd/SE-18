{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15948", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15948/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15948/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15948/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15948", "id": 286779520, "node_id": "MDU6SXNzdWUyODY3Nzk1MjA=", "number": 15948, "title": "Error converting to .tflite Using Toco", "user": {"login": "OluwoleOyetoke", "id": 12373582, "node_id": "MDQ6VXNlcjEyMzczNTgy", "avatar_url": "https://avatars1.githubusercontent.com/u/12373582?v=4", "gravatar_id": "", "url": "https://api.github.com/users/OluwoleOyetoke", "html_url": "https://github.com/OluwoleOyetoke", "followers_url": "https://api.github.com/users/OluwoleOyetoke/followers", "following_url": "https://api.github.com/users/OluwoleOyetoke/following{/other_user}", "gists_url": "https://api.github.com/users/OluwoleOyetoke/gists{/gist_id}", "starred_url": "https://api.github.com/users/OluwoleOyetoke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/OluwoleOyetoke/subscriptions", "organizations_url": "https://api.github.com/users/OluwoleOyetoke/orgs", "repos_url": "https://api.github.com/users/OluwoleOyetoke/repos", "events_url": "https://api.github.com/users/OluwoleOyetoke/events{/privacy}", "received_events_url": "https://api.github.com/users/OluwoleOyetoke/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-01-08T15:23:14Z", "updated_at": "2018-10-01T08:51:16Z", "closed_at": "2018-04-25T15:07:04Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: Yes. See network definition code <a href=\"https://gist.github.com/OluwoleOyetoke/30f2cac788042c495f1ae34a6b742a1d\">here</a>. For complete project, see <a href=\"https://github.com/OluwoleOyetoke/Computer_Vision_Using_TensorFlowLite\">here</a></li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Yes</li>\n<li><strong>TensorFlow version (use command below)</strong>: tensorflow (1.4.0)</li>\n<li><strong>Python version</strong>:  Python 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: Bazel 0.9.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc 5.4.1 20160904</li>\n<li><strong>CUDA/cuDNN version</strong>: Null</li>\n<li><strong>GPU model and memory</strong>: Null</li>\n<li><strong>Exact command to reproduce</strong>: bazel-bin/tensorflow/contrib/lite/toco/toco --input_format=TENSORFLOW_GRAPHDEF --input_file=$1 --output_format=TFLITE --output_file=$2 --inference_type=$3 --#input_type=$4 --input_arrays=$5 --output_arrays=$6 --inference_input_type=$7 --input_shapes=1,227,227,3</li>\n</ul>\n<h3>Problem Description, Source Code and Logs</h3>\n<p>When I try to use Toco to convert my Custom AlexNet Model from TensorFlow to TensorFlowLite, I repeatedly get a dimensions error as shown below</p>\n<pre><code>F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)\n</code></pre>\n<p>Here is how I call toco:</p>\n<pre><code>bazel-bin/tensorflow/contrib/lite/toco/toco \\\n--input_format=TENSORFLOW_GRAPHDEF \\\n--input_file=/tmp/output_graph.pb \\\n--output_format=TFLITE \\\n--output_file=/tmp/my_model.lite \\\n--inference_type=FLOAT \\\n--inference_input_type=FLOAT \\\n--input_arrays=input_layer \\\n--output_arrays=classes_tensor\\\n--input_shapes=1,227,227,3\n</code></pre>\n<p>Here is my terminal print out during the operation:</p>\n<pre><code>INFO: Analysed 0 targets (4 packages loaded).\nINFO: Found 0 targets...\nINFO: Elapsed time: 5.267s, Critical Path: 0.03s\nINFO: Build completed successfully, 1 total action\n2018-01-05 10:24:23.011483: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\n2018-01-05 10:24:25.853112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: IsVariableInitialized\n2018-01-05 10:24:25.853197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RefSwitch\n2018-01-05 10:24:25.853241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomShuffleQueueV2\n2018-01-05 10:24:25.853268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QueueDequeueUpToV2\n2018-01-05 10:24:26.207160: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 64 operators, 90 arrays (0 quantized)\n2018-01-05 10:24:27.327055: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 15 operators, 33 arrays (0 quantized)\n2018-01-05 10:24:27.327262: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 15 operators, 34 arrays (0 quantized)\n2018-01-05 10:24:27.327356: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)\n/home/olu/Dev/scratch_train_sign/freeze_graph_tf.sh: line 28:  8881 Aborted \n</code></pre>\n<p>I went into the propagate_fixed_sizes.cc file, and around line line 982 I found this comment below</p>\n<pre><code>// The current ArgMax implementation only supports 4-dimensional inputs with\n// the last dimension as the axis to perform ArgMax for.\n</code></pre>\n<p>The only place in my training code where I used ArgMax is as below:</p>\n<pre><code> predictions = { \"classes\": tf.argmax(input=logits, axis=1, name=\"classes_tensor\"), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\") }\n</code></pre>\n<p>The failure messaged printed out seem not to be sufficient enough for me to understand what exactly the problem is. My trained TensorFow Model is fine and I have been able to run inferences on it, however, converting these saved model to TFLite doesn't work. Could this mean the current TFLite version does not support the conversion of some custom TF models yet?. Please I will be glad to know what exactly I may be doing wrong.</p>\n<p>Thank you</p>", "body_text": "System information\n\nHave I written custom code: Yes. See network definition code here. For complete project, see here\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Yes\nTensorFlow version (use command below): tensorflow (1.4.0)\nPython version:  Python 2.7.12\nBazel version (if compiling from source): Bazel 0.9.0\nGCC/Compiler version (if compiling from source): gcc 5.4.1 20160904\nCUDA/cuDNN version: Null\nGPU model and memory: Null\nExact command to reproduce: bazel-bin/tensorflow/contrib/lite/toco/toco --input_format=TENSORFLOW_GRAPHDEF --input_file=$1 --output_format=TFLITE --output_file=$2 --inference_type=$3 --#input_type=$4 --input_arrays=$5 --output_arrays=$6 --inference_input_type=$7 --input_shapes=1,227,227,3\n\nProblem Description, Source Code and Logs\nWhen I try to use Toco to convert my Custom AlexNet Model from TensorFlow to TensorFlowLite, I repeatedly get a dimensions error as shown below\nF tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)\n\nHere is how I call toco:\nbazel-bin/tensorflow/contrib/lite/toco/toco \\\n--input_format=TENSORFLOW_GRAPHDEF \\\n--input_file=/tmp/output_graph.pb \\\n--output_format=TFLITE \\\n--output_file=/tmp/my_model.lite \\\n--inference_type=FLOAT \\\n--inference_input_type=FLOAT \\\n--input_arrays=input_layer \\\n--output_arrays=classes_tensor\\\n--input_shapes=1,227,227,3\n\nHere is my terminal print out during the operation:\nINFO: Analysed 0 targets (4 packages loaded).\nINFO: Found 0 targets...\nINFO: Elapsed time: 5.267s, Critical Path: 0.03s\nINFO: Build completed successfully, 1 total action\n2018-01-05 10:24:23.011483: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\n2018-01-05 10:24:25.853112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: IsVariableInitialized\n2018-01-05 10:24:25.853197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RefSwitch\n2018-01-05 10:24:25.853241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomShuffleQueueV2\n2018-01-05 10:24:25.853268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QueueDequeueUpToV2\n2018-01-05 10:24:26.207160: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 64 operators, 90 arrays (0 quantized)\n2018-01-05 10:24:27.327055: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 15 operators, 33 arrays (0 quantized)\n2018-01-05 10:24:27.327262: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 15 operators, 34 arrays (0 quantized)\n2018-01-05 10:24:27.327356: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)\n/home/olu/Dev/scratch_train_sign/freeze_graph_tf.sh: line 28:  8881 Aborted \n\nI went into the propagate_fixed_sizes.cc file, and around line line 982 I found this comment below\n// The current ArgMax implementation only supports 4-dimensional inputs with\n// the last dimension as the axis to perform ArgMax for.\n\nThe only place in my training code where I used ArgMax is as below:\n predictions = { \"classes\": tf.argmax(input=logits, axis=1, name=\"classes_tensor\"), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\") }\n\nThe failure messaged printed out seem not to be sufficient enough for me to understand what exactly the problem is. My trained TensorFow Model is fine and I have been able to run inferences on it, however, converting these saved model to TFLite doesn't work. Could this mean the current TFLite version does not support the conversion of some custom TF models yet?. Please I will be glad to know what exactly I may be doing wrong.\nThank you", "body": "### System information\r\n- **Have I written custom code**: Yes. See network definition code [here](https://gist.github.com/OluwoleOyetoke/30f2cac788042c495f1ae34a6b742a1d). For complete project, see [here](https://github.com/OluwoleOyetoke/Computer_Vision_Using_TensorFlowLite)\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Yes\r\n- **TensorFlow version (use command below)**: tensorflow (1.4.0)\r\n- **Python version**:  Python 2.7.12\r\n- **Bazel version (if compiling from source)**: Bazel 0.9.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc 5.4.1 20160904\r\n- **CUDA/cuDNN version**: Null\r\n- **GPU model and memory**: Null\r\n- **Exact command to reproduce**: bazel-bin/tensorflow/contrib/lite/toco/toco --input_format=TENSORFLOW_GRAPHDEF --input_file=$1 --output_format=TFLITE --output_file=$2 --inference_type=$3 --#input_type=$4 --input_arrays=$5 --output_arrays=$6 --inference_input_type=$7 --input_shapes=1,227,227,3\r\n\r\n### Problem Description, Source Code and Logs\r\nWhen I try to use Toco to convert my Custom AlexNet Model from TensorFlow to TensorFlowLite, I repeatedly get a dimensions error as shown below\r\n\r\n    F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)\r\n\r\nHere is how I call toco:\r\n\r\n    bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n    --input_format=TENSORFLOW_GRAPHDEF \\\r\n    --input_file=/tmp/output_graph.pb \\\r\n    --output_format=TFLITE \\\r\n    --output_file=/tmp/my_model.lite \\\r\n    --inference_type=FLOAT \\\r\n    --inference_input_type=FLOAT \\\r\n    --input_arrays=input_layer \\\r\n    --output_arrays=classes_tensor\\\r\n    --input_shapes=1,227,227,3\r\n\r\nHere is my terminal print out during the operation: \r\n\r\n    INFO: Analysed 0 targets (4 packages loaded).\r\n    INFO: Found 0 targets...\r\n    INFO: Elapsed time: 5.267s, Critical Path: 0.03s\r\n    INFO: Build completed successfully, 1 total action\r\n    2018-01-05 10:24:23.011483: W tensorflow/contrib/lite/toco/toco_cmdline_flags.cc:178] --input_type is deprecated. It was an ambiguous flag that set both --input_data_types and --inference_input_type. If you are trying to complement the input file with information about the type of input arrays, use --input_data_type. If you are trying to control the quantization/dequantization of real-numbers input arrays in the output file, use --inference_input_type.\r\n    2018-01-05 10:24:25.853112: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: IsVariableInitialized\r\n    2018-01-05 10:24:25.853197: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RefSwitch\r\n    2018-01-05 10:24:25.853241: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: RandomShuffleQueueV2\r\n    2018-01-05 10:24:25.853268: I tensorflow/contrib/lite/toco/import_tensorflow.cc:1122] Converting unsupported operation: QueueDequeueUpToV2\r\n    2018-01-05 10:24:26.207160: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 64 operators, 90 arrays (0 quantized)\r\n    2018-01-05 10:24:27.327055: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 15 operators, 33 arrays (0 quantized)\r\n    2018-01-05 10:24:27.327262: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 2: 15 operators, 34 arrays (0 quantized)\r\n    2018-01-05 10:24:27.327356: F tensorflow/contrib/lite/toco/graph_transformations/propagate_fixed_sizes.cc:982] Check failed: input_dims.size() == 4 (2 vs. 4)\r\n    /home/olu/Dev/scratch_train_sign/freeze_graph_tf.sh: line 28:  8881 Aborted \r\n\r\nI went into the propagate_fixed_sizes.cc file, and around line line 982 I found this comment below\r\n\r\n    // The current ArgMax implementation only supports 4-dimensional inputs with\r\n    // the last dimension as the axis to perform ArgMax for.\r\n\r\nThe only place in my training code where I used ArgMax is as below:\r\n\r\n     predictions = { \"classes\": tf.argmax(input=logits, axis=1, name=\"classes_tensor\"), \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\") }\r\n\r\nThe failure messaged printed out seem not to be sufficient enough for me to understand what exactly the problem is. My trained TensorFow Model is fine and I have been able to run inferences on it, however, converting these saved model to TFLite doesn't work. Could this mean the current TFLite version does not support the conversion of some custom TF models yet?. Please I will be glad to know what exactly I may be doing wrong. \r\n\r\nThank you \r\n\r\n  "}