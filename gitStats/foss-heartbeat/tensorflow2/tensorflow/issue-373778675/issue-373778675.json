{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23242", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23242/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23242/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23242/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/23242", "id": 373778675, "node_id": "MDExOlB1bGxSZXF1ZXN0MjI1NjI3ODkw", "number": 23242, "title": "[Features]mixed precision support enhancement using decorator", "user": {"login": "Dido0o0", "id": 16295526, "node_id": "MDQ6VXNlcjE2Mjk1NTI2", "avatar_url": "https://avatars2.githubusercontent.com/u/16295526?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dido0o0", "html_url": "https://github.com/Dido0o0", "followers_url": "https://api.github.com/users/Dido0o0/followers", "following_url": "https://api.github.com/users/Dido0o0/following{/other_user}", "gists_url": "https://api.github.com/users/Dido0o0/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dido0o0/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dido0o0/subscriptions", "organizations_url": "https://api.github.com/users/Dido0o0/orgs", "repos_url": "https://api.github.com/users/Dido0o0/repos", "events_url": "https://api.github.com/users/Dido0o0/events{/privacy}", "received_events_url": "https://api.github.com/users/Dido0o0/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136613, "node_id": "MDU6TGFiZWwzMDAxMzY2MTM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20no", "name": "cla: no", "color": "eb6420", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-10-25T05:27:04Z", "updated_at": "2018-10-25T05:35:12Z", "closed_at": "2018-10-25T05:35:12Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23242", "html_url": "https://github.com/tensorflow/tensorflow/pull/23242", "diff_url": "https://github.com/tensorflow/tensorflow/pull/23242.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/23242.patch"}, "body_html": "<p>This feature realize the automatic/constant loss scaling algorithm in Backprop, which can help preserve small gradient magnitudes ( ref to  <a href=\"https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html\" rel=\"nofollow\">https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html</a>)<br>\nMixed precision with loss scaling is already support by tf.contrib.mixed_precision.LossScaleOptimizer,  with the following usage:</p>\n<div class=\"highlight highlight-source-python\"><pre>loss <span class=\"pl-k\">=</span> loss_fn()\nopt <span class=\"pl-k\">=</span> tf.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">...</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Choose a loss scale manager which decides how to pick the right loss scale</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> throughout the training process.</span>\nloss_scale_manger <span class=\"pl-k\">=</span> tf.contrib.mixed_precision.FixedLossScaleManager(<span class=\"pl-c1\">5000</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Wraps the original optimizer in a LossScaleOptimizer.</span>\nloss_scale_optimizer <span class=\"pl-k\">=</span> LossScaleOptimizer(opt, loss_scale_manager)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Call minimize() on the loss scale optimizer.</span>\ntrain_op <span class=\"pl-k\">=</span> loss_scale_optimizer.minimize(loss)</pre></div>\n<p>The optimizer <code>LossScaleOptimizer</code> is actually a wrapper of other optimizers.  However, the usage using wrapper is too limited to support all cases.  For example,<br>\n+ if the users  are using <code>tf.gradients()</code> to compute the gradients rather than <code>optimizer.compute_gradients()</code>/<code>optimizer.minimize()</code>,  the wrapper is no longer in user.<br>\n+ the <code>LossScaleOptimizer</code> wrapper may have compatibility problem with other optimizer wrappers, e.g. <code>SyncReplicasOptimizer</code><br>\n+ the current implementation in <code>LossScaleOptimizer</code> did not take the colocation parameter into consideration.</p>\n<p>Instead, we implement loss scaling using a decorator on <code>gradients()</code>, which is the atom function to compute the backprop gradients. The usage is as following:</p>\n<div class=\"highlight highlight-source-python\"><pre>loss <span class=\"pl-k\">=</span> loss_fn()\nopt <span class=\"pl-k\">=</span> tf.AdamOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">...</span>)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> constant loss scaling with loss_scale=64</span>\n<span class=\"pl-k\">with</span> tf.mixed_precision_scope(<span class=\"pl-v\">automatic_loss_scaling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">loss_scale</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>):\n      grad <span class=\"pl-k\">=</span> tf.gradients()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> if using tf.gradients</span>\n      var_and_grad <span class=\"pl-k\">=</span>opt.minimize(loss) <span class=\"pl-c\"><span class=\"pl-c\">#</span> if using optimizer</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> automatic loss scaling</span>\n<span class=\"pl-k\">with</span> tf.mixed_precision_scope(<span class=\"pl-v\">automatic_loss_scaling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n      grad <span class=\"pl-k\">=</span> tf.gradients()  <span class=\"pl-c\"><span class=\"pl-c\">#</span> if using tf.gradients</span>\n      var_and_grad <span class=\"pl-k\">=</span>opt.minimize(loss) <span class=\"pl-c\"><span class=\"pl-c\">#</span> if using optimizer</span></pre></div>\n<p>The implementation can handle all of the above problems encounter by <code>LossScaleOptimizer</code> .</p>", "body_text": "This feature realize the automatic/constant loss scaling algorithm in Backprop, which can help preserve small gradient magnitudes ( ref to  https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html)\nMixed precision with loss scaling is already support by tf.contrib.mixed_precision.LossScaleOptimizer,  with the following usage:\nloss = loss_fn()\nopt = tf.AdamOptimizer(learning_rate=...)\n\n# Choose a loss scale manager which decides how to pick the right loss scale\n# throughout the training process.\nloss_scale_manger = tf.contrib.mixed_precision.FixedLossScaleManager(5000)\n\n# Wraps the original optimizer in a LossScaleOptimizer.\nloss_scale_optimizer = LossScaleOptimizer(opt, loss_scale_manager)\n\n# Call minimize() on the loss scale optimizer.\ntrain_op = loss_scale_optimizer.minimize(loss)\nThe optimizer LossScaleOptimizer is actually a wrapper of other optimizers.  However, the usage using wrapper is too limited to support all cases.  For example,\n+ if the users  are using tf.gradients() to compute the gradients rather than optimizer.compute_gradients()/optimizer.minimize(),  the wrapper is no longer in user.\n+ the LossScaleOptimizer wrapper may have compatibility problem with other optimizer wrappers, e.g. SyncReplicasOptimizer\n+ the current implementation in LossScaleOptimizer did not take the colocation parameter into consideration.\nInstead, we implement loss scaling using a decorator on gradients(), which is the atom function to compute the backprop gradients. The usage is as following:\nloss = loss_fn()\nopt = tf.AdamOptimizer(learning_rate=...)\n# constant loss scaling with loss_scale=64\nwith tf.mixed_precision_scope(automatic_loss_scaling=False, loss_scale=64):\n      grad = tf.gradients()  # if using tf.gradients\n      var_and_grad =opt.minimize(loss) # if using optimizer\n\n# automatic loss scaling\nwith tf.mixed_precision_scope(automatic_loss_scaling=True):\n      grad = tf.gradients()  # if using tf.gradients\n      var_and_grad =opt.minimize(loss) # if using optimizer\nThe implementation can handle all of the above problems encounter by LossScaleOptimizer .", "body": "This feature realize the automatic/constant loss scaling algorithm in Backprop, which can help preserve small gradient magnitudes ( ref to  https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html)\r\nMixed precision with loss scaling is already support by tf.contrib.mixed_precision.LossScaleOptimizer,  with the following usage:\r\n```python\r\nloss = loss_fn()\r\nopt = tf.AdamOptimizer(learning_rate=...)\r\n\r\n# Choose a loss scale manager which decides how to pick the right loss scale\r\n# throughout the training process.\r\nloss_scale_manger = tf.contrib.mixed_precision.FixedLossScaleManager(5000)\r\n\r\n# Wraps the original optimizer in a LossScaleOptimizer.\r\nloss_scale_optimizer = LossScaleOptimizer(opt, loss_scale_manager)\r\n\r\n# Call minimize() on the loss scale optimizer.\r\ntrain_op = loss_scale_optimizer.minimize(loss)\r\n```\r\n\r\nThe optimizer `LossScaleOptimizer` is actually a wrapper of other optimizers.  However, the usage using wrapper is too limited to support all cases.  For example,\r\n    + if the users  are using `tf.gradients()` to compute the gradients rather than `optimizer.compute_gradients()`/`optimizer.minimize()`,  the wrapper is no longer in user. \r\n    + the `LossScaleOptimizer` wrapper may have compatibility problem with other optimizer wrappers, e.g. `SyncReplicasOptimizer`\r\n    + the current implementation in `LossScaleOptimizer` did not take the colocation parameter into consideration. \r\n\r\nInstead, we implement loss scaling using a decorator on `gradients()`, which is the atom function to compute the backprop gradients. The usage is as following:\r\n```python\r\nloss = loss_fn()\r\nopt = tf.AdamOptimizer(learning_rate=...)\r\n# constant loss scaling with loss_scale=64\r\nwith tf.mixed_precision_scope(automatic_loss_scaling=False, loss_scale=64):\r\n      grad = tf.gradients()  # if using tf.gradients\r\n      var_and_grad =opt.minimize(loss) # if using optimizer\r\n\r\n# automatic loss scaling\r\nwith tf.mixed_precision_scope(automatic_loss_scaling=True):\r\n      grad = tf.gradients()  # if using tf.gradients\r\n      var_and_grad =opt.minimize(loss) # if using optimizer\r\n```\r\n\r\nThe implementation can handle all of the above problems encounter by `LossScaleOptimizer` ."}