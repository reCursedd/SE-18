{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/83579058", "pull_request_review_id": 2352239, "id": 83579058, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDgzNTc5MDU4", "diff_hunk": "@@ -20,15 +20,185 @@\n from __future__ import division\n from __future__ import print_function\n \n+from tensorflow.python.ops import tensor_array_ops\n from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import variable_scope as vs\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import rnn\n \n+from tensorflow.contrib.layers import fully_connected\n+\n+from functools import partial\n \n __all__ = [\"rnn_decoder\",\n            \"rnn_decoder_attention\"]\n \n+\"\"\"Used to project encoder state in `rnn_decoder`\"\"\"\n+encoder_projection = partial(fully_connected, activation_fn=math_ops.tanh)\n \n-def rnn_decoder(*args, **kwargs):\n-  pass\n+\n+def rnn_decoder(cell, decoder_inputs, initial_state,\n+                sequence_length, decoder_fn,\n+                encoder_projection=encoder_projection,\n+                parallel_iterations=None, swap_memory=False,\n+                time_major=False, scope=None):\n+  \"\"\"RNN decoder for a sequence-to-sequence model specified by RNNCell 'cell'.\n+\n+  The 'rnn_decoder' is similar to the 'tf.python.ops.rnn.dynamic_rnn'. As the\n+  decoder does not make any assumptions of sequence length of the input or how\n+  many steps it can decode, since 'rnn_decoder' uses dynamic unrolling. This\n+  allows `decoder_inputs` to have [None] in the sequence length of the decoder\n+  inputs.\n+\n+  The parameter decoder_inputs is nessesary for both training and evaluation.\n+  During training it is feed at every timestep. During evaluation it is only\n+  feed at time==0, as the decoder needs the `start-of-sequence` symbol, known\n+  from Sutskever et al., 2014 https://arxiv.org/abs/1409.3215, at the\n+  beginning of decoding.\n+\n+  The parameter sequence length is nessesary as it determines how many\n+  timesteps to decode for each sample. TODO: Could make it optional for\n+  training.\n+\n+  Args:\n+    cell: An instance of RNNCell.\n+    inputs: The RNN inputs.\n+      If `time_major == False` (default), this must be a `Tensor` of shape:\n+        `[batch_size, max_time, ...]`.\n+      If `time_major == True`, this must be a `Tensor` of shape:\n+        `[max_time, batch_size, ...]`.\n+      The input to `cell` at each time step will be a `Tensor` with dimensions\n+      `[batch_size, ...]`.\n+    sequence_length: An int32/int64 vector sized `[batch_size]`.\n+    initial_state: An initial state for the RNN.\n+      Must be [batch_size, num_features], where num_features does not have to\n+      match the cell.state_size. As a projection is performed at the beginning\n+      of the decoding.\n+    decoder_fn: A function that takes a state and returns an embedding.\n+      The decoder function is closely related to `_extract_argmax_and_embed`.\n+      Here is an example of a `decoder_fn`:\n+      def decoder_fn(embeddings, weight, bias):\n+        def dec_fn(state):\n+          prev = tf.matmul(state, weight) + bias\n+          return tf.gather(embeddings, tf.argmax(prev, 1))\n+        return dec_fn\n+    encoder_projection: (optional) given that the encoder might have a\n+      different size than the decoder, we project the intial state as\n+      described in Bahdanau, 2014 (https://arxiv.org/abs/1409.0473).\n+      The optional `encoder_projection` is a\n+      `tf.contrib.layers.fully_connected` with\n+      `activation_fn=tf.python.ops.nn.tanh`.\n+    parallel_iterations: (Default: 32).  The number of iterations to run in\n+      parallel.  Those operations which do not have any temporal dependency\n+      and can be run in parallel, will be.  This parameter trades off\n+      time for space.  Values >> 1 use more memory but take less time,\n+      while smaller values use less memory but computations take longer.\n+    swap_memory: Transparently swap the tensors produced in forward inference\n+      but needed for back prop from GPU to CPU.  This allows training RNNs\n+      which would typically not fit on a single GPU, with very minimal (or no)\n+      performance penalty.\n+    time_major: The shape format of the `inputs` and `outputs` Tensors.\n+      If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n+      If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n+      Using `time_major = True` is a bit more efficient because it avoids\n+      transposes at the beginning and end of the RNN calculation.  However,\n+      most TensorFlow data is batch-major, so by default this function\n+      accepts input and emits output in batch-major form.\n+    scope: VariableScope for the created subgraph; defaults to \"RNN\".\n+  Returns:\n+    A pair (outputs, state) where:\n+      outputs: The RNN output `Tensor`.\n+        If time_major == False (default), this will be a `Tensor` shaped:\n+          `[batch_size, max_time, cell.output_size]`.\n+        If time_major == True, this will be a `Tensor` shaped:\n+          `[max_time, batch_size, cell.output_size]`.\n+      state: The final state.  If `cell.state_size` is an int, this\n+        will be shaped `[batch_size, cell.state_size]`.  If it is a\n+        `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n+  Raises:\n+    TypeError: If `cell` is not an instance of RNNCell.\n+  \"\"\"\n+  with vs.variable_scope(scope or \"decoder\") as varscope:\n+    # Project initial_state as described in Bahdanau et al. 2014\n+    # https://arxiv.org/abs/1409.0473\n+    state = encoder_projection(initial_state, cell.output_size)\n+    # Setup of RNN (dimensions, sizes, length, initial state, dtype)\n+    # Setup dtype\n+    dtype = state.dtype\n+    if not time_major:\n+      # [batch, seq, features] -> [seq, batch, features]\n+      decoder_inputs = array_ops.transpose(decoder_inputs, perm=[1, 0, 2])\n+    # Get data input information\n+    batch_size = array_ops.shape(decoder_inputs)[1]\n+    decoder_input_depth = int(decoder_inputs.get_shape()[2])\n+    # Setup decoder inputs as TensorArray\n+    decoder_inputs_ta = tensor_array_ops.TensorArray(dtype, size=0,\n+                                                     dynamic_size=True)\n+    decoder_inputs_ta = decoder_inputs_ta.unpack(decoder_inputs)\n+\n+    # Define RNN: loop function for training.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_train(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None\n+      finished = math_ops.reduce_all(elements_finished)\n+      # Next input must return zero state for last element explanation below\n+      # https://github.com/tensorflow/tensorflow/issues/4519\n+      next_input = control_flow_ops.cond(\n+          finished,\n+          lambda: array_ops.zeros([batch_size, decoder_input_depth],\n+                                  dtype=dtype),\n+          lambda: decoder_inputs_ta.read(time))\n+      next_loop_state = None\n+      return (elements_finished, next_input, next_cell_state,\n+              emit_output, next_loop_state)\n+\n+    # Define RNN: loop function for evaluation.\n+    # This will run in the while_loop of 'raw_rnn'\n+    def loop_fn_eval(time, cell_output, cell_state, loop_state):\n+      emit_output = cell_output\n+      if cell_output is None:\n+        next_cell_state = state # use projection of prev encoder state\n+      else:\n+        next_cell_state = cell_state\n+      elements_finished = (time >= sequence_length) #TODO handle seq_len=None\n+      finished = math_ops.reduce_all(elements_finished)\n+      # Next input must return zero state for last element explanation below\n+      # https://github.com/tensorflow/tensorflow/issues/4519\n+      next_input = control_flow_ops.cond(\n+          finished,\n+          lambda: array_ops.zeros([batch_size, decoder_input_depth],\n+                                  dtype=dtype),\n+          lambda: control_flow_ops.cond(math_ops.greater(time, 0),\n+              lambda: decoder_fn(next_cell_state), # Gather max prediction.\n+              lambda: decoder_inputs_ta.read(0))) # Read <EOS> tag\n+      next_loop_state = None\n+      return (elements_finished, next_input, next_cell_state,\n+              emit_output, next_loop_state)\n+\n+    # Run raw_rnn function\n+    outputs_ta_train, _, _ = \\", "path": "tensorflow/contrib/seq2seq/python/ops/layers.py", "position": null, "original_position": 168, "commit_id": "cf9f5d32c2d618e7dd98fe222b92aec1a3cf9dd4", "original_commit_id": "e148d284d3d8bd8670186e917867962f599938ae", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "body": "don't use backslash continuations:\n\noutputs_ta_train, _, _ = rnn.raw_rnn(\n   ...)\n", "created_at": "2016-10-17T05:19:57Z", "updated_at": "2016-11-30T17:47:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4686#discussion_r83579058", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4686", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/83579058"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4686#discussion_r83579058"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4686"}}, "body_html": "<p>don't use backslash continuations:</p>\n<p>outputs_ta_train, _, _ = rnn.raw_rnn(<br>\n...)</p>", "body_text": "don't use backslash continuations:\noutputs_ta_train, _, _ = rnn.raw_rnn(\n...)"}