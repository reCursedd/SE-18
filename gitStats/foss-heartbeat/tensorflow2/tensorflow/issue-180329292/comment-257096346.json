{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/257096346", "html_url": "https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-257096346", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4686", "id": 257096346, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzA5NjM0Ng==", "user": {"login": "alrojo", "id": 12167999, "node_id": "MDQ6VXNlcjEyMTY3OTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/12167999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alrojo", "html_url": "https://github.com/alrojo", "followers_url": "https://api.github.com/users/alrojo/followers", "following_url": "https://api.github.com/users/alrojo/following{/other_user}", "gists_url": "https://api.github.com/users/alrojo/gists{/gist_id}", "starred_url": "https://api.github.com/users/alrojo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alrojo/subscriptions", "organizations_url": "https://api.github.com/users/alrojo/orgs", "repos_url": "https://api.github.com/users/alrojo/repos", "events_url": "https://api.github.com/users/alrojo/events{/privacy}", "received_events_url": "https://api.github.com/users/alrojo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-29T15:01:31Z", "updated_at": "2016-10-29T15:25:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Nice, great work! I like that you are moving the <code>encoder_state</code> to the <code>decoder_fn</code> as future seq2seq models might have different initialization approaches.</p>\n<p>You might want to consider having the <code>decoder_fn</code> as classes to allow user-specific parameters to be passed.<br>\nE.g. the code provided in <a href=\"https://codeshare.io/XBTzz\" rel=\"nofollow\">codeshare link</a> would not work if you had not defined <code>encoder_state</code> somewhere above.<br>\nBelow is <code>decoder_fn_train</code> and <code>decoder_fn_eval</code>, we should consider also providing an abstract class like the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L87\">RNNCell</a>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">decoder_fn_train</span>():\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">encoder_state</span>):\n    <span class=\"pl-c1\">self</span>.encoder_state <span class=\"pl-k\">=</span> encoder_state\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">cell_state</span>, <span class=\"pl-smi\">cell_output</span>, <span class=\"pl-smi\">context_state</span>):\n    <span class=\"pl-k\">if</span> cell_state <span class=\"pl-k\">==</span> <span class=\"pl-c1\">None</span>: <span class=\"pl-c\"><span class=\"pl-c\">#</span> first call, return encoder_state</span>\n      <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">self</span>.encoder_state, <span class=\"pl-c1\">None</span>, cell_output, context_state)\n    <span class=\"pl-k\">else</span>:\n      <span class=\"pl-k\">return</span> (<span class=\"pl-c1\">None</span>, cell_state, <span class=\"pl-c1\">None</span>, cell_output, context_state)\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">decoder_fn_eval</span>():\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">encoder_state</span>, <span class=\"pl-smi\">embeddings</span>, <span class=\"pl-smi\">sos_id</span>, <span class=\"pl-smi\">eos_id</span>,\n               <span class=\"pl-smi\">output_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">dtype</span><span class=\"pl-k\">=</span>tf.int32):\n    <span class=\"pl-c1\">self</span>.encoder_state <span class=\"pl-k\">=</span> encoder_state\n    <span class=\"pl-c1\">self</span>.embeddings <span class=\"pl-k\">=</span> embeddings\n    <span class=\"pl-c1\">self</span>.dtype <span class=\"pl-k\">=</span> dtype\n    <span class=\"pl-c1\">self</span>.output_fn <span class=\"pl-k\">=</span> output_fn\n    <span class=\"pl-c1\">self</span>.sos_id <span class=\"pl-k\">=</span> tf.Variable(sos_id, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dtype)\n    <span class=\"pl-c1\">self</span>.eos_id <span class=\"pl-k\">=</span> tf.Variable(eos_id, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dtype)\n\n    <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> array_ops.shape(<span class=\"pl-c1\">self</span>.encoder_state)[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> If you have an output projection on your cell, set output_fn to None</span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.output_fn <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n      <span class=\"pl-c1\">self</span>.output_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: x\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">cell_state</span>, <span class=\"pl-smi\">cell_output</span>, <span class=\"pl-smi\">context_state</span>):\n\n    <span class=\"pl-k\">if</span> cell_output <span class=\"pl-k\">==</span> <span class=\"pl-c1\">None</span>:\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> invariant that this is time == 0</span>\n      next_input_id <span class=\"pl-k\">=</span> tf.ones([<span class=\"pl-c1\">self</span>.batch_size], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.dtype) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.sos_id\n      done <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">self</span>.batch_size,], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.bool)\n      cell_state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.encoder_state\n    <span class=\"pl-k\">else</span>:\n      next_input_id <span class=\"pl-k\">=</span> tf.argmax(<span class=\"pl-c1\">self</span>.output_fn(cell_output), <span class=\"pl-c1\">1</span>)\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> set to None as I provide sequence_length when debugging</span>\n      done <span class=\"pl-k\">=</span> <span class=\"pl-c1\">None</span><span class=\"pl-c\"><span class=\"pl-c\">#</span>tf.equal(next_input_id, self.eos_id)</span>\n    next_input <span class=\"pl-k\">=</span> tf.gather(<span class=\"pl-c1\">self</span>.embeddings, next_input_id)\n\n    <span class=\"pl-k\">return</span> (done, cell_state, next_input, cell_output, context_state)</pre></div>\n<p>One note:<br>\nAs pointed in earlier post regarding allowing <code>sequence_lengths=None</code>:</p>\n<blockquote>\n<p>I don't think we can have sequence_length be None, this could cause the decoder to get stuck and eventually run OOM. Instead I have made it possible to just supply an integer value, representing the \"maximal_allowed\" amount of steps to decoder.</p>\n</blockquote>\n<p>By allowing <code>sequence_lengths = None</code> we rely on the <code>decoder_fn_eval</code> to eventually return <code>True</code> for all sequences.<br>\nImagine validating with an untrained network on a word-to-word model with 80k classes.<br>\nThen you would have a 1 in 80k chance of \"randomly\" reaching the <code>eos</code> symbol.</p>\n<p>EDIT:<br>\nThe alternative \"max_int\" value I refered in previous post to is used <a href=\"https://github.com/tensorflow/tensorflow/pull/4686/commits/15eba5e1160ab34d2940a916bc9de8f8502317a6#diff-193a5806e6330e98e72ce0499036b9a4R261\">here</a></p>\n<p>From here you can provide a logical \"or\" on the time&gt;=max_seq_len to allow earlier stopping.</p>\n<p>EDIT:<br>\nIs the <code>context_state</code> tuple only used for infering <code>batch_size</code> or will it be needed for later for beam search?</p>", "body_text": "Nice, great work! I like that you are moving the encoder_state to the decoder_fn as future seq2seq models might have different initialization approaches.\nYou might want to consider having the decoder_fn as classes to allow user-specific parameters to be passed.\nE.g. the code provided in codeshare link would not work if you had not defined encoder_state somewhere above.\nBelow is decoder_fn_train and decoder_fn_eval, we should consider also providing an abstract class like the RNNCell.\nclass decoder_fn_train():\n\n  def __init__(self, encoder_state):\n    self.encoder_state = encoder_state\n\n  def __call__(self, cell_state, cell_output, context_state):\n    if cell_state == None: # first call, return encoder_state\n      return (None, self.encoder_state, None, cell_output, context_state)\n    else:\n      return (None, cell_state, None, cell_output, context_state)\n\nclass decoder_fn_eval():\n\n  def __init__(self, encoder_state, embeddings, sos_id, eos_id,\n               output_fn=None, dtype=tf.int32):\n    self.encoder_state = encoder_state\n    self.embeddings = embeddings\n    self.dtype = dtype\n    self.output_fn = output_fn\n    self.sos_id = tf.Variable(sos_id, dtype=self.dtype)\n    self.eos_id = tf.Variable(eos_id, dtype=self.dtype)\n\n    self.batch_size = array_ops.shape(self.encoder_state)[0]\n    # If you have an output projection on your cell, set output_fn to None\n    if self.output_fn is None:\n      self.output_fn = lambda x: x\n\n  def __call__(self, cell_state, cell_output, context_state):\n\n    if cell_output == None:\n      # invariant that this is time == 0\n      next_input_id = tf.ones([self.batch_size], dtype=self.dtype) * self.sos_id\n      done = tf.zeros([self.batch_size,], dtype=tf.bool)\n      cell_state = self.encoder_state\n    else:\n      next_input_id = tf.argmax(self.output_fn(cell_output), 1)\n      # set to None as I provide sequence_length when debugging\n      done = None#tf.equal(next_input_id, self.eos_id)\n    next_input = tf.gather(self.embeddings, next_input_id)\n\n    return (done, cell_state, next_input, cell_output, context_state)\nOne note:\nAs pointed in earlier post regarding allowing sequence_lengths=None:\n\nI don't think we can have sequence_length be None, this could cause the decoder to get stuck and eventually run OOM. Instead I have made it possible to just supply an integer value, representing the \"maximal_allowed\" amount of steps to decoder.\n\nBy allowing sequence_lengths = None we rely on the decoder_fn_eval to eventually return True for all sequences.\nImagine validating with an untrained network on a word-to-word model with 80k classes.\nThen you would have a 1 in 80k chance of \"randomly\" reaching the eos symbol.\nEDIT:\nThe alternative \"max_int\" value I refered in previous post to is used here\nFrom here you can provide a logical \"or\" on the time>=max_seq_len to allow earlier stopping.\nEDIT:\nIs the context_state tuple only used for infering batch_size or will it be needed for later for beam search?", "body": "Nice, great work! I like that you are moving the `encoder_state` to the `decoder_fn` as future seq2seq models might have different initialization approaches.\n\nYou might want to consider having the `decoder_fn` as classes to allow user-specific parameters to be passed.\nE.g. the code provided in [codeshare link](https://codeshare.io/XBTzz) would not work if you had not defined `encoder_state` somewhere above.\nBelow is `decoder_fn_train` and `decoder_fn_eval`, we should consider also providing an abstract class like the [RNNCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L87).\n\n``` python\nclass decoder_fn_train():\n\n  def __init__(self, encoder_state):\n    self.encoder_state = encoder_state\n\n  def __call__(self, cell_state, cell_output, context_state):\n    if cell_state == None: # first call, return encoder_state\n      return (None, self.encoder_state, None, cell_output, context_state)\n    else:\n      return (None, cell_state, None, cell_output, context_state)\n\nclass decoder_fn_eval():\n\n  def __init__(self, encoder_state, embeddings, sos_id, eos_id,\n               output_fn=None, dtype=tf.int32):\n    self.encoder_state = encoder_state\n    self.embeddings = embeddings\n    self.dtype = dtype\n    self.output_fn = output_fn\n    self.sos_id = tf.Variable(sos_id, dtype=self.dtype)\n    self.eos_id = tf.Variable(eos_id, dtype=self.dtype)\n\n    self.batch_size = array_ops.shape(self.encoder_state)[0]\n    # If you have an output projection on your cell, set output_fn to None\n    if self.output_fn is None:\n      self.output_fn = lambda x: x\n\n  def __call__(self, cell_state, cell_output, context_state):\n\n    if cell_output == None:\n      # invariant that this is time == 0\n      next_input_id = tf.ones([self.batch_size], dtype=self.dtype) * self.sos_id\n      done = tf.zeros([self.batch_size,], dtype=tf.bool)\n      cell_state = self.encoder_state\n    else:\n      next_input_id = tf.argmax(self.output_fn(cell_output), 1)\n      # set to None as I provide sequence_length when debugging\n      done = None#tf.equal(next_input_id, self.eos_id)\n    next_input = tf.gather(self.embeddings, next_input_id)\n\n    return (done, cell_state, next_input, cell_output, context_state)\n```\n\nOne note:\nAs pointed in earlier post regarding allowing `sequence_lengths=None`:\n\n> I don't think we can have sequence_length be None, this could cause the decoder to get stuck and eventually run OOM. Instead I have made it possible to just supply an integer value, representing the \"maximal_allowed\" amount of steps to decoder.\n\nBy allowing `sequence_lengths = None` we rely on the `decoder_fn_eval` to eventually return `True` for all sequences.\nImagine validating with an untrained network on a word-to-word model with 80k classes.\nThen you would have a 1 in 80k chance of \"randomly\" reaching the `eos` symbol.\n\nEDIT:\nThe alternative \"max_int\" value I refered in previous post to is used [here](https://github.com/tensorflow/tensorflow/pull/4686/commits/15eba5e1160ab34d2940a916bc9de8f8502317a6#diff-193a5806e6330e98e72ce0499036b9a4R261)\n\nFrom here you can provide a logical \"or\" on the time>=max_seq_len to allow earlier stopping.\n\nEDIT:\nIs the `context_state` tuple only used for infering `batch_size` or will it be needed for later for beam search?\n"}