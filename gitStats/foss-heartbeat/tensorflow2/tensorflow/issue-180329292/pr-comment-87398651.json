{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/87398651", "pull_request_review_id": 8012463, "id": 87398651, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg3Mzk4NjUx", "diff_hunk": "@@ -0,0 +1,114 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"Tests for contrib.seq2seq.python.seq2seq.layers_ops.\"\"\"\n+# pylint: disable=unused-import,g-bad-import-order\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+# pylint: enable=unused-import\n+\n+import tensorflow as tf\n+\n+\n+class Seq2SeqTest(tf.test.TestCase):\n+\n+  # test a default call of rnn_decoder\n+  def test_rnn_decoder(self):\n+    pass\n+\n+  # test default call with time_major=True\n+  def test_dynamic_rnn_decoder_time_major(self):\n+    with self.test_session() as sess:\n+      with tf.variable_scope(\"root\", initializer=tf.constant_initializer(0.5)) as varscope:\n+        # Define inputs/outputs to model\n+        batch_size = 2\n+        input_size = 3\n+        decoder_input_size = 4\n+        encoder_size = 8\n+        decoder_size = encoder_size\n+        input_sequence_length = 8\n+        decoder_sequence_length = 9\n+        output_size = 20\n+        start_of_sequence_id = end_of_sequence_id = 3\n+        decoder_embeddings = tf.get_variable('decoder_embeddings',\n+            [output_size, decoder_input_size],\n+            initializer=tf.random_normal_initializer(stddev=0.1))\n+        inputs = tf.constant(0.5, shape=[input_sequence_length, batch_size,\n+                                         input_size])\n+        decoder_inputs = tf.constant(0.4, shape=[decoder_sequence_length,\n+                                                 batch_size,\n+                                                 decoder_input_size])\n+        decoder_length = tf.constant(decoder_sequence_length, dtype=tf.int32,\n+                                     shape=[batch_size,])\n+\n+        # Define model\n+        encoder_outputs, encoder_state = tf.nn.dynamic_rnn(\n+            cell=tf.nn.rnn_cell.GRUCell(encoder_size), inputs=inputs,\n+            dtype=tf.float32, time_major=True, scope=\"rnn\")\n+        decoder_cell = tf.nn.rnn_cell.GRUCell(decoder_size)\n+        decoder_cell = tf.nn.rnn_cell.OutputProjectionWrapper(decoder_cell,\n+                                                              output_size)\n+\n+        # Train decoder\n+        decoder_fn_train = tf.contrib.seq2seq.simple_decoder_fn_train(\n+            encoder_state=encoder_state)\n+        decoder_outputs_train, decoder_state_train = (\n+          tf.contrib.seq2seq.dynamic_rnn_decoder(\n+              cell=decoder_cell,\n+              decoder_fn=decoder_fn_train,\n+              inputs=decoder_inputs,\n+              sequence_lengths=decoder_length,\n+              time_major=True))\n+\n+        # Inference decoder\n+        varscope.reuse_variables()\n+        decoder_fn_inference = tf.contrib.seq2seq.simple_decoder_fn_inference(\n+            encoder_state=encoder_state,\n+            embeddings=decoder_embeddings,\n+            start_of_sequence_id=start_of_sequence_id,\n+            end_of_sequence_id=end_of_sequence_id,\n+            maximum_length=decoder_sequence_length,\n+            dtype=tf.int32)\n+        decoder_outputs_inference, decoder_state_inference = (\n+          tf.contrib.seq2seq.dynamic_rnn_decoder(\n+              cell=decoder_cell,\n+              decoder_fn=decoder_fn_inference,\n+              time_major=True))\n+\n+        # Run model\n+        tf.initialize_all_variables().run()\n+        decoder_outputs_train_res, decoder_state_train_res = sess.run(\n+            [decoder_outputs_train, decoder_state_train])\n+        decoder_outputs_inference_res, decoder_state_inference_res = sess.run(\n+            [decoder_outputs_inference, decoder_state_inference])\n+\n+        # Assert outputs\n+        self.assertEqual((decoder_sequence_length, batch_size, output_size),\n+                         decoder_outputs_train_res.shape)\n+        self.assertEqual((batch_size, decoder_size),\n+                         decoder_outputs_inference_res.shape[0:2])\n+        self.assertEqual((batch_size, decoder_size),\n+                         decoder_state_train_res.shape)\n+        self.assertEqual((batch_size, decoder_size),\n+                         decoder_state_inference_res.shape)\n+        # The dynamic decoder might end earlier than `maximal_length`\n+        # under inference\n+        true_value = (decoder_sequence_length>=", "path": "tensorflow/contrib/seq2seq/python/kernel_tests/seq2seq_test.py", "position": null, "original_position": 109, "commit_id": "cf9f5d32c2d618e7dd98fe222b92aec1a3cf9dd4", "original_commit_id": "59304d5cf967a584f7fccadc6cc68266343e79aa", "user": {"login": "alrojo", "id": 12167999, "node_id": "MDQ6VXNlcjEyMTY3OTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/12167999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alrojo", "html_url": "https://github.com/alrojo", "followers_url": "https://api.github.com/users/alrojo/followers", "following_url": "https://api.github.com/users/alrojo/following{/other_user}", "gists_url": "https://api.github.com/users/alrojo/gists{/gist_id}", "starred_url": "https://api.github.com/users/alrojo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alrojo/subscriptions", "organizations_url": "https://api.github.com/users/alrojo/orgs", "repos_url": "https://api.github.com/users/alrojo/repos", "events_url": "https://api.github.com/users/alrojo/events{/privacy}", "received_events_url": "https://api.github.com/users/alrojo/received_events", "type": "User", "site_admin": false}, "body": "Okay, I have been working some on this, here are my thoughts.\n\nI assume you want something in the regards of `decoder_outputs_inference == decoder_outputs_train`, given they have access to the same embedding.\n\nThis is really tricky because of the following reasons:\n\n**The `simple_decoder_fn_inference` picks random embeddings at every iteration**\n\nWe can solve this by taking the decoder_inference's output, argmax it and pick the same embeddings for the to match `decoder_inputs` (training decoder).\n\n**The `simple_decoder_fn_inference` randomly stops whenever it finds a EOS symbol, and it doesn't tell you when it stopped**\n\nThe problem here is that whenever it finishes (`True` in the `done` vector), `raw_rnn` just starts copying states.\nGiven that our current solution does not return the `done` vector to tell when it stopped, we can't assess when it started copying states. We could do some hack analyzing the `decoder_output_inference` looking for when it started copying, but I don't think this is the optimal solution.\n\nPerhaps we should consider returning the `done` vector from the `dynamic_rnn_decoder`, but not just a boolean vector, a vector with information on when every sequence in the batch stopped. So like a reverse engineered `sequence_lengths`.\nAfterwards we would use this as input for the training decoder, which should allow this test.\n\nOr, simply leave this test out ..?\n", "created_at": "2016-11-10T14:02:58Z", "updated_at": "2016-11-30T17:47:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4686#discussion_r87398651", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4686", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/87398651"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4686#discussion_r87398651"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4686"}}, "body_html": "<p>Okay, I have been working some on this, here are my thoughts.</p>\n<p>I assume you want something in the regards of <code>decoder_outputs_inference == decoder_outputs_train</code>, given they have access to the same embedding.</p>\n<p>This is really tricky because of the following reasons:</p>\n<p><strong>The <code>simple_decoder_fn_inference</code> picks random embeddings at every iteration</strong></p>\n<p>We can solve this by taking the decoder_inference's output, argmax it and pick the same embeddings for the to match <code>decoder_inputs</code> (training decoder).</p>\n<p><strong>The <code>simple_decoder_fn_inference</code> randomly stops whenever it finds a EOS symbol, and it doesn't tell you when it stopped</strong></p>\n<p>The problem here is that whenever it finishes (<code>True</code> in the <code>done</code> vector), <code>raw_rnn</code> just starts copying states.<br>\nGiven that our current solution does not return the <code>done</code> vector to tell when it stopped, we can't assess when it started copying states. We could do some hack analyzing the <code>decoder_output_inference</code> looking for when it started copying, but I don't think this is the optimal solution.</p>\n<p>Perhaps we should consider returning the <code>done</code> vector from the <code>dynamic_rnn_decoder</code>, but not just a boolean vector, a vector with information on when every sequence in the batch stopped. So like a reverse engineered <code>sequence_lengths</code>.<br>\nAfterwards we would use this as input for the training decoder, which should allow this test.</p>\n<p>Or, simply leave this test out ..?</p>", "body_text": "Okay, I have been working some on this, here are my thoughts.\nI assume you want something in the regards of decoder_outputs_inference == decoder_outputs_train, given they have access to the same embedding.\nThis is really tricky because of the following reasons:\nThe simple_decoder_fn_inference picks random embeddings at every iteration\nWe can solve this by taking the decoder_inference's output, argmax it and pick the same embeddings for the to match decoder_inputs (training decoder).\nThe simple_decoder_fn_inference randomly stops whenever it finds a EOS symbol, and it doesn't tell you when it stopped\nThe problem here is that whenever it finishes (True in the done vector), raw_rnn just starts copying states.\nGiven that our current solution does not return the done vector to tell when it stopped, we can't assess when it started copying states. We could do some hack analyzing the decoder_output_inference looking for when it started copying, but I don't think this is the optimal solution.\nPerhaps we should consider returning the done vector from the dynamic_rnn_decoder, but not just a boolean vector, a vector with information on when every sequence in the batch stopped. So like a reverse engineered sequence_lengths.\nAfterwards we would use this as input for the training decoder, which should allow this test.\nOr, simply leave this test out ..?", "in_reply_to_id": 86899685}