{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/88784199", "pull_request_review_id": 9349615, "id": 88784199, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDg4Nzg0MTk5", "diff_hunk": "@@ -0,0 +1,250 @@\n+# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"Seq2seq loss operations for use in neural networks.\n+\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import control_flow_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import rnn_cell\n+from tensorflow.python.util import nest\n+\n+__all__ = [\"simple_decoder_fn_train\",\n+           \"simple_decoder_fn_inference\"]\n+\n+def simple_decoder_fn_train(encoder_state, name=None):\n+  \"\"\" Simple decoder function for a sequence-to-sequence model used in the\n+  `dynamic_rnn_decoder`.\n+\n+  The `simple_decoder_fn_train` is a simple training function for a\n+  sequence-to-sequence model. It should be used when `dynamic_rnn_decoder` is\n+  in the training mode.\n+\n+  The `simple_decoder_fn_train` is called with a set of the user arguments and\n+  returns the `decoder_fn`, which can be passed to the `dynamic_rnn_decoder`,\n+  such that\n+\n+  ```\n+  dynamic_fn_train = simple_decoder_fn_train(encoder_state)\n+  outputs_train, state_train = dynamic_rnn_decoder(\n+      decoder_fn=dynamic_fn_train, ...)\n+  ```\n+\n+  Further usage can be found in the `kernel_tests/seq2seq_test.py`.\n+\n+  Args:\n+    encoder_state: The encoded state to initialize the `dynamic_rnn_decoder`.\n+    name: (default: `None`) NameScope for the decoder function;\n+      defaults to \"simple_decoder_fn_train\"\n+\n+  Returns:\n+    A decoder function with the required interface of `dynamic_rnn_decoder`\n+    intended for training.\n+  \"\"\"\n+  with ops.name_scope(name, \"simple_decoder_fn_train\", [encoder_state]):\n+    pass\n+\n+  def decoder_fn(time, cell_state, cell_input, cell_output, context_state):\n+    \"\"\" Decoder function used in the `dynamic_rnn_decoder` with the purpose of\n+    training.\n+\n+    Args:\n+      time: positive integer constant reflecting the current timestep.\n+      cell_state: state of RNNCell.\n+      cell_input: input provided by `dynamic_rnn_decoder`.\n+      cell_output: output of RNNCell.\n+      context_state: context state provided by `dynamic_rnn_decoder`.\n+\n+    Returns:\n+      A tuple (done, next state, next input, emit output, next context state)\n+      where:\n+\n+      done: `None`, which is used by the `dynamic_rnn_decoder` to indicate\n+      that `sequence_lengths` in `dynamic_rnn_decoder` should be used.\n+\n+      next state: `cell_state`, this decoder function does not modify the\n+      given state.\n+\n+      next input: `cell_input`, this decoder function does not modify the\n+      given input. The input could be modified when applying e.g. attention.\n+\n+      emit output: `cell_output`, this decoder function does not modify the\n+      given output.\n+\n+      next context state: `context_state`, this decoder function does not\n+      modify the given context state. The context state could be modified when\n+      applying e.g. beam search.\n+  \"\"\"\n+    with ops.name_scope(name, \"simple_decoder_fn_train\",\n+                        [time, cell_state, cell_input, cell_output,\n+                         context_state]):\n+      if cell_state is None:  # first call, return encoder_state\n+        return (None, encoder_state, cell_input, cell_output, context_state)\n+      else:\n+        return (None, cell_state, cell_input, cell_output, context_state)\n+  return decoder_fn\n+\n+\n+def simple_decoder_fn_inference(output_fn, encoder_state, embeddings,\n+                                start_of_sequence_id, end_of_sequence_id,\n+                                maximum_length, num_decoder_symbols,\n+                                dtype=dtypes.int32, name=None):\n+  \"\"\" Simple decoder function for a sequence-to-sequence model used in the\n+  `dynamic_rnn_decoder`.\n+\n+  The `simple_decoder_fn_inference` is a simple inference function for a\n+  sequence-to-sequence model. It should be used when `dynamic_rnn_decoder` is\n+  in the inference mode.\n+\n+  The `simple_decoder_fn_inference` is called with a set of the user arguments\n+  and returns the `decoder_fn`, which can be passed to the\n+  `dynamic_rnn_decoder`, such that\n+\n+  ```\n+  dynamic_fn_inference = simple_decoder_fn_inference(...)\n+  outputs_inference, state_inference = dynamic_rnn_decoder(\n+      decoder_fn=dynamic_fn_inference, ...)\n+  ```\n+\n+  Further usage can be found in the `kernel_tests/seq2seq_test.py`.\n+\n+  Args:\n+    output_fn: An output function to project your `cell_output` onto class\n+    logits.\n+\n+    An example of an output function;\n+\n+    ```\n+      tf.variable_scope(\"decoder\") as varscope\n+        output_fn = lambda x: layers.linear(x, num_decoder_symbols,\n+                                            scope=varscope)\n+\n+        outputs_train, state_train = seq2seq.dynamic_rnn_decoder(...)\n+        logits_train = output_fn(outputs_train)\n+\n+        varscope.reuse_variables()\n+        logits_inference, state_inference = seq2seq.dynamic_rnn_decoder(\n+            output_fn=output_fn, ...)\n+    ```\n+\n+    If `None` is supplied it will act as an identity function, which\n+    might be wanted when using the RNNCell `OutputProjectionWrapper`.\n+\n+    encoder_state: The encoded state to initialize the `dynamic_rnn_decoder`.\n+    embeddings: The embeddings matrix used for the decoder sized\n+    `[num_decoder_symbols, embedding_size]`.\n+    start_of_sequence_id: The start of sequence ID in the decoder embeddings.\n+    end_of_sequence_id: The end of sequence ID in the decoder embeddings.\n+    maximum_length: The maximum allowed of time steps to decode.\n+    num_decoder_symbols: The number of classes to decode at each time step.\n+    dtype: (default: `dtypes.int32`) The default data type to use when\n+    handling integer objects.\n+    name: (default: `None`) NameScope for the decoder function;\n+      defaults to \"simple_decoder_fn_inference\"\n+\n+  Returns:\n+    A decoder function with the required interface of `dynamic_rnn_decoder`\n+    intended for inference.\n+  \"\"\"\n+  with ops.name_scope(name, \"simple_decoder_fn_inference\",\n+                      [output_fn, encoder_state, embeddings,\n+                       start_of_sequence_id, end_of_sequence_id,\n+                       maximum_length, num_decoder_symbols, dtype]):\n+    start_of_sequence_id = ops.convert_to_tensor(start_of_sequence_id, dtype)\n+    end_of_sequence_id = ops.convert_to_tensor(end_of_sequence_id, dtype)\n+    maximum_length = ops.convert_to_tensor(maximum_length, dtype)\n+    num_decoder_symbols = ops.convert_to_tensor(num_decoder_symbols, dtype)\n+    encoder_info = nest.flatten(encoder_state)[0]\n+    batch_size = encoder_info.get_shape()[0].value\n+    if output_fn is None:\n+      output_fn = lambda x: x\n+    if batch_size is None:\n+      batch_size = array_ops.shape(encoder_info)[0]\n+\n+  def decoder_fn(time, cell_state, cell_input, cell_output, context_state):\n+    \"\"\" Decoder function used in the `dynamic_rnn_decoder` with the purpose of\n+    inference.\n+\n+    The main difference between this decoder function and the `decoder_fn` in\n+    `simple_decoder_fn_train` is how `next_cell_input` is calculated. In this\n+    decoder function we calculate the next input by applying an argmax across\n+    the feature dimension of the output from the decoder. This approach is\n+    taken from popular neural machine translation litterature such as:\n+    Bahhdanau et al., 2014 (https://arxiv.org/abs/1409.0473) and Sutskever\n+    et al., 2014 (https://arxiv.org/abs/1409.3215).\n+", "path": "tensorflow/contrib/seq2seq/python/ops/decoder_fn.py", "position": null, "original_position": 194, "commit_id": "cf9f5d32c2d618e7dd98fe222b92aec1a3cf9dd4", "original_commit_id": "b095937697db49cf2cad52a129d569e714a394f8", "user": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "body": "I think you should replace the last sentence by saying: This is a greedy-search approach.\n(Bahdanau et al., 2014) & (Sutskever et al., 2014) use beam-search instead.\n", "created_at": "2016-11-19T17:57:46Z", "updated_at": "2016-11-30T17:47:01Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/4686#discussion_r88784199", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4686", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/88784199"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/4686#discussion_r88784199"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4686"}}, "body_html": "<p>I think you should replace the last sentence by saying: This is a greedy-search approach.<br>\n(Bahdanau et al., 2014) &amp; (Sutskever et al., 2014) use beam-search instead.</p>", "body_text": "I think you should replace the last sentence by saying: This is a greedy-search approach.\n(Bahdanau et al., 2014) & (Sutskever et al., 2014) use beam-search instead."}