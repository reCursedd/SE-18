{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/254404232", "html_url": "https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-254404232", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4686", "id": 254404232, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NDQwNDIzMg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-18T04:32:12Z", "updated_at": "2016-10-18T04:32:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Lukasz and I discussed the existing API.  Here's how we think it could be improved.  First, remove the embedding stuff and make <code>inputs</code> optional.  <code>sequence_length</code> may be provided iff <code>inputs</code> is.  If <code>inputs</code> is provided, decoding is allowed to run up to the number of frames in <code>inputs</code> and no longer than that (regarldess of what <code>decoder_fn</code> says).  If <code>inputs</code> is not provided, <code>decoder_fn</code> must say when to stop for each minibatch entry.</p>\n<p>There should be separate <code>decoder_fn</code> impls for training and for inference; see below for details on that:</p>\n<ol>\n<li>change the decoder_fn to have the following signature (up to you what you call the arguments and the outputs, of course; these are just placeholders)</li>\n</ol>\n<pre><code>decoder_fn(time : scalar int,\n  cur_state : tensor_tuple [batch_size, ...],\n  cur_output : tensor_tuple [batch_size, ...],\n  cur_input tensor_tuple [batch_size, ...],\n  decoder_state : tensor_tuple [anything])\n-&gt; \n(done : bool vec [batch_size],\n next_state : tensor_tuple [batch_size, ...],\n next_input : tensor_tuple [batch_size, ...],\n next_decoder_state : tensor_tuple[anything])\n</code></pre>\n<p>here tensor_tuple means a (possibly nested) tuple of tensors or single tensor; and, e.g., <code>next_state</code> must have the same nested structure and shape as <code>cur_state</code> (same for <code>next_decoder_state</code>); and <code>next_input</code> should have consistent structure and shape across all calls.</p>\n<p>here is how decoder_fn gets called:</p>\n<pre><code>training time (inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **Not None** (first input)\n  decoder_state: None\ntraining time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: not None (first input)\n  decoder_state: previous emitted decoder state\n\ninference time (**NO** inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **None**\n  decoder_state: None\ninference time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: **None**\n  decoder_state: previous emitted decoder state\n</code></pre>\n<p>here's are the two simplest decoder_fns (one for training, one for inference):</p>\n<pre><code>simplest case, training:\n  1. if cur_state: None, cur_output: None, cur_input: not None, decoder_state: None\n   return\n     next_state: your initial state\n     next_input: cur_input\n     done = [False] * batch_size\n     next_decoder_state: None\n  2. if cur_state: not None, cur_output: not None, cur_input: not None, decoder_state: None\n    return\n      next_state = cur_state\n      next_input: cur_input\n      done: [False] * batch_size\n      next_decoder_state: None\n</code></pre>\n<pre><code>simplest case, inference:\n  1. if cur_state: None, cur_output: None, cur_input: None, decoder_state: None\n    return\n       next_state: your initial state\n       next input: your go symbol input\n       done: [False] * batch_size\n       next_decoder_state: None\n   2. if cur_state: not None, cur_output: not None, cur_input: None, decoder_state: None\n     return\n       next_state: cur_state\n       next_input: your choice, e.g. embed of argmax of cur_output\n       done: if argmax of cur output is done symbol\n       next_decoder_state: None\n</code></pre>\n<p>unit tests should contain these two decoder_fns.</p>", "body_text": "Lukasz and I discussed the existing API.  Here's how we think it could be improved.  First, remove the embedding stuff and make inputs optional.  sequence_length may be provided iff inputs is.  If inputs is provided, decoding is allowed to run up to the number of frames in inputs and no longer than that (regarldess of what decoder_fn says).  If inputs is not provided, decoder_fn must say when to stop for each minibatch entry.\nThere should be separate decoder_fn impls for training and for inference; see below for details on that:\n\nchange the decoder_fn to have the following signature (up to you what you call the arguments and the outputs, of course; these are just placeholders)\n\ndecoder_fn(time : scalar int,\n  cur_state : tensor_tuple [batch_size, ...],\n  cur_output : tensor_tuple [batch_size, ...],\n  cur_input tensor_tuple [batch_size, ...],\n  decoder_state : tensor_tuple [anything])\n-> \n(done : bool vec [batch_size],\n next_state : tensor_tuple [batch_size, ...],\n next_input : tensor_tuple [batch_size, ...],\n next_decoder_state : tensor_tuple[anything])\n\nhere tensor_tuple means a (possibly nested) tuple of tensors or single tensor; and, e.g., next_state must have the same nested structure and shape as cur_state (same for next_decoder_state); and next_input should have consistent structure and shape across all calls.\nhere is how decoder_fn gets called:\ntraining time (inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **Not None** (first input)\n  decoder_state: None\ntraining time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: not None (first input)\n  decoder_state: previous emitted decoder state\n\ninference time (**NO** inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **None**\n  decoder_state: None\ninference time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: **None**\n  decoder_state: previous emitted decoder state\n\nhere's are the two simplest decoder_fns (one for training, one for inference):\nsimplest case, training:\n  1. if cur_state: None, cur_output: None, cur_input: not None, decoder_state: None\n   return\n     next_state: your initial state\n     next_input: cur_input\n     done = [False] * batch_size\n     next_decoder_state: None\n  2. if cur_state: not None, cur_output: not None, cur_input: not None, decoder_state: None\n    return\n      next_state = cur_state\n      next_input: cur_input\n      done: [False] * batch_size\n      next_decoder_state: None\n\nsimplest case, inference:\n  1. if cur_state: None, cur_output: None, cur_input: None, decoder_state: None\n    return\n       next_state: your initial state\n       next input: your go symbol input\n       done: [False] * batch_size\n       next_decoder_state: None\n   2. if cur_state: not None, cur_output: not None, cur_input: None, decoder_state: None\n     return\n       next_state: cur_state\n       next_input: your choice, e.g. embed of argmax of cur_output\n       done: if argmax of cur output is done symbol\n       next_decoder_state: None\n\nunit tests should contain these two decoder_fns.", "body": "Lukasz and I discussed the existing API.  Here's how we think it could be improved.  First, remove the embedding stuff and make `inputs` optional.  `sequence_length` may be provided iff `inputs` is.  If `inputs` is provided, decoding is allowed to run up to the number of frames in `inputs` and no longer than that (regarldess of what `decoder_fn` says).  If `inputs` is not provided, `decoder_fn` must say when to stop for each minibatch entry.\n\nThere should be separate `decoder_fn` impls for training and for inference; see below for details on that:\n1. change the decoder_fn to have the following signature (up to you what you call the arguments and the outputs, of course; these are just placeholders)\n\n```\ndecoder_fn(time : scalar int,\n  cur_state : tensor_tuple [batch_size, ...],\n  cur_output : tensor_tuple [batch_size, ...],\n  cur_input tensor_tuple [batch_size, ...],\n  decoder_state : tensor_tuple [anything])\n-> \n(done : bool vec [batch_size],\n next_state : tensor_tuple [batch_size, ...],\n next_input : tensor_tuple [batch_size, ...],\n next_decoder_state : tensor_tuple[anything])\n```\n\nhere tensor_tuple means a (possibly nested) tuple of tensors or single tensor; and, e.g., `next_state` must have the same nested structure and shape as `cur_state` (same for `next_decoder_state`); and `next_input` should have consistent structure and shape across all calls.\n\nhere is how decoder_fn gets called:\n\n```\ntraining time (inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **Not None** (first input)\n  decoder_state: None\ntraining time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: not None (first input)\n  decoder_state: previous emitted decoder state\n\ninference time (**NO** inputs provided), first call:\n  cur_state: None\n  cur_output: None\n  cur_input: **None**\n  decoder_state: None\ninference time (inputs provided), subsequent calls:\n  cur_state: not None\n  cur_output: not None\n  cur_input: **None**\n  decoder_state: previous emitted decoder state\n```\n\nhere's are the two simplest decoder_fns (one for training, one for inference):\n\n```\nsimplest case, training:\n  1. if cur_state: None, cur_output: None, cur_input: not None, decoder_state: None\n   return\n     next_state: your initial state\n     next_input: cur_input\n     done = [False] * batch_size\n     next_decoder_state: None\n  2. if cur_state: not None, cur_output: not None, cur_input: not None, decoder_state: None\n    return\n      next_state = cur_state\n      next_input: cur_input\n      done: [False] * batch_size\n      next_decoder_state: None\n```\n\n```\nsimplest case, inference:\n  1. if cur_state: None, cur_output: None, cur_input: None, decoder_state: None\n    return\n       next_state: your initial state\n       next input: your go symbol input\n       done: [False] * batch_size\n       next_decoder_state: None\n   2. if cur_state: not None, cur_output: not None, cur_input: None, decoder_state: None\n     return\n       next_state: cur_state\n       next_input: your choice, e.g. embed of argmax of cur_output\n       done: if argmax of cur output is done symbol\n       next_decoder_state: None\n```\n\nunit tests should contain these two decoder_fns.\n"}