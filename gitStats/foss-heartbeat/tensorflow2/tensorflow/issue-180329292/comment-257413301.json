{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/257413301", "html_url": "https://github.com/tensorflow/tensorflow/pull/4686#issuecomment-257413301", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4686", "id": 257413301, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzQxMzMwMQ==", "user": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-31T20:37:09Z", "updated_at": "2016-10-31T20:37:09Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>: good question! It might be worth thinking (maybe later).</p>\n<p>Right now, at training time (when <code>inputs</code> is not None), users can pass whatever inputs they want (embedding lookup vectors or any vectors, etc). Our <code>simple_decoder_fn_inference</code> happens to be a specific decoder_fn that uses argmax for cell_output to get discrete words &amp; then do embedding lookup from the argmax results (so we made an implicit assumption that at training time, <code>inputs</code> passed to the <code>dynamic_rnn_decoder</code> was looked up from the same <code>embeddings</code> matrix).</p>\n<p>From that point of view, I think our current <code>dynamic_rnn_decoder</code> seems fine. Those decoder_fn_{train, inference} are just examples of a specific usage.</p>", "body_text": "@ebrevdo: good question! It might be worth thinking (maybe later).\nRight now, at training time (when inputs is not None), users can pass whatever inputs they want (embedding lookup vectors or any vectors, etc). Our simple_decoder_fn_inference happens to be a specific decoder_fn that uses argmax for cell_output to get discrete words & then do embedding lookup from the argmax results (so we made an implicit assumption that at training time, inputs passed to the dynamic_rnn_decoder was looked up from the same embeddings matrix).\nFrom that point of view, I think our current dynamic_rnn_decoder seems fine. Those decoder_fn_{train, inference} are just examples of a specific usage.", "body": "@ebrevdo: good question! It might be worth thinking (maybe later).\n\nRight now, at training time (when `inputs` is not None), users can pass whatever inputs they want (embedding lookup vectors or any vectors, etc). Our `simple_decoder_fn_inference` happens to be a specific decoder_fn that uses argmax for cell_output to get discrete words & then do embedding lookup from the argmax results (so we made an implicit assumption that at training time, `inputs` passed to the `dynamic_rnn_decoder` was looked up from the same `embeddings` matrix). \n\nFrom that point of view, I think our current `dynamic_rnn_decoder` seems fine. Those decoder_fn_{train, inference} are just examples of a specific usage.\n"}