{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/950", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/950/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/950/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/950/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/950", "id": 130411035, "node_id": "MDU6SXNzdWUxMzA0MTEwMzU=", "number": 950, "title": "sigmoid_cross_entropy_loss_with_logits: ReLU input is not finite", "user": {"login": "yoavz", "id": 2341691, "node_id": "MDQ6VXNlcjIzNDE2OTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2341691?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yoavz", "html_url": "https://github.com/yoavz", "followers_url": "https://api.github.com/users/yoavz/followers", "following_url": "https://api.github.com/users/yoavz/following{/other_user}", "gists_url": "https://api.github.com/users/yoavz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yoavz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yoavz/subscriptions", "organizations_url": "https://api.github.com/users/yoavz/orgs", "repos_url": "https://api.github.com/users/yoavz/repos", "events_url": "https://api.github.com/users/yoavz/events{/privacy}", "received_events_url": "https://api.github.com/users/yoavz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-02-01T16:30:10Z", "updated_at": "2016-02-01T20:46:53Z", "closed_at": "2016-02-01T20:46:53Z", "author_association": "NONE", "body_html": "<pre><code>W tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\n     [[Node: _send_nl_1_hs_100_lr_0p1/div_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=6247430638048453470, tensor_name=\"nl_1_hs_100_lr_0p1/div_1:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/div_1)]]\nTraceback (most recent call last):\n  File \"rnn.py\", line 80, in &lt;module&gt;\n    training=True)\n  File \"rnn.py\", line 18, in run_epoch\n    model.seq_targets: seq_targets\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nCaused by op u'nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics', defined at:\n  File \"rnn.py\", line 70, in &lt;module&gt;\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 61, in __init__\n    .minimize(self.loss)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 445, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 126, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 119, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'nl_1_hs_100_lr_0p1/logistic_loss/Relu', defined at:\n  File \"rnn.py\", line 70, in &lt;module&gt;\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 58, in __init__\n    losses = tf.nn.sigmoid_cross_entropy_with_logits(outputs, targets_concat)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 273, in sigmoid_cross_entropy_with_logits\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 547, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<p>When training a model using tf.nn.sigmoid_cross_entropy_loss_with_logits and RMSPropOptimizer, I am running into a ReLu gradient error. Since the initial learning rate for this instance of the model is quite high and this is a RNN, I am pretty confident that this is an exploding gradient issue with my model. However, the ReLU operation that my model is exploding on is part of an optimization to \"ensure stability and avoid overflow\", but it seems to be having the opposite effect.</p>", "body_text": "W tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\n     [[Node: _send_nl_1_hs_100_lr_0p1/div_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=6247430638048453470, tensor_name=\"nl_1_hs_100_lr_0p1/div_1:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/div_1)]]\nTraceback (most recent call last):\n  File \"rnn.py\", line 80, in <module>\n    training=True)\n  File \"rnn.py\", line 18, in run_epoch\n    model.seq_targets: seq_targets\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nCaused by op u'nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics', defined at:\n  File \"rnn.py\", line 70, in <module>\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 61, in __init__\n    .minimize(self.loss)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 445, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 126, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 119, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'nl_1_hs_100_lr_0p1/logistic_loss/Relu', defined at:\n  File \"rnn.py\", line 70, in <module>\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 58, in __init__\n    losses = tf.nn.sigmoid_cross_entropy_with_logits(outputs, targets_concat)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 273, in sigmoid_cross_entropy_with_logits\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 547, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\nWhen training a model using tf.nn.sigmoid_cross_entropy_loss_with_logits and RMSPropOptimizer, I am running into a ReLu gradient error. Since the initial learning rate for this instance of the model is quite high and this is a RNN, I am pretty confident that this is an exploding gradient issue with my model. However, the ReLU operation that my model is exploding on is part of an optimization to \"ensure stability and avoid overflow\", but it seems to be having the opposite effect.", "body": "```\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nW tensorflow/core/common_runtime/executor.cc:1076] 0x7fc328bccbd0 Compute status: Invalid argument: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\n     [[Node: _send_nl_1_hs_100_lr_0p1/div_1_0 = _Send[T=DT_FLOAT, client_terminated=true, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device_incarnation=6247430638048453470, tensor_name=\"nl_1_hs_100_lr_0p1/div_1:0\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/div_1)]]\nTraceback (most recent call last):\n  File \"rnn.py\", line 80, in <module>\n    training=True)\n  File \"rnn.py\", line 18, in run_epoch\n    model.seq_targets: seq_targets\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 368, in run\n    results = self._do_run(target_list, unique_fetch_targets, feed_dict_string)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 444, in _do_run\n    e.code)\ntensorflow.python.framework.errors.InvalidArgumentError: ReluGrad input is not finite. : Tensor had NaN values\n     [[Node: nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics = CheckNumerics[T=DT_FLOAT, message=\"ReluGrad input is not finite.\", _device=\"/job:localhost/replica:0/task:0/cpu:0\"](nl_1_hs_100_lr_0p1/add)]]\nCaused by op u'nl_1_hs_100_lr_0p1/gradients/nl_1_hs_100_lr_0p1/logistic_loss/Relu_grad/nl_1_hs_100_lr_0p1/logistic_loss/Relu/CheckNumerics', defined at:\n  File \"rnn.py\", line 70, in <module>\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 61, in __init__\n    .minimize(self.loss)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 186, in minimize\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 232, in compute_gradients\n    aggregation_method=aggregation_method)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 445, in gradients\n    in_grads = _AsList(grad_fn(op_wrapper, *out_grads))\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 126, in _ReluGrad\n    t = _VerifyTensor(op.inputs[0], op.name, \"ReluGrad input is not finite.\")\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn_grad.py\", line 119, in _VerifyTensor\n    verify_input = array_ops.check_numerics(t, message=msg)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 48, in check_numerics\n    name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n\n...which was originally created as op u'nl_1_hs_100_lr_0p1/logistic_loss/Relu', defined at:\n  File \"rnn.py\", line 70, in <module>\n    train_model = Model(set_config(config, \"train\"))\n  File \"/Users/yoav/projects/music_rnn/model.py\", line 58, in __init__\n    losses = tf.nn.sigmoid_cross_entropy_with_logits(outputs, targets_concat)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/nn.py\", line 273, in sigmoid_cross_entropy_with_logits\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 547, in relu\n    return _op_def_lib.apply_op(\"Relu\", features=features, name=name)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1834, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1043, in __init__\n    self._traceback = _extract_stack()\n```\n\nWhen training a model using tf.nn.sigmoid_cross_entropy_loss_with_logits and RMSPropOptimizer, I am running into a ReLu gradient error. Since the initial learning rate for this instance of the model is quite high and this is a RNN, I am pretty confident that this is an exploding gradient issue with my model. However, the ReLU operation that my model is exploding on is part of an optimization to \"ensure stability and avoid overflow\", but it seems to be having the opposite effect. \n"}