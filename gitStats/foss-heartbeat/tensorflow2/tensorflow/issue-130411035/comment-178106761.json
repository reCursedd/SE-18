{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/178106761", "html_url": "https://github.com/tensorflow/tensorflow/issues/950#issuecomment-178106761", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/950", "id": 178106761, "node_id": "MDEyOklzc3VlQ29tbWVudDE3ODEwNjc2MQ==", "user": {"login": "yoavz", "id": 2341691, "node_id": "MDQ6VXNlcjIzNDE2OTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2341691?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yoavz", "html_url": "https://github.com/yoavz", "followers_url": "https://api.github.com/users/yoavz/followers", "following_url": "https://api.github.com/users/yoavz/following{/other_user}", "gists_url": "https://api.github.com/users/yoavz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yoavz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yoavz/subscriptions", "organizations_url": "https://api.github.com/users/yoavz/orgs", "repos_url": "https://api.github.com/users/yoavz/repos", "events_url": "https://api.github.com/users/yoavz/events{/privacy}", "received_events_url": "https://api.github.com/users/yoavz/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-01T18:14:06Z", "updated_at": "2016-02-01T18:14:06Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>: Sorry, I should have been more clear: I agree with you that this is an exploding gradient issue with my model.</p>\n<p>The reason I believe this <strong>is</strong> also a bug with tensorflow is that I am using a loss function from the official tensorflow API (sigmoid_cross_entropy_loss_with_logits). The documentation of this function leads me to believe that it is supposed to ensure numerical stability during training, and this is a counterexample where it did not. Perhaps I am misunderstanding the documentation?</p>", "body_text": "@yaroslavvb: Sorry, I should have been more clear: I agree with you that this is an exploding gradient issue with my model.\nThe reason I believe this is also a bug with tensorflow is that I am using a loss function from the official tensorflow API (sigmoid_cross_entropy_loss_with_logits). The documentation of this function leads me to believe that it is supposed to ensure numerical stability during training, and this is a counterexample where it did not. Perhaps I am misunderstanding the documentation?", "body": "@yaroslavvb: Sorry, I should have been more clear: I agree with you that this is an exploding gradient issue with my model. \n\nThe reason I believe this **is** also a bug with tensorflow is that I am using a loss function from the official tensorflow API (sigmoid_cross_entropy_loss_with_logits). The documentation of this function leads me to believe that it is supposed to ensure numerical stability during training, and this is a counterexample where it did not. Perhaps I am misunderstanding the documentation?\n"}