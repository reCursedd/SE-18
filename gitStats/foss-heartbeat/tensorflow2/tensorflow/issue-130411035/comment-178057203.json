{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/178057203", "html_url": "https://github.com/tensorflow/tensorflow/issues/950#issuecomment-178057203", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/950", "id": 178057203, "node_id": "MDEyOklzc3VlQ29tbWVudDE3ODA1NzIwMw==", "user": {"login": "yoavz", "id": 2341691, "node_id": "MDQ6VXNlcjIzNDE2OTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2341691?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yoavz", "html_url": "https://github.com/yoavz", "followers_url": "https://api.github.com/users/yoavz/followers", "following_url": "https://api.github.com/users/yoavz/following{/other_user}", "gists_url": "https://api.github.com/users/yoavz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yoavz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yoavz/subscriptions", "organizations_url": "https://api.github.com/users/yoavz/orgs", "repos_url": "https://api.github.com/users/yoavz/repos", "events_url": "https://api.github.com/users/yoavz/events{/privacy}", "received_events_url": "https://api.github.com/users/yoavz/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-01T16:30:36Z", "updated_at": "2016-02-01T16:30:36Z", "author_association": "NONE", "body_html": "<p>For quick reference, here is the full source code of sigimoid_cross_entropy_with_logits</p>\n<pre><code>def sigmoid_cross_entropy_with_logits(logits, targets, name=None):\n  \"\"\"Computes sigmoid cross entropy given `logits`.\n\n  Measures the probability error in discrete classification tasks in which each\n  class is independent and not mutually exclusive.  For instance, one could\n  perform multilabel classification where a picture can contain both an elephant\n  and a dog at the same time.\n\n  For brevity, let `x = logits`, `z = targets`.  The logistic loss is\n\n        z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n      = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n      = (1 - z) * x + log(1 + exp(-x))\n      = x - x * z + log(1 + exp(-x))\n\n  To ensure stability and avoid overflow, the implementation uses\n\n      max(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n  `logits` and `targets` must have the same type and shape.\n\n  Args:\n    logits: A `Tensor` of type `float32` or `float64`.\n    targets: A `Tensor` of the same type and shape as `logits`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `logits` with the componentwise\n    logistic losses.\n  \"\"\"\n  with ops.op_scope([logits, targets], name, \"logistic_loss\") as name:\n    logits = ops.convert_to_tensor(logits, name=\"logits\")\n    targets = ops.convert_to_tensor(targets, name=\"targets\")\n    # The logistic loss formula from above is\n    #   x - x * z + log(1 + exp(-x))\n    # For x &lt; 0, a more numerically stable formula is\n    #   -x * z + log(1 + exp(x))\n    # To avoid branching, we use the combined version\n    #   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n                        math_ops.log(1 + math_ops.exp(-math_ops.abs(logits))),\n                        name=name)\n\n</code></pre>", "body_text": "For quick reference, here is the full source code of sigimoid_cross_entropy_with_logits\ndef sigmoid_cross_entropy_with_logits(logits, targets, name=None):\n  \"\"\"Computes sigmoid cross entropy given `logits`.\n\n  Measures the probability error in discrete classification tasks in which each\n  class is independent and not mutually exclusive.  For instance, one could\n  perform multilabel classification where a picture can contain both an elephant\n  and a dog at the same time.\n\n  For brevity, let `x = logits`, `z = targets`.  The logistic loss is\n\n        z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n      = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n      = (1 - z) * x + log(1 + exp(-x))\n      = x - x * z + log(1 + exp(-x))\n\n  To ensure stability and avoid overflow, the implementation uses\n\n      max(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n  `logits` and `targets` must have the same type and shape.\n\n  Args:\n    logits: A `Tensor` of type `float32` or `float64`.\n    targets: A `Tensor` of the same type and shape as `logits`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `logits` with the componentwise\n    logistic losses.\n  \"\"\"\n  with ops.op_scope([logits, targets], name, \"logistic_loss\") as name:\n    logits = ops.convert_to_tensor(logits, name=\"logits\")\n    targets = ops.convert_to_tensor(targets, name=\"targets\")\n    # The logistic loss formula from above is\n    #   x - x * z + log(1 + exp(-x))\n    # For x < 0, a more numerically stable formula is\n    #   -x * z + log(1 + exp(x))\n    # To avoid branching, we use the combined version\n    #   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n                        math_ops.log(1 + math_ops.exp(-math_ops.abs(logits))),\n                        name=name)", "body": "For quick reference, here is the full source code of sigimoid_cross_entropy_with_logits\n\n```\ndef sigmoid_cross_entropy_with_logits(logits, targets, name=None):\n  \"\"\"Computes sigmoid cross entropy given `logits`.\n\n  Measures the probability error in discrete classification tasks in which each\n  class is independent and not mutually exclusive.  For instance, one could\n  perform multilabel classification where a picture can contain both an elephant\n  and a dog at the same time.\n\n  For brevity, let `x = logits`, `z = targets`.  The logistic loss is\n\n        z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))\n      = z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))\n      = z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))\n      = (1 - z) * x + log(1 + exp(-x))\n      = x - x * z + log(1 + exp(-x))\n\n  To ensure stability and avoid overflow, the implementation uses\n\n      max(x, 0) - x * z + log(1 + exp(-abs(x)))\n\n  `logits` and `targets` must have the same type and shape.\n\n  Args:\n    logits: A `Tensor` of type `float32` or `float64`.\n    targets: A `Tensor` of the same type and shape as `logits`.\n    name: A name for the operation (optional).\n\n  Returns:\n    A `Tensor` of the same shape as `logits` with the componentwise\n    logistic losses.\n  \"\"\"\n  with ops.op_scope([logits, targets], name, \"logistic_loss\") as name:\n    logits = ops.convert_to_tensor(logits, name=\"logits\")\n    targets = ops.convert_to_tensor(targets, name=\"targets\")\n    # The logistic loss formula from above is\n    #   x - x * z + log(1 + exp(-x))\n    # For x < 0, a more numerically stable formula is\n    #   -x * z + log(1 + exp(x))\n    # To avoid branching, we use the combined version\n    #   max(x, 0) - x * z + log(1 + exp(-abs(x)))\n    return math_ops.add(nn_ops.relu(logits) - logits * targets,\n                        math_ops.log(1 + math_ops.exp(-math_ops.abs(logits))),\n                        name=name)\n\n```\n"}