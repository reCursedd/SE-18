{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16466", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16466/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16466/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16466/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16466", "id": 291995940, "node_id": "MDU6SXNzdWUyOTE5OTU5NDA=", "number": 16466, "title": "[Feature request] Adding a PR curves to canned estimators for (binary) classifiers", "user": {"login": "eisenjulian", "id": 7776575, "node_id": "MDQ6VXNlcjc3NzY1NzU=", "avatar_url": "https://avatars0.githubusercontent.com/u/7776575?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eisenjulian", "html_url": "https://github.com/eisenjulian", "followers_url": "https://api.github.com/users/eisenjulian/followers", "following_url": "https://api.github.com/users/eisenjulian/following{/other_user}", "gists_url": "https://api.github.com/users/eisenjulian/gists{/gist_id}", "starred_url": "https://api.github.com/users/eisenjulian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eisenjulian/subscriptions", "organizations_url": "https://api.github.com/users/eisenjulian/orgs", "repos_url": "https://api.github.com/users/eisenjulian/repos", "events_url": "https://api.github.com/users/eisenjulian/events{/privacy}", "received_events_url": "https://api.github.com/users/eisenjulian/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "chihuahua", "id": 4221553, "node_id": "MDQ6VXNlcjQyMjE1NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/4221553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chihuahua", "html_url": "https://github.com/chihuahua", "followers_url": "https://api.github.com/users/chihuahua/followers", "following_url": "https://api.github.com/users/chihuahua/following{/other_user}", "gists_url": "https://api.github.com/users/chihuahua/gists{/gist_id}", "starred_url": "https://api.github.com/users/chihuahua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chihuahua/subscriptions", "organizations_url": "https://api.github.com/users/chihuahua/orgs", "repos_url": "https://api.github.com/users/chihuahua/repos", "events_url": "https://api.github.com/users/chihuahua/events{/privacy}", "received_events_url": "https://api.github.com/users/chihuahua/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "chihuahua", "id": 4221553, "node_id": "MDQ6VXNlcjQyMjE1NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/4221553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chihuahua", "html_url": "https://github.com/chihuahua", "followers_url": "https://api.github.com/users/chihuahua/followers", "following_url": "https://api.github.com/users/chihuahua/following{/other_user}", "gists_url": "https://api.github.com/users/chihuahua/gists{/gist_id}", "starred_url": "https://api.github.com/users/chihuahua/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chihuahua/subscriptions", "organizations_url": "https://api.github.com/users/chihuahua/orgs", "repos_url": "https://api.github.com/users/chihuahua/repos", "events_url": "https://api.github.com/users/chihuahua/events{/privacy}", "received_events_url": "https://api.github.com/users/chihuahua/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-01-26T18:49:27Z", "updated_at": "2018-11-14T19:14:51Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 3.6.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>This is a feature request, and I'd be happy to <strong>contribute</strong> if you think it's a valuable addition.<br>\nI have been using estimators both pre-made and custom for classification tasks. I like that sharing the use of a <code>head</code>  as defined in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py\">tensorflow/python/estimator/canned/head.py</a> allows both the canned models and the custom ones to share prediction and evaluation metrics for comparison, however currently it feels that some key metrics are missing, mainly PR curves which are fully supported by tensorboard.  Currently, the <code>head</code> constructor allows a list of thresholds, although they are not used by default. The problem is that when used, it creates scalar summaries for the precision and recall at each threshold, which is not that useful as in general one wants to compare how different models compare precision wise while fixing recall and the other way round.</p>\n<p>Adding the PR summary op from <a href=\"https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/pr_curve\">here</a> would make the eval metrics more informative IMO.</p>\n<p>Thanks for taking the time to read this!</p>\n<h3>Source code / logs</h3>\n<p>Any code that uses pre-made estimator classifiers relies on the same head. <a href=\"https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep.py\">This</a> is one example.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4\nPython version: 3.6.3\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nThis is a feature request, and I'd be happy to contribute if you think it's a valuable addition.\nI have been using estimators both pre-made and custom for classification tasks. I like that sharing the use of a head  as defined in tensorflow/python/estimator/canned/head.py allows both the canned models and the custom ones to share prediction and evaluation metrics for comparison, however currently it feels that some key metrics are missing, mainly PR curves which are fully supported by tensorboard.  Currently, the head constructor allows a list of thresholds, although they are not used by default. The problem is that when used, it creates scalar summaries for the precision and recall at each threshold, which is not that useful as in general one wants to compare how different models compare precision wise while fixing recall and the other way round.\nAdding the PR summary op from here would make the eval metrics more informative IMO.\nThanks for taking the time to read this!\nSource code / logs\nAny code that uses pre-made estimator classifiers relies on the same head. This is one example.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 3.6.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nThis is a feature request, and I'd be happy to **contribute** if you think it's a valuable addition. \r\nI have been using estimators both pre-made and custom for classification tasks. I like that sharing the use of a `head`  as defined in [tensorflow/python/estimator/canned/head.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/estimator/canned/head.py) allows both the canned models and the custom ones to share prediction and evaluation metrics for comparison, however currently it feels that some key metrics are missing, mainly PR curves which are fully supported by tensorboard.  Currently, the `head` constructor allows a list of thresholds, although they are not used by default. The problem is that when used, it creates scalar summaries for the precision and recall at each threshold, which is not that useful as in general one wants to compare how different models compare precision wise while fixing recall and the other way round.\r\n\r\nAdding the PR summary op from [here](https://github.com/tensorflow/tensorboard/tree/master/tensorboard/plugins/pr_curve) would make the eval metrics more informative IMO.\r\n\r\nThanks for taking the time to read this!\r\n\r\n### Source code / logs\r\nAny code that uses pre-made estimator classifiers relies on the same head. [This](https://github.com/tensorflow/models/blob/master/official/wide_deep/wide_deep.py) is one example."}