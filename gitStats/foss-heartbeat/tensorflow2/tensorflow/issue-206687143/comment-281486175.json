{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281486175", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281486175", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281486175, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQ4NjE3NQ==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T21:26:50Z", "updated_at": "2017-02-21T21:31:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Thank you guys, I finally got it.</p>\n<p>So what optimization do you propose?</p>\n<p>The draft of solution can look like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SoftmaxCrossEntropyWithLogits<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_SoftmaxCrossEntropyWithLogitsGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad_0</span>, <span class=\"pl-smi\">grad_1</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gradient function for SoftmaxCrossEntropyWithLogits.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_0 is the backprop for cost, and we multiply it with the gradients</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (which is output[1])</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> There is no gradient for the labels</span>\n  softmax_grad <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">1</span>]\n   \n  <span class=\"pl-c\"><span class=\"pl-c\">#</span># HERE WE COMPUTE SECOND DERIVATIVE</span>\n  cost <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">1</span>]\n  logits <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n  second_deriv <span class=\"pl-k\">=</span> tf.gradients(cost, tf.gradients(cost, logits)[<span class=\"pl-c1\">0</span>])[<span class=\"pl-c1\">0</span>]\n  <span class=\"pl-k\">return</span> _BroadcastMul(grad_0, softmax_grad) <span class=\"pl-k\">+</span> _BroadcastMul(grad_1, second_deriv), <span class=\"pl-c1\">None</span></pre></div>\n<p>but<br>\n(1) I think there is a trouble with using <code>tf.gradients</code> directly.<br>\n(2) I will be slower as <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> pointed.</p>", "body_text": "@alextp @girving Thank you guys, I finally got it.\nSo what optimization do you propose?\nThe draft of solution can look like this:\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\n  # (which is output[1])\n  # There is no gradient for the labels\n  softmax_grad = op.outputs[1]\n   \n  ## HERE WE COMPUTE SECOND DERIVATIVE\n  cost = op.outputs[1]\n  logits = op.inputs[0]\n  second_deriv = tf.gradients(cost, tf.gradients(cost, logits)[0])[0]\n  return _BroadcastMul(grad_0, softmax_grad) + _BroadcastMul(grad_1, second_deriv), None\nbut\n(1) I think there is a trouble with using tf.gradients directly.\n(2) I will be slower as @girving pointed.", "body": "@alextp @girving Thank you guys, I finally got it.\r\n\r\nSo what optimization do you propose?\r\n\r\nThe draft of solution can look like this:\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # There is no gradient for the labels\r\n  softmax_grad = op.outputs[1]\r\n   \r\n  ## HERE WE COMPUTE SECOND DERIVATIVE\r\n  cost = op.outputs[1]\r\n  logits = op.inputs[0]\r\n  second_deriv = tf.gradients(cost, tf.gradients(cost, logits)[0])[0]\r\n  return _BroadcastMul(grad_0, softmax_grad) + _BroadcastMul(grad_1, second_deriv), None\r\n```\r\n\r\nbut \r\n(1) I think there is a trouble with using `tf.gradients` directly.\r\n(2) I will be slower as @girving pointed.\r\n\r\n"}