{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280990768", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-280990768", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 280990768, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDk5MDc2OA==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-20T05:21:56Z", "updated_at": "2017-02-20T05:21:56Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3997997\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/persiyanov\">@persiyanov</a> Ah, sorry, I misread the Python stub I pointed you to.  I thought it was more like the gradient of <code>SigmoidGrad</code> second example I linked, where the answer to your question would be obvious.</p>\n<p>In the softmax case, the native C++ op produces one output for the softmax loss and one for the gradient, so you're right that you can't know \"in this routine\" whether the second derivative is actually used.  However, you can know \"outside of this routine\", since normal users who ask for the second derivative will ask for the derivative of the <code>_BroadcastMul(grad_0, softmax_grad_without_gradient)</code>.  Thus, I see two ways to proceed:</p>\n<ol>\n<li>\n<p>Compute the second derivative unconditionally: always take grad_1 and process it appropriately.  This is the sanest option.  Unfortunately, unless an optimization is performed, nearly all classification models will get a bit slower.</p>\n</li>\n<li>\n<p>Continue to use <code>array_ops.prevent_gradient</code>, but wrap the returned <code>mul</code> in a complicated gradient override block to \"un-ban the gradients\".  If the overridden gradients happen, you'll know the second derivative was actually requested, so there's no slowdown.</p>\n</li>\n</ol>\n<p>The optimization that would make (1) perfect would be detecting that <code>grad_1</code> is zeros, and skipping the second derivative calculation if so.  This is harder than it looks, unfortunately, since the zeros appear to be originate from potentially nasty looking control flow code.  Sometimes it's just <code>ZerosLike</code>, sometimes it's weird switch logic.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> Do we have a nice way of checking whether a tensor is known to be zero that will work despite crazy loop logic?  Something as simple as <code>tensor.is_zero()</code> or <code>tf.is_zero(tensor)</code> would be lovely, and presumably useful elsewhere.</p>\n<p>Depressing how complicated this all is.  Or maybe I've misunderstood it?</p>", "body_text": "@persiyanov Ah, sorry, I misread the Python stub I pointed you to.  I thought it was more like the gradient of SigmoidGrad second example I linked, where the answer to your question would be obvious.\nIn the softmax case, the native C++ op produces one output for the softmax loss and one for the gradient, so you're right that you can't know \"in this routine\" whether the second derivative is actually used.  However, you can know \"outside of this routine\", since normal users who ask for the second derivative will ask for the derivative of the _BroadcastMul(grad_0, softmax_grad_without_gradient).  Thus, I see two ways to proceed:\n\n\nCompute the second derivative unconditionally: always take grad_1 and process it appropriately.  This is the sanest option.  Unfortunately, unless an optimization is performed, nearly all classification models will get a bit slower.\n\n\nContinue to use array_ops.prevent_gradient, but wrap the returned mul in a complicated gradient override block to \"un-ban the gradients\".  If the overridden gradients happen, you'll know the second derivative was actually requested, so there's no slowdown.\n\n\nThe optimization that would make (1) perfect would be detecting that grad_1 is zeros, and skipping the second derivative calculation if so.  This is harder than it looks, unfortunately, since the zeros appear to be originate from potentially nasty looking control flow code.  Sometimes it's just ZerosLike, sometimes it's weird switch logic.\n@mrry @ebrevdo Do we have a nice way of checking whether a tensor is known to be zero that will work despite crazy loop logic?  Something as simple as tensor.is_zero() or tf.is_zero(tensor) would be lovely, and presumably useful elsewhere.\nDepressing how complicated this all is.  Or maybe I've misunderstood it?", "body": "@persiyanov Ah, sorry, I misread the Python stub I pointed you to.  I thought it was more like the gradient of `SigmoidGrad` second example I linked, where the answer to your question would be obvious.\r\n\r\nIn the softmax case, the native C++ op produces one output for the softmax loss and one for the gradient, so you're right that you can't know \"in this routine\" whether the second derivative is actually used.  However, you can know \"outside of this routine\", since normal users who ask for the second derivative will ask for the derivative of the `_BroadcastMul(grad_0, softmax_grad_without_gradient)`.  Thus, I see two ways to proceed:\r\n\r\n1. Compute the second derivative unconditionally: always take grad_1 and process it appropriately.  This is the sanest option.  Unfortunately, unless an optimization is performed, nearly all classification models will get a bit slower.\r\n\r\n2. Continue to use `array_ops.prevent_gradient`, but wrap the returned `mul` in a complicated gradient override block to \"un-ban the gradients\".  If the overridden gradients happen, you'll know the second derivative was actually requested, so there's no slowdown.\r\n\r\nThe optimization that would make (1) perfect would be detecting that `grad_1` is zeros, and skipping the second derivative calculation if so.  This is harder than it looks, unfortunately, since the zeros appear to be originate from potentially nasty looking control flow code.  Sometimes it's just `ZerosLike`, sometimes it's weird switch logic.\r\n\r\n@mrry @ebrevdo Do we have a nice way of checking whether a tensor is known to be zero that will work despite crazy loop logic?  Something as simple as `tensor.is_zero()` or `tf.is_zero(tensor)` would be lovely, and presumably useful elsewhere.\r\n\r\nDepressing how complicated this all is.  Or maybe I've misunderstood it?"}