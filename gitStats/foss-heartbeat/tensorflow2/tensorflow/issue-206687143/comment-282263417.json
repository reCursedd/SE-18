{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282263417", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-282263417", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 282263417, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjI2MzQxNw==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-24T10:53:55Z", "updated_at": "2017-02-24T10:53:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Hi! Is something like that looks okay?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SoftmaxCrossEntropyWithLogits<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_SoftmaxCrossEntropyWithLogitsGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad_0</span>, <span class=\"pl-smi\">grad_1</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gradient function for SoftmaxCrossEntropyWithLogits.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_0 is the backprop for cost, and we multiply it with the gradients</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (which is output[1])</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_1 is the backprop for softmax gradient.</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> There is no gradient for the labels</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Second derivative is just softmax derivative w.r.t. logits.</span>\n  softmax_grad <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">1</span>]\n  first_deriv <span class=\"pl-k\">=</span> _BroadcastMul(grad_0, softmax_grad)\n\n  <span class=\"pl-k\">if</span> grad_1.op.type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>ZerosLike<span class=\"pl-pds\">'</span></span>:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> or `in ('Zeros', 'ZerosLike')`</span>\n      <span class=\"pl-k\">return</span> first_deriv, <span class=\"pl-c1\">None</span>\n\n  logits <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n  softmax <span class=\"pl-k\">=</span> nn_ops.softmax(logits)\n\n  second_deriv <span class=\"pl-k\">=</span> ((grad_1 <span class=\"pl-k\">-</span> gen_array_ops.diag_part(\n      math_ops.matmul(grad_1, softmax, <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n  )[:, <span class=\"pl-c1\">None</span>]) <span class=\"pl-k\">*</span> softmax)\n\n  <span class=\"pl-k\">return</span> first_deriv <span class=\"pl-k\">+</span> second_deriv, <span class=\"pl-c1\">None</span></pre></div>\n<p><strong>Question</strong> about unit test: How could I check the graph nodes after its construction? If second derivative is requested, then <code>_SoftmaxCrossEntropyWithLogitsGrad</code> will return tensor with 'add' at the end of the name (e.g., <code>Tensor(\"hessians_second_derivative_2/gradients_1/SoftmaxCrossEntropyWithLogits_grad/add:0\", shape=(1, 2), dtype=float32)</code>), and otherwise not. But if look at the result of <code>tf.hessians</code>, it will look like <code>Tensor(\"hessians_second_derivative_2/hessians:0\", shape=(2, 2), dtype=float32)</code>.</p>", "body_text": "@girving Hi! Is something like that looks okay?\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\n  # (which is output[1])\n  # grad_1 is the backprop for softmax gradient.\n  # There is no gradient for the labels\n  #\n  # Second derivative is just softmax derivative w.r.t. logits.\n  softmax_grad = op.outputs[1]\n  first_deriv = _BroadcastMul(grad_0, softmax_grad)\n\n  if grad_1.op.type == 'ZerosLike':  # or `in ('Zeros', 'ZerosLike')`\n      return first_deriv, None\n\n  logits = op.inputs[0]\n  softmax = nn_ops.softmax(logits)\n\n  second_deriv = ((grad_1 - gen_array_ops.diag_part(\n      math_ops.matmul(grad_1, softmax, transpose_b=True)\n  )[:, None]) * softmax)\n\n  return first_deriv + second_deriv, None\nQuestion about unit test: How could I check the graph nodes after its construction? If second derivative is requested, then _SoftmaxCrossEntropyWithLogitsGrad will return tensor with 'add' at the end of the name (e.g., Tensor(\"hessians_second_derivative_2/gradients_1/SoftmaxCrossEntropyWithLogits_grad/add:0\", shape=(1, 2), dtype=float32)), and otherwise not. But if look at the result of tf.hessians, it will look like Tensor(\"hessians_second_derivative_2/hessians:0\", shape=(2, 2), dtype=float32).", "body": "@girving Hi! Is something like that looks okay?\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # grad_1 is the backprop for softmax gradient.\r\n  # There is no gradient for the labels\r\n  #\r\n  # Second derivative is just softmax derivative w.r.t. logits.\r\n  softmax_grad = op.outputs[1]\r\n  first_deriv = _BroadcastMul(grad_0, softmax_grad)\r\n\r\n  if grad_1.op.type == 'ZerosLike':  # or `in ('Zeros', 'ZerosLike')`\r\n      return first_deriv, None\r\n\r\n  logits = op.inputs[0]\r\n  softmax = nn_ops.softmax(logits)\r\n\r\n  second_deriv = ((grad_1 - gen_array_ops.diag_part(\r\n      math_ops.matmul(grad_1, softmax, transpose_b=True)\r\n  )[:, None]) * softmax)\r\n\r\n  return first_deriv + second_deriv, None\r\n```\r\n\r\n\r\n**Question** about unit test: How could I check the graph nodes after its construction? If second derivative is requested, then `_SoftmaxCrossEntropyWithLogitsGrad` will return tensor with 'add' at the end of the name (e.g., `Tensor(\"hessians_second_derivative_2/gradients_1/SoftmaxCrossEntropyWithLogits_grad/add:0\", shape=(1, 2), dtype=float32)`), and otherwise not. But if look at the result of `tf.hessians`, it will look like `Tensor(\"hessians_second_derivative_2/hessians:0\", shape=(2, 2), dtype=float32)`."}