{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403", "id": 206687143, "node_id": "MDU6SXNzdWUyMDY2ODcxNDM=", "number": 7403, "title": "Incorrect second derivative of softmax cross entropy loss", "user": {"login": "ftramer", "id": 13002470, "node_id": "MDQ6VXNlcjEzMDAyNDcw", "avatar_url": "https://avatars0.githubusercontent.com/u/13002470?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ftramer", "html_url": "https://github.com/ftramer", "followers_url": "https://api.github.com/users/ftramer/followers", "following_url": "https://api.github.com/users/ftramer/following{/other_user}", "gists_url": "https://api.github.com/users/ftramer/gists{/gist_id}", "starred_url": "https://api.github.com/users/ftramer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ftramer/subscriptions", "organizations_url": "https://api.github.com/users/ftramer/orgs", "repos_url": "https://api.github.com/users/ftramer/repos", "events_url": "https://api.github.com/users/ftramer/events{/privacy}", "received_events_url": "https://api.github.com/users/ftramer/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 36, "created_at": "2017-02-10T02:07:22Z", "updated_at": "2017-06-16T21:15:24Z", "closed_at": "2017-06-16T21:15:24Z", "author_association": "NONE", "body_html": "<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nlogits <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>])\ny <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> direct computation</span>\nloss1 <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.reduce_sum(y <span class=\"pl-k\">*</span> tf.log(tf.nn.softmax(logits)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> optimized version</span>\nloss2 <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(logits, y)\n\nfeed <span class=\"pl-k\">=</span> {logits: [<span class=\"pl-c1\">0.5</span>, <span class=\"pl-c1\">0.5</span>]}\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    <span class=\"pl-k\">for</span> i, loss <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>([loss1, loss2]):\n        g <span class=\"pl-k\">=</span> tf.gradients(loss, [logits])[<span class=\"pl-c1\">0</span>]\n        \n        h0 <span class=\"pl-k\">=</span> tf.gradients(g[<span class=\"pl-c1\">0</span>], [logits])[<span class=\"pl-c1\">0</span>]\n        h1 <span class=\"pl-k\">=</span> tf.gradients(g[<span class=\"pl-c1\">1</span>], [logits])[<span class=\"pl-c1\">0</span>]\n        h <span class=\"pl-k\">=</span> tf.pack([h0, h1])\n        \n        <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-c1\">%d</span>:<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>)\n        <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>gradient:<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-c1\">print</span> g.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed)\n        <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>hessian:<span class=\"pl-pds\">'</span></span>\n        <span class=\"pl-c1\">print</span> h.eval(<span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>feed)\n        <span class=\"pl-c1\">print</span> </pre></div>\n<p>produces the output</p>\n<pre><code>loss1:\ngradient:\n[-0.5  0.5]\nhessian:\n[[ 0.25 -0.25]\n [-0.25  0.25]]\n\nloss2:\ngradient:\n[-0.5  0.5]\nhessian:\n[[-0.  0.]\n [-0.  0.]]\n</code></pre>\n<p>Although the gradient is correct in both cases, the hessian computed using <code>loss1</code> is correct while the one computed from <code>softmax_cross_entropy_with_logits</code> is not.<br>\nI have not figured out where the issue arises from.</p>", "body_text": "If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport tensorflow as tf\n\nlogits = tf.Variable([0.5, 0.5])\ny = tf.constant([1,0], dtype=tf.float32)\n\n# direct computation\nloss1 = -tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)))\n\n# optimized version\nloss2 = tf.nn.softmax_cross_entropy_with_logits(logits, y)\n\nfeed = {logits: [0.5, 0.5]}\n\nwith tf.Session() as sess:\n    for i, loss in enumerate([loss1, loss2]):\n        g = tf.gradients(loss, [logits])[0]\n        \n        h0 = tf.gradients(g[0], [logits])[0]\n        h1 = tf.gradients(g[1], [logits])[0]\n        h = tf.pack([h0, h1])\n        \n        print 'loss%d:' % (i+1)\n        print 'gradient:'\n        print g.eval(feed_dict=feed)\n        print 'hessian:'\n        print h.eval(feed_dict=feed)\n        print \nproduces the output\nloss1:\ngradient:\n[-0.5  0.5]\nhessian:\n[[ 0.25 -0.25]\n [-0.25  0.25]]\n\nloss2:\ngradient:\n[-0.5  0.5]\nhessian:\n[[-0.  0.]\n [-0.  0.]]\n\nAlthough the gradient is correct in both cases, the hessian computed using loss1 is correct while the one computed from softmax_cross_entropy_with_logits is not.\nI have not figured out where the issue arises from.", "body": "### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\nlogits = tf.Variable([0.5, 0.5])\r\ny = tf.constant([1,0], dtype=tf.float32)\r\n\r\n# direct computation\r\nloss1 = -tf.reduce_sum(y * tf.log(tf.nn.softmax(logits)))\r\n\r\n# optimized version\r\nloss2 = tf.nn.softmax_cross_entropy_with_logits(logits, y)\r\n\r\nfeed = {logits: [0.5, 0.5]}\r\n\r\nwith tf.Session() as sess:\r\n    for i, loss in enumerate([loss1, loss2]):\r\n        g = tf.gradients(loss, [logits])[0]\r\n        \r\n        h0 = tf.gradients(g[0], [logits])[0]\r\n        h1 = tf.gradients(g[1], [logits])[0]\r\n        h = tf.pack([h0, h1])\r\n        \r\n        print 'loss%d:' % (i+1)\r\n        print 'gradient:'\r\n        print g.eval(feed_dict=feed)\r\n        print 'hessian:'\r\n        print h.eval(feed_dict=feed)\r\n        print \r\n```\r\n\r\nproduces the output\r\n\r\n```\r\nloss1:\r\ngradient:\r\n[-0.5  0.5]\r\nhessian:\r\n[[ 0.25 -0.25]\r\n [-0.25  0.25]]\r\n\r\nloss2:\r\ngradient:\r\n[-0.5  0.5]\r\nhessian:\r\n[[-0.  0.]\r\n [-0.  0.]]\r\n```\r\n\r\nAlthough the gradient is correct in both cases, the hessian computed using ```loss1``` is correct while the one computed from ```softmax_cross_entropy_with_logits``` is not. \r\nI have not figured out where the issue arises from."}