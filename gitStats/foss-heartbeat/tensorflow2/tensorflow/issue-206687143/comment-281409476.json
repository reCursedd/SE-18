{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281409476", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281409476", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281409476, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQwOTQ3Ng==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T17:09:25Z", "updated_at": "2017-02-21T17:09:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Setting aside the optimization, what do you mean by \"I can't do this in <code>SoftmaxCrossEntropyWithLogitsGrad</code>\"?  That's where you'd do it.</p>\n<p>It may be simpler to ignore the fact that second derivatives are involved.  Consider the op as an arbitrary smooth function that takes in some inputs and produces some outputs.  You can write down the formulas for everything involved, so you can take all the required derivatives.</p>\n<p>Does that make sense?  We should make sure that part is clear before moving on to the optimization bit.</p>", "body_text": "Setting aside the optimization, what do you mean by \"I can't do this in SoftmaxCrossEntropyWithLogitsGrad\"?  That's where you'd do it.\nIt may be simpler to ignore the fact that second derivatives are involved.  Consider the op as an arbitrary smooth function that takes in some inputs and produces some outputs.  You can write down the formulas for everything involved, so you can take all the required derivatives.\nDoes that make sense?  We should make sure that part is clear before moving on to the optimization bit.", "body": "Setting aside the optimization, what do you mean by \"I can't do this in `SoftmaxCrossEntropyWithLogitsGrad`\"?  That's where you'd do it.\r\n\r\nIt may be simpler to ignore the fact that second derivatives are involved.  Consider the op as an arbitrary smooth function that takes in some inputs and produces some outputs.  You can write down the formulas for everything involved, so you can take all the required derivatives.\r\n\r\nDoes that make sense?  We should make sure that part is clear before moving on to the optimization bit."}