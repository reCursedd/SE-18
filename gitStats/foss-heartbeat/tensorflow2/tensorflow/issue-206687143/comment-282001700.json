{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282001700", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-282001700", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 282001700, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MjAwMTcwMA==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-23T14:14:07Z", "updated_at": "2017-02-23T14:17:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Hi, I've implemented first version, using the fact that second derivative of xentropy is actually gradient of softmax w.r.t logits.</p>\n<p>This code works with minimal example which was introduced above.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SoftmaxCrossEntropyWithLogits<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_SoftmaxCrossEntropyWithLogitsGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad_0</span>, <span class=\"pl-smi\">grad_1</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gradient function for SoftmaxCrossEntropyWithLogits.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_0 is the backprop for cost, and we multiply it with the gradients</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (which is output[1])</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> There is no gradient for the labels</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Second derivative is just softmax derivative w.r.t. logits.</span>\n  softmax_grad <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">1</span>]\n\n  logits <span class=\"pl-k\">=</span> op.inputs[<span class=\"pl-c1\">0</span>]\n\n  softmax <span class=\"pl-k\">=</span> nn_ops.softmax(logits)\n\n  second_deriv <span class=\"pl-k\">=</span> ((grad_1 <span class=\"pl-k\">-</span> array_ops.reshape(\n      math_ops.reduce_sum(grad_1 <span class=\"pl-k\">*</span> softmax, [<span class=\"pl-c1\">1</span>]), [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>])) <span class=\"pl-k\">*</span> softmax)\n\n  <span class=\"pl-k\">return</span> _BroadcastMul(grad_0, softmax_grad) <span class=\"pl-k\">+</span> second_deriv, <span class=\"pl-c1\">None</span></pre></div>\n<p>Can we now proceed to optimization?</p>\n<p>Also, though it is a rare situation but I think we need to check if (grad_0 != 0 and grad_1 != 0) then we need to raise some warning or exception, because the returned value will not be correct.</p>", "body_text": "@girving Hi, I've implemented first version, using the fact that second derivative of xentropy is actually gradient of softmax w.r.t logits.\nThis code works with minimal example which was introduced above.\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\n  # (which is output[1])\n  # There is no gradient for the labels\n  #\n  # Second derivative is just softmax derivative w.r.t. logits.\n  softmax_grad = op.outputs[1]\n\n  logits = op.inputs[0]\n\n  softmax = nn_ops.softmax(logits)\n\n  second_deriv = ((grad_1 - array_ops.reshape(\n      math_ops.reduce_sum(grad_1 * softmax, [1]), [-1, 1])) * softmax)\n\n  return _BroadcastMul(grad_0, softmax_grad) + second_deriv, None\nCan we now proceed to optimization?\nAlso, though it is a rare situation but I think we need to check if (grad_0 != 0 and grad_1 != 0) then we need to raise some warning or exception, because the returned value will not be correct.", "body": "@girving Hi, I've implemented first version, using the fact that second derivative of xentropy is actually gradient of softmax w.r.t logits.\r\n\r\nThis code works with minimal example which was introduced above.\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # There is no gradient for the labels\r\n  #\r\n  # Second derivative is just softmax derivative w.r.t. logits.\r\n  softmax_grad = op.outputs[1]\r\n\r\n  logits = op.inputs[0]\r\n\r\n  softmax = nn_ops.softmax(logits)\r\n\r\n  second_deriv = ((grad_1 - array_ops.reshape(\r\n      math_ops.reduce_sum(grad_1 * softmax, [1]), [-1, 1])) * softmax)\r\n\r\n  return _BroadcastMul(grad_0, softmax_grad) + second_deriv, None\r\n```\r\n\r\n\r\nCan we now proceed to optimization?\r\n\r\nAlso, though it is a rare situation but I think we need to check if (grad_0 != 0 and grad_1 != 0) then we need to raise some warning or exception, because the returned value will not be correct."}