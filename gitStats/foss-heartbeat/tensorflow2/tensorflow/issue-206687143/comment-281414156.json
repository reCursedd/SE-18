{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281414156", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281414156", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281414156, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQxNDE1Ng==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T17:24:41Z", "updated_at": "2017-02-21T17:30:57Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3997997\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/persiyanov\">@persiyanov</a> I think I don't quite understand your question.  Consider the following function from R -&gt; R^2:</p>\n<pre><code>def f(x):\n  return x**2, 2*x\n\ndef grad_f(x, dy, dz):\n  return 2*x*dy + 2*dz\n</code></pre>\n<p><code>f</code> returns both a function and its first derivative.  It's gradient computes only \"first derivatives\", but clearly contains enough information to compute the second derivative of <code>f</code>'s first result.</p>\n<p><strong>Note</strong>: In the TensorFlow equivalent we're discussing, the occurrence of <code>2*x</code> in <code>grad_f</code> is a reference to the <code>2*x</code> produced by <code>f</code>, so that everything is linked together explicitly.</p>", "body_text": "@persiyanov I think I don't quite understand your question.  Consider the following function from R -> R^2:\ndef f(x):\n  return x**2, 2*x\n\ndef grad_f(x, dy, dz):\n  return 2*x*dy + 2*dz\n\nf returns both a function and its first derivative.  It's gradient computes only \"first derivatives\", but clearly contains enough information to compute the second derivative of f's first result.\nNote: In the TensorFlow equivalent we're discussing, the occurrence of 2*x in grad_f is a reference to the 2*x produced by f, so that everything is linked together explicitly.", "body": "@persiyanov I think I don't quite understand your question.  Consider the following function from R -> R^2:\r\n\r\n    def f(x):\r\n      return x**2, 2*x\r\n\r\n    def grad_f(x, dy, dz):\r\n      return 2*x*dy + 2*dz\r\n\r\n`f` returns both a function and its first derivative.  It's gradient computes only \"first derivatives\", but clearly contains enough information to compute the second derivative of `f`'s first result.\r\n\r\n**Note**: In the TensorFlow equivalent we're discussing, the occurrence of `2*x` in `grad_f` is a reference to the `2*x` produced by `f`, so that everything is linked together explicitly."}