{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281406902", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281406902", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281406902, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQwNjkwMg==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T17:00:53Z", "updated_at": "2017-02-21T17:01:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> Ah, okay, I see.</p>\n<p><code>SoftmaxCrossentropyWithLogits(logits, labels) -&gt; (loss, grad of loss w.r.t. logits)</code></p>\n<p>Therefore,</p>\n<p><code>SoftmaxCrossentropyWithLogitsGrad(op, grad_0, grad_1) -&gt; (grad w.r.t logits, grad w.r.t labels)</code></p>\n<p>Where, <code>grad_0 -- backprop w.r.t loss, grad_1 -- backprop w.r.t (grad w.r.t logits)</code> and <code>grad w.r.t labels</code> is actually ``None`.</p>\n<p>I need to know <code>grad_1</code> for computing hessian.</p>\n<p>But you say that I need to compute second derivative myself. So where do I need to compute it? I can't do this in <code>SoftmaxCrossentropyWithLogitsGrad</code>.</p>\n<p>When you said about optimization and checking <code>grad_1</code> on zero values, I understood it as \"check grad_1 in <code>SoftmaxCrossentropyWithLogitsGrad</code>, and if it equals zeroes, then return gradient of softmax loss without fusion. Otherwise, use <code>op.outputs[1]</code> for computing gradient.\"</p>\n<p>P.S. Of course, when I say \"equals zeroes\" I mean your hacky checking.</p>", "body_text": "@girving Ah, okay, I see.\nSoftmaxCrossentropyWithLogits(logits, labels) -> (loss, grad of loss w.r.t. logits)\nTherefore,\nSoftmaxCrossentropyWithLogitsGrad(op, grad_0, grad_1) -> (grad w.r.t logits, grad w.r.t labels)\nWhere, grad_0 -- backprop w.r.t loss, grad_1 -- backprop w.r.t (grad w.r.t logits) and grad w.r.t labels is actually ``None`.\nI need to know grad_1 for computing hessian.\nBut you say that I need to compute second derivative myself. So where do I need to compute it? I can't do this in SoftmaxCrossentropyWithLogitsGrad.\nWhen you said about optimization and checking grad_1 on zero values, I understood it as \"check grad_1 in SoftmaxCrossentropyWithLogitsGrad, and if it equals zeroes, then return gradient of softmax loss without fusion. Otherwise, use op.outputs[1] for computing gradient.\"\nP.S. Of course, when I say \"equals zeroes\" I mean your hacky checking.", "body": "@girving Ah, okay, I see.\r\n\r\n`SoftmaxCrossentropyWithLogits(logits, labels) -> (loss, grad of loss w.r.t. logits)`\r\n\r\nTherefore,\r\n\r\n`SoftmaxCrossentropyWithLogitsGrad(op, grad_0, grad_1) -> (grad w.r.t logits, grad w.r.t labels)`\r\n\r\nWhere, `grad_0 -- backprop w.r.t loss, grad_1 -- backprop w.r.t (grad w.r.t logits)` and `grad w.r.t labels` is actually ``None`.\r\n\r\n\r\nI need to know ``grad_1`` for computing hessian. \r\n\r\nBut you say that I need to compute second derivative myself. So where do I need to compute it? I can't do this in `SoftmaxCrossentropyWithLogitsGrad`.\r\n\r\nWhen you said about optimization and checking `grad_1` on zero values, I understood it as \"check grad_1 in `SoftmaxCrossentropyWithLogitsGrad`, and if it equals zeroes, then return gradient of softmax loss without fusion. Otherwise, use `op.outputs[1]` for computing gradient.\"\r\n\r\n\r\nP.S. Of course, when I say \"equals zeroes\" I mean your hacky checking."}