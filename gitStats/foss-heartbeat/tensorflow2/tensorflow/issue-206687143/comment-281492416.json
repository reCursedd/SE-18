{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281492416", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281492416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281492416, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQ5MjQxNg==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T21:49:03Z", "updated_at": "2017-02-21T21:49:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> I meant smth like this:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SoftmaxCrossEntropyWithLogits<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_SoftmaxCrossEntropyWithLogitsGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad_0</span>, <span class=\"pl-smi\">grad_1</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gradient function for SoftmaxCrossEntropyWithLogits.<span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_0 is the backprop for cost, and we multiply it with the gradients</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (which is output[1])</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> There is no gradient for the labels</span>\n  softmax_grad <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">1</span>]\n   \n  <span class=\"pl-c\"><span class=\"pl-c\">#</span># HERE WE COMPUTE SECOND DERIVATIVE</span>\n  logits, labels <span class=\"pl-k\">=</span> op.inputs\n  cost <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.reduce_sum(labels <span class=\"pl-k\">*</span> tf.log(tf.nn.softmax(logits)))\n  second_deriv <span class=\"pl-k\">=</span> tf.gradients(cost, tf.gradients(cost, logits)[<span class=\"pl-c1\">0</span>])[<span class=\"pl-c1\">0</span>]\n\n  <span class=\"pl-k\">return</span> _BroadcastMul(grad_0, softmax_grad) <span class=\"pl-k\">+</span> _BroadcastMul(grad_1, second_deriv), <span class=\"pl-c1\">None</span></pre></div>", "body_text": "@girving I meant smth like this:\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\n  # (which is output[1])\n  # There is no gradient for the labels\n  softmax_grad = op.outputs[1]\n   \n  ## HERE WE COMPUTE SECOND DERIVATIVE\n  logits, labels = op.inputs\n  cost = -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits)))\n  second_deriv = tf.gradients(cost, tf.gradients(cost, logits)[0])[0]\n\n  return _BroadcastMul(grad_0, softmax_grad) + _BroadcastMul(grad_1, second_deriv), None", "body": "@girving I meant smth like this:\r\n\r\n```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, grad_1):\r\n  \"\"\"Gradient function for SoftmaxCrossEntropyWithLogits.\"\"\"\r\n  # grad_0 is the backprop for cost, and we multiply it with the gradients\r\n  # (which is output[1])\r\n  # There is no gradient for the labels\r\n  softmax_grad = op.outputs[1]\r\n   \r\n  ## HERE WE COMPUTE SECOND DERIVATIVE\r\n  logits, labels = op.inputs\r\n  cost = -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits)))\r\n  second_deriv = tf.gradients(cost, tf.gradients(cost, logits)[0])[0]\r\n\r\n  return _BroadcastMul(grad_0, softmax_grad) + _BroadcastMul(grad_1, second_deriv), None\r\n```"}