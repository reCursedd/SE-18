{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281423895", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281423895", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281423895, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQyMzg5NQ==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T17:56:14Z", "updated_at": "2017-02-21T17:57:54Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>SoftmaxCrossEntropyWithLogits<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_SoftmaxCrossEntropyWithLogitsGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad_0</span>, <span class=\"pl-smi\">_</span>):\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>## OLD VERSION ###</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> softmax_grad_without_gradient = array_ops.prevent_gradient(op.outputs[1])</span>\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>## NEW VERSION ###</span>\n  logits, labels <span class=\"pl-k\">=</span> op.inputs\n  loss <span class=\"pl-k\">=</span> <span class=\"pl-k\">-</span>tf.reduce_sum(labels <span class=\"pl-k\">*</span> tf.log(tf.nn.softmax(logits)))\n  grad_loss_wrt_logits <span class=\"pl-k\">=</span> tf.gradients(loss, logits)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>## OLD VERSION ###</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> return _BroadcastMul(grad_0, softmax_grad_without_gradient), None</span>\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span>## NEW VERSION ###</span>\n  <span class=\"pl-k\">return</span> _BroadcastMul(grad_0, grad_loss_wrt_logits), <span class=\"pl-c1\">None</span></pre></div>\n<p>I am talking about something like this. I cant return second derivative from this function, I only can provide a way to compute it properly from outside the function.</p>", "body_text": "@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, _):\n  ### OLD VERSION ###\n  # softmax_grad_without_gradient = array_ops.prevent_gradient(op.outputs[1])\n\n  ### NEW VERSION ###\n  logits, labels = op.inputs\n  loss = -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits)))\n  grad_loss_wrt_logits = tf.gradients(loss, logits)\n\n  ### OLD VERSION ###\n  # return _BroadcastMul(grad_0, softmax_grad_without_gradient), None\n\n  ### NEW VERSION ###\n  return _BroadcastMul(grad_0, grad_loss_wrt_logits), None\nI am talking about something like this. I cant return second derivative from this function, I only can provide a way to compute it properly from outside the function.", "body": "```python\r\n@ops.RegisterGradient(\"SoftmaxCrossEntropyWithLogits\")\r\ndef _SoftmaxCrossEntropyWithLogitsGrad(op, grad_0, _):\r\n  ### OLD VERSION ###\r\n  # softmax_grad_without_gradient = array_ops.prevent_gradient(op.outputs[1])\r\n\r\n  ### NEW VERSION ###\r\n  logits, labels = op.inputs\r\n  loss = -tf.reduce_sum(labels * tf.log(tf.nn.softmax(logits)))\r\n  grad_loss_wrt_logits = tf.gradients(loss, logits)\r\n\r\n  ### OLD VERSION ###\r\n  # return _BroadcastMul(grad_0, softmax_grad_without_gradient), None\r\n\r\n  ### NEW VERSION ###\r\n  return _BroadcastMul(grad_0, grad_loss_wrt_logits), None\r\n```\r\n\r\n\r\nI am talking about something like this. I cant return second derivative from this function, I only can provide a way to compute it properly from outside the function."}