{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281464002", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281464002", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281464002, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTQ2NDAwMg==", "user": {"login": "persiyanov", "id": 3997997, "node_id": "MDQ6VXNlcjM5OTc5OTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/3997997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/persiyanov", "html_url": "https://github.com/persiyanov", "followers_url": "https://api.github.com/users/persiyanov/followers", "following_url": "https://api.github.com/users/persiyanov/following{/other_user}", "gists_url": "https://api.github.com/users/persiyanov/gists{/gist_id}", "starred_url": "https://api.github.com/users/persiyanov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/persiyanov/subscriptions", "organizations_url": "https://api.github.com/users/persiyanov/orgs", "repos_url": "https://api.github.com/users/persiyanov/repos", "events_url": "https://api.github.com/users/persiyanov/events{/privacy}", "received_events_url": "https://api.github.com/users/persiyanov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T20:05:49Z", "updated_at": "2017-02-21T20:12:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5061\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alextp\">@alextp</a> Yes! Only with these assumptions everything will be okay.</p>\n<p>So when <code>g = tf.gradients(loss, [logits])[0]</code> is called, <code>_SoftmaxCrossEntropyWithLogitsGrad</code> is immediately called, because <code>loss</code> is an output of xentropy op. And <code>grad_0==1</code> and <code>grad_1==0</code> in this case.</p>\n<p>But when I call <code>h = tf.gradients(g, [logits])[0]</code>, what really happens in TF? I suppose that <code>_SoftmaxCrossEntropyWithLogitsGrad</code> will be called with <code>grad_0==0</code> and <code>grad_1==1</code>, but not immediately. So when? Or TF will recognize that <code>g</code> is actually second output from softmax_xentropy op, and will call immediately its gradient function?</p>", "body_text": "@alextp Yes! Only with these assumptions everything will be okay.\nSo when g = tf.gradients(loss, [logits])[0] is called, _SoftmaxCrossEntropyWithLogitsGrad is immediately called, because loss is an output of xentropy op. And grad_0==1 and grad_1==0 in this case.\nBut when I call h = tf.gradients(g, [logits])[0], what really happens in TF? I suppose that _SoftmaxCrossEntropyWithLogitsGrad will be called with grad_0==0 and grad_1==1, but not immediately. So when? Or TF will recognize that g is actually second output from softmax_xentropy op, and will call immediately its gradient function?", "body": "@alextp Yes! Only with these assumptions everything will be okay. \r\n\r\nSo when `g = tf.gradients(loss, [logits])[0]` is called, `_SoftmaxCrossEntropyWithLogitsGrad` is immediately called, because `loss` is an output of xentropy op. And `grad_0==1` and `grad_1==0` in this case.\r\n\r\nBut when I call `h = tf.gradients(g, [logits])[0]`, what really happens in TF? I suppose that `_SoftmaxCrossEntropyWithLogitsGrad` will be called with `grad_0==0` and `grad_1==1`, but not immediately. So when? Or TF will recognize that `g` is actually second output from softmax_xentropy op, and will call immediately its gradient function?"}