{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/281375827", "html_url": "https://github.com/tensorflow/tensorflow/issues/7403#issuecomment-281375827", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7403", "id": 281375827, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MTM3NTgyNw==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-21T15:25:16Z", "updated_at": "2017-02-21T15:25:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3997997\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/persiyanov\">@persiyanov</a> The <code>SoftmaxCrossEntropyWithLogits</code> op produces two outputs.  One is the tensor of values, and the other is the gradient.  Thus, the unused second argument to the registered gradient routine currently called <code>_</code>, or <code>grad_1</code> for short`, is the gradient of the loss w.r.t. the gradient, and is what you need to compute second order gradients.</p>\n<p>P.S.  No, that's the simpler case where the gradient is a different op.</p>\n<p>P.P.S. Yes.  Alternatively, you can grep the code for <code>REGISTER_OP</code>.</p>", "body_text": "@persiyanov The SoftmaxCrossEntropyWithLogits op produces two outputs.  One is the tensor of values, and the other is the gradient.  Thus, the unused second argument to the registered gradient routine currently called _, or grad_1 for short`, is the gradient of the loss w.r.t. the gradient, and is what you need to compute second order gradients.\nP.S.  No, that's the simpler case where the gradient is a different op.\nP.P.S. Yes.  Alternatively, you can grep the code for REGISTER_OP.", "body": "@persiyanov The `SoftmaxCrossEntropyWithLogits` op produces two outputs.  One is the tensor of values, and the other is the gradient.  Thus, the unused second argument to the registered gradient routine currently called `_`, or `grad_1` for short`, is the gradient of the loss w.r.t. the gradient, and is what you need to compute second order gradients.\r\n\r\nP.S.  No, that's the simpler case where the gradient is a different op.\r\n\r\nP.P.S. Yes.  Alternatively, you can grep the code for `REGISTER_OP`."}