{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4101", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4101/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4101/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4101/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4101", "id": 173965196, "node_id": "MDU6SXNzdWUxNzM5NjUxOTY=", "number": 4101, "title": "Numerical issues with automatic matrix derivatives", "user": {"login": "Bonnevie", "id": 5861991, "node_id": "MDQ6VXNlcjU4NjE5OTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/5861991?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Bonnevie", "html_url": "https://github.com/Bonnevie", "followers_url": "https://api.github.com/users/Bonnevie/followers", "following_url": "https://api.github.com/users/Bonnevie/following{/other_user}", "gists_url": "https://api.github.com/users/Bonnevie/gists{/gist_id}", "starred_url": "https://api.github.com/users/Bonnevie/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Bonnevie/subscriptions", "organizations_url": "https://api.github.com/users/Bonnevie/orgs", "repos_url": "https://api.github.com/users/Bonnevie/repos", "events_url": "https://api.github.com/users/Bonnevie/events{/privacy}", "received_events_url": "https://api.github.com/users/Bonnevie/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2016-08-30T08:59:00Z", "updated_at": "2016-11-14T18:23:59Z", "closed_at": "2016-11-14T18:23:59Z", "author_association": "NONE", "body_html": "<p>While it is my impression that automatic differentiation generally enjoys numerical stability, this does not seem to extend directly to complicated expressions involving matrix calculus.<br>\nIn particular, for objectives involving matrix inverses the error can grow disproportionately large, leading to e.g. scipy optimization routines failing to converge.<br>\nIn my case I see TF with absolute element-wise gradient errors (comparing to the numerical Jacobian from tf.test.compute_gradient) around 10e-4 when my analytical gradient (also written using TF, only explicitly) gets errors near 10e-15.<br>\nThe problem likely boils down to the TF graph computing the inverses naively and not exploiting e.g. Cholesky and/or triangle structure or reframing multiplications as more stable linear system solves.</p>\n<p>I'm not sure I can readily provide a MWE as the code is somewhat complicated, but it is basically Gaussian process regression with a heavily parameterized kernel function.<br>\nThe objective itself has a form similar to what is found in the snippet below, with the callable instance <code>kernel</code> holding several tuneable parameters.</p>\n<pre><code>   #kernel computations\n    self.K = self.kernel(self.Xtrain) + self.kernel.diag(self.Xtrain) + self.kernel.noise(self.Xtrain)  +    self.kernel.jitter(Xtrain)\n    self.Ktest = self.kernel(self.Xtest) + self.kernel.noise(self.Xtest)  + self.kernel.jitter(self.Xtest)\n    self.Kx = self.kernel(self.Xtrain, self.Xtest)\n    self.L = tf.cholesky(self.K)\n\n    self.Ly = tf.matrix_triangular_solve(self.L, self.ytrain, lower = True)\n    self.LKx = tf.matrix_triangular_solve(self.L, self.Kx, lower = True)\n    self.llk = -0.5*tf.reduce_sum(self.Ly**2.) - 0.5*logdet_chol(self.L) \n    self.objective = - self.llk\n</code></pre>\n<p>Continued matrix calculus support was one of the primary reasons I picked TF over the alternatives, so I am really hoping this is something that can be fixed in the future.</p>", "body_text": "While it is my impression that automatic differentiation generally enjoys numerical stability, this does not seem to extend directly to complicated expressions involving matrix calculus.\nIn particular, for objectives involving matrix inverses the error can grow disproportionately large, leading to e.g. scipy optimization routines failing to converge.\nIn my case I see TF with absolute element-wise gradient errors (comparing to the numerical Jacobian from tf.test.compute_gradient) around 10e-4 when my analytical gradient (also written using TF, only explicitly) gets errors near 10e-15.\nThe problem likely boils down to the TF graph computing the inverses naively and not exploiting e.g. Cholesky and/or triangle structure or reframing multiplications as more stable linear system solves.\nI'm not sure I can readily provide a MWE as the code is somewhat complicated, but it is basically Gaussian process regression with a heavily parameterized kernel function.\nThe objective itself has a form similar to what is found in the snippet below, with the callable instance kernel holding several tuneable parameters.\n   #kernel computations\n    self.K = self.kernel(self.Xtrain) + self.kernel.diag(self.Xtrain) + self.kernel.noise(self.Xtrain)  +    self.kernel.jitter(Xtrain)\n    self.Ktest = self.kernel(self.Xtest) + self.kernel.noise(self.Xtest)  + self.kernel.jitter(self.Xtest)\n    self.Kx = self.kernel(self.Xtrain, self.Xtest)\n    self.L = tf.cholesky(self.K)\n\n    self.Ly = tf.matrix_triangular_solve(self.L, self.ytrain, lower = True)\n    self.LKx = tf.matrix_triangular_solve(self.L, self.Kx, lower = True)\n    self.llk = -0.5*tf.reduce_sum(self.Ly**2.) - 0.5*logdet_chol(self.L) \n    self.objective = - self.llk\n\nContinued matrix calculus support was one of the primary reasons I picked TF over the alternatives, so I am really hoping this is something that can be fixed in the future.", "body": "While it is my impression that automatic differentiation generally enjoys numerical stability, this does not seem to extend directly to complicated expressions involving matrix calculus. \nIn particular, for objectives involving matrix inverses the error can grow disproportionately large, leading to e.g. scipy optimization routines failing to converge. \nIn my case I see TF with absolute element-wise gradient errors (comparing to the numerical Jacobian from tf.test.compute_gradient) around 10e-4 when my analytical gradient (also written using TF, only explicitly) gets errors near 10e-15. \nThe problem likely boils down to the TF graph computing the inverses naively and not exploiting e.g. Cholesky and/or triangle structure or reframing multiplications as more stable linear system solves.\n\nI'm not sure I can readily provide a MWE as the code is somewhat complicated, but it is basically Gaussian process regression with a heavily parameterized kernel function. \nThe objective itself has a form similar to what is found in the snippet below, with the callable instance `kernel` holding several tuneable parameters. \n\n```\n   #kernel computations\n    self.K = self.kernel(self.Xtrain) + self.kernel.diag(self.Xtrain) + self.kernel.noise(self.Xtrain)  +    self.kernel.jitter(Xtrain)\n    self.Ktest = self.kernel(self.Xtest) + self.kernel.noise(self.Xtest)  + self.kernel.jitter(self.Xtest)\n    self.Kx = self.kernel(self.Xtrain, self.Xtest)\n    self.L = tf.cholesky(self.K)\n\n    self.Ly = tf.matrix_triangular_solve(self.L, self.ytrain, lower = True)\n    self.LKx = tf.matrix_triangular_solve(self.L, self.Kx, lower = True)\n    self.llk = -0.5*tf.reduce_sum(self.Ly**2.) - 0.5*logdet_chol(self.L) \n    self.objective = - self.llk\n```\n\nContinued matrix calculus support was one of the primary reasons I picked TF over the alternatives, so I am really hoping this is something that can be fixed in the future.\n"}