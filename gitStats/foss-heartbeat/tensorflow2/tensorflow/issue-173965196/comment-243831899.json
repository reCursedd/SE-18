{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243831899", "html_url": "https://github.com/tensorflow/tensorflow/issues/4101#issuecomment-243831899", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4101", "id": 243831899, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzgzMTg5OQ==", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-31T17:05:33Z", "updated_at": "2016-08-31T17:18:10Z", "author_association": "MEMBER", "body_html": "<p>Are your gradients computed in single or double precision? 10e-4 doesn't sound surprising if your gradient computation is in single precision. Could you perhaps boil this down to a reproducible example program and include your analytical gradient computation for comparison? If you peel off one step at a time in your objective, can you identify a step that is particularly inaccurate, or is it a gradual loss across the chain of matrix gradients?</p>\n<p>I agree with your general point that long chains of matrix operations (especially inverses and solves) can cause a loss of accuracy (especially in single precision). And since TensorFlow is not really a computer algebra system, the gradients are constructed by \"naively\" composing the individual op gradients using the chain rule without any algebraic simplification  / graph rewriting that could possibly improve accuracy.</p>\n<p>I don't think your comment about the gradient implementations is accurate: All the gradient ops in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py</a> use triangular and general matrix solves rather than forming and multiplying by the inverse (apart from determinant where the gradient is a scalar times the inverse). The Cholesky gradient uses a blocked algorithm, but also uses proper backsolves at the core. <a href=\"https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/kernels/cholesky_grad.cc?rcl=131328798&amp;l=108\" rel=\"nofollow\">https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/kernels/cholesky_grad.cc?rcl=131328798&amp;l=108</a></p>\n<p>Please have a look at those functions and let us know if you find something that you think could be improved, or even better submit a pull request with improvements to the code.</p>", "body_text": "Are your gradients computed in single or double precision? 10e-4 doesn't sound surprising if your gradient computation is in single precision. Could you perhaps boil this down to a reproducible example program and include your analytical gradient computation for comparison? If you peel off one step at a time in your objective, can you identify a step that is particularly inaccurate, or is it a gradual loss across the chain of matrix gradients?\nI agree with your general point that long chains of matrix operations (especially inverses and solves) can cause a loss of accuracy (especially in single precision). And since TensorFlow is not really a computer algebra system, the gradients are constructed by \"naively\" composing the individual op gradients using the chain rule without any algebraic simplification  / graph rewriting that could possibly improve accuracy.\nI don't think your comment about the gradient implementations is accurate: All the gradient ops in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py use triangular and general matrix solves rather than forming and multiplying by the inverse (apart from determinant where the gradient is a scalar times the inverse). The Cholesky gradient uses a blocked algorithm, but also uses proper backsolves at the core. https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/kernels/cholesky_grad.cc?rcl=131328798&l=108\nPlease have a look at those functions and let us know if you find something that you think could be improved, or even better submit a pull request with improvements to the code.", "body": "Are your gradients computed in single or double precision? 10e-4 doesn't sound surprising if your gradient computation is in single precision. Could you perhaps boil this down to a reproducible example program and include your analytical gradient computation for comparison? If you peel off one step at a time in your objective, can you identify a step that is particularly inaccurate, or is it a gradual loss across the chain of matrix gradients?\n\nI agree with your general point that long chains of matrix operations (especially inverses and solves) can cause a loss of accuracy (especially in single precision). And since TensorFlow is not really a computer algebra system, the gradients are constructed by \"naively\" composing the individual op gradients using the chain rule without any algebraic simplification  / graph rewriting that could possibly improve accuracy.\n\nI don't think your comment about the gradient implementations is accurate: All the gradient ops in https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py use triangular and general matrix solves rather than forming and multiplying by the inverse (apart from determinant where the gradient is a scalar times the inverse). The Cholesky gradient uses a blocked algorithm, but also uses proper backsolves at the core. https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/kernels/cholesky_grad.cc?rcl=131328798&l=108\n\nPlease have a look at those functions and let us know if you find something that you think could be improved, or even better submit a pull request with improvements to the code.\n"}