{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23903", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23903/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23903/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23903/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23903", "id": 383193722, "node_id": "MDU6SXNzdWUzODMxOTM3MjI=", "number": 23903, "title": "Poor performance when scaling up data pipeline in multi CPU environments", "user": {"login": "koenhelwegen", "id": 29484762, "node_id": "MDQ6VXNlcjI5NDg0NzYy", "avatar_url": "https://avatars1.githubusercontent.com/u/29484762?v=4", "gravatar_id": "", "url": "https://api.github.com/users/koenhelwegen", "html_url": "https://github.com/koenhelwegen", "followers_url": "https://api.github.com/users/koenhelwegen/followers", "following_url": "https://api.github.com/users/koenhelwegen/following{/other_user}", "gists_url": "https://api.github.com/users/koenhelwegen/gists{/gist_id}", "starred_url": "https://api.github.com/users/koenhelwegen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/koenhelwegen/subscriptions", "organizations_url": "https://api.github.com/users/koenhelwegen/orgs", "repos_url": "https://api.github.com/users/koenhelwegen/repos", "events_url": "https://api.github.com/users/koenhelwegen/events{/privacy}", "received_events_url": "https://api.github.com/users/koenhelwegen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-21T16:18:24Z", "updated_at": "2018-11-21T16:28:51Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong><br>\nAWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.</p>\n<p><strong>Describe the current behavior</strong><br>\nI am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.</p>\n<p>I seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput <em>decreases</em> significantly, even while CPU use is higher.</p>\n<p><em>The two systems perform as expected on other tasks.</em>  I did some very elementary benchmarking of both systems:</p>\n<ul>\n<li>calculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads &lt; 32, and 3.16 performs better when n_threads = 128.</li>\n<li>reading a file with 1,000,000 lines (and repeat 100 times): identical performance</li>\n</ul>\n<p>This suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.</p>\n<p>Both systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.</p>\n<p><strong>Describe the expected behavior</strong><br>\nPerformance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.</p>\n<p><strong>Code to reproduce the issue</strong><br>\nTaking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.</p>\n<p>(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)</p>\n<pre><code>import tensorflow as tf\nimport os\nimport time\n\nDATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'\nBATCH_SIZE = 256\nN_TRIALS = 10\nSTEPS_PER_TRIAL = 100\n\ndef decode_imagenet(serialized_example):\n    feature_map = {\n        'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n        'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)\n    }\n    features = tf.parse_single_example(serialized_example, feature_map)\n\n    image = features['image/encoded']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize_images(image, [224, 224])\n\n    label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))\n    \n    return image, label\n\ntraining_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]\n\ndataset = tf.data.TFRecordDataset(training_files)\ndataset = dataset.repeat()\ndataset = dataset.prefetch(BATCH_SIZE*16)\ndataset = dataset.map(decode_imagenet, num_parallel_calls=128)\ndataset = dataset.prefetch(BATCH_SIZE*4)\ndataset = dataset.batch(BATCH_SIZE)\ndataset = dataset.prefetch(1)\n\n\niterator = dataset.make_initializable_iterator()\n\nbatch_x, batch_y = iterator.get_next()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(iterator.initializer)\n\n    res = []\n    for trial in range(N_TRIALS):\n        t0 = time.time()\n        for _ in range(STEPS_PER_TRIAL):\n            sess.run([batch_x, batch_y])\n        t1 = time.time()\n        images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)\n        res.append(images_per_second)\n        print('Trial %i: %f ips' % (trial, images_per_second))\n    print('Overall average: %f ips' % (sum(res)/N_TRIALS))\n</code></pre>\n<p><strong>Other info / logs</strong></p>\n<p>Output of above code on p3.8:</p>\n<pre><code>Trial 0: 2687.172262 ips\nTrial 1: 2861.731855 ips\nTrial 2: 2832.192681 ips\nTrial 3: 2826.211587 ips\nTrial 4: 2851.585431 ips\nTrial 5: 2809.069081 ips\nTrial 6: 2869.325024 ips\nTrial 7: 2858.402548 ips\nTrial 8: 2833.678433 ips\nTrial 9: 2859.654147 ips\nOverall average: 2828.902305 ips\n</code></pre>\n<p>Meanwhile, %CPU (observed using the top command) is 2300-2400.</p>\n<p>On p3.16:</p>\n<pre><code>Trial 0: 1585.289254 ips\nTrial 1: 2077.237178 ips\nTrial 2: 2165.624391 ips\nTrial 3: 2183.211334 ips\nTrial 4: 2226.638648 ips\nTrial 5: 2217.900892 ips\nTrial 6: 2217.662143 ips\nTrial 7: 2232.781854 ips\nTrial 8: 2178.928816 ips\nTrial 9: 2222.856626 ips\nOverall average: 2130.813114 ips\n</code></pre>\n<p>%CPU is between 3700-4000.</p>", "body_text": "System information\nAWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.\nDescribe the current behavior\nI am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.\nI seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput decreases significantly, even while CPU use is higher.\nThe two systems perform as expected on other tasks.  I did some very elementary benchmarking of both systems:\n\ncalculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads < 32, and 3.16 performs better when n_threads = 128.\nreading a file with 1,000,000 lines (and repeat 100 times): identical performance\n\nThis suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.\nBoth systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.\nDescribe the expected behavior\nPerformance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.\nCode to reproduce the issue\nTaking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.\n(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)\nimport tensorflow as tf\nimport os\nimport time\n\nDATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'\nBATCH_SIZE = 256\nN_TRIALS = 10\nSTEPS_PER_TRIAL = 100\n\ndef decode_imagenet(serialized_example):\n    feature_map = {\n        'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\n        'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)\n    }\n    features = tf.parse_single_example(serialized_example, feature_map)\n\n    image = features['image/encoded']\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize_images(image, [224, 224])\n\n    label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))\n    \n    return image, label\n\ntraining_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]\n\ndataset = tf.data.TFRecordDataset(training_files)\ndataset = dataset.repeat()\ndataset = dataset.prefetch(BATCH_SIZE*16)\ndataset = dataset.map(decode_imagenet, num_parallel_calls=128)\ndataset = dataset.prefetch(BATCH_SIZE*4)\ndataset = dataset.batch(BATCH_SIZE)\ndataset = dataset.prefetch(1)\n\n\niterator = dataset.make_initializable_iterator()\n\nbatch_x, batch_y = iterator.get_next()\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    sess.run(iterator.initializer)\n\n    res = []\n    for trial in range(N_TRIALS):\n        t0 = time.time()\n        for _ in range(STEPS_PER_TRIAL):\n            sess.run([batch_x, batch_y])\n        t1 = time.time()\n        images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)\n        res.append(images_per_second)\n        print('Trial %i: %f ips' % (trial, images_per_second))\n    print('Overall average: %f ips' % (sum(res)/N_TRIALS))\n\nOther info / logs\nOutput of above code on p3.8:\nTrial 0: 2687.172262 ips\nTrial 1: 2861.731855 ips\nTrial 2: 2832.192681 ips\nTrial 3: 2826.211587 ips\nTrial 4: 2851.585431 ips\nTrial 5: 2809.069081 ips\nTrial 6: 2869.325024 ips\nTrial 7: 2858.402548 ips\nTrial 8: 2833.678433 ips\nTrial 9: 2859.654147 ips\nOverall average: 2828.902305 ips\n\nMeanwhile, %CPU (observed using the top command) is 2300-2400.\nOn p3.16:\nTrial 0: 1585.289254 ips\nTrial 1: 2077.237178 ips\nTrial 2: 2165.624391 ips\nTrial 3: 2183.211334 ips\nTrial 4: 2226.638648 ips\nTrial 5: 2217.900892 ips\nTrial 6: 2217.662143 ips\nTrial 7: 2232.781854 ips\nTrial 8: 2178.928816 ips\nTrial 9: 2222.856626 ips\nOverall average: 2130.813114 ips\n\n%CPU is between 3700-4000.", "body": "**System information**\r\nAWS Deep Learning AMI (ubuntu 16.04, tensorflow 12) on AWS p3.8 and p3.16 instances.\r\n\r\n**Describe the current behavior**\r\nI am trying to optimize the speed of my code when training on Imagenet and started by looking at the data pipeline. I am using TFRecordDataset(), and use both prefetch() and num_parallel_calls for mappings. Here I just load in the batches without performing any action on them.\r\n\r\nI seem to get reasonably good results on AWS p3.8 (32 vCPU, 4 Nvidia Tesla V100 GPUs), although it's still a bit below the ~3000 images/second that I've seen mentioned elsewhere. However, if I run exactly the same code on AWS p3.16 (64 vCPU, 8 GPUs), throughput *decreases* significantly, even while CPU use is higher. \r\n\r\n*The two systems perform as expected on other tasks.*  I did some very elementary benchmarking of both systems:\r\n- calculating the number of primes, spawning a number of threads using multiprocessing.Process(). As expected, performance between p3.8 and p3.16 appears to be identical when n_threads < 32, and 3.16 performs better when n_threads = 128.  \r\n- reading a file with 1,000,000 lines (and repeat 100 times): identical performance\r\n\r\nThis suggests to me the problem is with the TFRecordDataset, but if there is any other benchmarking that makes sense, please let me know.\r\n\r\nBoth systems allow two threads for each CPU. I launched another p3.16 instance with a single thread per CPU, so that the number of virtual CPUs is identical to the p3.8 instance. This did not appear to make any difference.\r\n\r\n**Describe the expected behavior**\r\nPerformance should improve, or at least remain the same, on p3.16 as compared to p3.8, as there are more CPUs available.\r\n\r\n**Code to reproduce the issue**\r\nTaking only the data pipeline part and skipping all shuffling, preprocessing, etcetera I am left with this as a minimal working example.\r\n\r\n(Note that I use multiple prefetch statements; this significantly improves performance on both systems for me, but removing them does not change their relative performance. Similarly, using 128 parallel calls in the mapping function gives better results than 32 or 64, but changing that or making it dependent on the actual number of vCPUs doesn't solve the observed behaviour.)\r\n\r\n    import tensorflow as tf\r\n    import os\r\n    import time\r\n\r\n    DATA_FOLDER = '/home/ubuntu/datasets/imagenet-data'\r\n    BATCH_SIZE = 256\r\n    N_TRIALS = 10\r\n    STEPS_PER_TRIAL = 100\r\n\r\n    def decode_imagenet(serialized_example):\r\n        feature_map = {\r\n            'image/encoded': tf.FixedLenFeature([], dtype=tf.string, default_value=''),\r\n            'image/class/label': tf.FixedLenFeature([1], dtype=tf.int64, default_value=-1)\r\n        }\r\n        features = tf.parse_single_example(serialized_example, feature_map)\r\n\r\n        image = features['image/encoded']\r\n        image = tf.image.decode_jpeg(image, channels=3)\r\n        image = tf.image.convert_image_dtype(image, tf.float32)\r\n        image = tf.image.resize_images(image, [224, 224])\r\n\r\n        label = tf.squeeze(tf.cast(features['image/class/label'], tf.int32))\r\n        \r\n        return image, label\r\n\r\n    training_files = [os.path.join(DATA_FOLDER, fn) for fn in os.listdir(DATA_FOLDER) if 'train' in fn]\r\n\r\n    dataset = tf.data.TFRecordDataset(training_files)\r\n    dataset = dataset.repeat()\r\n    dataset = dataset.prefetch(BATCH_SIZE*16)\r\n    dataset = dataset.map(decode_imagenet, num_parallel_calls=128)\r\n    dataset = dataset.prefetch(BATCH_SIZE*4)\r\n    dataset = dataset.batch(BATCH_SIZE)\r\n    dataset = dataset.prefetch(1)\r\n\r\n\r\n    iterator = dataset.make_initializable_iterator()\r\n\r\n    batch_x, batch_y = iterator.get_next()\r\n\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        sess.run(iterator.initializer)\r\n\r\n        res = []\r\n        for trial in range(N_TRIALS):\r\n            t0 = time.time()\r\n            for _ in range(STEPS_PER_TRIAL):\r\n                sess.run([batch_x, batch_y])\r\n            t1 = time.time()\r\n            images_per_second = BATCH_SIZE*STEPS_PER_TRIAL/(t1-t0)\r\n            res.append(images_per_second)\r\n            print('Trial %i: %f ips' % (trial, images_per_second))\r\n        print('Overall average: %f ips' % (sum(res)/N_TRIALS))\r\n\r\n\r\n\r\n**Other info / logs**\r\n\r\nOutput of above code on p3.8:\r\n\r\n    Trial 0: 2687.172262 ips\r\n    Trial 1: 2861.731855 ips\r\n    Trial 2: 2832.192681 ips\r\n    Trial 3: 2826.211587 ips\r\n    Trial 4: 2851.585431 ips\r\n    Trial 5: 2809.069081 ips\r\n    Trial 6: 2869.325024 ips\r\n    Trial 7: 2858.402548 ips\r\n    Trial 8: 2833.678433 ips\r\n    Trial 9: 2859.654147 ips\r\n    Overall average: 2828.902305 ips\r\n\r\nMeanwhile, %CPU (observed using the top command) is 2300-2400.\r\n\r\nOn p3.16:\r\n\r\n    Trial 0: 1585.289254 ips\r\n    Trial 1: 2077.237178 ips\r\n    Trial 2: 2165.624391 ips\r\n    Trial 3: 2183.211334 ips\r\n    Trial 4: 2226.638648 ips\r\n    Trial 5: 2217.900892 ips\r\n    Trial 6: 2217.662143 ips\r\n    Trial 7: 2232.781854 ips\r\n    Trial 8: 2178.928816 ips\r\n    Trial 9: 2222.856626 ips\r\n    Overall average: 2130.813114 ips\r\n\r\n%CPU is between 3700-4000.\r\n\r\n"}