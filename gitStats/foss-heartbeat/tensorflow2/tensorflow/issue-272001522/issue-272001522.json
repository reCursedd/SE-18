{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14334", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14334/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14334/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14334/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14334", "id": 272001522, "node_id": "MDU6SXNzdWUyNzIwMDE1MjI=", "number": 14334, "title": "Float16 not supported by Maxpool3D", "user": {"login": "furybubu", "id": 6081702, "node_id": "MDQ6VXNlcjYwODE3MDI=", "avatar_url": "https://avatars0.githubusercontent.com/u/6081702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/furybubu", "html_url": "https://github.com/furybubu", "followers_url": "https://api.github.com/users/furybubu/followers", "following_url": "https://api.github.com/users/furybubu/following{/other_user}", "gists_url": "https://api.github.com/users/furybubu/gists{/gist_id}", "starred_url": "https://api.github.com/users/furybubu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/furybubu/subscriptions", "organizations_url": "https://api.github.com/users/furybubu/orgs", "repos_url": "https://api.github.com/users/furybubu/repos", "events_url": "https://api.github.com/users/furybubu/events{/privacy}", "received_events_url": "https://api.github.com/users/furybubu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-11-07T22:07:06Z", "updated_at": "2017-11-07T22:11:50Z", "closed_at": "2017-11-07T22:11:50Z", "author_association": "NONE", "body_html": "<p>I was trying to switch the training of my neural net to Mixed-precision but while conv/deconv3d layers are supporting the float16 type,  Maxpool3D layers still require a float32 input. Can we expect these to support float16 precision soon?</p>\n<p>Thanks!</p>", "body_text": "I was trying to switch the training of my neural net to Mixed-precision but while conv/deconv3d layers are supporting the float16 type,  Maxpool3D layers still require a float32 input. Can we expect these to support float16 precision soon?\nThanks!", "body": "I was trying to switch the training of my neural net to Mixed-precision but while conv/deconv3d layers are supporting the float16 type,  Maxpool3D layers still require a float32 input. Can we expect these to support float16 precision soon?\r\n\r\nThanks!"}