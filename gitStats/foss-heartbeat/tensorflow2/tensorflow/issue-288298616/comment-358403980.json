{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/358403980", "html_url": "https://github.com/tensorflow/tensorflow/issues/16093#issuecomment-358403980", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16093", "id": 358403980, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODQwMzk4MA==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-17T18:50:12Z", "updated_at": "2018-01-17T18:51:56Z", "author_association": "MEMBER", "body_html": "<p>Multiple GPUs are supported with eager execution in that you can use <code>with tf.device('/gpu:&lt;N&gt;')</code> blocks to place operations and tensors on different GPUs. For example, the following will execute one matmul on GPU0 and one on GPU1:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n  tf.matmul([[<span class=\"pl-c1\">1</span>.]], [[<span class=\"pl-c1\">2</span>.]])\n<span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:1<span class=\"pl-pds\">'</span></span>):\n  tf.matmul([[<span class=\"pl-c1\">1</span>.]], [[<span class=\"pl-c1\">2</span>.]])</pre></div>\n<p>However, it will schedule them serially - so your bottleneck will likely be the Python interpreter's single thread feeding the GPUs.</p>\n<p>We're working on APIs for more efficient use of multiple GPUs and overall cutting down the cost of initiating operations from the Python interpreter.</p>\n<p>I'm tempted to close out this issue since it is a bit open ended. Any significant APIs addressing this will be part of the release notes over the next few releases. Please let me know if you disagree.</p>\n<p>Thanks!</p>\n<p>(CC <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15258583\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/josh11b\">@josh11b</a> )</p>", "body_text": "Multiple GPUs are supported with eager execution in that you can use with tf.device('/gpu:<N>') blocks to place operations and tensors on different GPUs. For example, the following will execute one matmul on GPU0 and one on GPU1:\nwith tf.device('/gpu:0'):\n  tf.matmul([[1.]], [[2.]])\nwith tf.device('/gpu:1'):\n  tf.matmul([[1.]], [[2.]])\nHowever, it will schedule them serially - so your bottleneck will likely be the Python interpreter's single thread feeding the GPUs.\nWe're working on APIs for more efficient use of multiple GPUs and overall cutting down the cost of initiating operations from the Python interpreter.\nI'm tempted to close out this issue since it is a bit open ended. Any significant APIs addressing this will be part of the release notes over the next few releases. Please let me know if you disagree.\nThanks!\n(CC @josh11b )", "body": "Multiple GPUs are supported with eager execution in that you can use `with tf.device('/gpu:<N>')` blocks to place operations and tensors on different GPUs. For example, the following will execute one matmul on GPU0 and one on GPU1:\r\n\r\n```python\r\nwith tf.device('/gpu:0'):\r\n  tf.matmul([[1.]], [[2.]])\r\nwith tf.device('/gpu:1'):\r\n  tf.matmul([[1.]], [[2.]])\r\n```\r\nHowever, it will schedule them serially - so your bottleneck will likely be the Python interpreter's single thread feeding the GPUs.\r\n\r\nWe're working on APIs for more efficient use of multiple GPUs and overall cutting down the cost of initiating operations from the Python interpreter.\r\n\r\nI'm tempted to close out this issue since it is a bit open ended. Any significant APIs addressing this will be part of the release notes over the next few releases. Please let me know if you disagree.\r\n\r\nThanks!\r\n\r\n(CC @josh11b )"}