{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/219690150", "pull_request_review_id": 157930584, "id": 219690150, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxOTY5MDE1MA==", "diff_hunk": "@@ -0,0 +1,180 @@\n+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+# http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Implementation of Cluster Resolvers for Slurm workload manager.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import os\n+import subprocess\n+\n+from tensorflow.contrib.cluster_resolver.python.training.cluster_resolver import ClusterResolver\n+from tensorflow.python.training.server_lib import ClusterSpec\n+\n+class SlurmClusterResolver(ClusterResolver):\n+  \"\"\"Cluster Resolver for system with Slurm workload manager.\n+\n+  This is an implementation of cluster resolvers for Slurm clusters. This allows\n+  the specification of jobs and task counts, number of tasks per node, number of\n+  GPUs on each node and number of GPUs for each task, It retrieves system\n+  attributes by Slurm environment variables, resolve allocated computing node\n+  names, construct a cluster and return a Cluster Resolver object which an be\n+  use for distributed TensorFlow.\n+  \"\"\"\n+\n+  def __init__(self,\n+               jobs,\n+               port_base=8888,\n+               gpus_per_node=1,\n+               gpus_per_task=1,\n+               tasks_per_node=None,\n+               auto_set_gpu=True):\n+    \"\"\"Creates a new SlurmClusterResolver object.\n+\n+    This takes in parameters and creates a SlurmClusterResolver object. It uses\n+    those parameters to determine which nodes will processes reside and resolve\n+    their hostnames. With the number of the GPUs on each node and number of GPUs\n+    for each task it offsets the port number for each processes and allocate\n+    GPUs to tasks by setting environment variables. The resolver currently\n+    supports homogeneous tasks and default Slurm process allocation.\n+\n+    Args:\n+      jobs: Dictionary with job names as key and number of tasks in the job as\n+        value\n+      port_base: The first port number to start with for processes on a node.\n+      gpus_per_node: Number of GPUs avaliable on each node.\n+      gpus_per_task: Number of GPUs to be used for each task.\n+      tasks_per_node: Number of tasks to run on each node, if not set defaults\n+        to Slurm's output environment variable SLURM_NTASKS_PER_NODE.\n+      auto_set_gpu: Set the visible CUDA devices automatically while resolving\n+        the cluster by setting CUDA_VISIBLE_DEVICE environment variable.\n+        Defaults to True.\n+\n+    Raises:\n+      RuntimeError: If requested more GPUs per node then avaliable or requested\n+      more tasks then assigned tasks.\n+    \"\"\"\n+\n+    # check if launched by mpirun\n+    if 'OMPI_COMM_WORLD_RANK' in os.environ:\n+      self._rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n+      num_tasks = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n+    else:\n+      self._rank = int(os.environ['SLURM_PROCID'])\n+      num_tasks = int(os.environ['SLURM_NTASKS'])\n+\n+    self._jobs = jobs\n+    self._port_base = port_base\n+\n+    # user specification overrides SLURM specification\n+    if tasks_per_node is not None:\n+      self._tasks_per_node = tasks_per_node\n+    elif tasks_per_node is None and 'SLURM_NTASKS_PER_NODE' in os.environ:\n+      self._tasks_per_node = int(os.environ['SLURM_NTASKS_PER_NODE'])\n+    else:\n+      raise RuntimeError('Neither `tasks_per_node` or \\\n+                          SLURM_NTASKS_PER_NODE is set')\n+\n+    self._gpus_per_node = gpus_per_node\n+    self._gpus_per_task = gpus_per_task\n+\n+    self._auto_set_gpu = auto_set_gpu\n+    self._job_name = None\n+    self._task_index = None\n+\n+    self._gpu_allocation = []\n+    self._cluster_allocation = {}\n+\n+    if self._tasks_per_node * self._gpus_per_task > self._gpus_per_node:\n+      raise RuntimeError('Requested more GPUs per node then avaliable')\n+\n+    if sum(self._jobs.values()) != num_tasks:\n+      raise RuntimeError('Requested more tasks then assigned tasks')\n+\n+  def cluster_spec(self):\n+    \"\"\"Returns a ClusterSpec object based on the latest instance group info.\n+\n+    This returns a ClusterSpec object for use based on information from the\n+    specified initialization parameters and Slurm environment variables. The\n+    cluster specification is resolved each time this function is called. The\n+    resolver extract hostnames of nodes by scontrol and pack tasks in that\n+    order until a node a has number of tasks that is equal to specification.\n+    GPUs on nodes are allocated to tasks by specification through setting\n+    CUDA_VISIBLE_DEVICE environment variable.\n+\n+    Returns:\n+      A ClusterSpec containing host information retrieved from Slurm's\n+        environment variables.\n+    \"\"\"\n+    hostlist = subprocess.check_output(['scontrol', 'show', 'hostname']).\\\n+               decode(\"utf-8\").strip().split('\\n')", "path": "tensorflow/contrib/cluster_resolver/python/training/slurm_cluster_resolver.py", "position": null, "original_position": 123, "commit_id": "d4d0663c342c8112c741f5a48792ead64edfb39a", "original_commit_id": "7009a1584bed82a860b9b0af946dbbaa72255763", "user": {"login": "frankchn", "id": 691628, "node_id": "MDQ6VXNlcjY5MTYyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/691628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/frankchn", "html_url": "https://github.com/frankchn", "followers_url": "https://api.github.com/users/frankchn/followers", "following_url": "https://api.github.com/users/frankchn/following{/other_user}", "gists_url": "https://api.github.com/users/frankchn/gists{/gist_id}", "starred_url": "https://api.github.com/users/frankchn/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/frankchn/subscriptions", "organizations_url": "https://api.github.com/users/frankchn/orgs", "repos_url": "https://api.github.com/users/frankchn/repos", "events_url": "https://api.github.com/users/frankchn/events{/privacy}", "received_events_url": "https://api.github.com/users/frankchn/received_events", "type": "User", "site_admin": false}, "body": "Be consistent in quote usage (either all `\"` or all `'`)", "created_at": "2018-09-23T07:03:25Z", "updated_at": "2018-10-17T12:00:31Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22423#discussion_r219690150", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22423", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/219690150"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22423#discussion_r219690150"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22423"}}, "body_html": "<p>Be consistent in quote usage (either all <code>\"</code> or all <code>'</code>)</p>", "body_text": "Be consistent in quote usage (either all \" or all ')"}