{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1451", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1451/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1451/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1451/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1451", "id": 139845124, "node_id": "MDU6SXNzdWUxMzk4NDUxMjQ=", "number": 1451, "title": "Is there a way to cache intermediate Tensor result?", "user": {"login": "myme5261314", "id": 1814831, "node_id": "MDQ6VXNlcjE4MTQ4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1814831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myme5261314", "html_url": "https://github.com/myme5261314", "followers_url": "https://api.github.com/users/myme5261314/followers", "following_url": "https://api.github.com/users/myme5261314/following{/other_user}", "gists_url": "https://api.github.com/users/myme5261314/gists{/gist_id}", "starred_url": "https://api.github.com/users/myme5261314/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myme5261314/subscriptions", "organizations_url": "https://api.github.com/users/myme5261314/orgs", "repos_url": "https://api.github.com/users/myme5261314/repos", "events_url": "https://api.github.com/users/myme5261314/events{/privacy}", "received_events_url": "https://api.github.com/users/myme5261314/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-03-10T10:18:08Z", "updated_at": "2016-03-12T00:58:04Z", "closed_at": "2016-03-10T17:28:07Z", "author_association": "NONE", "body_html": "<p>I attempt to implement naive RBM/DBN through tensorflow's python API.<br>\nThe functionality is easy to implement if you don't care about computational performance.</p>\n<p>For now, I've tried many alternatives solutions and find it difficult to implement with high performance since there's no way to cache the intermediate tensor result.</p>\n<p>While lacking of the ability to cache the intermediate result, there will be too much memory migrate from GPU to RAM or vice versa (say I'm using GPU) if I want to do something through intermediate result.</p>\n<p>Here's part of my code to illustrate my meaning.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n    alpha <span class=\"pl-k\">=</span> <span class=\"pl-c1\">.01</span>\n    mnist <span class=\"pl-k\">=</span> input_data.read_data_sets(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MNIST_data/<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">one_hot</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n    trX, trY, teX, teY <span class=\"pl-k\">=</span> mnist.train.images, mnist.train.labels,\\\n        mnist.test.images, mnist.test.labels\n    W <span class=\"pl-k\">=</span> tf.Variable(tf.random_normal([<span class=\"pl-c1\">784</span>, <span class=\"pl-c1\">100</span>]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>weights<span class=\"pl-pds\">\"</span></span>)\n    hb <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">100</span>]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>hbias<span class=\"pl-pds\">\"</span></span>)\n    vb <span class=\"pl-k\">=</span> tf.Variable(tf.zeros([<span class=\"pl-c1\">784</span>]), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vbias<span class=\"pl-pds\">\"</span></span>)\n    v0 <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>float<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">784</span>])\n\n    h0 <span class=\"pl-k\">=</span> tf.nn.sigmoid(tf.matmul(v0, W) <span class=\"pl-k\">+</span> hb)\n    h0 <span class=\"pl-k\">=</span> sample_prob(h0)\n    v1 <span class=\"pl-k\">=</span> tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) <span class=\"pl-k\">+</span> vb)\n    v1 <span class=\"pl-k\">=</span> sample_prob(v1)\n    h1 <span class=\"pl-k\">=</span> tf.nn.sigmoid(tf.matmul(v1, W) <span class=\"pl-k\">+</span> hb)\n\n    pg <span class=\"pl-k\">=</span> tf.matmul(tf.transpose(v0), h0)\n    ng <span class=\"pl-k\">=</span> tf.matmul(tf.transpose(v1), h1)\n\n    dW <span class=\"pl-k\">=</span> (pg <span class=\"pl-k\">-</span> ng) <span class=\"pl-k\">/</span> tf.to_float(tf.shape(v0)[<span class=\"pl-c1\">0</span>])\n    dhb <span class=\"pl-k\">=</span> tf.reduce_mean(h0 <span class=\"pl-k\">-</span> h1, <span class=\"pl-c1\">0</span>)\n    dvb <span class=\"pl-k\">=</span> tf.reduce_mean(v0 <span class=\"pl-k\">-</span> v1, <span class=\"pl-c1\">0</span>)\n\n    update_W_op <span class=\"pl-k\">=</span> W.assign_add(alpha <span class=\"pl-k\">*</span> dW)\n    update_hb_op <span class=\"pl-k\">=</span> hb.assign_add(alpha <span class=\"pl-k\">*</span> dhb)\n    update_vb_op <span class=\"pl-k\">=</span> vb.assign_add(alpha <span class=\"pl-k\">*</span> dvb)\n\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        sess.run(tf.initialize_all_variables())\n        <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n            <span class=\"pl-k\">for</span> start, end <span class=\"pl-k\">in</span> <span class=\"pl-c1\">zip</span>(\n                    <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">50000</span>, <span class=\"pl-c1\">100</span>), <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">50000</span>, <span class=\"pl-c1\">100</span>)):\n                X <span class=\"pl-k\">=</span> trX[start:end]\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.run([dW, dhb, dvb, h1], feed_dict={v0: X})</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.run(h1, feed_dict={v0: X})</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.run(dW)</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.run(dhb)</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.run(dvb)</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> _W = sess.run(update_W_op)</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> _hb = sess.run(update_hb_op)</span>\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> _vb = sess.run(update_vb_op)</span>\n                [_W, _hb, _vb] <span class=\"pl-k\">=</span> sess.run([update_W_op, update_hb_op, update_vb_op], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{v0: X})</pre></div>\n<p>This is the most efficient way I could think out. But, there's no way to optimize the progress by calculating <code>[_W, _hb, _vb]</code> with the aid of intermediate result <code>pg, ng, h0, h1, v0, v1</code>. It's obvious that here comes so much duplicated  but useless extra computation.<br>\nIf feed <code>v0</code> with <code>X</code> once, the follower <code>Tensor</code> need the former <code>Tensor</code>.<br>\n<code>h0</code> needs <code>v0</code>, <strong>so needs <code>X</code></strong><br>\n<code>v1</code> needs  <code>h0</code>, <strong>so needs <code>X</code></strong><br>\n<code>h1</code> needs <code>v1</code>, <strong>so needs <code>X</code></strong><br>\n......<br>\n<code>update_W_op</code> needs <code>dW</code>, <strong>so needs <code>X</code></strong><br>\n<code>update_hb_op</code> needs  <code>dhb</code>, <strong>so needs <code>X</code></strong><br>\n<code>update_vb_op</code> needs <code>dvb</code>, <strong>so needs <code>X</code></strong></p>\n<p>Since there's no way to cache the intermediate result, you can only get one <code>Tensor</code> result for one <code>computation</code>. There's no way to avoid the duplication computation and <code>X</code> memory migration cost.</p>\n<p>I was wondering if there's a property of <code>Tensor</code> say <code>cached = False</code>, we could set it to <code>True</code> manually. And when intermediate cached result is enough to go on, we need not to feed data in, and just using intermediate cache to compute. If you want to flush the cache, just feed in the new data into the placeholder. But for now, if the graph contains <strong>placeholder</strong> in the path, the data must be feed in specifically.</p>\n<p>For example, if the cache is enabled.</p>\n<div class=\"highlight highlight-source-python\"><pre>h0.cached <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nh1.cached <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nv0.cached <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nv1.cached <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\nsess.run(h1, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{v0: X})\nsess.run([update_W_op, update_hb_op, update_vb_op])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> using cache h0,h1,v0,v1 is enough to compute the three update without X fed in.</span></pre></div>", "body_text": "I attempt to implement naive RBM/DBN through tensorflow's python API.\nThe functionality is easy to implement if you don't care about computational performance.\nFor now, I've tried many alternatives solutions and find it difficult to implement with high performance since there's no way to cache the intermediate tensor result.\nWhile lacking of the ability to cache the intermediate result, there will be too much memory migrate from GPU to RAM or vice versa (say I'm using GPU) if I want to do something through intermediate result.\nHere's part of my code to illustrate my meaning.\n    import tensorflow as tf\n    alpha = .01\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n    trX, trY, teX, teY = mnist.train.images, mnist.train.labels,\\\n        mnist.test.images, mnist.test.labels\n    W = tf.Variable(tf.random_normal([784, 100]), name=\"weights\")\n    hb = tf.Variable(tf.zeros([100]), name=\"hbias\")\n    vb = tf.Variable(tf.zeros([784]), name=\"vbias\")\n    v0 = tf.placeholder(\"float\", [None, 784])\n\n    h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n    h0 = sample_prob(h0)\n    v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb)\n    v1 = sample_prob(v1)\n    h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)\n\n    pg = tf.matmul(tf.transpose(v0), h0)\n    ng = tf.matmul(tf.transpose(v1), h1)\n\n    dW = (pg - ng) / tf.to_float(tf.shape(v0)[0])\n    dhb = tf.reduce_mean(h0 - h1, 0)\n    dvb = tf.reduce_mean(v0 - v1, 0)\n\n    update_W_op = W.assign_add(alpha * dW)\n    update_hb_op = hb.assign_add(alpha * dhb)\n    update_vb_op = vb.assign_add(alpha * dvb)\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        for _ in range(10):\n            for start, end in zip(\n                    range(0, 50000, 100), range(100, 50000, 100)):\n                X = trX[start:end]\n                # sess.run([dW, dhb, dvb, h1], feed_dict={v0: X})\n                # sess.run(h1, feed_dict={v0: X})\n                # sess.run(dW)\n                # sess.run(dhb)\n                # sess.run(dvb)\n                # _W = sess.run(update_W_op)\n                # _hb = sess.run(update_hb_op)\n                # _vb = sess.run(update_vb_op)\n                [_W, _hb, _vb] = sess.run([update_W_op, update_hb_op, update_vb_op], feed_dict={v0: X})\nThis is the most efficient way I could think out. But, there's no way to optimize the progress by calculating [_W, _hb, _vb] with the aid of intermediate result pg, ng, h0, h1, v0, v1. It's obvious that here comes so much duplicated  but useless extra computation.\nIf feed v0 with X once, the follower Tensor need the former Tensor.\nh0 needs v0, so needs X\nv1 needs  h0, so needs X\nh1 needs v1, so needs X\n......\nupdate_W_op needs dW, so needs X\nupdate_hb_op needs  dhb, so needs X\nupdate_vb_op needs dvb, so needs X\nSince there's no way to cache the intermediate result, you can only get one Tensor result for one computation. There's no way to avoid the duplication computation and X memory migration cost.\nI was wondering if there's a property of Tensor say cached = False, we could set it to True manually. And when intermediate cached result is enough to go on, we need not to feed data in, and just using intermediate cache to compute. If you want to flush the cache, just feed in the new data into the placeholder. But for now, if the graph contains placeholder in the path, the data must be feed in specifically.\nFor example, if the cache is enabled.\nh0.cached = True\nh1.cached = True\nv0.cached = True\nv1.cached = True\nsess.run(h1, feed_dict={v0: X})\nsess.run([update_W_op, update_hb_op, update_vb_op])  # using cache h0,h1,v0,v1 is enough to compute the three update without X fed in.", "body": "I attempt to implement naive RBM/DBN through tensorflow's python API.\nThe functionality is easy to implement if you don't care about computational performance.\n\nFor now, I've tried many alternatives solutions and find it difficult to implement with high performance since there's no way to cache the intermediate tensor result.\n\nWhile lacking of the ability to cache the intermediate result, there will be too much memory migrate from GPU to RAM or vice versa (say I'm using GPU) if I want to do something through intermediate result.\n\nHere's part of my code to illustrate my meaning.\n\n``` python\n    import tensorflow as tf\n    alpha = .01\n    mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n    trX, trY, teX, teY = mnist.train.images, mnist.train.labels,\\\n        mnist.test.images, mnist.test.labels\n    W = tf.Variable(tf.random_normal([784, 100]), name=\"weights\")\n    hb = tf.Variable(tf.zeros([100]), name=\"hbias\")\n    vb = tf.Variable(tf.zeros([784]), name=\"vbias\")\n    v0 = tf.placeholder(\"float\", [None, 784])\n\n    h0 = tf.nn.sigmoid(tf.matmul(v0, W) + hb)\n    h0 = sample_prob(h0)\n    v1 = tf.nn.sigmoid(tf.matmul(h0, tf.transpose(W)) + vb)\n    v1 = sample_prob(v1)\n    h1 = tf.nn.sigmoid(tf.matmul(v1, W) + hb)\n\n    pg = tf.matmul(tf.transpose(v0), h0)\n    ng = tf.matmul(tf.transpose(v1), h1)\n\n    dW = (pg - ng) / tf.to_float(tf.shape(v0)[0])\n    dhb = tf.reduce_mean(h0 - h1, 0)\n    dvb = tf.reduce_mean(v0 - v1, 0)\n\n    update_W_op = W.assign_add(alpha * dW)\n    update_hb_op = hb.assign_add(alpha * dhb)\n    update_vb_op = vb.assign_add(alpha * dvb)\n\n    with tf.Session() as sess:\n        sess.run(tf.initialize_all_variables())\n        for _ in range(10):\n            for start, end in zip(\n                    range(0, 50000, 100), range(100, 50000, 100)):\n                X = trX[start:end]\n                # sess.run([dW, dhb, dvb, h1], feed_dict={v0: X})\n                # sess.run(h1, feed_dict={v0: X})\n                # sess.run(dW)\n                # sess.run(dhb)\n                # sess.run(dvb)\n                # _W = sess.run(update_W_op)\n                # _hb = sess.run(update_hb_op)\n                # _vb = sess.run(update_vb_op)\n                [_W, _hb, _vb] = sess.run([update_W_op, update_hb_op, update_vb_op], feed_dict={v0: X})\n```\n\nThis is the most efficient way I could think out. But, there's no way to optimize the progress by calculating `[_W, _hb, _vb]` with the aid of intermediate result `pg, ng, h0, h1, v0, v1`. It's obvious that here comes so much duplicated  but useless extra computation.\nIf feed `v0` with `X` once, the follower `Tensor` need the former `Tensor`.\n`h0` needs `v0`, **so needs `X`**\n`v1` needs  `h0`, **so needs `X`**\n`h1` needs `v1`, **so needs `X`**\n......\n`update_W_op` needs `dW`, **so needs `X`**\n`update_hb_op` needs  `dhb`, **so needs `X`**\n`update_vb_op` needs `dvb`, **so needs `X`**\n\nSince there's no way to cache the intermediate result, you can only get one `Tensor` result for one `computation`. There's no way to avoid the duplication computation and `X` memory migration cost.\n\nI was wondering if there's a property of `Tensor` say `cached = False`, we could set it to `True` manually. And when intermediate cached result is enough to go on, we need not to feed data in, and just using intermediate cache to compute. If you want to flush the cache, just feed in the new data into the placeholder. But for now, if the graph contains **placeholder** in the path, the data must be feed in specifically.\n\nFor example, if the cache is enabled.\n\n``` python\nh0.cached = True\nh1.cached = True\nv0.cached = True\nv1.cached = True\nsess.run(h1, feed_dict={v0: X})\nsess.run([update_W_op, update_hb_op, update_vb_op])  # using cache h0,h1,v0,v1 is enough to compute the three update without X fed in.\n```\n"}