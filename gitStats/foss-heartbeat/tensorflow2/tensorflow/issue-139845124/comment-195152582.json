{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/195152582", "html_url": "https://github.com/tensorflow/tensorflow/issues/1451#issuecomment-195152582", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1451", "id": 195152582, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NTE1MjU4Mg==", "user": {"login": "myme5261314", "id": 1814831, "node_id": "MDQ6VXNlcjE4MTQ4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1814831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/myme5261314", "html_url": "https://github.com/myme5261314", "followers_url": "https://api.github.com/users/myme5261314/followers", "following_url": "https://api.github.com/users/myme5261314/following{/other_user}", "gists_url": "https://api.github.com/users/myme5261314/gists{/gist_id}", "starred_url": "https://api.github.com/users/myme5261314/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/myme5261314/subscriptions", "organizations_url": "https://api.github.com/users/myme5261314/orgs", "repos_url": "https://api.github.com/users/myme5261314/repos", "events_url": "https://api.github.com/users/myme5261314/events{/privacy}", "received_events_url": "https://api.github.com/users/myme5261314/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-11T02:37:48Z", "updated_at": "2016-03-11T02:37:48Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> I have no idea the C++ code implementation for <code>paritial_run</code>, but there seems kind of memory mallloc and release problem.</p>\n<p>Here's the code I use with <code>partial_run</code> as you suggest:</p>\n<div class=\"highlight highlight-source-python\"><pre>h <span class=\"pl-k\">=</span> sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\n<span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(epoches):\n    <span class=\"pl-k\">for</span> batch <span class=\"pl-k\">in</span> batches:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> h = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])</span>\n        X <span class=\"pl-k\">=</span> batch\n        sess.partial_run(h, h1, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{v0: X})\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.partial_run(h, h0, feed_dict={v0: X})</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.partial_run(h, v1)</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> sess.partial_run(h, h1)</span>\n        sess.partial_run(h, dW)\n        sess.partial_run(h, dhb)\n        sess.partial_run(h, dvb)\n        _W <span class=\"pl-k\">=</span> sess.partial_run(h, update_W_op)\n        _hb <span class=\"pl-k\">=</span> sess.partial_run(h, update_hb_op)\n        _vb <span class=\"pl-k\">=</span> sess.partial_run(h, update_vb_op)</pre></div>\n<p>This code will give the following error when calculating <code>h1</code> in the second iteration.</p>\n<pre><code>tensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: The feed Placeholder:0 had already been fed.\n</code></pre>\n<p>When I change <code>partial_run_setup</code> to the comment position, it will run out of GPU memory after several tens of seconds with a segment fault under Ubuntu. During the calculating, the utility of GPU is never higher than 5% according to nvidia-smi monitoring.</p>\n<p>So, any idea about how to solve this? Or could only wait until the API is stable?</p>", "body_text": "@girving I have no idea the C++ code implementation for paritial_run, but there seems kind of memory mallloc and release problem.\nHere's the code I use with partial_run as you suggest:\nh = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\nfor i in range(epoches):\n    for batch in batches:\n        # h = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\n        X = batch\n        sess.partial_run(h, h1, feed_dict={v0: X})\n        # sess.partial_run(h, h0, feed_dict={v0: X})\n        # sess.partial_run(h, v1)\n        # sess.partial_run(h, h1)\n        sess.partial_run(h, dW)\n        sess.partial_run(h, dhb)\n        sess.partial_run(h, dvb)\n        _W = sess.partial_run(h, update_W_op)\n        _hb = sess.partial_run(h, update_hb_op)\n        _vb = sess.partial_run(h, update_vb_op)\nThis code will give the following error when calculating h1 in the second iteration.\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: The feed Placeholder:0 had already been fed.\n\nWhen I change partial_run_setup to the comment position, it will run out of GPU memory after several tens of seconds with a segment fault under Ubuntu. During the calculating, the utility of GPU is never higher than 5% according to nvidia-smi monitoring.\nSo, any idea about how to solve this? Or could only wait until the API is stable?", "body": "@girving I have no idea the C++ code implementation for `paritial_run`, but there seems kind of memory mallloc and release problem.\n\nHere's the code I use with `partial_run` as you suggest:\n\n``` python\nh = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\nfor i in range(epoches):\n    for batch in batches:\n        # h = sess.partial_run_setup([h0, v1, h1, dW, dhb, dvb, update_W_op, update_hb_op, update_vb_op], [v0])\n        X = batch\n        sess.partial_run(h, h1, feed_dict={v0: X})\n        # sess.partial_run(h, h0, feed_dict={v0: X})\n        # sess.partial_run(h, v1)\n        # sess.partial_run(h, h1)\n        sess.partial_run(h, dW)\n        sess.partial_run(h, dhb)\n        sess.partial_run(h, dvb)\n        _W = sess.partial_run(h, update_W_op)\n        _hb = sess.partial_run(h, update_hb_op)\n        _vb = sess.partial_run(h, update_vb_op)\n```\n\nThis code will give the following error when calculating `h1` in the second iteration.\n\n```\ntensorflow.python.pywrap_tensorflow.StatusNotOK: Invalid argument: The feed Placeholder:0 had already been fed.\n```\n\nWhen I change `partial_run_setup` to the comment position, it will run out of GPU memory after several tens of seconds with a segment fault under Ubuntu. During the calculating, the utility of GPU is never higher than 5% according to nvidia-smi monitoring.\n\nSo, any idea about how to solve this? Or could only wait until the API is stable?\n"}