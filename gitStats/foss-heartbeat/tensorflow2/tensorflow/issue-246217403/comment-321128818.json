{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321128818", "html_url": "https://github.com/tensorflow/tensorflow/issues/11839#issuecomment-321128818", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11839", "id": 321128818, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTEyODgxOA==", "user": {"login": "gauss-clb", "id": 11674304, "node_id": "MDQ6VXNlcjExNjc0MzA0", "avatar_url": "https://avatars2.githubusercontent.com/u/11674304?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gauss-clb", "html_url": "https://github.com/gauss-clb", "followers_url": "https://api.github.com/users/gauss-clb/followers", "following_url": "https://api.github.com/users/gauss-clb/following{/other_user}", "gists_url": "https://api.github.com/users/gauss-clb/gists{/gist_id}", "starred_url": "https://api.github.com/users/gauss-clb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gauss-clb/subscriptions", "organizations_url": "https://api.github.com/users/gauss-clb/orgs", "repos_url": "https://api.github.com/users/gauss-clb/repos", "events_url": "https://api.github.com/users/gauss-clb/events{/privacy}", "received_events_url": "https://api.github.com/users/gauss-clb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-09T01:44:35Z", "updated_at": "2017-08-09T01:47:15Z", "author_association": "NONE", "body_html": "<p>I think it maybe a bug.<br>\nLook at the code:</p>\n<pre><code>x = tf.Variable(12.,name='x')\ny = tf.Variable(34.,name='y')\nz = tf.Variable(0, trainable=False)\nema = tf.train.ExponentialMovingAverage(.0)\nema.apply([x])\nprint(ema.variables_to_restore())\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    saver = tf.train.Saver()\n    saver.save(sess, './model.ckpt')\n\n</code></pre>\n<p>First, I save the variables to check point.</p>\n<pre><code>x = tf.Variable(12.,name='x')\ny = tf.Variable(34.,name='y')\nz = tf.Variable(0, trainable=False)\nema = tf.train.ExponentialMovingAverage(.0)\nema.apply([x])\nprint(ema.variables_to_restore())\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    saver = tf.train.Saver(ema.variables_to_restore())\n    saver.restore(sess, './model.ckpt')\n</code></pre>\n<p>Then, I want to restore variables from the check point. I want to load x with x_shadow, load y with y, and load z with z. But it will raise a error. Because <code>ema.variables_to_restore()</code> will map <code>y_shadow_name</code> to y, but there is no key of <code>y_shadow_name</code> in the check point, and z is untrainable variable, so it can be loaded correctly.</p>\n<p>So I think the purpose of  <code>variables_to_restore()</code> is to load moving average variables with shadow, such as x, but don't want to load non-moving average variables with shadow, such as y.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a></p>", "body_text": "I think it maybe a bug.\nLook at the code:\nx = tf.Variable(12.,name='x')\ny = tf.Variable(34.,name='y')\nz = tf.Variable(0, trainable=False)\nema = tf.train.ExponentialMovingAverage(.0)\nema.apply([x])\nprint(ema.variables_to_restore())\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    saver = tf.train.Saver()\n    saver.save(sess, './model.ckpt')\n\n\nFirst, I save the variables to check point.\nx = tf.Variable(12.,name='x')\ny = tf.Variable(34.,name='y')\nz = tf.Variable(0, trainable=False)\nema = tf.train.ExponentialMovingAverage(.0)\nema.apply([x])\nprint(ema.variables_to_restore())\n\ninit = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    init.run()\n    saver = tf.train.Saver(ema.variables_to_restore())\n    saver.restore(sess, './model.ckpt')\n\nThen, I want to restore variables from the check point. I want to load x with x_shadow, load y with y, and load z with z. But it will raise a error. Because ema.variables_to_restore() will map y_shadow_name to y, but there is no key of y_shadow_name in the check point, and z is untrainable variable, so it can be loaded correctly.\nSo I think the purpose of  variables_to_restore() is to load moving average variables with shadow, such as x, but don't want to load non-moving average variables with shadow, such as y.\n@reedwm", "body": "I think it maybe a bug.\r\nLook at the code:\r\n\r\n```\r\nx = tf.Variable(12.,name='x')\r\ny = tf.Variable(34.,name='y')\r\nz = tf.Variable(0, trainable=False)\r\nema = tf.train.ExponentialMovingAverage(.0)\r\nema.apply([x])\r\nprint(ema.variables_to_restore())\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    saver = tf.train.Saver()\r\n    saver.save(sess, './model.ckpt')\r\n\r\n```\r\nFirst, I save the variables to check point.\r\n\r\n```\r\nx = tf.Variable(12.,name='x')\r\ny = tf.Variable(34.,name='y')\r\nz = tf.Variable(0, trainable=False)\r\nema = tf.train.ExponentialMovingAverage(.0)\r\nema.apply([x])\r\nprint(ema.variables_to_restore())\r\n\r\ninit = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    init.run()\r\n    saver = tf.train.Saver(ema.variables_to_restore())\r\n    saver.restore(sess, './model.ckpt')\r\n```\r\n\r\nThen, I want to restore variables from the check point. I want to load x with x_shadow, load y with y, and load z with z. But it will raise a error. Because `ema.variables_to_restore()` will map `y_shadow_name` to y, but there is no key of `y_shadow_name` in the check point, and z is untrainable variable, so it can be loaded correctly.\r\n\r\nSo I think the purpose of  `variables_to_restore()` is to load moving average variables with shadow, such as x, but don't want to load non-moving average variables with shadow, such as y.\r\n\r\n@reedwm \r\n\r\n"}