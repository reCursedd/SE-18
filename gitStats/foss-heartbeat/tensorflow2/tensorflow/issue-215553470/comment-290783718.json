{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290783718", "html_url": "https://github.com/tensorflow/tensorflow/pull/8565#issuecomment-290783718", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8565", "id": 290783718, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDc4MzcxOA==", "user": {"login": "jakelee8", "id": 19253212, "node_id": "MDQ6VXNlcjE5MjUzMjEy", "avatar_url": "https://avatars3.githubusercontent.com/u/19253212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakelee8", "html_url": "https://github.com/jakelee8", "followers_url": "https://api.github.com/users/jakelee8/followers", "following_url": "https://api.github.com/users/jakelee8/following{/other_user}", "gists_url": "https://api.github.com/users/jakelee8/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakelee8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakelee8/subscriptions", "organizations_url": "https://api.github.com/users/jakelee8/orgs", "repos_url": "https://api.github.com/users/jakelee8/repos", "events_url": "https://api.github.com/users/jakelee8/events{/privacy}", "received_events_url": "https://api.github.com/users/jakelee8/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-31T17:57:37Z", "updated_at": "2017-03-31T17:57:37Z", "author_association": "NONE", "body_html": "<p>I believe the failures are due to lack of precision for float16.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The dynamic range of fp16 is too limited to support the collection of</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> sufficient statistics. As a workaround we simply perform the operations</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> on 32-bit floats before converting the mean and variance back to fp16</span>\n    y <span class=\"pl-k\">=</span> math_ops.cast(x, dtypes.float32) <span class=\"pl-k\">if</span> x.dtype <span class=\"pl-k\">==</span> dtypes.float16 <span class=\"pl-k\">else</span> x</pre></div>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634</a></p>\n<p>Fix options:</p>\n<ul>\n<li>Lower numerical tolerance when testing for float16; hides practical limits of float16 for batch norm. <g-emoji class=\"g-emoji\" alias=\"-1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44e.png\">\ud83d\udc4e</g-emoji></li>\n<li>Use float32 for running stats; converting float32 to float16 to apply batch norm still has problems with precision. <g-emoji class=\"g-emoji\" alias=\"-1\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f44e.png\">\ud83d\udc4e</g-emoji></li>\n<li>???</li>\n</ul>\n<p>Suggestions?</p>\n<p>P.S. Low priority pull request: I stopped using float16 in favor of smaller models/batches w/ multi-gpu setup. Works <g-emoji class=\"g-emoji\" alias=\"100\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f4af.png\">\ud83d\udcaf</g-emoji> for fully automated lung segmentation!</p>", "body_text": "I believe the failures are due to lack of precision for float16.\n    # The dynamic range of fp16 is too limited to support the collection of\n    # sufficient statistics. As a workaround we simply perform the operations\n    # on 32-bit floats before converting the mean and variance back to fp16\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\nFix options:\n\nLower numerical tolerance when testing for float16; hides practical limits of float16 for batch norm. \ud83d\udc4e\nUse float32 for running stats; converting float32 to float16 to apply batch norm still has problems with precision. \ud83d\udc4e\n???\n\nSuggestions?\nP.S. Low priority pull request: I stopped using float16 in favor of smaller models/batches w/ multi-gpu setup. Works \ud83d\udcaf for fully automated lung segmentation!", "body": "I believe the failures are due to lack of precision for float16.\r\n\r\n```python\r\n    # The dynamic range of fp16 is too limited to support the collection of\r\n    # sufficient statistics. As a workaround we simply perform the operations\r\n    # on 32-bit floats before converting the mean and variance back to fp16\r\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\r\n\r\nFix options:\r\n\r\n- Lower numerical tolerance when testing for float16; hides practical limits of float16 for batch norm. \ud83d\udc4e \r\n- Use float32 for running stats; converting float32 to float16 to apply batch norm still has problems with precision. \ud83d\udc4e \r\n- ???\r\n\r\nSuggestions?\r\n\r\nP.S. Low priority pull request: I stopped using float16 in favor of smaller models/batches w/ multi-gpu setup. Works \ud83d\udcaf for fully automated lung segmentation!"}