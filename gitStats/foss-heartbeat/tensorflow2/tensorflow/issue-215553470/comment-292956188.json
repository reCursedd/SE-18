{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/292956188", "html_url": "https://github.com/tensorflow/tensorflow/pull/8565#issuecomment-292956188", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8565", "id": 292956188, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Mjk1NjE4OA==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-10T13:54:57Z", "updated_at": "2017-04-10T13:54:57Z", "author_association": "MEMBER", "body_html": "<p>This to me sounds like getting this to work well with float16 will be more subtle than simply updating the dtypes. Maybe it's worth going back to float32 and changing each individual tensor to float16 to see which ones lead to breakage, and seeing then if it's possible to go around that somehow?</p>", "body_text": "This to me sounds like getting this to work well with float16 will be more subtle than simply updating the dtypes. Maybe it's worth going back to float32 and changing each individual tensor to float16 to see which ones lead to breakage, and seeing then if it's possible to go around that somehow?", "body": "This to me sounds like getting this to work well with float16 will be more subtle than simply updating the dtypes. Maybe it's worth going back to float32 and changing each individual tensor to float16 to see which ones lead to breakage, and seeing then if it's possible to go around that somehow?"}