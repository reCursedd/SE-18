{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293124693", "html_url": "https://github.com/tensorflow/tensorflow/pull/8565#issuecomment-293124693", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8565", "id": 293124693, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzEyNDY5Mw==", "user": {"login": "jakelee8", "id": 19253212, "node_id": "MDQ6VXNlcjE5MjUzMjEy", "avatar_url": "https://avatars3.githubusercontent.com/u/19253212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakelee8", "html_url": "https://github.com/jakelee8", "followers_url": "https://api.github.com/users/jakelee8/followers", "following_url": "https://api.github.com/users/jakelee8/following{/other_user}", "gists_url": "https://api.github.com/users/jakelee8/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakelee8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakelee8/subscriptions", "organizations_url": "https://api.github.com/users/jakelee8/orgs", "repos_url": "https://api.github.com/users/jakelee8/repos", "events_url": "https://api.github.com/users/jakelee8/events{/privacy}", "received_events_url": "https://api.github.com/users/jakelee8/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-11T01:36:46Z", "updated_at": "2017-04-11T01:36:46Z", "author_association": "NONE", "body_html": "<p>Unfortunately, simply changing a subset of the variable types is not a simple fix either.</p>\n<p>The original issue was that when the batch norm variables did not match the input tensor type, e.g. <code>float16</code> inputs vs <code>float32</code> variables, Tensorflow would throw a type error. We've seen that simply using <code>float16</code> runs into numerical precision issues.</p>\n<p>Casting <code>float32</code> to <code>float16</code> has issues with numerical precision as well. I don't know how truncating <code>float32</code> to <code>float16</code> would work out, seems like a rabbit hole I'd rather avoid...</p>\n<p>For reference, 4 variables are defined for <code>batch_norm</code>:</p>\n<ul>\n<li><code>beta</code></li>\n<li><code>gamma</code></li>\n<li><code>moving_mean</code></li>\n<li><code>moving_variance</code></li>\n</ul>\n<p>P.S. Implementing evolutionary learning as described by Salimans et al. (2017) sounds more promising in terms of the original goal to make more GPU memory available to build larger models. Salimans et al. estimated in their <a href=\"https://www.technologyreview.com/s/603921/elon-musks-openai-unveils-a-simpler-way-for-machines-to-learn/\" rel=\"nofollow\">talk</a> that not calculating gradients means models take up to 1/3 the memory. This is 33% better memory efficiency that a conversion to float16 would offer, assuming float16 models take 50% less memory than float32 (<code>1. - .33/.5 = .33</code>).</p>\n<p>In terms of performance, the Salisman et al. implementation scales linearly with additional nodes, something Tensorflow already support.</p>", "body_text": "Unfortunately, simply changing a subset of the variable types is not a simple fix either.\nThe original issue was that when the batch norm variables did not match the input tensor type, e.g. float16 inputs vs float32 variables, Tensorflow would throw a type error. We've seen that simply using float16 runs into numerical precision issues.\nCasting float32 to float16 has issues with numerical precision as well. I don't know how truncating float32 to float16 would work out, seems like a rabbit hole I'd rather avoid...\nFor reference, 4 variables are defined for batch_norm:\n\nbeta\ngamma\nmoving_mean\nmoving_variance\n\nP.S. Implementing evolutionary learning as described by Salimans et al. (2017) sounds more promising in terms of the original goal to make more GPU memory available to build larger models. Salimans et al. estimated in their talk that not calculating gradients means models take up to 1/3 the memory. This is 33% better memory efficiency that a conversion to float16 would offer, assuming float16 models take 50% less memory than float32 (1. - .33/.5 = .33).\nIn terms of performance, the Salisman et al. implementation scales linearly with additional nodes, something Tensorflow already support.", "body": "Unfortunately, simply changing a subset of the variable types is not a simple fix either.\r\n\r\nThe original issue was that when the batch norm variables did not match the input tensor type, e.g. `float16` inputs vs `float32` variables, Tensorflow would throw a type error. We've seen that simply using `float16` runs into numerical precision issues.\r\n\r\nCasting `float32` to `float16` has issues with numerical precision as well. I don't know how truncating `float32` to `float16` would work out, seems like a rabbit hole I'd rather avoid...\r\n\r\nFor reference, 4 variables are defined for `batch_norm`:\r\n\r\n- `beta`\r\n- `gamma`\r\n- `moving_mean`\r\n- `moving_variance`\r\n\r\nP.S. Implementing evolutionary learning as described by Salimans et al. (2017) sounds more promising in terms of the original goal to make more GPU memory available to build larger models. Salimans et al. estimated in their [talk](https://www.technologyreview.com/s/603921/elon-musks-openai-unveils-a-simpler-way-for-machines-to-learn/) that not calculating gradients means models take up to 1/3 the memory. This is 33% better memory efficiency that a conversion to float16 would offer, assuming float16 models take 50% less memory than float32 (`1. - .33/.5 = .33`).\r\n\r\nIn terms of performance, the Salisman et al. implementation scales linearly with additional nodes, something Tensorflow already support."}