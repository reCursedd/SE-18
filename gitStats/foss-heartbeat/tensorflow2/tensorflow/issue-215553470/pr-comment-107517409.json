{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107517409", "pull_request_review_id": 28495702, "id": 107517409, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEwNzUxNzQwOQ==", "diff_hunk": "@@ -2553,6 +2641,18 @@ def testNHWCAndNCHWTrainingProduceSameOutput(self):\n       self.assertAllClose(nhwc, nchw, atol=1e-4, rtol=1e-4)\n \n \n+class BatchNormFloat16Test(BatchNormTest):\n+  dtype = dtypes.float16\n+\n+\n+class BatchNormFloat32Test(BatchNormTest):\n+  dtype = dtypes.float32\n+\n+\n+class BatchNormFloat64Test(BatchNormTest):\n+  dtype = dtypes.float64\n+\n+\n class LayerNormTest(test.TestCase):", "path": "tensorflow/contrib/layers/python/layers/layers_test.py", "position": null, "original_position": 507, "commit_id": "4ab59a1b524d39d3d4f5d6f1b66b46d91dfa0d2a", "original_commit_id": "46fab39bbc281ee86aa30c5f450691d6a8475236", "user": {"login": "jakelee8", "id": 19253212, "node_id": "MDQ6VXNlcjE5MjUzMjEy", "avatar_url": "https://avatars3.githubusercontent.com/u/19253212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakelee8", "html_url": "https://github.com/jakelee8", "followers_url": "https://api.github.com/users/jakelee8/followers", "following_url": "https://api.github.com/users/jakelee8/following{/other_user}", "gists_url": "https://api.github.com/users/jakelee8/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakelee8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakelee8/subscriptions", "organizations_url": "https://api.github.com/users/jakelee8/orgs", "repos_url": "https://api.github.com/users/jakelee8/repos", "events_url": "https://api.github.com/users/jakelee8/events{/privacy}", "received_events_url": "https://api.github.com/users/jakelee8/received_events", "type": "User", "site_admin": false}, "body": "I originally updated the tests as you suggested. However, when I ran all the test cases for float16, one important test failed. I think float16 is not supported due to lack of precision? Description of the issue from #8535 reproduced below.\r\n\r\nI do agree that 3x the cost is quite substantial, especially since most people use float32. Though compared to how long it took to build Tensorflow, the cost seem negligible, but that's not to say I advocate always increasing testing costs.\r\n\r\nWould you think that only enabling full combinatorial testing before general releases to a more reasonable approach? Given how many changes I needed to parameterize BatchNormTest, it would likely take a good chunk of time to support optional combinatorial testing with types.\r\n\r\n---\r\n\r\nThere seems to be an issue for supporting `float16` concerning moment calculation. Non-fused BatchNorm uses `moments()` internally and `moments()` has this caveat. Except from source code follows.\r\n\r\n```py\r\n    # The dynamic range of fp16 is too limited to support the collection of\r\n    # sufficient statistics. As a workaround we simply perform the operations\r\n    # on 32-bit floats before converting the mean and variance back to fp16\r\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\r\n\r\n\r\nRunning `layers_test.py` with support for `float16` and `float32` BatchNorm resulted in the following numerical errors. The `float16` moving mean values are incorrect.\r\n", "created_at": "2017-03-22T20:04:56Z", "updated_at": "2017-04-09T12:31:57Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8565#discussion_r107517409", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8565", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/107517409"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8565#discussion_r107517409"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8565"}}, "body_html": "<p>I originally updated the tests as you suggested. However, when I ran all the test cases for float16, one important test failed. I think float16 is not supported due to lack of precision? Description of the issue from <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"215273304\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/8535\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/8535/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/8535\">#8535</a> reproduced below.</p>\n<p>I do agree that 3x the cost is quite substantial, especially since most people use float32. Though compared to how long it took to build Tensorflow, the cost seem negligible, but that's not to say I advocate always increasing testing costs.</p>\n<p>Would you think that only enabling full combinatorial testing before general releases to a more reasonable approach? Given how many changes I needed to parameterize BatchNormTest, it would likely take a good chunk of time to support optional combinatorial testing with types.</p>\n<hr>\n<p>There seems to be an issue for supporting <code>float16</code> concerning moment calculation. Non-fused BatchNorm uses <code>moments()</code> internally and <code>moments()</code> has this caveat. Except from source code follows.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The dynamic range of fp16 is too limited to support the collection of</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> sufficient statistics. As a workaround we simply perform the operations</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> on 32-bit floats before converting the mean and variance back to fp16</span>\n    y <span class=\"pl-k\">=</span> math_ops.cast(x, dtypes.float32) <span class=\"pl-k\">if</span> x.dtype <span class=\"pl-k\">==</span> dtypes.float16 <span class=\"pl-k\">else</span> x</pre></div>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634</a></p>\n<p>Running <code>layers_test.py</code> with support for <code>float16</code> and <code>float32</code> BatchNorm resulted in the following numerical errors. The <code>float16</code> moving mean values are incorrect.</p>", "body_text": "I originally updated the tests as you suggested. However, when I ran all the test cases for float16, one important test failed. I think float16 is not supported due to lack of precision? Description of the issue from #8535 reproduced below.\nI do agree that 3x the cost is quite substantial, especially since most people use float32. Though compared to how long it took to build Tensorflow, the cost seem negligible, but that's not to say I advocate always increasing testing costs.\nWould you think that only enabling full combinatorial testing before general releases to a more reasonable approach? Given how many changes I needed to parameterize BatchNormTest, it would likely take a good chunk of time to support optional combinatorial testing with types.\n\nThere seems to be an issue for supporting float16 concerning moment calculation. Non-fused BatchNorm uses moments() internally and moments() has this caveat. Except from source code follows.\n    # The dynamic range of fp16 is too limited to support the collection of\n    # sufficient statistics. As a workaround we simply perform the operations\n    # on 32-bit floats before converting the mean and variance back to fp16\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\nRunning layers_test.py with support for float16 and float32 BatchNorm resulted in the following numerical errors. The float16 moving mean values are incorrect.", "in_reply_to_id": 107454467}