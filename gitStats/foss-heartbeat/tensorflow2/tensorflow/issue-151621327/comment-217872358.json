{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217872358", "html_url": "https://github.com/tensorflow/tensorflow/issues/2150#issuecomment-217872358", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2150", "id": 217872358, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzg3MjM1OA==", "user": {"login": "alrojo", "id": 12167999, "node_id": "MDQ6VXNlcjEyMTY3OTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/12167999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alrojo", "html_url": "https://github.com/alrojo", "followers_url": "https://api.github.com/users/alrojo/followers", "following_url": "https://api.github.com/users/alrojo/following{/other_user}", "gists_url": "https://api.github.com/users/alrojo/gists{/gist_id}", "starred_url": "https://api.github.com/users/alrojo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alrojo/subscriptions", "organizations_url": "https://api.github.com/users/alrojo/orgs", "repos_url": "https://api.github.com/users/alrojo/repos", "events_url": "https://api.github.com/users/alrojo/events{/privacy}", "received_events_url": "https://api.github.com/users/alrojo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-09T14:00:34Z", "updated_at": "2016-05-09T14:00:34Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> I agree with the gates, also they would only work for basic GRU/LSTM.</p>\n<p>I have updated the code based on your comments (though omitted fail-safes for clarifying functionality).</p>\n<p>Would this be compatible with your plans on <code>GRUCell</code> and <code>BasicLSTMCell</code>?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">GRUCell</span>(<span class=\"pl-e\">rnn_cell</span>.<span class=\"pl-e\">RNNCell</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).<span class=\"pl-pds\">\"\"\"</span></span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n               <span class=\"pl-smi\">reset_W_in</span> <span class=\"pl-k\">=</span> init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">reset_W_hid</span> <span class=\"pl-k\">=</span> init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">reset_b</span> <span class=\"pl-k\">=</span> init_ops.constant_initializer(<span class=\"pl-c1\">0</span>.),\n               <span class=\"pl-smi\">reset_activation</span> <span class=\"pl-k\">=</span> sigmoid, \n               <span class=\"pl-smi\">update_W_in</span> <span class=\"pl-k\">=</span> init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">update_W_hid</span> <span class=\"pl-k\">=</span> init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">update_b</span> <span class=\"pl-k\">=</span> init_ops.constant_initializer(<span class=\"pl-c1\">0</span>.),\n               <span class=\"pl-smi\">update_activation</span> <span class=\"pl-k\">=</span> sigmoid,\n               <span class=\"pl-smi\">candidate_W_in</span> <span class=\"pl-k\">=</span> init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">candidate_W_hid</span> <span class=\"pl-k\">=</span> init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">candidate_b</span> <span class=\"pl-k\">=</span> init_ops.constant_initializer(<span class=\"pl-c1\">0</span>.),\n               <span class=\"pl-smi\">candidate_activation</span> <span class=\"pl-k\">=</span> tanh):\n    <span class=\"pl-c1\">self</span>._num_units <span class=\"pl-k\">=</span> num_units\n    <span class=\"pl-c1\">self</span>._input_size <span class=\"pl-k\">=</span> num_units <span class=\"pl-k\">if</span> input_size <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> input_size\n    <span class=\"pl-c1\">self</span>._reset_W_in <span class=\"pl-k\">=</span> reset_W_in\n    <span class=\"pl-c1\">self</span>._reset_W_hid <span class=\"pl-k\">=</span> reset_W_hid\n    <span class=\"pl-c1\">self</span>._reset_b <span class=\"pl-k\">=</span> reset_b\n    <span class=\"pl-c1\">self</span>._reset_activation <span class=\"pl-k\">=</span> reset_activation\n    <span class=\"pl-c1\">self</span>._update_W_in <span class=\"pl-k\">=</span> update_W_in\n    <span class=\"pl-c1\">self</span>._update_W_hid <span class=\"pl-k\">=</span> update_W_hid\n    <span class=\"pl-c1\">self</span>._update_b <span class=\"pl-k\">=</span> update_b\n    <span class=\"pl-c1\">self</span>._update_activation <span class=\"pl-k\">=</span> update_activation\n    <span class=\"pl-c1\">self</span>._candidate_W_in <span class=\"pl-k\">=</span> candidate_W_in\n    <span class=\"pl-c1\">self</span>._candidate_W_hid <span class=\"pl-k\">=</span> candidate_W_hid\n    <span class=\"pl-c1\">self</span>._candidate_b <span class=\"pl-k\">=</span> candidate_b\n    <span class=\"pl-c1\">self</span>._candidate_activation <span class=\"pl-k\">=</span> candidate_activation\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._input_size\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">output_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_compute</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">args</span>, <span class=\"pl-smi\">gate</span>):\n    name, W_in_init, W_hid_init, b_init <span class=\"pl-k\">=</span> gate\n    <span class=\"pl-k\">with</span> vs.variable_scope(name):\n      W_in <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W_in<span class=\"pl-pds\">\"</span></span>,\n        [args[<span class=\"pl-c1\">0</span>].get_shape()[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">self</span>._num_units],\n        <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>W_in_init)\n      W_hid <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W_hid<span class=\"pl-pds\">\"</span></span>,\n        [args[<span class=\"pl-c1\">1</span>].get_shape()[<span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">self</span>._num_units],\n        <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>W_hid_init)\n      b <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Bias<span class=\"pl-pds\">\"</span></span>, [<span class=\"pl-c1\">self</span>._num_units],\n        <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>b_init)\n      matrix <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">0</span>, [W_in, W_hid])\n    <span class=\"pl-k\">return</span> matrix, b\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gated recurrent unit (GRU) with nunits cells.<span class=\"pl-pds\">\"\"\"</span></span>\n    args <span class=\"pl-k\">=</span> [inputs, state]\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__name__</span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> \"GRUCell\"</span>\n      <span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Gates<span class=\"pl-pds\">\"</span></span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reset gate and update gate.</span>\n        matrices <span class=\"pl-k\">=</span> []\n        biases <span class=\"pl-k\">=</span> []\n        gates <span class=\"pl-k\">=</span> [\n          (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Reset<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">self</span>._reset_W_in, <span class=\"pl-c1\">self</span>._reset_W_hid, <span class=\"pl-c1\">self</span>._reset_b),\n          (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Update<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">self</span>._update_W_in, <span class=\"pl-c1\">self</span>._update_W_hid, <span class=\"pl-c1\">self</span>._update_b)]\n        <span class=\"pl-k\">for</span> gate <span class=\"pl-k\">in</span> gates:\n          matrix, bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._compute(args, gate)\n          matrices.append(matrix)\n          biases.append(bias)\n        total_matrix <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">1</span>, matrices)\n        total_bias <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">0</span>, biases)\n        res_gates <span class=\"pl-k\">=</span> math_ops.matmul(array_ops.concat(<span class=\"pl-c1\">1</span>, args), total_matrix)\n        res_gates <span class=\"pl-k\">+=</span> total_bias\n        r, u <span class=\"pl-k\">=</span> array_ops.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, res_gates)\n        r, u <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._reset_activation(r), <span class=\"pl-c1\">self</span>._update_activation(u)\n      <span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Candidate<span class=\"pl-pds\">\"</span></span>):\n        candidate <span class=\"pl-k\">=</span> (<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Candidate<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">self</span>._candidate_W_in, <span class=\"pl-c1\">self</span>._candidate_W_hid,\n            <span class=\"pl-c1\">self</span>._candidate_b)\n        matrix, bias <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._compute([inputs, r <span class=\"pl-k\">*</span> state], candidate)\n        c <span class=\"pl-k\">=</span> math_ops.matmul(array_ops.concat(<span class=\"pl-c1\">1</span>, args), matrix) <span class=\"pl-k\">+</span> bias\n        c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._candidate_activation(c)\n      new_h <span class=\"pl-k\">=</span> u <span class=\"pl-k\">*</span> state <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> u) <span class=\"pl-k\">*</span> c\n    <span class=\"pl-k\">return</span> new_h, new_h</pre></div>", "body_text": "@ebrevdo I agree with the gates, also they would only work for basic GRU/LSTM.\nI have updated the code based on your comments (though omitted fail-safes for clarifying functionality).\nWould this be compatible with your plans on GRUCell and BasicLSTMCell?\nclass GRUCell(rnn_cell.RNNCell):\n  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n\n  def __init__(self, num_units, input_size=None,\n               reset_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               reset_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               reset_b = init_ops.constant_initializer(0.),\n               reset_activation = sigmoid, \n               update_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               update_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               update_b = init_ops.constant_initializer(0.),\n               update_activation = sigmoid,\n               candidate_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               candidate_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               candidate_b = init_ops.constant_initializer(0.),\n               candidate_activation = tanh):\n    self._num_units = num_units\n    self._input_size = num_units if input_size is None else input_size\n    self._reset_W_in = reset_W_in\n    self._reset_W_hid = reset_W_hid\n    self._reset_b = reset_b\n    self._reset_activation = reset_activation\n    self._update_W_in = update_W_in\n    self._update_W_hid = update_W_hid\n    self._update_b = update_b\n    self._update_activation = update_activation\n    self._candidate_W_in = candidate_W_in\n    self._candidate_W_hid = candidate_W_hid\n    self._candidate_b = candidate_b\n    self._candidate_activation = candidate_activation\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def _compute(self, args, gate):\n    name, W_in_init, W_hid_init, b_init = gate\n    with vs.variable_scope(name):\n      W_in = vs.get_variable(\"W_in\",\n        [args[0].get_shape()[1], self._num_units],\n        initializer=W_in_init)\n      W_hid = vs.get_variable(\"W_hid\",\n        [args[1].get_shape()[1], self._num_units],\n        initializer=W_hid_init)\n      b = vs.get_variable(\"Bias\", [self._num_units],\n        initializer=b_init)\n      matrix = array_ops.concat(0, [W_in, W_hid])\n    return matrix, b\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n    args = [inputs, state]\n    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n        matrices = []\n        biases = []\n        gates = [\n          (\"Reset\", self._reset_W_in, self._reset_W_hid, self._reset_b),\n          (\"Update\", self._update_W_in, self._update_W_hid, self._update_b)]\n        for gate in gates:\n          matrix, bias = self._compute(args, gate)\n          matrices.append(matrix)\n          biases.append(bias)\n        total_matrix = array_ops.concat(1, matrices)\n        total_bias = array_ops.concat(0, biases)\n        res_gates = math_ops.matmul(array_ops.concat(1, args), total_matrix)\n        res_gates += total_bias\n        r, u = array_ops.split(1, 2, res_gates)\n        r, u = self._reset_activation(r), self._update_activation(u)\n      with vs.variable_scope(\"Candidate\"):\n        candidate = (\"Candidate\", self._candidate_W_in, self._candidate_W_hid,\n            self._candidate_b)\n        matrix, bias = self._compute([inputs, r * state], candidate)\n        c = math_ops.matmul(array_ops.concat(1, args), matrix) + bias\n        c = self._candidate_activation(c)\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h", "body": "@ebrevdo I agree with the gates, also they would only work for basic GRU/LSTM.\n\nI have updated the code based on your comments (though omitted fail-safes for clarifying functionality).\n\nWould this be compatible with your plans on `GRUCell` and `BasicLSTMCell`?\n\n``` python\nclass GRUCell(rnn_cell.RNNCell):\n  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n\n  def __init__(self, num_units, input_size=None,\n               reset_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               reset_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               reset_b = init_ops.constant_initializer(0.),\n               reset_activation = sigmoid, \n               update_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               update_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               update_b = init_ops.constant_initializer(0.),\n               update_activation = sigmoid,\n               candidate_W_in = init_ops.random_normal_initializer(stddev=0.1),\n               candidate_W_hid = init_ops.random_normal_initializer(stddev=0.1),\n               candidate_b = init_ops.constant_initializer(0.),\n               candidate_activation = tanh):\n    self._num_units = num_units\n    self._input_size = num_units if input_size is None else input_size\n    self._reset_W_in = reset_W_in\n    self._reset_W_hid = reset_W_hid\n    self._reset_b = reset_b\n    self._reset_activation = reset_activation\n    self._update_W_in = update_W_in\n    self._update_W_hid = update_W_hid\n    self._update_b = update_b\n    self._update_activation = update_activation\n    self._candidate_W_in = candidate_W_in\n    self._candidate_W_hid = candidate_W_hid\n    self._candidate_b = candidate_b\n    self._candidate_activation = candidate_activation\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def _compute(self, args, gate):\n    name, W_in_init, W_hid_init, b_init = gate\n    with vs.variable_scope(name):\n      W_in = vs.get_variable(\"W_in\",\n        [args[0].get_shape()[1], self._num_units],\n        initializer=W_in_init)\n      W_hid = vs.get_variable(\"W_hid\",\n        [args[1].get_shape()[1], self._num_units],\n        initializer=W_hid_init)\n      b = vs.get_variable(\"Bias\", [self._num_units],\n        initializer=b_init)\n      matrix = array_ops.concat(0, [W_in, W_hid])\n    return matrix, b\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n    args = [inputs, state]\n    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n        matrices = []\n        biases = []\n        gates = [\n          (\"Reset\", self._reset_W_in, self._reset_W_hid, self._reset_b),\n          (\"Update\", self._update_W_in, self._update_W_hid, self._update_b)]\n        for gate in gates:\n          matrix, bias = self._compute(args, gate)\n          matrices.append(matrix)\n          biases.append(bias)\n        total_matrix = array_ops.concat(1, matrices)\n        total_bias = array_ops.concat(0, biases)\n        res_gates = math_ops.matmul(array_ops.concat(1, args), total_matrix)\n        res_gates += total_bias\n        r, u = array_ops.split(1, 2, res_gates)\n        r, u = self._reset_activation(r), self._update_activation(u)\n      with vs.variable_scope(\"Candidate\"):\n        candidate = (\"Candidate\", self._candidate_W_in, self._candidate_W_hid,\n            self._candidate_b)\n        matrix, bias = self._compute([inputs, r * state], candidate)\n        c = math_ops.matmul(array_ops.concat(1, args), matrix) + bias\n        c = self._candidate_activation(c)\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h\n```\n"}