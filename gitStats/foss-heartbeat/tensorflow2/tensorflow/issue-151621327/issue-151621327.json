{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2150", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2150/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2150/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2150/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2150", "id": 151621327, "node_id": "MDU6SXNzdWUxNTE2MjEzMjc=", "number": 2150, "title": "rnn_cell.py improvements", "user": {"login": "alrojo", "id": 12167999, "node_id": "MDQ6VXNlcjEyMTY3OTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/12167999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alrojo", "html_url": "https://github.com/alrojo", "followers_url": "https://api.github.com/users/alrojo/followers", "following_url": "https://api.github.com/users/alrojo/following{/other_user}", "gists_url": "https://api.github.com/users/alrojo/gists{/gist_id}", "starred_url": "https://api.github.com/users/alrojo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alrojo/subscriptions", "organizations_url": "https://api.github.com/users/alrojo/orgs", "repos_url": "https://api.github.com/users/alrojo/repos", "events_url": "https://api.github.com/users/alrojo/events{/privacy}", "received_events_url": "https://api.github.com/users/alrojo/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-04-28T11:47:34Z", "updated_at": "2017-10-11T05:26:22Z", "closed_at": "2016-06-17T03:32:28Z", "author_association": "CONTRIBUTOR", "body_html": "<h2>Motivation</h2>\n<p>Current implementation of <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\">rnn_cell.py</a> does not support custom initialization on a gate and input-to-hidden/hidden-to-hidden level (like setting forgetgate bias to 0 while leaving updategate bias at 1, etc.). Further when debugging the gates in TensorBoard, the matrix is represented as one large matrix, which makes it difficult to see whats happening inside a specific matrix (e.g. hidden-to-hidden) of a specific gate (e.g. forgetgate).</p>\n<p>In recurrent neural networks (RNNs) GRU and LSTM uses various gates with separate weights, for both hidden-to-hidden and input-to-hidden, to computing steps in a recurrent sequence.<br>\nTo optimize computational speed these gates, and their separate weights, are often stacked and computed simultaneous at every step.<br>\nIn TensorFlows rnn_cell.py the <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L123\">GRUCell</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L153\">BasicLSTMCell</a> are implemented using <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L677\">linear</a> to handle weigths and computation hereof.<br>\nHowever, the implementation of <em>linear</em> does not initialize separate matrices for each gate, but initializes the gates, and their input-to-hidden/hidden-to-hidden matrices, as one big matrix for <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L712\">weights</a> and <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L719\">bias</a>.</p>\n<h2>Proposal</h2>\n<p>Implementing separate initialization of gates, and their input-to-hidden/hidden-to-hidden matrices, and concatenating these gates. This allows custom initializatio, TensorBoard information on hid_in/hid_hid/bias for every gate and still retains the advantage of weights in a large matrix.</p>\n<h2>Implementation</h2>\n<p>With minimal rewriting of the current structure in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py\">rnn_cell.py</a> I implemented a <a href=\"https://lasagne.readthedocs.org/en/latest/modules/layers/recurrent.html#lasagne.layers.Gate\" rel=\"nofollow\">lasagne-gate</a> like structure, made a new <em>linear</em> function and made some minor changes to <em>GRUCell</em>.<br>\nAll of these changes should, with minor modifications, work for <em>BasicLSTM</em> as well.</p>\n<p>Do notice that code below is for my own purpose, it is not rigorously tested yet.</p>\n<p>First, the gate to hold initialization for every weight matrix/bias in a gate (notice it also handles LSTM's by having W_cell)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">Gate</span>(<span class=\"pl-c1\">object</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gate to handle to handle initialization<span class=\"pl-pds\">\"\"\"</span></span>  \n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">W_in</span><span class=\"pl-k\">=</span>init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">W_hid</span><span class=\"pl-k\">=</span>init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">W_cell</span><span class=\"pl-k\">=</span>init_ops.random_normal_initializer(<span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>),\n               <span class=\"pl-smi\">b</span><span class=\"pl-k\">=</span>init_ops.constant_initializer(<span class=\"pl-c1\">0</span>.),\n               <span class=\"pl-smi\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-c1\">self</span>.W_in <span class=\"pl-k\">=</span> W_in\n    <span class=\"pl-c1\">self</span>.W_hid <span class=\"pl-k\">=</span> W_hid\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Don't store a cell weight vector when cell is None</span>\n    <span class=\"pl-k\">if</span> W_cell <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c1\">self</span>.W_cell <span class=\"pl-k\">=</span> W_cell\n    <span class=\"pl-k\">if</span> b <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n      <span class=\"pl-c1\">self</span>.b <span class=\"pl-k\">=</span> b\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> For the activation, if None is supplied, use identity</span>\n    <span class=\"pl-k\">if</span> activation <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n        <span class=\"pl-c1\">self</span>.activation <span class=\"pl-k\">=</span> control_flow_ops.identity\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c1\">self</span>.activation <span class=\"pl-k\">=</span> activation</pre></div>\n<p>A modified GRU cell to handle weigths (took only minimal modification to <strong>init</strong> and <strong>call</strong>)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">GRUCell</span>(<span class=\"pl-e\">rnn_cell</span>.<span class=\"pl-e\">RNNCell</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).<span class=\"pl-pds\">\"\"\"</span></span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">input_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n               <span class=\"pl-smi\">resetgate</span><span class=\"pl-k\">=</span>Gate(<span class=\"pl-v\">W_cell</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>sigmoid),\n               <span class=\"pl-smi\">updategate</span><span class=\"pl-k\">=</span>Gate(<span class=\"pl-v\">W_cell</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>sigmoid),\n               <span class=\"pl-smi\">candidategate</span><span class=\"pl-k\">=</span>Gate(<span class=\"pl-v\">W_cell</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tanh)):\n    <span class=\"pl-c1\">self</span>._num_units <span class=\"pl-k\">=</span> num_units\n    <span class=\"pl-c1\">self</span>._input_size <span class=\"pl-k\">=</span> num_units <span class=\"pl-k\">if</span> input_size <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">else</span> input_size\n    <span class=\"pl-c1\">self</span>._resetgate <span class=\"pl-k\">=</span> resetgate\n    <span class=\"pl-c1\">self</span>._updategate <span class=\"pl-k\">=</span> updategate\n    <span class=\"pl-c1\">self</span>._candidategate <span class=\"pl-k\">=</span> candidategate\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">input_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._input_size\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">output_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n  <span class=\"pl-en\">@</span><span class=\"pl-c1\">property</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">state_size</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._num_units\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__call__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">state</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Gated recurrent unit (GRU) with nunits cells.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-c1\">type</span>(<span class=\"pl-c1\">self</span>).<span class=\"pl-c1\">__name__</span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> \"GRUCell\"</span>\n      <span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Gates<span class=\"pl-pds\">\"</span></span>):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Reset gate and update gate.</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> We start with bias of 1.0 to not reset and not update.</span>\n        r, u <span class=\"pl-k\">=</span> array_ops.split(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, Modified_linear([inputs, state],\n          [(<span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Reset<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">self</span>._resetgate),\n           (<span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Update<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">self</span>._updategate)]))\n        r, u <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._resetgate.activation(r), <span class=\"pl-c1\">self</span>._updategate.activation(u)\n      <span class=\"pl-k\">with</span> vs.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Candidate<span class=\"pl-pds\">\"</span></span>):\n        c <span class=\"pl-k\">=</span> Modified_linear([inputs, r <span class=\"pl-k\">*</span> state],\n          (<span class=\"pl-c1\">self</span>._num_units, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Candidate<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-c1\">self</span>._candidategate))\n        c <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._candidategate.activation(c)\n      new_h <span class=\"pl-k\">=</span> u <span class=\"pl-k\">*</span> state <span class=\"pl-k\">+</span> (<span class=\"pl-c1\">1</span> <span class=\"pl-k\">-</span> u) <span class=\"pl-k\">*</span> c\n    <span class=\"pl-k\">return</span> new_h, new_h</pre></div>\n<p>I found <em>linear</em> required the largest amount of rewriting, however I have tried to keep the original structure and functionality intact.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">Modified_linear</span>(<span class=\"pl-smi\">args</span>, <span class=\"pl-smi\">output</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Modified linear takes args and output.</span>\n<span class=\"pl-s\">     Args is same as in linear, but output is a tuple consisting of:</span>\n<span class=\"pl-s\">     output_size, name of gate, gate object (with all initializations)</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n  <span class=\"pl-k\">if</span> args <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">or</span> (<span class=\"pl-c1\">isinstance</span>(args, (<span class=\"pl-c1\">list</span>, <span class=\"pl-c1\">tuple</span>)) <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> args):\n    <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>`args` must be specified<span class=\"pl-pds\">\"</span></span>)\n  <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(args, (<span class=\"pl-c1\">list</span>, <span class=\"pl-c1\">tuple</span>)):\n    args <span class=\"pl-k\">=</span> [args]\n  <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">isinstance</span>(output, <span class=\"pl-c1\">list</span>):\n    output <span class=\"pl-k\">=</span> [output]\n  shapes <span class=\"pl-k\">=</span> [a.get_shape().as_list() <span class=\"pl-k\">for</span> a <span class=\"pl-k\">in</span> args]\n  <span class=\"pl-k\">for</span> shape <span class=\"pl-k\">in</span> shapes:\n    <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(shape) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">2</span>:\n      <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Linear is expecting 2D arguments: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">str</span>(shapes))\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> shape[<span class=\"pl-c1\">1</span>]:\n      <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Linear expects shape[1] of arguments: <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> <span class=\"pl-c1\">str</span>(shapes))\n\n  matrices <span class=\"pl-k\">=</span> []\n  biases <span class=\"pl-k\">=</span> []\n  <span class=\"pl-k\">with</span> vs.variable_scope(scope <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Linear<span class=\"pl-pds\">\"</span></span>):\n    <span class=\"pl-k\">for</span> output_size, name, gate <span class=\"pl-k\">in</span> output: <span class=\"pl-c\"><span class=\"pl-c\">#</span> loops over every gate</span>\n      <span class=\"pl-k\">with</span> vs.variable_scope(name):\n        W_in <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W_in<span class=\"pl-pds\">\"</span></span>, [args[<span class=\"pl-c1\">0</span>].get_shape()[<span class=\"pl-c1\">1</span>], output_size],\n          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>gate.W_in)\n        W_hid <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W_hid<span class=\"pl-pds\">\"</span></span>, [args[<span class=\"pl-c1\">1</span>].get_shape()[<span class=\"pl-c1\">1</span>], output_size],\n          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>gate.W_hid)\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">hasattr</span>(gate, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>b<span class=\"pl-pds\">'</span></span>):\n          b <span class=\"pl-k\">=</span> vs.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Bias<span class=\"pl-pds\">\"</span></span>, [output_size],\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>gate.b)\n          biases.append(b)\n        <span class=\"pl-k\">if</span> <span class=\"pl-c1\">hasattr</span>(gate, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>W_cell<span class=\"pl-pds\">\"</span></span>):\n          <span class=\"pl-k\">pass</span>\n          <span class=\"pl-c\"><span class=\"pl-c\">#</span> do some LSTM stuff ...</span>\n        <span class=\"pl-k\">else</span>:\n          matrix <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">0</span>, [W_in, W_hid]) <span class=\"pl-c\"><span class=\"pl-c\">#</span> concats all matrices</span>\n        matrices.append(matrix)\n\n  total_matrix <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">1</span>, matrices) <span class=\"pl-c\"><span class=\"pl-c\">#</span> concats across gates</span>\n  res <span class=\"pl-k\">=</span> math_ops.matmul(array_ops.concat(<span class=\"pl-c1\">1</span>, args), total_matrix) <span class=\"pl-c\"><span class=\"pl-c\">#</span> computes the results</span>\n\n  <span class=\"pl-k\">if</span> biases <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> []:\n    total_bias <span class=\"pl-k\">=</span> array_ops.concat(<span class=\"pl-c1\">0</span>, biases) <span class=\"pl-c\"><span class=\"pl-c\">#</span> concats across gates biases</span>\n    <span class=\"pl-k\">if</span> total_matrix.get_shape()[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">!=</span> total_bias.get_shape()[<span class=\"pl-c1\">0</span>]:\n      <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Must have same output dimensions for W and b<span class=\"pl-pds\">'</span></span>)\n    res <span class=\"pl-k\">+=</span> total_bias\n  <span class=\"pl-k\">return</span> res</pre></div>\n<h2>Questions</h2>\n<ul>\n<li>Would this be of interest for a PR to <em>rnn_cell.py</em>? (given further development of code and <em>BasicLSTMCell</em> implementation)</li>\n<li>General comments/thoughts would be much appreciated</li>\n</ul>", "body_text": "Motivation\nCurrent implementation of rnn_cell.py does not support custom initialization on a gate and input-to-hidden/hidden-to-hidden level (like setting forgetgate bias to 0 while leaving updategate bias at 1, etc.). Further when debugging the gates in TensorBoard, the matrix is represented as one large matrix, which makes it difficult to see whats happening inside a specific matrix (e.g. hidden-to-hidden) of a specific gate (e.g. forgetgate).\nIn recurrent neural networks (RNNs) GRU and LSTM uses various gates with separate weights, for both hidden-to-hidden and input-to-hidden, to computing steps in a recurrent sequence.\nTo optimize computational speed these gates, and their separate weights, are often stacked and computed simultaneous at every step.\nIn TensorFlows rnn_cell.py the GRUCell and BasicLSTMCell are implemented using linear to handle weigths and computation hereof.\nHowever, the implementation of linear does not initialize separate matrices for each gate, but initializes the gates, and their input-to-hidden/hidden-to-hidden matrices, as one big matrix for weights and bias.\nProposal\nImplementing separate initialization of gates, and their input-to-hidden/hidden-to-hidden matrices, and concatenating these gates. This allows custom initializatio, TensorBoard information on hid_in/hid_hid/bias for every gate and still retains the advantage of weights in a large matrix.\nImplementation\nWith minimal rewriting of the current structure in rnn_cell.py I implemented a lasagne-gate like structure, made a new linear function and made some minor changes to GRUCell.\nAll of these changes should, with minor modifications, work for BasicLSTM as well.\nDo notice that code below is for my own purpose, it is not rigorously tested yet.\nFirst, the gate to hold initialization for every weight matrix/bias in a gate (notice it also handles LSTM's by having W_cell)\nclass Gate(object):\n  \"\"\"Gate to handle to handle initialization\"\"\"  \n\n  def __init__(self, W_in=init_ops.random_normal_initializer(stddev=0.1),\n               W_hid=init_ops.random_normal_initializer(stddev=0.1),\n               W_cell=init_ops.random_normal_initializer(stddev=0.1),\n               b=init_ops.constant_initializer(0.),\n               activation=None):\n    self.W_in = W_in\n    self.W_hid = W_hid\n    # Don't store a cell weight vector when cell is None\n    if W_cell is not None:\n        self.W_cell = W_cell\n    if b is not None:\n      self.b = b\n    # For the activation, if None is supplied, use identity\n    if activation is None:\n        self.activation = control_flow_ops.identity\n    else:\n        self.activation = activation\nA modified GRU cell to handle weigths (took only minimal modification to init and call)\nclass GRUCell(rnn_cell.RNNCell):\n  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n\n  def __init__(self, num_units, input_size=None,\n               resetgate=Gate(W_cell=None, activation=sigmoid),\n               updategate=Gate(W_cell=None, activation=sigmoid),\n               candidategate=Gate(W_cell=None, activation=tanh)):\n    self._num_units = num_units\n    self._input_size = num_units if input_size is None else input_size\n    self._resetgate = resetgate\n    self._updategate = updategate\n    self._candidategate = candidategate\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n        # We start with bias of 1.0 to not reset and not update.\n        r, u = array_ops.split(1, 2, Modified_linear([inputs, state],\n          [(self._num_units, \"Reset\", self._resetgate),\n           (self._num_units, \"Update\", self._updategate)]))\n        r, u = self._resetgate.activation(r), self._updategate.activation(u)\n      with vs.variable_scope(\"Candidate\"):\n        c = Modified_linear([inputs, r * state],\n          (self._num_units, \"Candidate\", self._candidategate))\n        c = self._candidategate.activation(c)\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h\nI found linear required the largest amount of rewriting, however I have tried to keep the original structure and functionality intact.\ndef Modified_linear(args, output, scope=None):\n  \"\"\"Modified linear takes args and output.\n     Args is same as in linear, but output is a tuple consisting of:\n     output_size, name of gate, gate object (with all initializations)\n  \"\"\"\n  if args is None or (isinstance(args, (list, tuple)) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not isinstance(args, (list, tuple)):\n    args = [args]\n  if not isinstance(output, list):\n    output = [output]\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n\n  matrices = []\n  biases = []\n  with vs.variable_scope(scope or \"Linear\"):\n    for output_size, name, gate in output: # loops over every gate\n      with vs.variable_scope(name):\n        W_in = vs.get_variable(\"W_in\", [args[0].get_shape()[1], output_size],\n          initializer=gate.W_in)\n        W_hid = vs.get_variable(\"W_hid\", [args[1].get_shape()[1], output_size],\n          initializer=gate.W_hid)\n        if hasattr(gate, 'b'):\n          b = vs.get_variable(\"Bias\", [output_size],\n            initializer=gate.b)\n          biases.append(b)\n        if hasattr(gate, \"W_cell\"):\n          pass\n          # do some LSTM stuff ...\n        else:\n          matrix = array_ops.concat(0, [W_in, W_hid]) # concats all matrices\n        matrices.append(matrix)\n\n  total_matrix = array_ops.concat(1, matrices) # concats across gates\n  res = math_ops.matmul(array_ops.concat(1, args), total_matrix) # computes the results\n\n  if biases is not []:\n    total_bias = array_ops.concat(0, biases) # concats across gates biases\n    if total_matrix.get_shape()[1] != total_bias.get_shape()[0]:\n      raise ValueError('Must have same output dimensions for W and b')\n    res += total_bias\n  return res\nQuestions\n\nWould this be of interest for a PR to rnn_cell.py? (given further development of code and BasicLSTMCell implementation)\nGeneral comments/thoughts would be much appreciated", "body": "## Motivation\n\nCurrent implementation of [rnn_cell.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py) does not support custom initialization on a gate and input-to-hidden/hidden-to-hidden level (like setting forgetgate bias to 0 while leaving updategate bias at 1, etc.). Further when debugging the gates in TensorBoard, the matrix is represented as one large matrix, which makes it difficult to see whats happening inside a specific matrix (e.g. hidden-to-hidden) of a specific gate (e.g. forgetgate).\n\nIn recurrent neural networks (RNNs) GRU and LSTM uses various gates with separate weights, for both hidden-to-hidden and input-to-hidden, to computing steps in a recurrent sequence.\nTo optimize computational speed these gates, and their separate weights, are often stacked and computed simultaneous at every step.\nIn TensorFlows rnn_cell.py the [GRUCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L123) and [BasicLSTMCell](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L153) are implemented using [linear](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L677) to handle weigths and computation hereof.\nHowever, the implementation of _linear_ does not initialize separate matrices for each gate, but initializes the gates, and their input-to-hidden/hidden-to-hidden matrices, as one big matrix for [weights](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L712) and [bias](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py#L719).\n## Proposal\n\nImplementing separate initialization of gates, and their input-to-hidden/hidden-to-hidden matrices, and concatenating these gates. This allows custom initializatio, TensorBoard information on hid_in/hid_hid/bias for every gate and still retains the advantage of weights in a large matrix.\n## Implementation\n\nWith minimal rewriting of the current structure in [rnn_cell.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/rnn_cell.py) I implemented a [lasagne-gate](https://lasagne.readthedocs.org/en/latest/modules/layers/recurrent.html#lasagne.layers.Gate) like structure, made a new _linear_ function and made some minor changes to _GRUCell_.\nAll of these changes should, with minor modifications, work for _BasicLSTM_ as well.\n\nDo notice that code below is for my own purpose, it is not rigorously tested yet.\n\nFirst, the gate to hold initialization for every weight matrix/bias in a gate (notice it also handles LSTM's by having W_cell)\n\n``` python\nclass Gate(object):\n  \"\"\"Gate to handle to handle initialization\"\"\"  \n\n  def __init__(self, W_in=init_ops.random_normal_initializer(stddev=0.1),\n               W_hid=init_ops.random_normal_initializer(stddev=0.1),\n               W_cell=init_ops.random_normal_initializer(stddev=0.1),\n               b=init_ops.constant_initializer(0.),\n               activation=None):\n    self.W_in = W_in\n    self.W_hid = W_hid\n    # Don't store a cell weight vector when cell is None\n    if W_cell is not None:\n        self.W_cell = W_cell\n    if b is not None:\n      self.b = b\n    # For the activation, if None is supplied, use identity\n    if activation is None:\n        self.activation = control_flow_ops.identity\n    else:\n        self.activation = activation\n```\n\nA modified GRU cell to handle weigths (took only minimal modification to **init** and **call**)\n\n``` python\nclass GRUCell(rnn_cell.RNNCell):\n  \"\"\"Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078).\"\"\"\n\n  def __init__(self, num_units, input_size=None,\n               resetgate=Gate(W_cell=None, activation=sigmoid),\n               updategate=Gate(W_cell=None, activation=sigmoid),\n               candidategate=Gate(W_cell=None, activation=tanh)):\n    self._num_units = num_units\n    self._input_size = num_units if input_size is None else input_size\n    self._resetgate = resetgate\n    self._updategate = updategate\n    self._candidategate = candidategate\n\n  @property\n  def input_size(self):\n    return self._input_size\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  @property\n  def state_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Gated recurrent unit (GRU) with nunits cells.\"\"\"\n    with vs.variable_scope(scope or type(self).__name__):  # \"GRUCell\"\n      with vs.variable_scope(\"Gates\"):  # Reset gate and update gate.\n        # We start with bias of 1.0 to not reset and not update.\n        r, u = array_ops.split(1, 2, Modified_linear([inputs, state],\n          [(self._num_units, \"Reset\", self._resetgate),\n           (self._num_units, \"Update\", self._updategate)]))\n        r, u = self._resetgate.activation(r), self._updategate.activation(u)\n      with vs.variable_scope(\"Candidate\"):\n        c = Modified_linear([inputs, r * state],\n          (self._num_units, \"Candidate\", self._candidategate))\n        c = self._candidategate.activation(c)\n      new_h = u * state + (1 - u) * c\n    return new_h, new_h\n```\n\nI found _linear_ required the largest amount of rewriting, however I have tried to keep the original structure and functionality intact.\n\n``` python\ndef Modified_linear(args, output, scope=None):\n  \"\"\"Modified linear takes args and output.\n     Args is same as in linear, but output is a tuple consisting of:\n     output_size, name of gate, gate object (with all initializations)\n  \"\"\"\n  if args is None or (isinstance(args, (list, tuple)) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not isinstance(args, (list, tuple)):\n    args = [args]\n  if not isinstance(output, list):\n    output = [output]\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n\n  matrices = []\n  biases = []\n  with vs.variable_scope(scope or \"Linear\"):\n    for output_size, name, gate in output: # loops over every gate\n      with vs.variable_scope(name):\n        W_in = vs.get_variable(\"W_in\", [args[0].get_shape()[1], output_size],\n          initializer=gate.W_in)\n        W_hid = vs.get_variable(\"W_hid\", [args[1].get_shape()[1], output_size],\n          initializer=gate.W_hid)\n        if hasattr(gate, 'b'):\n          b = vs.get_variable(\"Bias\", [output_size],\n            initializer=gate.b)\n          biases.append(b)\n        if hasattr(gate, \"W_cell\"):\n          pass\n          # do some LSTM stuff ...\n        else:\n          matrix = array_ops.concat(0, [W_in, W_hid]) # concats all matrices\n        matrices.append(matrix)\n\n  total_matrix = array_ops.concat(1, matrices) # concats across gates\n  res = math_ops.matmul(array_ops.concat(1, args), total_matrix) # computes the results\n\n  if biases is not []:\n    total_bias = array_ops.concat(0, biases) # concats across gates biases\n    if total_matrix.get_shape()[1] != total_bias.get_shape()[0]:\n      raise ValueError('Must have same output dimensions for W and b')\n    res += total_bias\n  return res\n```\n## Questions\n- Would this be of interest for a PR to _rnn_cell.py_? (given further development of code and _BasicLSTMCell_ implementation)\n- General comments/thoughts would be much appreciated\n"}