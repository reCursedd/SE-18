{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20873", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20873/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20873/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20873/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20873", "id": 341799025, "node_id": "MDU6SXNzdWUzNDE3OTkwMjU=", "number": 20873, "title": "Will tensorflow support 'PMI' like process management protocol for large scale cluster mode? ", "user": {"login": "lilbedwin", "id": 4852870, "node_id": "MDQ6VXNlcjQ4NTI4NzA=", "avatar_url": "https://avatars0.githubusercontent.com/u/4852870?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lilbedwin", "html_url": "https://github.com/lilbedwin", "followers_url": "https://api.github.com/users/lilbedwin/followers", "following_url": "https://api.github.com/users/lilbedwin/following{/other_user}", "gists_url": "https://api.github.com/users/lilbedwin/gists{/gist_id}", "starred_url": "https://api.github.com/users/lilbedwin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lilbedwin/subscriptions", "organizations_url": "https://api.github.com/users/lilbedwin/orgs", "repos_url": "https://api.github.com/users/lilbedwin/repos", "events_url": "https://api.github.com/users/lilbedwin/events{/privacy}", "received_events_url": "https://api.github.com/users/lilbedwin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-17T07:33:06Z", "updated_at": "2018-07-23T01:25:38Z", "closed_at": "2018-07-18T18:27:08Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<p>not related.</p>\n<h3>Describe the problem</h3>\n<p>I'm  trying to use distributed tensorflow in a very large scale mode, say, more that 5000 nodes.</p>\n<p>As the distributed tf using guide described in: <a href=\"https://www.tensorflow.org/deploy/distributed\" rel=\"nofollow\">https://www.tensorflow.org/deploy/distributed</a>, I have to launch the python script by giving it a very long parameter list,  which is used to specify the endpoint of each node in the cluster including itself. just like:</p>\n<p><code>python train.py --worker_hosts=\"ip1:port1,ip2:port2,...\" --ps_hosts=\"ip1:port1,ip2:port2,...\"</code></p>\n<p>This list will be very long in large scale mode, and may cause stack overflow. Because the stack used to store parameters is limited in size.</p>\n<p>Have the tf developers considered to add support for PMI (refer to: <a href=\"http://www.mcs.anl.gov/papers/P1760.pdf\" rel=\"nofollow\">http://www.mcs.anl.gov/papers/P1760.pdf</a>)? With PMI, we can use a \"tracker\" like role to manage all the endpoints. Each node has only to know the tracker endpoint when launching, the it can connect to the tracker to acquire endpoints of the other nodes.</p>", "body_text": "System information\nnot related.\nDescribe the problem\nI'm  trying to use distributed tensorflow in a very large scale mode, say, more that 5000 nodes.\nAs the distributed tf using guide described in: https://www.tensorflow.org/deploy/distributed, I have to launch the python script by giving it a very long parameter list,  which is used to specify the endpoint of each node in the cluster including itself. just like:\npython train.py --worker_hosts=\"ip1:port1,ip2:port2,...\" --ps_hosts=\"ip1:port1,ip2:port2,...\"\nThis list will be very long in large scale mode, and may cause stack overflow. Because the stack used to store parameters is limited in size.\nHave the tf developers considered to add support for PMI (refer to: http://www.mcs.anl.gov/papers/P1760.pdf)? With PMI, we can use a \"tracker\" like role to manage all the endpoints. Each node has only to know the tracker endpoint when launching, the it can connect to the tracker to acquire endpoints of the other nodes.", "body": "### System information\r\n\r\nnot related.\r\n\r\n### Describe the problem\r\n\r\nI'm  trying to use distributed tensorflow in a very large scale mode, say, more that 5000 nodes.\r\n\r\nAs the distributed tf using guide described in: https://www.tensorflow.org/deploy/distributed, I have to launch the python script by giving it a very long parameter list,  which is used to specify the endpoint of each node in the cluster including itself. just like:\r\n\r\n`python train.py --worker_hosts=\"ip1:port1,ip2:port2,...\" --ps_hosts=\"ip1:port1,ip2:port2,...\"` \r\n\r\nThis list will be very long in large scale mode, and may cause stack overflow. Because the stack used to store parameters is limited in size.\r\n\r\nHave the tf developers considered to add support for PMI (refer to: http://www.mcs.anl.gov/papers/P1760.pdf)? With PMI, we can use a \"tracker\" like role to manage all the endpoints. Each node has only to know the tracker endpoint when launching, the it can connect to the tracker to acquire endpoints of the other nodes."}