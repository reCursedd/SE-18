{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/204683938", "html_url": "https://github.com/tensorflow/tensorflow/issues/1740#issuecomment-204683938", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1740", "id": 204683938, "node_id": "MDEyOklzc3VlQ29tbWVudDIwNDY4MzkzOA==", "user": {"login": "Fhrozen", "id": 11988996, "node_id": "MDQ6VXNlcjExOTg4OTk2", "avatar_url": "https://avatars3.githubusercontent.com/u/11988996?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Fhrozen", "html_url": "https://github.com/Fhrozen", "followers_url": "https://api.github.com/users/Fhrozen/followers", "following_url": "https://api.github.com/users/Fhrozen/following{/other_user}", "gists_url": "https://api.github.com/users/Fhrozen/gists{/gist_id}", "starred_url": "https://api.github.com/users/Fhrozen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Fhrozen/subscriptions", "organizations_url": "https://api.github.com/users/Fhrozen/orgs", "repos_url": "https://api.github.com/users/Fhrozen/repos", "events_url": "https://api.github.com/users/Fhrozen/events{/privacy}", "received_events_url": "https://api.github.com/users/Fhrozen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-02T09:50:42Z", "updated_at": "2016-04-02T09:50:42Z", "author_association": "NONE", "body_html": "<p>I already find the error. I was using a trainable variable for the learning rate(I wanted to trak the lr but it does not look possible), and also add the list of variables to compute by the op at adam. I am not sure if this is a correct way but it looks like it works.</p>\n<pre><code>\n`   with tf.Graph().as_default(), tf.device('/cpu:0'):\n        devs = ['/job:prs/task:0/gpu:0','/job:worker/task:0/gpu:0'] #\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n        num_batches_per_epoch = dt_fdr.FLS_PER_ANGLE/ FLAGS.batch_size\n        #lr = tf.Variable(tf.constant(FLAGS.learning_rate, dtype=tf.float32))\n        opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n        tower_grads = []\n        for i in xrange(FLAGS.num_gpus):\n            with tf.device(devs[i]):\n                with tf.name_scope('%s_%d' % (tf_model.TOWER_NAME, i)) as scope:\n                    loss = tower_loss(scope)\n                    tf.get_variable_scope().reuse_variables()\n                    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                    #\"print('\\n'.join('{}: {}'.format(*k) for k in enumerate(summaries)))\n                    grads = opt.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope))\n                    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(grads)))\n                    tower_grads.append(grads)\n        grads = average_gradients(tower_grads)\n        #summaries.append(tf.scalar_summary('learning_rate', lr))\n        for grad, var in grads:\n            if grad:\n                summaries.append(\n                    tf.histogram_summary(var.op.name + '/gradients', grad))\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) \n        for var in tf.trainable_variables():\n            summaries.append(tf.histogram_summary(var.op.name, var))\n\n        train_op = apply_gradient_op\n\n        saver = tf.train.Saver(tf.all_variables())\n\n        summary_op = tf.merge_summary(summaries)\n\n        init = tf.initialize_all_variables()\n\n        sess = tf.Session(\"grpc://nelson-lab:2500\",config=tf.ConfigProto(\n            allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement))\n        sess.run(init)`\n\n\n</code></pre>\n<p>I wonder if someone also tried to do some double gpu training using adam.</p>\n<p>Regards</p>", "body_text": "I already find the error. I was using a trainable variable for the learning rate(I wanted to trak the lr but it does not look possible), and also add the list of variables to compute by the op at adam. I am not sure if this is a correct way but it looks like it works.\n\n`   with tf.Graph().as_default(), tf.device('/cpu:0'):\n        devs = ['/job:prs/task:0/gpu:0','/job:worker/task:0/gpu:0'] #\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n        num_batches_per_epoch = dt_fdr.FLS_PER_ANGLE/ FLAGS.batch_size\n        #lr = tf.Variable(tf.constant(FLAGS.learning_rate, dtype=tf.float32))\n        opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n        tower_grads = []\n        for i in xrange(FLAGS.num_gpus):\n            with tf.device(devs[i]):\n                with tf.name_scope('%s_%d' % (tf_model.TOWER_NAME, i)) as scope:\n                    loss = tower_loss(scope)\n                    tf.get_variable_scope().reuse_variables()\n                    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                    #\"print('\\n'.join('{}: {}'.format(*k) for k in enumerate(summaries)))\n                    grads = opt.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope))\n                    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(grads)))\n                    tower_grads.append(grads)\n        grads = average_gradients(tower_grads)\n        #summaries.append(tf.scalar_summary('learning_rate', lr))\n        for grad, var in grads:\n            if grad:\n                summaries.append(\n                    tf.histogram_summary(var.op.name + '/gradients', grad))\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) \n        for var in tf.trainable_variables():\n            summaries.append(tf.histogram_summary(var.op.name, var))\n\n        train_op = apply_gradient_op\n\n        saver = tf.train.Saver(tf.all_variables())\n\n        summary_op = tf.merge_summary(summaries)\n\n        init = tf.initialize_all_variables()\n\n        sess = tf.Session(\"grpc://nelson-lab:2500\",config=tf.ConfigProto(\n            allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement))\n        sess.run(init)`\n\n\n\nI wonder if someone also tried to do some double gpu training using adam.\nRegards", "body": "I already find the error. I was using a trainable variable for the learning rate(I wanted to trak the lr but it does not look possible), and also add the list of variables to compute by the op at adam. I am not sure if this is a correct way but it looks like it works.\n\n```\n\n`   with tf.Graph().as_default(), tf.device('/cpu:0'):\n        devs = ['/job:prs/task:0/gpu:0','/job:worker/task:0/gpu:0'] #\n        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n        num_batches_per_epoch = dt_fdr.FLS_PER_ANGLE/ FLAGS.batch_size\n        #lr = tf.Variable(tf.constant(FLAGS.learning_rate, dtype=tf.float32))\n        opt = tf.train.AdamOptimizer(FLAGS.learning_rate)\n        tower_grads = []\n        for i in xrange(FLAGS.num_gpus):\n            with tf.device(devs[i]):\n                with tf.name_scope('%s_%d' % (tf_model.TOWER_NAME, i)) as scope:\n                    loss = tower_loss(scope)\n                    tf.get_variable_scope().reuse_variables()\n                    summaries = tf.get_collection(tf.GraphKeys.SUMMARIES, scope)\n                    #\"print('\\n'.join('{}: {}'.format(*k) for k in enumerate(summaries)))\n                    grads = opt.compute_gradients(loss, tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope))\n                    #print('\\n'.join('{}: {}'.format(*k) for k in enumerate(grads)))\n                    tower_grads.append(grads)\n        grads = average_gradients(tower_grads)\n        #summaries.append(tf.scalar_summary('learning_rate', lr))\n        for grad, var in grads:\n            if grad:\n                summaries.append(\n                    tf.histogram_summary(var.op.name + '/gradients', grad))\n        apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) \n        for var in tf.trainable_variables():\n            summaries.append(tf.histogram_summary(var.op.name, var))\n\n        train_op = apply_gradient_op\n\n        saver = tf.train.Saver(tf.all_variables())\n\n        summary_op = tf.merge_summary(summaries)\n\n        init = tf.initialize_all_variables()\n\n        sess = tf.Session(\"grpc://nelson-lab:2500\",config=tf.ConfigProto(\n            allow_soft_placement=True,\n            log_device_placement=FLAGS.log_device_placement))\n        sess.run(init)`\n\n\n```\n\nI wonder if someone also tried to do some double gpu training using adam.\n\nRegards\n"}