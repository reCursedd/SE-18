{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/99292182", "pull_request_review_id": 19964456, "id": 99292182, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDk5MjkyMTgy", "diff_hunk": "@@ -232,232 +237,237 @@ def import_graph_def(graph_def, input_map=None, return_elements=None,\n     producer_op_dict = {op.name: op for op in producer_op_list.op}\n \n   # LINT.IfChange\n-  with ops.name_scope(name, 'import', input_map.values()) as scope:\n-    g = ops.get_default_graph()\n-    # TODO(ashankar): Should this just copy over or should it do some\n-    # more nuanced merging? For example, the graph may already have some\n-    # marked \"bad versions\" and we don't want to lose those because of\n-    # what's in graph_def.versions? The C++ ImporGraphDef does something\n-    # more nuanced.\n-    g.graph_def_versions.CopyFrom(graph_def.versions)\n-\n-    if input_map:\n-      if not scope:\n-        # The caller must have passed `name=''`.\n-        raise ValueError('tf.import_graph_def() requires a non-empty `name` '\n-                         'if `input_map` is used.')\n-      with ops.name_scope('_inputs'):\n-        input_map = {k: ops.convert_to_tensor(v) for k, v in input_map.items()}\n-\n-    # NOTE(mrry): We do this in two passes, because there may be a cycle in\n-    # `graph_def`.\n-\n-    # 1. Add operations without their inputs.\n-    for node in graph_def.node:\n-      # Set any default attr values that aren't present.\n-      if node.op not in op_dict:\n-        raise ValueError('No op named %s in defined operations.' % node.op)\n-      op_def = op_dict[node.op]\n-      for attr_def in op_def.attr:\n-        key = attr_def.name\n-        if attr_def.HasField('default_value'):\n-          value = node.attr[key]\n-          if value is None or value.WhichOneof('value') is None:\n-            node.attr[key].CopyFrom(attr_def.default_value)\n-      if producer_op_dict:\n-        # Remove any default attr values that aren't in op_def.\n-        if node.op in producer_op_dict:\n-          producer_op_def = producer_op_dict[node.op]\n-          # We make a copy of node.attr to iterate through since we\n-          # may modify node.attr inside the loop.\n-          for key in list(node.attr):\n-            if _FindAttrInOpDef(key, op_def) is None:\n-              # No attr_def in consumer, look in producer.\n-              attr_def = _FindAttrInOpDef(key, producer_op_def)\n-              if (attr_def and attr_def.HasField('default_value') and\n-                  node.attr[key] == attr_def.default_value):\n-                # Unknown attr had default value in producer, delete it\n-                # so it can be understood by consumer.\n-                del node.attr[key]\n-\n-      output_types = _OutputTypes(node, op_dict)\n-      name_to_op[node.name] = g.create_op(\n-          node.op, [], output_types, name=node.name, attrs=node.attr,\n-          compute_shapes=False, compute_device=False,\n-          op_def=op_def)\n-\n-    # 2. Add inputs to the operations.\n-    for node in graph_def.node:\n-      op = name_to_op[node.name]\n-      input_types = _InputTypes(node, op_dict)\n-\n-      # Rewrite the colocation attributes in the graph, since the\n-      # names of new ops may have changed.\n-      for key, value in op.node_def.attr.items():\n-        if key == '_class':\n-          class_values = value.list\n-          new_class_values = []\n-          for class_value in class_values.s:\n-            if class_value.startswith(b'loc:@'):\n-              op_to_bind_to = class_value[5:].decode()\n-              # Find the op by its original name.\n-              if op_to_bind_to not in name_to_op:\n-                raise ValueError('Specified colocation to an op that '\n-                                 'does not exist during import: %s in %s' % (\n-                                     op_to_bind_to, node.name))\n-              original_op = name_to_op[op_to_bind_to]\n-              new_class_values.append(compat.as_bytes(\n-                  'loc:@' + original_op.name))\n-            else:\n-              new_class_values.append(class_value)\n-          value.list.CopyFrom(attr_value_pb2.AttrValue.ListValue(\n-              s=new_class_values))\n-\n-      # NOTE(mrry): We cannot use zip here because control inputs do not appear\n-      # in the list of input_types.\n-      for i, input_name in enumerate(\n-          [_CanonicalInputName(x) for x in node.input]):\n-\n-        if _IsControlInput(input_name):\n-          # (a) Input is a control input that should be taken from an op\n-          #     in \"graph_def\".\n-          try:\n-            source_op = name_to_op[input_name[1:]]\n-          except KeyError:\n-            raise ValueError(\n-                _InvalidNodeMessage(\n-                    node,\n-                    'Control input %r not found in graph_def.' % (input_name,)))\n-          # pylint: disable=protected-access\n-          op._add_control_input(source_op)\n-          # pylint: enable=protected-access\n-\n-        else:\n-          try:\n-            input_type = input_types[i]\n-          except IndexError:\n-            raise ValueError(_InvalidNodeMessage(\n-                node, 'More inputs specified (%r) than the op expects.'\n-                % (input_name,)))\n-\n-          if input_name in input_map:\n-            # (b) Input should be replaced by a tensor from the caller.\n-            source_tensor = input_map[input_name]\n-            used_input_keys.add(input_name)\n-\n-          else:\n-            # (c) Input should be taken from an op in `graph_def`.\n-            operation_name, output_index = _ParseTensorName(input_name)\n+  g = ops.get_default_graph() if graph is None else graph\n+  with g.as_default():\n+    with ops.name_scope(name, 'import', input_map.values()) as scope:\n+      # TODO(ashankar): Should this just copy over or should it do some\n+      # more nuanced merging? For example, the graph may already have some\n+      # marked \"bad versions\" and we don't want to lose those because of\n+      # what's in graph_def.versions? The C++ ImporGraphDef does something\n+      # more nuanced.\n+      g.graph_def_versions.CopyFrom(graph_def.versions)\n+\n+      if input_map:\n+        if not scope:\n+          # The caller must have passed `name=''`.\n+          raise ValueError('tf.import_graph_def() requires a non-empty `name` '\n+                           'if `input_map` is used.')\n+        with ops.name_scope('_inputs'):\n+          input_map = {\n+                       k: ops.convert_to_tensor(v) for k, v in input_map.items()\n+                      }\n+\n+      # NOTE(mrry): We do this in two passes, because there may be a cycle in\n+      # `graph_def`.\n+\n+      # 1. Add operations without their inputs.\n+      for node in graph_def.node:\n+        # Set any default attr values that aren't present.\n+        if node.op not in op_dict:\n+          raise ValueError('No op named %s in defined operations.' % node.op)\n+        op_def = op_dict[node.op]\n+        for attr_def in op_def.attr:\n+          key = attr_def.name\n+          if attr_def.HasField('default_value'):\n+            value = node.attr[key]\n+            if value is None or value.WhichOneof('value') is None:\n+              node.attr[key].CopyFrom(attr_def.default_value)\n+        if producer_op_dict:\n+          # Remove any default attr values that aren't in op_def.\n+          if node.op in producer_op_dict:\n+            producer_op_def = producer_op_dict[node.op]\n+            # We make a copy of node.attr to iterate through since we\n+            # may modify node.attr inside the loop.\n+            for key in list(node.attr):\n+              if _FindAttrInOpDef(key, op_def) is None:\n+                # No attr_def in consumer, look in producer.\n+                attr_def = _FindAttrInOpDef(key, producer_op_def)\n+                if (attr_def and attr_def.HasField('default_value') and\n+                    node.attr[key] == attr_def.default_value):\n+                  # Unknown attr had default value in producer, delete it\n+                  # so it can be understood by consumer.\n+                  del node.attr[key]\n+\n+        output_types = _OutputTypes(node, op_dict)\n+        name_to_op[node.name] = g.create_op(\n+            node.op, [], output_types, name=node.name, attrs=node.attr,\n+            compute_shapes=False, compute_device=False,\n+            op_def=op_def)\n+\n+      # 2. Add inputs to the operations.\n+      for node in graph_def.node:\n+        op = name_to_op[node.name]\n+        input_types = _InputTypes(node, op_dict)\n+\n+        # Rewrite the colocation attributes in the graph, since the\n+        # names of new ops may have changed.\n+        for key, value in op.node_def.attr.items():\n+          if key == '_class':\n+            class_values = value.list\n+            new_class_values = []\n+            for class_value in class_values.s:\n+              if class_value.startswith(b'loc:@'):\n+                op_to_bind_to = class_value[5:].decode()\n+                # Find the op by its original name.\n+                if op_to_bind_to not in name_to_op:\n+                  raise ValueError('Specified colocation to an op that '\n+                                   'does not exist during import: %s in %s' % (\n+                                       op_to_bind_to, node.name))\n+                original_op = name_to_op[op_to_bind_to]\n+                new_class_values.append(compat.as_bytes(\n+                    'loc:@' + original_op.name))\n+              else:\n+                new_class_values.append(class_value)\n+            value.list.CopyFrom(attr_value_pb2.AttrValue.ListValue(\n+                s=new_class_values))\n+\n+        # NOTE(mrry): We cannot use zip here because control inputs do not \n+        # appear in the list of input_types.\n+        for i, input_name in enumerate(\n+            [_CanonicalInputName(x) for x in node.input]):\n+\n+          if _IsControlInput(input_name):\n+            # (a) Input is a control input that should be taken from an op\n+            #     in \"graph_def\".\n             try:\n-              source_op = name_to_op[operation_name]\n-              source_tensor = list(source_op.values())[output_index]\n-            except (KeyError, IndexError):\n+              source_op = name_to_op[input_name[1:]]\n+            except KeyError:\n               raise ValueError(\n                   _InvalidNodeMessage(\n                       node,\n-                      'Input tensor %r not found in graph_def.'\n-                      % (input_name,)))\n-\n-          try:\n+                      'Control input %r not found in graph_def.' % (input_name,)\n+                      ))\n             # pylint: disable=protected-access\n-            op._add_input(source_tensor, dtype=input_type)\n+            op._add_control_input(source_op)\n             # pylint: enable=protected-access\n-          except TypeError as te:\n-            raise ValueError(_InvalidNodeMessage(\n-                node, 'Input tensor %r %s' % (input_name, te)))\n \n-      # pylint: disable=protected_access\n-      if op._input_dtypes != input_types:\n-        raise ValueError(\n-            _InvalidNodeMessage(\n-                node,\n-                'Input types mismatch (expected %r but got %r)'\n-                % (', '.join(dtypes.as_dtype(x).name for x in input_types),\n-                   ', '.join(x.name for x in op._input_dtypes))))\n-      # pylint: enable=protected_access\n-\n-      # Execute shape inference for this op.\n-      # NOTE(mrry): If the graph contains a cycle, the full shape information\n-      # may not be available for this op's inputs.\n-      ops.set_shapes_for_outputs(op)\n-      # For nodes with _output_shapes set, set the output shapes.\n-      if '_output_shapes' in op.node_def.attr:\n-        for i, output in enumerate(op.outputs):\n-          dims = op.node_def.attr['_output_shapes'].list.shape[i]\n-          output_shape = tensor_shape.TensorShape(\n-              None if dims.unknown_rank else\n-              [dim.size if dim.size >= 0 else None for dim in dims.dim])\n-\n-          try:\n-            output.set_shape(output_shape)\n-          except ValueError as e:\n-            # If the output shape is incompatible with what is inferred\n-            # by the graph for a very specific whitelist of ops, then we\n-            # ignore this output shape.  This can happen if there is a\n-            # bug in the shape function for some operation, and the\n-            # serialized graph def has the incorrect shape set when\n-            # running on a newer binary with the fixed shape function.\n-            # This is an escape hatch that allows us to correct shape\n-            # functions that are not critical to correct execution but\n-            # would cause graphs to fail if imported after correcting.\n-            #\n-            # This can be removed after 2017/03/08.\n-            if op.type in ['RandomShuffleQueue', 'PaddingFIFOQueue',\n-                           'FIFOQueue', 'PriorityQueue', 'QueueSize',\n-                           'Stack', 'Barrier', 'BarrierReadySize',\n-                           'BarrierIncompleteSize', 'HashTable',\n-                           'MutableHashTable',\n-                           'MutableHashTableOfTensors', 'Mutex',\n-                           'CuckooTable', 'IndexTable',\n-                           'WholeFileReader', 'TextLineReader',\n-                           'FixedLengthRecordReader',\n-                           'TFRecordReader', 'IdentityReader',\n-                           'RefSwitch', 'RefEnter', 'RefNextIteration',\n-                           'RefMerge', 'RefIdentity']:\n-              pass\n-            elif op.type in [\n-                'ConditionalAccumulator', 'SparseConditionalAccumulator',\n-                'Table'\n-            ]:\n-              # This can be removed after 2017/04/24.\n-              pass\n+          else:\n+            try:\n+              input_type = input_types[i]\n+            except IndexError:\n+              raise ValueError(_InvalidNodeMessage(\n+                  node, 'More inputs specified (%r) than the op expects.'\n+                  % (input_name,)))\n+\n+            if input_name in input_map:\n+              # (b) Input should be replaced by a tensor from the caller.\n+              source_tensor = input_map[input_name]\n+              used_input_keys.add(input_name)\n+\n             else:\n-              raise e\n-\n-        del op.node_def.attr['_output_shapes']\n-\n-      # Apply device functions for this op.\n-      # NOTE(mrry): We do this after configuring the inputs, because\n-      # the result of the device functions may depend on the inputs.\n-      with _MaybeDevice(node.device):\n-        g._apply_device_functions(op)  # pylint: disable=protected-access\n-\n-    # Treat unused input mappings as an error, because they are likely to be\n-    # due to a typo.\n-    unused_input_keys = frozenset(input_map.keys()).difference(used_input_keys)\n-    if unused_input_keys:\n-      raise ValueError(\n-          'Attempted to map inputs that were not found in graph_def: [%s]'\n-          % ', '.join(unused_input_keys))\n-\n-    if return_elements is None:\n-      return None\n-    else:\n-      ret = []\n-      for name in return_elements:\n-        name = compat.as_str(name)\n-        if ':' in name:\n-          try:\n-            operation_name, output_index = _ParseTensorName(name)\n-            ret.append(name_to_op[operation_name].outputs[output_index])\n-          except (ValueError, KeyError, IndexError):\n-            raise ValueError(\n-                'Requested return_element %r not found in graph_def.' % name)\n-        else:\n-          try:\n-            ret.append(name_to_op[name])\n-          except KeyError:\n-            raise ValueError(\n-                'Requested return_element %r not found in graph_def.' % name)\n-      return ret\n-  # LINT.ThenChange(//tensorflow/core/graph/graph_constructor.cc)\n+              # (c) Input should be taken from an op in `graph_def`.\n+              operation_name, output_index = _ParseTensorName(input_name)\n+              try:\n+                source_op = name_to_op[operation_name]\n+                source_tensor = list(source_op.values())[output_index]\n+              except (KeyError, IndexError):\n+                raise ValueError(\n+                    _InvalidNodeMessage(\n+                        node,\n+                        'Input tensor %r not found in graph_def.'\n+                        % (input_name,)))\n+\n+            try:\n+              # pylint: disable=protected-access\n+              op._add_input(source_tensor, dtype=input_type)\n+              # pylint: enable=protected-access\n+            except TypeError as te:\n+              raise ValueError(_InvalidNodeMessage(\n+                  node, 'Input tensor %r %s' % (input_name, te)))\n+\n+        # pylint: disable=protected_access\n+        if op._input_dtypes != input_types:\n+          raise ValueError(\n+              _InvalidNodeMessage(\n+                  node,\n+                  'Input types mismatch (expected %r but got %r)'\n+                  % (', '.join(dtypes.as_dtype(x).name for x in input_types),\n+                     ', '.join(x.name for x in op._input_dtypes))))\n+        # pylint: enable=protected_access\n+\n+        # Execute shape inference for this op.\n+        # NOTE(mrry): If the graph contains a cycle, the full shape information\n+        # may not be available for this op's inputs.\n+        ops.set_shapes_for_outputs(op)\n+        # For nodes with _output_shapes set, set the output shapes.\n+        if '_output_shapes' in op.node_def.attr:\n+          for i, output in enumerate(op.outputs):\n+            dims = op.node_def.attr['_output_shapes'].list.shape[i]\n+            output_shape = tensor_shape.TensorShape(\n+                None if dims.unknown_rank else\n+                [dim.size if dim.size >= 0 else None for dim in dims.dim])\n+\n+            try:\n+              output.set_shape(output_shape)\n+            except ValueError as e:\n+              # If the output shape is incompatible with what is inferred\n+              # by the graph for a very specific whitelist of ops, then we\n+              # ignore this output shape.  This can happen if there is a\n+              # bug in the shape function for some operation, and the\n+              # serialized graph def has the incorrect shape set when\n+              # running on a newer binary with the fixed shape function.\n+              # This is an escape hatch that allows us to correct shape\n+              # functions that are not critical to correct execution but\n+              # would cause graphs to fail if imported after correcting.\n+              #\n+              # This can be removed after 2017/03/08.\n+              if op.type in ['RandomShuffleQueue', 'PaddingFIFOQueue',\n+                             'FIFOQueue', 'PriorityQueue', 'QueueSize',\n+                             'Stack', 'Barrier', 'BarrierReadySize',\n+                             'BarrierIncompleteSize', 'HashTable',\n+                             'MutableHashTable',\n+                             'MutableHashTableOfTensors', 'Mutex',\n+                             'CuckooTable', 'IndexTable',\n+                             'WholeFileReader', 'TextLineReader',\n+                             'FixedLengthRecordReader',\n+                             'TFRecordReader', 'IdentityReader',\n+                             'RefSwitch', 'RefEnter', 'RefNextIteration',\n+                             'RefMerge', 'RefIdentity']:\n+                pass\n+              elif op.type in [\n+                  'ConditionalAccumulator', 'SparseConditionalAccumulator',\n+                  'Table'\n+              ]:\n+                # This can be removed after 2017/04/24.\n+                pass\n+              else:\n+                raise e\n+\n+          del op.node_def.attr['_output_shapes']\n+\n+        # Apply device functions for this op.\n+        # NOTE(mrry): We do this after configuring the inputs, because\n+        # the result of the device functions may depend on the inputs.\n+        with _MaybeDevice(node.device):\n+          g._apply_device_functions(op)  # pylint: disable=protected-access\n+\n+      # Treat unused input mappings as an error, because they are likely to be\n+      # due to a typo.\n+      unused_input_keys = frozenset(\n+        input_map.keys()).difference(used_input_keys)\n+      if unused_input_keys:\n+        raise ValueError(\n+            'Attempted to map inputs that were not found in graph_def: [%s]'\n+            % ', '.join(unused_input_keys))\n+\n+      if return_elements is None:\n+        return None\n+      else:\n+        ret = []\n+        for name in return_elements:\n+          name = compat.as_str(name)\n+          if ':' in name:\n+            try:\n+              operation_name, output_index = _ParseTensorName(name)\n+              ret.append(name_to_op[operation_name].outputs[output_index])\n+            except (ValueError, KeyError, IndexError):\n+              raise ValueError(\n+                  'Requested return_element %r not found in graph_def.' % name)\n+          else:\n+            try:\n+              ret.append(name_to_op[name])\n+            except KeyError:\n+              raise ValueError(\n+                  'Requested return_element %r not found in graph_def.' % name)\n+        return ret\n+    # LINT.ThenChange(//tensorflow/core/graph/graph_constructor.cc)", "path": "tensorflow/python/framework/importer.py", "position": 490, "original_position": 490, "commit_id": "de7f6d9e2520e2fad74ec6c439ba80e142f71ae8", "original_commit_id": "de7f6d9e2520e2fad74ec6c439ba80e142f71ae8", "user": {"login": "samjabrahams", "id": 11607205, "node_id": "MDQ6VXNlcjExNjA3MjA1", "avatar_url": "https://avatars0.githubusercontent.com/u/11607205?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samjabrahams", "html_url": "https://github.com/samjabrahams", "followers_url": "https://api.github.com/users/samjabrahams/followers", "following_url": "https://api.github.com/users/samjabrahams/following{/other_user}", "gists_url": "https://api.github.com/users/samjabrahams/gists{/gist_id}", "starred_url": "https://api.github.com/users/samjabrahams/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samjabrahams/subscriptions", "organizations_url": "https://api.github.com/users/samjabrahams/orgs", "repos_url": "https://api.github.com/users/samjabrahams/repos", "events_url": "https://api.github.com/users/samjabrahams/events{/privacy}", "received_events_url": "https://api.github.com/users/samjabrahams/received_events", "type": "User", "site_admin": false}, "body": "END THE WHITE SPACE DIFF", "created_at": "2017-02-03T08:22:14Z", "updated_at": "2017-02-03T08:25:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/7240#discussion_r99292182", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7240", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/99292182"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/7240#discussion_r99292182"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/7240"}}, "body_html": "<p>END THE WHITE SPACE DIFF</p>", "body_text": "END THE WHITE SPACE DIFF"}