{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/278626888", "html_url": "https://github.com/tensorflow/tensorflow/issues/7354#issuecomment-278626888", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7354", "id": 278626888, "node_id": "MDEyOklzc3VlQ29tbWVudDI3ODYyNjg4OA==", "user": {"login": "mschonwe", "id": 101855, "node_id": "MDQ6VXNlcjEwMTg1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/101855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mschonwe", "html_url": "https://github.com/mschonwe", "followers_url": "https://api.github.com/users/mschonwe/followers", "following_url": "https://api.github.com/users/mschonwe/following{/other_user}", "gists_url": "https://api.github.com/users/mschonwe/gists{/gist_id}", "starred_url": "https://api.github.com/users/mschonwe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mschonwe/subscriptions", "organizations_url": "https://api.github.com/users/mschonwe/orgs", "repos_url": "https://api.github.com/users/mschonwe/repos", "events_url": "https://api.github.com/users/mschonwe/events{/privacy}", "received_events_url": "https://api.github.com/users/mschonwe/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-09T12:24:39Z", "updated_at": "2017-02-09T12:24:39Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>, +1 for the new attention decoder to also support beams.</p>\n<p>Looking forward to the new attention decoder!  We tried the original dynamic_rnn_decoder approach but got better results with fixed length padded buckets (with dynamic, attention never converged \"on the diagonal\").</p>\n<p>For attention we used an approach recommended by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17009658\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/pbhatia243\">@pbhatia243</a> in issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"124367200\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/654\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/654/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/654\">#654</a> - results are much better than naive argmax, but the implementation is limited to batch=1.</p>", "body_text": "@ebrevdo, +1 for the new attention decoder to also support beams.\nLooking forward to the new attention decoder!  We tried the original dynamic_rnn_decoder approach but got better results with fixed length padded buckets (with dynamic, attention never converged \"on the diagonal\").\nFor attention we used an approach recommended by @pbhatia243 in issue #654 - results are much better than naive argmax, but the implementation is limited to batch=1.", "body": "@ebrevdo, +1 for the new attention decoder to also support beams.\r\n\r\nLooking forward to the new attention decoder!  We tried the original dynamic_rnn_decoder approach but got better results with fixed length padded buckets (with dynamic, attention never converged \"on the diagonal\").\r\n\r\nFor attention we used an approach recommended by @pbhatia243 in issue #654 - results are much better than naive argmax, but the implementation is limited to batch=1."}