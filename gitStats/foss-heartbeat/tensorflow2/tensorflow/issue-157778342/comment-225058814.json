{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225058814", "html_url": "https://github.com/tensorflow/tensorflow/issues/2598#issuecomment-225058814", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2598", "id": 225058814, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTA1ODgxNA==", "user": {"login": "girving", "id": 70511, "node_id": "MDQ6VXNlcjcwNTEx", "avatar_url": "https://avatars1.githubusercontent.com/u/70511?v=4", "gravatar_id": "", "url": "https://api.github.com/users/girving", "html_url": "https://github.com/girving", "followers_url": "https://api.github.com/users/girving/followers", "following_url": "https://api.github.com/users/girving/following{/other_user}", "gists_url": "https://api.github.com/users/girving/gists{/gist_id}", "starred_url": "https://api.github.com/users/girving/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/girving/subscriptions", "organizations_url": "https://api.github.com/users/girving/orgs", "repos_url": "https://api.github.com/users/girving/repos", "events_url": "https://api.github.com/users/girving/events{/privacy}", "received_events_url": "https://api.github.com/users/girving/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-09T23:44:43Z", "updated_at": "2016-06-09T23:44:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=784063\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dementrock\">@dementrock</a> The problem is that the registered gradient routines would get significantly more complicated if both sides were nonscalar, and we don't want to support that kind of complexity.  As discussed in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"124740366\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/675\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/675/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/675\">#675</a>, it's possible one could implement registered gradients with some sort of automatic machinery to map scalar gradient routines to nonscalar gradient routines, but this is a lot of work and we don't have any plans to do it.</p>\n<p>The other problem is that the applications I know of nonscalar gradients aren't that compelling as yet, since they tend to be impractically huge.  However, there are cases where higher order gradient information arises where you're differentiating a scalar, specifically Hessian-free Krylov-ish methods where one evaluates the gradient dotted with a suitably chosen vector.</p>\n<p>If that last bit is what you're trying to do, or something similar, we'd be happy to accept pull requests to make control flow not interfere.  It might be pretty complicated, though.</p>", "body_text": "@dementrock The problem is that the registered gradient routines would get significantly more complicated if both sides were nonscalar, and we don't want to support that kind of complexity.  As discussed in #675, it's possible one could implement registered gradients with some sort of automatic machinery to map scalar gradient routines to nonscalar gradient routines, but this is a lot of work and we don't have any plans to do it.\nThe other problem is that the applications I know of nonscalar gradients aren't that compelling as yet, since they tend to be impractically huge.  However, there are cases where higher order gradient information arises where you're differentiating a scalar, specifically Hessian-free Krylov-ish methods where one evaluates the gradient dotted with a suitably chosen vector.\nIf that last bit is what you're trying to do, or something similar, we'd be happy to accept pull requests to make control flow not interfere.  It might be pretty complicated, though.", "body": "@dementrock The problem is that the registered gradient routines would get significantly more complicated if both sides were nonscalar, and we don't want to support that kind of complexity.  As discussed in #675, it's possible one could implement registered gradients with some sort of automatic machinery to map scalar gradient routines to nonscalar gradient routines, but this is a lot of work and we don't have any plans to do it.\n\nThe other problem is that the applications I know of nonscalar gradients aren't that compelling as yet, since they tend to be impractically huge.  However, there are cases where higher order gradient information arises where you're differentiating a scalar, specifically Hessian-free Krylov-ish methods where one evaluates the gradient dotted with a suitably chosen vector.\n\nIf that last bit is what you're trying to do, or something similar, we'd be happy to accept pull requests to make control flow not interfere.  It might be pretty complicated, though.\n"}