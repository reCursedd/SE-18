{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10816", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10816/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10816/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10816/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10816", "id": 236808388, "node_id": "MDU6SXNzdWUyMzY4MDgzODg=", "number": 10816, "title": "Training LSTM RNN  model after restoration starting again with high loss", "user": {"login": "amadupu", "id": 14251460, "node_id": "MDQ6VXNlcjE0MjUxNDYw", "avatar_url": "https://avatars3.githubusercontent.com/u/14251460?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amadupu", "html_url": "https://github.com/amadupu", "followers_url": "https://api.github.com/users/amadupu/followers", "following_url": "https://api.github.com/users/amadupu/following{/other_user}", "gists_url": "https://api.github.com/users/amadupu/gists{/gist_id}", "starred_url": "https://api.github.com/users/amadupu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amadupu/subscriptions", "organizations_url": "https://api.github.com/users/amadupu/orgs", "repos_url": "https://api.github.com/users/amadupu/repos", "events_url": "https://api.github.com/users/amadupu/events{/privacy}", "received_events_url": "https://api.github.com/users/amadupu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-06-19T07:58:02Z", "updated_at": "2018-07-15T12:18:33Z", "closed_at": "2017-06-21T05:01:02Z", "author_association": "NONE", "body_html": "<p>I have an LSTM based RNN language model where in after the initial training for 1000 iterations, the model is saved as follows<br>\nsaver = tf.train.Saver()<br>\nsaver.save(sess,'rnn_model.ckpt',global_step=1000)</p>\n<p>I've restored the model by</p>\n<pre><code>        saver = tf.train.import_meta_graph(\"rnn_model.ckpt-1000.meta\")\n        saver.restore(self.sess, tf.train.latest_checkpoint('./'))\n        graph = tf.get_default_graph()\n        # restore the operations and placeholder as required from the graph\n        # perform retraining\n</code></pre>\n<p>so far everything looks good, the model is restored, but however, when I plot the loss summary while training with the same input data as previous training, the loss again starts with high value as if it is training from start again freshly. I've tried to search all means to find the relevant forums but could not find a proper solution. please advise on the corrective steps</p>\n<p>(related links<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"199139303\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6683\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6683/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6683\">#6683</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"198114402\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/keras-team/keras/issues/4875\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/4875/hovercard\" href=\"https://github.com/keras-team/keras/issues/4875\">keras-team/keras#4875</a><br>\n<a href=\"https://stackoverflow.com/questions/41328769/tensorflow-loss-resets-after-successfully-restored-checkpoint\" rel=\"nofollow\">https://stackoverflow.com/questions/41328769/tensorflow-loss-resets-after-successfully-restored-checkpoint</a></p>\n<p>)</p>", "body_text": "I have an LSTM based RNN language model where in after the initial training for 1000 iterations, the model is saved as follows\nsaver = tf.train.Saver()\nsaver.save(sess,'rnn_model.ckpt',global_step=1000)\nI've restored the model by\n        saver = tf.train.import_meta_graph(\"rnn_model.ckpt-1000.meta\")\n        saver.restore(self.sess, tf.train.latest_checkpoint('./'))\n        graph = tf.get_default_graph()\n        # restore the operations and placeholder as required from the graph\n        # perform retraining\n\nso far everything looks good, the model is restored, but however, when I plot the loss summary while training with the same input data as previous training, the loss again starts with high value as if it is training from start again freshly. I've tried to search all means to find the relevant forums but could not find a proper solution. please advise on the corrective steps\n(related links\n#6683\nkeras-team/keras#4875\nhttps://stackoverflow.com/questions/41328769/tensorflow-loss-resets-after-successfully-restored-checkpoint\n)", "body": "I have an LSTM based RNN language model where in after the initial training for 1000 iterations, the model is saved as follows\r\nsaver = tf.train.Saver()\r\nsaver.save(sess,'rnn_model.ckpt',global_step=1000)\r\n\r\nI've restored the model by\r\n\r\n            saver = tf.train.import_meta_graph(\"rnn_model.ckpt-1000.meta\")\r\n            saver.restore(self.sess, tf.train.latest_checkpoint('./'))\r\n            graph = tf.get_default_graph()\r\n            # restore the operations and placeholder as required from the graph\r\n            # perform retraining\r\n\r\nso far everything looks good, the model is restored, but however, when I plot the loss summary while training with the same input data as previous training, the loss again starts with high value as if it is training from start again freshly. I've tried to search all means to find the relevant forums but could not find a proper solution. please advise on the corrective steps\r\n\r\n (related links\r\nhttps://github.com/tensorflow/tensorflow/issues/6683\r\nhttps://github.com/fchollet/keras/issues/4875\r\nhttps://stackoverflow.com/questions/41328769/tensorflow-loss-resets-after-successfully-restored-checkpoint\r\n\r\n)\r\n\r\n\r\n\r\n"}