{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/439571771", "html_url": "https://github.com/tensorflow/tensorflow/issues/23425#issuecomment-439571771", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23425", "id": 439571771, "node_id": "MDEyOklzc3VlQ29tbWVudDQzOTU3MTc3MQ==", "user": {"login": "rickdzekman", "id": 5784312, "node_id": "MDQ6VXNlcjU3ODQzMTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/5784312?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rickdzekman", "html_url": "https://github.com/rickdzekman", "followers_url": "https://api.github.com/users/rickdzekman/followers", "following_url": "https://api.github.com/users/rickdzekman/following{/other_user}", "gists_url": "https://api.github.com/users/rickdzekman/gists{/gist_id}", "starred_url": "https://api.github.com/users/rickdzekman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rickdzekman/subscriptions", "organizations_url": "https://api.github.com/users/rickdzekman/orgs", "repos_url": "https://api.github.com/users/rickdzekman/repos", "events_url": "https://api.github.com/users/rickdzekman/events{/privacy}", "received_events_url": "https://api.github.com/users/rickdzekman/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-17T00:51:59Z", "updated_at": "2018-11-17T01:02:33Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=42781361\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/harshini-gadige\">@harshini-gadige</a></p>\n<p>Sorry for the delayed reply.</p>\n<p><strong>What is the top-level directory of the model you are using:</strong><br>\nI'm not sure what this means?</p>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</strong><br>\nI am not using any stock example scripts</p>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):</strong><br>\nWindows 10</p>\n<p><strong>TensorFlow installed from (source or binary):</strong><br>\nBinary installed with pip</p>\n<p><strong>TensorFlow version (use command below):</strong><br>\nFrom my <code>pip list</code> &gt;&gt; <code>tensorflow-gpu      1.11.0</code></p>\n<p><strong>CUDA/cuDNN version:</strong><br>\nCUDA Version 9.0.176<br>\ncudnn-9.0-windows10-x64-v7.3.0.29</p>\n<p><strong>GPU model and memory:</strong><br>\nGeForce GTX 850M (4.0GB)</p>\n<p><strong>Exact command to reproduce:</strong><br>\nA minimal example to reproduce the error is below:</p>\n<pre><code>import tensorflow as tf\n\nbatch_size = 64\ndict_size = 100\nword_vec_size = 20\ninput_length = 10\ncontext_size = 2\n\ninput_xs = tf.keras.layers.Input(batch_shape=(batch_size,input_length),dtype=\"int32\",name=\"input_xs\")\ncontext = tf.keras.layers.Input(batch_shape=(batch_size,2),dtype=\"float32\",name=\"input_context\")\n\nword_embedding = tf.keras.layers.Embedding(dict_size, word_vec_size, input_length=input_length, name='word_embedding')\nword_vecs = word_embedding(input_xs)\n\nconcatenator = tf.keras.layers.TimeDistributed(tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,context])))\noutputs = concatenator(word_vecs)\n\nmodel = tf.keras.models.Model(inputs=[input_xs,context],outputs=outputs)\n</code></pre>\n<p>The full error output:</p>\n<pre><code>Traceback (most recent call last):\n  File \"td_error.py\", line 18, in &lt;module&gt;\n    outputs = concatenator(word_vecs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 769, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\", line 250, in call\n    unroll=False)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3193, in rnn\n    outputs, _ = step_function(inputs[0], initial_states + constants)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\", line 242, in step\n    uses_learning_phase)\nNameError: name 'uses_learning_phase' is not defined\n</code></pre>\n<p>This error <strong>only</strong> occurs if the <code>batch_shape</code> argument is passed to the input layer(s). Changing the input like so does not generate an error:</p>\n<pre><code>input_xs = tf.keras.layers.Input(shape=(input_length,),dtype=\"int32\",name=\"input_xs\")\ncontext = tf.keras.layers.Input(shape=(2,),dtype=\"float32\",name=\"input_context\")\n</code></pre>\n<p>However I am using a stateful RNN in my actual model so I have to specify the <code>batch_shape</code>.</p>\n<p>If I implement the concatenation manually using <code>tf.keras.backend.rnn</code> the error goes away but a new problem is introduced:</p>\n<pre><code>def apply_concats(layer,context):\n    concat_lambda = tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,context]))\n    input_shape = tf.keras.backend.int_shape(layer)\n    def lambda_step(x, _):\n        return concat_lambda(x), []\n\n    _, outputs, _ = tf.keras.backend.rnn(\n        lambda_step,\n        layer,\n        initial_states=[],\n        input_length=input_shape[1],\n        unroll=False\n    )\n    return outputs\n\nconcatenator = tf.keras.layers.Lambda(lambda x: apply_concats(x,context))\noutputs = concatenator(word_vecs)\nmodel = tf.keras.models.Model(inputs=[input_xs,context],outputs=outputs)\\\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy')\nprint(model.summary())\n</code></pre>\n<p>The output for <code>print(model.summary())</code> shows:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_xs (InputLayer)        (64, 10)                  0\n_________________________________________________________________\nword_embedding (Embedding)   (64, 10, 20)              2000\n_________________________________________________________________\nlambda (Lambda)              (64, 10, 22)              0\n=================================================================\nTotal params: 2,000\nTrainable params: 2,000\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n<p>Which does not include the context layer because it is in the Lambda as a closure.</p>\n<p>This same problem occurs when using the <code>TimeDistributed</code> layer:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_xs (InputLayer)        (None, 10)                0\n_________________________________________________________________\nword_embedding (Embedding)   (None, 10, 20)            2000\n_________________________________________________________________\ntime_distributed (TimeDistri (None, 10, 22)            0\n=================================================================\nTotal params: 2,000\nTrainable params: 2,000\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n<p>But if I change my <code>Lambda</code> layer in my fixed version everything is fixed:</p>\n<pre><code>concatenator = tf.keras.layers.Lambda(lambda x: apply_concats(x[0],x[1]))\noutputs = concatenator([word_vecs,context])\n</code></pre>\n<p>In this version the context is explicitly passed to the lambda instead of in the closure. Now the model summary includes all layers:</p>\n<pre><code>__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n==================================================================================================\ninput_xs (InputLayer)           (64, 10)             0\n__________________________________________________________________________________________________\nword_embedding (Embedding)      (64, 10, 20)         2000        input_xs[0][0]\n__________________________________________________________________________________________________\ninput_context (InputLayer)      (64, 2)              0\n__________________________________________________________________________________________________\nlambda (Lambda)                 (64, 10, 22)         0           word_embedding[0][0]\n                                                                 input_context[0][0]\n==================================================================================================\nTotal params: 2,000\nTrainable params: 2,000\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre>", "body_text": "Hi @harshini-gadige\nSorry for the delayed reply.\nWhat is the top-level directory of the model you are using:\nI'm not sure what this means?\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI am not using any stock example scripts\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nWindows 10\nTensorFlow installed from (source or binary):\nBinary installed with pip\nTensorFlow version (use command below):\nFrom my pip list >> tensorflow-gpu      1.11.0\nCUDA/cuDNN version:\nCUDA Version 9.0.176\ncudnn-9.0-windows10-x64-v7.3.0.29\nGPU model and memory:\nGeForce GTX 850M (4.0GB)\nExact command to reproduce:\nA minimal example to reproduce the error is below:\nimport tensorflow as tf\n\nbatch_size = 64\ndict_size = 100\nword_vec_size = 20\ninput_length = 10\ncontext_size = 2\n\ninput_xs = tf.keras.layers.Input(batch_shape=(batch_size,input_length),dtype=\"int32\",name=\"input_xs\")\ncontext = tf.keras.layers.Input(batch_shape=(batch_size,2),dtype=\"float32\",name=\"input_context\")\n\nword_embedding = tf.keras.layers.Embedding(dict_size, word_vec_size, input_length=input_length, name='word_embedding')\nword_vecs = word_embedding(input_xs)\n\nconcatenator = tf.keras.layers.TimeDistributed(tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,context])))\noutputs = concatenator(word_vecs)\n\nmodel = tf.keras.models.Model(inputs=[input_xs,context],outputs=outputs)\n\nThe full error output:\nTraceback (most recent call last):\n  File \"td_error.py\", line 18, in <module>\n    outputs = concatenator(word_vecs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 769, in __call__\n    outputs = self.call(inputs, *args, **kwargs)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\", line 250, in call\n    unroll=False)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3193, in rnn\n    outputs, _ = step_function(inputs[0], initial_states + constants)\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\", line 242, in step\n    uses_learning_phase)\nNameError: name 'uses_learning_phase' is not defined\n\nThis error only occurs if the batch_shape argument is passed to the input layer(s). Changing the input like so does not generate an error:\ninput_xs = tf.keras.layers.Input(shape=(input_length,),dtype=\"int32\",name=\"input_xs\")\ncontext = tf.keras.layers.Input(shape=(2,),dtype=\"float32\",name=\"input_context\")\n\nHowever I am using a stateful RNN in my actual model so I have to specify the batch_shape.\nIf I implement the concatenation manually using tf.keras.backend.rnn the error goes away but a new problem is introduced:\ndef apply_concats(layer,context):\n    concat_lambda = tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,context]))\n    input_shape = tf.keras.backend.int_shape(layer)\n    def lambda_step(x, _):\n        return concat_lambda(x), []\n\n    _, outputs, _ = tf.keras.backend.rnn(\n        lambda_step,\n        layer,\n        initial_states=[],\n        input_length=input_shape[1],\n        unroll=False\n    )\n    return outputs\n\nconcatenator = tf.keras.layers.Lambda(lambda x: apply_concats(x,context))\noutputs = concatenator(word_vecs)\nmodel = tf.keras.models.Model(inputs=[input_xs,context],outputs=outputs)\\\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy')\nprint(model.summary())\n\nThe output for print(model.summary()) shows:\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_xs (InputLayer)        (64, 10)                  0\n_________________________________________________________________\nword_embedding (Embedding)   (64, 10, 20)              2000\n_________________________________________________________________\nlambda (Lambda)              (64, 10, 22)              0\n=================================================================\nTotal params: 2,000\nTrainable params: 2,000\nNon-trainable params: 0\n_________________________________________________________________\n\nWhich does not include the context layer because it is in the Lambda as a closure.\nThis same problem occurs when using the TimeDistributed layer:\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\ninput_xs (InputLayer)        (None, 10)                0\n_________________________________________________________________\nword_embedding (Embedding)   (None, 10, 20)            2000\n_________________________________________________________________\ntime_distributed (TimeDistri (None, 10, 22)            0\n=================================================================\nTotal params: 2,000\nTrainable params: 2,000\nNon-trainable params: 0\n_________________________________________________________________\n\nBut if I change my Lambda layer in my fixed version everything is fixed:\nconcatenator = tf.keras.layers.Lambda(lambda x: apply_concats(x[0],x[1]))\noutputs = concatenator([word_vecs,context])\n\nIn this version the context is explicitly passed to the lambda instead of in the closure. Now the model summary includes all layers:\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n==================================================================================================\ninput_xs (InputLayer)           (64, 10)             0\n__________________________________________________________________________________________________\nword_embedding (Embedding)      (64, 10, 20)         2000        input_xs[0][0]\n__________________________________________________________________________________________________\ninput_context (InputLayer)      (64, 2)              0\n__________________________________________________________________________________________________\nlambda (Lambda)                 (64, 10, 22)         0           word_embedding[0][0]\n                                                                 input_context[0][0]\n==================================================================================================\nTotal params: 2,000\nTrainable params: 2,000\nNon-trainable params: 0\n__________________________________________________________________________________________________", "body": "Hi @harshini-gadige\r\n\r\nSorry for the delayed reply.\r\n\r\n\r\n**What is the top-level directory of the model you are using:**\r\nI'm not sure what this means?\r\n\r\n**Have I written custom code (as opposed to using a stock example script provided in TensorFlow):**\r\nI am not using any stock example scripts\r\n\r\n**OS Platform and Distribution (e.g., Linux Ubuntu 16.04):**\r\nWindows 10\r\n\r\n**TensorFlow installed from (source or binary):**\r\nBinary installed with pip\r\n\r\n**TensorFlow version (use command below):**\r\nFrom my `pip list` >> `tensorflow-gpu      1.11.0`\r\n\r\n**CUDA/cuDNN version:**\r\nCUDA Version 9.0.176\r\ncudnn-9.0-windows10-x64-v7.3.0.29\r\n\r\n**GPU model and memory:**\r\nGeForce GTX 850M (4.0GB)\r\n\r\n**Exact command to reproduce:**\r\nA minimal example to reproduce the error is below:\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nbatch_size = 64\r\ndict_size = 100\r\nword_vec_size = 20\r\ninput_length = 10\r\ncontext_size = 2\r\n\r\ninput_xs = tf.keras.layers.Input(batch_shape=(batch_size,input_length),dtype=\"int32\",name=\"input_xs\")\r\ncontext = tf.keras.layers.Input(batch_shape=(batch_size,2),dtype=\"float32\",name=\"input_context\")\r\n\r\nword_embedding = tf.keras.layers.Embedding(dict_size, word_vec_size, input_length=input_length, name='word_embedding')\r\nword_vecs = word_embedding(input_xs)\r\n\r\nconcatenator = tf.keras.layers.TimeDistributed(tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,context])))\r\noutputs = concatenator(word_vecs)\r\n\r\nmodel = tf.keras.models.Model(inputs=[input_xs,context],outputs=outputs)\r\n```\r\nThe full error output:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"td_error.py\", line 18, in <module>\r\n    outputs = concatenator(word_vecs)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\", line 769, in __call__\r\n    outputs = self.call(inputs, *args, **kwargs)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\", line 250, in call\r\n    unroll=False)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\", line 3193, in rnn\r\n    outputs, _ = step_function(inputs[0], initial_states + constants)\r\n  File \"C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\wrappers.py\", line 242, in step\r\n    uses_learning_phase)\r\nNameError: name 'uses_learning_phase' is not defined\r\n```\r\n\r\nThis error **only** occurs if the `batch_shape` argument is passed to the input layer(s). Changing the input like so does not generate an error:\r\n\r\n```\r\ninput_xs = tf.keras.layers.Input(shape=(input_length,),dtype=\"int32\",name=\"input_xs\")\r\ncontext = tf.keras.layers.Input(shape=(2,),dtype=\"float32\",name=\"input_context\")\r\n```\r\n\r\nHowever I am using a stateful RNN in my actual model so I have to specify the `batch_shape`.\r\n \r\nIf I implement the concatenation manually using `tf.keras.backend.rnn` the error goes away but a new problem is introduced:\r\n\r\n```\r\ndef apply_concats(layer,context):\r\n    concat_lambda = tf.keras.layers.Lambda(lambda x: tf.keras.layers.Concatenate()([x,context]))\r\n    input_shape = tf.keras.backend.int_shape(layer)\r\n    def lambda_step(x, _):\r\n        return concat_lambda(x), []\r\n\r\n    _, outputs, _ = tf.keras.backend.rnn(\r\n        lambda_step,\r\n        layer,\r\n        initial_states=[],\r\n        input_length=input_shape[1],\r\n        unroll=False\r\n    )\r\n    return outputs\r\n\r\nconcatenator = tf.keras.layers.Lambda(lambda x: apply_concats(x,context))\r\noutputs = concatenator(word_vecs)\r\nmodel = tf.keras.models.Model(inputs=[input_xs,context],outputs=outputs)\\\r\nmodel.compile(optimizer='adam',\r\n              loss='sparse_categorical_crossentropy')\r\nprint(model.summary())\r\n```\r\nThe output for `print(model.summary())` shows:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_xs (InputLayer)        (64, 10)                  0\r\n_________________________________________________________________\r\nword_embedding (Embedding)   (64, 10, 20)              2000\r\n_________________________________________________________________\r\nlambda (Lambda)              (64, 10, 22)              0\r\n=================================================================\r\nTotal params: 2,000\r\nTrainable params: 2,000\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\nWhich does not include the context layer because it is in the Lambda as a closure.\r\n\r\nThis same problem occurs when using the `TimeDistributed` layer:\r\n\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #\r\n=================================================================\r\ninput_xs (InputLayer)        (None, 10)                0\r\n_________________________________________________________________\r\nword_embedding (Embedding)   (None, 10, 20)            2000\r\n_________________________________________________________________\r\ntime_distributed (TimeDistri (None, 10, 22)            0\r\n=================================================================\r\nTotal params: 2,000\r\nTrainable params: 2,000\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nBut if I change my `Lambda` layer in my fixed version everything is fixed:\r\n\r\n```\r\nconcatenator = tf.keras.layers.Lambda(lambda x: apply_concats(x[0],x[1]))\r\noutputs = concatenator([word_vecs,context])\r\n```\r\nIn this version the context is explicitly passed to the lambda instead of in the closure. Now the model summary includes all layers:\r\n```\r\n__________________________________________________________________________________________________\r\nLayer (type)                    Output Shape         Param #     Connected to\r\n==================================================================================================\r\ninput_xs (InputLayer)           (64, 10)             0\r\n__________________________________________________________________________________________________\r\nword_embedding (Embedding)      (64, 10, 20)         2000        input_xs[0][0]\r\n__________________________________________________________________________________________________\r\ninput_context (InputLayer)      (64, 2)              0\r\n__________________________________________________________________________________________________\r\nlambda (Lambda)                 (64, 10, 22)         0           word_embedding[0][0]\r\n                                                                 input_context[0][0]\r\n==================================================================================================\r\nTotal params: 2,000\r\nTrainable params: 2,000\r\nNon-trainable params: 0\r\n__________________________________________________________________________________________________\r\n```"}