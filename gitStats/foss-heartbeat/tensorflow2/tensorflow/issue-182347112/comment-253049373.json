{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253049373", "html_url": "https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-253049373", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4897", "id": 253049373, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzA0OTM3Mw==", "user": {"login": "seerdecker", "id": 12588992, "node_id": "MDQ6VXNlcjEyNTg4OTky", "avatar_url": "https://avatars0.githubusercontent.com/u/12588992?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seerdecker", "html_url": "https://github.com/seerdecker", "followers_url": "https://api.github.com/users/seerdecker/followers", "following_url": "https://api.github.com/users/seerdecker/following{/other_user}", "gists_url": "https://api.github.com/users/seerdecker/gists{/gist_id}", "starred_url": "https://api.github.com/users/seerdecker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seerdecker/subscriptions", "organizations_url": "https://api.github.com/users/seerdecker/orgs", "repos_url": "https://api.github.com/users/seerdecker/repos", "events_url": "https://api.github.com/users/seerdecker/events{/privacy}", "received_events_url": "https://api.github.com/users/seerdecker/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-11T21:17:53Z", "updated_at": "2016-10-11T21:17:53Z", "author_association": "NONE", "body_html": "<p>Thanks for the answer. Yes, some examples of implementations would come handy.</p>\n<p>I don't understand how TF is providing freedom here. The implicit assumption that the last matrix rank corresponds to the batch instances seems to be built-in the design. For instance, how could you pack your data in rows in a matrix and pass that through a convolutional layer? The convolutional layer imposes semantics on the content of your tensors.</p>\n<p>The notion of a batch of gradients is well-defined in general (not speaking about TF specifically). You have a batch of inputs. The output of the network is a vector containing the loss value for each input. The batch of gradients is a tensor containing one entry per (input, loss) pair.</p>\n<p>In TF, in terms of semantics, I think all you'd have to do is to allow the user to specify a vector loss instead of a scalar loss in tf.gradients(), and then retrieve the requested batch of gradients.</p>\n<p>In practice TensorFlow is a deep learning framework, and the AD system is subservient to that goal. If it's hard to retrieve something so fundamental as a batch gradients, it seems to me that there's something missing in the API.</p>", "body_text": "Thanks for the answer. Yes, some examples of implementations would come handy.\nI don't understand how TF is providing freedom here. The implicit assumption that the last matrix rank corresponds to the batch instances seems to be built-in the design. For instance, how could you pack your data in rows in a matrix and pass that through a convolutional layer? The convolutional layer imposes semantics on the content of your tensors.\nThe notion of a batch of gradients is well-defined in general (not speaking about TF specifically). You have a batch of inputs. The output of the network is a vector containing the loss value for each input. The batch of gradients is a tensor containing one entry per (input, loss) pair.\nIn TF, in terms of semantics, I think all you'd have to do is to allow the user to specify a vector loss instead of a scalar loss in tf.gradients(), and then retrieve the requested batch of gradients.\nIn practice TensorFlow is a deep learning framework, and the AD system is subservient to that goal. If it's hard to retrieve something so fundamental as a batch gradients, it seems to me that there's something missing in the API.", "body": "Thanks for the answer. Yes, some examples of implementations would come handy.\n\nI don't understand how TF is providing freedom here. The implicit assumption that the last matrix rank corresponds to the batch instances seems to be built-in the design. For instance, how could you pack your data in rows in a matrix and pass that through a convolutional layer? The convolutional layer imposes semantics on the content of your tensors.\n\nThe notion of a batch of gradients is well-defined in general (not speaking about TF specifically). You have a batch of inputs. The output of the network is a vector containing the loss value for each input. The batch of gradients is a tensor containing one entry per (input, loss) pair.\n\nIn TF, in terms of semantics, I think all you'd have to do is to allow the user to specify a vector loss instead of a scalar loss in tf.gradients(), and then retrieve the requested batch of gradients.\n\nIn practice TensorFlow is a deep learning framework, and the AD system is subservient to that goal. If it's hard to retrieve something so fundamental as a batch gradients, it seems to me that there's something missing in the API.\n"}