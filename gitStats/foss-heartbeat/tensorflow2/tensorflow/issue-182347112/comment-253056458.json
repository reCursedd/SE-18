{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253056458", "html_url": "https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-253056458", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4897", "id": 253056458, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzA1NjQ1OA==", "user": {"login": "goodfeli", "id": 387866, "node_id": "MDQ6VXNlcjM4Nzg2Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/387866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goodfeli", "html_url": "https://github.com/goodfeli", "followers_url": "https://api.github.com/users/goodfeli/followers", "following_url": "https://api.github.com/users/goodfeli/following{/other_user}", "gists_url": "https://api.github.com/users/goodfeli/gists{/gist_id}", "starred_url": "https://api.github.com/users/goodfeli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goodfeli/subscriptions", "organizations_url": "https://api.github.com/users/goodfeli/orgs", "repos_url": "https://api.github.com/users/goodfeli/repos", "events_url": "https://api.github.com/users/goodfeli/events{/privacy}", "received_events_url": "https://api.github.com/users/goodfeli/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-11T21:46:18Z", "updated_at": "2016-10-11T21:46:18Z", "author_association": "NONE", "body_html": "<p>On the contrary, it's because it's an AD framework rather than a DL framework that it is somewhat difficult to get the batch gradients. The gradient of an individual example is a deep learning concept. As a pure math engine, tensorflow just exposes the concept of the gradient with respect to a specific tensor. Because there's no tensor representing the parameters as used on a single example, the AD engine doesn't have a concept for the DL value you want.</p>\n<p>This actually does have a reasonably efficient solution that doesn't require n tf.gradients calls. Do n calls to convolution, wrapping the same parameters in a different identity op each time. Then do one tf.gradients call getting the gradient with respect to each of the identity copies.</p>", "body_text": "On the contrary, it's because it's an AD framework rather than a DL framework that it is somewhat difficult to get the batch gradients. The gradient of an individual example is a deep learning concept. As a pure math engine, tensorflow just exposes the concept of the gradient with respect to a specific tensor. Because there's no tensor representing the parameters as used on a single example, the AD engine doesn't have a concept for the DL value you want.\nThis actually does have a reasonably efficient solution that doesn't require n tf.gradients calls. Do n calls to convolution, wrapping the same parameters in a different identity op each time. Then do one tf.gradients call getting the gradient with respect to each of the identity copies.", "body": "On the contrary, it's because it's an AD framework rather than a DL framework that it is somewhat difficult to get the batch gradients. The gradient of an individual example is a deep learning concept. As a pure math engine, tensorflow just exposes the concept of the gradient with respect to a specific tensor. Because there's no tensor representing the parameters as used on a single example, the AD engine doesn't have a concept for the DL value you want.\n\nThis actually does have a reasonably efficient solution that doesn't require n tf.gradients calls. Do n calls to convolution, wrapping the same parameters in a different identity op each time. Then do one tf.gradients call getting the gradient with respect to each of the identity copies.\n"}