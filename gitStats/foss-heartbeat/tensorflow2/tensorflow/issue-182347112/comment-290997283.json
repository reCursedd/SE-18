{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290997283", "html_url": "https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-290997283", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4897", "id": 290997283, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDk5NzI4Mw==", "user": {"login": "goodfeli", "id": 387866, "node_id": "MDQ6VXNlcjM4Nzg2Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/387866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goodfeli", "html_url": "https://github.com/goodfeli", "followers_url": "https://api.github.com/users/goodfeli/followers", "following_url": "https://api.github.com/users/goodfeli/following{/other_user}", "gists_url": "https://api.github.com/users/goodfeli/gists{/gist_id}", "starred_url": "https://api.github.com/users/goodfeli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goodfeli/subscriptions", "organizations_url": "https://api.github.com/users/goodfeli/orgs", "repos_url": "https://api.github.com/users/goodfeli/repos", "events_url": "https://api.github.com/users/goodfeli/events{/privacy}", "received_events_url": "https://api.github.com/users/goodfeli/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-02T16:29:57Z", "updated_at": "2017-04-02T16:29:57Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">This is only pseudocode, but basic idea is:\n\nexamples = tf.split(batch)\nweight_copies = [tf.identity(weights) for x in examples]\noutput = tf.stack(f(x, w) in zip(examples, weight_copies))\ncost = cost_function(output)\nper_example_gradients = tf.gradients(cost, weight_copies)</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Sat, Apr 1, 2017 at 1:47 PM, jjough ***@***.***&gt; wrote:\n Do n calls to convolution, wrapping the same parameters in a different\n identity op each time. Then do one tf.gradients call getting the gradient\n with respect to each of the identity copies.\n\n <a class=\"user-mention\" href=\"https://github.com/goodfeli\">@goodfeli</a> &lt;<a href=\"https://github.com/goodfeli\">https://github.com/goodfeli</a>&gt; could you please explain this\n further? Would love to see the trick, but not really sure what you mean.\n\n I'm manually calculating a full Jacobian by making a bunch of calls to\n tf.gradients, and it's very slow.\n\n\n def tf_jacobian(tensor2, tensor1, feed_dict):\n     \"\"\"\n     Computes the tensor d(tensor2)/d(tensor1) recursively.\n     \"\"\"\n     shape = list(sess.run(tf.shape(tensor2), feed_dict))\n     if shape:\n         return tf.stack([tf_jacobian(tf.squeeze(M, squeeze_dims = 0), tensor1, feed_dict) for M in tf.split(split_dim = 0, num_split = shape[0], value = tensor2)])\n     else:\n         grad = tf.gradients(tensor2, tensor1)\n         if grad[0] != None:\n             return tf.squeeze(grad, squeeze_dims = [0])\n         else:\n             return tf.zeros_like(tensor1)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"182347112\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4897\" href=\"https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-290946084\">#4897 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAXrGqYwDyn590JO8VdyFECt-I8LZwiIks5rrrfQgaJpZM4KT_Op\">https://github.com/notifications/unsubscribe-auth/AAXrGqYwDyn590JO8VdyFECt-I8LZwiIks5rrrfQgaJpZM4KT_Op</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "This is only pseudocode, but basic idea is:\n\nexamples = tf.split(batch)\nweight_copies = [tf.identity(weights) for x in examples]\noutput = tf.stack(f(x, w) in zip(examples, weight_copies))\ncost = cost_function(output)\nper_example_gradients = tf.gradients(cost, weight_copies)\n\u2026\nOn Sat, Apr 1, 2017 at 1:47 PM, jjough ***@***.***> wrote:\n Do n calls to convolution, wrapping the same parameters in a different\n identity op each time. Then do one tf.gradients call getting the gradient\n with respect to each of the identity copies.\n\n @goodfeli <https://github.com/goodfeli> could you please explain this\n further? Would love to see the trick, but not really sure what you mean.\n\n I'm manually calculating a full Jacobian by making a bunch of calls to\n tf.gradients, and it's very slow.\n\n\n def tf_jacobian(tensor2, tensor1, feed_dict):\n     \"\"\"\n     Computes the tensor d(tensor2)/d(tensor1) recursively.\n     \"\"\"\n     shape = list(sess.run(tf.shape(tensor2), feed_dict))\n     if shape:\n         return tf.stack([tf_jacobian(tf.squeeze(M, squeeze_dims = 0), tensor1, feed_dict) for M in tf.split(split_dim = 0, num_split = shape[0], value = tensor2)])\n     else:\n         grad = tf.gradients(tensor2, tensor1)\n         if grad[0] != None:\n             return tf.squeeze(grad, squeeze_dims = [0])\n         else:\n             return tf.zeros_like(tensor1)\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#4897 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAXrGqYwDyn590JO8VdyFECt-I8LZwiIks5rrrfQgaJpZM4KT_Op>\n .", "body": "This is only pseudocode, but basic idea is:\n\nexamples = tf.split(batch)\nweight_copies = [tf.identity(weights) for x in examples]\noutput = tf.stack(f(x, w) in zip(examples, weight_copies))\ncost = cost_function(output)\nper_example_gradients = tf.gradients(cost, weight_copies)\n\nOn Sat, Apr 1, 2017 at 1:47 PM, jjough <notifications@github.com> wrote:\n\n> Do n calls to convolution, wrapping the same parameters in a different\n> identity op each time. Then do one tf.gradients call getting the gradient\n> with respect to each of the identity copies.\n>\n> @goodfeli <https://github.com/goodfeli> could you please explain this\n> further? Would love to see the trick, but not really sure what you mean.\n>\n> I'm manually calculating a full Jacobian by making a bunch of calls to\n> tf.gradients, and it's very slow.\n>\n>\n> def tf_jacobian(tensor2, tensor1, feed_dict):\n>     \"\"\"\n>     Computes the tensor d(tensor2)/d(tensor1) recursively.\n>     \"\"\"\n>     shape = list(sess.run(tf.shape(tensor2), feed_dict))\n>     if shape:\n>         return tf.stack([tf_jacobian(tf.squeeze(M, squeeze_dims = 0), tensor1, feed_dict) for M in tf.split(split_dim = 0, num_split = shape[0], value = tensor2)])\n>     else:\n>         grad = tf.gradients(tensor2, tensor1)\n>         if grad[0] != None:\n>             return tf.squeeze(grad, squeeze_dims = [0])\n>         else:\n>             return tf.zeros_like(tensor1)\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-290946084>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAXrGqYwDyn590JO8VdyFECt-I8LZwiIks5rrrfQgaJpZM4KT_Op>\n> .\n>\n"}