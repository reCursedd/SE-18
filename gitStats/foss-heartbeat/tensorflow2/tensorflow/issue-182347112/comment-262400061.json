{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/262400061", "html_url": "https://github.com/tensorflow/tensorflow/issues/4897#issuecomment-262400061", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4897", "id": 262400061, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MjQwMDA2MQ==", "user": {"login": "jmacglashan", "id": 4700592, "node_id": "MDQ6VXNlcjQ3MDA1OTI=", "avatar_url": "https://avatars3.githubusercontent.com/u/4700592?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmacglashan", "html_url": "https://github.com/jmacglashan", "followers_url": "https://api.github.com/users/jmacglashan/followers", "following_url": "https://api.github.com/users/jmacglashan/following{/other_user}", "gists_url": "https://api.github.com/users/jmacglashan/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmacglashan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmacglashan/subscriptions", "organizations_url": "https://api.github.com/users/jmacglashan/orgs", "repos_url": "https://api.github.com/users/jmacglashan/repos", "events_url": "https://api.github.com/users/jmacglashan/events{/privacy}", "received_events_url": "https://api.github.com/users/jmacglashan/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-22T23:46:27Z", "updated_at": "2016-11-22T23:46:27Z", "author_association": "NONE", "body_html": "<p>I agree that it would be useful for various algorithms to be able to get each individual gradient per batch and it doesn't seem easy to do this. As jpiabrantes commented above, the issue is <strong>not</strong> limited to asking for the gradient of a tensor that does a a batch reduction. In fact, it's clear that the gradient on an aggregation operation should too be aggregated. The problem is tf.gradients also aggregates even when the output of the tensor by which you differentiating doesn't aggregate.</p>\n<p>I understand why it is the way it is for mathematical generality, but the very fact that tf.gradients has a flag for how aggregation is done suggests that we should be able to specify a batch mode where it does <em>not</em> aggregate the gradients and instead returns an NxMx... tensor where n is the number gradients over which it would normally aggregate and Mx... is the shape of the variable.</p>", "body_text": "I agree that it would be useful for various algorithms to be able to get each individual gradient per batch and it doesn't seem easy to do this. As jpiabrantes commented above, the issue is not limited to asking for the gradient of a tensor that does a a batch reduction. In fact, it's clear that the gradient on an aggregation operation should too be aggregated. The problem is tf.gradients also aggregates even when the output of the tensor by which you differentiating doesn't aggregate.\nI understand why it is the way it is for mathematical generality, but the very fact that tf.gradients has a flag for how aggregation is done suggests that we should be able to specify a batch mode where it does not aggregate the gradients and instead returns an NxMx... tensor where n is the number gradients over which it would normally aggregate and Mx... is the shape of the variable.", "body": "I agree that it would be useful for various algorithms to be able to get each individual gradient per batch and it doesn't seem easy to do this. As jpiabrantes commented above, the issue is **not** limited to asking for the gradient of a tensor that does a a batch reduction. In fact, it's clear that the gradient on an aggregation operation should too be aggregated. The problem is tf.gradients also aggregates even when the output of the tensor by which you differentiating doesn't aggregate.\r\n\r\nI understand why it is the way it is for mathematical generality, but the very fact that tf.gradients has a flag for how aggregation is done suggests that we should be able to specify a batch mode where it does *not* aggregate the gradients and instead returns an NxMx... tensor where n is the number gradients over which it would normally aggregate and Mx... is the shape of the variable."}