{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307215065", "html_url": "https://github.com/tensorflow/tensorflow/issues/10240#issuecomment-307215065", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10240", "id": 307215065, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzIxNTA2NQ==", "user": {"login": "lancerts", "id": 7495155, "node_id": "MDQ6VXNlcjc0OTUxNTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7495155?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lancerts", "html_url": "https://github.com/lancerts", "followers_url": "https://api.github.com/users/lancerts/followers", "following_url": "https://api.github.com/users/lancerts/following{/other_user}", "gists_url": "https://api.github.com/users/lancerts/gists{/gist_id}", "starred_url": "https://api.github.com/users/lancerts/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lancerts/subscriptions", "organizations_url": "https://api.github.com/users/lancerts/orgs", "repos_url": "https://api.github.com/users/lancerts/repos", "events_url": "https://api.github.com/users/lancerts/events{/privacy}", "received_events_url": "https://api.github.com/users/lancerts/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-08T20:16:11Z", "updated_at": "2017-06-08T20:16:11Z", "author_association": "NONE", "body_html": "<p>Another thing that bothers me is the following.  m = DNNclassifier(...), and after i trained the model m.fit(), the model is saved. Then i can later restore the model and use new data to train from there. However, i found that the data is accumulated into graph, eventually, i got a cpkt file ~300GB and breaks memory. The question is, how to adjust this? Why after m.fit, the input data as tensor will also be saved and after restore each time, this parts get accumulated...</p>", "body_text": "Another thing that bothers me is the following.  m = DNNclassifier(...), and after i trained the model m.fit(), the model is saved. Then i can later restore the model and use new data to train from there. However, i found that the data is accumulated into graph, eventually, i got a cpkt file ~300GB and breaks memory. The question is, how to adjust this? Why after m.fit, the input data as tensor will also be saved and after restore each time, this parts get accumulated...", "body": "Another thing that bothers me is the following.  m = DNNclassifier(...), and after i trained the model m.fit(), the model is saved. Then i can later restore the model and use new data to train from there. However, i found that the data is accumulated into graph, eventually, i got a cpkt file ~300GB and breaks memory. The question is, how to adjust this? Why after m.fit, the input data as tensor will also be saved and after restore each time, this parts get accumulated..."}