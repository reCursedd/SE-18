{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333623014", "html_url": "https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-333623014", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13221", "id": 333623014, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzYyMzAxNA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-02T18:29:07Z", "updated_at": "2017-10-02T18:29:07Z", "author_association": "MEMBER", "body_html": "<p>A const op is gradient in the gradient of tf.where. tf.where calls the \"Select\" op, whose gradient is <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L907\">here</a>. The gradient calls tf.zeros_like, <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1492\">which calls</a> tf.zeros, <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1439\">which calls</a> tf.constant.</p>\n<p>Everytime a GPU constant op is created, it allocates memory for the constant on the GPU. Every time a session is run with new feeds and fetches, it creates a new Executor which creates new OpKernels, including the new Const op which allocates memory again. Stateful nodes are cached so new OpKernels for stateful nodes are not recreated every time. See <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc#L1186\">here</a>. But Const is not stateful, so it is recreated every iteration since the sesssion is run with new fetches.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17578177\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cwhipkey\">@cwhipkey</a> what do you think is the solution here? Why are only stateful kernel cached? If all nodes were cached, this problem wouldn't occur.</p>", "body_text": "A const op is gradient in the gradient of tf.where. tf.where calls the \"Select\" op, whose gradient is here. The gradient calls tf.zeros_like, which calls tf.zeros, which calls tf.constant.\nEverytime a GPU constant op is created, it allocates memory for the constant on the GPU. Every time a session is run with new feeds and fetches, it creates a new Executor which creates new OpKernels, including the new Const op which allocates memory again. Stateful nodes are cached so new OpKernels for stateful nodes are not recreated every time. See here. But Const is not stateful, so it is recreated every iteration since the sesssion is run with new fetches.\n@cwhipkey what do you think is the solution here? Why are only stateful kernel cached? If all nodes were cached, this problem wouldn't occur.", "body": "A const op is gradient in the gradient of tf.where. tf.where calls the \"Select\" op, whose gradient is [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L907). The gradient calls tf.zeros_like, [which calls](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1492) tf.zeros, [which calls](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/array_ops.py#L1439) tf.constant.\r\n\r\nEverytime a GPU constant op is created, it allocates memory for the constant on the GPU. Every time a session is run with new feeds and fetches, it creates a new Executor which creates new OpKernels, including the new Const op which allocates memory again. Stateful nodes are cached so new OpKernels for stateful nodes are not recreated every time. See [here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/common_runtime/direct_session.cc#L1186). But Const is not stateful, so it is recreated every iteration since the sesssion is run with new fetches.\r\n\r\n@cwhipkey what do you think is the solution here? Why are only stateful kernel cached? If all nodes were cached, this problem wouldn't occur.\r\n"}