{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/332048860", "html_url": "https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-332048860", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13221", "id": 332048860, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMjA0ODg2MA==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-26T00:13:36Z", "updated_at": "2017-09-26T00:13:36Z", "author_association": "MEMBER", "body_html": "<p>In your example, you create new nodes in every iteration of the loop, which will make the graph larger every iteration. Moving all graph constructions outside the loop causes the memory issue to go away:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> test whether memory gets cleared on creating new sessions</span>\n<span class=\"pl-k\">import</span> sys, os, math, random\n\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span><span class=\"pl-k\">==</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  <span class=\"pl-k\">try</span>:\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">12000</span>\n    num_runs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\n\n    images <span class=\"pl-k\">=</span> tf.random_uniform([size, size])\n    var <span class=\"pl-k\">=</span> tf.Variable(tf.ones_like(images))\n    sess.run(var.initializer)\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">relu</span>(<span class=\"pl-smi\">x</span>):\n      <span class=\"pl-k\">return</span> tf.where(tf.less(x, <span class=\"pl-c1\">0.0</span>), x, x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>leaky_relu<span class=\"pl-pds\">'</span></span>)\n    cost <span class=\"pl-k\">=</span> tf.reduce_sum(relu(images<span class=\"pl-k\">+</span>var))\n\n    grads <span class=\"pl-k\">=</span> tf.gradients(cost, var)\n    memop <span class=\"pl-k\">=</span> tf.contrib.memory_stats.MaxBytesInUse()\n\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">10</span>):\n      _, memuse <span class=\"pl-k\">=</span> sess.run([grads, memop])\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Run <span class=\"pl-c1\">%d</span>, GBs in use <span class=\"pl-c1\">%.2f</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(i, memuse<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">9</span>))\n  <span class=\"pl-k\">except</span>:\n    <span class=\"pl-k\">pass</span>\n  <span class=\"pl-k\">finally</span>:\n    [memuse] <span class=\"pl-k\">=</span> sess.run([tf.contrib.memory_stats.MaxBytesInUse()])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Memory GBs in use <span class=\"pl-c1\">%.2f</span><span class=\"pl-pds\">\"</span></span><span class=\"pl-k\">%</span>(memuse<span class=\"pl-k\">/</span><span class=\"pl-c1\">10</span><span class=\"pl-k\">**</span><span class=\"pl-c1\">9</span>,))</pre></div>\n<p>Modifying the graph every iteration has a performance impact since it prevents some internal caching, but I am not sure why it causes the memory leak. Perhaps some tensors are being cached in GPU memory each time the session is run with a modified graph. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17578177\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cwhipkey\">@cwhipkey</a>, any ideas?</p>", "body_text": "In your example, you create new nodes in every iteration of the loop, which will make the graph larger every iteration. Moving all graph constructions outside the loop causes the memory issue to go away:\n# test whether memory gets cleared on creating new sessions\nimport sys, os, math, random\n\n\nimport tensorflow as tf\nimport numpy as np\n\nif __name__=='__main__':\n  try:\n    sess = tf.Session()\n    size = 12000\n    num_runs = 10\n\n    images = tf.random_uniform([size, size])\n    var = tf.Variable(tf.ones_like(images))\n    sess.run(var.initializer)\n    def relu(x):\n      return tf.where(tf.less(x, 0.0), x, x, name='leaky_relu')\n    cost = tf.reduce_sum(relu(images+var))\n\n    grads = tf.gradients(cost, var)\n    memop = tf.contrib.memory_stats.MaxBytesInUse()\n\n    for i in range(10):\n      _, memuse = sess.run([grads, memop])\n      print(\"Run %d, GBs in use %.2f\"%(i, memuse/10**9))\n  except:\n    pass\n  finally:\n    [memuse] = sess.run([tf.contrib.memory_stats.MaxBytesInUse()])\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\nModifying the graph every iteration has a performance impact since it prevents some internal caching, but I am not sure why it causes the memory leak. Perhaps some tensors are being cached in GPU memory each time the session is run with a modified graph. @cwhipkey, any ideas?", "body": "In your example, you create new nodes in every iteration of the loop, which will make the graph larger every iteration. Moving all graph constructions outside the loop causes the memory issue to go away:\r\n\r\n```python\r\n# test whether memory gets cleared on creating new sessions\r\nimport sys, os, math, random\r\n\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__=='__main__':\r\n  try:\r\n    sess = tf.Session()\r\n    size = 12000\r\n    num_runs = 10\r\n\r\n    images = tf.random_uniform([size, size])\r\n    var = tf.Variable(tf.ones_like(images))\r\n    sess.run(var.initializer)\r\n    def relu(x):\r\n      return tf.where(tf.less(x, 0.0), x, x, name='leaky_relu')\r\n    cost = tf.reduce_sum(relu(images+var))\r\n\r\n    grads = tf.gradients(cost, var)\r\n    memop = tf.contrib.memory_stats.MaxBytesInUse()\r\n\r\n    for i in range(10):\r\n      _, memuse = sess.run([grads, memop])\r\n      print(\"Run %d, GBs in use %.2f\"%(i, memuse/10**9))\r\n  except:\r\n    pass\r\n  finally:\r\n    [memuse] = sess.run([tf.contrib.memory_stats.MaxBytesInUse()])\r\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\r\n```\r\n\r\nModifying the graph every iteration has a performance impact since it prevents some internal caching, but I am not sure why it causes the memory leak. Perhaps some tensors are being cached in GPU memory each time the session is run with a modified graph. @cwhipkey, any ideas?"}