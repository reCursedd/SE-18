{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/347256496", "html_url": "https://github.com/tensorflow/tensorflow/issues/13221#issuecomment-347256496", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13221", "id": 347256496, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NzI1NjQ5Ng==", "user": {"login": "dantkz", "id": 5220571, "node_id": "MDQ6VXNlcjUyMjA1NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/5220571?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dantkz", "html_url": "https://github.com/dantkz", "followers_url": "https://api.github.com/users/dantkz/followers", "following_url": "https://api.github.com/users/dantkz/following{/other_user}", "gists_url": "https://api.github.com/users/dantkz/gists{/gist_id}", "starred_url": "https://api.github.com/users/dantkz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dantkz/subscriptions", "organizations_url": "https://api.github.com/users/dantkz/orgs", "repos_url": "https://api.github.com/users/dantkz/repos", "events_url": "https://api.github.com/users/dantkz/events{/privacy}", "received_events_url": "https://api.github.com/users/dantkz/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-27T17:22:55Z", "updated_at": "2017-11-27T17:22:55Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Actually, there may be no memory leak...</p>\n<p>I've put the memory_stat calls outside the graph, and everything started to work faster and without memory growth:</p>\n<pre><code>import sys, os, math, random\nimport tensorflow as tf\nimport numpy as np\n\nif __name__=='__main__':\n  def run_iters(relu):\n    from tensorflow.core.protobuf import rewriter_config_pb2\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\n      disable_model_pruning=True,\n      constant_folding=rewriter_config_pb2.RewriterConfig.OFF,\n      memory_optimization=rewriter_config_pb2.RewriterConfig.MANUAL)\n    optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\n    graph_options=tf.GraphOptions(optimizer_options=optimizer_options,\n                                  rewrite_options=rewrite_options)\n    config = tf.ConfigProto(graph_options=graph_options)\n\n    sess = tf.Session(config=config)\n    \n    size = 12000\n    num_runs = 20\n\n    images = tf.random_uniform([size, size])\n\n    var = tf.Variable(tf.ones_like(images))\n    sess.run(var.initializer)\n\n    cost = tf.reduce_sum(relu(images+var))\n    grads = tf.gradients(cost, var)\n\n    get_mem_max = tf.contrib.memory_stats.MaxBytesInUse()\n    get_mem_use = tf.contrib.memory_stats.BytesInUse()\n\n    memuse, memuse2 = sess.run([get_mem_max, get_mem_use])\n    print(\"Init: GBs in use %.2f, %.2f\"%(memuse/10**9,memuse2/10**9))\n    for i in range(10):\n      _, memuse, memuse2 = sess.run([grads, get_mem_max, get_mem_use])\n      print(\"Run %d, GBs in use %.2f, %.2f\"%(i, memuse/10**9,memuse2/10**9))\n    memuse = sess.run(get_mem_max)\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\n\n    sess.close()\n\n  alpha = 0.1\n\n  def relu_nowhere(x):\n    retval = alpha*x*tf.cast(tf.less(x, 0.), dtype=tf.float32) + x*tf.cast(tf.less(-x, 0.), dtype=tf.float32)\n    return retval\n  run_iters(relu_nowhere)\n\n  tf.reset_default_graph()\n\n  def relu_where(x):\n    retval = tf.where(tf.less(x, 0.0), alpha*x, x, name='leaky_relu')\n    return retval\n  run_iters(relu_where)\n\n</code></pre>\n<p>Result:</p>\n<pre><code>(python3) daniyar@sleepy-prism:~/tmp/tile_leak$ python test.py\n2017-11-27 17:21:07.930448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-27 17:21:08.190181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:02:00.0\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\n2017-11-27 17:21:08.190208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\nInit: GBs in use 1.15, 0.58\nRun 0, GBs in use 1.15, 0.58\nRun 1, GBs in use 2.45, 0.58\nRun 2, GBs in use 2.45, 0.58\nRun 3, GBs in use 2.45, 0.58\nRun 4, GBs in use 2.45, 0.58\nRun 5, GBs in use 2.45, 0.58\nRun 6, GBs in use 2.45, 0.58\nRun 7, GBs in use 2.45, 0.58\nRun 8, GBs in use 2.45, 0.58\nRun 9, GBs in use 2.45, 0.58\nMemory GBs in use 2.45\n2017-11-27 17:21:10.560484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\nInit: GBs in use 2.45, 0.58\nRun 0, GBs in use 2.45, 1.15\nRun 1, GBs in use 2.45, 1.15\nRun 2, GBs in use 2.45, 1.15\nRun 3, GBs in use 2.45, 1.15\nRun 4, GBs in use 2.45, 1.15\nRun 5, GBs in use 2.45, 1.15\nRun 6, GBs in use 2.45, 1.15\nRun 7, GBs in use 2.45, 1.15\nRun 8, GBs in use 2.45, 1.15\nRun 9, GBs in use 2.45, 1.15\nMemory GBs in use 2.45\n</code></pre>\n<p>It seems memory_stat has imperfect memory management. :)</p>", "body_text": "Actually, there may be no memory leak...\nI've put the memory_stat calls outside the graph, and everything started to work faster and without memory growth:\nimport sys, os, math, random\nimport tensorflow as tf\nimport numpy as np\n\nif __name__=='__main__':\n  def run_iters(relu):\n    from tensorflow.core.protobuf import rewriter_config_pb2\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\n      disable_model_pruning=True,\n      constant_folding=rewriter_config_pb2.RewriterConfig.OFF,\n      memory_optimization=rewriter_config_pb2.RewriterConfig.MANUAL)\n    optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\n    graph_options=tf.GraphOptions(optimizer_options=optimizer_options,\n                                  rewrite_options=rewrite_options)\n    config = tf.ConfigProto(graph_options=graph_options)\n\n    sess = tf.Session(config=config)\n    \n    size = 12000\n    num_runs = 20\n\n    images = tf.random_uniform([size, size])\n\n    var = tf.Variable(tf.ones_like(images))\n    sess.run(var.initializer)\n\n    cost = tf.reduce_sum(relu(images+var))\n    grads = tf.gradients(cost, var)\n\n    get_mem_max = tf.contrib.memory_stats.MaxBytesInUse()\n    get_mem_use = tf.contrib.memory_stats.BytesInUse()\n\n    memuse, memuse2 = sess.run([get_mem_max, get_mem_use])\n    print(\"Init: GBs in use %.2f, %.2f\"%(memuse/10**9,memuse2/10**9))\n    for i in range(10):\n      _, memuse, memuse2 = sess.run([grads, get_mem_max, get_mem_use])\n      print(\"Run %d, GBs in use %.2f, %.2f\"%(i, memuse/10**9,memuse2/10**9))\n    memuse = sess.run(get_mem_max)\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\n\n    sess.close()\n\n  alpha = 0.1\n\n  def relu_nowhere(x):\n    retval = alpha*x*tf.cast(tf.less(x, 0.), dtype=tf.float32) + x*tf.cast(tf.less(-x, 0.), dtype=tf.float32)\n    return retval\n  run_iters(relu_nowhere)\n\n  tf.reset_default_graph()\n\n  def relu_where(x):\n    retval = tf.where(tf.less(x, 0.0), alpha*x, x, name='leaky_relu')\n    return retval\n  run_iters(relu_where)\n\n\nResult:\n(python3) daniyar@sleepy-prism:~/tmp/tile_leak$ python test.py\n2017-11-27 17:21:07.930448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n2017-11-27 17:21:08.190181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\npciBusID: 0000:02:00.0\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\n2017-11-27 17:21:08.190208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\nInit: GBs in use 1.15, 0.58\nRun 0, GBs in use 1.15, 0.58\nRun 1, GBs in use 2.45, 0.58\nRun 2, GBs in use 2.45, 0.58\nRun 3, GBs in use 2.45, 0.58\nRun 4, GBs in use 2.45, 0.58\nRun 5, GBs in use 2.45, 0.58\nRun 6, GBs in use 2.45, 0.58\nRun 7, GBs in use 2.45, 0.58\nRun 8, GBs in use 2.45, 0.58\nRun 9, GBs in use 2.45, 0.58\nMemory GBs in use 2.45\n2017-11-27 17:21:10.560484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\nInit: GBs in use 2.45, 0.58\nRun 0, GBs in use 2.45, 1.15\nRun 1, GBs in use 2.45, 1.15\nRun 2, GBs in use 2.45, 1.15\nRun 3, GBs in use 2.45, 1.15\nRun 4, GBs in use 2.45, 1.15\nRun 5, GBs in use 2.45, 1.15\nRun 6, GBs in use 2.45, 1.15\nRun 7, GBs in use 2.45, 1.15\nRun 8, GBs in use 2.45, 1.15\nRun 9, GBs in use 2.45, 1.15\nMemory GBs in use 2.45\n\nIt seems memory_stat has imperfect memory management. :)", "body": "Actually, there may be no memory leak...\r\n\r\nI've put the memory_stat calls outside the graph, and everything started to work faster and without memory growth:\r\n\r\n```\r\nimport sys, os, math, random\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\nif __name__=='__main__':\r\n  def run_iters(relu):\r\n    from tensorflow.core.protobuf import rewriter_config_pb2\r\n    rewrite_options = rewriter_config_pb2.RewriterConfig(\r\n      disable_model_pruning=True,\r\n      constant_folding=rewriter_config_pb2.RewriterConfig.OFF,\r\n      memory_optimization=rewriter_config_pb2.RewriterConfig.MANUAL)\r\n    optimizer_options = tf.OptimizerOptions(opt_level=tf.OptimizerOptions.L0)\r\n    graph_options=tf.GraphOptions(optimizer_options=optimizer_options,\r\n                                  rewrite_options=rewrite_options)\r\n    config = tf.ConfigProto(graph_options=graph_options)\r\n\r\n    sess = tf.Session(config=config)\r\n    \r\n    size = 12000\r\n    num_runs = 20\r\n\r\n    images = tf.random_uniform([size, size])\r\n\r\n    var = tf.Variable(tf.ones_like(images))\r\n    sess.run(var.initializer)\r\n\r\n    cost = tf.reduce_sum(relu(images+var))\r\n    grads = tf.gradients(cost, var)\r\n\r\n    get_mem_max = tf.contrib.memory_stats.MaxBytesInUse()\r\n    get_mem_use = tf.contrib.memory_stats.BytesInUse()\r\n\r\n    memuse, memuse2 = sess.run([get_mem_max, get_mem_use])\r\n    print(\"Init: GBs in use %.2f, %.2f\"%(memuse/10**9,memuse2/10**9))\r\n    for i in range(10):\r\n      _, memuse, memuse2 = sess.run([grads, get_mem_max, get_mem_use])\r\n      print(\"Run %d, GBs in use %.2f, %.2f\"%(i, memuse/10**9,memuse2/10**9))\r\n    memuse = sess.run(get_mem_max)\r\n    print(\"Memory GBs in use %.2f\"%(memuse/10**9,))\r\n\r\n    sess.close()\r\n\r\n  alpha = 0.1\r\n\r\n  def relu_nowhere(x):\r\n    retval = alpha*x*tf.cast(tf.less(x, 0.), dtype=tf.float32) + x*tf.cast(tf.less(-x, 0.), dtype=tf.float32)\r\n    return retval\r\n  run_iters(relu_nowhere)\r\n\r\n  tf.reset_default_graph()\r\n\r\n  def relu_where(x):\r\n    retval = tf.where(tf.less(x, 0.0), alpha*x, x, name='leaky_relu')\r\n    return retval\r\n  run_iters(relu_where)\r\n\r\n```\r\n\r\nResult:\r\n\r\n```\r\n(python3) daniyar@sleepy-prism:~/tmp/tile_leak$ python test.py\r\n2017-11-27 17:21:07.930448: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n2017-11-27 17:21:08.190181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: TITAN X (Pascal) major: 6 minor: 1 memoryClockRate(GHz): 1.531\r\npciBusID: 0000:02:00.0\r\ntotalMemory: 11.90GiB freeMemory: 11.75GiB\r\n2017-11-27 17:21:08.190208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nInit: GBs in use 1.15, 0.58\r\nRun 0, GBs in use 1.15, 0.58\r\nRun 1, GBs in use 2.45, 0.58\r\nRun 2, GBs in use 2.45, 0.58\r\nRun 3, GBs in use 2.45, 0.58\r\nRun 4, GBs in use 2.45, 0.58\r\nRun 5, GBs in use 2.45, 0.58\r\nRun 6, GBs in use 2.45, 0.58\r\nRun 7, GBs in use 2.45, 0.58\r\nRun 8, GBs in use 2.45, 0.58\r\nRun 9, GBs in use 2.45, 0.58\r\nMemory GBs in use 2.45\r\n2017-11-27 17:21:10.560484: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: TITAN X (Pascal), pci bus id: 0000:02:00.0, compute capability: 6.1)\r\nInit: GBs in use 2.45, 0.58\r\nRun 0, GBs in use 2.45, 1.15\r\nRun 1, GBs in use 2.45, 1.15\r\nRun 2, GBs in use 2.45, 1.15\r\nRun 3, GBs in use 2.45, 1.15\r\nRun 4, GBs in use 2.45, 1.15\r\nRun 5, GBs in use 2.45, 1.15\r\nRun 6, GBs in use 2.45, 1.15\r\nRun 7, GBs in use 2.45, 1.15\r\nRun 8, GBs in use 2.45, 1.15\r\nRun 9, GBs in use 2.45, 1.15\r\nMemory GBs in use 2.45\r\n```\r\n\r\nIt seems memory_stat has imperfect memory management. :)\r\n"}