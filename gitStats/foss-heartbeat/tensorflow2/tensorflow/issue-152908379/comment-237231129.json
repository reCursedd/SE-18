{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237231129", "html_url": "https://github.com/tensorflow/tensorflow/pull/2213#issuecomment-237231129", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2213", "id": 237231129, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzIzMTEyOQ==", "user": {"login": "akors", "id": 3023492, "node_id": "MDQ6VXNlcjMwMjM0OTI=", "avatar_url": "https://avatars0.githubusercontent.com/u/3023492?v=4", "gravatar_id": "", "url": "https://api.github.com/users/akors", "html_url": "https://github.com/akors", "followers_url": "https://api.github.com/users/akors/followers", "following_url": "https://api.github.com/users/akors/following{/other_user}", "gists_url": "https://api.github.com/users/akors/gists{/gist_id}", "starred_url": "https://api.github.com/users/akors/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/akors/subscriptions", "organizations_url": "https://api.github.com/users/akors/orgs", "repos_url": "https://api.github.com/users/akors/repos", "events_url": "https://api.github.com/users/akors/events{/privacy}", "received_events_url": "https://api.github.com/users/akors/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-03T13:04:36Z", "updated_at": "2016-08-03T13:06:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20222547\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhongying811\">@zhongying811</a> The code on GPU is the same as on CPU, where it runs only depends on the TensorFlow version installed.</p>\n<p>I don't have a simple example, but basically, you can follow the <a href=\"https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts\" rel=\"nofollow\">tutorial for deep MNIST</a> for 2D data and go to 3D data, and replace</p>\n<pre><code>tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n</code></pre>\n<p>with</p>\n<pre><code>tf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME')\n</code></pre>\n<p>and</p>\n<pre><code>tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n</code></pre>\n<p>with</p>\n<pre><code>tf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n</code></pre>\n<p>And all shapes by addinge one dimension. Otherwise, it is really quite similar.</p>\n<p>If you are a beginner, here are two good online courses about machine learning in general:</p>\n<p><a href=\"https://www.coursera.org/learn/machine-learning\" rel=\"nofollow\">https://www.coursera.org/learn/machine-learning</a><br>\n<a href=\"https://www.udacity.com/course/machine-learning-engineer-nanodegree-by-google--nd009\" rel=\"nofollow\">https://www.udacity.com/course/machine-learning-engineer-nanodegree-by-google--nd009</a></p>\n<p>I suggest that you ask any other questions on <a href=\"http://stackoverflow.com/\" rel=\"nofollow\">StackOverflow</a> (using the tensorflow tag).</p>", "body_text": "@zhongying811 The code on GPU is the same as on CPU, where it runs only depends on the TensorFlow version installed.\nI don't have a simple example, but basically, you can follow the tutorial for deep MNIST for 2D data and go to 3D data, and replace\ntf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n\nwith\ntf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME')\n\nand\ntf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n\nwith\ntf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n\nAnd all shapes by addinge one dimension. Otherwise, it is really quite similar.\nIf you are a beginner, here are two good online courses about machine learning in general:\nhttps://www.coursera.org/learn/machine-learning\nhttps://www.udacity.com/course/machine-learning-engineer-nanodegree-by-google--nd009\nI suggest that you ask any other questions on StackOverflow (using the tensorflow tag).", "body": "@zhongying811 The code on GPU is the same as on CPU, where it runs only depends on the TensorFlow version installed.\n\nI don't have a simple example, but basically, you can follow the [tutorial for deep MNIST](https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html#deep-mnist-for-experts) for 2D data and go to 3D data, and replace\n\n```\ntf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n```\n\nwith\n\n```\ntf.nn.conv3d(x, W, strides=[1, 1, 1, 1, 1], padding='SAME')\n```\n\nand\n\n```\ntf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n```\n\nwith\n\n```\ntf.nn.max_pool3d(x, ksize=[1, 2, 2, 2, 1], strides=[1, 2, 2, 2, 1], padding='SAME')\n```\n\nAnd all shapes by addinge one dimension. Otherwise, it is really quite similar.\n\nIf you are a beginner, here are two good online courses about machine learning in general:\n\nhttps://www.coursera.org/learn/machine-learning\nhttps://www.udacity.com/course/machine-learning-engineer-nanodegree-by-google--nd009\n\nI suggest that you ask any other questions on [StackOverflow](http://stackoverflow.com/) (using the tensorflow tag).\n"}