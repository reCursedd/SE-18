{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3359", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3359/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3359/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3359/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3359", "id": 166055922, "node_id": "MDU6SXNzdWUxNjYwNTU5MjI=", "number": 3359, "title": "Gradient computation fails concatenation in while_loop() body", "user": {"login": "jacobvsdanniel", "id": 7235098, "node_id": "MDQ6VXNlcjcyMzUwOTg=", "avatar_url": "https://avatars1.githubusercontent.com/u/7235098?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jacobvsdanniel", "html_url": "https://github.com/jacobvsdanniel", "followers_url": "https://api.github.com/users/jacobvsdanniel/followers", "following_url": "https://api.github.com/users/jacobvsdanniel/following{/other_user}", "gists_url": "https://api.github.com/users/jacobvsdanniel/gists{/gist_id}", "starred_url": "https://api.github.com/users/jacobvsdanniel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jacobvsdanniel/subscriptions", "organizations_url": "https://api.github.com/users/jacobvsdanniel/orgs", "repos_url": "https://api.github.com/users/jacobvsdanniel/repos", "events_url": "https://api.github.com/users/jacobvsdanniel/events{/privacy}", "received_events_url": "https://api.github.com/users/jacobvsdanniel/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, {"login": "yuanbyu", "id": 2342391, "node_id": "MDQ6VXNlcjIzNDIzOTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2342391?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuanbyu", "html_url": "https://github.com/yuanbyu", "followers_url": "https://api.github.com/users/yuanbyu/followers", "following_url": "https://api.github.com/users/yuanbyu/following{/other_user}", "gists_url": "https://api.github.com/users/yuanbyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuanbyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuanbyu/subscriptions", "organizations_url": "https://api.github.com/users/yuanbyu/orgs", "repos_url": "https://api.github.com/users/yuanbyu/repos", "events_url": "https://api.github.com/users/yuanbyu/events{/privacy}", "received_events_url": "https://api.github.com/users/yuanbyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-07-18T09:40:56Z", "updated_at": "2017-02-09T22:02:11Z", "closed_at": "2016-08-31T00:06:46Z", "author_association": "NONE", "body_html": "<h3>Description</h3>\n<p>When I was trying to implement RNN with while_loop(), I tried to concatenate output to a matrix.<br>\nThis worked in forward passes but not in applying gradients.</p>\n<p>Also, I saw that there are discussions (<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"153256856\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2237\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2237/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2237\">#2237</a>) about supporting Recursive NN. There's a workaround by transforming tree structures to a matrix. For example, if we have a binary tree with its node values and structure be like</p>\n<pre><code>10------20------40\n |       |------50\n | \n |------30------60\n         |------70\n</code></pre>\n<p>Then we could transform it to a value vector V</p>\n<pre><code>[70 60 50 40 30 20 10]\n</code></pre>\n<p>and a (strictly bottom-up) structure matrix M</p>\n<pre><code>[[0 1 4]  # V[0] and V[1] are V[4]'s children\n [2 3 5]\n [4 5 6]]\n</code></pre>\n<p>Then we could build our graph with while_loop() by iteratively index into previous output. Note that this is without any specific inputs; only knowing they'll be a vector and a matrix instead.<br>\nFor more details, see<br>\n<a href=\"https://github.com/jacobvsdanniel/tf_rnn\">https://github.com/jacobvsdanniel/tf_rnn</a><br>\ninspired by<br>\n<a href=\"https://github.com/ofirnachum/tree_rnn\">https://github.com/ofirnachum/tree_rnn</a><br>\nI also ran into problems of computing gradients for nested gather inside while_loop(), but managed to worked around before fixes come up to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"120573591\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/418\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/418/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/418\">#418</a> and <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"116827263\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/206\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/206/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/206\">#206</a> .</p>\n<h3>Tensorflow version</h3>\n<p>0.9.0</p>\n<h3>Reproduction steps</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">test_concat_loop</span>():\n    x <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">1</span>.,<span class=\"pl-c1\">2</span>.]])\n    X <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>x)\n\n    i <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n    H <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">condition</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">H</span>):\n        <span class=\"pl-k\">return</span> i <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">2</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">body</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">H</span>):\n        <span class=\"pl-k\">return</span> i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, tf.concat(<span class=\"pl-c1\">0</span>, [H, X])\n\n    _, H <span class=\"pl-k\">=</span> tf.while_loop(condition, body, [i, H])\n    s <span class=\"pl-k\">=</span> tf.reduce_sum(H)\n\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    sess.run(tf.initialize_all_variables())\n    <span class=\"pl-c1\">print</span> sess.run(X)\n    <span class=\"pl-c1\">print</span> sess.run(H)\n    <span class=\"pl-c1\">print</span> sess.run(s)\n\n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.01</span>)\n    op <span class=\"pl-k\">=</span> optimizer.minimize(s) <span class=\"pl-c\"><span class=\"pl-c\">#</span>Raise</span>\n    <span class=\"pl-c1\">print</span> sess.run(op)\n    <span class=\"pl-c1\">print</span> sess.run(X)\n    <span class=\"pl-k\">return</span></pre></div>\n<h3>Workaround</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">test_concat_loop_workaround</span>():\n    x <span class=\"pl-k\">=</span> tf.constant([[<span class=\"pl-c1\">1</span>.,<span class=\"pl-c1\">2</span>.]])\n    X <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>X<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>x)\n\n    i <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">0</span>)\n    H <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">2</span>])\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">condition</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">H</span>):\n        <span class=\"pl-k\">return</span> i <span class=\"pl-k\">&lt;</span> <span class=\"pl-c1\">5</span>\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">body</span>(<span class=\"pl-smi\">i</span>, <span class=\"pl-smi\">H</span>):\n        past <span class=\"pl-k\">=</span> tf.zeros([i, <span class=\"pl-c1\">2</span>])\n        future <span class=\"pl-k\">=</span> tf.zeros([<span class=\"pl-c1\">4</span><span class=\"pl-k\">-</span>i, <span class=\"pl-c1\">2</span>])\n        <span class=\"pl-k\">return</span> i<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, H <span class=\"pl-k\">+</span> tf.concat(<span class=\"pl-c1\">0</span>, [past, X, future])\n\n    _, H <span class=\"pl-k\">=</span> tf.while_loop(condition, body, [i, H])\n    s <span class=\"pl-k\">=</span> tf.reduce_sum(H)\n\n    sess <span class=\"pl-k\">=</span> tf.Session()\n    sess.run(tf.initialize_all_variables())\n    <span class=\"pl-c1\">print</span> sess.run(X)\n    <span class=\"pl-c1\">print</span> sess.run(H)\n    <span class=\"pl-c1\">print</span> sess.run(s)\n\n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.01</span>)\n    op <span class=\"pl-k\">=</span> optimizer.minimize(s)\n    <span class=\"pl-c1\">print</span> sess.run(op)\n    <span class=\"pl-c1\">print</span> sess.run(X)\n    <span class=\"pl-k\">return</span></pre></div>\n<h3>Error Logs</h3>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.329\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n[[ 1.  2.]]\n[[ 1.  2.]\n [ 1.  2.]]\n6.0\nTraceback (most recent call last):\n  File \"issue-418.py\", line 244, in &lt;module&gt;\n    main()\n  File \"issue-418.py\", line 233, in main\n    test_concat_loop()\n  File \"issue-418.py\", line 53, in test_concat_loop\n    op = optimizer.minimize(s)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 193, in minimize\n    grad_loss=grad_loss)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 494, in gradients\n    in_grad.set_shape(t_in.get_shape())\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 404, in set_shape\n    self._shape = self._shape.merge_with(shape)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in merge_with\n    (self, other))\nValueError: Shapes (0, 2) and (1, 2) are not compatible\n</code></pre>", "body_text": "Description\nWhen I was trying to implement RNN with while_loop(), I tried to concatenate output to a matrix.\nThis worked in forward passes but not in applying gradients.\nAlso, I saw that there are discussions (#2237) about supporting Recursive NN. There's a workaround by transforming tree structures to a matrix. For example, if we have a binary tree with its node values and structure be like\n10------20------40\n |       |------50\n | \n |------30------60\n         |------70\n\nThen we could transform it to a value vector V\n[70 60 50 40 30 20 10]\n\nand a (strictly bottom-up) structure matrix M\n[[0 1 4]  # V[0] and V[1] are V[4]'s children\n [2 3 5]\n [4 5 6]]\n\nThen we could build our graph with while_loop() by iteratively index into previous output. Note that this is without any specific inputs; only knowing they'll be a vector and a matrix instead.\nFor more details, see\nhttps://github.com/jacobvsdanniel/tf_rnn\ninspired by\nhttps://github.com/ofirnachum/tree_rnn\nI also ran into problems of computing gradients for nested gather inside while_loop(), but managed to worked around before fixes come up to #418 and #206 .\nTensorflow version\n0.9.0\nReproduction steps\ndef test_concat_loop():\n    x = tf.constant([[1.,2.]])\n    X = tf.get_variable(\"X\", initializer=x)\n\n    i = tf.constant(0)\n    H = tf.zeros([0, 2])\n\n    def condition(i, H):\n        return i < 2\n\n    def body(i, H):\n        return i+1, tf.concat(0, [H, X])\n\n    _, H = tf.while_loop(condition, body, [i, H])\n    s = tf.reduce_sum(H)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    print sess.run(X)\n    print sess.run(H)\n    print sess.run(s)\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    op = optimizer.minimize(s) #Raise\n    print sess.run(op)\n    print sess.run(X)\n    return\nWorkaround\ndef test_concat_loop_workaround():\n    x = tf.constant([[1.,2.]])\n    X = tf.get_variable(\"X\", initializer=x)\n\n    i = tf.constant(0)\n    H = tf.zeros([5, 2])\n\n    def condition(i, H):\n        return i < 5\n\n    def body(i, H):\n        past = tf.zeros([i, 2])\n        future = tf.zeros([4-i, 2])\n        return i+1, H + tf.concat(0, [past, X, future])\n\n    _, H = tf.while_loop(condition, body, [i, H])\n    s = tf.reduce_sum(H)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    print sess.run(X)\n    print sess.run(H)\n    print sess.run(s)\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    op = optimizer.minimize(s)\n    print sess.run(op)\n    print sess.run(X)\n    return\nError Logs\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.329\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n[[ 1.  2.]]\n[[ 1.  2.]\n [ 1.  2.]]\n6.0\nTraceback (most recent call last):\n  File \"issue-418.py\", line 244, in <module>\n    main()\n  File \"issue-418.py\", line 233, in main\n    test_concat_loop()\n  File \"issue-418.py\", line 53, in test_concat_loop\n    op = optimizer.minimize(s)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 193, in minimize\n    grad_loss=grad_loss)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 494, in gradients\n    in_grad.set_shape(t_in.get_shape())\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 404, in set_shape\n    self._shape = self._shape.merge_with(shape)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in merge_with\n    (self, other))\nValueError: Shapes (0, 2) and (1, 2) are not compatible", "body": "### Description\n\nWhen I was trying to implement RNN with while_loop(), I tried to concatenate output to a matrix.\nThis worked in forward passes but not in applying gradients.\n\nAlso, I saw that there are discussions (#2237) about supporting Recursive NN. There's a workaround by transforming tree structures to a matrix. For example, if we have a binary tree with its node values and structure be like\n\n```\n10------20------40\n |       |------50\n | \n |------30------60\n         |------70\n```\n\nThen we could transform it to a value vector V\n\n```\n[70 60 50 40 30 20 10]\n```\n\nand a (strictly bottom-up) structure matrix M\n\n```\n[[0 1 4]  # V[0] and V[1] are V[4]'s children\n [2 3 5]\n [4 5 6]]\n```\n\nThen we could build our graph with while_loop() by iteratively index into previous output. Note that this is without any specific inputs; only knowing they'll be a vector and a matrix instead.\nFor more details, see\nhttps://github.com/jacobvsdanniel/tf_rnn\ninspired by\nhttps://github.com/ofirnachum/tree_rnn\nI also ran into problems of computing gradients for nested gather inside while_loop(), but managed to worked around before fixes come up to #418 and #206 .\n### Tensorflow version\n\n0.9.0\n### Reproduction steps\n\n``` python\ndef test_concat_loop():\n    x = tf.constant([[1.,2.]])\n    X = tf.get_variable(\"X\", initializer=x)\n\n    i = tf.constant(0)\n    H = tf.zeros([0, 2])\n\n    def condition(i, H):\n        return i < 2\n\n    def body(i, H):\n        return i+1, tf.concat(0, [H, X])\n\n    _, H = tf.while_loop(condition, body, [i, H])\n    s = tf.reduce_sum(H)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    print sess.run(X)\n    print sess.run(H)\n    print sess.run(s)\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    op = optimizer.minimize(s) #Raise\n    print sess.run(op)\n    print sess.run(X)\n    return\n```\n### Workaround\n\n``` python\ndef test_concat_loop_workaround():\n    x = tf.constant([[1.,2.]])\n    X = tf.get_variable(\"X\", initializer=x)\n\n    i = tf.constant(0)\n    H = tf.zeros([5, 2])\n\n    def condition(i, H):\n        return i < 5\n\n    def body(i, H):\n        past = tf.zeros([i, 2])\n        future = tf.zeros([4-i, 2])\n        return i+1, H + tf.concat(0, [past, X, future])\n\n    _, H = tf.while_loop(condition, body, [i, H])\n    s = tf.reduce_sum(H)\n\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables())\n    print sess.run(X)\n    print sess.run(H)\n    print sess.run(s)\n\n    optimizer = tf.train.GradientDescentOptimizer(0.01)\n    op = optimizer.minimize(s)\n    print sess.run(op)\n    print sess.run(X)\n    return\n```\n### Error Logs\n\n```\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:924] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 980\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.329\npciBusID 0000:01:00.0\nTotal memory: 4.00GiB\nFree memory: 3.86GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 980, pci bus id: 0000:01:00.0)\n[[ 1.  2.]]\n[[ 1.  2.]\n [ 1.  2.]]\n6.0\nTraceback (most recent call last):\n  File \"issue-418.py\", line 244, in <module>\n    main()\n  File \"issue-418.py\", line 233, in main\n    test_concat_loop()\n  File \"issue-418.py\", line 53, in test_concat_loop\n    op = optimizer.minimize(s)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 193, in minimize\n    grad_loss=grad_loss)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.py\", line 250, in compute_gradients\n    colocate_gradients_with_ops=colocate_gradients_with_ops)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/ops/gradients.py\", line 494, in gradients\n    in_grad.set_shape(t_in.get_shape())\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 404, in set_shape\n    self._shape = self._shape.merge_with(shape)\n  File \"/home/danniel/.local/lib/python2.7/site-packages/tensorflow/python/framework/tensor_shape.py\", line 570, in merge_with\n    (self, other))\nValueError: Shapes (0, 2) and (1, 2) are not compatible\n```\n"}