{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23320", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23320/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23320/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23320/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23320", "id": 374700585, "node_id": "MDU6SXNzdWUzNzQ3MDA1ODU=", "number": 23320, "title": "[Feature Request] Addition of new operation to Tensorflow Lite for \"ENet\"", "user": {"login": "PINTO0309", "id": 33194443, "node_id": "MDQ6VXNlcjMzMTk0NDQz", "avatar_url": "https://avatars3.githubusercontent.com/u/33194443?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PINTO0309", "html_url": "https://github.com/PINTO0309", "followers_url": "https://api.github.com/users/PINTO0309/followers", "following_url": "https://api.github.com/users/PINTO0309/following{/other_user}", "gists_url": "https://api.github.com/users/PINTO0309/gists{/gist_id}", "starred_url": "https://api.github.com/users/PINTO0309/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PINTO0309/subscriptions", "organizations_url": "https://api.github.com/users/PINTO0309/orgs", "repos_url": "https://api.github.com/users/PINTO0309/repos", "events_url": "https://api.github.com/users/PINTO0309/events{/privacy}", "received_events_url": "https://api.github.com/users/PINTO0309/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-27T23:36:56Z", "updated_at": "2018-11-20T13:25:16Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a feature request. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version (you are using): <strong><a href=\"https://github.com/PINTO0309/Tensorflow-bin.git\">Tensorflow Lite v1.11.0 (Self-Build)</a></strong></li>\n<li>Hardware: RaspberryPi3</li>\n<li>OS: Raspbian Stretch</li>\n<li>Are you willing to contribute it (Yes/No): No (Because, There are few Custom Operation tutorials, and I can not write C++ programs.)</li>\n</ul>\n<p><strong>Describe the feature and the current behavior/state.</strong><br>\nIf I implement \"semantic segmentation\" model \"ENet\", I can not do it unless you support various behaviors of the unpooling layer.</p>\n<ul>\n<li>\n<p><strong>Layer that I would like to support with Tensorflow Lite</strong></p>\n<ul>\n<li>FloorMod</li>\n<li>Range</li>\n<li>Rank</li>\n<li>Abs</li>\n<li>MaxPoolWithArgmax</li>\n<li>ScatterNd</li>\n<li>SparseTensor</li>\n<li>sparse_add</li>\n<li>gather_nd</li>\n</ul>\n</li>\n<li>\n<p><strong>Sample message of Unsupport Error</strong></p>\n</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre>Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>main.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">5</span>, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    interpreter <span class=\"pl-k\">=</span> tf.contrib.lite.Interpreter(<span class=\"pl-v\">model_path</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>semanticsegmentation_enet_non_quantized.tflite<span class=\"pl-pds\">\"</span></span>)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/interpreter.py<span class=\"pl-pds\">\"</span></span>, line <span class=\"pl-c1\">53</span>, <span class=\"pl-k\">in</span> <span class=\"pl-c1\">__init__</span>\n    model_path))\n<span class=\"pl-c1\">ValueError</span>: Didn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t find custom op for name <span class=\"pl-pds\">'</span></span>FloorMod<span class=\"pl-s\"><span class=\"pl-pds\">'</span> with version 1<span class=\"pl-ii\"></span></span>\nDidn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t find custom op for name <span class=\"pl-pds\">'</span></span>Range<span class=\"pl-s\"><span class=\"pl-pds\">'</span> with version 1<span class=\"pl-ii\"></span></span>\nDidn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t find custom op for name <span class=\"pl-pds\">'</span></span>Rank<span class=\"pl-s\"><span class=\"pl-pds\">'</span> with version 1<span class=\"pl-ii\"></span></span>\nDidn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t find custom op for name <span class=\"pl-pds\">'</span></span>Abs<span class=\"pl-s\"><span class=\"pl-pds\">'</span> with version 1<span class=\"pl-ii\"></span></span>\nDidn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t find custom op for name <span class=\"pl-pds\">'</span></span>MaxPoolWithArgmax<span class=\"pl-s\"><span class=\"pl-pds\">'</span> with version 1<span class=\"pl-ii\"></span></span>\nDidn<span class=\"pl-s\"><span class=\"pl-pds\">'</span>t find custom op for name <span class=\"pl-pds\">'</span></span>ScatterNd<span class=\"pl-s\"><span class=\"pl-pds\">'</span> with version 1<span class=\"pl-ii\"></span></span>\nRegistration failed.</pre></div>\n<p><strong>Will this change the current api? How?</strong><br>\nYes.<br>\nCustom Operation must be officially implemented.</p>\n<p><strong>Who will benefit with this feature?</strong></p>\n<ul>\n<li>Engineers who study automatic driving technology in general</li>\n<li>Engineer who studies fast inference using lightweight mobile</li>\n<li>Engineers to study lightweight models</li>\n</ul>\n<p><strong>Any Other info.</strong><br>\nMy repository and sample program are below.<br>\nFor Python2.x / Python3.x.</p>\n<p><strong><a href=\"https://github.com/PINTO0309/TensorFlow-ENet.git\">https://github.com/PINTO0309/TensorFlow-ENet.git</a></strong><br>\n<strong><a href=\"https://github.com/PINTO0309/TensorflowLite-UNet.git\">https://github.com/PINTO0309/TensorflowLite-UNet.git</a></strong></p>\n<details><summary>\u3010Reference\u3011 Model Logic</summary><div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.layers.python.layers <span class=\"pl-k\">import</span> initializers\nslim <span class=\"pl-k\">=</span> tf.contrib.slim\n\n<span class=\"pl-en\">@slim.add_arg_scope</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">prelu</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">scope</span>, <span class=\"pl-smi\">decoder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>If decoder, then perform relu and just return the output</span>\n    <span class=\"pl-k\">if</span> decoder:\n        <span class=\"pl-k\">return</span> tf.nn.relu(x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope)\n\n    alpha<span class=\"pl-k\">=</span> tf.get_variable(scope <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>alpha<span class=\"pl-pds\">'</span></span>, x.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>],\n                       <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>),\n                        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    pos <span class=\"pl-k\">=</span> tf.nn.relu(x)\n    neg <span class=\"pl-k\">=</span> alpha <span class=\"pl-k\">*</span> (x <span class=\"pl-k\">-</span> <span class=\"pl-c1\">abs</span>(x)) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">0.5</span>\n    <span class=\"pl-k\">return</span> pos <span class=\"pl-k\">+</span> neg\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">spatial_dropout</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">p</span>, <span class=\"pl-smi\">seed</span>, <span class=\"pl-smi\">scope</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n\n    <span class=\"pl-k\">if</span> is_training:\n        keep_prob <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1.0</span> <span class=\"pl-k\">-</span> p\n        input_shape <span class=\"pl-k\">=</span> x.get_shape().as_list()\n        noise_shape <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-v\">value</span><span class=\"pl-k\">=</span>[input_shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, input_shape[<span class=\"pl-c1\">3</span>]])\n        output <span class=\"pl-k\">=</span> tf.nn.dropout(x, keep_prob, noise_shape, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope)\n\n        <span class=\"pl-k\">return</span> output\n\n    <span class=\"pl-k\">return</span> x\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">unpool</span>(<span class=\"pl-smi\">updates</span>, <span class=\"pl-smi\">mask</span>, <span class=\"pl-smi\">k_size</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-smi\">output_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-pds\">'</span></span>):\n\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n        mask <span class=\"pl-k\">=</span> tf.cast(mask, tf.int32)\n        input_shape <span class=\"pl-k\">=</span> tf.shape(updates, <span class=\"pl-v\">out_type</span><span class=\"pl-k\">=</span>tf.int32)\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>  calculation new shape</span>\n        <span class=\"pl-k\">if</span> output_shape <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n            output_shape <span class=\"pl-k\">=</span> (input_shape[<span class=\"pl-c1\">0</span>], input_shape[<span class=\"pl-c1\">1</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">1</span>], input_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> ksize[<span class=\"pl-c1\">2</span>], input_shape[<span class=\"pl-c1\">3</span>])\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> calculation indices for batch, height, width and feature maps</span>\n        one_like_mask <span class=\"pl-k\">=</span> tf.ones_like(mask, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n        batch_shape <span class=\"pl-k\">=</span> tf.concat([[input_shape[<span class=\"pl-c1\">0</span>]], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>], [<span class=\"pl-c1\">1</span>]], <span class=\"pl-c1\">0</span>)\n        batch_range <span class=\"pl-k\">=</span> tf.reshape(tf.range(output_shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32), <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>batch_shape)\n        b <span class=\"pl-k\">=</span> one_like_mask <span class=\"pl-k\">*</span> batch_range\n        y <span class=\"pl-k\">=</span> mask <span class=\"pl-k\">//</span> (output_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-k\">*</span> output_shape[<span class=\"pl-c1\">3</span>])\n        x <span class=\"pl-k\">=</span> (mask <span class=\"pl-k\">//</span> output_shape[<span class=\"pl-c1\">3</span>]) <span class=\"pl-k\">%</span> output_shape[<span class=\"pl-c1\">2</span>] <span class=\"pl-c\"><span class=\"pl-c\">#</span>mask % (output_shape[2] * output_shape[3]) // output_shape[3]</span>\n        feature_range <span class=\"pl-k\">=</span> tf.range(output_shape[<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\n        f <span class=\"pl-k\">=</span> one_like_mask <span class=\"pl-k\">*</span> feature_range\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> transpose indices &amp; reshape update values to one dimension</span>\n        updates_size <span class=\"pl-k\">=</span> tf.size(updates)\n        indices <span class=\"pl-k\">=</span> tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [<span class=\"pl-c1\">4</span>, updates_size]))\n        values <span class=\"pl-k\">=</span> tf.reshape(updates, [updates_size])\n        ret <span class=\"pl-k\">=</span> tf.scatter_nd(indices, values, output_shape)\n        <span class=\"pl-k\">return</span> ret\n\n<span class=\"pl-en\">@slim.add_arg_scope</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">initial_block</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>initial_block<span class=\"pl-pds\">'</span></span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Convolutional branch</span>\n    net_conv <span class=\"pl-k\">=</span> slim.conv2d(inputs, <span class=\"pl-c1\">13</span>, [<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv<span class=\"pl-pds\">'</span></span>)\n    net_conv <span class=\"pl-k\">=</span> slim.batch_norm(net_conv, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batchnorm<span class=\"pl-pds\">'</span></span>)\n    net_conv <span class=\"pl-k\">=</span> prelu(net_conv, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Max pool branch</span>\n    net_pool <span class=\"pl-k\">=</span> slim.max_pool2d(inputs, [<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_max_pool<span class=\"pl-pds\">'</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Concatenated output - does it matter max pool comes first or conv comes first? probably not.</span>\n    net_concatenated <span class=\"pl-k\">=</span> tf.concat([net_conv, net_pool], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_concat<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-k\">return</span> net_concatenated\n\n<span class=\"pl-en\">@slim.add_arg_scope</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">bottleneck</span>(<span class=\"pl-smi\">inputs</span>,\n               <span class=\"pl-smi\">output_depth</span>,\n               <span class=\"pl-smi\">filter_size</span>,\n               <span class=\"pl-smi\">regularizer_prob</span>,\n               <span class=\"pl-smi\">projection_ratio</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>,\n               <span class=\"pl-smi\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>,\n               <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n               <span class=\"pl-smi\">downsampling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n               <span class=\"pl-smi\">upsampling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n               <span class=\"pl-smi\">pooling_indices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n               <span class=\"pl-smi\">output_shape</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n               <span class=\"pl-smi\">dilated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n               <span class=\"pl-smi\">dilation_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n               <span class=\"pl-smi\">asymmetric</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n               <span class=\"pl-smi\">decoder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,\n               <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Calculate the depth reduction based on the projection ratio used in 1x1 convolution.</span>\n    reduced_depth <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(inputs.get_shape().as_list()[<span class=\"pl-c1\">3</span>] <span class=\"pl-k\">/</span> projection_ratio)\n\n    <span class=\"pl-k\">with</span> slim.arg_scope([prelu], <span class=\"pl-v\">decoder</span><span class=\"pl-k\">=</span>decoder):\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>=============DOWNSAMPLING BOTTLENECK====================</span>\n        <span class=\"pl-k\">if</span> downsampling:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>=============MAIN BRANCH=============</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Just perform a max pooling</span>\n            net_main, pooling_indices <span class=\"pl-k\">=</span> tf.nn.max_pool_with_argmax(inputs,\n                                                                   <span class=\"pl-v\">ksize</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],\n                                                                   <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>],\n                                                                   <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>,\n                                                                   <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_main_max_pool<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>First get the difference in depth to pad, then pad with zeros only on the last dimension.</span>\n            inputs_shape <span class=\"pl-k\">=</span> inputs.get_shape().as_list()\n            depth_to_pad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">abs</span>(inputs_shape[<span class=\"pl-c1\">3</span>] <span class=\"pl-k\">-</span> output_depth)\n            paddings <span class=\"pl-k\">=</span> tf.convert_to_tensor([[<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, depth_to_pad]])\n            net_main <span class=\"pl-k\">=</span> tf.pad(net_main, <span class=\"pl-v\">paddings</span><span class=\"pl-k\">=</span>paddings, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_main_padding<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>=============SUB BRANCH==============</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>First projection that has a 2x2 kernel and stride 2</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(inputs, reduced_depth, [<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu1<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Second conv block</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, reduced_depth, [filter_size, filter_size], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu2<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Final projection with 1x1 kernel</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, output_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu3<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Regularizer</span>\n            net <span class=\"pl-k\">=</span> spatial_dropout(net, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>regularizer_prob, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_spatial_dropout<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Finally, combine the two branches together via an element-wise addition</span>\n            net <span class=\"pl-k\">=</span> tf.add(net, net_main, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_add<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_last_prelu<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>also return inputs shape for convenience later</span>\n            <span class=\"pl-k\">return</span> net, pooling_indices, inputs_shape\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>============DILATION CONVOLUTION BOTTLENECK====================</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Everything is the same as a regular bottleneck except for the dilation rate argument</span>\n        <span class=\"pl-k\">elif</span> dilated:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Check if dilation rate is given</span>\n            <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> dilation_rate:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Dilation rate is not given.<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Save the main branch for addition later</span>\n            net_main <span class=\"pl-k\">=</span> inputs\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>First projection with 1x1 kernel (dimensionality reduction)</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(inputs, reduced_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu1<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Second conv block --- apply dilated convolution here</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, reduced_depth, [filter_size, filter_size], <span class=\"pl-v\">rate</span><span class=\"pl-k\">=</span>dilation_rate, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_dilated_conv2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu2<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Final projection with 1x1 kernel (Expansion)</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, output_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu3<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Regularizer</span>\n            net <span class=\"pl-k\">=</span> spatial_dropout(net, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>regularizer_prob, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_spatial_dropout<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu4<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Add the main branch</span>\n            net <span class=\"pl-k\">=</span> tf.add(net_main, net, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_add_dilated<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_last_prelu<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-k\">return</span> net\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>===========ASYMMETRIC CONVOLUTION BOTTLENECK==============</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Everything is the same as a regular bottleneck except for a [5,5] kernel decomposed into two [5,1] then [1,5]</span>\n        <span class=\"pl-k\">elif</span> asymmetric:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Save the main branch for addition later</span>\n            net_main <span class=\"pl-k\">=</span> inputs\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>First projection with 1x1 kernel (dimensionality reduction)</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(inputs, reduced_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu1<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Second conv block --- apply asymmetric conv here</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, reduced_depth, [filter_size, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_asymmetric_conv2a<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, reduced_depth, [<span class=\"pl-c1\">1</span>, filter_size], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_asymmetric_conv2b<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu2<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Final projection with 1x1 kernel</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, output_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu3<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Regularizer</span>\n            net <span class=\"pl-k\">=</span> spatial_dropout(net, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>regularizer_prob, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_spatial_dropout<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu4<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Add the main branch</span>\n            net <span class=\"pl-k\">=</span> tf.add(net_main, net, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_add_asymmetric<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_last_prelu<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-k\">return</span> net\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>============UPSAMPLING BOTTLENECK================</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Everything is the same as a regular one, except convolution becomes transposed.</span>\n        <span class=\"pl-k\">elif</span> upsampling:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Check if pooling indices is given</span>\n            <span class=\"pl-k\">if</span> pooling_indices <span class=\"pl-k\">==</span> <span class=\"pl-c1\">None</span>:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Pooling indices are not given.<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Check output_shape given or not</span>\n            <span class=\"pl-k\">if</span> output_shape <span class=\"pl-k\">==</span> <span class=\"pl-c1\">None</span>:\n                <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">ValueError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Output depth is not given<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>=======MAIN BRANCH=======</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Main branch to upsample. output shape must match with the shape of the layer that was pooled initially, in order</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>for the pooling indices to work correctly. However, the initial pooled layer was padded, so need to reduce dimension</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>before unpooling. In the paper, padding is replaced with convolution for this purpose of reducing the depth!</span>\n            net_unpool <span class=\"pl-k\">=</span> slim.conv2d(inputs, output_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_main_conv1<span class=\"pl-pds\">'</span></span>)\n            net_unpool <span class=\"pl-k\">=</span> slim.batch_norm(net_unpool, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_norm1<span class=\"pl-pds\">'</span></span>)\n            net_unpool <span class=\"pl-k\">=</span> unpool(net_unpool, pooling_indices, <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>output_shape, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>unpool<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>======SUB BRANCH=======</span>\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>First 1x1 projection to reduce depth</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(inputs, reduced_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu1<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Second conv block -----------------------------&gt; <span class=\"pl-k\">NOTE</span>: using tf.nn.conv2d_transpose for variable input shape.</span>\n            net_unpool_shape <span class=\"pl-k\">=</span> net_unpool.get_shape().as_list()\n            output_shape <span class=\"pl-k\">=</span> [net_unpool_shape[<span class=\"pl-c1\">0</span>], net_unpool_shape[<span class=\"pl-c1\">1</span>], net_unpool_shape[<span class=\"pl-c1\">2</span>], reduced_depth]\n            output_shape <span class=\"pl-k\">=</span> tf.convert_to_tensor(output_shape)\n            filter_size <span class=\"pl-k\">=</span> [filter_size, filter_size, reduced_depth, reduced_depth]\n            filters <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>filter_size, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>initializers.xavier_initializer(), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_transposed_conv2_filters<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> net = slim.conv2d_transpose(net, reduced_depth, [filter_size, filter_size], stride=2, scope=scope+'_transposed_conv2')</span>\n            net <span class=\"pl-k\">=</span> tf.nn.conv2d_transpose(net, <span class=\"pl-v\">filter</span><span class=\"pl-k\">=</span>filters, <span class=\"pl-v\">strides</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>output_shape, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_transposed_conv2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu2<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Final projection with 1x1 kernel</span>\n            net <span class=\"pl-k\">=</span> slim.conv2d(net, output_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm4<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu3<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Regularizer</span>\n            net <span class=\"pl-k\">=</span> spatial_dropout(net, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>regularizer_prob, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_spatial_dropout<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu4<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Finally, add the unpooling layer and the sub branch together</span>\n            net <span class=\"pl-k\">=</span> tf.add(net, net_unpool, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_add_upsample<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_last_prelu<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-k\">return</span> net\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>OTHERWISE, just perform a regular bottleneck!</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>==============REGULAR BOTTLENECK==================</span>\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Save the main branch for addition later</span>\n        net_main <span class=\"pl-k\">=</span> inputs\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>First projection with 1x1 kernel</span>\n        net <span class=\"pl-k\">=</span> slim.conv2d(inputs, reduced_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv1<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm1<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu1<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Second conv block</span>\n        net <span class=\"pl-k\">=</span> slim.conv2d(net, reduced_depth, [filter_size, filter_size], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv2<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm2<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu2<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Final projection with 1x1 kernel</span>\n        net <span class=\"pl-k\">=</span> slim.conv2d(net, output_depth, [<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>], <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_conv3<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> slim.batch_norm(net, <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_batch_norm3<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu3<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Regularizer</span>\n        net <span class=\"pl-k\">=</span> spatial_dropout(net, <span class=\"pl-v\">p</span><span class=\"pl-k\">=</span>regularizer_prob, <span class=\"pl-v\">seed</span><span class=\"pl-k\">=</span>seed, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_spatial_dropout<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_prelu4<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Add the main branch</span>\n        net <span class=\"pl-k\">=</span> tf.add(net_main, net, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_add_regular<span class=\"pl-pds\">'</span></span>)\n        net <span class=\"pl-k\">=</span> prelu(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>scope<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_last_prelu<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-k\">return</span> net\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span>Now actually start building the network</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">ENet</span>(<span class=\"pl-smi\">inputs</span>,\n         <span class=\"pl-smi\">num_classes</span>,\n         <span class=\"pl-smi\">batch_size</span>,\n         <span class=\"pl-smi\">num_initial_blocks</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n         <span class=\"pl-smi\">stage_two_repeat</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>,\n         <span class=\"pl-smi\">skip_connections</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n         <span class=\"pl-smi\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>,\n         <span class=\"pl-smi\">is_training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n         <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>ENet<span class=\"pl-pds\">'</span></span>):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>Set the shape of the inputs first to get the batch_size information</span>\n    inputs_shape <span class=\"pl-k\">=</span> inputs.get_shape().as_list()\n    inputs.set_shape(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(batch_size, inputs_shape[<span class=\"pl-c1\">1</span>], inputs_shape[<span class=\"pl-c1\">2</span>], inputs_shape[<span class=\"pl-c1\">3</span>]))\n\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span>reuse):\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span>Set the primary arg scopes. Fused batch_norm is faster than normal batch norm.</span>\n        <span class=\"pl-k\">with</span> slim.arg_scope([initial_block, bottleneck], <span class=\"pl-v\">is_training</span><span class=\"pl-k\">=</span>is_training),\\\n             slim.arg_scope([slim.batch_norm], <span class=\"pl-v\">fused</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>), \\\n             slim.arg_scope([slim.conv2d, slim.conv2d_transpose], <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>): \n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>=================INITIAL BLOCK=================</span>\n            net <span class=\"pl-k\">=</span> initial_block(inputs, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>initial_block_1<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">max</span>(num_initial_blocks, <span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n                net <span class=\"pl-k\">=</span> initial_block(net, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>initial_block_<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i))\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Save for skip connection later</span>\n            <span class=\"pl-k\">if</span> skip_connections:\n                net_one <span class=\"pl-k\">=</span> net\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>===================STAGE ONE=======================</span>\n            net, pooling_indices_1, inputs_shape_1 <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">downsampling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck1_0<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck1_1<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck1_2<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck1_3<span class=\"pl-pds\">'</span></span>)\n            net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.01</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck1_4<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>Save for skip connection later</span>\n            <span class=\"pl-k\">if</span> skip_connections:\n                net_two <span class=\"pl-k\">=</span> net\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>regularization prob is 0.1 from bottleneck 2.0 onwards</span>\n            <span class=\"pl-k\">with</span> slim.arg_scope([bottleneck], <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>):\n                net, pooling_indices_2, inputs_shape_2 <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">downsampling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck2_0<span class=\"pl-pds\">'</span></span>)\n                \n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>Repeat the stage two at least twice to get stage 2 and 3:</span>\n                <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">max</span>(stage_two_repeat, <span class=\"pl-c1\">2</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>):\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_1<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dilation_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_2<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">asymmetric</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_3<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dilation_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_4<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_5<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dilation_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">8</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_6<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">asymmetric</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_7<span class=\"pl-pds\">'</span></span>)\n                    net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">128</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">dilated</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">dilation_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">16</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bottleneck<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(i)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_8<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-k\">with</span> slim.arg_scope([bottleneck], <span class=\"pl-v\">regularizer_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">decoder</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>===================STAGE FOUR========================</span>\n                bottleneck_scope_name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bottleneck<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>The decoder section, so start to upsample.</span>\n                net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">upsampling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                 <span class=\"pl-v\">pooling_indices</span><span class=\"pl-k\">=</span>pooling_indices_2, <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>inputs_shape_2, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_0<span class=\"pl-pds\">'</span></span>)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>Perform skip connections here</span>\n                <span class=\"pl-k\">if</span> skip_connections:\n                    net <span class=\"pl-k\">=</span> tf.add(net, net_two, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_skip_connection<span class=\"pl-pds\">'</span></span>)\n\n                net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_1<span class=\"pl-pds\">'</span></span>)\n                net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_2<span class=\"pl-pds\">'</span></span>)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>===================STAGE FIVE========================</span>\n                bottleneck_scope_name <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>bottleneck<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">str</span>(i <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>)\n\n                net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">16</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">upsampling</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                 <span class=\"pl-v\">pooling_indices</span><span class=\"pl-k\">=</span>pooling_indices_1, <span class=\"pl-v\">output_shape</span><span class=\"pl-k\">=</span>inputs_shape_1, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_0<span class=\"pl-pds\">'</span></span>)\n\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span>perform skip connections here</span>\n                <span class=\"pl-k\">if</span> skip_connections:\n                    net <span class=\"pl-k\">=</span> tf.add(net, net_one, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_skip_connection<span class=\"pl-pds\">'</span></span>)\n\n                net <span class=\"pl-k\">=</span> bottleneck(net, <span class=\"pl-v\">output_depth</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">16</span>, <span class=\"pl-v\">filter_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span>bottleneck_scope_name<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_1<span class=\"pl-pds\">'</span></span>)\n\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span>=============FINAL CONVOLUTION=============</span>\n            logits <span class=\"pl-k\">=</span> slim.conv2d_transpose(net, num_classes, [<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">2</span>], <span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>fullconv<span class=\"pl-pds\">'</span></span>)\n            probabilities <span class=\"pl-k\">=</span> tf.nn.softmax(logits, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>logits_to_softmax<span class=\"pl-pds\">'</span></span>)\n\n        <span class=\"pl-k\">return</span> logits, probabilities\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">ENet_arg_scope</span>(<span class=\"pl-smi\">weight_decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2e-4</span>,\n                   <span class=\"pl-smi\">batch_norm_decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.1</span>,\n                   <span class=\"pl-smi\">batch_norm_epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.001</span>):\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set weight_decay for weights in conv2d and separable_conv2d layers.</span>\n  <span class=\"pl-k\">with</span> slim.arg_scope([slim.conv2d],\n                      <span class=\"pl-v\">weights_regularizer</span><span class=\"pl-k\">=</span>slim.l2_regularizer(weight_decay),\n                      <span class=\"pl-v\">biases_regularizer</span><span class=\"pl-k\">=</span>slim.l2_regularizer(weight_decay)):\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Set parameters for batch_norm.</span>\n    <span class=\"pl-k\">with</span> slim.arg_scope([slim.batch_norm],\n                        <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span>batch_norm_decay,\n                        <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span>batch_norm_epsilon) <span class=\"pl-k\">as</span> scope:\n      <span class=\"pl-k\">return</span> scope</pre></div>\n</div></details><br>", "body_text": "Please make sure that this is a feature request. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template\nSystem information\n\nTensorFlow version (you are using): Tensorflow Lite v1.11.0 (Self-Build)\nHardware: RaspberryPi3\nOS: Raspbian Stretch\nAre you willing to contribute it (Yes/No): No (Because, There are few Custom Operation tutorials, and I can not write C++ programs.)\n\nDescribe the feature and the current behavior/state.\nIf I implement \"semantic segmentation\" model \"ENet\", I can not do it unless you support various behaviors of the unpooling layer.\n\n\nLayer that I would like to support with Tensorflow Lite\n\nFloorMod\nRange\nRank\nAbs\nMaxPoolWithArgmax\nScatterNd\nSparseTensor\nsparse_add\ngather_nd\n\n\n\nSample message of Unsupport Error\n\n\nTraceback (most recent call last):\n  File \"main.py\", line 5, in <module>\n    interpreter = tf.contrib.lite.Interpreter(model_path=\"semanticsegmentation_enet_non_quantized.tflite\")\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 53, in __init__\n    model_path))\nValueError: Didn't find custom op for name 'FloorMod' with version 1\nDidn't find custom op for name 'Range' with version 1\nDidn't find custom op for name 'Rank' with version 1\nDidn't find custom op for name 'Abs' with version 1\nDidn't find custom op for name 'MaxPoolWithArgmax' with version 1\nDidn't find custom op for name 'ScatterNd' with version 1\nRegistration failed.\nWill this change the current api? How?\nYes.\nCustom Operation must be officially implemented.\nWho will benefit with this feature?\n\nEngineers who study automatic driving technology in general\nEngineer who studies fast inference using lightweight mobile\nEngineers to study lightweight models\n\nAny Other info.\nMy repository and sample program are below.\nFor Python2.x / Python3.x.\nhttps://github.com/PINTO0309/TensorFlow-ENet.git\nhttps://github.com/PINTO0309/TensorflowLite-UNet.git\n\u3010Reference\u3011 Model Logic\nimport tensorflow as tf\nfrom tensorflow.contrib.layers.python.layers import initializers\nslim = tf.contrib.slim\n\n@slim.add_arg_scope\ndef prelu(x, scope, decoder=False):\n\n    #If decoder, then perform relu and just return the output\n    if decoder:\n        return tf.nn.relu(x, name=scope)\n\n    alpha= tf.get_variable(scope + 'alpha', x.get_shape()[-1],\n                       initializer=tf.constant_initializer(0.0),\n                        dtype=tf.float32)\n    pos = tf.nn.relu(x)\n    neg = alpha * (x - abs(x)) * 0.5\n    return pos + neg\n\ndef spatial_dropout(x, p, seed, scope, is_training=True):\n\n    if is_training:\n        keep_prob = 1.0 - p\n        input_shape = x.get_shape().as_list()\n        noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])\n        output = tf.nn.dropout(x, keep_prob, noise_shape, seed=seed, name=scope)\n\n        return output\n\n    return x\n\ndef unpool(updates, mask, k_size=[1, 2, 2, 1], output_shape=None, scope=''):\n\n    with tf.variable_scope(scope):\n        mask = tf.cast(mask, tf.int32)\n        input_shape = tf.shape(updates, out_type=tf.int32)\n        #  calculation new shape\n        if output_shape is None:\n            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\n\n        # calculation indices for batch, height, width and feature maps\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\n        b = one_like_mask * batch_range\n        y = mask // (output_shape[2] * output_shape[3])\n        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\n        f = one_like_mask * feature_range\n\n        # transpose indices & reshape update values to one dimension\n        updates_size = tf.size(updates)\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\n        values = tf.reshape(updates, [updates_size])\n        ret = tf.scatter_nd(indices, values, output_shape)\n        return ret\n\n@slim.add_arg_scope\ndef initial_block(inputs, is_training=True, scope='initial_block'):\n\n    #Convolutional branch\n    net_conv = slim.conv2d(inputs, 13, [3,3], stride=2, activation_fn=None, scope=scope+'_conv')\n    net_conv = slim.batch_norm(net_conv, is_training=is_training, fused=True, scope=scope+'_batchnorm')\n    net_conv = prelu(net_conv, scope=scope+'_prelu')\n\n    #Max pool branch\n    net_pool = slim.max_pool2d(inputs, [2,2], stride=2, scope=scope+'_max_pool')\n\n    #Concatenated output - does it matter max pool comes first or conv comes first? probably not.\n    net_concatenated = tf.concat([net_conv, net_pool], axis=3, name=scope+'_concat')\n    return net_concatenated\n\n@slim.add_arg_scope\ndef bottleneck(inputs,\n               output_depth,\n               filter_size,\n               regularizer_prob,\n               projection_ratio=4,\n               seed=0,\n               is_training=True,\n               downsampling=False,\n               upsampling=False,\n               pooling_indices=None,\n               output_shape=None,\n               dilated=False,\n               dilation_rate=None,\n               asymmetric=False,\n               decoder=False,\n               scope='bottleneck'):\n\n    #Calculate the depth reduction based on the projection ratio used in 1x1 convolution.\n    reduced_depth = int(inputs.get_shape().as_list()[3] / projection_ratio)\n\n    with slim.arg_scope([prelu], decoder=decoder):\n\n        #=============DOWNSAMPLING BOTTLENECK====================\n        if downsampling:\n            #=============MAIN BRANCH=============\n            #Just perform a max pooling\n            net_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\n                                                                   ksize=[1,2,2,1],\n                                                                   strides=[1,2,2,1],\n                                                                   padding='SAME',\n                                                                   name=scope+'_main_max_pool')\n\n            #First get the difference in depth to pad, then pad with zeros only on the last dimension.\n            inputs_shape = inputs.get_shape().as_list()\n            depth_to_pad = abs(inputs_shape[3] - output_depth)\n            paddings = tf.convert_to_tensor([[0,0], [0,0], [0,0], [0, depth_to_pad]])\n            net_main = tf.pad(net_main, paddings=paddings, name=scope+'_main_padding')\n\n            #=============SUB BRANCH==============\n            #First projection that has a 2x2 kernel and stride 2\n            net = slim.conv2d(inputs, reduced_depth, [2,2], stride=2, scope=scope+'_conv1')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\n            net = prelu(net, scope=scope+'_prelu1')\n\n            #Second conv block\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\n            net = prelu(net, scope=scope+'_prelu2')\n\n            #Final projection with 1x1 kernel\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\n            net = prelu(net, scope=scope+'_prelu3')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\n\n            #Finally, combine the two branches together via an element-wise addition\n            net = tf.add(net, net_main, name=scope+'_add')\n            net = prelu(net, scope=scope+'_last_prelu')\n\n            #also return inputs shape for convenience later\n            return net, pooling_indices, inputs_shape\n\n        #============DILATION CONVOLUTION BOTTLENECK====================\n        #Everything is the same as a regular bottleneck except for the dilation rate argument\n        elif dilated:\n            #Check if dilation rate is given\n            if not dilation_rate:\n                raise ValueError('Dilation rate is not given.')\n\n            #Save the main branch for addition later\n            net_main = inputs\n\n            #First projection with 1x1 kernel (dimensionality reduction)\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\n            net = prelu(net, scope=scope+'_prelu1')\n\n            #Second conv block --- apply dilated convolution here\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], rate=dilation_rate, scope=scope+'_dilated_conv2')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\n            net = prelu(net, scope=scope+'_prelu2')\n\n            #Final projection with 1x1 kernel (Expansion)\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\n            net = prelu(net, scope=scope+'_prelu3')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\n            net = prelu(net, scope=scope+'_prelu4')\n\n            #Add the main branch\n            net = tf.add(net_main, net, name=scope+'_add_dilated')\n            net = prelu(net, scope=scope+'_last_prelu')\n\n            return net\n\n        #===========ASYMMETRIC CONVOLUTION BOTTLENECK==============\n        #Everything is the same as a regular bottleneck except for a [5,5] kernel decomposed into two [5,1] then [1,5]\n        elif asymmetric:\n            #Save the main branch for addition later\n            net_main = inputs\n\n            #First projection with 1x1 kernel (dimensionality reduction)\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\n            net = prelu(net, scope=scope+'_prelu1')\n\n            #Second conv block --- apply asymmetric conv here\n            net = slim.conv2d(net, reduced_depth, [filter_size, 1], scope=scope+'_asymmetric_conv2a')\n            net = slim.conv2d(net, reduced_depth, [1, filter_size], scope=scope+'_asymmetric_conv2b')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\n            net = prelu(net, scope=scope+'_prelu2')\n\n            #Final projection with 1x1 kernel\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\n            net = prelu(net, scope=scope+'_prelu3')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\n            net = prelu(net, scope=scope+'_prelu4')\n\n            #Add the main branch\n            net = tf.add(net_main, net, name=scope+'_add_asymmetric')\n            net = prelu(net, scope=scope+'_last_prelu')\n\n            return net\n\n        #============UPSAMPLING BOTTLENECK================\n        #Everything is the same as a regular one, except convolution becomes transposed.\n        elif upsampling:\n            #Check if pooling indices is given\n            if pooling_indices == None:\n                raise ValueError('Pooling indices are not given.')\n\n            #Check output_shape given or not\n            if output_shape == None:\n                raise ValueError('Output depth is not given')\n\n            #=======MAIN BRANCH=======\n            #Main branch to upsample. output shape must match with the shape of the layer that was pooled initially, in order\n            #for the pooling indices to work correctly. However, the initial pooled layer was padded, so need to reduce dimension\n            #before unpooling. In the paper, padding is replaced with convolution for this purpose of reducing the depth!\n            net_unpool = slim.conv2d(inputs, output_depth, [1,1], scope=scope+'_main_conv1')\n            net_unpool = slim.batch_norm(net_unpool, is_training=is_training, scope=scope+'batch_norm1')\n            net_unpool = unpool(net_unpool, pooling_indices, output_shape=output_shape, scope='unpool')\n\n            #======SUB BRANCH=======\n            #First 1x1 projection to reduce depth\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\n            net = prelu(net, scope=scope+'_prelu1')\n\n            #Second conv block -----------------------------> NOTE: using tf.nn.conv2d_transpose for variable input shape.\n            net_unpool_shape = net_unpool.get_shape().as_list()\n            output_shape = [net_unpool_shape[0], net_unpool_shape[1], net_unpool_shape[2], reduced_depth]\n            output_shape = tf.convert_to_tensor(output_shape)\n            filter_size = [filter_size, filter_size, reduced_depth, reduced_depth]\n            filters = tf.get_variable(shape=filter_size, initializer=initializers.xavier_initializer(), dtype=tf.float32, name=scope+'_transposed_conv2_filters')\n\n            # net = slim.conv2d_transpose(net, reduced_depth, [filter_size, filter_size], stride=2, scope=scope+'_transposed_conv2')\n            net = tf.nn.conv2d_transpose(net, filter=filters, strides=[1,2,2,1], output_shape=output_shape, name=scope+'_transposed_conv2')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\n            net = prelu(net, scope=scope+'_prelu2')\n\n            #Final projection with 1x1 kernel\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm4')\n            net = prelu(net, scope=scope+'_prelu3')\n\n            #Regularizer\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\n            net = prelu(net, scope=scope+'_prelu4')\n\n            #Finally, add the unpooling layer and the sub branch together\n            net = tf.add(net, net_unpool, name=scope+'_add_upsample')\n            net = prelu(net, scope=scope+'_last_prelu')\n\n            return net\n\n        #OTHERWISE, just perform a regular bottleneck!\n        #==============REGULAR BOTTLENECK==================\n        #Save the main branch for addition later\n        net_main = inputs\n\n        #First projection with 1x1 kernel\n        net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\n        net = prelu(net, scope=scope+'_prelu1')\n\n        #Second conv block\n        net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\n        net = prelu(net, scope=scope+'_prelu2')\n\n        #Final projection with 1x1 kernel\n        net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\n        net = prelu(net, scope=scope+'_prelu3')\n\n        #Regularizer\n        net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\n        net = prelu(net, scope=scope+'_prelu4')\n\n        #Add the main branch\n        net = tf.add(net_main, net, name=scope+'_add_regular')\n        net = prelu(net, scope=scope+'_last_prelu')\n\n        return net\n\n#Now actually start building the network\ndef ENet(inputs,\n         num_classes,\n         batch_size,\n         num_initial_blocks=1,\n         stage_two_repeat=2,\n         skip_connections=True,\n         reuse=None,\n         is_training=True,\n         scope='ENet'):\n\n    #Set the shape of the inputs first to get the batch_size information\n    inputs_shape = inputs.get_shape().as_list()\n    inputs.set_shape(shape=(batch_size, inputs_shape[1], inputs_shape[2], inputs_shape[3]))\n\n    with tf.variable_scope(scope, reuse=reuse):\n        #Set the primary arg scopes. Fused batch_norm is faster than normal batch norm.\n        with slim.arg_scope([initial_block, bottleneck], is_training=is_training),\\\n             slim.arg_scope([slim.batch_norm], fused=True), \\\n             slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=None): \n            #=================INITIAL BLOCK=================\n            net = initial_block(inputs, scope='initial_block_1')\n            for i in xrange(2, max(num_initial_blocks, 1) + 1):\n                net = initial_block(net, scope='initial_block_' + str(i))\n\n            #Save for skip connection later\n            if skip_connections:\n                net_one = net\n\n            #===================STAGE ONE=======================\n            net, pooling_indices_1, inputs_shape_1 = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, downsampling=True, scope='bottleneck1_0')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_1')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_2')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_3')\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_4')\n\n            #Save for skip connection later\n            if skip_connections:\n                net_two = net\n\n            #regularization prob is 0.1 from bottleneck 2.0 onwards\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1):\n                net, pooling_indices_2, inputs_shape_2 = bottleneck(net, output_depth=128, filter_size=3, downsampling=True, scope='bottleneck2_0')\n                \n                #Repeat the stage two at least twice to get stage 2 and 3:\n                for i in xrange(2, max(stage_two_repeat, 2) + 2):\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_1')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=2, scope='bottleneck'+str(i)+'_2')\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_3')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=4, scope='bottleneck'+str(i)+'_4')\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_5')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=8, scope='bottleneck'+str(i)+'_6')\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_7')\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=16, scope='bottleneck'+str(i)+'_8')\n\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1, decoder=True):\n                #===================STAGE FOUR========================\n                bottleneck_scope_name = \"bottleneck\" + str(i + 1)\n\n                #The decoder section, so start to upsample.\n                net = bottleneck(net, output_depth=64, filter_size=3, upsampling=True,\n                                 pooling_indices=pooling_indices_2, output_shape=inputs_shape_2, scope=bottleneck_scope_name+'_0')\n\n                #Perform skip connections here\n                if skip_connections:\n                    net = tf.add(net, net_two, name=bottleneck_scope_name+'_skip_connection')\n\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_1')\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_2')\n\n                #===================STAGE FIVE========================\n                bottleneck_scope_name = \"bottleneck\" + str(i + 2)\n\n                net = bottleneck(net, output_depth=16, filter_size=3, upsampling=True,\n                                 pooling_indices=pooling_indices_1, output_shape=inputs_shape_1, scope=bottleneck_scope_name+'_0')\n\n                #perform skip connections here\n                if skip_connections:\n                    net = tf.add(net, net_one, name=bottleneck_scope_name+'_skip_connection')\n\n                net = bottleneck(net, output_depth=16, filter_size=3, scope=bottleneck_scope_name+'_1')\n\n            #=============FINAL CONVOLUTION=============\n            logits = slim.conv2d_transpose(net, num_classes, [2,2], stride=2, scope='fullconv')\n            probabilities = tf.nn.softmax(logits, name='logits_to_softmax')\n\n        return logits, probabilities\n\n\ndef ENet_arg_scope(weight_decay=2e-4,\n                   batch_norm_decay=0.1,\n                   batch_norm_epsilon=0.001):\n\n  # Set weight_decay for weights in conv2d and separable_conv2d layers.\n  with slim.arg_scope([slim.conv2d],\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\n\n    # Set parameters for batch_norm.\n    with slim.arg_scope([slim.batch_norm],\n                        decay=batch_norm_decay,\n                        epsilon=batch_norm_epsilon) as scope:\n      return scope", "body": "<em>Please make sure that this is a feature request. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:feature_template</em>\r\n\r\n**System information**\r\n- TensorFlow version (you are using): **[Tensorflow Lite v1.11.0 (Self-Build)](https://github.com/PINTO0309/Tensorflow-bin.git)**\r\n- Hardware: RaspberryPi3\r\n- OS: Raspbian Stretch\r\n- Are you willing to contribute it (Yes/No): No (Because, There are few Custom Operation tutorials, and I can not write C++ programs.)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nIf I implement \"semantic segmentation\" model \"ENet\", I can not do it unless you support various behaviors of the unpooling layer.\r\n\r\n- **Layer that I would like to support with Tensorflow Lite**\r\n    - FloorMod\r\n    - Range\r\n    - Rank\r\n    - Abs\r\n    - MaxPoolWithArgmax\r\n    - ScatterNd\r\n    - SparseTensor\r\n    - sparse_add\r\n    - gather_nd\r\n\r\n- **Sample message of Unsupport Error**\r\n```Python\r\nTraceback (most recent call last):\r\n  File \"main.py\", line 5, in <module>\r\n    interpreter = tf.contrib.lite.Interpreter(model_path=\"semanticsegmentation_enet_non_quantized.tflite\")\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/lite/python/interpreter.py\", line 53, in __init__\r\n    model_path))\r\nValueError: Didn't find custom op for name 'FloorMod' with version 1\r\nDidn't find custom op for name 'Range' with version 1\r\nDidn't find custom op for name 'Rank' with version 1\r\nDidn't find custom op for name 'Abs' with version 1\r\nDidn't find custom op for name 'MaxPoolWithArgmax' with version 1\r\nDidn't find custom op for name 'ScatterNd' with version 1\r\nRegistration failed.\r\n```\r\n\r\n**Will this change the current api? How?**\r\nYes.\r\nCustom Operation must be officially implemented.\r\n\r\n**Who will benefit with this feature?**\r\n- Engineers who study automatic driving technology in general\r\n- Engineer who studies fast inference using lightweight mobile\r\n- Engineers to study lightweight models\r\n\r\n**Any Other info.**\r\nMy repository and sample program are below.  \r\nFor Python2.x / Python3.x.\r\n  \r\n**https://github.com/PINTO0309/TensorFlow-ENet.git**  \r\n**https://github.com/PINTO0309/TensorflowLite-UNet.git**  \r\n  \r\n<details><summary>\u3010Reference\u3011 Model Logic</summary><div>\r\n\r\n```Python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.layers.python.layers import initializers\r\nslim = tf.contrib.slim\r\n\r\n@slim.add_arg_scope\r\ndef prelu(x, scope, decoder=False):\r\n\r\n    #If decoder, then perform relu and just return the output\r\n    if decoder:\r\n        return tf.nn.relu(x, name=scope)\r\n\r\n    alpha= tf.get_variable(scope + 'alpha', x.get_shape()[-1],\r\n                       initializer=tf.constant_initializer(0.0),\r\n                        dtype=tf.float32)\r\n    pos = tf.nn.relu(x)\r\n    neg = alpha * (x - abs(x)) * 0.5\r\n    return pos + neg\r\n\r\ndef spatial_dropout(x, p, seed, scope, is_training=True):\r\n\r\n    if is_training:\r\n        keep_prob = 1.0 - p\r\n        input_shape = x.get_shape().as_list()\r\n        noise_shape = tf.constant(value=[input_shape[0], 1, 1, input_shape[3]])\r\n        output = tf.nn.dropout(x, keep_prob, noise_shape, seed=seed, name=scope)\r\n\r\n        return output\r\n\r\n    return x\r\n\r\ndef unpool(updates, mask, k_size=[1, 2, 2, 1], output_shape=None, scope=''):\r\n\r\n    with tf.variable_scope(scope):\r\n        mask = tf.cast(mask, tf.int32)\r\n        input_shape = tf.shape(updates, out_type=tf.int32)\r\n        #  calculation new shape\r\n        if output_shape is None:\r\n            output_shape = (input_shape[0], input_shape[1] * ksize[1], input_shape[2] * ksize[2], input_shape[3])\r\n\r\n        # calculation indices for batch, height, width and feature maps\r\n        one_like_mask = tf.ones_like(mask, dtype=tf.int32)\r\n        batch_shape = tf.concat([[input_shape[0]], [1], [1], [1]], 0)\r\n        batch_range = tf.reshape(tf.range(output_shape[0], dtype=tf.int32), shape=batch_shape)\r\n        b = one_like_mask * batch_range\r\n        y = mask // (output_shape[2] * output_shape[3])\r\n        x = (mask // output_shape[3]) % output_shape[2] #mask % (output_shape[2] * output_shape[3]) // output_shape[3]\r\n        feature_range = tf.range(output_shape[3], dtype=tf.int32)\r\n        f = one_like_mask * feature_range\r\n\r\n        # transpose indices & reshape update values to one dimension\r\n        updates_size = tf.size(updates)\r\n        indices = tf.transpose(tf.reshape(tf.stack([b, y, x, f]), [4, updates_size]))\r\n        values = tf.reshape(updates, [updates_size])\r\n        ret = tf.scatter_nd(indices, values, output_shape)\r\n        return ret\r\n\r\n@slim.add_arg_scope\r\ndef initial_block(inputs, is_training=True, scope='initial_block'):\r\n\r\n    #Convolutional branch\r\n    net_conv = slim.conv2d(inputs, 13, [3,3], stride=2, activation_fn=None, scope=scope+'_conv')\r\n    net_conv = slim.batch_norm(net_conv, is_training=is_training, fused=True, scope=scope+'_batchnorm')\r\n    net_conv = prelu(net_conv, scope=scope+'_prelu')\r\n\r\n    #Max pool branch\r\n    net_pool = slim.max_pool2d(inputs, [2,2], stride=2, scope=scope+'_max_pool')\r\n\r\n    #Concatenated output - does it matter max pool comes first or conv comes first? probably not.\r\n    net_concatenated = tf.concat([net_conv, net_pool], axis=3, name=scope+'_concat')\r\n    return net_concatenated\r\n\r\n@slim.add_arg_scope\r\ndef bottleneck(inputs,\r\n               output_depth,\r\n               filter_size,\r\n               regularizer_prob,\r\n               projection_ratio=4,\r\n               seed=0,\r\n               is_training=True,\r\n               downsampling=False,\r\n               upsampling=False,\r\n               pooling_indices=None,\r\n               output_shape=None,\r\n               dilated=False,\r\n               dilation_rate=None,\r\n               asymmetric=False,\r\n               decoder=False,\r\n               scope='bottleneck'):\r\n\r\n    #Calculate the depth reduction based on the projection ratio used in 1x1 convolution.\r\n    reduced_depth = int(inputs.get_shape().as_list()[3] / projection_ratio)\r\n\r\n    with slim.arg_scope([prelu], decoder=decoder):\r\n\r\n        #=============DOWNSAMPLING BOTTLENECK====================\r\n        if downsampling:\r\n            #=============MAIN BRANCH=============\r\n            #Just perform a max pooling\r\n            net_main, pooling_indices = tf.nn.max_pool_with_argmax(inputs,\r\n                                                                   ksize=[1,2,2,1],\r\n                                                                   strides=[1,2,2,1],\r\n                                                                   padding='SAME',\r\n                                                                   name=scope+'_main_max_pool')\r\n\r\n            #First get the difference in depth to pad, then pad with zeros only on the last dimension.\r\n            inputs_shape = inputs.get_shape().as_list()\r\n            depth_to_pad = abs(inputs_shape[3] - output_depth)\r\n            paddings = tf.convert_to_tensor([[0,0], [0,0], [0,0], [0, depth_to_pad]])\r\n            net_main = tf.pad(net_main, paddings=paddings, name=scope+'_main_padding')\r\n\r\n            #=============SUB BRANCH==============\r\n            #First projection that has a 2x2 kernel and stride 2\r\n            net = slim.conv2d(inputs, reduced_depth, [2,2], stride=2, scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block\r\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n\r\n            #Finally, combine the two branches together via an element-wise addition\r\n            net = tf.add(net, net_main, name=scope+'_add')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            #also return inputs shape for convenience later\r\n            return net, pooling_indices, inputs_shape\r\n\r\n        #============DILATION CONVOLUTION BOTTLENECK====================\r\n        #Everything is the same as a regular bottleneck except for the dilation rate argument\r\n        elif dilated:\r\n            #Check if dilation rate is given\r\n            if not dilation_rate:\r\n                raise ValueError('Dilation rate is not given.')\r\n\r\n            #Save the main branch for addition later\r\n            net_main = inputs\r\n\r\n            #First projection with 1x1 kernel (dimensionality reduction)\r\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block --- apply dilated convolution here\r\n            net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], rate=dilation_rate, scope=scope+'_dilated_conv2')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel (Expansion)\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n            net = prelu(net, scope=scope+'_prelu4')\r\n\r\n            #Add the main branch\r\n            net = tf.add(net_main, net, name=scope+'_add_dilated')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            return net\r\n\r\n        #===========ASYMMETRIC CONVOLUTION BOTTLENECK==============\r\n        #Everything is the same as a regular bottleneck except for a [5,5] kernel decomposed into two [5,1] then [1,5]\r\n        elif asymmetric:\r\n            #Save the main branch for addition later\r\n            net_main = inputs\r\n\r\n            #First projection with 1x1 kernel (dimensionality reduction)\r\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block --- apply asymmetric conv here\r\n            net = slim.conv2d(net, reduced_depth, [filter_size, 1], scope=scope+'_asymmetric_conv2a')\r\n            net = slim.conv2d(net, reduced_depth, [1, filter_size], scope=scope+'_asymmetric_conv2b')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n            net = prelu(net, scope=scope+'_prelu4')\r\n\r\n            #Add the main branch\r\n            net = tf.add(net_main, net, name=scope+'_add_asymmetric')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            return net\r\n\r\n        #============UPSAMPLING BOTTLENECK================\r\n        #Everything is the same as a regular one, except convolution becomes transposed.\r\n        elif upsampling:\r\n            #Check if pooling indices is given\r\n            if pooling_indices == None:\r\n                raise ValueError('Pooling indices are not given.')\r\n\r\n            #Check output_shape given or not\r\n            if output_shape == None:\r\n                raise ValueError('Output depth is not given')\r\n\r\n            #=======MAIN BRANCH=======\r\n            #Main branch to upsample. output shape must match with the shape of the layer that was pooled initially, in order\r\n            #for the pooling indices to work correctly. However, the initial pooled layer was padded, so need to reduce dimension\r\n            #before unpooling. In the paper, padding is replaced with convolution for this purpose of reducing the depth!\r\n            net_unpool = slim.conv2d(inputs, output_depth, [1,1], scope=scope+'_main_conv1')\r\n            net_unpool = slim.batch_norm(net_unpool, is_training=is_training, scope=scope+'batch_norm1')\r\n            net_unpool = unpool(net_unpool, pooling_indices, output_shape=output_shape, scope='unpool')\r\n\r\n            #======SUB BRANCH=======\r\n            #First 1x1 projection to reduce depth\r\n            net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n            net = prelu(net, scope=scope+'_prelu1')\r\n\r\n            #Second conv block -----------------------------> NOTE: using tf.nn.conv2d_transpose for variable input shape.\r\n            net_unpool_shape = net_unpool.get_shape().as_list()\r\n            output_shape = [net_unpool_shape[0], net_unpool_shape[1], net_unpool_shape[2], reduced_depth]\r\n            output_shape = tf.convert_to_tensor(output_shape)\r\n            filter_size = [filter_size, filter_size, reduced_depth, reduced_depth]\r\n            filters = tf.get_variable(shape=filter_size, initializer=initializers.xavier_initializer(), dtype=tf.float32, name=scope+'_transposed_conv2_filters')\r\n\r\n            # net = slim.conv2d_transpose(net, reduced_depth, [filter_size, filter_size], stride=2, scope=scope+'_transposed_conv2')\r\n            net = tf.nn.conv2d_transpose(net, filter=filters, strides=[1,2,2,1], output_shape=output_shape, name=scope+'_transposed_conv2')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n            net = prelu(net, scope=scope+'_prelu2')\r\n\r\n            #Final projection with 1x1 kernel\r\n            net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n            net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm4')\r\n            net = prelu(net, scope=scope+'_prelu3')\r\n\r\n            #Regularizer\r\n            net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n            net = prelu(net, scope=scope+'_prelu4')\r\n\r\n            #Finally, add the unpooling layer and the sub branch together\r\n            net = tf.add(net, net_unpool, name=scope+'_add_upsample')\r\n            net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n            return net\r\n\r\n        #OTHERWISE, just perform a regular bottleneck!\r\n        #==============REGULAR BOTTLENECK==================\r\n        #Save the main branch for addition later\r\n        net_main = inputs\r\n\r\n        #First projection with 1x1 kernel\r\n        net = slim.conv2d(inputs, reduced_depth, [1,1], scope=scope+'_conv1')\r\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm1')\r\n        net = prelu(net, scope=scope+'_prelu1')\r\n\r\n        #Second conv block\r\n        net = slim.conv2d(net, reduced_depth, [filter_size, filter_size], scope=scope+'_conv2')\r\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm2')\r\n        net = prelu(net, scope=scope+'_prelu2')\r\n\r\n        #Final projection with 1x1 kernel\r\n        net = slim.conv2d(net, output_depth, [1,1], scope=scope+'_conv3')\r\n        net = slim.batch_norm(net, is_training=is_training, scope=scope+'_batch_norm3')\r\n        net = prelu(net, scope=scope+'_prelu3')\r\n\r\n        #Regularizer\r\n        net = spatial_dropout(net, p=regularizer_prob, seed=seed, scope=scope+'_spatial_dropout')\r\n        net = prelu(net, scope=scope+'_prelu4')\r\n\r\n        #Add the main branch\r\n        net = tf.add(net_main, net, name=scope+'_add_regular')\r\n        net = prelu(net, scope=scope+'_last_prelu')\r\n\r\n        return net\r\n\r\n#Now actually start building the network\r\ndef ENet(inputs,\r\n         num_classes,\r\n         batch_size,\r\n         num_initial_blocks=1,\r\n         stage_two_repeat=2,\r\n         skip_connections=True,\r\n         reuse=None,\r\n         is_training=True,\r\n         scope='ENet'):\r\n\r\n    #Set the shape of the inputs first to get the batch_size information\r\n    inputs_shape = inputs.get_shape().as_list()\r\n    inputs.set_shape(shape=(batch_size, inputs_shape[1], inputs_shape[2], inputs_shape[3]))\r\n\r\n    with tf.variable_scope(scope, reuse=reuse):\r\n        #Set the primary arg scopes. Fused batch_norm is faster than normal batch norm.\r\n        with slim.arg_scope([initial_block, bottleneck], is_training=is_training),\\\r\n             slim.arg_scope([slim.batch_norm], fused=True), \\\r\n             slim.arg_scope([slim.conv2d, slim.conv2d_transpose], activation_fn=None): \r\n            #=================INITIAL BLOCK=================\r\n            net = initial_block(inputs, scope='initial_block_1')\r\n            for i in xrange(2, max(num_initial_blocks, 1) + 1):\r\n                net = initial_block(net, scope='initial_block_' + str(i))\r\n\r\n            #Save for skip connection later\r\n            if skip_connections:\r\n                net_one = net\r\n\r\n            #===================STAGE ONE=======================\r\n            net, pooling_indices_1, inputs_shape_1 = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, downsampling=True, scope='bottleneck1_0')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_1')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_2')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_3')\r\n            net = bottleneck(net, output_depth=64, filter_size=3, regularizer_prob=0.01, scope='bottleneck1_4')\r\n\r\n            #Save for skip connection later\r\n            if skip_connections:\r\n                net_two = net\r\n\r\n            #regularization prob is 0.1 from bottleneck 2.0 onwards\r\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1):\r\n                net, pooling_indices_2, inputs_shape_2 = bottleneck(net, output_depth=128, filter_size=3, downsampling=True, scope='bottleneck2_0')\r\n                \r\n                #Repeat the stage two at least twice to get stage 2 and 3:\r\n                for i in xrange(2, max(stage_two_repeat, 2) + 2):\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_1')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=2, scope='bottleneck'+str(i)+'_2')\r\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_3')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=4, scope='bottleneck'+str(i)+'_4')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, scope='bottleneck'+str(i)+'_5')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=8, scope='bottleneck'+str(i)+'_6')\r\n                    net = bottleneck(net, output_depth=128, filter_size=5, asymmetric=True, scope='bottleneck'+str(i)+'_7')\r\n                    net = bottleneck(net, output_depth=128, filter_size=3, dilated=True, dilation_rate=16, scope='bottleneck'+str(i)+'_8')\r\n\r\n            with slim.arg_scope([bottleneck], regularizer_prob=0.1, decoder=True):\r\n                #===================STAGE FOUR========================\r\n                bottleneck_scope_name = \"bottleneck\" + str(i + 1)\r\n\r\n                #The decoder section, so start to upsample.\r\n                net = bottleneck(net, output_depth=64, filter_size=3, upsampling=True,\r\n                                 pooling_indices=pooling_indices_2, output_shape=inputs_shape_2, scope=bottleneck_scope_name+'_0')\r\n\r\n                #Perform skip connections here\r\n                if skip_connections:\r\n                    net = tf.add(net, net_two, name=bottleneck_scope_name+'_skip_connection')\r\n\r\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_1')\r\n                net = bottleneck(net, output_depth=64, filter_size=3, scope=bottleneck_scope_name+'_2')\r\n\r\n                #===================STAGE FIVE========================\r\n                bottleneck_scope_name = \"bottleneck\" + str(i + 2)\r\n\r\n                net = bottleneck(net, output_depth=16, filter_size=3, upsampling=True,\r\n                                 pooling_indices=pooling_indices_1, output_shape=inputs_shape_1, scope=bottleneck_scope_name+'_0')\r\n\r\n                #perform skip connections here\r\n                if skip_connections:\r\n                    net = tf.add(net, net_one, name=bottleneck_scope_name+'_skip_connection')\r\n\r\n                net = bottleneck(net, output_depth=16, filter_size=3, scope=bottleneck_scope_name+'_1')\r\n\r\n            #=============FINAL CONVOLUTION=============\r\n            logits = slim.conv2d_transpose(net, num_classes, [2,2], stride=2, scope='fullconv')\r\n            probabilities = tf.nn.softmax(logits, name='logits_to_softmax')\r\n\r\n        return logits, probabilities\r\n\r\n\r\ndef ENet_arg_scope(weight_decay=2e-4,\r\n                   batch_norm_decay=0.1,\r\n                   batch_norm_epsilon=0.001):\r\n\r\n  # Set weight_decay for weights in conv2d and separable_conv2d layers.\r\n  with slim.arg_scope([slim.conv2d],\r\n                      weights_regularizer=slim.l2_regularizer(weight_decay),\r\n                      biases_regularizer=slim.l2_regularizer(weight_decay)):\r\n\r\n    # Set parameters for batch_norm.\r\n    with slim.arg_scope([slim.batch_norm],\r\n                        decay=batch_norm_decay,\r\n                        epsilon=batch_norm_epsilon) as scope:\r\n      return scope\r\n```\r\n</div></details><br>"}