{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351531869", "html_url": "https://github.com/tensorflow/tensorflow/issues/15284#issuecomment-351531869", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15284", "id": 351531869, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTUzMTg2OQ==", "user": {"login": "flx42", "id": 3645581, "node_id": "MDQ6VXNlcjM2NDU1ODE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3645581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/flx42", "html_url": "https://github.com/flx42", "followers_url": "https://api.github.com/users/flx42/followers", "following_url": "https://api.github.com/users/flx42/following{/other_user}", "gists_url": "https://api.github.com/users/flx42/gists{/gist_id}", "starred_url": "https://api.github.com/users/flx42/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/flx42/subscriptions", "organizations_url": "https://api.github.com/users/flx42/orgs", "repos_url": "https://api.github.com/users/flx42/repos", "events_url": "https://api.github.com/users/flx42/events{/privacy}", "received_events_url": "https://api.github.com/users/flx42/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-13T21:33:31Z", "updated_at": "2017-12-13T21:40:50Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I tested that yesterday, I only built for <code>code=sm_61,compute_61</code> (the PTX is useless here actually). I gained ~300MB compared to all the archs we have today.<br>\nIt also speeds-up build times.</p>\n<p>Our libraries (like <code>cublas</code>, <code>cudnn</code>, <code>cufft</code>) are large because they are also compiled for many compute archs. Even if you create a build of TensorFlow only for <code>6.1</code>, <code>libcublas.so</code> will still carry the compute archs you don't care about.</p>\n<p>We do have a tool called <code>nvprune</code>, it can allow you to strip some binary/PTX code, but it doesn't work for shared libraries today, unfortunately. If you really wanted to have a minimal TensorFlow GPU build targeted at only one GPU type, I think the only option would be to use nvprune on <code>lib{cufft,cudnn,cublas,cusparse}_static.a</code> and then statically link your TensorFlow binary (somehow).</p>", "body_text": "I tested that yesterday, I only built for code=sm_61,compute_61 (the PTX is useless here actually). I gained ~300MB compared to all the archs we have today.\nIt also speeds-up build times.\nOur libraries (like cublas, cudnn, cufft) are large because they are also compiled for many compute archs. Even if you create a build of TensorFlow only for 6.1, libcublas.so will still carry the compute archs you don't care about.\nWe do have a tool called nvprune, it can allow you to strip some binary/PTX code, but it doesn't work for shared libraries today, unfortunately. If you really wanted to have a minimal TensorFlow GPU build targeted at only one GPU type, I think the only option would be to use nvprune on lib{cufft,cudnn,cublas,cusparse}_static.a and then statically link your TensorFlow binary (somehow).", "body": "I tested that yesterday, I only built for `code=sm_61,compute_61` (the PTX is useless here actually). I gained ~300MB compared to all the archs we have today.\r\nIt also speeds-up build times.\r\n\r\nOur libraries (like `cublas`, `cudnn`, `cufft`) are large because they are also compiled for many compute archs. Even if you create a build of TensorFlow only for `6.1`, `libcublas.so` will still carry the compute archs you don't care about.\r\n\r\nWe do have a tool called `nvprune`, it can allow you to strip some binary/PTX code, but it doesn't work for shared libraries today, unfortunately. If you really wanted to have a minimal TensorFlow GPU build targeted at only one GPU type, I think the only option would be to use nvprune on `lib{cufft,cudnn,cublas,cusparse}_static.a` and then statically link your TensorFlow binary (somehow).\r\n"}