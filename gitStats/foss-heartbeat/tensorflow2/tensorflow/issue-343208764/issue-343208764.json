{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20999", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20999/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20999/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20999/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20999", "id": 343208764, "node_id": "MDU6SXNzdWUzNDMyMDg3NjQ=", "number": 20999, "title": "incompatibility : Keras LearningRateScheduler callback and tf.train.optimizer", "user": {"login": "nairouz", "id": 10966954, "node_id": "MDQ6VXNlcjEwOTY2OTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/10966954?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nairouz", "html_url": "https://github.com/nairouz", "followers_url": "https://api.github.com/users/nairouz/followers", "following_url": "https://api.github.com/users/nairouz/following{/other_user}", "gists_url": "https://api.github.com/users/nairouz/gists{/gist_id}", "starred_url": "https://api.github.com/users/nairouz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nairouz/subscriptions", "organizations_url": "https://api.github.com/users/nairouz/orgs", "repos_url": "https://api.github.com/users/nairouz/repos", "events_url": "https://api.github.com/users/nairouz/events{/privacy}", "received_events_url": "https://api.github.com/users/nairouz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tanzhenyu", "id": 15220929, "node_id": "MDQ6VXNlcjE1MjIwOTI5", "avatar_url": "https://avatars3.githubusercontent.com/u/15220929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanzhenyu", "html_url": "https://github.com/tanzhenyu", "followers_url": "https://api.github.com/users/tanzhenyu/followers", "following_url": "https://api.github.com/users/tanzhenyu/following{/other_user}", "gists_url": "https://api.github.com/users/tanzhenyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanzhenyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanzhenyu/subscriptions", "organizations_url": "https://api.github.com/users/tanzhenyu/orgs", "repos_url": "https://api.github.com/users/tanzhenyu/repos", "events_url": "https://api.github.com/users/tanzhenyu/events{/privacy}", "received_events_url": "https://api.github.com/users/tanzhenyu/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tanzhenyu", "id": 15220929, "node_id": "MDQ6VXNlcjE1MjIwOTI5", "avatar_url": "https://avatars3.githubusercontent.com/u/15220929?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tanzhenyu", "html_url": "https://github.com/tanzhenyu", "followers_url": "https://api.github.com/users/tanzhenyu/followers", "following_url": "https://api.github.com/users/tanzhenyu/following{/other_user}", "gists_url": "https://api.github.com/users/tanzhenyu/gists{/gist_id}", "starred_url": "https://api.github.com/users/tanzhenyu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tanzhenyu/subscriptions", "organizations_url": "https://api.github.com/users/tanzhenyu/orgs", "repos_url": "https://api.github.com/users/tanzhenyu/repos", "events_url": "https://api.github.com/users/tanzhenyu/events{/privacy}", "received_events_url": "https://api.github.com/users/tanzhenyu/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-07-20T18:38:56Z", "updated_at": "2018-11-20T07:54:22Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Google Colab</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>To make it short, there is an incompatibility between Keras LearningRateScheduler callback and tf.train.optimizer.  Keras optimizers have some specific attributes which are required for Keras callbacks and that does not seem to be the case for tf.train.optimizer.</p>\n<p>Unfortunately, Keras optimizers are incompatible with the eager execution mode. So, basically, the user is compelled to choose between using Keras callbacks and the eager execution mode.</p>\n<p>Here is a code proving that. It runs fine with a typical Keras optimizer and fails with the tensorflow one.</p>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\ndef step_decay(epoch):\n  initial_rate = 1e-3\n  factor = int(epoch / 5)\n  lr = initial_rate / (10 ** factor)\n  return lr\n\nlr_schedule = LearningRateScheduler(step_decay)\n\ninput1 = Input(shape=(10,), name=\"input\")\nout = Dense(5, activation=\"relu\")(input1)\nmodel = Model(inputs=input1, outputs=out)\nmodel.compile(optimizer= tf.train.AdamOptimizer(1e-3), loss='mse')\n\nnp.random.seed(0)\nX = np.random.random((20, 10)).astype(np.float32)\nY = np.random.random((20, 5)).astype(np.float32)\n\nmodel.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\n</code></pre>\n<h3>Logs</h3>\n<p>Epoch 1/10</p>\n<p>ValueErrorTraceback (most recent call last)<br>\n in ()<br>\n24 Y = np.random.random((20, 5)).astype(np.float32)<br>\n25<br>\n---&gt; 26 model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)<br>\n1346           initial_epoch=initial_epoch,<br>\n1347           steps_per_epoch=steps_per_epoch,<br>\n-&gt; 1348           validation_steps=validation_steps)<br>\n1349<br>\n1350   def evaluate(self,</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)<br>\n183       m.reset_states()<br>\n184     # Update callbacks<br>\n--&gt; 185     callbacks.on_epoch_begin(epoch)<br>\n186     epoch_logs = {}<br>\n187     if steps_per_epoch is not None:</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)<br>\n79     logs = logs or {}<br>\n80     for callback in self.callbacks:<br>\n---&gt; 81       callback.on_epoch_begin(epoch, logs)<br>\n82     self._delta_t_batch = 0.<br>\n83     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)</p>\n<p>/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)<br>\n635   def on_epoch_begin(self, epoch, logs=None):<br>\n636     if not hasattr(self.model.optimizer, 'lr'):<br>\n--&gt; 637       raise ValueError('Optimizer must have a \"lr\" attribute.')<br>\n638     lr = self.schedule(epoch)<br>\n639     if not isinstance(lr, (float, np.float32, np.float64)):</p>\n<p>ValueError: Optimizer must have a \"lr\" attribute.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Google Colab\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below): 1.9.0\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nTo make it short, there is an incompatibility between Keras LearningRateScheduler callback and tf.train.optimizer.  Keras optimizers have some specific attributes which are required for Keras callbacks and that does not seem to be the case for tf.train.optimizer.\nUnfortunately, Keras optimizers are incompatible with the eager execution mode. So, basically, the user is compelled to choose between using Keras callbacks and the eager execution mode.\nHere is a code proving that. It runs fine with a typical Keras optimizer and fails with the tensorflow one.\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras as keras \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.callbacks import LearningRateScheduler\n\ndef step_decay(epoch):\n  initial_rate = 1e-3\n  factor = int(epoch / 5)\n  lr = initial_rate / (10 ** factor)\n  return lr\n\nlr_schedule = LearningRateScheduler(step_decay)\n\ninput1 = Input(shape=(10,), name=\"input\")\nout = Dense(5, activation=\"relu\")(input1)\nmodel = Model(inputs=input1, outputs=out)\nmodel.compile(optimizer= tf.train.AdamOptimizer(1e-3), loss='mse')\n\nnp.random.seed(0)\nX = np.random.random((20, 10)).astype(np.float32)\nY = np.random.random((20, 5)).astype(np.float32)\n\nmodel.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\n\nLogs\nEpoch 1/10\nValueErrorTraceback (most recent call last)\n in ()\n24 Y = np.random.random((20, 5)).astype(np.float32)\n25\n---> 26 model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n1346           initial_epoch=initial_epoch,\n1347           steps_per_epoch=steps_per_epoch,\n-> 1348           validation_steps=validation_steps)\n1349\n1350   def evaluate(self,\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\n183       m.reset_states()\n184     # Update callbacks\n--> 185     callbacks.on_epoch_begin(epoch)\n186     epoch_logs = {}\n187     if steps_per_epoch is not None:\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\n79     logs = logs or {}\n80     for callback in self.callbacks:\n---> 81       callback.on_epoch_begin(epoch, logs)\n82     self._delta_t_batch = 0.\n83     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\n635   def on_epoch_begin(self, epoch, logs=None):\n636     if not hasattr(self.model.optimizer, 'lr'):\n--> 637       raise ValueError('Optimizer must have a \"lr\" attribute.')\n638     lr = self.schedule(epoch)\n639     if not isinstance(lr, (float, np.float32, np.float64)):\nValueError: Optimizer must have a \"lr\" attribute.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Google Colab\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**: 1.9.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nTo make it short, there is an incompatibility between Keras LearningRateScheduler callback and tf.train.optimizer.  Keras optimizers have some specific attributes which are required for Keras callbacks and that does not seem to be the case for tf.train.optimizer.\r\n\r\nUnfortunately, Keras optimizers are incompatible with the eager execution mode. So, basically, the user is compelled to choose between using Keras callbacks and the eager execution mode.\r\n\r\nHere is a code proving that. It runs fine with a typical Keras optimizer and fails with the tensorflow one.\r\n\r\n### Source code / logs\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.keras as keras \r\nfrom tensorflow.keras.models import Model\r\nfrom tensorflow.keras.layers import Dense, Input\r\nfrom tensorflow.keras.callbacks import LearningRateScheduler\r\n\r\ndef step_decay(epoch):\r\n  initial_rate = 1e-3\r\n  factor = int(epoch / 5)\r\n  lr = initial_rate / (10 ** factor)\r\n  return lr\r\n\r\nlr_schedule = LearningRateScheduler(step_decay)\r\n\r\ninput1 = Input(shape=(10,), name=\"input\")\r\nout = Dense(5, activation=\"relu\")(input1)\r\nmodel = Model(inputs=input1, outputs=out)\r\nmodel.compile(optimizer= tf.train.AdamOptimizer(1e-3), loss='mse')\r\n\r\nnp.random.seed(0)\r\nX = np.random.random((20, 10)).astype(np.float32)\r\nY = np.random.random((20, 5)).astype(np.float32)\r\n\r\nmodel.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\r\n```\r\n\r\n### Logs\r\n\r\nEpoch 1/10\r\n\r\n\r\nValueErrorTraceback (most recent call last)\r\n<ipython-input-9-c41c71fed68e> in <module>()\r\n     24 Y = np.random.random((20, 5)).astype(np.float32)\r\n     25 \r\n---> 26 model.fit(x=X, y=Y, batch_size=1, epochs=10, callbacks=[lr_schedule])\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training.pyc in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1346           initial_epoch=initial_epoch,\r\n   1347           steps_per_epoch=steps_per_epoch,\r\n-> 1348           validation_steps=validation_steps)\r\n   1349 \r\n   1350   def evaluate(self,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/engine/training_arrays.pyc in fit_loop(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\r\n    183       m.reset_states()\r\n    184     # Update callbacks\r\n--> 185     callbacks.on_epoch_begin(epoch)\r\n    186     epoch_logs = {}\r\n    187     if steps_per_epoch is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\r\n     79     logs = logs or {}\r\n     80     for callback in self.callbacks:\r\n---> 81       callback.on_epoch_begin(epoch, logs)\r\n     82     self._delta_t_batch = 0.\r\n     83     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/callbacks.pyc in on_epoch_begin(self, epoch, logs)\r\n    635   def on_epoch_begin(self, epoch, logs=None):\r\n    636     if not hasattr(self.model.optimizer, 'lr'):\r\n--> 637       raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n    638     lr = self.schedule(epoch)\r\n    639     if not isinstance(lr, (float, np.float32, np.float64)):\r\n\r\nValueError: Optimizer must have a \"lr\" attribute.\r\n\r\n\r\n"}