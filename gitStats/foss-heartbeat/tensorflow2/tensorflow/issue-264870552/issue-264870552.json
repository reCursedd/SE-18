{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13657", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13657/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13657/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13657/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13657", "id": 264870552, "node_id": "MDU6SXNzdWUyNjQ4NzA1NTI=", "number": 13657, "title": "tf.train.import_meta_graph() works strangely compared to restored session with tf.train.Supervisor", "user": {"login": "houssamzenati", "id": 23338676, "node_id": "MDQ6VXNlcjIzMzM4Njc2", "avatar_url": "https://avatars3.githubusercontent.com/u/23338676?v=4", "gravatar_id": "", "url": "https://api.github.com/users/houssamzenati", "html_url": "https://github.com/houssamzenati", "followers_url": "https://api.github.com/users/houssamzenati/followers", "following_url": "https://api.github.com/users/houssamzenati/following{/other_user}", "gists_url": "https://api.github.com/users/houssamzenati/gists{/gist_id}", "starred_url": "https://api.github.com/users/houssamzenati/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/houssamzenati/subscriptions", "organizations_url": "https://api.github.com/users/houssamzenati/orgs", "repos_url": "https://api.github.com/users/houssamzenati/repos", "events_url": "https://api.github.com/users/houssamzenati/events{/privacy}", "received_events_url": "https://api.github.com/users/houssamzenati/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-10-12T09:23:18Z", "updated_at": "2017-10-13T14:57:56Z", "closed_at": "2017-10-13T14:57:00Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.1.0-13-g8ddd727 1.1.0</li>\n<li><strong>Python version</strong>: Python 3.4.3</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate (GHz) 1.076<br>\npciBusID 0000:02:00.0<br>\nTotal memory: 11.92GiB</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Working on GANs in tensorflow.</p>\n<p>I would like to load a generator from a different saved session and graph, in order to do some new ops on this part of the graph that has been trained. I use tf.train.import_meta_graph() but it works strangely compared to restored session with tf.train.Supervisor.<br>\nIt seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way.<br>\nI don't know if it is a bug.</p>\n<h3>Source code / logs</h3>\n<p>When I use tf.train.import_meta_graph():</p>\n<pre><code>train_dir = \"./train_logs/mnist/0\" \nckpt = tf.train.latest_checkpoint(train_dir)\nfilename = \".\".join([ckpt, 'meta'])\nsaver = tf.train.import_meta_graph(filename)\n\nz_optim = tf.get_variable(name='z_optim', shape= [number_ini_z * batch_imgs_number, 100], initializer=tf.truncated_normal_initializer())\ngen_z = dcgan.generator(z_optim, is_training=False, reuse=False, name='generator_z')\n</code></pre>\n<p>Then I create basic image summary:</p>\n<pre><code>image_test = tf.cast(((gen_z / 2.0) + 0.5) * 255.0, tf.uint8)\ns_test = tf.summary.image('reconstructed_image', image_test, max_outputs=3)\n</code></pre>\n<p>In order to reassign all the trained values to the new generator created from scratch, I collect all the variables that were in the scope of the \"generator\" and then create my own list of initializers:</p>\n<pre><code>gen_variables_to_initialize = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_z')\n\ngen_tensors_to_restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n\nlist_assign_op = []\n\nfor variables, tensors in zip(gen_variables_to_initialize, gen_tensors_to_restore):\n    list_assign_op.append(tf.assign(variables, tensors))\n\n</code></pre>\n<p>I have to explicitly create another list of variables that have to be initialized:</p>\n<pre><code>variables_initializer_except_for_generator = []\n\nvariables_to_initialize = gen_variables_to_initialize + dis_variables_to_initialize \n\nfor variable in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n        if variable not in variables_to_initialize:\n            variables_initializer_except_for_generator.append(variable.initializer)\n</code></pre>\n<p>so I run the session:</p>\n<pre><code>with tf.Session() as sess:\n    logwriter = tf.summary.FileWriter('./logs', sess.graph)\n    saver.restore(sess, ckpt)\n    sess.run((list_assign_op, variables_initializer_except_for_generator))\n    s = sess.run(s_test)\n    logwriter.add_summary(s)\n</code></pre>\n<p>This code gives me such digits on tensorboard:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/23338676/31488738-4d73e66c-af71-11e7-94a2-161745e052d0.jpg\"><img src=\"https://user-images.githubusercontent.com/23338676/31488738-4d73e66c-af71-11e7-94a2-161745e052d0.jpg\" alt=\"yo\" style=\"max-width:100%;\"></a></p>\n<p>And then I remembered I used tf.train.Supervisor during the training, At the beginning I thought my GAN was not trained enough, but I instead ran:</p>\n<pre><code>logdir = \"./train_logs/mnist/0\"\nsv = tf.train.Supervisor(logdir=logdir,\n                           save_summaries_secs=None, save_model_secs=120)\n\nwith sv.managed_session() as sess:\n    logwriter = tf.summary.FileWriter('./logs2', sess.graph)\n    s = sess.run(s_test)\n    logwriter.add_summary(s)\n</code></pre>\n<p>I had this on tensorboard:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/23338676/31488901-bf877566-af71-11e7-9f22-419a7b0f425b.jpg\"><img src=\"https://user-images.githubusercontent.com/23338676/31488901-bf877566-af71-11e7-9f22-419a7b0f425b.jpg\" alt=\"yo2\" style=\"max-width:100%;\"></a></p>\n<p>It seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way.</p>\n<p>The problem is that I can't use the second method if I want to do some new stuffs with the generator (such as inverting it through another training, but his time on z_optim) because it does not want to add new nodes to the graph when it has not found it in previous training checkpoint. (Btw I had to change z_optim in tf.random_normal([number_ini_z * batch_imgs_number, 100], mean=0.0, stddev=1.0,name='random_z') for that same reason.</p>\n<p>Do you have an any idea of why such a thing occurs? Or any other suggestion in loading the generator?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): v1.1.0-13-g8ddd727 1.1.0\nPython version: Python 3.4.3\nCUDA/cuDNN version: CUDA\nGPU model and memory: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 11.92GiB\n\nDescribe the problem\nWorking on GANs in tensorflow.\nI would like to load a generator from a different saved session and graph, in order to do some new ops on this part of the graph that has been trained. I use tf.train.import_meta_graph() but it works strangely compared to restored session with tf.train.Supervisor.\nIt seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way.\nI don't know if it is a bug.\nSource code / logs\nWhen I use tf.train.import_meta_graph():\ntrain_dir = \"./train_logs/mnist/0\" \nckpt = tf.train.latest_checkpoint(train_dir)\nfilename = \".\".join([ckpt, 'meta'])\nsaver = tf.train.import_meta_graph(filename)\n\nz_optim = tf.get_variable(name='z_optim', shape= [number_ini_z * batch_imgs_number, 100], initializer=tf.truncated_normal_initializer())\ngen_z = dcgan.generator(z_optim, is_training=False, reuse=False, name='generator_z')\n\nThen I create basic image summary:\nimage_test = tf.cast(((gen_z / 2.0) + 0.5) * 255.0, tf.uint8)\ns_test = tf.summary.image('reconstructed_image', image_test, max_outputs=3)\n\nIn order to reassign all the trained values to the new generator created from scratch, I collect all the variables that were in the scope of the \"generator\" and then create my own list of initializers:\ngen_variables_to_initialize = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_z')\n\ngen_tensors_to_restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\n\nlist_assign_op = []\n\nfor variables, tensors in zip(gen_variables_to_initialize, gen_tensors_to_restore):\n    list_assign_op.append(tf.assign(variables, tensors))\n\n\nI have to explicitly create another list of variables that have to be initialized:\nvariables_initializer_except_for_generator = []\n\nvariables_to_initialize = gen_variables_to_initialize + dis_variables_to_initialize \n\nfor variable in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\n        if variable not in variables_to_initialize:\n            variables_initializer_except_for_generator.append(variable.initializer)\n\nso I run the session:\nwith tf.Session() as sess:\n    logwriter = tf.summary.FileWriter('./logs', sess.graph)\n    saver.restore(sess, ckpt)\n    sess.run((list_assign_op, variables_initializer_except_for_generator))\n    s = sess.run(s_test)\n    logwriter.add_summary(s)\n\nThis code gives me such digits on tensorboard:\n\nAnd then I remembered I used tf.train.Supervisor during the training, At the beginning I thought my GAN was not trained enough, but I instead ran:\nlogdir = \"./train_logs/mnist/0\"\nsv = tf.train.Supervisor(logdir=logdir,\n                           save_summaries_secs=None, save_model_secs=120)\n\nwith sv.managed_session() as sess:\n    logwriter = tf.summary.FileWriter('./logs2', sess.graph)\n    s = sess.run(s_test)\n    logwriter.add_summary(s)\n\nI had this on tensorboard:\n\nIt seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way.\nThe problem is that I can't use the second method if I want to do some new stuffs with the generator (such as inverting it through another training, but his time on z_optim) because it does not want to add new nodes to the graph when it has not found it in previous training checkpoint. (Btw I had to change z_optim in tf.random_normal([number_ini_z * batch_imgs_number, 100], mean=0.0, stddev=1.0,name='random_z') for that same reason.\nDo you have an any idea of why such a thing occurs? Or any other suggestion in loading the generator?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: v1.1.0-13-g8ddd727 1.1.0\r\n- **Python version**: Python 3.4.3 \r\n- **CUDA/cuDNN version**: CUDA\r\n- **GPU model and memory**: GeForce GTX TITAN X major: 5 minor: 2 memoryClockRate (GHz) 1.076\r\npciBusID 0000:02:00.0\r\nTotal memory: 11.92GiB\r\n\r\n### Describe the problem\r\n\r\nWorking on GANs in tensorflow.\r\n\r\nI would like to load a generator from a different saved session and graph, in order to do some new ops on this part of the graph that has been trained. I use tf.train.import_meta_graph() but it works strangely compared to restored session with tf.train.Supervisor.\r\nIt seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way. \r\nI don't know if it is a bug.\r\n\r\n### Source code / logs\r\n\r\nWhen I use tf.train.import_meta_graph():\r\n```\r\ntrain_dir = \"./train_logs/mnist/0\" \r\nckpt = tf.train.latest_checkpoint(train_dir)\r\nfilename = \".\".join([ckpt, 'meta'])\r\nsaver = tf.train.import_meta_graph(filename)\r\n\r\nz_optim = tf.get_variable(name='z_optim', shape= [number_ini_z * batch_imgs_number, 100], initializer=tf.truncated_normal_initializer())\r\ngen_z = dcgan.generator(z_optim, is_training=False, reuse=False, name='generator_z')\r\n```\r\nThen I create basic image summary:\r\n```\r\nimage_test = tf.cast(((gen_z / 2.0) + 0.5) * 255.0, tf.uint8)\r\ns_test = tf.summary.image('reconstructed_image', image_test, max_outputs=3)\r\n```\r\nIn order to reassign all the trained values to the new generator created from scratch, I collect all the variables that were in the scope of the \"generator\" and then create my own list of initializers:\r\n```\r\ngen_variables_to_initialize = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator_z')\r\n\r\ngen_tensors_to_restore = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='generator')\r\n\r\nlist_assign_op = []\r\n\r\nfor variables, tensors in zip(gen_variables_to_initialize, gen_tensors_to_restore):\r\n    list_assign_op.append(tf.assign(variables, tensors))\r\n\r\n```\r\nI have to explicitly create another list of variables that have to be initialized:\r\n```\r\nvariables_initializer_except_for_generator = []\r\n\r\nvariables_to_initialize = gen_variables_to_initialize + dis_variables_to_initialize \r\n\r\nfor variable in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES):\r\n        if variable not in variables_to_initialize:\r\n            variables_initializer_except_for_generator.append(variable.initializer)\r\n```\r\n so I run the session:\r\n```\r\nwith tf.Session() as sess:\r\n    logwriter = tf.summary.FileWriter('./logs', sess.graph)\r\n    saver.restore(sess, ckpt)\r\n    sess.run((list_assign_op, variables_initializer_except_for_generator))\r\n    s = sess.run(s_test)\r\n    logwriter.add_summary(s)\r\n```\r\nThis code gives me such digits on tensorboard: \r\n![yo](https://user-images.githubusercontent.com/23338676/31488738-4d73e66c-af71-11e7-94a2-161745e052d0.jpg)\r\n\r\nAnd then I remembered I used tf.train.Supervisor during the training, At the beginning I thought my GAN was not trained enough, but I instead ran:\r\n```\r\nlogdir = \"./train_logs/mnist/0\"\r\nsv = tf.train.Supervisor(logdir=logdir,\r\n                           save_summaries_secs=None, save_model_secs=120)\r\n\r\nwith sv.managed_session() as sess:\r\n    logwriter = tf.summary.FileWriter('./logs2', sess.graph)\r\n    s = sess.run(s_test)\r\n    logwriter.add_summary(s)\r\n```\r\nI had this on tensorboard:\r\n\r\n![yo2](https://user-images.githubusercontent.com/23338676/31488901-bf877566-af71-11e7-9f22-419a7b0f425b.jpg)\r\n\r\nIt seems like the tf.train.Supervisor reassigns the tensors in a better way. I would have guessed that the generator would have been successfully loaded or not, but not in such a strange way.\r\n\r\nThe problem is that I can't use the second method if I want to do some new stuffs with the generator (such as inverting it through another training, but his time on z_optim) because it does not want to add new nodes to the graph when it has not found it in previous training checkpoint. (Btw I had to change z_optim in tf.random_normal([number_ini_z * batch_imgs_number, 100], mean=0.0, stddev=1.0,name='random_z') for that same reason.\r\n\r\nDo you have an any idea of why such a thing occurs? Or any other suggestion in loading the generator?"}