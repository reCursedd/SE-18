{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16764", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16764/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16764/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16764/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16764", "id": 294263476, "node_id": "MDU6SXNzdWUyOTQyNjM0NzY=", "number": 16764, "title": "Does it makes sense to use AdamOptimizer with Dropout?", "user": {"login": "jmlipman", "id": 3540650, "node_id": "MDQ6VXNlcjM1NDA2NTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/3540650?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jmlipman", "html_url": "https://github.com/jmlipman", "followers_url": "https://api.github.com/users/jmlipman/followers", "following_url": "https://api.github.com/users/jmlipman/following{/other_user}", "gists_url": "https://api.github.com/users/jmlipman/gists{/gist_id}", "starred_url": "https://api.github.com/users/jmlipman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jmlipman/subscriptions", "organizations_url": "https://api.github.com/users/jmlipman/orgs", "repos_url": "https://api.github.com/users/jmlipman/repos", "events_url": "https://api.github.com/users/jmlipman/events{/privacy}", "received_events_url": "https://api.github.com/users/jmlipman/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-02-05T03:26:57Z", "updated_at": "2018-02-05T07:23:02Z", "closed_at": "2018-02-05T07:23:02Z", "author_association": "NONE", "body_html": "<p>I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.<br>\nMy network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.</p>\n<p>I was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.</p>\n<p>Here is the code</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\n# As input, 100 random numbers.\ninput_size = 100\noutput_size = 1\n\nx = tf.placeholder(tf.float32,[None, input_size],name=\"input\")\ny = tf.placeholder(tf.float32,[None, output_size],name=\"labels\")\n\nwith tf.variable_scope(\"dense1\") as scope:\n    W = tf.get_variable(\"W\",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())\n    b = tf.get_variable(\"b\",initializer=tf.zeros([output_size]))\n    dropped = tf.nn.dropout(x,0.8)\n    dense = tf.matmul(dropped,W)+b\n\neval_pred = tf.nn.sigmoid(dense,name=\"prediction\")\n\ncost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))\n#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\ntrain_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)\n\n\n# 20 epochs, batch size of 1\nepochs = 20\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    allWeights = []\n    for i in range(epochs):\n\n        x_raw = np.random.random((1,input_size))\n        y_raw = np.random.random((1,output_size))\n        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})\n        #print(\"Epoch {0}/{1}. Loss: {2}\".format(i+1,epochs,c))\n\n        # Numbers will be around 20% of input_size (17-22)\n        print(np.sum(d==0))\n        allWeights.append(w)\n\nprint(\"Calculate the difference between W_i and W_{i-1}\")\nfor wi in range(1,len(allWeights)):\n    difference = allWeights[wi]-allWeights[wi-1]\n    # I expect that there will be around 20 weights that won't be updated\n    # so the difference between the current weight and the previous one\n    # should be zero.\n    print(np.sum(difference==0))\n</code></pre>\n<p>Just in case is not clear enough in the code, I'm printing two sets of numbers:<br>\nThe first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.<br>\nThe second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.</p>\n<p>Again, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.</p>\n<p>Question:<br>\nIs this a bug or is it supposed to be like this?<br>\nIn the latter case, does it make sense to use Adam with Dropout?</p>", "body_text": "I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.\nMy network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.\nI was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.\nHere is the code\nimport numpy as np\nimport tensorflow as tf\n\n# As input, 100 random numbers.\ninput_size = 100\noutput_size = 1\n\nx = tf.placeholder(tf.float32,[None, input_size],name=\"input\")\ny = tf.placeholder(tf.float32,[None, output_size],name=\"labels\")\n\nwith tf.variable_scope(\"dense1\") as scope:\n    W = tf.get_variable(\"W\",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())\n    b = tf.get_variable(\"b\",initializer=tf.zeros([output_size]))\n    dropped = tf.nn.dropout(x,0.8)\n    dense = tf.matmul(dropped,W)+b\n\neval_pred = tf.nn.sigmoid(dense,name=\"prediction\")\n\ncost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))\n#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\ntrain_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)\n\n\n# 20 epochs, batch size of 1\nepochs = 20\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    allWeights = []\n    for i in range(epochs):\n\n        x_raw = np.random.random((1,input_size))\n        y_raw = np.random.random((1,output_size))\n        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})\n        #print(\"Epoch {0}/{1}. Loss: {2}\".format(i+1,epochs,c))\n\n        # Numbers will be around 20% of input_size (17-22)\n        print(np.sum(d==0))\n        allWeights.append(w)\n\nprint(\"Calculate the difference between W_i and W_{i-1}\")\nfor wi in range(1,len(allWeights)):\n    difference = allWeights[wi]-allWeights[wi-1]\n    # I expect that there will be around 20 weights that won't be updated\n    # so the difference between the current weight and the previous one\n    # should be zero.\n    print(np.sum(difference==0))\n\nJust in case is not clear enough in the code, I'm printing two sets of numbers:\nThe first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.\nThe second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.\nAgain, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.\nQuestion:\nIs this a bug or is it supposed to be like this?\nIn the latter case, does it make sense to use Adam with Dropout?", "body": "I was experimenting with Dropout and I tried to check the number of weights updated in every iteration.\r\nMy network has an input layer of size 100 and output layer of size 1, and I use dropout with keep_prob of 0.8.  With this configuration, I am expecting to update every time around 80 neurons. I tried to check this, and I got weird results. I asked in stackexchange and someone got the right answer: the optimizer was updating all the weights.\r\n\r\nI was using Adam, and when I changed to GradientDescent, Adagrad and Adadelta it worked well. I haven't tried more optimizers thought.\r\n\r\nHere is the code\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\n# As input, 100 random numbers.\r\ninput_size = 100\r\noutput_size = 1\r\n\r\nx = tf.placeholder(tf.float32,[None, input_size],name=\"input\")\r\ny = tf.placeholder(tf.float32,[None, output_size],name=\"labels\")\r\n\r\nwith tf.variable_scope(\"dense1\") as scope:\r\n    W = tf.get_variable(\"W\",shape=[input_size,output_size],initializer=tf.keras.initializers.he_uniform())\r\n    b = tf.get_variable(\"b\",initializer=tf.zeros([output_size]))\r\n    dropped = tf.nn.dropout(x,0.8)\r\n    dense = tf.matmul(dropped,W)+b\r\n\r\neval_pred = tf.nn.sigmoid(dense,name=\"prediction\")\r\n\r\ncost = tf.reduce_mean(tf.losses.absolute_difference(eval_pred,y))\r\n#train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\r\ntrain_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\r\ntrain_step = tf.train.AdadeltaOptimizer(learning_rate=0.01).minimize(cost)\r\n\r\n\r\n# 20 epochs, batch size of 1\r\nepochs = 20\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    allWeights = []\r\n    for i in range(epochs):\r\n\r\n        x_raw = np.random.random((1,input_size))\r\n        y_raw = np.random.random((1,output_size))\r\n        [_,c,d,w]=sess.run([train_step,cost,dropped,W], feed_dict={x: x_raw, y: y_raw})\r\n        #print(\"Epoch {0}/{1}. Loss: {2}\".format(i+1,epochs,c))\r\n\r\n        # Numbers will be around 20% of input_size (17-22)\r\n        print(np.sum(d==0))\r\n        allWeights.append(w)\r\n\r\nprint(\"Calculate the difference between W_i and W_{i-1}\")\r\nfor wi in range(1,len(allWeights)):\r\n    difference = allWeights[wi]-allWeights[wi-1]\r\n    # I expect that there will be around 20 weights that won't be updated\r\n    # so the difference between the current weight and the previous one\r\n    # should be zero.\r\n    print(np.sum(difference==0))\r\n```\r\n\r\nJust in case is not clear enough in the code, I'm printing two sets of numbers:\r\nThe first set is the number of zeros in the masked dropout layer, and since I'm using 0.8 keep_prob, I should have around 20% of zeros (so, I should get a number around 20). This part works well.\r\nThe second set counts how many weights were NOT updated (difference between the previous weight and the current weight). Therefore, I am expecting these two sets to display the same numbers.\r\n\r\nAgain, with Adam doesn't work because it updates more weights whereas GradientDescend, Adadelta and Adagrad it works well.\r\n\r\nQuestion:\r\nIs this a bug or is it supposed to be like this?\r\nIn the latter case, does it make sense to use Adam with Dropout?\r\n"}