{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8608", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8608/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8608/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8608/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8608", "id": 215917628, "node_id": "MDU6SXNzdWUyMTU5MTc2Mjg=", "number": 8608, "title": "gradient_override_map not working for tf.matmul", "user": {"login": "FrankWork", "id": 13351333, "node_id": "MDQ6VXNlcjEzMzUxMzMz", "avatar_url": "https://avatars3.githubusercontent.com/u/13351333?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FrankWork", "html_url": "https://github.com/FrankWork", "followers_url": "https://api.github.com/users/FrankWork/followers", "following_url": "https://api.github.com/users/FrankWork/following{/other_user}", "gists_url": "https://api.github.com/users/FrankWork/gists{/gist_id}", "starred_url": "https://api.github.com/users/FrankWork/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FrankWork/subscriptions", "organizations_url": "https://api.github.com/users/FrankWork/orgs", "repos_url": "https://api.github.com/users/FrankWork/repos", "events_url": "https://api.github.com/users/FrankWork/events{/privacy}", "received_events_url": "https://api.github.com/users/FrankWork/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-03-22T00:20:46Z", "updated_at": "2017-03-22T03:16:37Z", "closed_at": "2017-03-22T03:16:37Z", "author_association": "NONE", "body_html": "<h3><code>gradient_override_map</code> not working for <code>tf.matmul</code></h3>\n<p>I want to mask the gradient of <code>tf.matmul</code>, but I found that the <code>gradient_override_map</code> didn't work while it worked for <code>tf.square</code>.</p>\n<h3>Gradient override for <code>tf.matmul</code>: failed</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-en\">@tf.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CustomMatmul<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_custom_matmul_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n    mask <span class=\"pl-k\">=</span> tf.eye(<span class=\"pl-c1\">3</span>)\n    <span class=\"pl-k\">return</span> [tf.multiply(grad, mask)]\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> g:\n    w <span class=\"pl-k\">=</span> tf.Variable(tf.eye(<span class=\"pl-c1\">3</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n    x <span class=\"pl-k\">=</span> tf.constant([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">1</span>])\n    <span class=\"pl-k\">with</span> g.gradient_override_map({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Matmul<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CustomMatmul<span class=\"pl-pds\">\"</span></span>}):\n            logits <span class=\"pl-k\">=</span> tf.matmul(w, x, <span class=\"pl-v\">a_is_sparse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Matmul<span class=\"pl-pds\">\"</span></span>)\n    loss <span class=\"pl-k\">=</span> logits\n    optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.1</span>)<span class=\"pl-c\"><span class=\"pl-c\">#</span>.minimize(loss)</span>\n    grad_val <span class=\"pl-k\">=</span> optimizer.compute_gradients(loss)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> I do not use the following method because my actual code has many other gradients</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> mask = tf.eye(3)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> grad_val = [(tf.multiply(g, mask), v) for g, v in grad_val]</span>\n    train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(grad_val)\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>g) <span class=\"pl-k\">as</span> session:\n    session.run(tf.global_variables_initializer())\n    _, g_v, w <span class=\"pl-k\">=</span> session.run([train_op, grad_val, w])\n\n    <span class=\"pl-k\">for</span> g, v <span class=\"pl-k\">in</span> g_v:\n        <span class=\"pl-c1\">print</span>(g)\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">80</span>)\n        <span class=\"pl-c1\">print</span>(v)    </pre></div>\n<p>The output:</p>\n<pre><code>[[ 1.  2.  3.]\n [ 1.  2.  3.]\n [ 1.  2.  3.]]\n\n[[ 0.89999998 -0.2        -0.30000001]\n [-0.1         0.80000001 -0.30000001]\n [-0.1        -0.2         0.69999999]]\n</code></pre>\n<p>The expected output:</p>\n<pre><code>[[ 1.  0.  0.]\n [ 0.  2.  0.]\n [ 0.  0.  3.]]\n\n[[ 0.89999998  0.          0.        ]\n [ 0.          0.80000001  0.        ]\n [ 0.          0.          0.69999999]]\n</code></pre>\n<h3>Gradient override for <code>tf.square</code>: worked</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-en\">@tf.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CustomSquare<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_custom_square_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-k\">return</span> tf.constant([<span class=\"pl-c1\">101.0</span>])\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> g:\n  c <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">5.0</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n  s_1 <span class=\"pl-k\">=</span> tf.square(c)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Uses the default gradient for tf.square.</span>\n  <span class=\"pl-k\">with</span> g.gradient_override_map({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Square<span class=\"pl-pds\">\"</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>CustomSquare<span class=\"pl-pds\">\"</span></span>}):\n    s_2 <span class=\"pl-k\">=</span> tf.square(c, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>Square<span class=\"pl-pds\">'</span></span>)\n\n  optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">0.1</span>)<span class=\"pl-c\"><span class=\"pl-c\">#</span>.minimize(loss)</span>\n  grad_val <span class=\"pl-k\">=</span> optimizer.compute_gradients(s_2)\n  train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(grad_val)\n\n<span class=\"pl-k\">with</span> tf.Session(<span class=\"pl-v\">graph</span><span class=\"pl-k\">=</span>g) <span class=\"pl-k\">as</span> session:\n  session.run(tf.global_variables_initializer())\n\n  _, g_v <span class=\"pl-k\">=</span> session.run([train_op, grad_val])\n\n  <span class=\"pl-k\">for</span> g, v <span class=\"pl-k\">in</span> g_v:\n      <span class=\"pl-c1\">print</span>(g)\n      <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>*<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">80</span>)\n      <span class=\"pl-c1\">print</span>(v)</pre></div>\n<p>The output:</p>\n<pre><code>[ 101.]\n\n[-5.10000038]\n</code></pre>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nUbuntu 16.04<br>\nTensorFlow:<br>\n1.0.1</p>", "body_text": "gradient_override_map not working for tf.matmul\nI want to mask the gradient of tf.matmul, but I found that the gradient_override_map didn't work while it worked for tf.square.\nGradient override for tf.matmul: failed\nimport tensorflow as tf\n\n@tf.RegisterGradient(\"CustomMatmul\")\ndef _custom_matmul_grad(op, grad):\n    mask = tf.eye(3)\n    return [tf.multiply(grad, mask)]\n\nwith tf.Graph().as_default() as g:\n    w = tf.Variable(tf.eye(3), dtype=tf.float32)\n    x = tf.constant([1,2,3], dtype=tf.float32, shape=[3,1])\n    with g.gradient_override_map({\"Matmul\": \"CustomMatmul\"}):\n            logits = tf.matmul(w, x, a_is_sparse=True, name=\"Matmul\")\n    loss = logits\n    optimizer = tf.train.GradientDescentOptimizer(0.1)#.minimize(loss)\n    grad_val = optimizer.compute_gradients(loss)\n    # I do not use the following method because my actual code has many other gradients\n    # mask = tf.eye(3)\n    # grad_val = [(tf.multiply(g, mask), v) for g, v in grad_val]\n    train_op = optimizer.apply_gradients(grad_val)\n\nwith tf.Session(graph=g) as session:\n    session.run(tf.global_variables_initializer())\n    _, g_v, w = session.run([train_op, grad_val, w])\n\n    for g, v in g_v:\n        print(g)\n        print('*' * 80)\n        print(v)    \nThe output:\n[[ 1.  2.  3.]\n [ 1.  2.  3.]\n [ 1.  2.  3.]]\n\n[[ 0.89999998 -0.2        -0.30000001]\n [-0.1         0.80000001 -0.30000001]\n [-0.1        -0.2         0.69999999]]\n\nThe expected output:\n[[ 1.  0.  0.]\n [ 0.  2.  0.]\n [ 0.  0.  3.]]\n\n[[ 0.89999998  0.          0.        ]\n [ 0.          0.80000001  0.        ]\n [ 0.          0.          0.69999999]]\n\nGradient override for tf.square: worked\nimport tensorflow as tf\n\n@tf.RegisterGradient(\"CustomSquare\")\ndef _custom_square_grad(op, grad):\n  return tf.constant([101.0])\n\nwith tf.Graph().as_default() as g:\n  c = tf.Variable([5.0], dtype=tf.float32)\n  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n    s_2 = tf.square(c, name='Square')\n\n  optimizer = tf.train.GradientDescentOptimizer(0.1)#.minimize(loss)\n  grad_val = optimizer.compute_gradients(s_2)\n  train_op = optimizer.apply_gradients(grad_val)\n\nwith tf.Session(graph=g) as session:\n  session.run(tf.global_variables_initializer())\n\n  _, g_v = session.run([train_op, grad_val])\n\n  for g, v in g_v:\n      print(g)\n      print('*' * 80)\n      print(v)\nThe output:\n[ 101.]\n\n[-5.10000038]\n\nEnvironment info\nOperating System:\nUbuntu 16.04\nTensorFlow:\n1.0.1", "body": "### `gradient_override_map` not working for `tf.matmul`\r\n\r\nI want to mask the gradient of `tf.matmul`, but I found that the `gradient_override_map` didn't work while it worked for `tf.square`.\r\n\r\n### Gradient override for `tf.matmul`: failed\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.RegisterGradient(\"CustomMatmul\")\r\ndef _custom_matmul_grad(op, grad):\r\n    mask = tf.eye(3)\r\n    return [tf.multiply(grad, mask)]\r\n\r\nwith tf.Graph().as_default() as g:\r\n    w = tf.Variable(tf.eye(3), dtype=tf.float32)\r\n    x = tf.constant([1,2,3], dtype=tf.float32, shape=[3,1])\r\n    with g.gradient_override_map({\"Matmul\": \"CustomMatmul\"}):\r\n            logits = tf.matmul(w, x, a_is_sparse=True, name=\"Matmul\")\r\n    loss = logits\r\n    optimizer = tf.train.GradientDescentOptimizer(0.1)#.minimize(loss)\r\n    grad_val = optimizer.compute_gradients(loss)\r\n    # I do not use the following method because my actual code has many other gradients\r\n    # mask = tf.eye(3)\r\n    # grad_val = [(tf.multiply(g, mask), v) for g, v in grad_val]\r\n    train_op = optimizer.apply_gradients(grad_val)\r\n\r\nwith tf.Session(graph=g) as session:\r\n    session.run(tf.global_variables_initializer())\r\n    _, g_v, w = session.run([train_op, grad_val, w])\r\n\r\n    for g, v in g_v:\r\n        print(g)\r\n        print('*' * 80)\r\n        print(v)    \r\n```\r\n\r\nThe output:\r\n```\r\n[[ 1.  2.  3.]\r\n [ 1.  2.  3.]\r\n [ 1.  2.  3.]]\r\n\r\n[[ 0.89999998 -0.2        -0.30000001]\r\n [-0.1         0.80000001 -0.30000001]\r\n [-0.1        -0.2         0.69999999]]\r\n```\r\n\r\nThe expected output:\r\n```\r\n[[ 1.  0.  0.]\r\n [ 0.  2.  0.]\r\n [ 0.  0.  3.]]\r\n\r\n[[ 0.89999998  0.          0.        ]\r\n [ 0.          0.80000001  0.        ]\r\n [ 0.          0.          0.69999999]]\r\n```\r\n\r\n### Gradient override for `tf.square`: worked\r\n\r\n```python\r\nimport tensorflow as tf\r\n\r\n@tf.RegisterGradient(\"CustomSquare\")\r\ndef _custom_square_grad(op, grad):\r\n  return tf.constant([101.0])\r\n\r\nwith tf.Graph().as_default() as g:\r\n  c = tf.Variable([5.0], dtype=tf.float32)\r\n  s_1 = tf.square(c)  # Uses the default gradient for tf.square.\r\n  with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\r\n    s_2 = tf.square(c, name='Square')\r\n\r\n  optimizer = tf.train.GradientDescentOptimizer(0.1)#.minimize(loss)\r\n  grad_val = optimizer.compute_gradients(s_2)\r\n  train_op = optimizer.apply_gradients(grad_val)\r\n\r\nwith tf.Session(graph=g) as session:\r\n  session.run(tf.global_variables_initializer())\r\n\r\n  _, g_v = session.run([train_op, grad_val])\r\n\r\n  for g, v in g_v:\r\n      print(g)\r\n      print('*' * 80)\r\n      print(v)\r\n```\r\n\r\nThe output:\r\n```\r\n[ 101.]\r\n\r\n[-5.10000038]\r\n```\r\n\r\n### Environment info\r\nOperating System:\r\nUbuntu 16.04\r\nTensorFlow:\r\n1.0.1\r\n"}