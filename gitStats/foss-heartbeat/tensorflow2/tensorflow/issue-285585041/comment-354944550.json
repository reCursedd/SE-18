{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354944550", "html_url": "https://github.com/tensorflow/tensorflow/issues/15802#issuecomment-354944550", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15802", "id": 354944550, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDk0NDU1MA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-03T06:27:07Z", "updated_at": "2018-01-03T06:27:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is another instance of the \"constructing new graph nodes in a loop\" memory leak, where the proximate cause is not actually the <code>tf.stack()</code> operation, but rather the implicit conversion of <code>image1</code> and <code>image2</code> to <code>tf.constant()</code> nodes, which get added to the graph over time.</p>\n<p>The simplest solution is to construct the graph once and feed different values to it in each iteration:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@profile</span>\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">stack_images</span>():\n  image_file_list <span class=\"pl-k\">=</span> glob.glob(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>car_images/*.jpg<span class=\"pl-pds\">\"</span></span>)\n\n  image1 <span class=\"pl-k\">=</span> tf.placeholder(tf.string, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[])\n  image2 <span class=\"pl-k\">=</span> tf.placeholder(tf.string, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[])\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> decode image</span>\n  image1_decode <span class=\"pl-k\">=</span> tf.image.decode_image(image1, <span class=\"pl-v\">channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n  image2_decode <span class=\"pl-k\">=</span> tf.image.decode_image(image2, <span class=\"pl-v\">channels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> stack image</span>\n  image_stack <span class=\"pl-k\">=</span> tf.stack([image1_decode, image2_decode])\n\n  sess <span class=\"pl-k\">=</span> tf.Session()\n\n  <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">300</span>):\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> read image</span>\n    image1_data <span class=\"pl-k\">=</span> tf.gfile.FastGFile(image_file_list[<span class=\"pl-c1\">0</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rb<span class=\"pl-pds\">'</span></span>).read()\n    image2_data <span class=\"pl-k\">=</span> tf.gfile.FastGFile(image_file_list[<span class=\"pl-c1\">1</span>], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>rb<span class=\"pl-pds\">'</span></span>).read()\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> run session</span>\n    r_image_stack <span class=\"pl-k\">=</span> sess.run(image_stack, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{image1: image1_data, image2: image2_data})\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> mark function. so I can check the memory-usage of every loop.</span>\n    function_mark()\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> force garbage collection, so all the un-reference variable will be freed.</span>\n    <span class=\"pl-k\">del</span> r_image_stack\n    gc.collect()</pre></div>", "body_text": "This is another instance of the \"constructing new graph nodes in a loop\" memory leak, where the proximate cause is not actually the tf.stack() operation, but rather the implicit conversion of image1 and image2 to tf.constant() nodes, which get added to the graph over time.\nThe simplest solution is to construct the graph once and feed different values to it in each iteration:\n@profile\ndef stack_images():\n  image_file_list = glob.glob(\"car_images/*.jpg\")\n\n  image1 = tf.placeholder(tf.string, shape=[])\n  image2 = tf.placeholder(tf.string, shape=[])\n  # decode image\n  image1_decode = tf.image.decode_image(image1, channels=3)\n  image2_decode = tf.image.decode_image(image2, channels=3)\n  # stack image\n  image_stack = tf.stack([image1_decode, image2_decode])\n\n  sess = tf.Session()\n\n  for _ in range(300):\n    # read image\n    image1_data = tf.gfile.FastGFile(image_file_list[0], 'rb').read()\n    image2_data = tf.gfile.FastGFile(image_file_list[1], 'rb').read()\n\n    # run session\n    r_image_stack = sess.run(image_stack, feed_dict={image1: image1_data, image2: image2_data})\n\n    # mark function. so I can check the memory-usage of every loop.\n    function_mark()\n    # force garbage collection, so all the un-reference variable will be freed.\n    del r_image_stack\n    gc.collect()", "body": "This is another instance of the \"constructing new graph nodes in a loop\" memory leak, where the proximate cause is not actually the `tf.stack()` operation, but rather the implicit conversion of `image1` and `image2` to `tf.constant()` nodes, which get added to the graph over time.\r\n\r\nThe simplest solution is to construct the graph once and feed different values to it in each iteration:\r\n\r\n```python\r\n@profile\r\ndef stack_images():\r\n  image_file_list = glob.glob(\"car_images/*.jpg\")\r\n\r\n  image1 = tf.placeholder(tf.string, shape=[])\r\n  image2 = tf.placeholder(tf.string, shape=[])\r\n  # decode image\r\n  image1_decode = tf.image.decode_image(image1, channels=3)\r\n  image2_decode = tf.image.decode_image(image2, channels=3)\r\n  # stack image\r\n  image_stack = tf.stack([image1_decode, image2_decode])\r\n\r\n  sess = tf.Session()\r\n\r\n  for _ in range(300):\r\n    # read image\r\n    image1_data = tf.gfile.FastGFile(image_file_list[0], 'rb').read()\r\n    image2_data = tf.gfile.FastGFile(image_file_list[1], 'rb').read()\r\n\r\n    # run session\r\n    r_image_stack = sess.run(image_stack, feed_dict={image1: image1_data, image2: image2_data})\r\n\r\n    # mark function. so I can check the memory-usage of every loop.\r\n    function_mark()\r\n    # force garbage collection, so all the un-reference variable will be freed.\r\n    del r_image_stack\r\n    gc.collect()\r\n```"}