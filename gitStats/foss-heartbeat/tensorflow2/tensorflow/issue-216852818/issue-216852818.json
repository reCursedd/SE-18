{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8696", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8696/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8696/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8696/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8696", "id": 216852818, "node_id": "MDU6SXNzdWUyMTY4NTI4MTg=", "number": 8696, "title": "TensorFlow hangs during training while using with tf.device('/device:CPU:0'):", "user": {"login": "derekhh", "id": 1497445, "node_id": "MDQ6VXNlcjE0OTc0NDU=", "avatar_url": "https://avatars2.githubusercontent.com/u/1497445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/derekhh", "html_url": "https://github.com/derekhh", "followers_url": "https://api.github.com/users/derekhh/followers", "following_url": "https://api.github.com/users/derekhh/following{/other_user}", "gists_url": "https://api.github.com/users/derekhh/gists{/gist_id}", "starred_url": "https://api.github.com/users/derekhh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/derekhh/subscriptions", "organizations_url": "https://api.github.com/users/derekhh/orgs", "repos_url": "https://api.github.com/users/derekhh/repos", "events_url": "https://api.github.com/users/derekhh/events{/privacy}", "received_events_url": "https://api.github.com/users/derekhh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 12, "created_at": "2017-03-24T17:22:03Z", "updated_at": "2017-09-26T20:00:45Z", "closed_at": "2017-04-17T22:53:09Z", "author_association": "NONE", "body_html": "<h3>Description</h3>\n<p>I'm trying to fine-tune an Inception-V1 model and my latest implementation is based on <a href=\"https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py\">train_image_classifier.py</a>. I've noticed that sometimes TF hangs (might be similar to Issue <a href=\"https://github.com/tensorflow/tensorflow/issues/2788\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2788/hovercard\">#2788</a>) and I'm trying to figure out why.</p>\n<p>The following snippet is my <code>load_batch</code> function. I've also used this <code>session_config=tf.ConfigProto(operation_timeout_in_ms=60000)</code> to throw a <code>DeadlineExceededError</code> when things timed out.</p>\n<p>I've noticed that if I remove the <code>with tf.device('/device:CPU:0')</code> line, I can run the model for a whole night without any issue. If I add this line back I'll get a <code>DeadlineExceededError</code> fairly quickly (at around 1K steps with a batch size of 32) and I have a consistent repro.</p>\n<pre><code>def load_batch(dataset, batch_size, height, width):\n    dataset_basename = os.path.basename(dataset.data_sources)\n    with tf.device('/device:CPU:0'):\n        with tf.name_scope(name=dataset_basename):\n            data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset)\n            raw_image, label = data_provider.get(items=['image', 'label'])\n            tf.summary.image('raw_image', tf.expand_dims(input=raw_image, axis=0))\n            image = tf.cast(raw_image, tf.float32) / 255.0\n            image = tf.image.resize_images(images=image, size=[height, width], align_corners=True)\n            tf.summary.image('resized_image', tf.expand_dims(input=image, axis=0))\n            # TensorFlow recommendation:\n            # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n            # https://www.tensorflow.org/programmers_guide/reading_data\n            num_threads = 8\n            images, labels = tf.train.batch(tensors=[image, label],\n                                            batch_size=batch_size,\n                                            num_threads=num_threads,\n                                            capacity=(num_threads + 2) * batch_size,\n                                            allow_smaller_final_batch=True)\n    return images, labels\n</code></pre>\n<p>The error message is:</p>\n<pre><code>INFO:tensorflow:global step 1040: loss = 0.2107 (0.17 sec/step)\n2017-03-24 10:09:57.915250: W tensorflow/core/kernels/queue_base.cc:294] _0_magazines_train.tfrecord/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\nINFO:tensorflow:Error reported to Coordinator: &lt;class 'tensorflow.python.framework.errors_impl.DeadlineExceededError'&gt;, Timed out waiting for notification\nINFO:tensorflow:global step 1040: validation loss = 0.3864, validation accuracy = 97.66%\nINFO:tensorflow:Finished training! Saving model to disk.\nTraceback (most recent call last):\n  File \"fine-tune.py\", line 215, in &lt;module&gt;\n    run()\n  File \"fine-tune.py\", line 211, in run\n    session_config=tf.ConfigProto(operation_timeout_in_ms=60000))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 752, in train\n    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 960, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 788, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 786, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 994, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1044, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1064, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification\n</code></pre>\n<p>Since I have a consistent repro I'll be willing to provide more information to help debug this issue. Right now I'm not exactly sure what I should provide to help understand the root cause.</p>\n<p>This is the TensorBoard information from <code>/batch/fraction_of_320_full</code>:</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/1497445/24305854/aa7c4da8-107b-11e7-864d-dfe1296b9503.png\"><img width=\"643\" alt=\"image\" src=\"https://cloud.githubusercontent.com/assets/1497445/24305854/aa7c4da8-107b-11e7-864d-dfe1296b9503.png\" style=\"max-width:100%;\"></a></p>", "body_text": "Description\nI'm trying to fine-tune an Inception-V1 model and my latest implementation is based on train_image_classifier.py. I've noticed that sometimes TF hangs (might be similar to Issue #2788) and I'm trying to figure out why.\nThe following snippet is my load_batch function. I've also used this session_config=tf.ConfigProto(operation_timeout_in_ms=60000) to throw a DeadlineExceededError when things timed out.\nI've noticed that if I remove the with tf.device('/device:CPU:0') line, I can run the model for a whole night without any issue. If I add this line back I'll get a DeadlineExceededError fairly quickly (at around 1K steps with a batch size of 32) and I have a consistent repro.\ndef load_batch(dataset, batch_size, height, width):\n    dataset_basename = os.path.basename(dataset.data_sources)\n    with tf.device('/device:CPU:0'):\n        with tf.name_scope(name=dataset_basename):\n            data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset)\n            raw_image, label = data_provider.get(items=['image', 'label'])\n            tf.summary.image('raw_image', tf.expand_dims(input=raw_image, axis=0))\n            image = tf.cast(raw_image, tf.float32) / 255.0\n            image = tf.image.resize_images(images=image, size=[height, width], align_corners=True)\n            tf.summary.image('resized_image', tf.expand_dims(input=image, axis=0))\n            # TensorFlow recommendation:\n            # min_after_dequeue + (num_threads + a small safety margin) * batch_size\n            # https://www.tensorflow.org/programmers_guide/reading_data\n            num_threads = 8\n            images, labels = tf.train.batch(tensors=[image, label],\n                                            batch_size=batch_size,\n                                            num_threads=num_threads,\n                                            capacity=(num_threads + 2) * batch_size,\n                                            allow_smaller_final_batch=True)\n    return images, labels\n\nThe error message is:\nINFO:tensorflow:global step 1040: loss = 0.2107 (0.17 sec/step)\n2017-03-24 10:09:57.915250: W tensorflow/core/kernels/queue_base.cc:294] _0_magazines_train.tfrecord/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.DeadlineExceededError'>, Timed out waiting for notification\nINFO:tensorflow:global step 1040: validation loss = 0.3864, validation accuracy = 97.66%\nINFO:tensorflow:Finished training! Saving model to disk.\nTraceback (most recent call last):\n  File \"fine-tune.py\", line 215, in <module>\n    run()\n  File \"fine-tune.py\", line 211, in run\n    session_config=tf.ConfigProto(operation_timeout_in_ms=60000))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 752, in train\n    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 960, in managed_session\n    self.stop(close_summary_writer=close_summary_writer)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 788, in stop\n    stop_grace_period_secs=self._stop_grace_secs)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\n    six.reraise(*self._exc_info_to_raise)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\n    sess.run(enqueue_op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 786, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 994, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1044, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1064, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification\n\nSince I have a consistent repro I'll be willing to provide more information to help debug this issue. Right now I'm not exactly sure what I should provide to help understand the root cause.\nThis is the TensorBoard information from /batch/fraction_of_320_full:", "body": "### Description\r\n\r\nI'm trying to fine-tune an Inception-V1 model and my latest implementation is based on [train_image_classifier.py](https://github.com/tensorflow/models/blob/master/slim/train_image_classifier.py). I've noticed that sometimes TF hangs (might be similar to Issue [#2788](https://github.com/tensorflow/tensorflow/issues/2788)) and I'm trying to figure out why. \r\n\r\nThe following snippet is my `load_batch` function. I've also used this `session_config=tf.ConfigProto(operation_timeout_in_ms=60000)` to throw a `DeadlineExceededError` when things timed out.\r\n\r\nI've noticed that if I remove the `with tf.device('/device:CPU:0')` line, I can run the model for a whole night without any issue. If I add this line back I'll get a `DeadlineExceededError` fairly quickly (at around 1K steps with a batch size of 32) and I have a consistent repro.\r\n\r\n```\r\ndef load_batch(dataset, batch_size, height, width):\r\n    dataset_basename = os.path.basename(dataset.data_sources)\r\n    with tf.device('/device:CPU:0'):\r\n        with tf.name_scope(name=dataset_basename):\r\n            data_provider = slim.dataset_data_provider.DatasetDataProvider(dataset=dataset)\r\n            raw_image, label = data_provider.get(items=['image', 'label'])\r\n            tf.summary.image('raw_image', tf.expand_dims(input=raw_image, axis=0))\r\n            image = tf.cast(raw_image, tf.float32) / 255.0\r\n            image = tf.image.resize_images(images=image, size=[height, width], align_corners=True)\r\n            tf.summary.image('resized_image', tf.expand_dims(input=image, axis=0))\r\n            # TensorFlow recommendation:\r\n            # min_after_dequeue + (num_threads + a small safety margin) * batch_size\r\n            # https://www.tensorflow.org/programmers_guide/reading_data\r\n            num_threads = 8\r\n            images, labels = tf.train.batch(tensors=[image, label],\r\n                                            batch_size=batch_size,\r\n                                            num_threads=num_threads,\r\n                                            capacity=(num_threads + 2) * batch_size,\r\n                                            allow_smaller_final_batch=True)\r\n    return images, labels\r\n```\r\n\r\nThe error message is:\r\n```\r\nINFO:tensorflow:global step 1040: loss = 0.2107 (0.17 sec/step)\r\n2017-03-24 10:09:57.915250: W tensorflow/core/kernels/queue_base.cc:294] _0_magazines_train.tfrecord/parallel_read/filenames: Skipping cancelled enqueue attempt with queue not closed\r\nINFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.DeadlineExceededError'>, Timed out waiting for notification\r\nINFO:tensorflow:global step 1040: validation loss = 0.3864, validation accuracy = 97.66%\r\nINFO:tensorflow:Finished training! Saving model to disk.\r\nTraceback (most recent call last):\r\n  File \"fine-tune.py\", line 215, in <module>\r\n    run()\r\n  File \"fine-tune.py\", line 211, in run\r\n    session_config=tf.ConfigProto(operation_timeout_in_ms=60000))\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/slim/python/slim/learning.py\", line 752, in train\r\n    sv.saver.save(sess, sv.save_path, global_step=sv.global_step)\r\n  File \"/usr/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 960, in managed_session\r\n    self.stop(close_summary_writer=close_summary_writer)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/supervisor.py\", line 788, in stop\r\n    stop_grace_period_secs=self._stop_grace_secs)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/coordinator.py\", line 389, in join\r\n    six.reraise(*self._exc_info_to_raise)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/queue_runner_impl.py\", line 234, in _run\r\n    sess.run(enqueue_op)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 786, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 994, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1044, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 1064, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DeadlineExceededError: Timed out waiting for notification\r\n```\r\n\r\nSince I have a consistent repro I'll be willing to provide more information to help debug this issue. Right now I'm not exactly sure what I should provide to help understand the root cause.\r\n\r\nThis is the TensorBoard information from `/batch/fraction_of_320_full`:\r\n\r\n<img width=\"643\" alt=\"image\" src=\"https://cloud.githubusercontent.com/assets/1497445/24305854/aa7c4da8-107b-11e7-864d-dfe1296b9503.png\">\r\n"}