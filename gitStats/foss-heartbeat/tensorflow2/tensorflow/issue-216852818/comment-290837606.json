{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/290837606", "html_url": "https://github.com/tensorflow/tensorflow/issues/8696#issuecomment-290837606", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8696", "id": 290837606, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MDgzNzYwNg==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-31T21:43:21Z", "updated_at": "2017-03-31T21:43:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>One possible issue is the <code>tf.summary.image()</code> calls in your <code>load_batch()</code> function. Since (if I understand the code in <code>slim.learning.train</code> correctly) the merged summaries will be collected from a separate thread, this will cause certain images to be dequeued from the input pipeline (in order to make summaries) but never enqueued to the batch queue for training.</p>\n<p>Is your input finite or infinite? If it's finite, maybe running CPU-only means that your program completes steps more slowly (relative to the GPU version, and to the periodic summary thread) so it runs out of input data after 1000 steps? (If that's true, I'd expect the same fate to befall the GPU version, but it would complete more steps before that happens.)</p>", "body_text": "One possible issue is the tf.summary.image() calls in your load_batch() function. Since (if I understand the code in slim.learning.train correctly) the merged summaries will be collected from a separate thread, this will cause certain images to be dequeued from the input pipeline (in order to make summaries) but never enqueued to the batch queue for training.\nIs your input finite or infinite? If it's finite, maybe running CPU-only means that your program completes steps more slowly (relative to the GPU version, and to the periodic summary thread) so it runs out of input data after 1000 steps? (If that's true, I'd expect the same fate to befall the GPU version, but it would complete more steps before that happens.)", "body": "One possible issue is the `tf.summary.image()` calls in your `load_batch()` function. Since (if I understand the code in `slim.learning.train` correctly) the merged summaries will be collected from a separate thread, this will cause certain images to be dequeued from the input pipeline (in order to make summaries) but never enqueued to the batch queue for training. \r\n\r\nIs your input finite or infinite? If it's finite, maybe running CPU-only means that your program completes steps more slowly (relative to the GPU version, and to the periodic summary thread) so it runs out of input data after 1000 steps? (If that's true, I'd expect the same fate to befall the GPU version, but it would complete more steps before that happens.)"}