{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/286220582", "html_url": "https://github.com/tensorflow/tensorflow/issues/8337#issuecomment-286220582", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8337", "id": 286220582, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NjIyMDU4Mg==", "user": {"login": "al626", "id": 19927736, "node_id": "MDQ6VXNlcjE5OTI3NzM2", "avatar_url": "https://avatars3.githubusercontent.com/u/19927736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/al626", "html_url": "https://github.com/al626", "followers_url": "https://api.github.com/users/al626/followers", "following_url": "https://api.github.com/users/al626/following{/other_user}", "gists_url": "https://api.github.com/users/al626/gists{/gist_id}", "starred_url": "https://api.github.com/users/al626/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/al626/subscriptions", "organizations_url": "https://api.github.com/users/al626/orgs", "repos_url": "https://api.github.com/users/al626/repos", "events_url": "https://api.github.com/users/al626/events{/privacy}", "received_events_url": "https://api.github.com/users/al626/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-13T19:41:38Z", "updated_at": "2017-03-13T19:47:39Z", "author_association": "NONE", "body_html": "<p>I think it might be a GPU thing.</p>\n<p>The example below errors if run as <code>python tf_8337_minimal.py</code> but is fine is run as <code>CUDA_VISIBLE_DEVICES=-1 python tf_8337_minimal.py</code>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> division\n<span class=\"pl-k\">import</span> logging\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.rnn <span class=\"pl-k\">import</span> BasicLSTMCell, MultiRNNCell\n<span class=\"pl-k\">from</span> tensorflow.python <span class=\"pl-k\">import</span> debug <span class=\"pl-k\">as</span> tfdbg\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">output_layer</span>(<span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">out_size</span>, <span class=\"pl-smi\">keep_prob</span>):\n  batch_size, time_size, in_size <span class=\"pl-k\">=</span> inputs.get_shape().as_list()\n  inputs <span class=\"pl-k\">=</span> tf.reshape(inputs, [batch_size <span class=\"pl-k\">*</span> time_size, in_size])\n  weights <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>weights<span class=\"pl-pds\">'</span></span>, [in_size, out_size], tf.float32)\n  biases <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>biases<span class=\"pl-pds\">'</span></span>, [out_size], tf.float32, tf.zeros_initializer())\n  outputs <span class=\"pl-k\">=</span> tf.matmul(tf.nn.dropout(inputs, keep_prob), weights) <span class=\"pl-k\">+</span> biases\n  <span class=\"pl-k\">return</span> tf.reshape(outputs, [batch_size, time_size, out_size])\n\ntf._log <span class=\"pl-k\">=</span> tf.log\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">log</span>(<span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">name</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  <span class=\"pl-k\">return</span> tf._log(x <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1e-10</span>, name)\ntf.log <span class=\"pl-k\">=</span> log\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">Network</span>(<span class=\"pl-c1\">object</span>):\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">self</span>.sess <span class=\"pl-k\">=</span> tf.Session()\n    <span class=\"pl-c1\">self</span>.sess <span class=\"pl-k\">=</span> tfdbg.LocalCLIDebugWrapperSession(<span class=\"pl-c1\">self</span>.sess)\n    <span class=\"pl-c1\">self</span>.sess.add_tensor_filter(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>has_inf_or_nan<span class=\"pl-pds\">'</span></span>, tfdbg.has_inf_or_nan)\n    <span class=\"pl-c1\">self</span>.x <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">6</span>])\n    <span class=\"pl-c1\">self</span>.p <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, [<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>])\n    <span class=\"pl-c1\">self</span>.keep_prob <span class=\"pl-k\">=</span> tf.placeholder_with_default(tf.ones([], tf.float32), [])\n    sequence_length <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._get_sequence_length()\n    multicell <span class=\"pl-k\">=</span> MultiRNNCell([BasicLSTMCell(<span class=\"pl-c1\">128</span>) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">2</span>)])\n    output, _state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(multicell, <span class=\"pl-c1\">self</span>.x,\n                                       <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>sequence_length,\n                                       <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n                                       <span class=\"pl-v\">parallel_iterations</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n    output <span class=\"pl-k\">=</span> output_layer(output, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">self</span>.keep_prob)\n    <span class=\"pl-c1\">self</span>.p_hat <span class=\"pl-k\">=</span> tf.nn.softmax(output)\n    mask <span class=\"pl-k\">=</span> tf.sequence_mask(sequence_length, <span class=\"pl-c1\">6</span>, tf.float32)\n    num_events <span class=\"pl-k\">=</span> tf.to_float(tf.reduce_sum(sequence_length))\n    <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_sum(<span class=\"pl-k\">-</span>tf.log(tf.reduce_sum(<span class=\"pl-c1\">self</span>.p_hat <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.p, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>)) <span class=\"pl-k\">*</span> mask) <span class=\"pl-k\">/</span> num_events\n    optimiser <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n    gradients <span class=\"pl-k\">=</span> optimiser.compute_gradients(<span class=\"pl-c1\">self</span>.loss)\n    <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> optimiser.apply_gradients(gradients)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">train</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">p</span>):\n    feed_dict <span class=\"pl-k\">=</span> {<span class=\"pl-c1\">self</span>.x: x, <span class=\"pl-c1\">self</span>.p: p, <span class=\"pl-c1\">self</span>.keep_prob: <span class=\"pl-c1\">0.5</span>}\n    fetches <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">self</span>.train_op, <span class=\"pl-c1\">self</span>.loss]\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>.sess.run(fetches, feed_dict)[<span class=\"pl-c1\">1</span>:]\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_get_sequence_length</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    used <span class=\"pl-k\">=</span> tf.sign(tf.reduce_max(tf.abs(<span class=\"pl-c1\">self</span>.x), <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>))\n    <span class=\"pl-c1\">self</span>._sequence_length <span class=\"pl-k\">=</span> tf.cast(tf.reduce_sum(used, <span class=\"pl-v\">reduction_indices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>), tf.int32)\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">self</span>._sequence_length\n\n\nx <span class=\"pl-k\">=</span> np.hstack((np.random.randn(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">6</span>), np.zeros((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">4</span>, <span class=\"pl-c1\">6</span>))))\np <span class=\"pl-k\">=</span> np.zeros((<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>))\np[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\np[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n\nnetwork <span class=\"pl-k\">=</span> Network()\nnetwork.sess.run(tf.global_variables_initializer())\nresults <span class=\"pl-k\">=</span> network.train(x, p)</pre></div>\n<pre><code>Traceback (most recent call last):\n  File \"tf_8337_minimal.py\", line 64, in &lt;module&gt;\n    results = network.train(x, p)\n  File \"tf_8337_minimal.py\", line 49, in train\n    return self.sess.run(fetches, feed_dict)[1:]\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 470, in run\n    run_end_resp = self.on_run_end(run_end_req)\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 267, in on_run_end\n    self._dump_root, partition_graphs=partition_graphs)\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 502, in __init__\n    self._load_partition_graphs(partition_graphs, validate)\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 760, in _load_partition_graphs\n    self._validate_dump_with_graphs()\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 927, in _validate_dump_with_graphs\n    (node, datum.timestamp, repr(pending_inputs[node])))\nValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_2 (1489433694505012): these input(s) are not satisfied: [(u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_1', 0), (u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/NextIteration', 0)]\n</code></pre>\n<p>The final error tends to change in a non-deterministic fashion. I have also got:</p>\n<pre><code>ValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/Switch_5_grad/b_switch (1489433796485511): these input(s) are not satisfied: [(u'gradients/rnn/while/Exit_5_grad/b_exit', 0), (u'gradients/rnn/while/Switch_5_grad_1/NextIteration', 0)]\n</code></pre>\n<p>Ubuntu 16.04.2<br>\nSource: <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/12a98726e769e988f6368a029ec2f5b0ac3ccbd4/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/12a98726e769e988f6368a029ec2f5b0ac3ccbd4\"><tt>12a9872</tt></a><br>\nBazel 0.4.4<br>\nCUDA 8.0<br>\nCUDNN 5.1.5<br>\npython 2.7.12<br>\n2 GPUs: Titan X (pascal)</p>", "body_text": "I think it might be a GPU thing.\nThe example below errors if run as python tf_8337_minimal.py but is fine is run as CUDA_VISIBLE_DEVICES=-1 python tf_8337_minimal.py.\nfrom __future__ import division\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\nfrom tensorflow.python import debug as tfdbg\n\n\ndef output_layer(inputs, out_size, keep_prob):\n  batch_size, time_size, in_size = inputs.get_shape().as_list()\n  inputs = tf.reshape(inputs, [batch_size * time_size, in_size])\n  weights = tf.get_variable('weights', [in_size, out_size], tf.float32)\n  biases = tf.get_variable('biases', [out_size], tf.float32, tf.zeros_initializer())\n  outputs = tf.matmul(tf.nn.dropout(inputs, keep_prob), weights) + biases\n  return tf.reshape(outputs, [batch_size, time_size, out_size])\n\ntf._log = tf.log\ndef log(x, name=None):\n  return tf._log(x + 1e-10, name)\ntf.log = log\n\n\nclass Network(object):\n  def __init__(self):\n    self.sess = tf.Session()\n    self.sess = tfdbg.LocalCLIDebugWrapperSession(self.sess)\n    self.sess.add_tensor_filter('has_inf_or_nan', tfdbg.has_inf_or_nan)\n    self.x = tf.placeholder(tf.float32, [1, 6, 6])\n    self.p = tf.placeholder(tf.float32, [1, 6, 3])\n    self.keep_prob = tf.placeholder_with_default(tf.ones([], tf.float32), [])\n    sequence_length = self._get_sequence_length()\n    multicell = MultiRNNCell([BasicLSTMCell(128) for _ in xrange(2)])\n    output, _state = tf.nn.dynamic_rnn(multicell, self.x,\n                                       sequence_length=sequence_length,\n                                       dtype=tf.float32,\n                                       parallel_iterations=1)\n    output = output_layer(output, 3, self.keep_prob)\n    self.p_hat = tf.nn.softmax(output)\n    mask = tf.sequence_mask(sequence_length, 6, tf.float32)\n    num_events = tf.to_float(tf.reduce_sum(sequence_length))\n    self.loss = tf.reduce_sum(-tf.log(tf.reduce_sum(self.p_hat * self.p, axis=2)) * mask) / num_events\n    optimiser = tf.train.GradientDescentOptimizer(learning_rate=0)\n    gradients = optimiser.compute_gradients(self.loss)\n    self.train_op = optimiser.apply_gradients(gradients)\n\n  def train(self, x, p):\n    feed_dict = {self.x: x, self.p: p, self.keep_prob: 0.5}\n    fetches = [self.train_op, self.loss]\n    return self.sess.run(fetches, feed_dict)[1:]\n\n  def _get_sequence_length(self):\n    used = tf.sign(tf.reduce_max(tf.abs(self.x), reduction_indices=2))\n    self._sequence_length = tf.cast(tf.reduce_sum(used, reduction_indices=1), tf.int32)\n    return self._sequence_length\n\n\nx = np.hstack((np.random.randn(1, 2, 6), np.zeros((1, 4, 6))))\np = np.zeros((1, 6, 3))\np[0, 0, 2] = 1\np[0, 1, 1] = 1\n\nnetwork = Network()\nnetwork.sess.run(tf.global_variables_initializer())\nresults = network.train(x, p)\nTraceback (most recent call last):\n  File \"tf_8337_minimal.py\", line 64, in <module>\n    results = network.train(x, p)\n  File \"tf_8337_minimal.py\", line 49, in train\n    return self.sess.run(fetches, feed_dict)[1:]\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 470, in run\n    run_end_resp = self.on_run_end(run_end_req)\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 267, in on_run_end\n    self._dump_root, partition_graphs=partition_graphs)\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 502, in __init__\n    self._load_partition_graphs(partition_graphs, validate)\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 760, in _load_partition_graphs\n    self._validate_dump_with_graphs()\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 927, in _validate_dump_with_graphs\n    (node, datum.timestamp, repr(pending_inputs[node])))\nValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_2 (1489433694505012): these input(s) are not satisfied: [(u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_1', 0), (u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/NextIteration', 0)]\n\nThe final error tends to change in a non-deterministic fashion. I have also got:\nValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/Switch_5_grad/b_switch (1489433796485511): these input(s) are not satisfied: [(u'gradients/rnn/while/Exit_5_grad/b_exit', 0), (u'gradients/rnn/while/Switch_5_grad_1/NextIteration', 0)]\n\nUbuntu 16.04.2\nSource: 12a9872\nBazel 0.4.4\nCUDA 8.0\nCUDNN 5.1.5\npython 2.7.12\n2 GPUs: Titan X (pascal)", "body": "I think it might be a GPU thing.\r\n\r\nThe example below errors if run as `python tf_8337_minimal.py` but is fine is run as `CUDA_VISIBLE_DEVICES=-1 python tf_8337_minimal.py`.\r\n\r\n```python\r\nfrom __future__ import division\r\nimport logging\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn import BasicLSTMCell, MultiRNNCell\r\nfrom tensorflow.python import debug as tfdbg\r\n\r\n\r\ndef output_layer(inputs, out_size, keep_prob):\r\n  batch_size, time_size, in_size = inputs.get_shape().as_list()\r\n  inputs = tf.reshape(inputs, [batch_size * time_size, in_size])\r\n  weights = tf.get_variable('weights', [in_size, out_size], tf.float32)\r\n  biases = tf.get_variable('biases', [out_size], tf.float32, tf.zeros_initializer())\r\n  outputs = tf.matmul(tf.nn.dropout(inputs, keep_prob), weights) + biases\r\n  return tf.reshape(outputs, [batch_size, time_size, out_size])\r\n\r\ntf._log = tf.log\r\ndef log(x, name=None):\r\n  return tf._log(x + 1e-10, name)\r\ntf.log = log\r\n\r\n\r\nclass Network(object):\r\n  def __init__(self):\r\n    self.sess = tf.Session()\r\n    self.sess = tfdbg.LocalCLIDebugWrapperSession(self.sess)\r\n    self.sess.add_tensor_filter('has_inf_or_nan', tfdbg.has_inf_or_nan)\r\n    self.x = tf.placeholder(tf.float32, [1, 6, 6])\r\n    self.p = tf.placeholder(tf.float32, [1, 6, 3])\r\n    self.keep_prob = tf.placeholder_with_default(tf.ones([], tf.float32), [])\r\n    sequence_length = self._get_sequence_length()\r\n    multicell = MultiRNNCell([BasicLSTMCell(128) for _ in xrange(2)])\r\n    output, _state = tf.nn.dynamic_rnn(multicell, self.x,\r\n                                       sequence_length=sequence_length,\r\n                                       dtype=tf.float32,\r\n                                       parallel_iterations=1)\r\n    output = output_layer(output, 3, self.keep_prob)\r\n    self.p_hat = tf.nn.softmax(output)\r\n    mask = tf.sequence_mask(sequence_length, 6, tf.float32)\r\n    num_events = tf.to_float(tf.reduce_sum(sequence_length))\r\n    self.loss = tf.reduce_sum(-tf.log(tf.reduce_sum(self.p_hat * self.p, axis=2)) * mask) / num_events\r\n    optimiser = tf.train.GradientDescentOptimizer(learning_rate=0)\r\n    gradients = optimiser.compute_gradients(self.loss)\r\n    self.train_op = optimiser.apply_gradients(gradients)\r\n\r\n  def train(self, x, p):\r\n    feed_dict = {self.x: x, self.p: p, self.keep_prob: 0.5}\r\n    fetches = [self.train_op, self.loss]\r\n    return self.sess.run(fetches, feed_dict)[1:]\r\n\r\n  def _get_sequence_length(self):\r\n    used = tf.sign(tf.reduce_max(tf.abs(self.x), reduction_indices=2))\r\n    self._sequence_length = tf.cast(tf.reduce_sum(used, reduction_indices=1), tf.int32)\r\n    return self._sequence_length\r\n\r\n\r\nx = np.hstack((np.random.randn(1, 2, 6), np.zeros((1, 4, 6))))\r\np = np.zeros((1, 6, 3))\r\np[0, 0, 2] = 1\r\np[0, 1, 1] = 1\r\n\r\nnetwork = Network()\r\nnetwork.sess.run(tf.global_variables_initializer())\r\nresults = network.train(x, p)\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"tf_8337_minimal.py\", line 64, in <module>\r\n    results = network.train(x, p)\r\n  File \"tf_8337_minimal.py\", line 49, in train\r\n    return self.sess.run(fetches, feed_dict)[1:]\r\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/framework.py\", line 470, in run\r\n    run_end_resp = self.on_run_end(run_end_req)\r\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/wrappers/local_cli_wrapper.py\", line 267, in on_run_end\r\n    self._dump_root, partition_graphs=partition_graphs)\r\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 502, in __init__\r\n    self._load_partition_graphs(partition_graphs, validate)\r\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 760, in _load_partition_graphs\r\n    self._validate_dump_with_graphs()\r\n  File \"/home/arun/tensorflows/master/local/lib/python2.7/site-packages/tensorflow/python/debug/lib/debug_data.py\", line 927, in _validate_dump_with_graphs\r\n    (node, datum.timestamp, repr(pending_inputs[node])))\r\nValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_2 (1489433694505012): these input(s) are not satisfied: [(u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/b_acc_1', 0), (u'gradients/rnn/while/multi_rnn_cell/cell_0/basic_lstm_cell/basic_lstm_cell_1/MatMul/Enter_grad/NextIteration', 0)]\r\n```\r\n\r\nThe final error tends to change in a non-deterministic fashion. I have also got:\r\n```\r\nValueError: Causality violated in timing relations of debug dumps: gradients/rnn/while/Switch_5_grad/b_switch (1489433796485511): these input(s) are not satisfied: [(u'gradients/rnn/while/Exit_5_grad/b_exit', 0), (u'gradients/rnn/while/Switch_5_grad_1/NextIteration', 0)]\r\n```\r\n\r\nUbuntu 16.04.2\r\nSource: 12a9872\r\nBazel 0.4.4\r\nCUDA 8.0\r\nCUDNN 5.1.5\r\npython 2.7.12\r\n2 GPUs: Titan X (pascal)"}