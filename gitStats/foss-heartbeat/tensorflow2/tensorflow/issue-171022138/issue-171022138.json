{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3797", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3797/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3797/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3797/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3797", "id": 171022138, "node_id": "MDU6SXNzdWUxNzEwMjIxMzg=", "number": 3797, "title": "RMSProp errors with embedding_lookup", "user": {"login": "kuza55", "id": 387986, "node_id": "MDQ6VXNlcjM4Nzk4Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/387986?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kuza55", "html_url": "https://github.com/kuza55", "followers_url": "https://api.github.com/users/kuza55/followers", "following_url": "https://api.github.com/users/kuza55/following{/other_user}", "gists_url": "https://api.github.com/users/kuza55/gists{/gist_id}", "starred_url": "https://api.github.com/users/kuza55/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kuza55/subscriptions", "organizations_url": "https://api.github.com/users/kuza55/orgs", "repos_url": "https://api.github.com/users/kuza55/repos", "events_url": "https://api.github.com/users/kuza55/events{/privacy}", "received_events_url": "https://api.github.com/users/kuza55/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 19, "created_at": "2016-08-13T18:50:35Z", "updated_at": "2017-11-24T15:01:00Z", "closed_at": "2016-08-18T21:26:35Z", "author_association": "NONE", "body_html": "<p>I've been trying to train an LSTM with a word embedding and got some errors that seem related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"133918214\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1117\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1117/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1117\">#1117</a></p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\nIf installed from binary pip package, provide:</p>\n<p>Linux CPU Only<br>\n0.10.0rc0</p>\n<h3>Steps to reproduce</h3>\n<p>This is the code I've been running, which should repro the issue:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport pandas\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n#from preprocessing import VocabularyProcessor\nVocabularyProcessor = learn.preprocessing.VocabularyProcessor\n\ndef partition_length(x_train, y_train):\n  #Partition the training data by length of sequences\n  x_train_dict = {}\n  y_train_dict = {}\n  for i in range(x_train.shape[0]):\n    x = x_train[i]\n    y = y_train[i]\n\n    l = len(x)\n    if l not in x_train_dict:\n      x_train_dict[l] = []\n      y_train_dict[l] = []\n    x_train_dict[l].append(x)\n    y_train_dict[l].append(y)\n\n\n  x_train_np = {}\n  y_train_np = {}\n  for l in x_train_dict:\n    x_train_np[l] = np.asarray(x_train_dict[l])\n    y_train_np[l] = np.asarray(y_train_dict[l])\n\n  return (x_train_np, y_train_np)\n\nn_words = 0\nMAX_DOCUMENT_LENGTH = 10\nEMBEDDING_SIZE = 50\n\n# Prepare training and testing data\ndbpedia = learn.datasets.load_dataset('dbpedia')\n\nx_train = pandas.DataFrame(dbpedia.train.data)[1]\ny_train = dbpedia.train.target\nx_test = pandas.DataFrame(dbpedia.test.data)[1]\ny_test = dbpedia.test.target\n\nprint x_train.shape, y_train.shape\nprint x_test.shape, y_test.shape\n\n# Process vocabulary\nvocab_processor = VocabularyProcessor(10)\nx_train = np.array(list(vocab_processor.fit_transform(x_train)))\nx_test = np.array(list(vocab_processor.transform(x_test)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n\nprint x_train.shape, y_train.shape\n\n(x_train_part, y_train_part) = partition_length(x_train, y_train)\n\ndata = tf.placeholder(tf.int32, [None, None], name='data')\nword_vectors = learn.ops.categorical_variable(data, n_classes=n_words,\n  embedding_size=EMBEDDING_SIZE, name='words')\n\nprint('data: ', data.get_shape())\nprint('word_vectors: ', word_vectors.get_shape())\nprint('word_vectors: ', tf.shape(word_vectors))\n\ntarget = tf.placeholder(tf.int32, [None], name='target')\none_hot = tf.one_hot(target, 15, 1.0, 0.0, dtype=tf.float32)\n\nprint('target: ', target.get_shape())\nprint('one_hot: ', one_hot.get_shape())\n\nnum_hidden = 250\n\n_, state = tf.nn.dynamic_rnn(\n    tf.nn.rnn_cell.GRUCell(num_hidden),\n    word_vectors,\n    dtype=tf.float32,\n)\n\nprint('state: ', state.get_shape())\n\nin_size, out_size = (num_hidden, 15)\n\nweight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.01))\nbias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n\nprediction = tf.nn.softmax(tf.matmul(state, weight) + bias)\n#prediction = tf.contrib.losses.softmax_cross_entropy(logits, onehot_labels, weight=1.0, label_smoothing=0, scope=None)\nprediction_idx = tf.argmax(prediction, 1)\none_hot_idx = tf.argmax(one_hot, 1)\n\nprint('prediction: ', prediction.get_shape())\nprint('prediction_idx: ', prediction_idx.get_shape())\nprint('one_hot_idx: ', one_hot_idx.get_shape())\n\nmistakes = tf.not_equal(one_hot_idx, prediction_idx)\nerror =  tf.reduce_mean(tf.cast(mistakes, tf.float32))\n\ncross_entropy = -tf.reduce_sum(one_hot * tf.log(prediction))\n\nlearning_rate = 0.003\noptimizer = tf.train.RMSPropOptimizer(learning_rate)\noptimize = optimizer.minimize(cross_entropy)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\n\n\nbatch_size = 5\nfor length in x_train_part:\n  x_train = x_train_part[length]\n  y_train = y_train_part[length]\n\n  print \"Training on length \", length, \" # elements: \", x_train.shape[0]\n\n  error_pct = sess.run(error, {data: x_test, target: y_test})\n  print('Epoch {:2d} error {:3.1f}%'.format(0, 100 * error_pct))\n\n  for epoch in range(10):\n    print('Epoch {:2d}'.format(epoch+1))\n    for i in range(x_train.shape[0]/batch_size):\n      sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n\n    error_pct = sess.run(error, {data: x_test, target: y_test})\n    print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error_pct))\n\n</code></pre>\n<h3>What have you tried?</h3>\n<p>Using any non-RMSProp Optimizer works</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>This is the exception I get:</p>\n<pre><code>Traceback (most recent call last):\n  File \"lstm.py\", line 131, in &lt;module&gt;\n    sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[7664,50] [50,50]\n         [[Node: RMSProp/update_words/words_embeddings/SparseApplyRMSProp = SparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@words/words_embeddings\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](words/words_embeddings, words/words_embeddings/RMSProp, words/words_embeddings/RMSProp_1, RMSProp/learning_rate, RMSProp/decay, RMSProp/momentum, RMSProp/epsilon, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape_1)]]\nCaused by op u'RMSProp/update_words/words_embeddings/SparseApplyRMSProp', defined at:\n  File \"lstm.py\", line 111, in &lt;module&gt;\n    optimize = optimizer.minimize(cross_entropy)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 313, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 122, in _apply_sparse\n    use_locking=self._use_locking)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 664, in sparse_apply_rms_prop\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n</code></pre>", "body_text": "I've been trying to train an LSTM with a word embedding and got some errors that seem related to #1117\nEnvironment info\nOperating System:\nIf installed from binary pip package, provide:\nLinux CPU Only\n0.10.0rc0\nSteps to reproduce\nThis is the code I've been running, which should repro the issue:\nimport tensorflow as tf\nimport numpy as np\nimport pandas\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n#from preprocessing import VocabularyProcessor\nVocabularyProcessor = learn.preprocessing.VocabularyProcessor\n\ndef partition_length(x_train, y_train):\n  #Partition the training data by length of sequences\n  x_train_dict = {}\n  y_train_dict = {}\n  for i in range(x_train.shape[0]):\n    x = x_train[i]\n    y = y_train[i]\n\n    l = len(x)\n    if l not in x_train_dict:\n      x_train_dict[l] = []\n      y_train_dict[l] = []\n    x_train_dict[l].append(x)\n    y_train_dict[l].append(y)\n\n\n  x_train_np = {}\n  y_train_np = {}\n  for l in x_train_dict:\n    x_train_np[l] = np.asarray(x_train_dict[l])\n    y_train_np[l] = np.asarray(y_train_dict[l])\n\n  return (x_train_np, y_train_np)\n\nn_words = 0\nMAX_DOCUMENT_LENGTH = 10\nEMBEDDING_SIZE = 50\n\n# Prepare training and testing data\ndbpedia = learn.datasets.load_dataset('dbpedia')\n\nx_train = pandas.DataFrame(dbpedia.train.data)[1]\ny_train = dbpedia.train.target\nx_test = pandas.DataFrame(dbpedia.test.data)[1]\ny_test = dbpedia.test.target\n\nprint x_train.shape, y_train.shape\nprint x_test.shape, y_test.shape\n\n# Process vocabulary\nvocab_processor = VocabularyProcessor(10)\nx_train = np.array(list(vocab_processor.fit_transform(x_train)))\nx_test = np.array(list(vocab_processor.transform(x_test)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n\nprint x_train.shape, y_train.shape\n\n(x_train_part, y_train_part) = partition_length(x_train, y_train)\n\ndata = tf.placeholder(tf.int32, [None, None], name='data')\nword_vectors = learn.ops.categorical_variable(data, n_classes=n_words,\n  embedding_size=EMBEDDING_SIZE, name='words')\n\nprint('data: ', data.get_shape())\nprint('word_vectors: ', word_vectors.get_shape())\nprint('word_vectors: ', tf.shape(word_vectors))\n\ntarget = tf.placeholder(tf.int32, [None], name='target')\none_hot = tf.one_hot(target, 15, 1.0, 0.0, dtype=tf.float32)\n\nprint('target: ', target.get_shape())\nprint('one_hot: ', one_hot.get_shape())\n\nnum_hidden = 250\n\n_, state = tf.nn.dynamic_rnn(\n    tf.nn.rnn_cell.GRUCell(num_hidden),\n    word_vectors,\n    dtype=tf.float32,\n)\n\nprint('state: ', state.get_shape())\n\nin_size, out_size = (num_hidden, 15)\n\nweight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.01))\nbias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n\nprediction = tf.nn.softmax(tf.matmul(state, weight) + bias)\n#prediction = tf.contrib.losses.softmax_cross_entropy(logits, onehot_labels, weight=1.0, label_smoothing=0, scope=None)\nprediction_idx = tf.argmax(prediction, 1)\none_hot_idx = tf.argmax(one_hot, 1)\n\nprint('prediction: ', prediction.get_shape())\nprint('prediction_idx: ', prediction_idx.get_shape())\nprint('one_hot_idx: ', one_hot_idx.get_shape())\n\nmistakes = tf.not_equal(one_hot_idx, prediction_idx)\nerror =  tf.reduce_mean(tf.cast(mistakes, tf.float32))\n\ncross_entropy = -tf.reduce_sum(one_hot * tf.log(prediction))\n\nlearning_rate = 0.003\noptimizer = tf.train.RMSPropOptimizer(learning_rate)\noptimize = optimizer.minimize(cross_entropy)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\n\n\nbatch_size = 5\nfor length in x_train_part:\n  x_train = x_train_part[length]\n  y_train = y_train_part[length]\n\n  print \"Training on length \", length, \" # elements: \", x_train.shape[0]\n\n  error_pct = sess.run(error, {data: x_test, target: y_test})\n  print('Epoch {:2d} error {:3.1f}%'.format(0, 100 * error_pct))\n\n  for epoch in range(10):\n    print('Epoch {:2d}'.format(epoch+1))\n    for i in range(x_train.shape[0]/batch_size):\n      sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n\n    error_pct = sess.run(error, {data: x_test, target: y_test})\n    print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error_pct))\n\n\nWhat have you tried?\nUsing any non-RMSProp Optimizer works\nLogs or other output that would be helpful\nThis is the exception I get:\nTraceback (most recent call last):\n  File \"lstm.py\", line 131, in <module>\n    sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[7664,50] [50,50]\n         [[Node: RMSProp/update_words/words_embeddings/SparseApplyRMSProp = SparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@words/words_embeddings\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](words/words_embeddings, words/words_embeddings/RMSProp, words/words_embeddings/RMSProp_1, RMSProp/learning_rate, RMSProp/decay, RMSProp/momentum, RMSProp/epsilon, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape_1)]]\nCaused by op u'RMSProp/update_words/words_embeddings/SparseApplyRMSProp', defined at:\n  File \"lstm.py\", line 111, in <module>\n    optimize = optimizer.minimize(cross_entropy)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 313, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 122, in _apply_sparse\n    use_locking=self._use_locking)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 664, in sparse_apply_rms_prop\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()", "body": "I've been trying to train an LSTM with a word embedding and got some errors that seem related to https://github.com/tensorflow/tensorflow/issues/1117\n### Environment info\n\nOperating System:\nIf installed from binary pip package, provide:\n\nLinux CPU Only\n0.10.0rc0\n### Steps to reproduce\n\nThis is the code I've been running, which should repro the issue:\n\n```\nimport tensorflow as tf\nimport numpy as np\nimport pandas\nimport tensorflow as tf\nfrom tensorflow.contrib import learn\n#from preprocessing import VocabularyProcessor\nVocabularyProcessor = learn.preprocessing.VocabularyProcessor\n\ndef partition_length(x_train, y_train):\n  #Partition the training data by length of sequences\n  x_train_dict = {}\n  y_train_dict = {}\n  for i in range(x_train.shape[0]):\n    x = x_train[i]\n    y = y_train[i]\n\n    l = len(x)\n    if l not in x_train_dict:\n      x_train_dict[l] = []\n      y_train_dict[l] = []\n    x_train_dict[l].append(x)\n    y_train_dict[l].append(y)\n\n\n  x_train_np = {}\n  y_train_np = {}\n  for l in x_train_dict:\n    x_train_np[l] = np.asarray(x_train_dict[l])\n    y_train_np[l] = np.asarray(y_train_dict[l])\n\n  return (x_train_np, y_train_np)\n\nn_words = 0\nMAX_DOCUMENT_LENGTH = 10\nEMBEDDING_SIZE = 50\n\n# Prepare training and testing data\ndbpedia = learn.datasets.load_dataset('dbpedia')\n\nx_train = pandas.DataFrame(dbpedia.train.data)[1]\ny_train = dbpedia.train.target\nx_test = pandas.DataFrame(dbpedia.test.data)[1]\ny_test = dbpedia.test.target\n\nprint x_train.shape, y_train.shape\nprint x_test.shape, y_test.shape\n\n# Process vocabulary\nvocab_processor = VocabularyProcessor(10)\nx_train = np.array(list(vocab_processor.fit_transform(x_train)))\nx_test = np.array(list(vocab_processor.transform(x_test)))\nn_words = len(vocab_processor.vocabulary_)\nprint('Total words: %d' % n_words)\n\nprint x_train.shape, y_train.shape\n\n(x_train_part, y_train_part) = partition_length(x_train, y_train)\n\ndata = tf.placeholder(tf.int32, [None, None], name='data')\nword_vectors = learn.ops.categorical_variable(data, n_classes=n_words,\n  embedding_size=EMBEDDING_SIZE, name='words')\n\nprint('data: ', data.get_shape())\nprint('word_vectors: ', word_vectors.get_shape())\nprint('word_vectors: ', tf.shape(word_vectors))\n\ntarget = tf.placeholder(tf.int32, [None], name='target')\none_hot = tf.one_hot(target, 15, 1.0, 0.0, dtype=tf.float32)\n\nprint('target: ', target.get_shape())\nprint('one_hot: ', one_hot.get_shape())\n\nnum_hidden = 250\n\n_, state = tf.nn.dynamic_rnn(\n    tf.nn.rnn_cell.GRUCell(num_hidden),\n    word_vectors,\n    dtype=tf.float32,\n)\n\nprint('state: ', state.get_shape())\n\nin_size, out_size = (num_hidden, 15)\n\nweight = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.01))\nbias = tf.Variable(tf.constant(0.1, shape=[out_size]))\n\nprediction = tf.nn.softmax(tf.matmul(state, weight) + bias)\n#prediction = tf.contrib.losses.softmax_cross_entropy(logits, onehot_labels, weight=1.0, label_smoothing=0, scope=None)\nprediction_idx = tf.argmax(prediction, 1)\none_hot_idx = tf.argmax(one_hot, 1)\n\nprint('prediction: ', prediction.get_shape())\nprint('prediction_idx: ', prediction_idx.get_shape())\nprint('one_hot_idx: ', one_hot_idx.get_shape())\n\nmistakes = tf.not_equal(one_hot_idx, prediction_idx)\nerror =  tf.reduce_mean(tf.cast(mistakes, tf.float32))\n\ncross_entropy = -tf.reduce_sum(one_hot * tf.log(prediction))\n\nlearning_rate = 0.003\noptimizer = tf.train.RMSPropOptimizer(learning_rate)\noptimize = optimizer.minimize(cross_entropy)\n\nsess = tf.Session()\nsess.run(tf.initialize_all_variables())\n\n\n\nbatch_size = 5\nfor length in x_train_part:\n  x_train = x_train_part[length]\n  y_train = y_train_part[length]\n\n  print \"Training on length \", length, \" # elements: \", x_train.shape[0]\n\n  error_pct = sess.run(error, {data: x_test, target: y_test})\n  print('Epoch {:2d} error {:3.1f}%'.format(0, 100 * error_pct))\n\n  for epoch in range(10):\n    print('Epoch {:2d}'.format(epoch+1))\n    for i in range(x_train.shape[0]/batch_size):\n      sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n\n    error_pct = sess.run(error, {data: x_test, target: y_test})\n    print('Epoch {:2d} error {:3.1f}%'.format(epoch + 1, 100 * error_pct))\n\n```\n### What have you tried?\n\nUsing any non-RMSProp Optimizer works\n### Logs or other output that would be helpful\n\nThis is the exception I get:\n\n```\nTraceback (most recent call last):\n  File \"lstm.py\", line 131, in <module>\n    sess.run(optimize, {data: x_train[i:i+batch_size], target: y_train[i:i+batch_size]})\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 710, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 908, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 958, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py\", line 978, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors.InvalidArgumentError: var and grad do not have the same shape[7664,50] [50,50]\n         [[Node: RMSProp/update_words/words_embeddings/SparseApplyRMSProp = SparseApplyRMSProp[T=DT_FLOAT, Tindices=DT_INT32, _class=[\"loc:@words/words_embeddings\"], use_locking=false, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](words/words_embeddings, words/words_embeddings/RMSProp, words/words_embeddings/RMSProp_1, RMSProp/learning_rate, RMSProp/decay, RMSProp/momentum, RMSProp/epsilon, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape, gradients/words/embedding_lookup/embedding_lookup_grad/Reshape_1)]]\nCaused by op u'RMSProp/update_words/words_embeddings/SparseApplyRMSProp', defined at:\n  File \"lstm.py\", line 111, in <module>\n    optimize = optimizer.minimize(cross_entropy)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 198, in minimize\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/optimizer.py\", line 313, in apply_gradients\n    update_ops.append(self._apply_sparse(grad, var))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/rmsprop.py\", line 122, in _apply_sparse\n    use_locking=self._use_locking)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/gen_training_ops.py\", line 664, in sparse_apply_rms_prop\n    use_locking=use_locking, name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/op_def_library.py\", line 703, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 2317, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1239, in __init__\n    self._traceback = _extract_stack()\n```\n"}