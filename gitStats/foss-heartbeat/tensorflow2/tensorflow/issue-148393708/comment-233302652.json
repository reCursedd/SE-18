{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/233302652", "html_url": "https://github.com/tensorflow/tensorflow/issues/1947#issuecomment-233302652", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1947", "id": 233302652, "node_id": "MDEyOklzc3VlQ29tbWVudDIzMzMwMjY1Mg==", "user": {"login": "dipanjan06", "id": 19681682, "node_id": "MDQ6VXNlcjE5NjgxNjgy", "avatar_url": "https://avatars0.githubusercontent.com/u/19681682?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dipanjan06", "html_url": "https://github.com/dipanjan06", "followers_url": "https://api.github.com/users/dipanjan06/followers", "following_url": "https://api.github.com/users/dipanjan06/following{/other_user}", "gists_url": "https://api.github.com/users/dipanjan06/gists{/gist_id}", "starred_url": "https://api.github.com/users/dipanjan06/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dipanjan06/subscriptions", "organizations_url": "https://api.github.com/users/dipanjan06/orgs", "repos_url": "https://api.github.com/users/dipanjan06/repos", "events_url": "https://api.github.com/users/dipanjan06/events{/privacy}", "received_events_url": "https://api.github.com/users/dipanjan06/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-18T11:17:35Z", "updated_at": "2016-07-18T11:17:35Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>As advised I have taken multiple snap shots of the call stack when the<br>\nactual process hangs after printing the following</p>\n<hr>\n<p><em>I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful<br>\nNUMA node read from SysFS had negative value (-1), but there must be at<br>\nleast one NUMA node, so returning NUMA node zero</em></p>\n<p>*I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with<br>\nproperties: *</p>\n<p><em>name: GRID K520</em></p>\n<p><em>major: 3 minor: 0 memoryClockRate (GHz) 0.797</em></p>\n<p><em>pciBusID 0000:00:03.0</em></p>\n<p><em>Total memory: 4.00GiB</em></p>\n<p><em>Free memory: 3.95GiB</em></p>\n<p>*I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 *</p>\n<p>*I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y *</p>\n<p><em>I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating<br>\nTensorFlow device (/gpu:0) -&gt; (device: 0, name: GRID K520, pci bus id:<br>\n0000:00:03.0)</em></p>\n<hr>\n<p>StackTrace<br>\n<a href=\"http://pastebin.com/D5j4uKra\" rel=\"nofollow\">http://pastebin.com/D5j4uKra</a></p>\n<p>On Mon, Jul 18, 2016 at 12:48 AM, zheng-xq <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5978929\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ChrisHowlin\">@ChrisHowlin</a> <a href=\"https://github.com/ChrisHowlin\">https://github.com/ChrisHowlin</a>, one thing I found in your<br>\nlog is \"cuMemAlloc_v2\", I wonder whether it consistently show up in the<br>\nhanging? Could you repeat the process and take multiple snapshots of the<br>\nhanging callstacks?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19681682\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/dipanjan06\">@dipanjan06</a> <a href=\"https://github.com/dipanjan06\">https://github.com/dipanjan06</a>, thank you for the stack<br>\ntrace. However, it is not complete. The last line is \"---Type to continue,<br>\nor q to quit---\". Please press to get all the stack printout. Also it is<br>\nworth repeating the process and taking multiple snapshots of the hanging<br>\ncallstacks, so we can find what are common among them, which are likely to<br>\nbe the culprit.</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148393708\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1947\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1947/hovercard?comment_id=233198674&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/1947#issuecomment-233198674\">#1947 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ASxRkgzFmnIDaciw-cdFIuoiSVkbDAkYks5qWn_9gaJpZM4IHeU8\">https://github.com/notifications/unsubscribe-auth/ASxRkgzFmnIDaciw-cdFIuoiSVkbDAkYks5qWn_9gaJpZM4IHeU8</a><br>\n.</p>\n</blockquote>", "body_text": "Hello,\nAs advised I have taken multiple snap shots of the call stack when the\nactual process hangs after printing the following\n\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful\nNUMA node read from SysFS had negative value (-1), but there must be at\nleast one NUMA node, so returning NUMA node zero\n*I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with\nproperties: *\nname: GRID K520\nmajor: 3 minor: 0 memoryClockRate (GHz) 0.797\npciBusID 0000:00:03.0\nTotal memory: 4.00GiB\nFree memory: 3.95GiB\n*I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 *\n*I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y *\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating\nTensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id:\n0000:00:03.0)\n\nStackTrace\nhttp://pastebin.com/D5j4uKra\nOn Mon, Jul 18, 2016 at 12:48 AM, zheng-xq notifications@github.com wrote:\n\n@ChrisHowlin https://github.com/ChrisHowlin, one thing I found in your\nlog is \"cuMemAlloc_v2\", I wonder whether it consistently show up in the\nhanging? Could you repeat the process and take multiple snapshots of the\nhanging callstacks?\n@dipanjan06 https://github.com/dipanjan06, thank you for the stack\ntrace. However, it is not complete. The last line is \"---Type to continue,\nor q to quit---\". Please press to get all the stack printout. Also it is\nworth repeating the process and taking multiple snapshots of the hanging\ncallstacks, so we can find what are common among them, which are likely to\nbe the culprit.\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#1947 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ASxRkgzFmnIDaciw-cdFIuoiSVkbDAkYks5qWn_9gaJpZM4IHeU8\n.", "body": "Hello,\n\nAs advised I have taken multiple snap shots of the call stack when the\nactual process hangs after printing the following\n\n______________________________________\n\n_I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:900] successful\nNUMA node read from SysFS had negative value (-1), but there must be at\nleast one NUMA node, so returning NUMA node zero_\n\n*I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with\nproperties: *\n\n_name: GRID K520_\n\n_major: 3 minor: 0 memoryClockRate (GHz) 0.797_\n\n_pciBusID 0000:00:03.0_\n\n_Total memory: 4.00GiB_\n\n_Free memory: 3.95GiB_\n\n*I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 *\n\n*I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y *\n\n_I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating\nTensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id:\n0000:00:03.0)_\n\n___________________________________________________\n\nStackTrace\nhttp://pastebin.com/D5j4uKra\n\nOn Mon, Jul 18, 2016 at 12:48 AM, zheng-xq notifications@github.com wrote:\n\n> @ChrisHowlin https://github.com/ChrisHowlin, one thing I found in your\n> log is \"cuMemAlloc_v2\", I wonder whether it consistently show up in the\n> hanging? Could you repeat the process and take multiple snapshots of the\n> hanging callstacks?\n> \n> @dipanjan06 https://github.com/dipanjan06, thank you for the stack\n> trace. However, it is not complete. The last line is \"---Type to continue,\n> or q to quit---\". Please press to get all the stack printout. Also it is\n> worth repeating the process and taking multiple snapshots of the hanging\n> callstacks, so we can find what are common among them, which are likely to\n> be the culprit.\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/1947#issuecomment-233198674,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ASxRkgzFmnIDaciw-cdFIuoiSVkbDAkYks5qWn_9gaJpZM4IHeU8\n> .\n"}