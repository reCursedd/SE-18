{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18813", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18813/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18813/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18813/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18813", "id": 317012871, "node_id": "MDU6SXNzdWUzMTcwMTI4NzE=", "number": 18813, "title": "Support in the Dataset API for sharding dataset used in stateful LSTMs", "user": {"login": "terrykong", "id": 7576060, "node_id": "MDQ6VXNlcjc1NzYwNjA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7576060?v=4", "gravatar_id": "", "url": "https://api.github.com/users/terrykong", "html_url": "https://github.com/terrykong", "followers_url": "https://api.github.com/users/terrykong/followers", "following_url": "https://api.github.com/users/terrykong/following{/other_user}", "gists_url": "https://api.github.com/users/terrykong/gists{/gist_id}", "starred_url": "https://api.github.com/users/terrykong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/terrykong/subscriptions", "organizations_url": "https://api.github.com/users/terrykong/orgs", "repos_url": "https://api.github.com/users/terrykong/repos", "events_url": "https://api.github.com/users/terrykong/events{/privacy}", "received_events_url": "https://api.github.com/users/terrykong/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2018-04-23T23:02:42Z", "updated_at": "2018-05-21T15:30:42Z", "closed_at": null, "author_association": "NONE", "body_html": "<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):<br>\nyes</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):<br>\nubuntu 16.04</li>\n<li>TensorFlow installed from (source or binary):<br>\nusing docker from google container registry</li>\n<li>TensorFlow version (use command below):<br>\n1.7.0</li>\n<li>Python version:<br>\n3.5</li>\n<li>Bazel version (if compiling from source):<br>\nn/a</li>\n<li>GCC/Compiler version (if compiling from source):<br>\nn/a</li>\n<li>CUDA/cuDNN version:<br>\nn/a</li>\n<li>GPU model and memory:<br>\nn/a</li>\n<li>Exact command to reproduce:<br>\nn/a</li>\n</ul>\n<p>Hi,</p>\n<p>I was wondering if there will ever be support for sharding a dataset where the order matters? As an example, consider the ptb_word_lm.py script. In the training of the LSTM, batches must be kept in order since the hidden/cell state is never reset (until the end of the epoch). If we use tf.data, we wouldn't be able to use Dataset.shard(...) on the examples themselves because we would get batches like this (assuming 4 workers in a distributed training and batch_size=3):</p>\n<p>worker0_batch0 = [example_0, example_4, example_8]<br>\nworker0_batch1 = [example_12, example_16, example_20]<br>\n...</p>\n<p>worker1_batch0 = [example_1, example_5, example_9]<br>\nworker1_batch1 = [example_13, example_17, example_21]<br>\n...</p>\n<p>worker2_batch0 = [example_2, example_6, example_10]<br>\nworker2_batch1 = [example_14, example_18, example_22]<br>\n...</p>\n<p>worker3_batch0 = [example_3, example_7, example_11]<br>\nworker3_batch1 = [example_15, example_19, example_23]<br>\n...</p>\n<p><strong>Assuming the size of the dataset is N, then what we really want is</strong></p>\n<p>worker0_batch0 = [example_0, example_1, example_2]<br>\nworker0_batch1 = [example_3, example_4, example_5]<br>\n...</p>\n<p>worker1_batch0 = [example_(N/4)+0, example_(N/4)+1, example_(N/4)+2]<br>\nworker1_batch1 = [example_(N/4)+3, example_(N/4)+4, example_(N/4)+5]<br>\n...</p>\n<p>worker2_batch0 = [example_2*(N/4)+0, example_2*(N/4)+1, example_2*(N/4)+2]<br>\nworker2_batch1 = [example_2*(N/4)+3, example_2*(N/4)+4, example_2*(N/4)+5]<br>\n...</p>\n<p>worker3_batch0 = [example_3*(N/4)+0, example_3*(N/4)+1, example_3*(N/4)+2]<br>\nworker3_batch1 =[example_3*(N/4)+3, example_3*(N/4)+4, example_3*(N/4)+5]<br>\n...</p>\n<p>Even sharding the batches instead of the examples would lead to a similar picture.</p>\n<p>So I guess if I had a list of TFRecord filenames, I could split the filenames, but I have a concern in the context of synchronous training. Suppose I have 10 tfrecord files (each with 10k examples) and 4 workers. The first two workers would get 3 files and the other two would get 2 files. If my understanding of SyncReplicasOptimizer is correct, I would either have to toss each of the 10k examples on the first two workers, or create a barrier for the second two workers to not proceed until the first two are done. Is there another solution here aside from equally splitting up my training data among files?</p>\n<p>I can think of a couple of workarounds to this problem, but I'd like to know if this problem has been, or will be, solved?</p>", "body_text": "Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nubuntu 16.04\nTensorFlow installed from (source or binary):\nusing docker from google container registry\nTensorFlow version (use command below):\n1.7.0\nPython version:\n3.5\nBazel version (if compiling from source):\nn/a\nGCC/Compiler version (if compiling from source):\nn/a\nCUDA/cuDNN version:\nn/a\nGPU model and memory:\nn/a\nExact command to reproduce:\nn/a\n\nHi,\nI was wondering if there will ever be support for sharding a dataset where the order matters? As an example, consider the ptb_word_lm.py script. In the training of the LSTM, batches must be kept in order since the hidden/cell state is never reset (until the end of the epoch). If we use tf.data, we wouldn't be able to use Dataset.shard(...) on the examples themselves because we would get batches like this (assuming 4 workers in a distributed training and batch_size=3):\nworker0_batch0 = [example_0, example_4, example_8]\nworker0_batch1 = [example_12, example_16, example_20]\n...\nworker1_batch0 = [example_1, example_5, example_9]\nworker1_batch1 = [example_13, example_17, example_21]\n...\nworker2_batch0 = [example_2, example_6, example_10]\nworker2_batch1 = [example_14, example_18, example_22]\n...\nworker3_batch0 = [example_3, example_7, example_11]\nworker3_batch1 = [example_15, example_19, example_23]\n...\nAssuming the size of the dataset is N, then what we really want is\nworker0_batch0 = [example_0, example_1, example_2]\nworker0_batch1 = [example_3, example_4, example_5]\n...\nworker1_batch0 = [example_(N/4)+0, example_(N/4)+1, example_(N/4)+2]\nworker1_batch1 = [example_(N/4)+3, example_(N/4)+4, example_(N/4)+5]\n...\nworker2_batch0 = [example_2*(N/4)+0, example_2*(N/4)+1, example_2*(N/4)+2]\nworker2_batch1 = [example_2*(N/4)+3, example_2*(N/4)+4, example_2*(N/4)+5]\n...\nworker3_batch0 = [example_3*(N/4)+0, example_3*(N/4)+1, example_3*(N/4)+2]\nworker3_batch1 =[example_3*(N/4)+3, example_3*(N/4)+4, example_3*(N/4)+5]\n...\nEven sharding the batches instead of the examples would lead to a similar picture.\nSo I guess if I had a list of TFRecord filenames, I could split the filenames, but I have a concern in the context of synchronous training. Suppose I have 10 tfrecord files (each with 10k examples) and 4 workers. The first two workers would get 3 files and the other two would get 2 files. If my understanding of SyncReplicasOptimizer is correct, I would either have to toss each of the 10k examples on the first two workers, or create a barrier for the second two workers to not proceed until the first two are done. Is there another solution here aside from equally splitting up my training data among files?\nI can think of a couple of workarounds to this problem, but I'd like to know if this problem has been, or will be, solved?", "body": "- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\nyes\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nubuntu 16.04\r\n- TensorFlow installed from (source or binary):\r\nusing docker from google container registry\r\n- TensorFlow version (use command below):\r\n1.7.0\r\n- Python version:\r\n3.5\r\n- Bazel version (if compiling from source):\r\nn/a\r\n- GCC/Compiler version (if compiling from source):\r\nn/a\r\n- CUDA/cuDNN version:\r\nn/a\r\n- GPU model and memory:\r\nn/a\r\n- Exact command to reproduce:\r\nn/a\r\n\r\nHi,\r\n\r\nI was wondering if there will ever be support for sharding a dataset where the order matters? As an example, consider the ptb_word_lm.py script. In the training of the LSTM, batches must be kept in order since the hidden/cell state is never reset (until the end of the epoch). If we use tf.data, we wouldn't be able to use Dataset.shard(...) on the examples themselves because we would get batches like this (assuming 4 workers in a distributed training and batch_size=3):\r\n\r\nworker0_batch0 = [example_0, example_4, example_8]\r\nworker0_batch1 = [example_12, example_16, example_20]\r\n...\r\n\r\nworker1_batch0 = [example_1, example_5, example_9]\r\nworker1_batch1 = [example_13, example_17, example_21]\r\n...\r\n\r\nworker2_batch0 = [example_2, example_6, example_10]\r\nworker2_batch1 = [example_14, example_18, example_22]\r\n...\r\n\r\nworker3_batch0 = [example_3, example_7, example_11]\r\nworker3_batch1 = [example_15, example_19, example_23]\r\n...\r\n\r\n**Assuming the size of the dataset is N, then what we really want is** \r\n\r\nworker0_batch0 = [example_0, example_1, example_2]\r\nworker0_batch1 = [example_3, example_4, example_5]\r\n...\r\n\r\nworker1_batch0 = [example_(N/4)+0, example_(N/4)+1, example_(N/4)+2]\r\nworker1_batch1 = [example_(N/4)+3, example_(N/4)+4, example_(N/4)+5]\r\n...\r\n\r\nworker2_batch0 = [example_2*(N/4)+0, example_2*(N/4)+1, example_2*(N/4)+2]\r\nworker2_batch1 = [example_2*(N/4)+3, example_2*(N/4)+4, example_2*(N/4)+5]\r\n...\r\n\r\nworker3_batch0 = [example_3*(N/4)+0, example_3*(N/4)+1, example_3*(N/4)+2]\r\nworker3_batch1 =[example_3*(N/4)+3, example_3*(N/4)+4, example_3*(N/4)+5]\r\n...\r\n\r\nEven sharding the batches instead of the examples would lead to a similar picture.\r\n\r\nSo I guess if I had a list of TFRecord filenames, I could split the filenames, but I have a concern in the context of synchronous training. Suppose I have 10 tfrecord files (each with 10k examples) and 4 workers. The first two workers would get 3 files and the other two would get 2 files. If my understanding of SyncReplicasOptimizer is correct, I would either have to toss each of the 10k examples on the first two workers, or create a barrier for the second two workers to not proceed until the first two are done. Is there another solution here aside from equally splitting up my training data among files?\r\n\r\nI can think of a couple of workarounds to this problem, but I'd like to know if this problem has been, or will be, solved?"}