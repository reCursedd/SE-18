{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/277623495", "html_url": "https://github.com/tensorflow/tensorflow/issues/7226#issuecomment-277623495", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7226", "id": 277623495, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NzYyMzQ5NQ==", "user": {"login": "kshmelkov", "id": 10819534, "node_id": "MDQ6VXNlcjEwODE5NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/10819534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kshmelkov", "html_url": "https://github.com/kshmelkov", "followers_url": "https://api.github.com/users/kshmelkov/followers", "following_url": "https://api.github.com/users/kshmelkov/following{/other_user}", "gists_url": "https://api.github.com/users/kshmelkov/gists{/gist_id}", "starred_url": "https://api.github.com/users/kshmelkov/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kshmelkov/subscriptions", "organizations_url": "https://api.github.com/users/kshmelkov/orgs", "repos_url": "https://api.github.com/users/kshmelkov/repos", "events_url": "https://api.github.com/users/kshmelkov/events{/privacy}", "received_events_url": "https://api.github.com/users/kshmelkov/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-06T09:12:16Z", "updated_at": "2017-02-06T09:12:16Z", "author_association": "NONE", "body_html": "<p>Are you kidding me or what? How can it not have enough capacity when we start we just one convolution?</p>\n<p>Fine, let's modify an example. Choose conv layer with zero weights</p>\n<pre><code>net = slim.conv2d(inpt, 16, [3, 3], scope='conv', weights_initializer=tf.zeros_initializer())\n</code></pre>\n<p>and as well zero batch:</p>\n<pre><code>        val = np.zeros(shape)\n</code></pre>\n<p>It still fails. Do you imply that float16 does not have enough capacity to backpropagate on zero batch through all zero convolution? It is clearly a bug somewhere in native code. Please reopen this issue, it can't be intended behaviour.</p>", "body_text": "Are you kidding me or what? How can it not have enough capacity when we start we just one convolution?\nFine, let's modify an example. Choose conv layer with zero weights\nnet = slim.conv2d(inpt, 16, [3, 3], scope='conv', weights_initializer=tf.zeros_initializer())\n\nand as well zero batch:\n        val = np.zeros(shape)\n\nIt still fails. Do you imply that float16 does not have enough capacity to backpropagate on zero batch through all zero convolution? It is clearly a bug somewhere in native code. Please reopen this issue, it can't be intended behaviour.", "body": "Are you kidding me or what? How can it not have enough capacity when we start we just one convolution?\r\n\r\nFine, let's modify an example. Choose conv layer with zero weights\r\n```\r\nnet = slim.conv2d(inpt, 16, [3, 3], scope='conv', weights_initializer=tf.zeros_initializer())\r\n```\r\nand as well zero batch:\r\n```\r\n        val = np.zeros(shape)\r\n```\r\nIt still fails. Do you imply that float16 does not have enough capacity to backpropagate on zero batch through all zero convolution? It is clearly a bug somewhere in native code. Please reopen this issue, it can't be intended behaviour."}