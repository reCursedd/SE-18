{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23451", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23451/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23451/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23451/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23451", "id": 376713121, "node_id": "MDU6SXNzdWUzNzY3MTMxMjE=", "number": 23451, "title": "Current implementation does not yet support strides in the batch and depth dimensions.", "user": {"login": "xav12358", "id": 12400238, "node_id": "MDQ6VXNlcjEyNDAwMjM4", "avatar_url": "https://avatars2.githubusercontent.com/u/12400238?v=4", "gravatar_id": "", "url": "https://api.github.com/users/xav12358", "html_url": "https://github.com/xav12358", "followers_url": "https://api.github.com/users/xav12358/followers", "following_url": "https://api.github.com/users/xav12358/following{/other_user}", "gists_url": "https://api.github.com/users/xav12358/gists{/gist_id}", "starred_url": "https://api.github.com/users/xav12358/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/xav12358/subscriptions", "organizations_url": "https://api.github.com/users/xav12358/orgs", "repos_url": "https://api.github.com/users/xav12358/repos", "events_url": "https://api.github.com/users/xav12358/events{/privacy}", "received_events_url": "https://api.github.com/users/xav12358/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547147, "node_id": "MDU6TGFiZWwxMDk3NTQ3MTQ3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:ops", "name": "comp:ops", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "harshini-gadige", "id": 42781361, "node_id": "MDQ6VXNlcjQyNzgxMzYx", "avatar_url": "https://avatars1.githubusercontent.com/u/42781361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harshini-gadige", "html_url": "https://github.com/harshini-gadige", "followers_url": "https://api.github.com/users/harshini-gadige/followers", "following_url": "https://api.github.com/users/harshini-gadige/following{/other_user}", "gists_url": "https://api.github.com/users/harshini-gadige/gists{/gist_id}", "starred_url": "https://api.github.com/users/harshini-gadige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harshini-gadige/subscriptions", "organizations_url": "https://api.github.com/users/harshini-gadige/orgs", "repos_url": "https://api.github.com/users/harshini-gadige/repos", "events_url": "https://api.github.com/users/harshini-gadige/events{/privacy}", "received_events_url": "https://api.github.com/users/harshini-gadige/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-11-02T08:30:14Z", "updated_at": "2018-11-20T18:29:47Z", "closed_at": "2018-11-20T18:29:46Z", "author_association": "NONE", "body_html": "<p>Hello,</p>\n<p>I try to design autoencoder in tensorflow that was previously made with tensorflow contrib tf.contrib.layer.<br>\nHere is the code :</p>\n<pre><code># An undercomplete autoencoder on MNIST dataset\nfrom __future__ import division, print_function, absolute_import\nimport tensorflow.contrib.layers as lays\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import transform\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nbatch_size = 500  # Number of samples in each batch\nepoch_num = 5     # Number of epochs to train the network\nlr = 0.001        # Learning rate\n\ndef resize_batch(imgs):\n    # A function to resize a batch of MNIST images to (32, 32)\n    # Args:\n    #   imgs: a numpy array of size [batch_size, 28 X 28].\n    # Returns:\n    #   a numpy array of size [batch_size, 32, 32].\n    imgs = imgs.reshape((-1, 28, 28, 1))\n    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\n    for i in range(imgs.shape[0]):\n        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\n    return resized_imgs\n\n\ndef autoencoder(inputs):\n    # encoder\n    # 32 x 32 x 1   -&gt;  16 x 16 x 32\n    # 16 x 16 x 32  -&gt;  8 x 8 x 16\n    # 8 x 8 x 16    -&gt;  2 x 2 x 8\n    # print('inputs {0}'.format(inputs))\n\n    # net = lays.conv2d(inputs, 32, [5, 5], stride=2, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d(net, 16, [5, 5], stride=2, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d(net, 8, [5, 5], stride=4, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # # decoder\n    # # 2 x 2 x 8    -&gt;  8 x 8 x 16\n    # # 8 x 8 x 16   -&gt;  16 x 16 x 32\n    # # 16 x 16 x 32  -&gt;  32 x 32 x 1\n    # net = lays.conv2d_transpose(net, 16, [5, 5], stride=4, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d_transpose(net, 32, [5, 5], stride=2, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d_transpose(net, 1, [5, 5], stride=2, padding='SAME', activation_fn=tf.nn.tanh)\n\n    # print('net {0}'.format(net))\n    # return net\n\n    xi = tf.nn.conv2d(ae_inputs,\n                 filter=tf.Variable(tf.random_normal([5,5,1,32])),\n                 strides=[1,2,2,1],\n                 padding='SAME')\n    xi = tf.nn.relu(xi)\n    print(\"xi {0}\".format(xi))\n\n    xi = tf.nn.conv2d(xi,\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\n                     strides=[1,2,2,32],\n                     padding='SAME')\n    xi = tf.nn.relu(xi)\n    print(\"xi {0}\".format(xi))\n\n    xi = tf.nn.conv2d(xi,\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\n                     strides=[1,4,4,16],\n                     padding='SAME')\n    xi = tf.nn.relu(xi)\n\n    print(\"xi {0}\".format(xi))\n\n\n\n\n\n\n    xo = tf.nn.conv2d_transpose(xi,\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\n                     output_shape=[tf.shape(xi)[0], 8, 8, 16],\n                     strides=[1,4,4,1],\n                     padding='SAME')\n    xo = tf.nn.relu(xo)\n\n    print(\"xo {0}\".format(xo))\n\n    xo = tf.nn.conv2d_transpose(xo,\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\n                     output_shape=[tf.shape(xo)[0], 16, 16, 32],\n                     strides=[1,2,2,1],\n                     padding='SAME')\n    xo = tf.nn.relu(xo)\n\n    print(\"xo {0}\".format(xo))\n\n    xo = tf.nn.conv2d_transpose(xo,\n                     filter=tf.Variable(tf.random_normal([5,5,1,32])),\n                     output_shape=[tf.shape(xo)[0], 32, 32, 1],\n                     strides=[1,2,2,1],\n                     padding='SAME')\n    xo = tf.nn.tanh(xo)\n\n\n    print(\"xo {0}\".format(xo))\n    return xo\n# read MNIST dataset\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n\n# calculate the number of batches per epoch\nbatch_per_ep = mnist.train.num_examples // batch_size\n\nae_inputs = tf.placeholder(tf.float32, (None, 32, 32, 1))  # input to the network (MNIST images)\nae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\n\n# calculate the loss and optimize the network\nloss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss\nwith tf.name_scope('train'):\n    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n\n\n\n# initialize the network\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nstddev = tf.sqrt(tf.reduce_mean(tf.square(ae_inputs - ae_outputs)))\n\nwith tf.name_scope('stddev'):\n        tf.summary.scalar('accuracy', stddev)\nwith tf.Session() as sess:\n    sess.run(init)\n    train_writer = tf.summary.FileWriter('/tmp/autoencoder/', sess.graph)\n    index = 0\n    for ep in range(epoch_num):  # epochs loop\n        for batch_n in range(batch_per_ep):  # batches loop\n            batch_img, batch_label = mnist.train.next_batch(batch_size)  # read a batch\n            batch_img = batch_img.reshape((-1, 28, 28, 1))               # reshape each sample to an (28, 28) image\n            batch_img = resize_batch(batch_img)                          # reshape the images to (32, 32)\n            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n            run_metadata = tf.RunMetadata()\n            accuracy, c = sess.run([train_op, loss], feed_dict={ae_inputs: batch_img},\n                              options=run_options,\n                              run_metadata=run_metadata)\n            print('Epoch: {} - cost= {:.5f}'.format((ep + 1), c))\n            if batch_n % 10 == 0:\n\n              summ=tf.Summary()\n              summ.value.add(tag='Cost', simple_value = c)\n              summ.value.add(tag='Accuracy', simple_value = accuracy)\n              #train_writer.add_run_metadata(run_metadata, 'step%03d' % batch_n)\n              train_writer.add_summary(summ,index)\n              index+=1\n              train_writer.flush()\n              save_path = saver.save(sess, \"/tmp/autoencoder/model.ckpt\")\n\n              #file_writer = tf.summary.FileWriter(logdir='/tmp/autoencoder/')\n              print(\"Model saved in path: %s\" % save_path)\n\n    train_writer.close()\n\n    # test the trained network\n    batch_img, batch_label = mnist.test.next_batch(50)\n    batch_img = resize_batch(batch_img)\n    recon_img = sess.run([ae_outputs], feed_dict={ae_inputs: batch_img})[0]\n\n    # plot the reconstructed images and their ground truths (inputs)\n    plt.figure(1)\n    plt.title('Reconstructed Images')\n    for i in range(50):\n        plt.subplot(5, 10, i+1)\n        plt.imshow(recon_img[i, ..., 0], cmap='gray')\n    plt.figure(2)\n    plt.title('Input Images')\n    for i in range(50):\n        plt.subplot(5, 10, i+1)\n        plt.imshow(batch_img[i, ..., 0], cmap='gray')\n    plt.show()\n\n\n</code></pre>\n<p>Where I run the code I get that error:</p>\n<pre><code>`WARNING:tensorflow:From ./simple_autoencoder.py:115: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nxi Tensor(\"Relu:0\", shape=(?, 16, 16, 32), dtype=float32)\nxi Tensor(\"Relu_1:0\", shape=(?, 8, 8, 16), dtype=float32)\nxi Tensor(\"Relu_2:0\", shape=(?, 2, 2, 8), dtype=float32)\nxo Tensor(\"Relu_3:0\", shape=(?, 8, 8, 16), dtype=float32)\nxo Tensor(\"Relu_4:0\", shape=(?, 16, 16, 32), dtype=float32)\nxo Tensor(\"Tanh:0\", shape=(?, 32, 32, 1), dtype=float32)\n2018-11-02 09:21:05.882917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n2018-11-02 09:21:06.345232: E tensorflow/core/common_runtime/executor.cc:630] Executor failed to create kernel. Invalid argument: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\n    return fn(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\n    options, feed_dict, fetch_list, target_list, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"./simple_autoencoder.py\", line 150, in &lt;module&gt;\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 887, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\n\nCaused by op 'Conv2D_1', defined at:\n  File \"./simple_autoencoder.py\", line 121, in &lt;module&gt;\n    ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\n  File \"./simple_autoencoder.py\", line 69, in autoencoder\n    padding='SAME')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\n`\n\n</code></pre>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:</li>\n<li>TensorFlow installed from (source or binary): 1.10</li>\n<li>Python version: 3.5</li>\n<li>Bazel version (if compiling from source): None</li>\n<li>GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609</li>\n</ul>", "body_text": "Hello,\nI try to design autoencoder in tensorflow that was previously made with tensorflow contrib tf.contrib.layer.\nHere is the code :\n# An undercomplete autoencoder on MNIST dataset\nfrom __future__ import division, print_function, absolute_import\nimport tensorflow.contrib.layers as lays\n\nimport tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import transform\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nbatch_size = 500  # Number of samples in each batch\nepoch_num = 5     # Number of epochs to train the network\nlr = 0.001        # Learning rate\n\ndef resize_batch(imgs):\n    # A function to resize a batch of MNIST images to (32, 32)\n    # Args:\n    #   imgs: a numpy array of size [batch_size, 28 X 28].\n    # Returns:\n    #   a numpy array of size [batch_size, 32, 32].\n    imgs = imgs.reshape((-1, 28, 28, 1))\n    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\n    for i in range(imgs.shape[0]):\n        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\n    return resized_imgs\n\n\ndef autoencoder(inputs):\n    # encoder\n    # 32 x 32 x 1   ->  16 x 16 x 32\n    # 16 x 16 x 32  ->  8 x 8 x 16\n    # 8 x 8 x 16    ->  2 x 2 x 8\n    # print('inputs {0}'.format(inputs))\n\n    # net = lays.conv2d(inputs, 32, [5, 5], stride=2, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d(net, 16, [5, 5], stride=2, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d(net, 8, [5, 5], stride=4, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # # decoder\n    # # 2 x 2 x 8    ->  8 x 8 x 16\n    # # 8 x 8 x 16   ->  16 x 16 x 32\n    # # 16 x 16 x 32  ->  32 x 32 x 1\n    # net = lays.conv2d_transpose(net, 16, [5, 5], stride=4, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d_transpose(net, 32, [5, 5], stride=2, padding='SAME')\n    # print('net {0}'.format(net))\n\n    # net = lays.conv2d_transpose(net, 1, [5, 5], stride=2, padding='SAME', activation_fn=tf.nn.tanh)\n\n    # print('net {0}'.format(net))\n    # return net\n\n    xi = tf.nn.conv2d(ae_inputs,\n                 filter=tf.Variable(tf.random_normal([5,5,1,32])),\n                 strides=[1,2,2,1],\n                 padding='SAME')\n    xi = tf.nn.relu(xi)\n    print(\"xi {0}\".format(xi))\n\n    xi = tf.nn.conv2d(xi,\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\n                     strides=[1,2,2,32],\n                     padding='SAME')\n    xi = tf.nn.relu(xi)\n    print(\"xi {0}\".format(xi))\n\n    xi = tf.nn.conv2d(xi,\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\n                     strides=[1,4,4,16],\n                     padding='SAME')\n    xi = tf.nn.relu(xi)\n\n    print(\"xi {0}\".format(xi))\n\n\n\n\n\n\n    xo = tf.nn.conv2d_transpose(xi,\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\n                     output_shape=[tf.shape(xi)[0], 8, 8, 16],\n                     strides=[1,4,4,1],\n                     padding='SAME')\n    xo = tf.nn.relu(xo)\n\n    print(\"xo {0}\".format(xo))\n\n    xo = tf.nn.conv2d_transpose(xo,\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\n                     output_shape=[tf.shape(xo)[0], 16, 16, 32],\n                     strides=[1,2,2,1],\n                     padding='SAME')\n    xo = tf.nn.relu(xo)\n\n    print(\"xo {0}\".format(xo))\n\n    xo = tf.nn.conv2d_transpose(xo,\n                     filter=tf.Variable(tf.random_normal([5,5,1,32])),\n                     output_shape=[tf.shape(xo)[0], 32, 32, 1],\n                     strides=[1,2,2,1],\n                     padding='SAME')\n    xo = tf.nn.tanh(xo)\n\n\n    print(\"xo {0}\".format(xo))\n    return xo\n# read MNIST dataset\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n\n# calculate the number of batches per epoch\nbatch_per_ep = mnist.train.num_examples // batch_size\n\nae_inputs = tf.placeholder(tf.float32, (None, 32, 32, 1))  # input to the network (MNIST images)\nae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\n\n# calculate the loss and optimize the network\nloss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss\nwith tf.name_scope('train'):\n    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n\n\n\n# initialize the network\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()\nstddev = tf.sqrt(tf.reduce_mean(tf.square(ae_inputs - ae_outputs)))\n\nwith tf.name_scope('stddev'):\n        tf.summary.scalar('accuracy', stddev)\nwith tf.Session() as sess:\n    sess.run(init)\n    train_writer = tf.summary.FileWriter('/tmp/autoencoder/', sess.graph)\n    index = 0\n    for ep in range(epoch_num):  # epochs loop\n        for batch_n in range(batch_per_ep):  # batches loop\n            batch_img, batch_label = mnist.train.next_batch(batch_size)  # read a batch\n            batch_img = batch_img.reshape((-1, 28, 28, 1))               # reshape each sample to an (28, 28) image\n            batch_img = resize_batch(batch_img)                          # reshape the images to (32, 32)\n            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n            run_metadata = tf.RunMetadata()\n            accuracy, c = sess.run([train_op, loss], feed_dict={ae_inputs: batch_img},\n                              options=run_options,\n                              run_metadata=run_metadata)\n            print('Epoch: {} - cost= {:.5f}'.format((ep + 1), c))\n            if batch_n % 10 == 0:\n\n              summ=tf.Summary()\n              summ.value.add(tag='Cost', simple_value = c)\n              summ.value.add(tag='Accuracy', simple_value = accuracy)\n              #train_writer.add_run_metadata(run_metadata, 'step%03d' % batch_n)\n              train_writer.add_summary(summ,index)\n              index+=1\n              train_writer.flush()\n              save_path = saver.save(sess, \"/tmp/autoencoder/model.ckpt\")\n\n              #file_writer = tf.summary.FileWriter(logdir='/tmp/autoencoder/')\n              print(\"Model saved in path: %s\" % save_path)\n\n    train_writer.close()\n\n    # test the trained network\n    batch_img, batch_label = mnist.test.next_batch(50)\n    batch_img = resize_batch(batch_img)\n    recon_img = sess.run([ae_outputs], feed_dict={ae_inputs: batch_img})[0]\n\n    # plot the reconstructed images and their ground truths (inputs)\n    plt.figure(1)\n    plt.title('Reconstructed Images')\n    for i in range(50):\n        plt.subplot(5, 10, i+1)\n        plt.imshow(recon_img[i, ..., 0], cmap='gray')\n    plt.figure(2)\n    plt.title('Input Images')\n    for i in range(50):\n        plt.subplot(5, 10, i+1)\n        plt.imshow(batch_img[i, ..., 0], cmap='gray')\n    plt.show()\n\n\n\nWhere I run the code I get that error:\n`WARNING:tensorflow:From ./simple_autoencoder.py:115: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\nxi Tensor(\"Relu:0\", shape=(?, 16, 16, 32), dtype=float32)\nxi Tensor(\"Relu_1:0\", shape=(?, 8, 8, 16), dtype=float32)\nxi Tensor(\"Relu_2:0\", shape=(?, 2, 2, 8), dtype=float32)\nxo Tensor(\"Relu_3:0\", shape=(?, 8, 8, 16), dtype=float32)\nxo Tensor(\"Relu_4:0\", shape=(?, 16, 16, 32), dtype=float32)\nxo Tensor(\"Tanh:0\", shape=(?, 32, 32, 1), dtype=float32)\n2018-11-02 09:21:05.882917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n2018-11-02 09:21:06.345232: E tensorflow/core/common_runtime/executor.cc:630] Executor failed to create kernel. Invalid argument: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\n    return fn(*args)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\n    options, feed_dict, fetch_list, target_list, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"./simple_autoencoder.py\", line 150, in <module>\n    run_metadata=run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 887, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\n    run_metadata)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\n\nCaused by op 'Conv2D_1', defined at:\n  File \"./simple_autoencoder.py\", line 121, in <module>\n    ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\n  File \"./simple_autoencoder.py\", line 69, in autoencoder\n    padding='SAME')\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\n    data_format=data_format, dilations=dilations, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\n`\n\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary): 1.10\nPython version: 3.5\nBazel version (if compiling from source): None\nGCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609", "body": "Hello,\r\n\r\nI try to design autoencoder in tensorflow that was previously made with tensorflow contrib tf.contrib.layer.\r\nHere is the code : \r\n```\r\n# An undercomplete autoencoder on MNIST dataset\r\nfrom __future__ import division, print_function, absolute_import\r\nimport tensorflow.contrib.layers as lays\r\n\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\nfrom skimage import transform\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\n\r\nbatch_size = 500  # Number of samples in each batch\r\nepoch_num = 5     # Number of epochs to train the network\r\nlr = 0.001        # Learning rate\r\n\r\ndef resize_batch(imgs):\r\n    # A function to resize a batch of MNIST images to (32, 32)\r\n    # Args:\r\n    #   imgs: a numpy array of size [batch_size, 28 X 28].\r\n    # Returns:\r\n    #   a numpy array of size [batch_size, 32, 32].\r\n    imgs = imgs.reshape((-1, 28, 28, 1))\r\n    resized_imgs = np.zeros((imgs.shape[0], 32, 32, 1))\r\n    for i in range(imgs.shape[0]):\r\n        resized_imgs[i, ..., 0] = transform.resize(imgs[i, ..., 0], (32, 32))\r\n    return resized_imgs\r\n\r\n\r\ndef autoencoder(inputs):\r\n    # encoder\r\n    # 32 x 32 x 1   ->  16 x 16 x 32\r\n    # 16 x 16 x 32  ->  8 x 8 x 16\r\n    # 8 x 8 x 16    ->  2 x 2 x 8\r\n    # print('inputs {0}'.format(inputs))\r\n\r\n    # net = lays.conv2d(inputs, 32, [5, 5], stride=2, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d(net, 16, [5, 5], stride=2, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d(net, 8, [5, 5], stride=4, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # # decoder\r\n    # # 2 x 2 x 8    ->  8 x 8 x 16\r\n    # # 8 x 8 x 16   ->  16 x 16 x 32\r\n    # # 16 x 16 x 32  ->  32 x 32 x 1\r\n    # net = lays.conv2d_transpose(net, 16, [5, 5], stride=4, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d_transpose(net, 32, [5, 5], stride=2, padding='SAME')\r\n    # print('net {0}'.format(net))\r\n\r\n    # net = lays.conv2d_transpose(net, 1, [5, 5], stride=2, padding='SAME', activation_fn=tf.nn.tanh)\r\n\r\n    # print('net {0}'.format(net))\r\n    # return net\r\n\r\n    xi = tf.nn.conv2d(ae_inputs,\r\n                 filter=tf.Variable(tf.random_normal([5,5,1,32])),\r\n                 strides=[1,2,2,1],\r\n                 padding='SAME')\r\n    xi = tf.nn.relu(xi)\r\n    print(\"xi {0}\".format(xi))\r\n\r\n    xi = tf.nn.conv2d(xi,\r\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\r\n                     strides=[1,2,2,32],\r\n                     padding='SAME')\r\n    xi = tf.nn.relu(xi)\r\n    print(\"xi {0}\".format(xi))\r\n\r\n    xi = tf.nn.conv2d(xi,\r\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\r\n                     strides=[1,4,4,16],\r\n                     padding='SAME')\r\n    xi = tf.nn.relu(xi)\r\n\r\n    print(\"xi {0}\".format(xi))\r\n\r\n\r\n\r\n\r\n\r\n\r\n    xo = tf.nn.conv2d_transpose(xi,\r\n                     filter=tf.Variable(tf.random_normal([5,5,16,8])),\r\n                     output_shape=[tf.shape(xi)[0], 8, 8, 16],\r\n                     strides=[1,4,4,1],\r\n                     padding='SAME')\r\n    xo = tf.nn.relu(xo)\r\n\r\n    print(\"xo {0}\".format(xo))\r\n\r\n    xo = tf.nn.conv2d_transpose(xo,\r\n                     filter=tf.Variable(tf.random_normal([5,5,32,16])),\r\n                     output_shape=[tf.shape(xo)[0], 16, 16, 32],\r\n                     strides=[1,2,2,1],\r\n                     padding='SAME')\r\n    xo = tf.nn.relu(xo)\r\n\r\n    print(\"xo {0}\".format(xo))\r\n\r\n    xo = tf.nn.conv2d_transpose(xo,\r\n                     filter=tf.Variable(tf.random_normal([5,5,1,32])),\r\n                     output_shape=[tf.shape(xo)[0], 32, 32, 1],\r\n                     strides=[1,2,2,1],\r\n                     padding='SAME')\r\n    xo = tf.nn.tanh(xo)\r\n\r\n\r\n    print(\"xo {0}\".format(xo))\r\n    return xo\r\n# read MNIST dataset\r\nmnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\r\n\r\n# calculate the number of batches per epoch\r\nbatch_per_ep = mnist.train.num_examples // batch_size\r\n\r\nae_inputs = tf.placeholder(tf.float32, (None, 32, 32, 1))  # input to the network (MNIST images)\r\nae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\r\n\r\n# calculate the loss and optimize the network\r\nloss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss\r\nwith tf.name_scope('train'):\r\n    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\r\n\r\n\r\n\r\n# initialize the network\r\ninit = tf.global_variables_initializer()\r\nsaver = tf.train.Saver()\r\nstddev = tf.sqrt(tf.reduce_mean(tf.square(ae_inputs - ae_outputs)))\r\n\r\nwith tf.name_scope('stddev'):\r\n        tf.summary.scalar('accuracy', stddev)\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    train_writer = tf.summary.FileWriter('/tmp/autoencoder/', sess.graph)\r\n    index = 0\r\n    for ep in range(epoch_num):  # epochs loop\r\n        for batch_n in range(batch_per_ep):  # batches loop\r\n            batch_img, batch_label = mnist.train.next_batch(batch_size)  # read a batch\r\n            batch_img = batch_img.reshape((-1, 28, 28, 1))               # reshape each sample to an (28, 28) image\r\n            batch_img = resize_batch(batch_img)                          # reshape the images to (32, 32)\r\n            run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n            run_metadata = tf.RunMetadata()\r\n            accuracy, c = sess.run([train_op, loss], feed_dict={ae_inputs: batch_img},\r\n                              options=run_options,\r\n                              run_metadata=run_metadata)\r\n            print('Epoch: {} - cost= {:.5f}'.format((ep + 1), c))\r\n            if batch_n % 10 == 0:\r\n\r\n              summ=tf.Summary()\r\n              summ.value.add(tag='Cost', simple_value = c)\r\n              summ.value.add(tag='Accuracy', simple_value = accuracy)\r\n              #train_writer.add_run_metadata(run_metadata, 'step%03d' % batch_n)\r\n              train_writer.add_summary(summ,index)\r\n              index+=1\r\n              train_writer.flush()\r\n              save_path = saver.save(sess, \"/tmp/autoencoder/model.ckpt\")\r\n\r\n              #file_writer = tf.summary.FileWriter(logdir='/tmp/autoencoder/')\r\n              print(\"Model saved in path: %s\" % save_path)\r\n\r\n    train_writer.close()\r\n\r\n    # test the trained network\r\n    batch_img, batch_label = mnist.test.next_batch(50)\r\n    batch_img = resize_batch(batch_img)\r\n    recon_img = sess.run([ae_outputs], feed_dict={ae_inputs: batch_img})[0]\r\n\r\n    # plot the reconstructed images and their ground truths (inputs)\r\n    plt.figure(1)\r\n    plt.title('Reconstructed Images')\r\n    for i in range(50):\r\n        plt.subplot(5, 10, i+1)\r\n        plt.imshow(recon_img[i, ..., 0], cmap='gray')\r\n    plt.figure(2)\r\n    plt.title('Input Images')\r\n    for i in range(50):\r\n        plt.subplot(5, 10, i+1)\r\n        plt.imshow(batch_img[i, ..., 0], cmap='gray')\r\n    plt.show()\r\n\r\n\r\n```\r\n\r\nWhere I run the code I get that error:\r\n\r\n```\r\n`WARNING:tensorflow:From ./simple_autoencoder.py:115: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease write your own downloading logic.\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-images-idx3-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.data to implement this functionality.\r\nExtracting MNIST_data/train-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use tf.one_hot on tensors.\r\nExtracting MNIST_data/t10k-images-idx3-ubyte.gz\r\nExtracting MNIST_data/t10k-labels-idx1-ubyte.gz\r\nWARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\nInstructions for updating:\r\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\nxi Tensor(\"Relu:0\", shape=(?, 16, 16, 32), dtype=float32)\r\nxi Tensor(\"Relu_1:0\", shape=(?, 8, 8, 16), dtype=float32)\r\nxi Tensor(\"Relu_2:0\", shape=(?, 2, 2, 8), dtype=float32)\r\nxo Tensor(\"Relu_3:0\", shape=(?, 8, 8, 16), dtype=float32)\r\nxo Tensor(\"Relu_4:0\", shape=(?, 16, 16, 32), dtype=float32)\r\nxo Tensor(\"Tanh:0\", shape=(?, 32, 32, 1), dtype=float32)\r\n2018-11-02 09:21:05.882917: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\r\n  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\r\n2018-11-02 09:21:06.345232: E tensorflow/core/common_runtime/executor.cc:630] Executor failed to create kernel. Invalid argument: Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1292, in _do_call\r\n    return fn(*args)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1277, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1367, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"./simple_autoencoder.py\", line 150, in <module>\r\n    run_metadata=run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 887, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1110, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1286, in _do_run\r\n    run_metadata)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 1308, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\n\r\nCaused by op 'Conv2D_1', defined at:\r\n  File \"./simple_autoencoder.py\", line 121, in <module>\r\n    ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\r\n  File \"./simple_autoencoder.py\", line 69, in autoencoder\r\n    padding='SAME')\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 957, in conv2d\r\n    data_format=data_format, dilations=dilations, name=name)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3272, in create_op\r\n    op_def=op_def)\r\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1768, in __init__\r\n    self._traceback = tf_stack.extract_stack()\r\n\r\nInvalidArgumentError (see above for traceback): Current implementation does not yet support strides in the batch and depth dimensions.\r\n\t [[{{node Conv2D_1}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 2, 2, 32], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Relu, Variable_1/read)]]\r\n`\r\n\r\n```\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary): 1.10\r\n- Python version: 3.5\r\n- Bazel version (if compiling from source): None\r\n- GCC/Compiler version (if compiling from source): gcc (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\n\r\n\r\n"}