{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13462", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13462/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13462/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13462/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13462", "id": 262392890, "node_id": "MDU6SXNzdWUyNjIzOTI4OTA=", "number": 13462, "title": "Feature Request: recompute gradient with updated weights within a graph", "user": {"login": "linlin2010", "id": 32482904, "node_id": "MDQ6VXNlcjMyNDgyOTA0", "avatar_url": "https://avatars1.githubusercontent.com/u/32482904?v=4", "gravatar_id": "", "url": "https://api.github.com/users/linlin2010", "html_url": "https://github.com/linlin2010", "followers_url": "https://api.github.com/users/linlin2010/followers", "following_url": "https://api.github.com/users/linlin2010/following{/other_user}", "gists_url": "https://api.github.com/users/linlin2010/gists{/gist_id}", "starred_url": "https://api.github.com/users/linlin2010/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/linlin2010/subscriptions", "organizations_url": "https://api.github.com/users/linlin2010/orgs", "repos_url": "https://api.github.com/users/linlin2010/repos", "events_url": "https://api.github.com/users/linlin2010/events{/privacy}", "received_events_url": "https://api.github.com/users/linlin2010/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-10-03T11:28:44Z", "updated_at": "2018-02-08T18:27:09Z", "closed_at": "2018-02-08T18:27:09Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I wonder could there could be some new features to recompute gradients with updated weights within a graph or if there is any better way to do this. For example, for estimating hessian norm, we need to compute</p>\n<p>delta ~ N(0, I)<br>\nhessian_norm = 1/M \\sum_{1}^{M}  gradient(f(x+delta))- gradient(f(x-delta))/(2delta)</p>\n<p>we need to gradient value on x+delta. Currently we will get None type if we use tf.gradient on var+delta directly.</p>\n<p>Thank you very much.</p>", "body_text": "Hi,\nI wonder could there could be some new features to recompute gradients with updated weights within a graph or if there is any better way to do this. For example, for estimating hessian norm, we need to compute\ndelta ~ N(0, I)\nhessian_norm = 1/M \\sum_{1}^{M}  gradient(f(x+delta))- gradient(f(x-delta))/(2delta)\nwe need to gradient value on x+delta. Currently we will get None type if we use tf.gradient on var+delta directly.\nThank you very much.", "body": "Hi,\r\n\r\nI wonder could there could be some new features to recompute gradients with updated weights within a graph or if there is any better way to do this. For example, for estimating hessian norm, we need to compute\r\n\r\ndelta ~ N(0, I)\r\nhessian_norm = 1/M \\sum_{1}^{M}  gradient(f(x+delta))- gradient(f(x-delta))/(2delta)\r\n\r\nwe need to gradient value on x+delta. Currently we will get None type if we use tf.gradient on var+delta directly. \r\n\r\nThank you very much."}