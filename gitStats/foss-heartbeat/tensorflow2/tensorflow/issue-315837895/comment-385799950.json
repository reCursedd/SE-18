{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/385799950", "html_url": "https://github.com/tensorflow/tensorflow/issues/18689#issuecomment-385799950", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18689", "id": 385799950, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NTc5OTk1MA==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-01T21:43:33Z", "updated_at": "2018-05-01T21:43:33Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11500638\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/makeandbreak\">@makeandbreak</a></p>\n<p>I have some questions to better understand the situation and please anyone response not just makeandbreak:</p>\n<ul>\n<li>Do you do training on the older CPUs? and If so what do you train and how long does it take?</li>\n<li>What are you doing with TensorFlow on those systems?  (I have a guess I do not want to lead the answer)</li>\n</ul>\n<p>We are looking into options.  AVX very much speeds up performance for CPU over SSE, this is also true for AMD processors.  The move was done to improve out of the box performance for CPU users in general as well as those doing inference, which is often done on CPU and gets a large boost from AVX.</p>\n<p>This section is not to win you over but you let you know the thought process at a high level.  I looked at the Intel CPU line.  Sandy bridge, which is where AVX started came out, was in 2011.  AWS makes it fairly hard to get non-AVX systems for anything but random web serving and that is still limited.  I suspected many users are using mac book pros and other laptops that are sandy bridge and newer.  AVX provides a significant boost and I wanted to get that to end users.</p>\n<p>Finally for this comment, the build matrix is very large because it needs to be supported, e.g. all unit tests need to pass and it should be performance tested.  We want to make a pragmatic decision and your feed back is very helpful.  We do not know what we do not know and I personally want to balance ensuring people can use TensorFlow, with TensorFlow being fast, and the testing workload.</p>\n<p>The MKL build from Intel might work but I think they still default to AVX for ops other than those handled by MKL.  We are looking at this option.  It has been almost a year but I did test MKL on AMD Ryzen and it was fine and can be tested again.  Painful to test as at the time and even still now there are not many or any cloud providers with AMD systems, I suspect some schools still have opterons laying around.</p>", "body_text": "@makeandbreak\nI have some questions to better understand the situation and please anyone response not just makeandbreak:\n\nDo you do training on the older CPUs? and If so what do you train and how long does it take?\nWhat are you doing with TensorFlow on those systems?  (I have a guess I do not want to lead the answer)\n\nWe are looking into options.  AVX very much speeds up performance for CPU over SSE, this is also true for AMD processors.  The move was done to improve out of the box performance for CPU users in general as well as those doing inference, which is often done on CPU and gets a large boost from AVX.\nThis section is not to win you over but you let you know the thought process at a high level.  I looked at the Intel CPU line.  Sandy bridge, which is where AVX started came out, was in 2011.  AWS makes it fairly hard to get non-AVX systems for anything but random web serving and that is still limited.  I suspected many users are using mac book pros and other laptops that are sandy bridge and newer.  AVX provides a significant boost and I wanted to get that to end users.\nFinally for this comment, the build matrix is very large because it needs to be supported, e.g. all unit tests need to pass and it should be performance tested.  We want to make a pragmatic decision and your feed back is very helpful.  We do not know what we do not know and I personally want to balance ensuring people can use TensorFlow, with TensorFlow being fast, and the testing workload.\nThe MKL build from Intel might work but I think they still default to AVX for ops other than those handled by MKL.  We are looking at this option.  It has been almost a year but I did test MKL on AMD Ryzen and it was fine and can be tested again.  Painful to test as at the time and even still now there are not many or any cloud providers with AMD systems, I suspect some schools still have opterons laying around.", "body": "@makeandbreak \r\n\r\nI have some questions to better understand the situation and please anyone response not just makeandbreak:\r\n\r\n- Do you do training on the older CPUs? and If so what do you train and how long does it take?\r\n- What are you doing with TensorFlow on those systems?  (I have a guess I do not want to lead the answer)\r\n\r\nWe are looking into options.  AVX very much speeds up performance for CPU over SSE, this is also true for AMD processors.  The move was done to improve out of the box performance for CPU users in general as well as those doing inference, which is often done on CPU and gets a large boost from AVX.  \r\n\r\nThis section is not to win you over but you let you know the thought process at a high level.  I looked at the Intel CPU line.  Sandy bridge, which is where AVX started came out, was in 2011.  AWS makes it fairly hard to get non-AVX systems for anything but random web serving and that is still limited.  I suspected many users are using mac book pros and other laptops that are sandy bridge and newer.  AVX provides a significant boost and I wanted to get that to end users.\r\n\r\nFinally for this comment, the build matrix is very large because it needs to be supported, e.g. all unit tests need to pass and it should be performance tested.  We want to make a pragmatic decision and your feed back is very helpful.  We do not know what we do not know and I personally want to balance ensuring people can use TensorFlow, with TensorFlow being fast, and the testing workload.  \r\n\r\nThe MKL build from Intel might work but I think they still default to AVX for ops other than those handled by MKL.  We are looking at this option.  It has been almost a year but I did test MKL on AMD Ryzen and it was fine and can be tested again.  Painful to test as at the time and even still now there are not many or any cloud providers with AMD systems, I suspect some schools still have opterons laying around.  "}