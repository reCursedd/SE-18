{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351664466", "html_url": "https://github.com/tensorflow/tensorflow/issues/15341#issuecomment-351664466", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15341", "id": 351664466, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTY2NDQ2Ng==", "user": {"login": "bar513", "id": 33786313, "node_id": "MDQ6VXNlcjMzNzg2MzEz", "avatar_url": "https://avatars3.githubusercontent.com/u/33786313?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bar513", "html_url": "https://github.com/bar513", "followers_url": "https://api.github.com/users/bar513/followers", "following_url": "https://api.github.com/users/bar513/following{/other_user}", "gists_url": "https://api.github.com/users/bar513/gists{/gist_id}", "starred_url": "https://api.github.com/users/bar513/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bar513/subscriptions", "organizations_url": "https://api.github.com/users/bar513/orgs", "repos_url": "https://api.github.com/users/bar513/repos", "events_url": "https://api.github.com/users/bar513/events{/privacy}", "received_events_url": "https://api.github.com/users/bar513/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-14T10:01:54Z", "updated_at": "2017-12-14T10:01:54Z", "author_association": "NONE", "body_html": "<p>Here I demonstrate the missing feature on the mnist classic example</p>\n<pre><code># using tensorflow 1.3 \n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\n\n# create a model and save the graph and the tensors for later training\ndef createModelAndSave(my_gpu):\n    learning_rate = 0.01\n    with tf.device(my_gpu): # all model's tensors are on this gpu, for example '/gpu:0'\n        x = tf.placeholder(\"float\", [None, 784])\n        y = tf.placeholder(\"float\", [None, 10])\n\n        # Set model weights\n        W = tf.Variable(tf.zeros([784, 10]))\n        b = tf.Variable(tf.zeros([10]))\n\n        model = tf.nn.softmax(tf.matmul(x, W) + b)\n\n        # cost function\n        cost_function = -tf.reduce_sum(y*tf.log(model))\n\n        # Gradient descent\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n\n    # save all tensors in the collection\n    tf.add_to_collection('cost_function',cost_function)\n    tf.add_to_collection('optimizer',optimizer)\n    tf.add_to_collection('x', x)\n    tf.add_to_collection('y', y)\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        saver.save(sess,'myModel.ckpt')\n\n\n# load the saved graph and the tensors for training\ndef loadModelAndTrain():\n    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n    training_iteration = 1000\n    batch_size = 100\n\n    # load graph\n    tf.train.import_meta_graph('myModel.ckpt.meta')\n    with tf.Session() as sess:\n        # load tensors\n        cost_function = sess.graph.get_collection(\"cost_function\")[0]\n        optimizer = sess.graph.get_collection(\"optimizer\")[0]\n        x = sess.graph.get_collection(\"x\")[0]\n        y = sess.graph.get_collection(\"y\")[0]\n\n        # ~~~~~~~\n        # At this point the tensors and the graph are loaded but all of them are on GPU that was chosen when the model was saved.\n        # for example, the training will run on '/gpu:0' as chosen years ago.\n        # assuming we don't have the function to create the model again, we cannot relocate the tensors to anther GPU.\n        # ~~~~~~~\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        # Training cycle - all on the chosen GPU\n        for iteration in range(training_iteration):\n            avg_cost = 0.\n            total_batch = int(mnist.train.num_examples/batch_size)\n            for i in range(total_batch):\n                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n                avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n            # Display logs per iteration step\n            print (\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n\nif __name__ == \"__main__\":\n\n    # ~~~~~ first run createModelAndSave(), later comment this function and run loadModelAndTrain()\n\n    createModelAndSave('/gpu:0') # save a model to a specific GPU\n    # loadModelAndTrain() # load the saved model and train it, cannot rechoose GPU\n\n</code></pre>", "body_text": "Here I demonstrate the missing feature on the mnist classic example\n# using tensorflow 1.3 \n\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport tensorflow as tf\n\n\n# create a model and save the graph and the tensors for later training\ndef createModelAndSave(my_gpu):\n    learning_rate = 0.01\n    with tf.device(my_gpu): # all model's tensors are on this gpu, for example '/gpu:0'\n        x = tf.placeholder(\"float\", [None, 784])\n        y = tf.placeholder(\"float\", [None, 10])\n\n        # Set model weights\n        W = tf.Variable(tf.zeros([784, 10]))\n        b = tf.Variable(tf.zeros([10]))\n\n        model = tf.nn.softmax(tf.matmul(x, W) + b)\n\n        # cost function\n        cost_function = -tf.reduce_sum(y*tf.log(model))\n\n        # Gradient descent\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\n\n    # save all tensors in the collection\n    tf.add_to_collection('cost_function',cost_function)\n    tf.add_to_collection('optimizer',optimizer)\n    tf.add_to_collection('x', x)\n    tf.add_to_collection('y', y)\n\n    saver = tf.train.Saver()\n    with tf.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        saver.save(sess,'myModel.ckpt')\n\n\n# load the saved graph and the tensors for training\ndef loadModelAndTrain():\n    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\n    training_iteration = 1000\n    batch_size = 100\n\n    # load graph\n    tf.train.import_meta_graph('myModel.ckpt.meta')\n    with tf.Session() as sess:\n        # load tensors\n        cost_function = sess.graph.get_collection(\"cost_function\")[0]\n        optimizer = sess.graph.get_collection(\"optimizer\")[0]\n        x = sess.graph.get_collection(\"x\")[0]\n        y = sess.graph.get_collection(\"y\")[0]\n\n        # ~~~~~~~\n        # At this point the tensors and the graph are loaded but all of them are on GPU that was chosen when the model was saved.\n        # for example, the training will run on '/gpu:0' as chosen years ago.\n        # assuming we don't have the function to create the model again, we cannot relocate the tensors to anther GPU.\n        # ~~~~~~~\n        init = tf.global_variables_initializer()\n        sess.run(init)\n\n        # Training cycle - all on the chosen GPU\n        for iteration in range(training_iteration):\n            avg_cost = 0.\n            total_batch = int(mnist.train.num_examples/batch_size)\n            for i in range(total_batch):\n                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\n                avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\n            # Display logs per iteration step\n            print (\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\n\n\nif __name__ == \"__main__\":\n\n    # ~~~~~ first run createModelAndSave(), later comment this function and run loadModelAndTrain()\n\n    createModelAndSave('/gpu:0') # save a model to a specific GPU\n    # loadModelAndTrain() # load the saved model and train it, cannot rechoose GPU", "body": "\r\nHere I demonstrate the missing feature on the mnist classic example\r\n\r\n```\r\n# using tensorflow 1.3 \r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nimport tensorflow as tf\r\n\r\n\r\n# create a model and save the graph and the tensors for later training\r\ndef createModelAndSave(my_gpu):\r\n    learning_rate = 0.01\r\n    with tf.device(my_gpu): # all model's tensors are on this gpu, for example '/gpu:0'\r\n        x = tf.placeholder(\"float\", [None, 784])\r\n        y = tf.placeholder(\"float\", [None, 10])\r\n\r\n        # Set model weights\r\n        W = tf.Variable(tf.zeros([784, 10]))\r\n        b = tf.Variable(tf.zeros([10]))\r\n\r\n        model = tf.nn.softmax(tf.matmul(x, W) + b)\r\n\r\n        # cost function\r\n        cost_function = -tf.reduce_sum(y*tf.log(model))\r\n\r\n        # Gradient descent\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost_function)\r\n\r\n    # save all tensors in the collection\r\n    tf.add_to_collection('cost_function',cost_function)\r\n    tf.add_to_collection('optimizer',optimizer)\r\n    tf.add_to_collection('x', x)\r\n    tf.add_to_collection('y', y)\r\n\r\n    saver = tf.train.Saver()\r\n    with tf.Session() as sess:\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n        saver.save(sess,'myModel.ckpt')\r\n\r\n\r\n# load the saved graph and the tensors for training\r\ndef loadModelAndTrain():\r\n    mnist = input_data.read_data_sets(\"MNIST_data\", one_hot=True)\r\n    training_iteration = 1000\r\n    batch_size = 100\r\n\r\n    # load graph\r\n    tf.train.import_meta_graph('myModel.ckpt.meta')\r\n    with tf.Session() as sess:\r\n        # load tensors\r\n        cost_function = sess.graph.get_collection(\"cost_function\")[0]\r\n        optimizer = sess.graph.get_collection(\"optimizer\")[0]\r\n        x = sess.graph.get_collection(\"x\")[0]\r\n        y = sess.graph.get_collection(\"y\")[0]\r\n\r\n        # ~~~~~~~\r\n        # At this point the tensors and the graph are loaded but all of them are on GPU that was chosen when the model was saved.\r\n        # for example, the training will run on '/gpu:0' as chosen years ago.\r\n        # assuming we don't have the function to create the model again, we cannot relocate the tensors to anther GPU.\r\n        # ~~~~~~~\r\n        init = tf.global_variables_initializer()\r\n        sess.run(init)\r\n\r\n        # Training cycle - all on the chosen GPU\r\n        for iteration in range(training_iteration):\r\n            avg_cost = 0.\r\n            total_batch = int(mnist.train.num_examples/batch_size)\r\n            for i in range(total_batch):\r\n                batch_xs, batch_ys = mnist.train.next_batch(batch_size)\r\n                sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys})\r\n                avg_cost += sess.run(cost_function, feed_dict={x: batch_xs, y: batch_ys})/total_batch\r\n            # Display logs per iteration step\r\n            print (\"Iteration:\", '%04d' % (iteration + 1), \"cost=\", \"{:.9f}\".format(avg_cost))\r\n\r\n\r\nif __name__ == \"__main__\":\r\n\r\n    # ~~~~~ first run createModelAndSave(), later comment this function and run loadModelAndTrain()\r\n\r\n    createModelAndSave('/gpu:0') # save a model to a specific GPU\r\n    # loadModelAndTrain() # load the saved model and train it, cannot rechoose GPU\r\n\r\n```"}