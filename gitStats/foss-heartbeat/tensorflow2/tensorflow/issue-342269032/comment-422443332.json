{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422443332", "html_url": "https://github.com/tensorflow/tensorflow/pull/20918#issuecomment-422443332", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20918", "id": 422443332, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjQ0MzMzMg==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T15:37:10Z", "updated_at": "2018-09-18T15:37:10Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">Sorry if I wasn't clear. Passing a non-scalar loss to tf.gradients will\nimplicitly reduce it to a scalar (though the scalar is never actually\ncomputed). Since your code doesn't seem to use the non-scalar loss in any\nway other than to pass it to tf.gradients I believe you do not need to\nreduce it.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Mon, Sep 17, 2018 at 5:15 PM Ciprian F. ***@***.***&gt; wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In tensorflow/contrib/opt/python/training/irprop_plus.py\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"342269032\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20918\" href=\"https://github.com/tensorflow/tensorflow/pull/20918#discussion_r218266054\">#20918 (comment)</a>&gt;\n :\n\n &gt; +    <a class=\"user-mention\" href=\"https://github.com/compatibility\">@compatibility</a>(eager)\n +    When eager execution is enabled, `loss` should be a Python function that\n +    takes elements of `var_list` as arguments and computes the value to be\n +    minimized. If `var_list` is `None`, `loss` should take no arguments.\n +    Minimization (and gradient computation) is done with respect to the\n +    elements of `var_list` if not `None`, else with respect to any trainable\n +    variables created during the execution of the `loss` function.\n +    `gate_gradients`, `aggregation_method`, `colocate_gradients_with_ops` and\n +    `grad_loss` are ignored when eager execution is enabled.\n +    @end_compatibility\n +    \"\"\"\n +    # Override method from base class, the loss is required to be scalar\n +\n +    # Error E(t)\n +    if not self._is_scalar(loss):\n +      raise ValueError(\"'loss' (%s) must be a 0-D tensor.\" % loss)\n\n <a class=\"user-mention\" href=\"https://github.com/alextp\">@alextp</a> &lt;<a href=\"https://github.com/alextp\">https://github.com/alextp</a>&gt; It does not reduce the loss to a\n scalar for me (tested in OptimizerV2), unless the weights are scalars. For\n example, after running the code below, the cost is a vector which needs to\n be reduced to a scalar in order to feed the error to IRprop+. Am I missing\n something? some help would be much appreciated. Thank you.\n\n import tensorflow as tf\n\n x1 = tf.Variable([1.0, 2.0])\n x2 = tf.Variable([2.0, 3.0])\n loss = 2*x1**2 + 3*x2\n\n grad = tf.gradients(loss, [x1, x2])\n\n init = tf.global_variables_initializer()\n\n with tf.Session() as sess:\n sess.run(init)\n grad_value = sess.run(grad)\n print(sess.run(loss))\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"342269032\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20918\" href=\"https://github.com/tensorflow/tensorflow/pull/20918#discussion_r218266054\">#20918 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxcPk9s1sDpUbJw3vebngZzbv14qqks5ucDsogaJpZM4VUV9I\">https://github.com/notifications/unsubscribe-auth/AAATxcPk9s1sDpUbJw3vebngZzbv14qqks5ucDsogaJpZM4VUV9I</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "Sorry if I wasn't clear. Passing a non-scalar loss to tf.gradients will\nimplicitly reduce it to a scalar (though the scalar is never actually\ncomputed). Since your code doesn't seem to use the non-scalar loss in any\nway other than to pass it to tf.gradients I believe you do not need to\nreduce it.\n\u2026\nOn Mon, Sep 17, 2018 at 5:15 PM Ciprian F. ***@***.***> wrote:\n ***@***.**** commented on this pull request.\n ------------------------------\n\n In tensorflow/contrib/opt/python/training/irprop_plus.py\n <#20918 (comment)>\n :\n\n > +    @compatibility(eager)\n +    When eager execution is enabled, `loss` should be a Python function that\n +    takes elements of `var_list` as arguments and computes the value to be\n +    minimized. If `var_list` is `None`, `loss` should take no arguments.\n +    Minimization (and gradient computation) is done with respect to the\n +    elements of `var_list` if not `None`, else with respect to any trainable\n +    variables created during the execution of the `loss` function.\n +    `gate_gradients`, `aggregation_method`, `colocate_gradients_with_ops` and\n +    `grad_loss` are ignored when eager execution is enabled.\n +    @end_compatibility\n +    \"\"\"\n +    # Override method from base class, the loss is required to be scalar\n +\n +    # Error E(t)\n +    if not self._is_scalar(loss):\n +      raise ValueError(\"'loss' (%s) must be a 0-D tensor.\" % loss)\n\n @alextp <https://github.com/alextp> It does not reduce the loss to a\n scalar for me (tested in OptimizerV2), unless the weights are scalars. For\n example, after running the code below, the cost is a vector which needs to\n be reduced to a scalar in order to feed the error to IRprop+. Am I missing\n something? some help would be much appreciated. Thank you.\n\n import tensorflow as tf\n\n x1 = tf.Variable([1.0, 2.0])\n x2 = tf.Variable([2.0, 3.0])\n loss = 2*x1**2 + 3*x2\n\n grad = tf.gradients(loss, [x1, x2])\n\n init = tf.global_variables_initializer()\n\n with tf.Session() as sess:\n sess.run(init)\n grad_value = sess.run(grad)\n print(sess.run(loss))\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#20918 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxcPk9s1sDpUbJw3vebngZzbv14qqks5ucDsogaJpZM4VUV9I>\n .\n\n\n-- \n - Alex", "body": "Sorry if I wasn't clear. Passing a non-scalar loss to tf.gradients will\nimplicitly reduce it to a scalar (though the scalar is never actually\ncomputed). Since your code doesn't seem to use the non-scalar loss in any\nway other than to pass it to tf.gradients I believe you do not need to\nreduce it.\n\nOn Mon, Sep 17, 2018 at 5:15 PM Ciprian F. <notifications@github.com> wrote:\n\n> *@ciprianflow* commented on this pull request.\n> ------------------------------\n>\n> In tensorflow/contrib/opt/python/training/irprop_plus.py\n> <https://github.com/tensorflow/tensorflow/pull/20918#discussion_r218266054>\n> :\n>\n> > +    @compatibility(eager)\n> +    When eager execution is enabled, `loss` should be a Python function that\n> +    takes elements of `var_list` as arguments and computes the value to be\n> +    minimized. If `var_list` is `None`, `loss` should take no arguments.\n> +    Minimization (and gradient computation) is done with respect to the\n> +    elements of `var_list` if not `None`, else with respect to any trainable\n> +    variables created during the execution of the `loss` function.\n> +    `gate_gradients`, `aggregation_method`, `colocate_gradients_with_ops` and\n> +    `grad_loss` are ignored when eager execution is enabled.\n> +    @end_compatibility\n> +    \"\"\"\n> +    # Override method from base class, the loss is required to be scalar\n> +\n> +    # Error E(t)\n> +    if not self._is_scalar(loss):\n> +      raise ValueError(\"'loss' (%s) must be a 0-D tensor.\" % loss)\n>\n> @alextp <https://github.com/alextp> It does not reduce the loss to a\n> scalar for me (tested in OptimizerV2), unless the weights are scalars. For\n> example, after running the code below, the cost is a vector which needs to\n> be reduced to a scalar in order to feed the error to IRprop+. Am I missing\n> something? some help would be much appreciated. Thank you.\n>\n> import tensorflow as tf\n>\n> x1 = tf.Variable([1.0, 2.0])\n> x2 = tf.Variable([2.0, 3.0])\n> loss = 2*x1**2 + 3*x2\n>\n> grad = tf.gradients(loss, [x1, x2])\n>\n> init = tf.global_variables_initializer()\n>\n> with tf.Session() as sess:\n> sess.run(init)\n> grad_value = sess.run(grad)\n> print(sess.run(loss))\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/pull/20918#discussion_r218266054>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxcPk9s1sDpUbJw3vebngZzbv14qqks5ucDsogaJpZM4VUV9I>\n> .\n>\n\n\n-- \n - Alex\n"}