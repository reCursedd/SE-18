{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/208305611", "pull_request_review_id": 144087782, "id": 208305611, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwODMwNTYxMQ==", "diff_hunk": "@@ -3664,4 +3739,282 @@ REGISTER_KERNELS(GPU, double);\n #undef REGISTER_CPU_KERNELS\n #undef REGISTER_KERNELS\n \n+template <typename Device, typename T>\n+class ApplyIRpropPlusOp : public OpKernel {\n+ public:\n+  explicit ApplyIRpropPlusOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_locking\", &use_exclusive_lock_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    auto locks = MaybeLockVariableInputMutexesInOrder(ctx, use_exclusive_lock_,\n+                                                      {0, 1, 2});\n+\n+    Tensor var;\n+    OP_REQUIRES_OK(ctx, GetInputTensorFromVariable<Device, T>(\n+                            ctx, 0, use_exclusive_lock_, false, &var));\n+    Tensor old_grad;\n+    OP_REQUIRES_OK(ctx, GetInputTensorFromVariable<Device, T>(\n+                            ctx, 1, use_exclusive_lock_, false, &old_grad));\n+    Tensor delta_update;\n+    OP_REQUIRES_OK(ctx, GetInputTensorFromVariable<Device, T>(\n+                            ctx, 2, use_exclusive_lock_, false, &delta_update));\n+    OP_REQUIRES(\n+        ctx, var.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", requested_input(0)));\n+\n+    OP_REQUIRES(\n+        ctx, old_grad.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", requested_input(1)));\n+\n+    OP_REQUIRES(\n+        ctx, delta_update.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", requested_input(2)));\n+\n+    const Tensor& eta_minus = ctx->input(3);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(eta_minus.shape()),\n+                errors::InvalidArgument(\"eta_minus is not a scalar: \",\n+                                        eta_minus.shape().DebugString()));\n+    const Tensor& eta_plus = ctx->input(4);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(eta_plus.shape()),\n+                errors::InvalidArgument(\"eta_plus is not a scalar: \",\n+                                        eta_plus.shape().DebugString()));\n+    const Tensor& delta_min = ctx->input(5);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(delta_min.shape()),\n+                errors::InvalidArgument(\"delta_min is not a scalar: \",\n+                                        delta_min.shape().DebugString()));\n+    const Tensor& delta_max = ctx->input(6);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(delta_max.shape()),\n+                errors::InvalidArgument(\"delta_max is not a scalar: \",\n+                                        delta_max.shape().DebugString()));\n+\n+    const Tensor& error = ctx->input(7);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(error.shape()),\n+                errors::InvalidArgument(\"error is not a scalar: \",\n+                                        error.shape().DebugString()));\n+    const Tensor& old_error = ctx->input(8);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(old_error.shape()),\n+                errors::InvalidArgument(\"old_error is not a scalar: \",\n+                                        old_error.shape().DebugString()));\n+\n+    const Tensor& grad = ctx->input(9);\n+    OP_REQUIRES(\n+        ctx, var.shape().IsSameSize(grad.shape()),\n+        errors::InvalidArgument(\"var and grad do not have the same shape\",\n+                                var.shape().DebugString(), \" \",\n+                                grad.shape().DebugString()));\n+\n+    OP_REQUIRES(ctx, var.shape().IsSameSize(delta_update.shape()),\n+                errors::InvalidArgument(\n+                    \"var and delta_update do not have the same shape\",\n+                    var.shape().DebugString(), \" \",\n+                    delta_update.shape().DebugString()));\n+\n+    OP_REQUIRES(\n+        ctx, var.shape().IsSameSize(old_grad.shape()),\n+        errors::InvalidArgument(\"var and old_grad do not have the same shape\",\n+                                var.shape().DebugString(), \" \",\n+                                old_grad.shape().DebugString()));\n+\n+    const Device& device = ctx->template eigen_device<Device>();\n+\n+    functor::ApplyIRpropPlus<Device, T>()(\n+        device, var.flat<T>(), old_grad.flat<T>(), delta_update.flat<T>(),\n+        eta_minus.scalar<T>(), eta_plus.scalar<T>(), delta_min.scalar<T>(),\n+        delta_max.scalar<T>(), error.scalar<T>(), old_error.scalar<T>(),\n+        grad.flat<T>());\n+    MaybeForwardRefInputToRefOutput(ctx, 0, 0);\n+  }\n+\n+ private:\n+  bool use_exclusive_lock_;\n+};\n+\n+#define REGISTER_KERNELS(D, T)                                           \\\n+  REGISTER_KERNEL_BUILDER(                                               \\\n+      Name(\"ApplyIRpropPlus\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n+      ApplyIRpropPlusOp<D##Device, T>);                                  \\\n+  REGISTER_KERNEL_BUILDER(Name(\"ResourceApplyIRpropPlus\")                \\\n+                              .Device(DEVICE_##D)                        \\\n+                              .HostMemory(\"var\")                         \\\n+                              .HostMemory(\"old_grad\")                    \\\n+                              .HostMemory(\"delta_update\")                \\\n+                              .TypeConstraint<T>(\"T\"),                   \\\n+                          ApplyIRpropPlusOp<D##Device, T>);\n+#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T);\n+\n+TF_CALL_half(REGISTER_CPU_KERNELS);\n+TF_CALL_bfloat16(REGISTER_CPU_KERNELS);\n+TF_CALL_float(REGISTER_CPU_KERNELS);\n+TF_CALL_double(REGISTER_CPU_KERNELS);\n+\n+#if GOOGLE_CUDA\n+// Forward declarations of the functor specializations for GPU.\n+namespace functor {\n+#define DECLARE_GPU_SPEC(T)                             \\\n+  template <>                                           \\\n+  void ApplyIRpropPlus<GPUDevice, T>::operator()(       \\\n+      const GPUDevice& d, typename TTypes<T>::Flat var, \\\n+      typename TTypes<T>::Flat old_grad,                \\\n+      typename TTypes<T>::Flat delta_update,            \\\n+      typename TTypes<T>::ConstScalar eta_minus,        \\\n+      typename TTypes<T>::ConstScalar eta_plus,         \\\n+      typename TTypes<T>::ConstScalar delta_min,        \\\n+      typename TTypes<T>::ConstScalar delta_max,        \\\n+      typename TTypes<T>::ConstScalar error,            \\\n+      typename TTypes<T>::ConstScalar old_error,        \\\n+      typename TTypes<T>::ConstFlat grad);              \\\n+  extern template struct ApplyIRpropPlus<GPUDevice, T>;\n+DECLARE_GPU_SPEC(Eigen::half);\n+DECLARE_GPU_SPEC(float);\n+DECLARE_GPU_SPEC(double);\n+#undef DECLARE_GPU_SPEC\n+}  // namespace functor\n+\n+REGISTER_KERNELS(GPU, Eigen::half);\n+REGISTER_KERNELS(GPU, float);\n+REGISTER_KERNELS(GPU, double);\n+#endif\n+#undef REGISTER_CPU_KERNELS\n+#undef REGISTER_KERNELS\n+\n+template <typename Device, typename T>\n+class ApplyRpropMinusOp : public OpKernel {\n+ public:\n+  explicit ApplyRpropMinusOp(OpKernelConstruction* ctx) : OpKernel(ctx) {\n+    OP_REQUIRES_OK(ctx, ctx->GetAttr(\"use_locking\", &use_exclusive_lock_));\n+  }\n+\n+  void Compute(OpKernelContext* ctx) override {\n+    auto locks = MaybeLockVariableInputMutexesInOrder(ctx, use_exclusive_lock_,\n+                                                      {0, 1, 2});\n+\n+    Tensor var;\n+    OP_REQUIRES_OK(ctx, GetInputTensorFromVariable<Device, T>(\n+                            ctx, 0, use_exclusive_lock_, false, &var));\n+    Tensor old_grad;\n+    OP_REQUIRES_OK(ctx, GetInputTensorFromVariable<Device, T>(\n+                            ctx, 1, use_exclusive_lock_, false, &old_grad));\n+    Tensor delta_update;\n+    OP_REQUIRES_OK(ctx, GetInputTensorFromVariable<Device, T>(\n+                            ctx, 2, use_exclusive_lock_, false, &delta_update));\n+    OP_REQUIRES(\n+        ctx, var.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", requested_input(0)));\n+\n+    OP_REQUIRES(\n+        ctx, old_grad.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", requested_input(1)));\n+\n+    OP_REQUIRES(\n+        ctx, delta_update.IsInitialized(),\n+        errors::FailedPrecondition(\n+            \"Attempting to use uninitialized variables: \", requested_input(2)));\n+\n+    const Tensor& eta_minus = ctx->input(3);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(eta_minus.shape()),\n+                errors::InvalidArgument(\"eta_minus is not a scalar: \",\n+                                        eta_minus.shape().DebugString()));\n+    const Tensor& eta_plus = ctx->input(4);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(eta_plus.shape()),\n+                errors::InvalidArgument(\"eta_plus is not a scalar: \",\n+                                        eta_plus.shape().DebugString()));\n+    const Tensor& delta_min = ctx->input(5);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(delta_min.shape()),\n+                errors::InvalidArgument(\"delta_min is not a scalar: \",\n+                                        delta_min.shape().DebugString()));\n+    const Tensor& delta_max = ctx->input(6);\n+    OP_REQUIRES(ctx, TensorShapeUtils::IsScalar(delta_max.shape()),\n+                errors::InvalidArgument(\"delta_max is not a scalar: \",\n+                                        delta_max.shape().DebugString()));\n+\n+    const Tensor& grad = ctx->input(7);\n+    OP_REQUIRES(\n+        ctx, var.shape().IsSameSize(grad.shape()),\n+        errors::InvalidArgument(\"var and grad do not have the same shape\",\n+                                var.shape().DebugString(), \" \",\n+                                grad.shape().DebugString()));\n+\n+    OP_REQUIRES(ctx, var.shape().IsSameSize(delta_update.shape()),\n+                errors::InvalidArgument(\n+                    \"var and delta_update do not have the same shape\",\n+                    var.shape().DebugString(), \" \",\n+                    delta_update.shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, var.shape().IsSameSize(old_grad.shape()),\n+        errors::InvalidArgument(\"var and old_grad do not have the same shape\",\n+                                var.shape().DebugString(), \" \",\n+                                old_grad.shape().DebugString()));\n+    OP_REQUIRES(\n+        ctx, var.shape().IsSameSize(grad.shape()),\n+        errors::InvalidArgument(\"var and delta do not have the same shape\",\n+                                var.shape().DebugString(), \" \",\n+                                grad.shape().DebugString()));\n+\n+    const Device& device = ctx->template eigen_device<Device>();\n+\n+    functor::ApplyRpropMinus<Device, T>()(\n+        device, var.flat<T>(), old_grad.flat<T>(), delta_update.flat<T>(),\n+        eta_minus.scalar<T>(), eta_plus.scalar<T>(), delta_min.scalar<T>(),\n+        delta_max.scalar<T>(), grad.flat<T>());\n+    MaybeForwardRefInputToRefOutput(ctx, 0, 0);\n+  }\n+\n+ private:\n+  bool use_exclusive_lock_;\n+};\n+\n+#define REGISTER_KERNELS(D, T)                                           \\\n+  REGISTER_KERNEL_BUILDER(                                               \\\n+      Name(\"ApplyRpropMinus\").Device(DEVICE_##D).TypeConstraint<T>(\"T\"), \\\n+      ApplyRpropMinusOp<D##Device, T>);                                  \\\n+  REGISTER_KERNEL_BUILDER(Name(\"ResourceApplyRpropMinus\")                \\\n+                              .Device(DEVICE_##D)                        \\\n+                              .HostMemory(\"var\")                         \\\n+                              .HostMemory(\"old_grad\")                    \\\n+                              .HostMemory(\"delta_update\")                \\\n+                              .TypeConstraint<T>(\"T\"),                   \\\n+                          ApplyRpropMinusOp<D##Device, T>);\n+#define REGISTER_CPU_KERNELS(T) REGISTER_KERNELS(CPU, T);\n+\n+TF_CALL_half(REGISTER_CPU_KERNELS);\n+TF_CALL_bfloat16(REGISTER_CPU_KERNELS);\n+TF_CALL_float(REGISTER_CPU_KERNELS);\n+TF_CALL_double(REGISTER_CPU_KERNELS);\n+\n+#if GOOGLE_CUDA\n+// Forward declarations of the functor specializations for GPU.\n+namespace functor {\n+\n+#define DECLARE_GPU_SPEC(T)                             \\\n+  template <>                                           \\\n+  void ApplyRpropMinus<GPUDevice, T>::operator()(       \\\n+      const GPUDevice& d, typename TTypes<T>::Flat var, \\\n+      typename TTypes<T>::Flat old_grad,                \\\n+      typename TTypes<T>::Flat delta_update,            \\\n+      typename TTypes<T>::ConstScalar eta_minus,        \\\n+      typename TTypes<T>::ConstScalar eta_plus,         \\\n+      typename TTypes<T>::ConstScalar delta_min,        \\\n+      typename TTypes<T>::ConstScalar delta_max,        \\\n+      typename TTypes<T>::ConstFlat grad);              \\\n+  extern template struct ApplyRpropMinus<GPUDevice, T>;\n+DECLARE_GPU_SPEC(Eigen::half);\n+DECLARE_GPU_SPEC(float);\n+DECLARE_GPU_SPEC(double);\n+#undef DECLARE_GPU_SPEC\n+\n+}  // namespace functor\n+\n+REGISTER_KERNELS(GPU, Eigen::half);", "path": "tensorflow/core/kernels/training_ops.cc", "position": null, "original_position": 357, "commit_id": "7e1c185bb1d31eeac606f7300303718dab2e9c8f", "original_commit_id": "4cb6c2502f8f44a709057c48d96d02cc56934740", "user": {"login": "ciprianflow", "id": 7080826, "node_id": "MDQ6VXNlcjcwODA4MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7080826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ciprianflow", "html_url": "https://github.com/ciprianflow", "followers_url": "https://api.github.com/users/ciprianflow/followers", "following_url": "https://api.github.com/users/ciprianflow/following{/other_user}", "gists_url": "https://api.github.com/users/ciprianflow/gists{/gist_id}", "starred_url": "https://api.github.com/users/ciprianflow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ciprianflow/subscriptions", "organizations_url": "https://api.github.com/users/ciprianflow/orgs", "repos_url": "https://api.github.com/users/ciprianflow/repos", "events_url": "https://api.github.com/users/ciprianflow/events{/privacy}", "received_events_url": "https://api.github.com/users/ciprianflow/received_events", "type": "User", "site_admin": false}, "body": "ok", "created_at": "2018-08-07T16:49:52Z", "updated_at": "2018-08-07T16:49:52Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20918#discussion_r208305611", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20918", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/208305611"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20918#discussion_r208305611"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20918"}}, "body_html": "<p>ok</p>", "body_text": "ok", "in_reply_to_id": 207286671}