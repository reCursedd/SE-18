{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/207165636", "pull_request_review_id": 142719510, "id": 207165636, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwNzE2NTYzNg==", "diff_hunk": "@@ -0,0 +1,272 @@\n+# Copyright 2018 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\"\"\"Tests for IRpropPlus.\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+import numpy as np\n+\n+from tensorflow.contrib.opt.python.training import irprop_plus\n+from tensorflow.python.eager import context\n+from tensorflow.python.framework import constant_op\n+from tensorflow.python.framework import dtypes\n+from tensorflow.python.ops import resource_variable_ops\n+from tensorflow.python.ops import variables\n+\n+from tensorflow.python.platform import test\n+\n+def irprop_update_numpy(params,\n+                        old_w_t,\n+                        g_t,\n+                        old_g_t,\n+                        delta_t,\n+                        eta_minus,\n+                        eta_plus,\n+                        delta_min,\n+                        delta_max,\n+                        error,\n+                        old_error):\n+\n+  # Hold an extra var to restore the previous weight\n+  old_params = np.copy(params)\n+  old_g = np.copy(g_t)\n+\n+  for i, _ in enumerate(g_t):\n+    if g_t[i] * old_g_t[i] > 0:\n+      delta_t[i] = np.fmin(delta_max, delta_t[i] * eta_plus)\n+      params[i] -= np.sign(g_t[i]) * delta_t[i]\n+\n+    elif g_t[i] * old_g_t[i] < 0:\n+      delta_t[i] = np.fmax(delta_min, delta_t[i] * eta_minus)\n+      if error > old_error:\n+        # retract step\n+        params[i] = old_w_t[i]\n+      old_g[i] = 0\n+\n+    else:\n+      params[i] -= np.sign(g_t[i]) * delta_t[i]\n+  return params, delta_t, old_g, old_params\n+\n+\n+class IRpropPlusTest(test.TestCase):\n+\n+  def _testDense(self,\n+                 use_resource=False,\n+                 eta_minus=0.5,\n+                 eta_plus=1.2,\n+                 delta_min=1e-6,\n+                 delta_max=50,\n+                 delta_zero=0.5):\n+\n+    for dtype in [dtypes.half, dtypes.float32, dtypes.float64]:\n+      with self.test_session(use_gpu=True):\n+        # Initialize variables for numpy implementation.\n+        var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+        grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n+        neg_grads0_np = np.negative(grads0_np)\n+        old_grads0 = np.array([0, 0], dtype=dtype.as_numpy_dtype)\n+        delta0 = np.array([delta_zero, delta_zero], dtype=dtype.as_numpy_dtype)\n+\n+        # errors\n+        loss0_np = 0.5\n+        old_loss0_np = -0.25\n+        large_loss0_np = 2.5\n+\n+        var1_np = np.array([3.0, 4.0], dtype=dtype.as_numpy_dtype)\n+        grads1_np = np.array([0.01, 0.01], dtype=dtype.as_numpy_dtype)\n+        neg_grads1_np = np.negative(grads1_np)\n+        old_grads1 = np.array([0, 0], dtype=dtype.as_numpy_dtype)\n+        delta1 = np.array([delta_zero, delta_zero], dtype=dtype.as_numpy_dtype)\n+\n+        if use_resource:\n+          var0 = resource_variable_ops.ResourceVariable(var0_np)\n+          var1 = resource_variable_ops.ResourceVariable(var1_np)\n+        else:\n+          var0 = variables.Variable(var0_np)\n+          var1 = variables.Variable(var1_np)\n+\n+        loss0_var = variables.Variable(loss0_np)\n+        large_loss0_var = variables.Variable(large_loss0_np)\n+\n+        grads0 = constant_op.constant(grads0_np)\n+        grads1 = constant_op.constant(grads1_np)\n+\n+        opt = irprop_plus.IRpropPlusOptimizer(eta_minus=eta_minus,\n+                                              eta_plus=eta_plus,\n+                                              delta_min=delta_min,\n+                                              delta_max=delta_max,\n+                                              delta_zero=delta_zero)\n+\n+        pos_update = opt.apply_gradients(zip([grads0, grads1], [var0, var1]),\n+                                         loss0_var)\n+        neg_update = opt.apply_gradients(zip([-grads0, -grads1], [var0, var1]),\n+                                         loss0_var)\n+\n+        # large loss value to retract the previous weight\n+        pos_large_update = opt.apply_gradients(\n+            zip([grads0, grads1], [var0, var1]),\n+            large_loss0_var)\n+\n+        neg_large_update = opt.apply_gradients(\n+            zip([-grads0, -grads1], [var0, var1]),\n+            large_loss0_var)\n+\n+        if not context.executing_eagerly():\n+          self.evaluate(variables.global_variables_initializer())\n+          # Fetch params to validate initial values\n+          self.assertAllClose([1.0, 2.0], self.evaluate(var0))\n+          self.assertAllClose([3.0, 4.0], self.evaluate(var1))\n+\n+        old_w_t0 = var0\n+        old_w_t1 = var1\n+\n+        # Run 10 steps of irprop+\n+        # first 3 steps with positive gradient (same consecutive sign)\n+        # next 3 steps with negative gradient\n+        # - first iteration with negative sign will retract the weight\n+        # - second iteration will have 0 sign\n+        # - next iterations will have the same sign\n+        # last 4 steps with alternate gradient\n+        # - first iteration won't retract the weight (neg sign)\n+        # - second it will have 0 sign\n+        # - third iteration will retract the weight (neg sign)\n+        # - fourth iteration 0 sign\n+        for t in range(1, 11):\n+          loss0 = loss0_np\n+          old_loss0 = old_loss0_np\n+\n+          if t < 4:\n+            grads0_sign = grads0_np\n+            grads1_sign = grads1_np\n+            if not context.executing_eagerly():\n+              self.evaluate(pos_update)\n+            elif t > 1:\n+              opt.apply_gradients(zip([grads0, grads1], [var0, var1]),\n+                                  loss0_var)\n+\n+          elif t < 7:\n+            grads0_sign = neg_grads0_np\n+            grads1_sign = neg_grads1_np\n+\n+            if not context.executing_eagerly():\n+              loss0 = large_loss0_np\n+              self.evaluate(neg_large_update)\n+            else:\n+              opt.apply_gradients(zip([-grads0, -grads1], [var0, var1]),\n+                                  large_loss0_var)\n+          else:\n+            if t & 1 == 0:\n+              grads0_sign = neg_grads0_np\n+              grads1_sign = neg_grads1_np\n+              # retract weight\n+              update = neg_update\n+              if context.executing_eagerly():\n+                opt.apply_gradients(zip([-grads0, -grads1], [var0, var1]),\n+                                    loss0_var)\n+            else:\n+              # pos grad\n+              grads0_sign = grads0_np\n+              grads1_sign = grads1_np\n+              old_loss0 = loss0\n+              update = pos_update\n+\n+              if t > 8:\n+                loss0 = large_loss0_np\n+                update = pos_large_update\n+                if context.executing_eagerly():\n+                  pos_large_update = opt.apply_gradients(zip([grads0, grads1],\n+                                                             [var0, var1]),\n+                                                         large_loss0_var)\n+              else:\n+                if context.executing_eagerly():\n+                  opt.apply_gradients(zip([grads0, grads1], [var0, var1]),\n+                                      loss0_var)\n+\n+            if not context.executing_eagerly():\n+              self.evaluate(update)\n+\n+          var0_np, delta0, old_grads0, old_w_t0 = irprop_update_numpy(\n+              var0_np,\n+              old_w_t0,\n+              grads0_sign,\n+              old_grads0,\n+              delta0,\n+              eta_minus=eta_minus,\n+              eta_plus=eta_plus,\n+              delta_min=delta_min,\n+              delta_max=delta_max,\n+              error=loss0,\n+              old_error=old_loss0)\n+\n+          var1_np, delta1, old_grads1, old_w_t1 = irprop_update_numpy(\n+              var1_np,\n+              old_w_t1,\n+              grads1_sign,\n+              old_grads1,\n+              delta1,\n+              eta_minus=eta_minus,\n+              eta_plus=eta_plus,\n+              delta_min=delta_min,\n+              delta_max=delta_max,\n+              error=loss0,\n+              old_error=old_loss0)\n+\n+          self.assertAllCloseAccordingToType(var0_np, self.evaluate(var0))\n+          self.assertAllCloseAccordingToType(var1_np, self.evaluate(var1))\n+\n+  def _testInvalidLoss(self):\n+\n+    for dtype in [dtypes.float32]:\n+\n+      loss_var_none = None\n+      loss_var_vector = variables.Variable(\n+          np.array([1.0], dtype=dtype.as_numpy_dtype))\n+\n+      var0_np = np.array([1.0, 2.0], dtype=dtype.as_numpy_dtype)\n+      grads0_np = np.array([0.1, 0.1], dtype=dtype.as_numpy_dtype)\n+\n+      var0 = constant_op.constant(var0_np)\n+      grads0 = constant_op.constant(grads0_np)\n+\n+      opt = irprop_plus.IRpropPlusOptimizer()\n+      grads = zip([grads0], [var0])\n+      self.assertRaises(ValueError, opt.apply_gradients, grads, loss_var_none)\n+      self.assertRaises(ValueError, opt.apply_gradients, grads, loss_var_vector)\n+      # type error in case no loss variable is passed, sanity check\n+      self.assertRaises(TypeError, opt.apply_gradients, grads)\n+\n+  def testDense(self):", "path": "tensorflow/contrib/opt/python/training/irprop_plus_test.py", "position": null, "original_position": 252, "commit_id": "7e1c185bb1d31eeac606f7300303718dab2e9c8f", "original_commit_id": "178e89fdfad1fee46bd19572e80a41efde31b40f", "user": {"login": "ciprianflow", "id": 7080826, "node_id": "MDQ6VXNlcjcwODA4MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7080826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ciprianflow", "html_url": "https://github.com/ciprianflow", "followers_url": "https://api.github.com/users/ciprianflow/followers", "following_url": "https://api.github.com/users/ciprianflow/following{/other_user}", "gists_url": "https://api.github.com/users/ciprianflow/gists{/gist_id}", "starred_url": "https://api.github.com/users/ciprianflow/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ciprianflow/subscriptions", "organizations_url": "https://api.github.com/users/ciprianflow/orgs", "repos_url": "https://api.github.com/users/ciprianflow/repos", "events_url": "https://api.github.com/users/ciprianflow/events{/privacy}", "received_events_url": "https://api.github.com/users/ciprianflow/received_events", "type": "User", "site_admin": false}, "body": "ok", "created_at": "2018-08-02T09:46:43Z", "updated_at": "2018-08-07T16:48:46Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20918#discussion_r207165636", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20918", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/207165636"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20918#discussion_r207165636"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20918"}}, "body_html": "<p>ok</p>", "body_text": "ok", "in_reply_to_id": 203814235}