{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16284", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16284/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16284/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16284/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16284", "id": 290450957, "node_id": "MDU6SXNzdWUyOTA0NTA5NTc=", "number": 16284, "title": "[bug?] Tensorflow accepts CUDA_VISIBLE_DEVICES but still allocates memory on multiple GPUs", "user": {"login": "e-ka", "id": 7968675, "node_id": "MDQ6VXNlcjc5Njg2NzU=", "avatar_url": "https://avatars3.githubusercontent.com/u/7968675?v=4", "gravatar_id": "", "url": "https://api.github.com/users/e-ka", "html_url": "https://github.com/e-ka", "followers_url": "https://api.github.com/users/e-ka/followers", "following_url": "https://api.github.com/users/e-ka/following{/other_user}", "gists_url": "https://api.github.com/users/e-ka/gists{/gist_id}", "starred_url": "https://api.github.com/users/e-ka/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/e-ka/subscriptions", "organizations_url": "https://api.github.com/users/e-ka/orgs", "repos_url": "https://api.github.com/users/e-ka/repos", "events_url": "https://api.github.com/users/e-ka/events{/privacy}", "received_events_url": "https://api.github.com/users/e-ka/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-01-22T12:36:26Z", "updated_at": "2018-10-24T21:06:33Z", "closed_at": "2018-10-24T21:06:33Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>I have written a custom script, but effect is also visible for just <code>import tensorflow as tf; sess = tf.Session()</code></strong>:</li>\n<li><strong>Windows 7 Professional</strong>:</li>\n<li><strong>TensorFlow installed from binary (pip, in Anaconda environment)</strong>:</li>\n<li><strong>TensorFlow version 1.4.0</strong>:</li>\n<li><strong>Python version 3.5.4</strong>:</li>\n<li><strong>CUDA/cuDNN version 8.0/6.0</strong>:</li>\n<li><strong>4 GeForce GTX 1080Ti, 11GB</strong>:</li>\n</ul>\n<h3>Problem description</h3>\n<p>I am facing the following issue:</p>\n<p>If I set<br>\n<code>CUDA_VISIBLE_DEVICES=1</code><br>\nand start my python script, everything works as expected, only GPU 1 is used/only memory from GPU 1 is allocated. GPU1 hosts the Desktop Window Manager.</p>\n<p>If I set<br>\n<code>CUDA_VISIBLE_DEVICES=0 # or 2 or 3 </code><br>\nbefore running my python script<br>\n<code>sess = tf.Session()</code><br>\nfaithfully reports only one available GPU (with the expected PCI bus id), see attached file ipython.txt.</p>\n<p>However, nvidia_smi.exe shows that memory on all remaining GPUs except for GPU1 is allocated. GPU-Util shows that expected GPU is used for actual computation, see attached file nvidia_smi_output.txt.</p>\n<p>I see this effect both for my actual tensorflow script as well for simple interactive python with</p>\n<pre><code>import tensorflow as tf\nsess = tf.Session()\n</code></pre>\n<p>Unfortunately, the dual boot Ubuntu is not working at the moment, but once it is running again, I can try to check whether a similar effect presents itself there.<br>\nCould this be a bug, possibly related to Windows? Or a driver or hardware issue?</p>\n<p>Attached files:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1651908/ipython.txt\">ipython.txt</a>: ipython script and output.<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1651905/nvidia_smi_output.txt\">nvidia_smi_output.txt</a>: output of nvidia-smi.exe</p>", "body_text": "System information\n\nI have written a custom script, but effect is also visible for just import tensorflow as tf; sess = tf.Session():\nWindows 7 Professional:\nTensorFlow installed from binary (pip, in Anaconda environment):\nTensorFlow version 1.4.0:\nPython version 3.5.4:\nCUDA/cuDNN version 8.0/6.0:\n4 GeForce GTX 1080Ti, 11GB:\n\nProblem description\nI am facing the following issue:\nIf I set\nCUDA_VISIBLE_DEVICES=1\nand start my python script, everything works as expected, only GPU 1 is used/only memory from GPU 1 is allocated. GPU1 hosts the Desktop Window Manager.\nIf I set\nCUDA_VISIBLE_DEVICES=0 # or 2 or 3 \nbefore running my python script\nsess = tf.Session()\nfaithfully reports only one available GPU (with the expected PCI bus id), see attached file ipython.txt.\nHowever, nvidia_smi.exe shows that memory on all remaining GPUs except for GPU1 is allocated. GPU-Util shows that expected GPU is used for actual computation, see attached file nvidia_smi_output.txt.\nI see this effect both for my actual tensorflow script as well for simple interactive python with\nimport tensorflow as tf\nsess = tf.Session()\n\nUnfortunately, the dual boot Ubuntu is not working at the moment, but once it is running again, I can try to check whether a similar effect presents itself there.\nCould this be a bug, possibly related to Windows? Or a driver or hardware issue?\nAttached files:\nipython.txt: ipython script and output.\nnvidia_smi_output.txt: output of nvidia-smi.exe", "body": "### System information\r\n- **I have written a custom script, but effect is also visible for just `import tensorflow as tf; sess = tf.Session()`**:\r\n- **Windows 7 Professional**:\r\n- **TensorFlow installed from binary (pip, in Anaconda environment)**:\r\n- **TensorFlow version 1.4.0**:\r\n- **Python version 3.5.4**: \r\n- **CUDA/cuDNN version 8.0/6.0**:\r\n- **4 GeForce GTX 1080Ti, 11GB**:\r\n\r\n### Problem description\r\nI am facing the following issue:\r\n\r\nIf I set\r\n`CUDA_VISIBLE_DEVICES=1`\r\nand start my python script, everything works as expected, only GPU 1 is used/only memory from GPU 1 is allocated. GPU1 hosts the Desktop Window Manager.\r\n\r\nIf I set\r\n`CUDA_VISIBLE_DEVICES=0 # or 2 or 3 `\r\nbefore running my python script\r\n`sess = tf.Session()`\r\nfaithfully reports only one available GPU (with the expected PCI bus id), see attached file ipython.txt.\r\n\r\nHowever, nvidia_smi.exe shows that memory on all remaining GPUs except for GPU1 is allocated. GPU-Util shows that expected GPU is used for actual computation, see attached file nvidia_smi_output.txt.\r\n\r\nI see this effect both for my actual tensorflow script as well for simple interactive python with\r\n\r\n```\r\nimport tensorflow as tf\r\nsess = tf.Session()\r\n```\r\n\r\nUnfortunately, the dual boot Ubuntu is not working at the moment, but once it is running again, I can try to check whether a similar effect presents itself there.\r\nCould this be a bug, possibly related to Windows? Or a driver or hardware issue?\r\n\r\nAttached files:\r\n[ipython.txt](https://github.com/tensorflow/tensorflow/files/1651908/ipython.txt): ipython script and output.\r\n[nvidia_smi_output.txt](https://github.com/tensorflow/tensorflow/files/1651905/nvidia_smi_output.txt): output of nvidia-smi.exe\r\n"}