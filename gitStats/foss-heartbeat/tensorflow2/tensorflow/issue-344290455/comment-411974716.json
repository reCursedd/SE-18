{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/411974716", "html_url": "https://github.com/tensorflow/tensorflow/issues/21115#issuecomment-411974716", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21115", "id": 411974716, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMTk3NDcxNg==", "user": {"login": "guptapriya", "id": 14104855, "node_id": "MDQ6VXNlcjE0MTA0ODU1", "avatar_url": "https://avatars1.githubusercontent.com/u/14104855?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guptapriya", "html_url": "https://github.com/guptapriya", "followers_url": "https://api.github.com/users/guptapriya/followers", "following_url": "https://api.github.com/users/guptapriya/following{/other_user}", "gists_url": "https://api.github.com/users/guptapriya/gists{/gist_id}", "starred_url": "https://api.github.com/users/guptapriya/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guptapriya/subscriptions", "organizations_url": "https://api.github.com/users/guptapriya/orgs", "repos_url": "https://api.github.com/users/guptapriya/repos", "events_url": "https://api.github.com/users/guptapriya/events{/privacy}", "received_events_url": "https://api.github.com/users/guptapriya/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-10T04:52:12Z", "updated_at": "2018-08-10T04:52:12Z", "author_association": "MEMBER", "body_html": "<p>It seems like you're using the approach described here: <a href=\"https://www.tensorflow.org/deploy/distributed\" rel=\"nofollow\">https://www.tensorflow.org/deploy/distributed</a>.<br>\nUnfortunately, this approach doesn't work with keras fit, as far as I know. cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=32556631\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/anj-s\">@anj-s</a> - who can perhaps add more.</p>\n<p>If you'd like to run your training in keras and distribute on multiple GPUs, you can try using DistributionStrategy instead, which is now supported in keras fit. See example here for now: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/keras_test.py#L244\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/keras_test.py#L244</a><br>\nWe have more documentation coming soon.</p>", "body_text": "It seems like you're using the approach described here: https://www.tensorflow.org/deploy/distributed.\nUnfortunately, this approach doesn't work with keras fit, as far as I know. cc @anj-s - who can perhaps add more.\nIf you'd like to run your training in keras and distribute on multiple GPUs, you can try using DistributionStrategy instead, which is now supported in keras fit. See example here for now: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/keras_test.py#L244\nWe have more documentation coming soon.", "body": "It seems like you're using the approach described here: https://www.tensorflow.org/deploy/distributed. \r\nUnfortunately, this approach doesn't work with keras fit, as far as I know. cc @anj-s - who can perhaps add more. \r\n\r\nIf you'd like to run your training in keras and distribute on multiple GPUs, you can try using DistributionStrategy instead, which is now supported in keras fit. See example here for now: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distribute/python/keras_test.py#L244\r\nWe have more documentation coming soon. "}