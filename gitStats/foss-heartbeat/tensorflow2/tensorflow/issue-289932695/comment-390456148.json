{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/390456148", "html_url": "https://github.com/tensorflow/tensorflow/issues/16239#issuecomment-390456148", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16239", "id": 390456148, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDQ1NjE0OA==", "user": {"login": "ghostplant", "id": 12099308, "node_id": "MDQ6VXNlcjEyMDk5MzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/12099308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghostplant", "html_url": "https://github.com/ghostplant", "followers_url": "https://api.github.com/users/ghostplant/followers", "following_url": "https://api.github.com/users/ghostplant/following{/other_user}", "gists_url": "https://api.github.com/users/ghostplant/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghostplant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghostplant/subscriptions", "organizations_url": "https://api.github.com/users/ghostplant/orgs", "repos_url": "https://api.github.com/users/ghostplant/repos", "events_url": "https://api.github.com/users/ghostplant/events{/privacy}", "received_events_url": "https://api.github.com/users/ghostplant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-20T04:03:58Z", "updated_at": "2018-05-20T04:05:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23200711\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ontheway16\">@ontheway16</a><br>\nI think unified memory will just allow you to do training/inference using large models without caring about GPU OOM. But when the model is huge to completely fit into whole device memory, it is also likely to slow down.<br>\nHowever, if not using unified memory, such huge models are even not able to run on TF.</p>", "body_text": "@ontheway16\nI think unified memory will just allow you to do training/inference using large models without caring about GPU OOM. But when the model is huge to completely fit into whole device memory, it is also likely to slow down.\nHowever, if not using unified memory, such huge models are even not able to run on TF.", "body": "@ontheway16 \r\nI think unified memory will just allow you to do training/inference using large models without caring about GPU OOM. But when the model is huge to completely fit into whole device memory, it is also likely to slow down.\r\nHowever, if not using unified memory, such huge models are even not able to run on TF."}