{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/359399238", "html_url": "https://github.com/tensorflow/tensorflow/issues/16239#issuecomment-359399238", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16239", "id": 359399238, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTM5OTIzOA==", "user": {"login": "ghostplant", "id": 12099308, "node_id": "MDQ6VXNlcjEyMDk5MzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/12099308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghostplant", "html_url": "https://github.com/ghostplant", "followers_url": "https://api.github.com/users/ghostplant/followers", "following_url": "https://api.github.com/users/ghostplant/following{/other_user}", "gists_url": "https://api.github.com/users/ghostplant/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghostplant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghostplant/subscriptions", "organizations_url": "https://api.github.com/users/ghostplant/orgs", "repos_url": "https://api.github.com/users/ghostplant/repos", "events_url": "https://api.github.com/users/ghostplant/events{/privacy}", "received_events_url": "https://api.github.com/users/ghostplant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-22T11:40:15Z", "updated_at": "2018-01-22T11:42:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=326106\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/aselle\">@aselle</a> It is really necessary. :)</p>\n<p>I will provide some facts of deep learning models and why current tensorflow memory management is not suitable.</p>\n<h2>1. Tensorflow model migration problem:</h2>\n<p>Whatever MNIST_CNN, Alexnet, Inception, Resnet. etc, all nerual networks have a fixed structure so their GPU memory occupation can be precisely calculated if <code>batch_size</code> is given. So users expect a way that Tensorflow can provide something like <code>per_process_gpu_memory_quota_mb</code> in place of <code>per_process_gpu_memory_fraction</code>, because the latter makes the model migration very inconvenient. We expect a way like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>(Assume MNIST_CNN model costs only 400MB GPU memory)\nconfig.gpu_options.per_process_gpu_memory_quota_mb=400</pre></div>\n<p>However, currently the source code can be only like this:</p>\n<div class=\"highlight highlight-source-shell\"><pre>config.gpu_options.per_process_gpu_memory_fraction=0.2</pre></div>\n<p>When the above code is executed using different GPUs (e.g. 16GB, 1GB), 16GB-GPU can succeed but 1GB-GPU will report BFC out-of-memory which is really annoying...</p>\n<h2>2. Huge model (inceptionV4, etc) training on Tensorflow problem:</h2>\n<p>VGG_VP and Inception models are known as huge deep learning models, whose convolutional filters are too large so they usually costs extremely-large GPU memory, e.g. at least 8GB memory is required by VGG_VP.</p>\n<p>Assume we have a GPU memory of 2GB, and a host memory of 16GB. Based on the current Tensorflow management, VGG_VP and Inception CANNOT efficiently run on this GPU since Tensorflow only allocate full 2GB GPU memory which is way not enough.  If we use GpuManagedAllocator, it can allow to allocate up to 2GB + 16GB memory in total, so the VGG_VP and Inception models can be well fit in this GPU, and the training performance is very near to all-data-on-GPU-memory (85-95% according to our experiment).</p>\n<p>So we expect something like:</p>\n<div class=\"highlight highlight-source-shell\"><pre>config.gpu_options.allocator_type=<span class=\"pl-s\"><span class=\"pl-pds\">'</span>GpuManaged<span class=\"pl-pds\">'</span></span></pre></div>\n<p>In a word, I think BFC out-of-memory is really a high-frequent problem that reduces the user experience when using Tensorflow to train HUGE model, e.g. we even have no way to train a huge model on a small-memory GPU, and above 2 suggestions are very helpful and supporting GpuManagedAllocator is a must to fix all of them in the first place.</p>", "body_text": "@aselle It is really necessary. :)\nI will provide some facts of deep learning models and why current tensorflow memory management is not suitable.\n1. Tensorflow model migration problem:\nWhatever MNIST_CNN, Alexnet, Inception, Resnet. etc, all nerual networks have a fixed structure so their GPU memory occupation can be precisely calculated if batch_size is given. So users expect a way that Tensorflow can provide something like per_process_gpu_memory_quota_mb in place of per_process_gpu_memory_fraction, because the latter makes the model migration very inconvenient. We expect a way like this:\n(Assume MNIST_CNN model costs only 400MB GPU memory)\nconfig.gpu_options.per_process_gpu_memory_quota_mb=400\nHowever, currently the source code can be only like this:\nconfig.gpu_options.per_process_gpu_memory_fraction=0.2\nWhen the above code is executed using different GPUs (e.g. 16GB, 1GB), 16GB-GPU can succeed but 1GB-GPU will report BFC out-of-memory which is really annoying...\n2. Huge model (inceptionV4, etc) training on Tensorflow problem:\nVGG_VP and Inception models are known as huge deep learning models, whose convolutional filters are too large so they usually costs extremely-large GPU memory, e.g. at least 8GB memory is required by VGG_VP.\nAssume we have a GPU memory of 2GB, and a host memory of 16GB. Based on the current Tensorflow management, VGG_VP and Inception CANNOT efficiently run on this GPU since Tensorflow only allocate full 2GB GPU memory which is way not enough.  If we use GpuManagedAllocator, it can allow to allocate up to 2GB + 16GB memory in total, so the VGG_VP and Inception models can be well fit in this GPU, and the training performance is very near to all-data-on-GPU-memory (85-95% according to our experiment).\nSo we expect something like:\nconfig.gpu_options.allocator_type='GpuManaged'\nIn a word, I think BFC out-of-memory is really a high-frequent problem that reduces the user experience when using Tensorflow to train HUGE model, e.g. we even have no way to train a huge model on a small-memory GPU, and above 2 suggestions are very helpful and supporting GpuManagedAllocator is a must to fix all of them in the first place.", "body": "@aselle It is really necessary. :)\r\n\r\nI will provide some facts of deep learning models and why current tensorflow memory management is not suitable.\r\n\r\n## 1. Tensorflow model migration problem:\r\n\r\nWhatever MNIST_CNN, Alexnet, Inception, Resnet. etc, all nerual networks have a fixed structure so their GPU memory occupation can be precisely calculated if `batch_size` is given. So users expect a way that Tensorflow can provide something like `per_process_gpu_memory_quota_mb` in place of `per_process_gpu_memory_fraction`, because the latter makes the model migration very inconvenient. We expect a way like this:\r\n\r\n```sh\r\n(Assume MNIST_CNN model costs only 400MB GPU memory)\r\nconfig.gpu_options.per_process_gpu_memory_quota_mb=400\r\n```\r\n\r\nHowever, currently the source code can be only like this:\r\n```sh\r\nconfig.gpu_options.per_process_gpu_memory_fraction=0.2\r\n```\r\nWhen the above code is executed using different GPUs (e.g. 16GB, 1GB), 16GB-GPU can succeed but 1GB-GPU will report BFC out-of-memory which is really annoying...\r\n\r\n\r\n## 2. Huge model (inceptionV4, etc) training on Tensorflow problem:\r\n\r\nVGG_VP and Inception models are known as huge deep learning models, whose convolutional filters are too large so they usually costs extremely-large GPU memory, e.g. at least 8GB memory is required by VGG_VP.\r\n\r\nAssume we have a GPU memory of 2GB, and a host memory of 16GB. Based on the current Tensorflow management, VGG_VP and Inception CANNOT efficiently run on this GPU since Tensorflow only allocate full 2GB GPU memory which is way not enough.  If we use GpuManagedAllocator, it can allow to allocate up to 2GB + 16GB memory in total, so the VGG_VP and Inception models can be well fit in this GPU, and the training performance is very near to all-data-on-GPU-memory (85-95% according to our experiment).\r\n\r\nSo we expect something like:\r\n```sh\r\nconfig.gpu_options.allocator_type='GpuManaged'\r\n```\r\n\r\nIn a word, I think BFC out-of-memory is really a high-frequent problem that reduces the user experience when using Tensorflow to train HUGE model, e.g. we even have no way to train a huge model on a small-memory GPU, and above 2 suggestions are very helpful and supporting GpuManagedAllocator is a must to fix all of them in the first place.\r\n"}