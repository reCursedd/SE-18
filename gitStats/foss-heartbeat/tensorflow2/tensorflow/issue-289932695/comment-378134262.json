{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/378134262", "html_url": "https://github.com/tensorflow/tensorflow/issues/16239#issuecomment-378134262", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16239", "id": 378134262, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODEzNDI2Mg==", "user": {"login": "ghostplant", "id": 12099308, "node_id": "MDQ6VXNlcjEyMDk5MzA4", "avatar_url": "https://avatars2.githubusercontent.com/u/12099308?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghostplant", "html_url": "https://github.com/ghostplant", "followers_url": "https://api.github.com/users/ghostplant/followers", "following_url": "https://api.github.com/users/ghostplant/following{/other_user}", "gists_url": "https://api.github.com/users/ghostplant/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghostplant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghostplant/subscriptions", "organizations_url": "https://api.github.com/users/ghostplant/orgs", "repos_url": "https://api.github.com/users/ghostplant/repos", "events_url": "https://api.github.com/users/ghostplant/events{/privacy}", "received_events_url": "https://api.github.com/users/ghostplant/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-03T05:39:11Z", "updated_at": "2018-04-03T05:44:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1990079\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/smit-hinsu\">@smit-hinsu</a> , the source code of VGG16 is a keras-based implementation and executed with Tensorflow backend, and all each the time I posted stand for the real time spent to finish 1 epoch training, this is related to the scale of input data so absolute value is meaningless here.</p>\n<p><strong>Huge model training</strong> I just want a friendly way to get over BFC-overflow problem when using tensorflow, because the overflow is slow, not catch-able by python runtime and it is even hard to kill. Using GpuManagedAlloc is a suitable way to avoid such bad status for huge model.</p>\n<p><strong>Model migration problem</strong> User sensitive to absolute memory is definitely needed because they want to not only evaluate the resource usage for their defined models, but also know a model is runnable on a certain GPU from a hybrid cluster. However, considering the absolute memory is related to both model itself and also the batch_size, this value might not be a global constant one, so I recently think it would be better to be able to pre-evaluate the memory occupation before sess.run, something like:</p>\n<div class=\"highlight highlight-source-shell\"><pre>memory, .. = sess.evalutate_resources(target, ...)\nsess.run(target, ...)</pre></div>\n<p>The value of memory can be a map of different devices, e.g:</p>\n<div class=\"highlight highlight-source-shell\"><pre>memory = {\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>: 6000000000,\n    <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:1<span class=\"pl-pds\">'</span></span>: 0\n}</pre></div>\n<p>Thus, user can judge whether their model can fit into an unknown GPU, and also able to adjust a suitable batch_size completed before real session run.</p>\n<div class=\"highlight highlight-source-shell\"><pre>free_memory = cuda.getMemFree(..)\nbatch_size = 256\n<span class=\"pl-k\">while</span> True:\n    target = make_model(.., batch_size)\n    <span class=\"pl-k\">if</span> sess.evaluate_memory(target) <span class=\"pl-k\">&gt;</span> free_memory:\n        batch_size /= 2\n    else:\n        <span class=\"pl-c1\">break</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Safe to execute a model</span>\nsess.run(target)</pre></div>", "body_text": "Hi @smit-hinsu , the source code of VGG16 is a keras-based implementation and executed with Tensorflow backend, and all each the time I posted stand for the real time spent to finish 1 epoch training, this is related to the scale of input data so absolute value is meaningless here.\nHuge model training I just want a friendly way to get over BFC-overflow problem when using tensorflow, because the overflow is slow, not catch-able by python runtime and it is even hard to kill. Using GpuManagedAlloc is a suitable way to avoid such bad status for huge model.\nModel migration problem User sensitive to absolute memory is definitely needed because they want to not only evaluate the resource usage for their defined models, but also know a model is runnable on a certain GPU from a hybrid cluster. However, considering the absolute memory is related to both model itself and also the batch_size, this value might not be a global constant one, so I recently think it would be better to be able to pre-evaluate the memory occupation before sess.run, something like:\nmemory, .. = sess.evalutate_resources(target, ...)\nsess.run(target, ...)\nThe value of memory can be a map of different devices, e.g:\nmemory = {\n    '/gpu:0': 6000000000,\n    '/gpu:1': 0\n}\nThus, user can judge whether their model can fit into an unknown GPU, and also able to adjust a suitable batch_size completed before real session run.\nfree_memory = cuda.getMemFree(..)\nbatch_size = 256\nwhile True:\n    target = make_model(.., batch_size)\n    if sess.evaluate_memory(target) > free_memory:\n        batch_size /= 2\n    else:\n        break\n# Safe to execute a model\nsess.run(target)", "body": "Hi @smit-hinsu , the source code of VGG16 is a keras-based implementation and executed with Tensorflow backend, and all each the time I posted stand for the real time spent to finish 1 epoch training, this is related to the scale of input data so absolute value is meaningless here.\r\n\r\n**Huge model training** I just want a friendly way to get over BFC-overflow problem when using tensorflow, because the overflow is slow, not catch-able by python runtime and it is even hard to kill. Using GpuManagedAlloc is a suitable way to avoid such bad status for huge model.\r\n\r\n**Model migration problem** User sensitive to absolute memory is definitely needed because they want to not only evaluate the resource usage for their defined models, but also know a model is runnable on a certain GPU from a hybrid cluster. However, considering the absolute memory is related to both model itself and also the batch_size, this value might not be a global constant one, so I recently think it would be better to be able to pre-evaluate the memory occupation before sess.run, something like:\r\n\r\n```sh\r\nmemory, .. = sess.evalutate_resources(target, ...)\r\nsess.run(target, ...)\r\n```\r\nThe value of memory can be a map of different devices, e.g:\r\n```sh\r\nmemory = {\r\n    '/gpu:0': 6000000000,\r\n    '/gpu:1': 0\r\n}\r\n```\r\n\r\nThus, user can judge whether their model can fit into an unknown GPU, and also able to adjust a suitable batch_size completed before real session run.\r\n\r\n```sh\r\nfree_memory = cuda.getMemFree(..)\r\nbatch_size = 256\r\nwhile True:\r\n    target = make_model(.., batch_size)\r\n    if sess.evaluate_memory(target) > free_memory:\r\n        batch_size /= 2\r\n    else:\r\n        break\r\n# Safe to execute a model\r\nsess.run(target)\r\n```\r\n"}