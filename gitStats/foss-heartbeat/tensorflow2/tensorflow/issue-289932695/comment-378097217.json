{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/378097217", "html_url": "https://github.com/tensorflow/tensorflow/issues/16239#issuecomment-378097217", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16239", "id": 378097217, "node_id": "MDEyOklzc3VlQ29tbWVudDM3ODA5NzIxNw==", "user": {"login": "smit-hinsu", "id": 1990079, "node_id": "MDQ6VXNlcjE5OTAwNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/1990079?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smit-hinsu", "html_url": "https://github.com/smit-hinsu", "followers_url": "https://api.github.com/users/smit-hinsu/followers", "following_url": "https://api.github.com/users/smit-hinsu/following{/other_user}", "gists_url": "https://api.github.com/users/smit-hinsu/gists{/gist_id}", "starred_url": "https://api.github.com/users/smit-hinsu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smit-hinsu/subscriptions", "organizations_url": "https://api.github.com/users/smit-hinsu/orgs", "repos_url": "https://api.github.com/users/smit-hinsu/repos", "events_url": "https://api.github.com/users/smit-hinsu/events{/privacy}", "received_events_url": "https://api.github.com/users/smit-hinsu/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-03T01:18:11Z", "updated_at": "2018-04-03T01:42:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry for the delay in the follow-up.</p>\n<p>I ran various experiments with some of the models in tf_cnn_benchmarks to observe model throughput using different batch sizes with cuMemAlloc and cuMemAllocManaged. It seems that cuMemAllocManaged does not really help increase the peak throughput observed across different batch sizes for a model. So, it should be possible to increase throughput for vgg16 model by adjusting the batch size without using batch size of 8192 and managed memory. By the way, the reported runtime for vgg16 model seems far too low for it to be the per-step duration. It should be possible to get much higher throughput for vgg16 using P100. Is that duration for N steps or something else?</p>\n<p>Now going back to the two points you mentioned in your initial comments:</p>\n<p><strong>Huge model training</strong>: As zheng-xq mentioned earlier, memory requirement for huge models can be decreased by decreasing the batch size. Using higher batch size by using managed memory does not seem to be giving a higher throughput. On the flip side, this can significantly slow down the training. This can be observed in the runtime you reported for Resnet20 model with batch size of 8192 which runs 6x times slower compared to batch size of 4096. I believe the slower model will be much more difficult to debug compared to the BFC out-of-memory failure.</p>\n<p><strong>Model migration problem</strong>: I believe, it is worth considering the absolute memory limits support along with specifying it as a fraction of the GPU memory. Just to further understand your use-case, could you provide more details about your use-case that will benefit with such an option? For example, do you generally run multiple models concurrently on a single GPU? If that is the case, then the scheduler will still need information about the total memory available in a particular GPU to decide how many models can safely run at the same time. If that is true then it should be possible to get memory requirement as a fraction of GPU memory based on the absolute limit and total memory in the GPU. However, I believe your use-case is different.</p>\n<p>Thanks for reporting this and also sharing the experiment results.</p>", "body_text": "Sorry for the delay in the follow-up.\nI ran various experiments with some of the models in tf_cnn_benchmarks to observe model throughput using different batch sizes with cuMemAlloc and cuMemAllocManaged. It seems that cuMemAllocManaged does not really help increase the peak throughput observed across different batch sizes for a model. So, it should be possible to increase throughput for vgg16 model by adjusting the batch size without using batch size of 8192 and managed memory. By the way, the reported runtime for vgg16 model seems far too low for it to be the per-step duration. It should be possible to get much higher throughput for vgg16 using P100. Is that duration for N steps or something else?\nNow going back to the two points you mentioned in your initial comments:\nHuge model training: As zheng-xq mentioned earlier, memory requirement for huge models can be decreased by decreasing the batch size. Using higher batch size by using managed memory does not seem to be giving a higher throughput. On the flip side, this can significantly slow down the training. This can be observed in the runtime you reported for Resnet20 model with batch size of 8192 which runs 6x times slower compared to batch size of 4096. I believe the slower model will be much more difficult to debug compared to the BFC out-of-memory failure.\nModel migration problem: I believe, it is worth considering the absolute memory limits support along with specifying it as a fraction of the GPU memory. Just to further understand your use-case, could you provide more details about your use-case that will benefit with such an option? For example, do you generally run multiple models concurrently on a single GPU? If that is the case, then the scheduler will still need information about the total memory available in a particular GPU to decide how many models can safely run at the same time. If that is true then it should be possible to get memory requirement as a fraction of GPU memory based on the absolute limit and total memory in the GPU. However, I believe your use-case is different.\nThanks for reporting this and also sharing the experiment results.", "body": "Sorry for the delay in the follow-up.\r\n\r\nI ran various experiments with some of the models in tf_cnn_benchmarks to observe model throughput using different batch sizes with cuMemAlloc and cuMemAllocManaged. It seems that cuMemAllocManaged does not really help increase the peak throughput observed across different batch sizes for a model. So, it should be possible to increase throughput for vgg16 model by adjusting the batch size without using batch size of 8192 and managed memory. By the way, the reported runtime for vgg16 model seems far too low for it to be the per-step duration. It should be possible to get much higher throughput for vgg16 using P100. Is that duration for N steps or something else? \r\n\r\nNow going back to the two points you mentioned in your initial comments:\r\n\r\n**Huge model training**: As zheng-xq mentioned earlier, memory requirement for huge models can be decreased by decreasing the batch size. Using higher batch size by using managed memory does not seem to be giving a higher throughput. On the flip side, this can significantly slow down the training. This can be observed in the runtime you reported for Resnet20 model with batch size of 8192 which runs 6x times slower compared to batch size of 4096. I believe the slower model will be much more difficult to debug compared to the BFC out-of-memory failure.\r\n\r\n**Model migration problem**: I believe, it is worth considering the absolute memory limits support along with specifying it as a fraction of the GPU memory. Just to further understand your use-case, could you provide more details about your use-case that will benefit with such an option? For example, do you generally run multiple models concurrently on a single GPU? If that is the case, then the scheduler will still need information about the total memory available in a particular GPU to decide how many models can safely run at the same time. If that is true then it should be possible to get memory requirement as a fraction of GPU memory based on the absolute limit and total memory in the GPU. However, I believe your use-case is different.\r\n\r\nThanks for reporting this and also sharing the experiment results."}