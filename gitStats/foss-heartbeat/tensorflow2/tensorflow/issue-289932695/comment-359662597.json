{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/359662597", "html_url": "https://github.com/tensorflow/tensorflow/issues/16239#issuecomment-359662597", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16239", "id": 359662597, "node_id": "MDEyOklzc3VlQ29tbWVudDM1OTY2MjU5Nw==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-23T03:16:43Z", "updated_at": "2018-01-23T03:16:43Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'd like to see some measurements before enabling this feature.</p>\n<p>In your example, it is unclear whether it is better to ask for 8GB of memory on a 2G GPU and having the Cuda system software swapping around. Or having the model use a smaller batch size. Many models have a batch size lower limit, mainly because of the batchnorm, but it is often much smaller than the regular setting.</p>\n<p>The alternative is to swap memory explicitly in your model, such as set \"swap_memory=True\" in tf.while_loop. When the framework knows your execution schedule, it has more knowledge to make good choice on when to swap the memory before the execution.</p>", "body_text": "I'd like to see some measurements before enabling this feature.\nIn your example, it is unclear whether it is better to ask for 8GB of memory on a 2G GPU and having the Cuda system software swapping around. Or having the model use a smaller batch size. Many models have a batch size lower limit, mainly because of the batchnorm, but it is often much smaller than the regular setting.\nThe alternative is to swap memory explicitly in your model, such as set \"swap_memory=True\" in tf.while_loop. When the framework knows your execution schedule, it has more knowledge to make good choice on when to swap the memory before the execution.", "body": "I'd like to see some measurements before enabling this feature.\r\n\r\nIn your example, it is unclear whether it is better to ask for 8GB of memory on a 2G GPU and having the Cuda system software swapping around. Or having the model use a smaller batch size. Many models have a batch size lower limit, mainly because of the batchnorm, but it is often much smaller than the regular setting.\r\n\r\nThe alternative is to swap memory explicitly in your model, such as set \"swap_memory=True\" in tf.while_loop. When the framework knows your execution schedule, it has more knowledge to make good choice on when to swap the memory before the execution. "}