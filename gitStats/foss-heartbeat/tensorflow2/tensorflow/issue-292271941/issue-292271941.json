{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16532", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16532/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16532/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16532/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16532", "id": 292271941, "node_id": "MDU6SXNzdWUyOTIyNzE5NDE=", "number": 16532, "title": "Keras/TF1.5.0 does not consider TensorBoard embeddings_freq/embeddings_metadata", "user": {"login": "sakaia", "id": 144597, "node_id": "MDQ6VXNlcjE0NDU5Nw==", "avatar_url": "https://avatars0.githubusercontent.com/u/144597?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sakaia", "html_url": "https://github.com/sakaia", "followers_url": "https://api.github.com/users/sakaia/followers", "following_url": "https://api.github.com/users/sakaia/following{/other_user}", "gists_url": "https://api.github.com/users/sakaia/gists{/gist_id}", "starred_url": "https://api.github.com/users/sakaia/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sakaia/subscriptions", "organizations_url": "https://api.github.com/users/sakaia/orgs", "repos_url": "https://api.github.com/users/sakaia/repos", "events_url": "https://api.github.com/users/sakaia/events{/privacy}", "received_events_url": "https://api.github.com/users/sakaia/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-01-29T03:10:41Z", "updated_at": "2018-11-14T19:14:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow) describe in following</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary) binary</strong>:</li>\n<li><strong>TensorFlow version (use command below) 1.5.0</strong>:</li>\n<li><strong>Python version 3.5.1</strong>:</li>\n<li><strong>Bazel version (if compiling from source) NOT USED</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source) NOT USED</strong>:</li>\n<li><strong>CUDA/cuDNN version NOT USED</strong>:</li>\n<li><strong>GPU model and memory NOT USED</strong>:</li>\n<li><strong>Exact command to reproduce python imdb_fasttext.py</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>TF1.5.0/Keras does not consider TensorBoard embeddings_freq/embeddings_metadata.</p>\n<p>This parameter is added in 10 month ago and removed in 5 month ago.<br>\nAnd currently it does not exist</p>\n<ul>\n<li>Current code<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blame/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L606\">https://github.com/tensorflow/tensorflow/blame/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L606</a></li>\n<li>Apr 12, 2017 (10month ago) (add argument)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/b8b8ebcf851df71ebb5209ae27d75e2befc50f0d/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/b8b8ebcf851df71ebb5209ae27d75e2befc50f0d\"><tt>b8b8ebc</tt></a></li>\n<li>Sep 6, 2017 (5month ago) (remove argument)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/eaaa0b93852054dee086a3ed5373cf8bbe3d2fb3/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/eaaa0b93852054dee086a3ed5373cf8bbe3d2fb3\"><tt>eaaa0b9</tt></a></li>\n</ul>\n<p>Which is already supported on Keras 2.1.3.<br>\n<a href=\"https://github.com/keras-team/keras/blame/7d1e0bc5872855af5bf35a725025d3bdb6f07d6c/keras/callbacks.py#L641\">https://github.com/keras-team/keras/blame/7d1e0bc5872855af5bf35a725025d3bdb6f07d6c/keras/callbacks.py#L641</a></p>\n<h3>Source code / logs</h3>\n<p>Current outputs are following. It reports the keyword \"embeddings_metadata\" does not exist.</p>\n<pre><code>C:\\Users\\sakaia\\work\\tensorflow\\keras&gt;python imdb_fasttext.py\nUsing TensorFlow backend.\nLoading data...\n25000 train sequences\n25000 test sequences\nAverage train sequence length: 238\nAverage test sequence length: 230\nPad sequences (samples x time)\nx_train shape: (25000, 400)\nx_test shape: (25000, 400)\nBuild model...\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\p\nython\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.py\nthon.ops.math_ops) with keep_dims is deprecated and will be removed in a future\nversion.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nTraceback (most recent call last):\n  File \"imdb_fasttext.py\", line 146, in &lt;module&gt;\n    embeddings_metadata= embeddingsMetadata\nTypeError: __init__() got an unexpected keyword argument 'embeddings_metadata'\n</code></pre>\n<p>Source code is follows (based on <a href=\"https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py\">https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py</a>)</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-s\"><span class=\"pl-pds\">'''</span>This example demonstrates the use of fasttext for text classification</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Based on Joulin et al's paper:</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Bags of Tricks for Efficient Text Classification</span>\n<span class=\"pl-s\">https://arxiv.org/abs/1607.01759</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">Results on IMDB datasets with uni and bi-gram embeddings:</span>\n<span class=\"pl-s\">    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.</span>\n<span class=\"pl-s\">    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.</span>\n<span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n\n<span class=\"pl-k\">from</span> <span class=\"pl-c1\">__future__</span> <span class=\"pl-k\">import</span> print_function\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Changed from keras to tensorflow.python.keras</span>\n<span class=\"pl-k\">from</span> tensorflow.python.keras.preprocessing <span class=\"pl-k\">import</span> sequence\n<span class=\"pl-k\">from</span> tensorflow.python.keras.models <span class=\"pl-k\">import</span> Sequential\n<span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> Dense\n<span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> Embedding\n<span class=\"pl-k\">from</span> tensorflow.python.keras.layers <span class=\"pl-k\">import</span> GlobalAveragePooling1D\n<span class=\"pl-k\">from</span> tensorflow.python.keras.callbacks <span class=\"pl-k\">import</span> TensorBoard\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Followings are workaround for https://github.com/tensorflow/tensorflow/issues/16358</span>\n<span class=\"pl-k\">from</span> keras.datasets <span class=\"pl-k\">import</span> imdb\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">create_ngram_set</span>(<span class=\"pl-smi\">input_list</span>, <span class=\"pl-smi\">ngram_value</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Extract a set of n-grams from a list of integers.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)</span>\n<span class=\"pl-s\">    {(4, 9), (4, 1), (1, 4), (9, 4)}</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)</span>\n<span class=\"pl-s\">    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> <span class=\"pl-c1\">set</span>(<span class=\"pl-c1\">zip</span>(<span class=\"pl-k\">*</span>[input_list[i:] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(ngram_value)]))\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">add_ngram</span>(<span class=\"pl-smi\">sequences</span>, <span class=\"pl-smi\">token_indice</span>, <span class=\"pl-smi\">ngram_range</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span></span>\n<span class=\"pl-s\">    Augment the input list of list (sequences) by appending n-grams values.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Example: adding bi-gram</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>add_ngram(sequences, token_indice, ngram_range=2)</span>\n<span class=\"pl-s\">    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Example: adding tri-gram</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}</span>\n<span class=\"pl-s\">    <span class=\"pl-k\">&gt;&gt;&gt; </span>add_ngram(sequences, token_indice, ngram_range=3)</span>\n<span class=\"pl-s\">    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    new_sequences <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> input_list <span class=\"pl-k\">in</span> sequences:\n        new_list <span class=\"pl-k\">=</span> input_list[:]\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(new_list) <span class=\"pl-k\">-</span> ngram_range <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n            <span class=\"pl-k\">for</span> ngram_value <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>, ngram_range <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n                ngram <span class=\"pl-k\">=</span> <span class=\"pl-c1\">tuple</span>(new_list[i:i <span class=\"pl-k\">+</span> ngram_value])\n                <span class=\"pl-k\">if</span> ngram <span class=\"pl-k\">in</span> token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    <span class=\"pl-k\">return</span> new_sequences\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Set parameters:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> ngram_range = 2 will add bi-grams features</span>\nngram_range <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\nmax_features <span class=\"pl-k\">=</span> <span class=\"pl-c1\">20000</span>\nmaxlen <span class=\"pl-k\">=</span> <span class=\"pl-c1\">400</span>\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nembedding_dims <span class=\"pl-k\">=</span> <span class=\"pl-c1\">50</span>\nepochs <span class=\"pl-k\">=</span> <span class=\"pl-c1\">5</span>\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Loading data...<span class=\"pl-pds\">'</span></span>)\n(x_train, y_train), (x_test, y_test) <span class=\"pl-k\">=</span> imdb.load_data(<span class=\"pl-v\">num_words</span><span class=\"pl-k\">=</span>max_features)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">len</span>(x_train), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train sequences<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-c1\">len</span>(x_test), <span class=\"pl-s\"><span class=\"pl-pds\">'</span>test sequences<span class=\"pl-pds\">'</span></span>)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average train sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_train)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average test sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_test)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n\n<span class=\"pl-k\">if</span> ngram_range <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">1</span>:\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Adding <span class=\"pl-c1\">{}</span>-gram features<span class=\"pl-pds\">'</span></span>.format(ngram_range))\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create set of unique n-gram from the training set.</span>\n    ngram_set <span class=\"pl-k\">=</span> <span class=\"pl-c1\">set</span>()\n    <span class=\"pl-k\">for</span> input_list <span class=\"pl-k\">in</span> x_train:\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">2</span>, ngram_range <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>):\n            set_of_ngram <span class=\"pl-k\">=</span> create_ngram_set(input_list, <span class=\"pl-v\">ngram_value</span><span class=\"pl-k\">=</span>i)\n            ngram_set.update(set_of_ngram)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Dictionary mapping n-gram token to a unique integer.</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Integer values are greater than max_features in order</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> to avoid collision with existing features.</span>\n    start_index <span class=\"pl-k\">=</span> max_features <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n    token_indice <span class=\"pl-k\">=</span> {v: k <span class=\"pl-k\">+</span> start_index <span class=\"pl-k\">for</span> k, v <span class=\"pl-k\">in</span> <span class=\"pl-c1\">enumerate</span>(ngram_set)}\n    indice_token <span class=\"pl-k\">=</span> {token_indice[k]: k <span class=\"pl-k\">for</span> k <span class=\"pl-k\">in</span> token_indice}\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> max_features is the highest integer that could be found in the dataset.</span>\n    max_features <span class=\"pl-k\">=</span> np.max(<span class=\"pl-c1\">list</span>(indice_token.keys())) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Augmenting x_train and x_test with n-grams features</span>\n    x_train <span class=\"pl-k\">=</span> add_ngram(x_train, token_indice, ngram_range)\n    x_test <span class=\"pl-k\">=</span> add_ngram(x_test, token_indice, ngram_range)\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average train sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_train)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Average test sequence length: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(np.mean(<span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">map</span>(<span class=\"pl-c1\">len</span>, x_test)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">int</span>)))\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Pad sequences (samples x time)<span class=\"pl-pds\">'</span></span>)\nx_train <span class=\"pl-k\">=</span> sequence.pad_sequences(x_train, <span class=\"pl-v\">maxlen</span><span class=\"pl-k\">=</span>maxlen)\nx_test <span class=\"pl-k\">=</span> sequence.pad_sequences(x_test, <span class=\"pl-v\">maxlen</span><span class=\"pl-k\">=</span>maxlen)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x_train shape:<span class=\"pl-pds\">'</span></span>, x_train.shape)\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>x_test shape:<span class=\"pl-pds\">'</span></span>, x_test.shape)\n\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>Build model...<span class=\"pl-pds\">'</span></span>)\nmodel <span class=\"pl-k\">=</span> Sequential()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> we start off with an efficient embedding layer which maps</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> our vocab indices into embedding_dims dimensions</span>\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    <span class=\"pl-v\">input_length</span><span class=\"pl-k\">=</span>maxlen))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> we add a GlobalAveragePooling1D, which will average the embeddings</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> of all words in the document</span>\nmodel.add(GlobalAveragePooling1D())\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> We project onto a single unit output layer, and squash it with a sigmoid:</span>\nmodel.add(Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>))\n\nmodel.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>binary_crossentropy<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>,\n              <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])\nembeddingsMetadata <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>embedding<span class=\"pl-pds\">'</span></span>: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>metadata.tsv<span class=\"pl-pds\">'</span></span>}\nmodel.fit(x_train, y_train,\n          <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span>batch_size,\n          <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span>epochs,\n          <span class=\"pl-v\">validation_data</span><span class=\"pl-k\">=</span>(x_test, y_test),\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> problem occured here!!!</span>\n          <span class=\"pl-v\">callbacks</span><span class=\"pl-k\">=</span>[TensorBoard(<span class=\"pl-v\">log_dir</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">histogram_freq</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">embeddings_freq</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>,\n                                 <span class=\"pl-v\">embeddings_metadata</span><span class=\"pl-k\">=</span> embeddingsMetadata\n                                ),\n          ])</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow) describe in following:\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7:\nTensorFlow installed from (source or binary) binary:\nTensorFlow version (use command below) 1.5.0:\nPython version 3.5.1:\nBazel version (if compiling from source) NOT USED:\nGCC/Compiler version (if compiling from source) NOT USED:\nCUDA/cuDNN version NOT USED:\nGPU model and memory NOT USED:\nExact command to reproduce python imdb_fasttext.py:\n\nDescribe the problem\nTF1.5.0/Keras does not consider TensorBoard embeddings_freq/embeddings_metadata.\nThis parameter is added in 10 month ago and removed in 5 month ago.\nAnd currently it does not exist\n\nCurrent code\nhttps://github.com/tensorflow/tensorflow/blame/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L606\nApr 12, 2017 (10month ago) (add argument)\nb8b8ebc\nSep 6, 2017 (5month ago) (remove argument)\neaaa0b9\n\nWhich is already supported on Keras 2.1.3.\nhttps://github.com/keras-team/keras/blame/7d1e0bc5872855af5bf35a725025d3bdb6f07d6c/keras/callbacks.py#L641\nSource code / logs\nCurrent outputs are following. It reports the keyword \"embeddings_metadata\" does not exist.\nC:\\Users\\sakaia\\work\\tensorflow\\keras>python imdb_fasttext.py\nUsing TensorFlow backend.\nLoading data...\n25000 train sequences\n25000 test sequences\nAverage train sequence length: 238\nAverage test sequence length: 230\nPad sequences (samples x time)\nx_train shape: (25000, 400)\nx_test shape: (25000, 400)\nBuild model...\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\p\nython\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.py\nthon.ops.math_ops) with keep_dims is deprecated and will be removed in a future\nversion.\nInstructions for updating:\nkeep_dims is deprecated, use keepdims instead\nTraceback (most recent call last):\n  File \"imdb_fasttext.py\", line 146, in <module>\n    embeddings_metadata= embeddingsMetadata\nTypeError: __init__() got an unexpected keyword argument 'embeddings_metadata'\n\nSource code is follows (based on https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py)\n'''This example demonstrates the use of fasttext for text classification\n\nBased on Joulin et al's paper:\n\nBags of Tricks for Efficient Text Classification\nhttps://arxiv.org/abs/1607.01759\n\nResults on IMDB datasets with uni and bi-gram embeddings:\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n'''\n\nfrom __future__ import print_function\nimport numpy as np\n\n# Changed from keras to tensorflow.python.keras\nfrom tensorflow.python.keras.preprocessing import sequence\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.python.keras.layers import Embedding\nfrom tensorflow.python.keras.layers import GlobalAveragePooling1D\nfrom tensorflow.python.keras.callbacks import TensorBoard\n# Followings are workaround for https://github.com/tensorflow/tensorflow/issues/16358\nfrom keras.datasets import imdb\n\ndef create_ngram_set(input_list, ngram_value=2):\n    \"\"\"\n    Extract a set of n-grams from a list of integers.\n\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\n\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n    \"\"\"\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n\n\ndef add_ngram(sequences, token_indice, ngram_range=2):\n    \"\"\"\n    Augment the input list of list (sequences) by appending n-grams values.\n\n    Example: adding bi-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n\n    Example: adding tri-gram\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\n    \"\"\"\n    new_sequences = []\n    for input_list in sequences:\n        new_list = input_list[:]\n        for i in range(len(new_list) - ngram_range + 1):\n            for ngram_value in range(2, ngram_range + 1):\n                ngram = tuple(new_list[i:i + ngram_value])\n                if ngram in token_indice:\n                    new_list.append(token_indice[ngram])\n        new_sequences.append(new_list)\n\n    return new_sequences\n\n# Set parameters:\n# ngram_range = 2 will add bi-grams features\nngram_range = 1\nmax_features = 20000\nmaxlen = 400\nbatch_size = 32\nembedding_dims = 50\nepochs = 5\n\nprint('Loading data...')\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\nprint(len(x_train), 'train sequences')\nprint(len(x_test), 'test sequences')\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\nif ngram_range > 1:\n    print('Adding {}-gram features'.format(ngram_range))\n    # Create set of unique n-gram from the training set.\n    ngram_set = set()\n    for input_list in x_train:\n        for i in range(2, ngram_range + 1):\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n            ngram_set.update(set_of_ngram)\n\n    # Dictionary mapping n-gram token to a unique integer.\n    # Integer values are greater than max_features in order\n    # to avoid collision with existing features.\n    start_index = max_features + 1\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n    indice_token = {token_indice[k]: k for k in token_indice}\n\n    # max_features is the highest integer that could be found in the dataset.\n    max_features = np.max(list(indice_token.keys())) + 1\n\n    # Augmenting x_train and x_test with n-grams features\n    x_train = add_ngram(x_train, token_indice, ngram_range)\n    x_test = add_ngram(x_test, token_indice, ngram_range)\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n\nprint('Pad sequences (samples x time)')\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n\nprint('Build model...')\nmodel = Sequential()\n\n# we start off with an efficient embedding layer which maps\n# our vocab indices into embedding_dims dimensions\nmodel.add(Embedding(max_features,\n                    embedding_dims,\n                    input_length=maxlen))\n\n# we add a GlobalAveragePooling1D, which will average the embeddings\n# of all words in the document\nmodel.add(GlobalAveragePooling1D())\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\nembeddingsMetadata = {'embedding': 'metadata.tsv'}\nmodel.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_data=(x_test, y_test),\n# problem occured here!!!\n          callbacks=[TensorBoard(log_dir=\".\", histogram_freq=1, embeddings_freq=1,\n                                 embeddings_metadata= embeddingsMetadata\n                                ),\n          ])", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow) describe in following**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04) Windows 7**:\r\n- **TensorFlow installed from (source or binary) binary**:\r\n- **TensorFlow version (use command below) 1.5.0**:\r\n- **Python version 3.5.1**: \r\n- **Bazel version (if compiling from source) NOT USED**:\r\n- **GCC/Compiler version (if compiling from source) NOT USED**:\r\n- **CUDA/cuDNN version NOT USED**:\r\n- **GPU model and memory NOT USED**:\r\n- **Exact command to reproduce python imdb_fasttext.py**:\r\n\r\n### Describe the problem\r\nTF1.5.0/Keras does not consider TensorBoard embeddings_freq/embeddings_metadata.\r\n\r\nThis parameter is added in 10 month ago and removed in 5 month ago.\r\nAnd currently it does not exist \r\n-  Current code\r\n    https://github.com/tensorflow/tensorflow/blame/579125e87af201ae6b6fa872b6dc3f3ecb400de9/tensorflow/python/keras/_impl/keras/callbacks.py#L606\r\n-  Apr 12, 2017 (10month ago) (add argument)\r\n    https://github.com/tensorflow/tensorflow/commit/b8b8ebcf851df71ebb5209ae27d75e2befc50f0d\r\n- Sep 6, 2017 (5month ago) (remove argument)\r\n    https://github.com/tensorflow/tensorflow/commit/eaaa0b93852054dee086a3ed5373cf8bbe3d2fb3\r\n\r\nWhich is already supported on Keras 2.1.3.\r\nhttps://github.com/keras-team/keras/blame/7d1e0bc5872855af5bf35a725025d3bdb6f07d6c/keras/callbacks.py#L641\r\n\r\n### Source code / logs\r\nCurrent outputs are following. It reports the keyword \"embeddings_metadata\" does not exist. \r\n```\r\nC:\\Users\\sakaia\\work\\tensorflow\\keras>python imdb_fasttext.py\r\nUsing TensorFlow backend.\r\nLoading data...\r\n25000 train sequences\r\n25000 test sequences\r\nAverage train sequence length: 238\r\nAverage test sequence length: 230\r\nPad sequences (samples x time)\r\nx_train shape: (25000, 400)\r\nx_test shape: (25000, 400)\r\nBuild model...\r\nWARNING:tensorflow:From C:\\Program Files\\Python35\\lib\\site-packages\\tensorflow\\p\r\nython\\keras\\_impl\\keras\\backend.py:1557: calling reduce_mean (from tensorflow.py\r\nthon.ops.math_ops) with keep_dims is deprecated and will be removed in a future\r\nversion.\r\nInstructions for updating:\r\nkeep_dims is deprecated, use keepdims instead\r\nTraceback (most recent call last):\r\n  File \"imdb_fasttext.py\", line 146, in <module>\r\n    embeddings_metadata= embeddingsMetadata\r\nTypeError: __init__() got an unexpected keyword argument 'embeddings_metadata'\r\n```\r\n\r\nSource code is follows (based on https://github.com/keras-team/keras/blob/master/examples/imdb_fasttext.py)\r\n```Python\r\n'''This example demonstrates the use of fasttext for text classification\r\n\r\nBased on Joulin et al's paper:\r\n\r\nBags of Tricks for Efficient Text Classification\r\nhttps://arxiv.org/abs/1607.01759\r\n\r\nResults on IMDB datasets with uni and bi-gram embeddings:\r\n    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\r\n    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\r\n'''\r\n\r\nfrom __future__ import print_function\r\nimport numpy as np\r\n\r\n# Changed from keras to tensorflow.python.keras\r\nfrom tensorflow.python.keras.preprocessing import sequence\r\nfrom tensorflow.python.keras.models import Sequential\r\nfrom tensorflow.python.keras.layers import Dense\r\nfrom tensorflow.python.keras.layers import Embedding\r\nfrom tensorflow.python.keras.layers import GlobalAveragePooling1D\r\nfrom tensorflow.python.keras.callbacks import TensorBoard\r\n# Followings are workaround for https://github.com/tensorflow/tensorflow/issues/16358\r\nfrom keras.datasets import imdb\r\n\r\ndef create_ngram_set(input_list, ngram_value=2):\r\n    \"\"\"\r\n    Extract a set of n-grams from a list of integers.\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\r\n    {(4, 9), (4, 1), (1, 4), (9, 4)}\r\n\r\n    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\r\n    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\r\n    \"\"\"\r\n    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\r\n\r\n\r\ndef add_ngram(sequences, token_indice, ngram_range=2):\r\n    \"\"\"\r\n    Augment the input list of list (sequences) by appending n-grams values.\r\n\r\n    Example: adding bi-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=2)\r\n    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\r\n\r\n    Example: adding tri-gram\r\n    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\r\n    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\r\n    >>> add_ngram(sequences, token_indice, ngram_range=3)\r\n    [[1, 3, 4, 5, 1337], [1, 3, 7, 9, 2, 1337, 2018]]\r\n    \"\"\"\r\n    new_sequences = []\r\n    for input_list in sequences:\r\n        new_list = input_list[:]\r\n        for i in range(len(new_list) - ngram_range + 1):\r\n            for ngram_value in range(2, ngram_range + 1):\r\n                ngram = tuple(new_list[i:i + ngram_value])\r\n                if ngram in token_indice:\r\n                    new_list.append(token_indice[ngram])\r\n        new_sequences.append(new_list)\r\n\r\n    return new_sequences\r\n\r\n# Set parameters:\r\n# ngram_range = 2 will add bi-grams features\r\nngram_range = 1\r\nmax_features = 20000\r\nmaxlen = 400\r\nbatch_size = 32\r\nembedding_dims = 50\r\nepochs = 5\r\n\r\nprint('Loading data...')\r\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\r\nprint(len(x_train), 'train sequences')\r\nprint(len(x_test), 'test sequences')\r\nprint('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\nprint('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nif ngram_range > 1:\r\n    print('Adding {}-gram features'.format(ngram_range))\r\n    # Create set of unique n-gram from the training set.\r\n    ngram_set = set()\r\n    for input_list in x_train:\r\n        for i in range(2, ngram_range + 1):\r\n            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\r\n            ngram_set.update(set_of_ngram)\r\n\r\n    # Dictionary mapping n-gram token to a unique integer.\r\n    # Integer values are greater than max_features in order\r\n    # to avoid collision with existing features.\r\n    start_index = max_features + 1\r\n    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\r\n    indice_token = {token_indice[k]: k for k in token_indice}\r\n\r\n    # max_features is the highest integer that could be found in the dataset.\r\n    max_features = np.max(list(indice_token.keys())) + 1\r\n\r\n    # Augmenting x_train and x_test with n-grams features\r\n    x_train = add_ngram(x_train, token_indice, ngram_range)\r\n    x_test = add_ngram(x_test, token_indice, ngram_range)\r\n    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\r\n    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\r\n\r\nprint('Pad sequences (samples x time)')\r\nx_train = sequence.pad_sequences(x_train, maxlen=maxlen)\r\nx_test = sequence.pad_sequences(x_test, maxlen=maxlen)\r\nprint('x_train shape:', x_train.shape)\r\nprint('x_test shape:', x_test.shape)\r\n\r\nprint('Build model...')\r\nmodel = Sequential()\r\n\r\n# we start off with an efficient embedding layer which maps\r\n# our vocab indices into embedding_dims dimensions\r\nmodel.add(Embedding(max_features,\r\n                    embedding_dims,\r\n                    input_length=maxlen))\r\n\r\n# we add a GlobalAveragePooling1D, which will average the embeddings\r\n# of all words in the document\r\nmodel.add(GlobalAveragePooling1D())\r\n\r\n# We project onto a single unit output layer, and squash it with a sigmoid:\r\nmodel.add(Dense(1, activation='sigmoid'))\r\n\r\nmodel.compile(loss='binary_crossentropy',\r\n              optimizer='adam',\r\n              metrics=['accuracy'])\r\nembeddingsMetadata = {'embedding': 'metadata.tsv'}\r\nmodel.fit(x_train, y_train,\r\n          batch_size=batch_size,\r\n          epochs=epochs,\r\n          validation_data=(x_test, y_test),\r\n# problem occured here!!!\r\n          callbacks=[TensorBoard(log_dir=\".\", histogram_freq=1, embeddings_freq=1,\r\n                                 embeddings_metadata= embeddingsMetadata\r\n                                ),\r\n          ])\r\n```\r\n"}