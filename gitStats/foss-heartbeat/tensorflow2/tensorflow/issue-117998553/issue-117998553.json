{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/307", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/307/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/307/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/307/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/307", "id": 117998553, "node_id": "MDU6SXNzdWUxMTc5OTg1NTM=", "number": 307, "title": "Setting large batch_size in logistic regression method will result output nan", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2015-11-20T09:16:55Z", "updated_at": "2016-02-13T13:46:32Z", "closed_at": "2015-11-21T07:10:33Z", "author_association": "NONE", "body_html": "<p>import tensorflow as tf<br>\nimport numpy as np<br>\nimport melt_dataset<br>\nimport sys<br>\nfrom sklearn.metrics import roc_auc_score</p>\n<p>def init_weights(shape):<br>\nreturn tf.Variable(tf.random_normal(shape, stddev=0.01))</p>\n<p>def model(X, w):<br>\nreturn 1.0/(1.0 + tf.exp(-(tf.matmul(X, w))))</p>\n<h1>./logistic_regression.py corpus/feature.normed.rand.12000.0_2.txt corpus/feature.normed.rand.12000.1_2.txt</h1>\n<h1>notice if setting batch_size too big here 500 will result in learning turn output nan...  why? <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=57632\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/todo\">@todo</a></h1>\n<p>batch_size = 100<br>\nlearning_rate = 0.01<br>\nnum_iters = 100</p>\n<p>argv = sys.argv<br>\ntrainset = argv[1]<br>\ntestset = argv[2]</p>\n<p>trX, trY = melt_dataset.load_dense_data(trainset)<br>\nprint \"finish loading train set \",trainset<br>\nteX, teY = melt_dataset.load_dense_data(testset)<br>\nprint \"finish loading test set \", testset</p>\n<p>num_features = trX[0].shape[0]<br>\nprint 'num_features: ',num_features<br>\nprint 'trainSet size: ', len(trX)<br>\nprint 'testSet size: ', len(teX)<br>\nprint 'batch_size:', batch_size, ' learning_rate:', learning_rate, ' num_iters:', num_iters</p>\n<p>X = tf.placeholder(\"float\", [None, num_features]) # create symbolic variables<br>\nY = tf.placeholder(\"float\", [None, 1])</p>\n<p>w = init_weights([num_features, 1]) # like in linear regression, we need a shared variable weight matrix for logistic regression</p>\n<p>py_x = model(X, w)</p>\n<p>cost = -tf.reduce_sum(Y*tf.log(py_x) + (1 - Y) * tf.log(1 - py_x))<br>\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # construct optimizer</p>\n<p>predict_op = py_x</p>\n<p>sess = tf.Session()<br>\ninit = tf.initialize_all_variables()<br>\nsess.run(init)</p>\n<p>for i in range(num_iters):<br>\npredicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})<br>\nprint i, 'auc:', roc_auc_score(teY, predicts), 'cost:', cost_<br>\nfor start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):<br>\nsess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})</p>\n<p>predicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})<br>\nprint 'final ', 'auc:', roc_auc_score(teY, predicts),'cost:', cost_</p>\n<p>if  setting batch_size to 100<br>\n0 auc: 0.595120586861 cost: 1422.4<br>\n1 auc: 0.911648704913 cost: 477.082<br>\n2 auc: 0.916009327839 cost: 459.99<br>\n3 auc: 0.918605639188 cost: 460.72<br>\n4 auc: 0.919915596278 cost: 474.58<br>\n5 auc: 0.920871510912 cost: 487.933<br>\n6 auc: 0.921381332049 cost: 500.054<br>\n7 auc: 0.921853388658 cost: 510.762</p>\n<p>if setting batch_size to 500<br>\n0 auc: 0.615236099113 cost: 1354.38<br>\n1 auc: 0.560017277272 cost: nan<br>\n2 auc: 0.560017277272 cost: nan<br>\n3 auc: 0.560017277272 cost: nan<br>\n4 auc: 0.560017277272 cost: nan<br>\n5 auc: 0.560017277272 cost: nan<br>\n6 auc: 0.560017277272 cost: nan<br>\n7 auc: 0.560017277272 cost: nan<br>\n8 auc: 0.560017277272 cost: nan</p>\n<p>I am a bit curious as I will not face nan for the same train,test data using thenao implemented logistic regression.</p>", "body_text": "import tensorflow as tf\nimport numpy as np\nimport melt_dataset\nimport sys\nfrom sklearn.metrics import roc_auc_score\ndef init_weights(shape):\nreturn tf.Variable(tf.random_normal(shape, stddev=0.01))\ndef model(X, w):\nreturn 1.0/(1.0 + tf.exp(-(tf.matmul(X, w))))\n./logistic_regression.py corpus/feature.normed.rand.12000.0_2.txt corpus/feature.normed.rand.12000.1_2.txt\nnotice if setting batch_size too big here 500 will result in learning turn output nan...  why? @todo\nbatch_size = 100\nlearning_rate = 0.01\nnum_iters = 100\nargv = sys.argv\ntrainset = argv[1]\ntestset = argv[2]\ntrX, trY = melt_dataset.load_dense_data(trainset)\nprint \"finish loading train set \",trainset\nteX, teY = melt_dataset.load_dense_data(testset)\nprint \"finish loading test set \", testset\nnum_features = trX[0].shape[0]\nprint 'num_features: ',num_features\nprint 'trainSet size: ', len(trX)\nprint 'testSet size: ', len(teX)\nprint 'batch_size:', batch_size, ' learning_rate:', learning_rate, ' num_iters:', num_iters\nX = tf.placeholder(\"float\", [None, num_features]) # create symbolic variables\nY = tf.placeholder(\"float\", [None, 1])\nw = init_weights([num_features, 1]) # like in linear regression, we need a shared variable weight matrix for logistic regression\npy_x = model(X, w)\ncost = -tf.reduce_sum(Y*tf.log(py_x) + (1 - Y) * tf.log(1 - py_x))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # construct optimizer\npredict_op = py_x\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\nfor i in range(num_iters):\npredicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})\nprint i, 'auc:', roc_auc_score(teY, predicts), 'cost:', cost_\nfor start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\nsess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\npredicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})\nprint 'final ', 'auc:', roc_auc_score(teY, predicts),'cost:', cost_\nif  setting batch_size to 100\n0 auc: 0.595120586861 cost: 1422.4\n1 auc: 0.911648704913 cost: 477.082\n2 auc: 0.916009327839 cost: 459.99\n3 auc: 0.918605639188 cost: 460.72\n4 auc: 0.919915596278 cost: 474.58\n5 auc: 0.920871510912 cost: 487.933\n6 auc: 0.921381332049 cost: 500.054\n7 auc: 0.921853388658 cost: 510.762\nif setting batch_size to 500\n0 auc: 0.615236099113 cost: 1354.38\n1 auc: 0.560017277272 cost: nan\n2 auc: 0.560017277272 cost: nan\n3 auc: 0.560017277272 cost: nan\n4 auc: 0.560017277272 cost: nan\n5 auc: 0.560017277272 cost: nan\n6 auc: 0.560017277272 cost: nan\n7 auc: 0.560017277272 cost: nan\n8 auc: 0.560017277272 cost: nan\nI am a bit curious as I will not face nan for the same train,test data using thenao implemented logistic regression.", "body": "import tensorflow as tf\nimport numpy as np\nimport melt_dataset\nimport sys\nfrom sklearn.metrics import roc_auc_score\n\ndef init_weights(shape):\n    return tf.Variable(tf.random_normal(shape, stddev=0.01))\n\ndef model(X, w):\n    return 1.0/(1.0 + tf.exp(-(tf.matmul(X, w))))\n# ./logistic_regression.py corpus/feature.normed.rand.12000.0_2.txt corpus/feature.normed.rand.12000.1_2.txt\n# notice if setting batch_size too big here 500 will result in learning turn output nan...  why? @TODO\n\nbatch_size = 100\nlearning_rate = 0.01\nnum_iters = 100\n\nargv = sys.argv \ntrainset = argv[1]\ntestset = argv[2]\n\ntrX, trY = melt_dataset.load_dense_data(trainset)\nprint \"finish loading train set \",trainset\nteX, teY = melt_dataset.load_dense_data(testset)\nprint \"finish loading test set \", testset\n\nnum_features = trX[0].shape[0]\nprint 'num_features: ',num_features \nprint 'trainSet size: ', len(trX)\nprint 'testSet size: ', len(teX)\nprint 'batch_size:', batch_size, ' learning_rate:', learning_rate, ' num_iters:', num_iters\n\nX = tf.placeholder(\"float\", [None, num_features]) # create symbolic variables\nY = tf.placeholder(\"float\", [None, 1])\n\nw = init_weights([num_features, 1]) # like in linear regression, we need a shared variable weight matrix for logistic regression\n\npy_x = model(X, w)\n\ncost = -tf.reduce_sum(Y*tf.log(py_x) + (1 - Y) \\* tf.log(1 - py_x))\ntrain_op = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # construct optimizer\n\npredict_op = py_x\n\nsess = tf.Session()\ninit = tf.initialize_all_variables()\nsess.run(init)\n\nfor i in range(num_iters):\n    predicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})\n    print i, 'auc:', roc_auc_score(teY, predicts), 'cost:', cost_\n    for start, end in zip(range(0, len(trX), batch_size), range(batch_size, len(trX), batch_size)):\n            sess.run(train_op, feed_dict={X: trX[start:end], Y: trY[start:end]})\n\npredicts, cost_ = sess.run([predict_op, cost], feed_dict={X: teX, Y: teY})\nprint 'final ', 'auc:', roc_auc_score(teY, predicts),'cost:', cost_\n\nif  setting batch_size to 100\n0 auc: 0.595120586861 cost: 1422.4\n1 auc: 0.911648704913 cost: 477.082\n2 auc: 0.916009327839 cost: 459.99\n3 auc: 0.918605639188 cost: 460.72\n4 auc: 0.919915596278 cost: 474.58\n5 auc: 0.920871510912 cost: 487.933\n6 auc: 0.921381332049 cost: 500.054\n7 auc: 0.921853388658 cost: 510.762\n\nif setting batch_size to 500\n0 auc: 0.615236099113 cost: 1354.38\n1 auc: 0.560017277272 cost: nan\n2 auc: 0.560017277272 cost: nan\n3 auc: 0.560017277272 cost: nan\n4 auc: 0.560017277272 cost: nan\n5 auc: 0.560017277272 cost: nan\n6 auc: 0.560017277272 cost: nan\n7 auc: 0.560017277272 cost: nan\n8 auc: 0.560017277272 cost: nan\n\nI am a bit curious as I will not face nan for the same train,test data using thenao implemented logistic regression.\n"}