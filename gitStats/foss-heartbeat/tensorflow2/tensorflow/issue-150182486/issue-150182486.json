{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2054", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2054/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2054/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2054/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2054", "id": 150182486, "node_id": "MDU6SXNzdWUxNTAxODI0ODY=", "number": 2054, "title": "Manual placement on GPU of a custom operator with both CPU and GPU implementation will always run the CPU version", "user": {"login": "kbrems", "id": 456665, "node_id": "MDQ6VXNlcjQ1NjY2NQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/456665?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kbrems", "html_url": "https://github.com/kbrems", "followers_url": "https://api.github.com/users/kbrems/followers", "following_url": "https://api.github.com/users/kbrems/following{/other_user}", "gists_url": "https://api.github.com/users/kbrems/gists{/gist_id}", "starred_url": "https://api.github.com/users/kbrems/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kbrems/subscriptions", "organizations_url": "https://api.github.com/users/kbrems/orgs", "repos_url": "https://api.github.com/users/kbrems/repos", "events_url": "https://api.github.com/users/kbrems/events{/privacy}", "received_events_url": "https://api.github.com/users/kbrems/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "keveman", "id": 229914, "node_id": "MDQ6VXNlcjIyOTkxNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/229914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keveman", "html_url": "https://github.com/keveman", "followers_url": "https://api.github.com/users/keveman/followers", "following_url": "https://api.github.com/users/keveman/following{/other_user}", "gists_url": "https://api.github.com/users/keveman/gists{/gist_id}", "starred_url": "https://api.github.com/users/keveman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keveman/subscriptions", "organizations_url": "https://api.github.com/users/keveman/orgs", "repos_url": "https://api.github.com/users/keveman/repos", "events_url": "https://api.github.com/users/keveman/events{/privacy}", "received_events_url": "https://api.github.com/users/keveman/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "keveman", "id": 229914, "node_id": "MDQ6VXNlcjIyOTkxNA==", "avatar_url": "https://avatars1.githubusercontent.com/u/229914?v=4", "gravatar_id": "", "url": "https://api.github.com/users/keveman", "html_url": "https://github.com/keveman", "followers_url": "https://api.github.com/users/keveman/followers", "following_url": "https://api.github.com/users/keveman/following{/other_user}", "gists_url": "https://api.github.com/users/keveman/gists{/gist_id}", "starred_url": "https://api.github.com/users/keveman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/keveman/subscriptions", "organizations_url": "https://api.github.com/users/keveman/orgs", "repos_url": "https://api.github.com/users/keveman/repos", "events_url": "https://api.github.com/users/keveman/events{/privacy}", "received_events_url": "https://api.github.com/users/keveman/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2016-04-21T20:27:20Z", "updated_at": "2017-02-09T22:02:16Z", "closed_at": "2016-06-06T21:19:02Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>Environment info</h3>\n<p>Operating System: ubuntu 14.04 64-bit</p>\n<p>Installed version of CUDA and cuDNN: 7.5 and 4<br>\n$ ls -l /usr/local/cuda/lib64/libcud*<br>\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -&gt; libcudart.so.7.5<br>\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18<br>\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a<br>\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so<br>\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4<br>\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4.0.7<br>\n-rw-r--r-- 1 root root 62025862 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn_static.a</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed. tensorflow==0.8.0rc0</li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\".<br>\npython -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\"<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally<br>\n0.8.0rc0</li>\n</ol>\n<h3>Steps to reproduce</h3>\n<ol>\n<li>Modify the How-to example cuda_op_kernel.cc to create both a CPU and GPU implementation for the AddOneOp operator</li>\n<li>Make the GPU version do a different operation - I added 2 instead of 1 - so you can verify which version is running</li>\n<li>Change the input and output type from int32 to float. This step is bizarre but critical!</li>\n<li>Test the operator with manual placement - ie. with tf.device('/gpu:0'). This has to be done NOT in the self.test_session as in the example, but rather with a regular tf session - ie:  with tf.Session(config=tf.ConfigProto(log_device_placement=True)). This step is also critical. The tf.test.TestCase.test_session()  masks the issue.</li>\n<li>The operator will run the CPU version despite the placer saying it is being placed on the GPU and the test fails.<br>\n$ python cuda_op_unittest.py<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\nname: GeForce GTX TITAN<br>\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928<br>\npciBusID 0000:05:00.0<br>\nTotal memory: 6.00GiB<br>\nFree memory: 5.29GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)<br>\nDevice mapping:<br>\n/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0<br>\nI tensorflow/core/common_runtime/direct_session.cc:149] Device mapping:<br>\n/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0</li>\n</ol>\n<p>AddOne/input: /job:localhost/replica:0/task:0/gpu:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne/input: /job:localhost/replica:0/task:0/gpu:0<br>\nAddOne: /job:localhost/replica:0/task:0/gpu:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne: /job:localhost/replica:0/task:0/gpu:0<br>\n*** running on CPU ***</p>\n<h1>F</h1>\n<h2>FAIL: test (<strong>main</strong>.AddOneTest)</h2>\n<p>Traceback (most recent call last):<br>\nFile \"cuda_op_unittest.py\", line 30, in test<br>\nassert allclose(result.eval(), [7.0, 6.0, 5.0, 4.0, 3.0])<br>\nAssertionError</p>\n<hr>\n<p>Ran 1 test in 0.342s</p>\n<p>FAILED (failures=1)</p>\n<h3>What have you tried?</h3>\n<p>We first noticed this on a much more complicated custom operator. This is a regression from the tensorflow version 0.7.1, which worked for us. The steps above are the result of several days spent trying to reproduce the problem with a minimal operator. The critical things seem to be using floats instead of ints and using the standard session instead of the tf test one.</p>\n<p>Note also, that if I comment out the REGISTER_KERNEL_BUILDER line for the CPU and try to run only on the gpu device, the test passes, but the executor is still trying to create a CPU version. Log looks like:<br>\n$ python cuda_op_unittest.py<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:<br>\nname: GeForce GTX TITAN<br>\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928<br>\npciBusID 0000:05:00.0<br>\nTotal memory: 6.00GiB<br>\nFree memory: 5.29GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)<br>\nDevice mapping:<br>\n/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0<br>\nI tensorflow/core/common_runtime/direct_session.cc:149] Device mapping:<br>\n/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0</p>\n<p>AddOne/input: /job:localhost/replica:0/task:0/gpu:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne/input: /job:localhost/replica:0/task:0/gpu:0<br>\nAddOne: /job:localhost/replica:0/task:0/gpu:0<br>\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne: /job:localhost/replica:0/task:0/gpu:0<br>\nE tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Not found: No registered 'AddOne' OpKernel for CPU devices compatible with node AddOne = AddOne<a href=\"AddOne/input\">_device=\"/job:localhost/replica:0/task:0/gpu:0\"</a><br>\n[[Node: AddOne = AddOne<a href=\"AddOne/input\">_device=\"/job:localhost/replica:0/task:0/gpu:0\"</a>]]<br>\n*** running on GPU ***</p>\n<h2>.</h2>\n<p>Ran 1 test in 0.552s</p>\n<p>OK</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>attaching source files. To build I do:<br>\n/usr/local/cuda/bin/nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc -I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC<br>\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc cuda_op_kernel.cu.o -I $TF_INC -fPIC -Wl,-rpath .<br>\npython cuda_op_unittest.py</p>\n<p>I am running python 2.7.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/230677/cuda_op.tar.gz\">cuda_op.tar.gz</a></p>", "body_text": "Environment info\nOperating System: ubuntu 14.04 64-bit\nInstalled version of CUDA and cuDNN: 7.5 and 4\n$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed. tensorflow==0.8.0rc0\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\npython -c \"import tensorflow; print(tensorflow.version)\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0rc0\n\nSteps to reproduce\n\nModify the How-to example cuda_op_kernel.cc to create both a CPU and GPU implementation for the AddOneOp operator\nMake the GPU version do a different operation - I added 2 instead of 1 - so you can verify which version is running\nChange the input and output type from int32 to float. This step is bizarre but critical!\nTest the operator with manual placement - ie. with tf.device('/gpu:0'). This has to be done NOT in the self.test_session as in the example, but rather with a regular tf session - ie:  with tf.Session(config=tf.ConfigProto(log_device_placement=True)). This step is also critical. The tf.test.TestCase.test_session()  masks the issue.\nThe operator will run the CPU version despite the placer saying it is being placed on the GPU and the test fails.\n$ python cuda_op_unittest.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.29GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nI tensorflow/core/common_runtime/direct_session.cc:149] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\n\nAddOne/input: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne/input: /job:localhost/replica:0/task:0/gpu:0\nAddOne: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne: /job:localhost/replica:0/task:0/gpu:0\n*** running on CPU ***\nF\nFAIL: test (main.AddOneTest)\nTraceback (most recent call last):\nFile \"cuda_op_unittest.py\", line 30, in test\nassert allclose(result.eval(), [7.0, 6.0, 5.0, 4.0, 3.0])\nAssertionError\n\nRan 1 test in 0.342s\nFAILED (failures=1)\nWhat have you tried?\nWe first noticed this on a much more complicated custom operator. This is a regression from the tensorflow version 0.7.1, which worked for us. The steps above are the result of several days spent trying to reproduce the problem with a minimal operator. The critical things seem to be using floats instead of ints and using the standard session instead of the tf test one.\nNote also, that if I comment out the REGISTER_KERNEL_BUILDER line for the CPU and try to run only on the gpu device, the test passes, but the executor is still trying to create a CPU version. Log looks like:\n$ python cuda_op_unittest.py\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties:\nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.29GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nI tensorflow/core/common_runtime/direct_session.cc:149] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nAddOne/input: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne/input: /job:localhost/replica:0/task:0/gpu:0\nAddOne: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne: /job:localhost/replica:0/task:0/gpu:0\nE tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Not found: No registered 'AddOne' OpKernel for CPU devices compatible with node AddOne = AddOne_device=\"/job:localhost/replica:0/task:0/gpu:0\"\n[[Node: AddOne = AddOne_device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n*** running on GPU ***\n.\nRan 1 test in 0.552s\nOK\nLogs or other output that would be helpful\nattaching source files. To build I do:\n/usr/local/cuda/bin/nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc -I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc cuda_op_kernel.cu.o -I $TF_INC -fPIC -Wl,-rpath .\npython cuda_op_unittest.py\nI am running python 2.7.\ncuda_op.tar.gz", "body": "### Environment info\n\nOperating System: ubuntu 14.04 64-bit\n\nInstalled version of CUDA and cuDNN: 7.5 and 4\n$ ls -l /usr/local/cuda/lib64/libcud*\n-rw-r--r-- 1 root root   322936 Aug 15  2015 /usr/local/cuda/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root       19 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root   383336 Aug 15  2015 /usr/local/cuda/lib64/libcudart.so.7.5.18\n-rw-r--r-- 1 root root   720192 Aug 15  2015 /usr/local/cuda/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4\n-rwxr-xr-x 1 root root 61453024 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn.so.4.0.7\n-rw-r--r-- 1 root root 62025862 Feb 23 16:00 /usr/local/cuda/lib64/libcudnn_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed. tensorflow==0.8.0rc0\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\npython -c \"import tensorflow; print(tensorflow.**version**)\"\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n0.8.0rc0\n### Steps to reproduce\n1. Modify the How-to example cuda_op_kernel.cc to create both a CPU and GPU implementation for the AddOneOp operator\n2. Make the GPU version do a different operation - I added 2 instead of 1 - so you can verify which version is running\n3. Change the input and output type from int32 to float. This step is bizarre but critical!\n4. Test the operator with manual placement - ie. with tf.device('/gpu:0'). This has to be done NOT in the self.test_session as in the example, but rather with a regular tf session - ie:  with tf.Session(config=tf.ConfigProto(log_device_placement=True)). This step is also critical. The tf.test.TestCase.test_session()  masks the issue. \n5. The operator will run the CPU version despite the placer saying it is being placed on the GPU and the test fails. \n   $ python cuda_op_unittest.py \n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\n   I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\n   I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \n   name: GeForce GTX TITAN\n   major: 3 minor: 5 memoryClockRate (GHz) 0.928\n   pciBusID 0000:05:00.0\n   Total memory: 6.00GiB\n   Free memory: 5.29GiB\n   I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \n   I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \n   I tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\n   Device mapping:\n   /job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\n   I tensorflow/core/common_runtime/direct_session.cc:149] Device mapping:\n   /job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\n\nAddOne/input: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne/input: /job:localhost/replica:0/task:0/gpu:0\nAddOne: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne: /job:localhost/replica:0/task:0/gpu:0\n**\\* running on CPU ***\n# F\n## FAIL: test (**main**.AddOneTest)\n\nTraceback (most recent call last):\n  File \"cuda_op_unittest.py\", line 30, in test\n    assert allclose(result.eval(), [7.0, 6.0, 5.0, 4.0, 3.0])\nAssertionError\n\n---\n\nRan 1 test in 0.342s\n\nFAILED (failures=1)\n### What have you tried?\n\nWe first noticed this on a much more complicated custom operator. This is a regression from the tensorflow version 0.7.1, which worked for us. The steps above are the result of several days spent trying to reproduce the problem with a minimal operator. The critical things seem to be using floats instead of ints and using the standard session instead of the tf test one. \n\nNote also, that if I comment out the REGISTER_KERNEL_BUILDER line for the CPU and try to run only on the gpu device, the test passes, but the executor is still trying to create a CPU version. Log looks like:\n$ python cuda_op_unittest.py \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN\nmajor: 3 minor: 5 memoryClockRate (GHz) 0.928\npciBusID 0000:05:00.0\nTotal memory: 6.00GiB\nFree memory: 5.29GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0)\nDevice mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\nI tensorflow/core/common_runtime/direct_session.cc:149] Device mapping:\n/job:localhost/replica:0/task:0/gpu:0 -> device: 0, name: GeForce GTX TITAN, pci bus id: 0000:05:00.0\n\nAddOne/input: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne/input: /job:localhost/replica:0/task:0/gpu:0\nAddOne: /job:localhost/replica:0/task:0/gpu:0\nI tensorflow/core/common_runtime/simple_placer.cc:388] AddOne: /job:localhost/replica:0/task:0/gpu:0\nE tensorflow/core/common_runtime/executor.cc:332] Executor failed to create kernel. Not found: No registered 'AddOne' OpKernel for CPU devices compatible with node AddOne = AddOne[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](AddOne/input)\n     [[Node: AddOne = AddOne[_device=\"/job:localhost/replica:0/task:0/gpu:0\"](AddOne/input)]]\n**\\* running on GPU ***\n## .\n\nRan 1 test in 0.552s\n\nOK\n### Logs or other output that would be helpful\n\nattaching source files. To build I do:\n/usr/local/cuda/bin/nvcc -std=c++11 -c -o cuda_op_kernel.cu.o cuda_op_kernel.cu.cc -I $TF_INC -D GOOGLE_CUDA=1 -x cu -Xcompiler -fPIC\ng++ -std=c++11 -shared -o cuda_op_kernel.so cuda_op_kernel.cc cuda_op_kernel.cu.o -I $TF_INC -fPIC -Wl,-rpath .\npython cuda_op_unittest.py\n\nI am running python 2.7. \n\n[cuda_op.tar.gz](https://github.com/tensorflow/tensorflow/files/230677/cuda_op.tar.gz)\n"}