{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/804", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/804/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/804/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/804/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/804", "id": 127320186, "node_id": "MDU6SXNzdWUxMjczMjAxODY=", "number": 804, "title": "Applying batch normalization to a non-convolutional layer fails due to restriction of input to rank 4", "user": {"login": "panmari", "id": 719020, "node_id": "MDQ6VXNlcjcxOTAyMA==", "avatar_url": "https://avatars1.githubusercontent.com/u/719020?v=4", "gravatar_id": "", "url": "https://api.github.com/users/panmari", "html_url": "https://github.com/panmari", "followers_url": "https://api.github.com/users/panmari/followers", "following_url": "https://api.github.com/users/panmari/following{/other_user}", "gists_url": "https://api.github.com/users/panmari/gists{/gist_id}", "starred_url": "https://api.github.com/users/panmari/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/panmari/subscriptions", "organizations_url": "https://api.github.com/users/panmari/orgs", "repos_url": "https://api.github.com/users/panmari/repos", "events_url": "https://api.github.com/users/panmari/events{/privacy}", "received_events_url": "https://api.github.com/users/panmari/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-01-18T22:05:02Z", "updated_at": "2017-02-09T22:37:38Z", "closed_at": "2016-06-08T22:59:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Starting from <a href=\"http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top\" rel=\"nofollow\">this top answer of stackoverflow</a>, I tried to make batch normalization work for a fully connected layer. This is the relevant part of the code:</p>\n<pre><code>            if self.convolutional:\n              mean, variance = tf.nn.moments(x, [0, 1, 2])\n            else:\n              mean, variance = tf.nn.moments(x, [0])\n            assign_mean = self.mean.assign(mean)\n            assign_variance = self.variance.assign(variance)\n            with tf.control_dependencies([assign_mean, assign_variance]):\n                return tf.nn.batch_norm_with_global_normalization(\n                    x, mean, variance, self.beta, self.gamma, self.epsilon,\n                    scale_after_normalization=self.scale_after_normalization)\n</code></pre>\n<p>The call to tf.nn.batch_norm_with_global_normalization fails however:</p>\n<pre><code>  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 104, in batch_norm_with_global_normalization\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 659, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1896, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1524, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 313, in _BatchNormShape\n    input_shape = op.inputs[0].get_shape().with_rank(4)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 614, in with_rank\n    return self.merge_with(unknown_shape(ndims=rank))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 542, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 585, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes (64, 512) and (?, ?, ?, ?) must have the same rank\n</code></pre>\n<p>since, as far as I can tell, the input is constraint to have rank 4 in in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L313\">nn_ops.py</a>. Is this really a necessary constraint?</p>", "body_text": "Starting from this top answer of stackoverflow, I tried to make batch normalization work for a fully connected layer. This is the relevant part of the code:\n            if self.convolutional:\n              mean, variance = tf.nn.moments(x, [0, 1, 2])\n            else:\n              mean, variance = tf.nn.moments(x, [0])\n            assign_mean = self.mean.assign(mean)\n            assign_variance = self.variance.assign(variance)\n            with tf.control_dependencies([assign_mean, assign_variance]):\n                return tf.nn.batch_norm_with_global_normalization(\n                    x, mean, variance, self.beta, self.gamma, self.epsilon,\n                    scale_after_normalization=self.scale_after_normalization)\n\nThe call to tf.nn.batch_norm_with_global_normalization fails however:\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 104, in batch_norm_with_global_normalization\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 659, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1896, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1524, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 313, in _BatchNormShape\n    input_shape = op.inputs[0].get_shape().with_rank(4)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 614, in with_rank\n    return self.merge_with(unknown_shape(ndims=rank))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 542, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 585, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes (64, 512) and (?, ?, ?, ?) must have the same rank\n\nsince, as far as I can tell, the input is constraint to have rank 4 in in nn_ops.py. Is this really a necessary constraint?", "body": "Starting from [this top answer of stackoverflow](http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow?answertab=votes#tab-top), I tried to make batch normalization work for a fully connected layer. This is the relevant part of the code:\n\n```\n            if self.convolutional:\n              mean, variance = tf.nn.moments(x, [0, 1, 2])\n            else:\n              mean, variance = tf.nn.moments(x, [0])\n            assign_mean = self.mean.assign(mean)\n            assign_variance = self.variance.assign(variance)\n            with tf.control_dependencies([assign_mean, assign_variance]):\n                return tf.nn.batch_norm_with_global_normalization(\n                    x, mean, variance, self.beta, self.gamma, self.epsilon,\n                    scale_after_normalization=self.scale_after_normalization)\n```\n\nThe call to tf.nn.batch_norm_with_global_normalization fails however:\n\n```\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 104, in batch_norm_with_global_normalization\n    name=name)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/op_def_library.py\", line 659, in apply_op\n    op_def=op_def)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1896, in create_op\n    set_shapes_for_outputs(ret)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/ops.py\", line 1524, in set_shapes_for_outputs\n    shapes = shape_func(op)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/nn_ops.py\", line 313, in _BatchNormShape\n    input_shape = op.inputs[0].get_shape().with_rank(4)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 614, in with_rank\n    return self.merge_with(unknown_shape(ndims=rank))\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 542, in merge_with\n    self.assert_same_rank(other)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/framework/tensor_shape.py\", line 585, in assert_same_rank\n    \"Shapes %s and %s must have the same rank\" % (self, other))\nValueError: Shapes (64, 512) and (?, ?, ?, ?) must have the same rank\n```\n\nsince, as far as I can tell, the input is constraint to have rank 4 in in [nn_ops.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_ops.py#L313). Is this really a necessary constraint?\n"}