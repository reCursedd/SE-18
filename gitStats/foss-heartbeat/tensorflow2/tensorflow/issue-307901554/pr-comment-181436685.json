{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/181436685", "pull_request_review_id": 111844974, "id": 181436685, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MTQzNjY4NQ==", "diff_hunk": "@@ -603,98 +863,71 @@ class MklConv2DOp : public OpKernel {\n                                           memory::format::hwio);\n       filter.SetUsrMem(filter_md, &filter_tensor);\n \n-      // Set output shape (output_dims) required in MKL-DNN order.\n-      // Currently, we set output layout as Tensorflow's layout (NHWC or NCHW\n-      // depending on data format). But later we propagate Mkl layout of the\n-      // output to the next op directly.\n-      output.SetUsrMem(output_dims_mkl_order, tf_fmt);\n-\n-      // Create memory descriptors for convolution data w/ no specified format.\n-      src.SetOpMemDesc(src_dims, memory::format::any);\n-      filter.SetOpMemDesc(filter_dims, memory::format::any);\n-      output.SetOpMemDesc(output_dims_mkl_order, memory::format::any);\n-\n       // MKLDNN dilation starts from 0.\n       dilations[kDilationH] -= 1;\n       dilations[kDilationW] -= 1;\n \n+      // get a conv2d fwd from primitive pool\n+      Conv2DFwd<T> *conv2d_fwd = nullptr;\n       if (biasEnabled) {\n-          // Create convolution primitive with Bias.\n-          MklDnnData<T> bias(&cpu_engine);\n-          memory::dims bias_size;\n-          conv_utl.GetBiasSizeInMklOrder(kInputIndex_Bias, &bias_size);\n-          const Tensor& bias_tensor = MklGetInput(context, kInputIndex_Bias);\n-          bias.SetUsrMem(bias_size, memory::format::x, &bias_tensor);\n-          bias.SetOpMemDesc(bias_size, memory::format::any);\n-\n-          // Create convolution primitive with Bias.\n-          // Use MKLDNN dilated convolution in case of dilated rate (>0).\n-          auto conv_desc = (dilations[kDilationH] > 0 ||\n-              dilations[kDilationW] > 0) ?\n-              convolution_forward::desc(prop_kind::forward,\n-                      convolution_direct, src.GetOpMemDesc(),\n-                      filter.GetOpMemDesc(), bias.GetOpMemDesc(),\n-                      output.GetOpMemDesc(), strides, dilations,\n-                      padding_l, padding_r,\n-                      TFPaddingToMklDnnPadding(padding_)):\n-              convolution_forward::desc(prop_kind::forward,\n-                      convolution_direct, src.GetOpMemDesc(),\n-                      filter.GetOpMemDesc(), bias.GetOpMemDesc(),\n-                      output.GetOpMemDesc(), strides,\n-                      padding_l, padding_r,\n-                      TFPaddingToMklDnnPadding(padding_));\n-\n-          auto conv_prim_desc = convolution_forward::primitive_desc(conv_desc,\n-                                                                  cpu_engine);\n-          AllocateOutputTensor(context, conv_prim_desc,\n-                               output_dims_mkl_order, tf_fmt, &output_tensor);\n-          // Set data handle for output.\n-          output.SetUsrMemDataHandle(output_tensor);\n-\n-          Tensor* filter_out_tensor = nullptr;\n-          AllocateFilterOutputTensor(context, conv_prim_desc,\n-                TFShapeToMklDnnDims(filter_tf_shape),\n-                &filter_out_tensor);\n-\n-          PrepareAndExecuteNet(conv_prim_desc, &src, &filter, &bias, &output,\n-                               filter_out_tensor);\n+        memory::dims bias_dims = {};\n+        conv_utl.GetBiasSizeInMklOrder(kInputIndex_Bias, &bias_dims);\n+        conv2d_fwd = Conv2DFwdFactory<T>::Get(src_dims, filter_dims,\n+                        bias_dims, dst_dims_mkl_order,\n+                        strides, dilations, padding_l, padding_r);\n       } else {\n-          // Create convolution primitive without Bias.\n-          // Use MKLDNN dilated convolution in case of dilated rate (>0).\n-          auto conv_desc = (dilations[kDilationH] > 0 ||\n-            dilations[kDilationW] > 0) ?\n-            convolution_forward::desc(prop_kind::forward,\n-              convolution_direct, src.GetOpMemDesc(),\n-              filter.GetOpMemDesc(), output.GetOpMemDesc(),\n-              strides, dilations, padding_l, padding_r,\n-              TFPaddingToMklDnnPadding(padding_)):\n-          convolution_forward::desc(prop_kind::forward,\n-              convolution_direct, src.GetOpMemDesc(),\n-              filter.GetOpMemDesc(), output.GetOpMemDesc(),\n-              strides, padding_l, padding_r,\n-              TFPaddingToMklDnnPadding(padding_));\n-\n-          auto conv_prim_desc = convolution_forward::primitive_desc(conv_desc,\n-                                                                  cpu_engine);\n-          AllocateOutputTensor(context, conv_prim_desc, output_dims_mkl_order,\n-                               tf_fmt, &output_tensor);\n-          // Set data handle for output.\n-          output.SetUsrMemDataHandle(output_tensor);\n-\n-          Tensor* filter_out_tensor = nullptr;\n-          AllocateFilterOutputTensor(context, conv_prim_desc,\n-                TFShapeToMklDnnDims(filter_tf_shape),\n-                &filter_out_tensor);\n-          PrepareAndExecuteNet(conv_prim_desc, &src, &filter,\n-                              nullptr, &output, filter_out_tensor);\n+        conv2d_fwd = Conv2DFwdFactory<T>::Get(src_dims, filter_dims,\n+                        NONE_DIMS, dst_dims_mkl_order,\n+                        strides, dilations, padding_l, padding_r);\n       }\n-    } catch (mkldnn::error& e) {\n+\n+      // allocate output tensors output_tensor and filter_out_tensor\n+      std::shared_ptr<mkldnn::convolution_forward::primitive_desc>\n+      conv_fwd_pd = conv2d_fwd->fwd_pd_;\n+      AllocateOutputTensor(context, *conv_fwd_pd,\n+                       dst_dims_mkl_order, tf_fmt, &dst_tensor);\n+      Tensor* filter_out_tensor = nullptr;\n+      AllocateFilterOutputTensor(context, *conv_fwd_pd,\n+                                 TFShapeToMklDnnDims(filter_tf_shape),\n+                                 &filter_out_tensor);\n+\n+      void* dst_data = static_cast<void*>(const_cast<T*>(", "path": "tensorflow/core/kernels/mkl_conv_ops.cc", "position": null, "original_position": 502, "commit_id": "147a783408be86865ebcec3f3684a2df21d094c2", "original_commit_id": "6d5d7321885b11010a2e7fa3c4668ba6d7b66c86", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "All this casting to void* should be avoided.", "created_at": "2018-04-13T16:06:49Z", "updated_at": "2018-04-18T17:12:04Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/17943#discussion_r181436685", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17943", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/181436685"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/17943#discussion_r181436685"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/17943"}}, "body_html": "<p>All this casting to void* should be avoided.</p>", "body_text": "All this casting to void* should be avoided."}