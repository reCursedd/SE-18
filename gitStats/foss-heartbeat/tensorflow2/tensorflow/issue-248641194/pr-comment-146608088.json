{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146608088", "pull_request_review_id": 71574229, "id": 146608088, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjYwODA4OA==", "diff_hunk": "@@ -1092,6 +1096,7 @@ cc_library(\n     deps = [\n         \":protos_all_cc_impl\",\n         \"//third_party/eigen3\",\n+        \"@double_conversion//:double-conversion\",", "path": "tensorflow/core/BUILD", "position": 15, "original_position": 31, "commit_id": "32dbc5c7a67fd56094ef72c8869b0d61a3eb23c8", "original_commit_id": "bb1bd807fcbb26822a3872d04e0223f6d6e6f06d", "user": {"login": "cwhipkey", "id": 17578177, "node_id": "MDQ6VXNlcjE3NTc4MTc3", "avatar_url": "https://avatars0.githubusercontent.com/u/17578177?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cwhipkey", "html_url": "https://github.com/cwhipkey", "followers_url": "https://api.github.com/users/cwhipkey/followers", "following_url": "https://api.github.com/users/cwhipkey/following{/other_user}", "gists_url": "https://api.github.com/users/cwhipkey/gists{/gist_id}", "starred_url": "https://api.github.com/users/cwhipkey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cwhipkey/subscriptions", "organizations_url": "https://api.github.com/users/cwhipkey/orgs", "repos_url": "https://api.github.com/users/cwhipkey/repos", "events_url": "https://api.github.com/users/cwhipkey/events{/privacy}", "received_events_url": "https://api.github.com/users/cwhipkey/received_events", "type": "User", "site_admin": false}, "body": "It's a little unfortunate that mobile builds will end up using it, when I assume they don't need the extra speed of parsing (since they will only be parsing a few attribute values, not large files of text).  Do you know how large the double-conversion library is when compiled for android?\r\n\r\n(one option if the net change in binary size would be too large is to have the double conversion library only used by the op that needs it for speed).", "created_at": "2017-10-24T15:54:25Z", "updated_at": "2017-11-27T07:48:11Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/12102#discussion_r146608088", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12102", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146608088"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/12102#discussion_r146608088"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12102"}}, "body_html": "<p>It's a little unfortunate that mobile builds will end up using it, when I assume they don't need the extra speed of parsing (since they will only be parsing a few attribute values, not large files of text).  Do you know how large the double-conversion library is when compiled for android?</p>\n<p>(one option if the net change in binary size would be too large is to have the double conversion library only used by the op that needs it for speed).</p>", "body_text": "It's a little unfortunate that mobile builds will end up using it, when I assume they don't need the extra speed of parsing (since they will only be parsing a few attribute values, not large files of text).  Do you know how large the double-conversion library is when compiled for android?\n(one option if the net change in binary size would be too large is to have the double conversion library only used by the op that needs it for speed)."}