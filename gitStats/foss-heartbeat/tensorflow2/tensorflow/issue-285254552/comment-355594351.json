{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/355594351", "html_url": "https://github.com/tensorflow/tensorflow/issues/15755#issuecomment-355594351", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15755", "id": 355594351, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTU5NDM1MQ==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-05T16:13:29Z", "updated_at": "2018-01-05T16:13:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ah, so the problem is that your program produces elements in a non-deterministic order, and not that it deadlocks?</p>\n<p>Well, there are several potential races in that code:</p>\n<ol>\n<li>The worker threads race to acquire an element from the queue.</li>\n<li>The worker threads race to acquire the interpreter when they transition out of the TensorFlow runtime and into Python.</li>\n<li>There's a race between printing the <code>qsize</code> and threads accessing it, so it's quite possible that there are multiple elements in the queue at some points, even if your logging indicates otherwise.</li>\n<li>The worker threads race to print their result to the screen. It's possible that the events are being processed in the right order, but printed in the wrong order!</li>\n</ol>\n<p>Adding <code>time.sleep()</code> calls between enqueuing events is insufficient to serialize the execution. It's possible for your thread to be woken after less time than you requested, and it's also possible for the thread contention on the GIL to be large enough that multiple elements end up in the queue at once.</p>\n<p>If you still think there is a bug in the <code>Dataset.from_generator()</code>, the first step must be coming up with a test program that is demonstrably correct... there are too many potential sources of non-determinism in that example program to conclude that the library is the source of the error.</p>", "body_text": "Ah, so the problem is that your program produces elements in a non-deterministic order, and not that it deadlocks?\nWell, there are several potential races in that code:\n\nThe worker threads race to acquire an element from the queue.\nThe worker threads race to acquire the interpreter when they transition out of the TensorFlow runtime and into Python.\nThere's a race between printing the qsize and threads accessing it, so it's quite possible that there are multiple elements in the queue at some points, even if your logging indicates otherwise.\nThe worker threads race to print their result to the screen. It's possible that the events are being processed in the right order, but printed in the wrong order!\n\nAdding time.sleep() calls between enqueuing events is insufficient to serialize the execution. It's possible for your thread to be woken after less time than you requested, and it's also possible for the thread contention on the GIL to be large enough that multiple elements end up in the queue at once.\nIf you still think there is a bug in the Dataset.from_generator(), the first step must be coming up with a test program that is demonstrably correct... there are too many potential sources of non-determinism in that example program to conclude that the library is the source of the error.", "body": "Ah, so the problem is that your program produces elements in a non-deterministic order, and not that it deadlocks?\r\n\r\nWell, there are several potential races in that code:\r\n\r\n1. The worker threads race to acquire an element from the queue.\r\n2. The worker threads race to acquire the interpreter when they transition out of the TensorFlow runtime and into Python.\r\n3. There's a race between printing the `qsize` and threads accessing it, so it's quite possible that there are multiple elements in the queue at some points, even if your logging indicates otherwise.\r\n4. The worker threads race to print their result to the screen. It's possible that the events are being processed in the right order, but printed in the wrong order!\r\n\r\nAdding `time.sleep()` calls between enqueuing events is insufficient to serialize the execution. It's possible for your thread to be woken after less time than you requested, and it's also possible for the thread contention on the GIL to be large enough that multiple elements end up in the queue at once.\r\n\r\nIf you still think there is a bug in the `Dataset.from_generator()`, the first step must be coming up with a test program that is demonstrably correct... there are too many potential sources of non-determinism in that example program to conclude that the library is the source of the error."}