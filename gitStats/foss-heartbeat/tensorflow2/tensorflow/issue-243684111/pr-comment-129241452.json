{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/129241452", "pull_request_review_id": 51992323, "id": 129241452, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTI0MTQ1Mg==", "diff_hunk": "@@ -2095,3 +2096,113 @@ def call(self, inputs, state):\n \n     new_state = rnn_cell_impl.LSTMStateTuple(c, m)\n     return m, new_state\n+\n+\n+class PrepareableMultiRNNCell(rnn_cell_impl.MultiRNNCell):\n+  \"\"\"Wrapper to implement multi-layer weight normalized RNNs\"\"\"\n+  def prepare(self):\n+    for i, cell in enumerate(self._cells):\n+      with vs.variable_scope(\"cell_%d\" % i):\n+        cell.prepare()\n+\n+\n+class WeightNormBasicLSTMCell(rnn_cell_impl.BasicLSTMCell):\n+  \"\"\"Weight normalized Basic LSTM Cell.\n+\n+        The implementation is based on\n+        https://arxiv.org/abs/1602.07868\n+        Tim Salimans, Diederik P. Kingma\n+        Weight Normalization: A Simple Reparameterization to Accelerate\n+        Training of Deep Neural Networks\n+\n+        LSTM implementation is the most basic, non-peephole based.\n+  \"\"\"\n+\n+  def __init__(self, num_units, norm=True,\n+               dtype=dtypes.float32, scope='wn_basic_lstm_cell'):\n+    super(WeightNormBasicLSTMCell, self).__init__(num_units,\n+                                                  forget_bias=1.0,\n+                                                  state_is_tuple=True)\n+    self.scope = scope\n+    self.dtype = dtype\n+    self.prepared = False\n+    self.norm = norm\n+\n+  def prepare(self):\n+    \"\"\"Called by dynamic_rnn just before the time-loop.\n+       In this case implements weight-normalization on the weight matrices\"\"\"\n+    output_size = 4*self._num_units\n+\n+    # Dynamic RNN requires number of input dimensions = hidden units\n+    h_size = x_size = self._num_units\n+\n+    with vs.variable_scope(self.scope):\n+      wx = vs.get_variable(\"wx\", [x_size, output_size], dtype=self.dtype)\n+      wh = vs.get_variable(\"wh\", [h_size, output_size], dtype=self.dtype)\n+\n+      if self.norm:\n+        gx = vs.get_variable(\"gx\", [output_size], dtype=self.dtype)\n+        gh = vs.get_variable(\"gh\", [output_size], dtype=self.dtype)\n+\n+        wx = nn_impl.l2_normalize(wx, dim=0) * gx", "path": "tensorflow/contrib/rnn/python/ops/rnn_cell.py", "position": null, "original_position": 60, "commit_id": "945a5ecb172122e3b34eb5bbaf50b7e76b5fa155", "original_commit_id": "a0bf344c9cb5521d3741a737e9f17d07fd6653cd", "user": {"login": "discoveredcheck", "id": 25035016, "node_id": "MDQ6VXNlcjI1MDM1MDE2", "avatar_url": "https://avatars1.githubusercontent.com/u/25035016?v=4", "gravatar_id": "", "url": "https://api.github.com/users/discoveredcheck", "html_url": "https://github.com/discoveredcheck", "followers_url": "https://api.github.com/users/discoveredcheck/followers", "following_url": "https://api.github.com/users/discoveredcheck/following{/other_user}", "gists_url": "https://api.github.com/users/discoveredcheck/gists{/gist_id}", "starred_url": "https://api.github.com/users/discoveredcheck/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/discoveredcheck/subscriptions", "organizations_url": "https://api.github.com/users/discoveredcheck/orgs", "repos_url": "https://api.github.com/users/discoveredcheck/repos", "events_url": "https://api.github.com/users/discoveredcheck/events{/privacy}", "received_events_url": "https://api.github.com/users/discoveredcheck/received_events", "type": "User", "site_admin": false}, "body": "Hi @ebrevdo. Thanks for your review. It's mainly because the weights should be normalized *once* before entering the RNN while loop (following https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py). Without the prepare() method we would have to add the normalization code inside the cell's call() function which will (incorrectly) normalize the weights after each time step. We did try this implementation and found that it gives OOM errors during backpropagation in even moderately sized networks due to the wasteful normalization op at every time step.\r\n\r\nWe also thought about putting this code inside the cell's constructor, but that would go against the functional semantics of how other existing RNNCell classes operate. Also, it won't be possible to inherit the variable scopes of any wrappers one might want to use (specifically the MultiRNNCell which adds cell_%d to the variable scope).\r\n\r\nOur priority has been to try and not make any changes to core tensorflow, but this seemed to be way forward based on the above reasoning. Does this make sense? Or am I missing something here?\r\n\r\nAshwini", "created_at": "2017-07-25T08:26:59Z", "updated_at": "2018-01-03T04:43:42Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11573#discussion_r129241452", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11573", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/129241452"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11573#discussion_r129241452"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11573"}}, "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>. Thanks for your review. It's mainly because the weights should be normalized <em>once</em> before entering the RNN while loop (following <a href=\"https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py\">https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py</a>). Without the prepare() method we would have to add the normalization code inside the cell's call() function which will (incorrectly) normalize the weights after each time step. We did try this implementation and found that it gives OOM errors during backpropagation in even moderately sized networks due to the wasteful normalization op at every time step.</p>\n<p>We also thought about putting this code inside the cell's constructor, but that would go against the functional semantics of how other existing RNNCell classes operate. Also, it won't be possible to inherit the variable scopes of any wrappers one might want to use (specifically the MultiRNNCell which adds cell_%d to the variable scope).</p>\n<p>Our priority has been to try and not make any changes to core tensorflow, but this seemed to be way forward based on the above reasoning. Does this make sense? Or am I missing something here?</p>\n<p>Ashwini</p>", "body_text": "Hi @ebrevdo. Thanks for your review. It's mainly because the weights should be normalized once before entering the RNN while loop (following https://github.com/openai/generating-reviews-discovering-sentiment/blob/master/encoder.py). Without the prepare() method we would have to add the normalization code inside the cell's call() function which will (incorrectly) normalize the weights after each time step. We did try this implementation and found that it gives OOM errors during backpropagation in even moderately sized networks due to the wasteful normalization op at every time step.\nWe also thought about putting this code inside the cell's constructor, but that would go against the functional semantics of how other existing RNNCell classes operate. Also, it won't be possible to inherit the variable scopes of any wrappers one might want to use (specifically the MultiRNNCell which adds cell_%d to the variable scope).\nOur priority has been to try and not make any changes to core tensorflow, but this seemed to be way forward based on the above reasoning. Does this make sense? Or am I missing something here?\nAshwini", "in_reply_to_id": 129178674}