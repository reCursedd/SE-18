{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362681089", "html_url": "https://github.com/tensorflow/tensorflow/issues/16343#issuecomment-362681089", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16343", "id": 362681089, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjY4MTA4OQ==", "user": {"login": "davidparks21", "id": 964997, "node_id": "MDQ6VXNlcjk2NDk5Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/964997?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davidparks21", "html_url": "https://github.com/davidparks21", "followers_url": "https://api.github.com/users/davidparks21/followers", "following_url": "https://api.github.com/users/davidparks21/following{/other_user}", "gists_url": "https://api.github.com/users/davidparks21/gists{/gist_id}", "starred_url": "https://api.github.com/users/davidparks21/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davidparks21/subscriptions", "organizations_url": "https://api.github.com/users/davidparks21/orgs", "repos_url": "https://api.github.com/users/davidparks21/repos", "events_url": "https://api.github.com/users/davidparks21/events{/privacy}", "received_events_url": "https://api.github.com/users/davidparks21/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-02T19:23:48Z", "updated_at": "2018-02-09T20:56:18Z", "author_association": "NONE", "body_html": "<p>I've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.</p>\n<p><a href=\"https://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\" rel=\"nofollow\">https://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464</a></p>\n<h2>Update to this post:</h2>\n<p>The solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an <code>unsupported object type numpy.ndarray</code> error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\n\ndef mypymap(data_numpy):\n  result = np.array([\n    np.array([100,101,102]),\n    np.array([200,201,202,203])\n  ])\n  return result\n\ndata_input = ['fileA','fileB','fileC']\n\nds = tf.data.Dataset.from_tensor_slices(data_input)\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\n\nelement = ds.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n  for _ in range(3):\n    print(sess.run(element))\n</code></pre>\n<p>So that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the <code>Dataset</code> pipeline.</p>", "body_text": "I've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.\nhttps://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\nUpdate to this post:\nThe solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an unsupported object type numpy.ndarray error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file.\nimport tensorflow as tf\nimport numpy as np\n\ndef mypymap(data_numpy):\n  result = np.array([\n    np.array([100,101,102]),\n    np.array([200,201,202,203])\n  ])\n  return result\n\ndata_input = ['fileA','fileB','fileC']\n\nds = tf.data.Dataset.from_tensor_slices(data_input)\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\n\nelement = ds.make_one_shot_iterator().get_next()\n\nwith tf.Session() as sess:\n  for _ in range(3):\n    print(sess.run(element))\n\nSo that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the Dataset pipeline.", "body": "I've implemented this process using map functions by splitting large files into filename/index pairs so as to specify a subset of the file to read in each map call. This was not a trivial task. A final sample solution can be found on stack overflow below. I continue to believe that Dataset should be extended to better support this workflow. In particular with respect to supporting generators throughout the pipeline.\r\n\r\nhttps://stackoverflow.com/questions/48471688/using-tensorflows-dataset-pipeline-how-do-i-name-the-results-of-a-map-oper/48589464#48589464\r\n\r\nUpdate to this post:\r\n---------------\r\nThe solution referenced above doesn't work in the case where each sample needs to be different shapes. This code example demonstrates an attempt to return an ndarray of ndarray's which produces an `unsupported object type numpy.ndarray` error in tensorflow. The goal of the example is to demonstrate reading 2 samples of variable length from a file. \r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\n\r\ndef mypymap(data_numpy):\r\n  result = np.array([\r\n    np.array([100,101,102]),\r\n    np.array([200,201,202,203])\r\n  ])\r\n  return result\r\n\r\ndata_input = ['fileA','fileB','fileC']\r\n\r\nds = tf.data.Dataset.from_tensor_slices(data_input)\r\nds = ds.map(lambda data_tensor: tf.py_func(mypymap, [data_tensor], Tout=[tf.int64], stateful=False))\r\n\r\nelement = ds.make_one_shot_iterator().get_next()\r\n\r\nwith tf.Session() as sess:\r\n  for _ in range(3):\r\n    print(sess.run(element))\r\n```\r\nSo that leaves the only solution being to incorporate a bunch of logic into my generator (for interleaving, parallel file reads, and shuffling) and putting the generator up at the beginning of the preprocessing pipeline. All things that are better done as part of the `Dataset` pipeline."}