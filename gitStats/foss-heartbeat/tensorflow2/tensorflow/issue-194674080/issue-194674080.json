{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6220", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6220/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6220/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6220/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6220", "id": 194674080, "node_id": "MDU6SXNzdWUxOTQ2NzQwODA=", "number": 6220, "title": "Upgrade from r11 to r12 prodeuces \"Variables not defined\" when using any optimizer but GradientDescentOptimizer", "user": {"login": "germanRos-TRI", "id": 23382137, "node_id": "MDQ6VXNlcjIzMzgyMTM3", "avatar_url": "https://avatars2.githubusercontent.com/u/23382137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/germanRos-TRI", "html_url": "https://github.com/germanRos-TRI", "followers_url": "https://api.github.com/users/germanRos-TRI/followers", "following_url": "https://api.github.com/users/germanRos-TRI/following{/other_user}", "gists_url": "https://api.github.com/users/germanRos-TRI/gists{/gist_id}", "starred_url": "https://api.github.com/users/germanRos-TRI/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/germanRos-TRI/subscriptions", "organizations_url": "https://api.github.com/users/germanRos-TRI/orgs", "repos_url": "https://api.github.com/users/germanRos-TRI/repos", "events_url": "https://api.github.com/users/germanRos-TRI/events{/privacy}", "received_events_url": "https://api.github.com/users/germanRos-TRI/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 18, "created_at": "2016-12-09T19:14:03Z", "updated_at": "2018-11-13T18:05:28Z", "closed_at": "2016-12-10T08:29:34Z", "author_association": "NONE", "body_html": "<p>After a recent upgrade to the latest version of tensorflow in github, several things stop working. I found out that all the optimizers, such as Adam or Adagrad are now producing an error related to variable scope that I have not managed to solve yet. However, GradientDescentOptimizer works fine.</p>\n<p>It may be related to the issue: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"189883954\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5652\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5652/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/5652\">#5652</a></p>\n<p>The error looks like this:</p>\n<pre><code>File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 651, in _get_single_variable\n    \"VarScope?\" % name)\nValueError: Variable filter/Adadelta/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n\n</code></pre>\n<p>It works fine with tensorflow r11</p>\n<p>Operating System: Ubuntu 16 and Ubuntu 14<br>\nInstalled version of CUDA and cuDNN: cuda 8.0, cuda 5.1<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/642888/cuda.txt\">cuda.txt</a><br>\nThe commit hash  <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/6dc8deaed8d8bd9cc6d52a03474d0b82891c8b86/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/6dc8deaed8d8bd9cc6d52a03474d0b82891c8b86\"><tt>6dc8dea</tt></a><br>\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)<br>\nBuild timestamp: 1478109254<br>\nBuild timestamp as int: 1478109254</p>\n<p>Find below a minimal version that causes the error:</p>\n<pre><code>import tensorflow as tf\nimport pdb\n\ndef main():\n\n    ## !!! change this to test the different behaviors !!!\n    #optimizer = tf.train.GradientDescentOptimizer(1e-3)                 # This one is working\n    optimizer = tf.train.AdamOptimizer(1e-3, beta1=0.9, beta2=0.999999) # This one is not working\n    #optimizer = tf.train.AdagradOptimizer(1e-3)                         # This one is not working\n    #optimizer = tf.train.AdadeltaOptimizer(1e-3)                        # This one is not working\n\t\n    list_grads = []\n    for i in xrange(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%d' % i) as scope:\n                W = tf.get_variable(name=\"filter\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n                X = tf.get_variable(name=\"data\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n                Y_ = tf.get_variable(name=\"out\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n                Y = W+X\n                loss =tf.reduce_mean(Y-Y_)\n                grad = optimizer.compute_gradients(loss)\n                list_grads.append(grad)\n\n                tf.get_variable_scope().reuse_variables()\t\n    \n    grads = list_grads[0] + list_grads[1]\n    #pdb.set_trace()\n\n    op_train = optimizer.apply_gradients(grads)\n\n    init_global = tf.global_variables_initializer()\n    init_local =  tf.local_variables_initializer()\n\n    sess = tf.Session()\n    sess.run([init_global, init_local])\n\n    _, sol = sess.run([op_train, loss])\n    print(str(sol))\n\nif (__name__ == '__main__'):\n\tmain()\n</code></pre>", "body_text": "After a recent upgrade to the latest version of tensorflow in github, several things stop working. I found out that all the optimizers, such as Adam or Adagrad are now producing an error related to variable scope that I have not managed to solve yet. However, GradientDescentOptimizer works fine.\nIt may be related to the issue: #5652\nThe error looks like this:\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 651, in _get_single_variable\n    \"VarScope?\" % name)\nValueError: Variable filter/Adadelta/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\n\n\nIt works fine with tensorflow r11\nOperating System: Ubuntu 16 and Ubuntu 14\nInstalled version of CUDA and cuDNN: cuda 8.0, cuda 5.1\ncuda.txt\nThe commit hash  6dc8dea\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\nBuild timestamp: 1478109254\nBuild timestamp as int: 1478109254\nFind below a minimal version that causes the error:\nimport tensorflow as tf\nimport pdb\n\ndef main():\n\n    ## !!! change this to test the different behaviors !!!\n    #optimizer = tf.train.GradientDescentOptimizer(1e-3)                 # This one is working\n    optimizer = tf.train.AdamOptimizer(1e-3, beta1=0.9, beta2=0.999999) # This one is not working\n    #optimizer = tf.train.AdagradOptimizer(1e-3)                         # This one is not working\n    #optimizer = tf.train.AdadeltaOptimizer(1e-3)                        # This one is not working\n\t\n    list_grads = []\n    for i in xrange(2):\n        with tf.device('/gpu:%d' % i):\n            with tf.name_scope('%d' % i) as scope:\n                W = tf.get_variable(name=\"filter\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n                X = tf.get_variable(name=\"data\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n                Y_ = tf.get_variable(name=\"out\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\n                Y = W+X\n                loss =tf.reduce_mean(Y-Y_)\n                grad = optimizer.compute_gradients(loss)\n                list_grads.append(grad)\n\n                tf.get_variable_scope().reuse_variables()\t\n    \n    grads = list_grads[0] + list_grads[1]\n    #pdb.set_trace()\n\n    op_train = optimizer.apply_gradients(grads)\n\n    init_global = tf.global_variables_initializer()\n    init_local =  tf.local_variables_initializer()\n\n    sess = tf.Session()\n    sess.run([init_global, init_local])\n\n    _, sol = sess.run([op_train, loss])\n    print(str(sol))\n\nif (__name__ == '__main__'):\n\tmain()", "body": "After a recent upgrade to the latest version of tensorflow in github, several things stop working. I found out that all the optimizers, such as Adam or Adagrad are now producing an error related to variable scope that I have not managed to solve yet. However, GradientDescentOptimizer works fine.\r\n\r\nIt may be related to the issue: https://github.com/tensorflow/tensorflow/issues/5652\r\n\r\nThe error looks like this:\r\n```\r\nFile \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.py\", line 651, in _get_single_variable\r\n    \"VarScope?\" % name)\r\nValueError: Variable filter/Adadelta/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n```\r\n\r\nIt works fine with tensorflow r11\r\n\r\nOperating System: Ubuntu 16 and Ubuntu 14\r\nInstalled version of CUDA and cuDNN: cuda 8.0, cuda 5.1\r\n[cuda.txt](https://github.com/tensorflow/tensorflow/files/642888/cuda.txt)\r\n The commit hash  6dc8deaed8d8bd9cc6d52a03474d0b82891c8b86\r\nBuild time: Wed Nov 2 17:54:14 2016 (1478109254)\r\nBuild timestamp: 1478109254\r\nBuild timestamp as int: 1478109254\r\n\r\nFind below a minimal version that causes the error:\r\n```\r\nimport tensorflow as tf\r\nimport pdb\r\n\r\ndef main():\r\n\r\n    ## !!! change this to test the different behaviors !!!\r\n    #optimizer = tf.train.GradientDescentOptimizer(1e-3)                 # This one is working\r\n    optimizer = tf.train.AdamOptimizer(1e-3, beta1=0.9, beta2=0.999999) # This one is not working\r\n    #optimizer = tf.train.AdagradOptimizer(1e-3)                         # This one is not working\r\n    #optimizer = tf.train.AdadeltaOptimizer(1e-3)                        # This one is not working\r\n\t\r\n    list_grads = []\r\n    for i in xrange(2):\r\n        with tf.device('/gpu:%d' % i):\r\n            with tf.name_scope('%d' % i) as scope:\r\n                W = tf.get_variable(name=\"filter\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\r\n                X = tf.get_variable(name=\"data\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\r\n                Y_ = tf.get_variable(name=\"out\", initializer=tf.random_uniform_initializer(dtype=tf.float32), shape=[5, 1])\r\n                Y = W+X\r\n                loss =tf.reduce_mean(Y-Y_)\r\n                grad = optimizer.compute_gradients(loss)\r\n                list_grads.append(grad)\r\n\r\n                tf.get_variable_scope().reuse_variables()\t\r\n    \r\n    grads = list_grads[0] + list_grads[1]\r\n    #pdb.set_trace()\r\n\r\n    op_train = optimizer.apply_gradients(grads)\r\n\r\n    init_global = tf.global_variables_initializer()\r\n    init_local =  tf.local_variables_initializer()\r\n\r\n    sess = tf.Session()\r\n    sess.run([init_global, init_local])\r\n\r\n    _, sol = sess.run([op_train, loss])\r\n    print(str(sol))\r\n\r\nif (__name__ == '__main__'):\r\n\tmain()\r\n```\r\n"}