{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321491668", "html_url": "https://github.com/tensorflow/tensorflow/issues/6220#issuecomment-321491668", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6220", "id": 321491668, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTQ5MTY2OA==", "user": {"login": "Huayra007", "id": 22915807, "node_id": "MDQ6VXNlcjIyOTE1ODA3", "avatar_url": "https://avatars0.githubusercontent.com/u/22915807?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Huayra007", "html_url": "https://github.com/Huayra007", "followers_url": "https://api.github.com/users/Huayra007/followers", "following_url": "https://api.github.com/users/Huayra007/following{/other_user}", "gists_url": "https://api.github.com/users/Huayra007/gists{/gist_id}", "starred_url": "https://api.github.com/users/Huayra007/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Huayra007/subscriptions", "organizations_url": "https://api.github.com/users/Huayra007/orgs", "repos_url": "https://api.github.com/users/Huayra007/repos", "events_url": "https://api.github.com/users/Huayra007/events{/privacy}", "received_events_url": "https://api.github.com/users/Huayra007/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-10T08:50:39Z", "updated_at": "2017-08-10T08:52:03Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> Hi, I've confronted with this problem when using AdamOptimizer, I've tried your suggestion but it still doesn't work. Could you please help me change the code?<br>\nBesides, I'm not very familiar with TF, wish you can help me point out anything not appropriate in this code. Thank you!</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport os\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data/')\nos.environ['CUDA_VISIBLE_DEVICES']='4'\n\nsample_image = mnist.train.next_batch(1)[0]\nprint (sample_image.shape)\n\nsample_image = sample_image.reshape([28,28])\nplt.imshow(sample_image,cmap='Greys')\n\ndef discriminator(images,reuse=False,):\n    if(reuse):\n        tf.get_variable_scope().reuse_variables()\n\n    with tf.variable_scope('D_conv1'):\n        d_w1 = tf.get_variable('d_w1',[5,5,1,32],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b1 = tf.get_variable('d_b1',[32],initializer=tf.constant_initializer(0))\n        d1 = tf.nn.conv2d(input=images,filter=d_w1,strides=[1,1,1,1],padding='SAME')+d_b1\n        d1 = tf.nn.relu(d1)\n        d1 = tf.nn.avg_pool(d1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n    with tf.variable_scope('D_conv2'):\n        d_w2 = tf.get_variable('d_w2',[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b2 = tf.get_variable('d_b2',[64],initializer=tf.constant_initializer(0))\n        d2 = tf.nn.conv2d(input=d1,filter=d_w2,strides=[1,1,1,1],padding='SAME')+d_b2\n        d2 = tf.nn.relu(d2)\n        d2 = tf.nn.avg_pool(d2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n    with tf.variable_scope('D_fcn3'):\n        d_w3 = tf.get_variable('d_w3',[7*7*64,1024],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b3 = tf.get_variable('d_b3',[1024],initializer=tf.constant_initializer(0))\n        d3 = tf.matmul(tf.reshape(d2,[-1,7*7*64]),d_w3)+d_b3\n        d3 = tf.nn.relu(d3)\n\n    with tf.variable_scope('D_fc4'):\n        d_w4 = tf.get_variable('d_w4',[1024,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b4 = tf.get_variable('d_b4',[1],initializer=tf.constant_initializer(0))\n        d4 = tf.matmul(d3,d_w4)+d_b4\n        d4 = tf.nn.sigmoid(d4,name='d4')\n\n    return d4\n\ndef generator(z,batch_size,z_dim):\n    g_w1 = tf.get_variable('g_w1',[z_dim,56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b1 = tf.get_variable('g_b1',[56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g1 = tf.matmul(z,g_w1)+g_b1\n    g1 = tf.reshape(g1,[-1,56,56,1])\n    g1 = tf.contrib.layers.batch_norm(g1,epsilon=1e-5,scope='bn1')\n    g1 = tf.nn.relu(g1)\n\n    g_w2 = tf.get_variable('g_w2',[3,3,1,z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b2 = tf.get_variable('g_b2',[z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g2 = tf.nn.conv2d(g1,filter=g_w2,strides=[1,1,1,1],padding='SAME')+g_b2\n    g2 = tf.contrib.layers.batch_norm(g2,epsilon=1e-5,scope='bn2')\n    g2 = tf.nn.relu(g2)\n    g2 = tf.image.resize_images(g2,[56,56])\n\n    g_w3 = tf.get_variable('g_w3',[3,3,z_dim/2,z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b3 = tf.get_variable('g_b3',[z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g3 = tf.nn.conv2d(g2,filter=g_w3,strides=[1,1,1,1],padding='SAME')+g_b3\n    g3 = tf.contrib.layers.batch_norm(g3,epsilon=1e-5,scope='bn3')\n    g3 = tf.nn.relu(g3)\n    g3 = tf.image.resize_images(g3,[56,56])\n\n    g_w4 = tf.get_variable('g_w4',[1,1,z_dim/4,1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b4 = tf.get_variable('g_b4',[1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g4 = tf.nn.conv2d(g3,filter=g_w4,strides=[1,2,2,1],padding='SAME')+g_b4\n    g4 = tf.nn.sigmoid(g4)\n\n    return g4\n\ntf.reset_default_graph()\nbatch_size =100\nz_dimension = 100\n\nz_placeholder = tf.placeholder(tf.float32,[None,z_dimension],name='z_placeholder')\nx_placeholder = tf.placeholder(tf.float32,[None,28,28,1],name='x_placeholder')\n\nwith tf.variable_scope(tf.get_variable_scope()):\n    Gz = generator(z_placeholder,batch_size,z_dimension)\n    Dx = discriminator(x_placeholder)\n    Dg = discriminator(Gz,reuse=True)\n\nd_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dx),logits=Dx))\nd_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(Dg),logits=Dg))\ng_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dg),logits=Dg))\n\ntvars = tf.trainable_variables()\nd_vars = [var for var in tvars if 'd_' in var.name]\ng_vars = [var for var in tvars if 'g_' in var.name]\n\nprint ([v.name for v in d_vars])\nprint ([v.name for v in g_vars])\n'''\nd_trainer_real = tf.train.GradientDescentOptimizer(0.0003).minimize(d_loss_real,var_list=d_vars)\nd_trainer_fake = tf.train.GradientDescentOptimizer(0.003).minimize(d_loss_fake,var_list=d_vars)\ng_trainer = tf.train.GradientDescentOptimizer(0.001).minimize(g_loss,var_list=g_vars)\n'''\nwith tf.variable_scope(tf.get_variable_scope()):\n    d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_loss_real, var_list=d_vars)\n    tf.get_variable_scope().reuse_variables()\n    d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_loss_fake, var_list=d_vars)\n    g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)\n\ntf.summary.scalar('Discriminator_loss_real',d_loss_real)\ntf.summary.scalar('Discriminator_loss_fake',d_loss_fake)\ntf.summary.scalar('Generator_loss',g_loss)\n\nimages_for_tensorboard = generator(z_placeholder,batch_size,z_dimension)\ntf.summary.image('Generated_images',images_for_tensorboard,5)\nmerged = tf.summary.merge_all()\nlogdir = 'Tensorboard/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\nwriter = tf.summary.FileWriter(logdir,sess.graph)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#Pre-train discriminator\nfor i in range(3000):\n    z_batch = np.random.normal(0, 1, size=[batch_size,z_dimension])\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\n                                       feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\n    if(i%100==0):\n        print ('dLossReal: ',dLossReal,'dLossFake: ',dLossFake)\n\n#Train discriminator and generator together\nfor i in range(100000):\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\n                                        feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n    _ = sess.run(g_trainer,feed_dict={z_placeholder:z_batch})\n\n    if i%10 ==0:\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n        summary = sess.run(merged,feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\n        writer.add_summary(summary,i)\n\n    if i%100==0:\n        print ('Iteration:',i,'at',datetime.datetime.now())\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n        generated_image = generator(z_placeholder,1,z_dimension)\n        images = sess.run(generated_image,feed_dict={z_placeholder:z_batch})\n        plt.imshow(images[0].reshape([28,28]),cmap='Greys')\n        plt.savefig('Generated_images/'+str(i)+'.jpg')\n\n        img = images[0].reshape([1,28,28,1])\n        result = discriminator(x_placeholder)\n        estimate = sess.run(result,feed_dict={x_placeholder:img})\n        print ('Estimate:',estimate)\n\n</code></pre>", "body_text": "@lukaszkaiser Hi, I've confronted with this problem when using AdamOptimizer, I've tried your suggestion but it still doesn't work. Could you please help me change the code?\nBesides, I'm not very familiar with TF, wish you can help me point out anything not appropriate in this code. Thank you!\nimport tensorflow as tf\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport os\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist = input_data.read_data_sets('MNIST_data/')\nos.environ['CUDA_VISIBLE_DEVICES']='4'\n\nsample_image = mnist.train.next_batch(1)[0]\nprint (sample_image.shape)\n\nsample_image = sample_image.reshape([28,28])\nplt.imshow(sample_image,cmap='Greys')\n\ndef discriminator(images,reuse=False,):\n    if(reuse):\n        tf.get_variable_scope().reuse_variables()\n\n    with tf.variable_scope('D_conv1'):\n        d_w1 = tf.get_variable('d_w1',[5,5,1,32],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b1 = tf.get_variable('d_b1',[32],initializer=tf.constant_initializer(0))\n        d1 = tf.nn.conv2d(input=images,filter=d_w1,strides=[1,1,1,1],padding='SAME')+d_b1\n        d1 = tf.nn.relu(d1)\n        d1 = tf.nn.avg_pool(d1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n    with tf.variable_scope('D_conv2'):\n        d_w2 = tf.get_variable('d_w2',[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b2 = tf.get_variable('d_b2',[64],initializer=tf.constant_initializer(0))\n        d2 = tf.nn.conv2d(input=d1,filter=d_w2,strides=[1,1,1,1],padding='SAME')+d_b2\n        d2 = tf.nn.relu(d2)\n        d2 = tf.nn.avg_pool(d2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\n\n    with tf.variable_scope('D_fcn3'):\n        d_w3 = tf.get_variable('d_w3',[7*7*64,1024],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b3 = tf.get_variable('d_b3',[1024],initializer=tf.constant_initializer(0))\n        d3 = tf.matmul(tf.reshape(d2,[-1,7*7*64]),d_w3)+d_b3\n        d3 = tf.nn.relu(d3)\n\n    with tf.variable_scope('D_fc4'):\n        d_w4 = tf.get_variable('d_w4',[1024,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\n        d_b4 = tf.get_variable('d_b4',[1],initializer=tf.constant_initializer(0))\n        d4 = tf.matmul(d3,d_w4)+d_b4\n        d4 = tf.nn.sigmoid(d4,name='d4')\n\n    return d4\n\ndef generator(z,batch_size,z_dim):\n    g_w1 = tf.get_variable('g_w1',[z_dim,56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b1 = tf.get_variable('g_b1',[56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g1 = tf.matmul(z,g_w1)+g_b1\n    g1 = tf.reshape(g1,[-1,56,56,1])\n    g1 = tf.contrib.layers.batch_norm(g1,epsilon=1e-5,scope='bn1')\n    g1 = tf.nn.relu(g1)\n\n    g_w2 = tf.get_variable('g_w2',[3,3,1,z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b2 = tf.get_variable('g_b2',[z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g2 = tf.nn.conv2d(g1,filter=g_w2,strides=[1,1,1,1],padding='SAME')+g_b2\n    g2 = tf.contrib.layers.batch_norm(g2,epsilon=1e-5,scope='bn2')\n    g2 = tf.nn.relu(g2)\n    g2 = tf.image.resize_images(g2,[56,56])\n\n    g_w3 = tf.get_variable('g_w3',[3,3,z_dim/2,z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b3 = tf.get_variable('g_b3',[z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g3 = tf.nn.conv2d(g2,filter=g_w3,strides=[1,1,1,1],padding='SAME')+g_b3\n    g3 = tf.contrib.layers.batch_norm(g3,epsilon=1e-5,scope='bn3')\n    g3 = tf.nn.relu(g3)\n    g3 = tf.image.resize_images(g3,[56,56])\n\n    g_w4 = tf.get_variable('g_w4',[1,1,z_dim/4,1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g_b4 = tf.get_variable('g_b4',[1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\n    g4 = tf.nn.conv2d(g3,filter=g_w4,strides=[1,2,2,1],padding='SAME')+g_b4\n    g4 = tf.nn.sigmoid(g4)\n\n    return g4\n\ntf.reset_default_graph()\nbatch_size =100\nz_dimension = 100\n\nz_placeholder = tf.placeholder(tf.float32,[None,z_dimension],name='z_placeholder')\nx_placeholder = tf.placeholder(tf.float32,[None,28,28,1],name='x_placeholder')\n\nwith tf.variable_scope(tf.get_variable_scope()):\n    Gz = generator(z_placeholder,batch_size,z_dimension)\n    Dx = discriminator(x_placeholder)\n    Dg = discriminator(Gz,reuse=True)\n\nd_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dx),logits=Dx))\nd_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(Dg),logits=Dg))\ng_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dg),logits=Dg))\n\ntvars = tf.trainable_variables()\nd_vars = [var for var in tvars if 'd_' in var.name]\ng_vars = [var for var in tvars if 'g_' in var.name]\n\nprint ([v.name for v in d_vars])\nprint ([v.name for v in g_vars])\n'''\nd_trainer_real = tf.train.GradientDescentOptimizer(0.0003).minimize(d_loss_real,var_list=d_vars)\nd_trainer_fake = tf.train.GradientDescentOptimizer(0.003).minimize(d_loss_fake,var_list=d_vars)\ng_trainer = tf.train.GradientDescentOptimizer(0.001).minimize(g_loss,var_list=g_vars)\n'''\nwith tf.variable_scope(tf.get_variable_scope()):\n    d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_loss_real, var_list=d_vars)\n    tf.get_variable_scope().reuse_variables()\n    d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_loss_fake, var_list=d_vars)\n    g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)\n\ntf.summary.scalar('Discriminator_loss_real',d_loss_real)\ntf.summary.scalar('Discriminator_loss_fake',d_loss_fake)\ntf.summary.scalar('Generator_loss',g_loss)\n\nimages_for_tensorboard = generator(z_placeholder,batch_size,z_dimension)\ntf.summary.image('Generated_images',images_for_tensorboard,5)\nmerged = tf.summary.merge_all()\nlogdir = 'Tensorboard/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\nwriter = tf.summary.FileWriter(logdir,sess.graph)\n\nsess = tf.Session()\nsess.run(tf.global_variables_initializer())\n\n#Pre-train discriminator\nfor i in range(3000):\n    z_batch = np.random.normal(0, 1, size=[batch_size,z_dimension])\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\n                                       feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\n    if(i%100==0):\n        print ('dLossReal: ',dLossReal,'dLossFake: ',dLossFake)\n\n#Train discriminator and generator together\nfor i in range(100000):\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\n                                        feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n    _ = sess.run(g_trainer,feed_dict={z_placeholder:z_batch})\n\n    if i%10 ==0:\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n        summary = sess.run(merged,feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\n        writer.add_summary(summary,i)\n\n    if i%100==0:\n        print ('Iteration:',i,'at',datetime.datetime.now())\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\n        generated_image = generator(z_placeholder,1,z_dimension)\n        images = sess.run(generated_image,feed_dict={z_placeholder:z_batch})\n        plt.imshow(images[0].reshape([28,28]),cmap='Greys')\n        plt.savefig('Generated_images/'+str(i)+'.jpg')\n\n        img = images[0].reshape([1,28,28,1])\n        result = discriminator(x_placeholder)\n        estimate = sess.run(result,feed_dict={x_placeholder:img})\n        print ('Estimate:',estimate)", "body": "@lukaszkaiser Hi, I've confronted with this problem when using AdamOptimizer, I've tried your suggestion but it still doesn't work. Could you please help me change the code?\r\nBesides, I'm not very familiar with TF, wish you can help me point out anything not appropriate in this code. Thank you!\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport datetime\r\nimport matplotlib.pyplot as plt\r\nimport os\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nmnist = input_data.read_data_sets('MNIST_data/')\r\nos.environ['CUDA_VISIBLE_DEVICES']='4'\r\n\r\nsample_image = mnist.train.next_batch(1)[0]\r\nprint (sample_image.shape)\r\n\r\nsample_image = sample_image.reshape([28,28])\r\nplt.imshow(sample_image,cmap='Greys')\r\n\r\ndef discriminator(images,reuse=False,):\r\n    if(reuse):\r\n        tf.get_variable_scope().reuse_variables()\r\n\r\n    with tf.variable_scope('D_conv1'):\r\n        d_w1 = tf.get_variable('d_w1',[5,5,1,32],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b1 = tf.get_variable('d_b1',[32],initializer=tf.constant_initializer(0))\r\n        d1 = tf.nn.conv2d(input=images,filter=d_w1,strides=[1,1,1,1],padding='SAME')+d_b1\r\n        d1 = tf.nn.relu(d1)\r\n        d1 = tf.nn.avg_pool(d1,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\r\n\r\n    with tf.variable_scope('D_conv2'):\r\n        d_w2 = tf.get_variable('d_w2',[5,5,32,64],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b2 = tf.get_variable('d_b2',[64],initializer=tf.constant_initializer(0))\r\n        d2 = tf.nn.conv2d(input=d1,filter=d_w2,strides=[1,1,1,1],padding='SAME')+d_b2\r\n        d2 = tf.nn.relu(d2)\r\n        d2 = tf.nn.avg_pool(d2,ksize=[1,2,2,1],strides=[1,2,2,1],padding='SAME')\r\n\r\n    with tf.variable_scope('D_fcn3'):\r\n        d_w3 = tf.get_variable('d_w3',[7*7*64,1024],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b3 = tf.get_variable('d_b3',[1024],initializer=tf.constant_initializer(0))\r\n        d3 = tf.matmul(tf.reshape(d2,[-1,7*7*64]),d_w3)+d_b3\r\n        d3 = tf.nn.relu(d3)\r\n\r\n    with tf.variable_scope('D_fc4'):\r\n        d_w4 = tf.get_variable('d_w4',[1024,1],initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n        d_b4 = tf.get_variable('d_b4',[1],initializer=tf.constant_initializer(0))\r\n        d4 = tf.matmul(d3,d_w4)+d_b4\r\n        d4 = tf.nn.sigmoid(d4,name='d4')\r\n\r\n    return d4\r\n\r\ndef generator(z,batch_size,z_dim):\r\n    g_w1 = tf.get_variable('g_w1',[z_dim,56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b1 = tf.get_variable('g_b1',[56*56],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g1 = tf.matmul(z,g_w1)+g_b1\r\n    g1 = tf.reshape(g1,[-1,56,56,1])\r\n    g1 = tf.contrib.layers.batch_norm(g1,epsilon=1e-5,scope='bn1')\r\n    g1 = tf.nn.relu(g1)\r\n\r\n    g_w2 = tf.get_variable('g_w2',[3,3,1,z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b2 = tf.get_variable('g_b2',[z_dim/2],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g2 = tf.nn.conv2d(g1,filter=g_w2,strides=[1,1,1,1],padding='SAME')+g_b2\r\n    g2 = tf.contrib.layers.batch_norm(g2,epsilon=1e-5,scope='bn2')\r\n    g2 = tf.nn.relu(g2)\r\n    g2 = tf.image.resize_images(g2,[56,56])\r\n\r\n    g_w3 = tf.get_variable('g_w3',[3,3,z_dim/2,z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b3 = tf.get_variable('g_b3',[z_dim/4],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g3 = tf.nn.conv2d(g2,filter=g_w3,strides=[1,1,1,1],padding='SAME')+g_b3\r\n    g3 = tf.contrib.layers.batch_norm(g3,epsilon=1e-5,scope='bn3')\r\n    g3 = tf.nn.relu(g3)\r\n    g3 = tf.image.resize_images(g3,[56,56])\r\n\r\n    g_w4 = tf.get_variable('g_w4',[1,1,z_dim/4,1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g_b4 = tf.get_variable('g_b4',[1],dtype=tf.float32,initializer=tf.truncated_normal_initializer(stddev=0.02))\r\n    g4 = tf.nn.conv2d(g3,filter=g_w4,strides=[1,2,2,1],padding='SAME')+g_b4\r\n    g4 = tf.nn.sigmoid(g4)\r\n\r\n    return g4\r\n\r\ntf.reset_default_graph()\r\nbatch_size =100\r\nz_dimension = 100\r\n\r\nz_placeholder = tf.placeholder(tf.float32,[None,z_dimension],name='z_placeholder')\r\nx_placeholder = tf.placeholder(tf.float32,[None,28,28,1],name='x_placeholder')\r\n\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n    Gz = generator(z_placeholder,batch_size,z_dimension)\r\n    Dx = discriminator(x_placeholder)\r\n    Dg = discriminator(Gz,reuse=True)\r\n\r\nd_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dx),logits=Dx))\r\nd_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.zeros_like(Dg),logits=Dg))\r\ng_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(Dg),logits=Dg))\r\n\r\ntvars = tf.trainable_variables()\r\nd_vars = [var for var in tvars if 'd_' in var.name]\r\ng_vars = [var for var in tvars if 'g_' in var.name]\r\n\r\nprint ([v.name for v in d_vars])\r\nprint ([v.name for v in g_vars])\r\n'''\r\nd_trainer_real = tf.train.GradientDescentOptimizer(0.0003).minimize(d_loss_real,var_list=d_vars)\r\nd_trainer_fake = tf.train.GradientDescentOptimizer(0.003).minimize(d_loss_fake,var_list=d_vars)\r\ng_trainer = tf.train.GradientDescentOptimizer(0.001).minimize(g_loss,var_list=g_vars)\r\n'''\r\nwith tf.variable_scope(tf.get_variable_scope()):\r\n    d_trainer_real = tf.train.AdamOptimizer(0.0003).minimize(d_loss_real, var_list=d_vars)\r\n    tf.get_variable_scope().reuse_variables()\r\n    d_trainer_fake = tf.train.AdamOptimizer(0.0003).minimize(d_loss_fake, var_list=d_vars)\r\n    g_trainer = tf.train.AdamOptimizer(0.0001).minimize(g_loss, var_list=g_vars)\r\n\r\ntf.summary.scalar('Discriminator_loss_real',d_loss_real)\r\ntf.summary.scalar('Discriminator_loss_fake',d_loss_fake)\r\ntf.summary.scalar('Generator_loss',g_loss)\r\n\r\nimages_for_tensorboard = generator(z_placeholder,batch_size,z_dimension)\r\ntf.summary.image('Generated_images',images_for_tensorboard,5)\r\nmerged = tf.summary.merge_all()\r\nlogdir = 'Tensorboard/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S') + '/'\r\nwriter = tf.summary.FileWriter(logdir,sess.graph)\r\n\r\nsess = tf.Session()\r\nsess.run(tf.global_variables_initializer())\r\n\r\n#Pre-train discriminator\r\nfor i in range(3000):\r\n    z_batch = np.random.normal(0, 1, size=[batch_size,z_dimension])\r\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\r\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\r\n                                       feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\r\n    if(i%100==0):\r\n        print ('dLossReal: ',dLossReal,'dLossFake: ',dLossFake)\r\n\r\n#Train discriminator and generator together\r\nfor i in range(100000):\r\n    real_image_batch = mnist.train.next_batch(batch_size)[0].reshape([batch_size,28,28,1])\r\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n\r\n    _,__,dLossReal,dLossFake = sess.run([d_trainer_real,d_trainer_fake,d_loss_real,d_loss_fake],\r\n                                        feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\r\n    z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n    _ = sess.run(g_trainer,feed_dict={z_placeholder:z_batch})\r\n\r\n    if i%10 ==0:\r\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n        summary = sess.run(merged,feed_dict={x_placeholder:real_image_batch,z_placeholder:z_batch})\r\n        writer.add_summary(summary,i)\r\n\r\n    if i%100==0:\r\n        print ('Iteration:',i,'at',datetime.datetime.now())\r\n        z_batch = np.random.normal(0,1,size=[batch_size,z_dimension])\r\n        generated_image = generator(z_placeholder,1,z_dimension)\r\n        images = sess.run(generated_image,feed_dict={z_placeholder:z_batch})\r\n        plt.imshow(images[0].reshape([28,28]),cmap='Greys')\r\n        plt.savefig('Generated_images/'+str(i)+'.jpg')\r\n\r\n        img = images[0].reshape([1,28,28,1])\r\n        result = discriminator(x_placeholder)\r\n        estimate = sess.run(result,feed_dict={x_placeholder:img})\r\n        print ('Estimate:',estimate)\r\n\r\n```"}