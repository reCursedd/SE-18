{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/272683122", "html_url": "https://github.com/tensorflow/tensorflow/issues/6220#issuecomment-272683122", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6220", "id": 272683122, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MjY4MzEyMg==", "user": {"login": "wookayin", "id": 1009873, "node_id": "MDQ6VXNlcjEwMDk4NzM=", "avatar_url": "https://avatars3.githubusercontent.com/u/1009873?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wookayin", "html_url": "https://github.com/wookayin", "followers_url": "https://api.github.com/users/wookayin/followers", "following_url": "https://api.github.com/users/wookayin/following{/other_user}", "gists_url": "https://api.github.com/users/wookayin/gists{/gist_id}", "starred_url": "https://api.github.com/users/wookayin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wookayin/subscriptions", "organizations_url": "https://api.github.com/users/wookayin/orgs", "repos_url": "https://api.github.com/users/wookayin/repos", "events_url": "https://api.github.com/users/wookayin/events{/privacy}", "received_events_url": "https://api.github.com/users/wookayin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-15T09:13:41Z", "updated_at": "2017-01-15T09:19:02Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> Hello, I found that your workaround to put a variable_scope which wraps the outermost num_gpus loop, but I am still confused why it does eliminate the error.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.variable_scope(tf.get_variable_scope()) <span class=\"pl-k\">as</span> vscope:\n  <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">FLAGS</span>.num_gpus):\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> i):\n      <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-c1\">%s</span>_<span class=\"pl-c1\">%d</span><span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">%</span> (cifar10.<span class=\"pl-c1\">TOWER_NAME</span>, i)) <span class=\"pl-k\">as</span> scope:\n        loss <span class=\"pl-k\">=</span> tower_loss(scope)\n        tf.get_variable_scope().reuse_variables()     <span class=\"pl-c\"><span class=\"pl-c\">#</span> HERE</span></pre></div>\n<p>Is it just because that the <code>tf.get_variable_scope()</code> (which is identical to <code>vscope</code>) is explicitly created than the implicit default? Then, what do these two <code>VariableScope</code> objects differ in?</p>\n<p>What do you mean by \"leaky reuse\"? Could you please clarify me?<br>\n/cc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15012648\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cesc-park\">@cesc-park</a></p>", "body_text": "@lukaszkaiser Hello, I found that your workaround to put a variable_scope which wraps the outermost num_gpus loop, but I am still confused why it does eliminate the error.\nwith tf.variable_scope(tf.get_variable_scope()) as vscope:\n  for i in xrange(FLAGS.num_gpus):\n    with tf.device('/gpu:%d' % i):\n      with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\n        loss = tower_loss(scope)\n        tf.get_variable_scope().reuse_variables()     # HERE\nIs it just because that the tf.get_variable_scope() (which is identical to vscope) is explicitly created than the implicit default? Then, what do these two VariableScope objects differ in?\nWhat do you mean by \"leaky reuse\"? Could you please clarify me?\n/cc @cesc-park", "body": "@lukaszkaiser Hello, I found that your workaround to put a variable_scope which wraps the outermost num_gpus loop, but I am still confused why it does eliminate the error. \r\n\r\n```python\r\nwith tf.variable_scope(tf.get_variable_scope()) as vscope:\r\n  for i in xrange(FLAGS.num_gpus):\r\n    with tf.device('/gpu:%d' % i):\r\n      with tf.name_scope('%s_%d' % (cifar10.TOWER_NAME, i)) as scope:\r\n        loss = tower_loss(scope)\r\n        tf.get_variable_scope().reuse_variables()     # HERE\r\n```\r\n\r\nIs it just because that the `tf.get_variable_scope()` (which is identical to `vscope`) is explicitly created than the implicit default? Then, what do these two `VariableScope` objects differ in?\r\n\r\nWhat do you mean by \"leaky reuse\"? Could you please clarify me? \r\n/cc @cesc-park "}