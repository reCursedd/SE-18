{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17064", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17064/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17064/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17064/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17064", "id": 297752052, "node_id": "MDU6SXNzdWUyOTc3NTIwNTI=", "number": 17064, "title": "Allocating C++ types instead of Tensors in a new Op - Feature Request", "user": {"login": "MiguelMonteiro", "id": 29569659, "node_id": "MDQ6VXNlcjI5NTY5NjU5", "avatar_url": "https://avatars3.githubusercontent.com/u/29569659?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MiguelMonteiro", "html_url": "https://github.com/MiguelMonteiro", "followers_url": "https://api.github.com/users/MiguelMonteiro/followers", "following_url": "https://api.github.com/users/MiguelMonteiro/following{/other_user}", "gists_url": "https://api.github.com/users/MiguelMonteiro/gists{/gist_id}", "starred_url": "https://api.github.com/users/MiguelMonteiro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MiguelMonteiro/subscriptions", "organizations_url": "https://api.github.com/users/MiguelMonteiro/orgs", "repos_url": "https://api.github.com/users/MiguelMonteiro/repos", "events_url": "https://api.github.com/users/MiguelMonteiro/events{/privacy}", "received_events_url": "https://api.github.com/users/MiguelMonteiro/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2018-02-16T11:15:36Z", "updated_at": "2018-03-27T17:50:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS</li>\n<li>TensorFlow installed from (source or binary): No</li>\n<li>TensorFlow version: 1.5</li>\n<li>Python version: 2.7</li>\n<li>Bazel version (if compiling from source): N/A</li>\n<li>GCC/Compiler version (if compiling from source): N/A</li>\n<li>CUDA/cuDNN version: N/A</li>\n<li>GPU model and memory: N/A</li>\n<li>Exact command to reproduce: N/A</li>\n</ul>\n<h3>Feature Request</h3>\n<h5>Feature requested: the ability to allocate memory in a new op as C++ types and not only as tensors;</h5>\n<p>I have been implementing a new op on Tensorflow following the <a href=\"https://www.tensorflow.org/extend/adding_an_op\" rel=\"nofollow\">guide</a> and have noticed what could be a useful feature for people implementing new ops in C++/CUDA.</p>\n<p>Currently using <code>OpKernelConstruction* context</code> it is only possible to allocate memory (CPU or GPU) in the form of tensors. For basic  C++ types you can obtain  a pointer for that type easily, for example:</p>\n<pre><code>Tensor tensor;\nOP_REQUIRES_OK(context, context-&gt;allocate_temp(DT_FLOAT,  TensorShape({5}), &amp;tensor));\nfloat * ptr = tensor.flat&lt;float&gt;.data();\n</code></pre>\n<p>However, for more complex types like <code>structs</code> this is not possible (or at least not direct). Why not have something like:</p>\n<pre><code>struct A {\n    int a;\n    int b;\n};\nA * a = nullptr;\nOP_REQUIRES_OK(context, context-&gt;allocate_bytes(n_bytes=sizeof(A), address = a));\n</code></pre>\n<p>I think this could be useful when porting C++ code from elsewhere and simplifies memory allocation of non-tensor types in C++.<br>\nCould this be a useful feature or is there a good reason it's not implemented?</p>\n<h3>Note:</h3>\n<p>One possible work-around with the current system is to allocate a tensor of type <code>UINT_8</code>with the number of bytes required and then used <code>reinterpret_cast </code>:</p>\n<pre><code>struct A {\n    int a;\n    int b;\n};\nA * a = nullptr;\nTensor tensor;\nOP_REQUIRES_OK(context, context-&gt;allocate_temp(DT_UINT8, TensorShape({sizeof(A)}), &amp;tensor));\na = reinterpret_cast&lt;A*&gt;(tensor.flat&lt;unsigned char&gt;().data());\n</code></pre>\n<p>This kind of feels like cheating and over complex just for allocating memory.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\nTensorFlow installed from (source or binary): No\nTensorFlow version: 1.5\nPython version: 2.7\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nFeature Request\nFeature requested: the ability to allocate memory in a new op as C++ types and not only as tensors;\nI have been implementing a new op on Tensorflow following the guide and have noticed what could be a useful feature for people implementing new ops in C++/CUDA.\nCurrently using OpKernelConstruction* context it is only possible to allocate memory (CPU or GPU) in the form of tensors. For basic  C++ types you can obtain  a pointer for that type easily, for example:\nTensor tensor;\nOP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT,  TensorShape({5}), &tensor));\nfloat * ptr = tensor.flat<float>.data();\n\nHowever, for more complex types like structs this is not possible (or at least not direct). Why not have something like:\nstruct A {\n    int a;\n    int b;\n};\nA * a = nullptr;\nOP_REQUIRES_OK(context, context->allocate_bytes(n_bytes=sizeof(A), address = a));\n\nI think this could be useful when porting C++ code from elsewhere and simplifies memory allocation of non-tensor types in C++.\nCould this be a useful feature or is there a good reason it's not implemented?\nNote:\nOne possible work-around with the current system is to allocate a tensor of type UINT_8with the number of bytes required and then used reinterpret_cast :\nstruct A {\n    int a;\n    int b;\n};\nA * a = nullptr;\nTensor tensor;\nOP_REQUIRES_OK(context, context->allocate_temp(DT_UINT8, TensorShape({sizeof(A)}), &tensor));\na = reinterpret_cast<A*>(tensor.flat<unsigned char>().data());\n\nThis kind of feels like cheating and over complex just for allocating memory.", "body": "### System information\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow): No\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS\r\n- TensorFlow installed from (source or binary): No\r\n- TensorFlow version: 1.5\r\n- Python version: 2.7\r\n- Bazel version (if compiling from source): N/A\r\n- GCC/Compiler version (if compiling from source): N/A\r\n- CUDA/cuDNN version: N/A\r\n- GPU model and memory: N/A\r\n- Exact command to reproduce: N/A\r\n\r\n### Feature Request\r\n##### Feature requested: the ability to allocate memory in a new op as C++ types and not only as tensors;\r\n\r\nI have been implementing a new op on Tensorflow following the [guide](https://www.tensorflow.org/extend/adding_an_op) and have noticed what could be a useful feature for people implementing new ops in C++/CUDA.\r\n\r\nCurrently using `OpKernelConstruction* context` it is only possible to allocate memory (CPU or GPU) in the form of tensors. For basic  C++ types you can obtain  a pointer for that type easily, for example:\r\n\r\n```\r\nTensor tensor;\r\nOP_REQUIRES_OK(context, context->allocate_temp(DT_FLOAT,  TensorShape({5}), &tensor));\r\nfloat * ptr = tensor.flat<float>.data();\r\n```\r\n\r\nHowever, for more complex types like `structs` this is not possible (or at least not direct). Why not have something like:\r\n\r\n```\r\nstruct A {\r\n    int a;\r\n    int b;\r\n};\r\nA * a = nullptr;\r\nOP_REQUIRES_OK(context, context->allocate_bytes(n_bytes=sizeof(A), address = a));\r\n```\r\n\r\nI think this could be useful when porting C++ code from elsewhere and simplifies memory allocation of non-tensor types in C++.\r\nCould this be a useful feature or is there a good reason it's not implemented?\r\n\r\n### Note:\r\nOne possible work-around with the current system is to allocate a tensor of type `UINT_8`with the number of bytes required and then used `reinterpret_cast `:\r\n\r\n```\r\nstruct A {\r\n    int a;\r\n    int b;\r\n};\r\nA * a = nullptr;\r\nTensor tensor;\r\nOP_REQUIRES_OK(context, context->allocate_temp(DT_UINT8, TensorShape({sizeof(A)}), &tensor));\r\na = reinterpret_cast<A*>(tensor.flat<unsigned char>().data());\r\n```\r\nThis kind of feels like cheating and over complex just for allocating memory.\r\n"}