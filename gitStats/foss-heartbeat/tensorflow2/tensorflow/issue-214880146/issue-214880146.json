{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8484", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8484/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8484/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8484/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8484", "id": 214880146, "node_id": "MDU6SXNzdWUyMTQ4ODAxNDY=", "number": 8484, "title": "sparse_softmax_cross_entropy_with_logits returns NaN instead of raising an error when the class int is not in the range [0, logit_size) when run on GPU", "user": {"login": "w4nderlust", "id": 349256, "node_id": "MDQ6VXNlcjM0OTI1Ng==", "avatar_url": "https://avatars1.githubusercontent.com/u/349256?v=4", "gravatar_id": "", "url": "https://api.github.com/users/w4nderlust", "html_url": "https://github.com/w4nderlust", "followers_url": "https://api.github.com/users/w4nderlust/followers", "following_url": "https://api.github.com/users/w4nderlust/following{/other_user}", "gists_url": "https://api.github.com/users/w4nderlust/gists{/gist_id}", "starred_url": "https://api.github.com/users/w4nderlust/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/w4nderlust/subscriptions", "organizations_url": "https://api.github.com/users/w4nderlust/orgs", "repos_url": "https://api.github.com/users/w4nderlust/repos", "events_url": "https://api.github.com/users/w4nderlust/events{/privacy}", "received_events_url": "https://api.github.com/users/w4nderlust/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2017-03-17T00:57:00Z", "updated_at": "2018-10-20T02:03:40Z", "closed_at": "2017-06-16T20:50:55Z", "author_association": "NONE", "body_html": "<p>If you provide to tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits() labels that are not inside the number of classes  (that is the length of the logits, or the length of the second dimension of the logits if using the first dimension for batching), you get a loss of NaN and then everything in the model becomes NaN. I spent quite some time debugging this in my model and I believe that this should not be silent, there should be at least a warning at execution time.</p>\n<p>More detail I just discovered: if you run it on CPU an InvalidAgrumentError is raised, if you run it on GPU, you get NaN, so you may want to fix only the GPU implementation to behave like the CPU one.</p>\n<p>Environment info</p>\n<p>Operating System: Ubuntu 16.04 64bit<br>\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1<br>\nTensorflow version: 1.0.0</p>\n<p>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</p>\n<p>Run it with CUDA_VISIBLE_DEVICE=\"\" to see the difference on CPU and GPU.</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nclasses = 5\nnum_datapoints = 100\n\nxs = np.random.rand(num_datapoints,4)\nys = np.random.randint(classes + 1, size=num_datapoints)\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x = tf.placeholder(tf.float32, shape=[None, 4])\n    y = tf.placeholder(tf.int32, shape=[None, ])\n\n    w = tf.Variable(tf.random_normal([4, classes]))\n    b = tf.Variable(tf.random_normal([classes]))\n\n    logits = tf.matmul(x, w) + b\n\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    for step in range(201):\n        _, loss_val = session.run(\n            [optimizer, loss],\n            feed_dict={x: xs, y: ys})\n        print(\"step {step} - loss: {loss_val:.6f}\".format(step=step, loss_val=loss_val))\n</code></pre>", "body_text": "If you provide to tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits() labels that are not inside the number of classes  (that is the length of the logits, or the length of the second dimension of the logits if using the first dimension for batching), you get a loss of NaN and then everything in the model becomes NaN. I spent quite some time debugging this in my model and I believe that this should not be silent, there should be at least a warning at execution time.\nMore detail I just discovered: if you run it on CPU an InvalidAgrumentError is raised, if you run it on GPU, you get NaN, so you may want to fix only the GPU implementation to behave like the CPU one.\nEnvironment info\nOperating System: Ubuntu 16.04 64bit\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\nTensorflow version: 1.0.0\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nRun it with CUDA_VISIBLE_DEVICE=\"\" to see the difference on CPU and GPU.\nimport numpy as np\nimport tensorflow as tf\n\nclasses = 5\nnum_datapoints = 100\n\nxs = np.random.rand(num_datapoints,4)\nys = np.random.randint(classes + 1, size=num_datapoints)\n\ngraph = tf.Graph()\nwith graph.as_default():\n    x = tf.placeholder(tf.float32, shape=[None, 4])\n    y = tf.placeholder(tf.int32, shape=[None, ])\n\n    w = tf.Variable(tf.random_normal([4, classes]))\n    b = tf.Variable(tf.random_normal([classes]))\n\n    logits = tf.matmul(x, w) + b\n\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\n    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\n\n\nwith tf.Session(graph=graph) as session:\n    tf.global_variables_initializer().run()\n    for step in range(201):\n        _, loss_val = session.run(\n            [optimizer, loss],\n            feed_dict={x: xs, y: ys})\n        print(\"step {step} - loss: {loss_val:.6f}\".format(step=step, loss_val=loss_val))", "body": "If you provide to tf.nn.tf.nn.sparse_softmax_cross_entropy_with_logits() labels that are not inside the number of classes  (that is the length of the logits, or the length of the second dimension of the logits if using the first dimension for batching), you get a loss of NaN and then everything in the model becomes NaN. I spent quite some time debugging this in my model and I believe that this should not be silent, there should be at least a warning at execution time.\r\n\r\nMore detail I just discovered: if you run it on CPU an InvalidAgrumentError is raised, if you run it on GPU, you get NaN, so you may want to fix only the GPU implementation to behave like the CPU one.\r\n\r\nEnvironment info\r\n\r\nOperating System: Ubuntu 16.04 64bit\r\nInstalled version of CUDA and cuDNN: cuda 8.0 cudnn 5.1\r\nTensorflow version: 1.0.0\r\n\r\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n\r\nRun it with CUDA_VISIBLE_DEVICE=\"\" to see the difference on CPU and GPU.\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nclasses = 5\r\nnum_datapoints = 100\r\n\r\nxs = np.random.rand(num_datapoints,4)\r\nys = np.random.randint(classes + 1, size=num_datapoints)\r\n\r\ngraph = tf.Graph()\r\nwith graph.as_default():\r\n    x = tf.placeholder(tf.float32, shape=[None, 4])\r\n    y = tf.placeholder(tf.int32, shape=[None, ])\r\n\r\n    w = tf.Variable(tf.random_normal([4, classes]))\r\n    b = tf.Variable(tf.random_normal([classes]))\r\n\r\n    logits = tf.matmul(x, w) + b\r\n\r\n    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y))\r\n    optimizer = tf.train.AdamOptimizer(0.01).minimize(loss)\r\n\r\n\r\nwith tf.Session(graph=graph) as session:\r\n    tf.global_variables_initializer().run()\r\n    for step in range(201):\r\n        _, loss_val = session.run(\r\n            [optimizer, loss],\r\n            feed_dict={x: xs, y: ys})\r\n        print(\"step {step} - loss: {loss_val:.6f}\".format(step=step, loss_val=loss_val))\r\n```\r\n"}