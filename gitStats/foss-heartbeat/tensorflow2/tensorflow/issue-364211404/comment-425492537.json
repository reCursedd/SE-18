{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/425492537", "html_url": "https://github.com/tensorflow/tensorflow/issues/22551#issuecomment-425492537", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22551", "id": 425492537, "node_id": "MDEyOklzc3VlQ29tbWVudDQyNTQ5MjUzNw==", "user": {"login": "axch", "id": 233710, "node_id": "MDQ6VXNlcjIzMzcxMA==", "avatar_url": "https://avatars3.githubusercontent.com/u/233710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/axch", "html_url": "https://github.com/axch", "followers_url": "https://api.github.com/users/axch/followers", "following_url": "https://api.github.com/users/axch/following{/other_user}", "gists_url": "https://api.github.com/users/axch/gists{/gist_id}", "starred_url": "https://api.github.com/users/axch/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/axch/subscriptions", "organizations_url": "https://api.github.com/users/axch/orgs", "repos_url": "https://api.github.com/users/axch/repos", "events_url": "https://api.github.com/users/axch/events{/privacy}", "received_events_url": "https://api.github.com/users/axch/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-28T16:31:47Z", "updated_at": "2018-09-28T16:31:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Actually, you <em>would</em> expect about that many collisions in 10,000 draws from a space of 2^23 possibilities, because of the \"<a href=\"https://en.wikipedia.org/wiki/Birthday_problem\" rel=\"nofollow\">birthday paradox</a>\".  For instance,</p>\n<pre><code>N = int(1e4)\nlen(np.unique(np.random.randint(low=0, high=2**23, size=N)))\n9993\n</code></pre>\n<p>There are 2^23 32-bit floats between 1.0 and 2.0.  In that range, the behavior of numpy and of TensorFlow agree (once you correct for numpy's default floating point precision being 64 bits):</p>\n<pre><code>len(np.unique(np.random.uniform(low=1., high=2., size=N).astype(np.float32)))\n9992\nlen(np.unique(tf.distributions.Uniform(low=1., high=2.).sample(N).eval()))\n9994\n</code></pre>\n<p>The range 0.0 to 1.0 is more interesting, because floating-point precision increases exponentially near 0.  There are ~2^30 32-bit floats between 0.0 and 1.0, but, of course, the continuous-uniform distribution on [0.0, 1.0) does not map into the discrete-uniform distribution on those 2^30 floats, because the floats near 0 are denser and thus lower probability.  Here TensorFlow and numpy disagree somewhat: In the interest of speed, TensorFlow loses precision near 0 (internally, it actually generates a float between 1.0 and 2.0, and then subtracts 1).  Numpy goes to greater lengths to conserve this precision, but even that doesn't defeat the brithday attack completely: there are 2^23 32-bit floats between 0.5 and 1.0, so that space is sparse enough to generate collisions by itself regardless of how careful one is at 0:</p>\n<pre><code>for _ in range(10):\n  print(len(np.unique(np.random.uniform(low=0., high=1., size=N).astype(np.float32))))\n10000\n9999\n9999\n9998\n9996\n9995\n9998\n9999\n9996\n10000\n</code></pre>", "body_text": "Actually, you would expect about that many collisions in 10,000 draws from a space of 2^23 possibilities, because of the \"birthday paradox\".  For instance,\nN = int(1e4)\nlen(np.unique(np.random.randint(low=0, high=2**23, size=N)))\n9993\n\nThere are 2^23 32-bit floats between 1.0 and 2.0.  In that range, the behavior of numpy and of TensorFlow agree (once you correct for numpy's default floating point precision being 64 bits):\nlen(np.unique(np.random.uniform(low=1., high=2., size=N).astype(np.float32)))\n9992\nlen(np.unique(tf.distributions.Uniform(low=1., high=2.).sample(N).eval()))\n9994\n\nThe range 0.0 to 1.0 is more interesting, because floating-point precision increases exponentially near 0.  There are ~2^30 32-bit floats between 0.0 and 1.0, but, of course, the continuous-uniform distribution on [0.0, 1.0) does not map into the discrete-uniform distribution on those 2^30 floats, because the floats near 0 are denser and thus lower probability.  Here TensorFlow and numpy disagree somewhat: In the interest of speed, TensorFlow loses precision near 0 (internally, it actually generates a float between 1.0 and 2.0, and then subtracts 1).  Numpy goes to greater lengths to conserve this precision, but even that doesn't defeat the brithday attack completely: there are 2^23 32-bit floats between 0.5 and 1.0, so that space is sparse enough to generate collisions by itself regardless of how careful one is at 0:\nfor _ in range(10):\n  print(len(np.unique(np.random.uniform(low=0., high=1., size=N).astype(np.float32))))\n10000\n9999\n9999\n9998\n9996\n9995\n9998\n9999\n9996\n10000", "body": "Actually, you _would_ expect about that many collisions in 10,000 draws from a space of 2^23 possibilities, because of the \"[birthday paradox](https://en.wikipedia.org/wiki/Birthday_problem)\".  For instance,\r\n\r\n```\r\nN = int(1e4)\r\nlen(np.unique(np.random.randint(low=0, high=2**23, size=N)))\r\n9993\r\n```\r\n\r\nThere are 2^23 32-bit floats between 1.0 and 2.0.  In that range, the behavior of numpy and of TensorFlow agree (once you correct for numpy's default floating point precision being 64 bits):\r\n```\r\nlen(np.unique(np.random.uniform(low=1., high=2., size=N).astype(np.float32)))\r\n9992\r\nlen(np.unique(tf.distributions.Uniform(low=1., high=2.).sample(N).eval()))\r\n9994\r\n```\r\n\r\nThe range 0.0 to 1.0 is more interesting, because floating-point precision increases exponentially near 0.  There are ~2^30 32-bit floats between 0.0 and 1.0, but, of course, the continuous-uniform distribution on [0.0, 1.0) does not map into the discrete-uniform distribution on those 2^30 floats, because the floats near 0 are denser and thus lower probability.  Here TensorFlow and numpy disagree somewhat: In the interest of speed, TensorFlow loses precision near 0 (internally, it actually generates a float between 1.0 and 2.0, and then subtracts 1).  Numpy goes to greater lengths to conserve this precision, but even that doesn't defeat the brithday attack completely: there are 2^23 32-bit floats between 0.5 and 1.0, so that space is sparse enough to generate collisions by itself regardless of how careful one is at 0:\r\n\r\n```\r\nfor _ in range(10):\r\n  print(len(np.unique(np.random.uniform(low=0., high=1., size=N).astype(np.float32))))\r\n10000\r\n9999\r\n9999\r\n9998\r\n9996\r\n9995\r\n9998\r\n9999\r\n9996\r\n10000\r\n```"}