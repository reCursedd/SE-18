{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10216", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10216/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10216/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10216/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10216", "id": 231616447, "node_id": "MDU6SXNzdWUyMzE2MTY0NDc=", "number": 10216, "title": "tf.nn.max_pool_with_argmax bug with padding", "user": {"login": "Panaetius", "id": 664486, "node_id": "MDQ6VXNlcjY2NDQ4Ng==", "avatar_url": "https://avatars2.githubusercontent.com/u/664486?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Panaetius", "html_url": "https://github.com/Panaetius", "followers_url": "https://api.github.com/users/Panaetius/followers", "following_url": "https://api.github.com/users/Panaetius/following{/other_user}", "gists_url": "https://api.github.com/users/Panaetius/gists{/gist_id}", "starred_url": "https://api.github.com/users/Panaetius/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Panaetius/subscriptions", "organizations_url": "https://api.github.com/users/Panaetius/orgs", "repos_url": "https://api.github.com/users/Panaetius/repos", "events_url": "https://api.github.com/users/Panaetius/events{/privacy}", "received_events_url": "https://api.github.com/users/Panaetius/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-05-26T12:43:42Z", "updated_at": "2017-07-13T06:23:28Z", "closed_at": "2017-06-13T16:07:39Z", "author_association": "NONE", "body_html": "<p>There is a bug in the indices returned from <code>tf.nn.max_pool_with_argmax</code> when a padding is applied. The indices with be based on the shape supplied to <code>max_pool_with_argmax</code>, instead of the shape+padding.</p>\n<p>Simple code example to reproduce:</p>\n<pre><code>import tensorflow as tf\n\ndef main():\n    with tf.Session() as session:\n        input = tf.get_variable('weights',\n                                  shape=[1, 301, 201, 1],\n                                  initializer=tf.truncated_normal_initializer(stddev=0.5, dtype=tf.float32),\n                                  dtype=tf.float32)\n\n        val, idx = tf.nn.max_pool_with_argmax(input, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\") #padding will turn dimensions to 302x202\n\n        y1 = idx // 201\n        x1 = idx % 201\n\n        y2 = idx // 202\n        x2 = idx % 202\n\n        max_x1 = tf.reduce_max(x1)\n        max_y1 = tf.reduce_max(y1)\n        max_x2 = tf.reduce_max(x2)\n        max_y2 = tf.reduce_max(y2)\n\n\n        session.run(tf.global_variables_initializer())\n        m_x1, m_y1, m_x2, m_y2 = session.run([max_x1, max_y1, max_x2, max_y2])\n\n        print(\"%d, %d, %d, %d\"%(m_x1, m_y1, m_x2, m_y2))\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<p>This prints <code>\"200, 300, 201, 299\"</code>. As you can see, the padding would increase the dimensions of the tensor to <code>302x202</code>, so the maximum <code>y</code> coordinate should be <code>301</code> and <code>x</code> should be <code>201</code>. But if we unravel the argmax indices with a width of <code>202</code>, we get a maximum <code>x</code> of <code>201</code>, but maximum <code>y</code> of only <code>299</code>. If we instead use <code>201</code> as width for unraveling (the unpadded width of the input tensor), we get <code>200</code> and <code>300</code>, respectively, which are the correct values for the unpadded input tensor.</p>\n<p>So the <code>((b * height + y) * width + x) * channels + c</code> formula for <code>tf.nn.max_pool_with_argmax</code> uses the input tensor dimensions for <code>width</code>, not the input+padding dimensions.</p>\n<p>This is relevant if you then use the indices to unpool/reverse the max_pool, since often you'll multiple the dimensions of the max_pool output by 2 to get the input dimensions, which would be off with a naive implementation like this.</p>\n<p>When using this to implement an unpooling operation (for instance for SegNet), this will cause every line of the image to shift (by 1 pixel) if padding is applied to the width of an image,  basically slightly tilting an image. It's especially obvious with multiple argmax&amp;unpool in succession.</p>\n<p>It also means that, with zero-padding, if the whole tensor is negative values (in which case the zero-padding would be the highest value in the tensor), it won't return the coordinates of the padding. Basically, it doesn't actually add any padding to the input tensor, it just pretends it does to make dimensions line up, but doesn't consider the actual values/zeros in the padding. This might be intended behaviour, though it would strike me as odd given the usual understanding of padding. If so, the documentation should be changed to make it clear that this is the way the operation works. But I think this would be against the principle of least astonishment, since I assume most people would think that an op talking about padding would actually add padding values.</p>\n<p>See also <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"151878335\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2169\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2169/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2169\">#2169</a> for a use-case for this with additional discussion</p>", "body_text": "There is a bug in the indices returned from tf.nn.max_pool_with_argmax when a padding is applied. The indices with be based on the shape supplied to max_pool_with_argmax, instead of the shape+padding.\nSimple code example to reproduce:\nimport tensorflow as tf\n\ndef main():\n    with tf.Session() as session:\n        input = tf.get_variable('weights',\n                                  shape=[1, 301, 201, 1],\n                                  initializer=tf.truncated_normal_initializer(stddev=0.5, dtype=tf.float32),\n                                  dtype=tf.float32)\n\n        val, idx = tf.nn.max_pool_with_argmax(input, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\") #padding will turn dimensions to 302x202\n\n        y1 = idx // 201\n        x1 = idx % 201\n\n        y2 = idx // 202\n        x2 = idx % 202\n\n        max_x1 = tf.reduce_max(x1)\n        max_y1 = tf.reduce_max(y1)\n        max_x2 = tf.reduce_max(x2)\n        max_y2 = tf.reduce_max(y2)\n\n\n        session.run(tf.global_variables_initializer())\n        m_x1, m_y1, m_x2, m_y2 = session.run([max_x1, max_y1, max_x2, max_y2])\n\n        print(\"%d, %d, %d, %d\"%(m_x1, m_y1, m_x2, m_y2))\n\nif __name__ == \"__main__\":\n    main()\n\nThis prints \"200, 300, 201, 299\". As you can see, the padding would increase the dimensions of the tensor to 302x202, so the maximum y coordinate should be 301 and x should be 201. But if we unravel the argmax indices with a width of 202, we get a maximum x of 201, but maximum y of only 299. If we instead use 201 as width for unraveling (the unpadded width of the input tensor), we get 200 and 300, respectively, which are the correct values for the unpadded input tensor.\nSo the ((b * height + y) * width + x) * channels + c formula for tf.nn.max_pool_with_argmax uses the input tensor dimensions for width, not the input+padding dimensions.\nThis is relevant if you then use the indices to unpool/reverse the max_pool, since often you'll multiple the dimensions of the max_pool output by 2 to get the input dimensions, which would be off with a naive implementation like this.\nWhen using this to implement an unpooling operation (for instance for SegNet), this will cause every line of the image to shift (by 1 pixel) if padding is applied to the width of an image,  basically slightly tilting an image. It's especially obvious with multiple argmax&unpool in succession.\nIt also means that, with zero-padding, if the whole tensor is negative values (in which case the zero-padding would be the highest value in the tensor), it won't return the coordinates of the padding. Basically, it doesn't actually add any padding to the input tensor, it just pretends it does to make dimensions line up, but doesn't consider the actual values/zeros in the padding. This might be intended behaviour, though it would strike me as odd given the usual understanding of padding. If so, the documentation should be changed to make it clear that this is the way the operation works. But I think this would be against the principle of least astonishment, since I assume most people would think that an op talking about padding would actually add padding values.\nSee also #2169 for a use-case for this with additional discussion", "body": "There is a bug in the indices returned from `tf.nn.max_pool_with_argmax` when a padding is applied. The indices with be based on the shape supplied to `max_pool_with_argmax`, instead of the shape+padding.\r\n\r\nSimple code example to reproduce:\r\n```\r\nimport tensorflow as tf\r\n\r\ndef main():\r\n    with tf.Session() as session:\r\n        input = tf.get_variable('weights',\r\n                                  shape=[1, 301, 201, 1],\r\n                                  initializer=tf.truncated_normal_initializer(stddev=0.5, dtype=tf.float32),\r\n                                  dtype=tf.float32)\r\n\r\n        val, idx = tf.nn.max_pool_with_argmax(input, [1, 2, 2, 1], [1, 2, 2, 1], padding=\"SAME\") #padding will turn dimensions to 302x202\r\n\r\n        y1 = idx // 201\r\n        x1 = idx % 201\r\n\r\n        y2 = idx // 202\r\n        x2 = idx % 202\r\n\r\n        max_x1 = tf.reduce_max(x1)\r\n        max_y1 = tf.reduce_max(y1)\r\n        max_x2 = tf.reduce_max(x2)\r\n        max_y2 = tf.reduce_max(y2)\r\n\r\n\r\n        session.run(tf.global_variables_initializer())\r\n        m_x1, m_y1, m_x2, m_y2 = session.run([max_x1, max_y1, max_x2, max_y2])\r\n\r\n        print(\"%d, %d, %d, %d\"%(m_x1, m_y1, m_x2, m_y2))\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nThis prints `\"200, 300, 201, 299\"`. As you can see, the padding would increase the dimensions of the tensor to `302x202`, so the maximum `y` coordinate should be `301` and `x` should be `201`. But if we unravel the argmax indices with a width of `202`, we get a maximum `x` of `201`, but maximum `y` of only `299`. If we instead use `201` as width for unraveling (the unpadded width of the input tensor), we get `200` and `300`, respectively, which are the correct values for the unpadded input tensor.\r\n\r\nSo the `((b * height + y) * width + x) * channels + c` formula for `tf.nn.max_pool_with_argmax` uses the input tensor dimensions for `width`, not the input+padding dimensions.\r\n\r\nThis is relevant if you then use the indices to unpool/reverse the max_pool, since often you'll multiple the dimensions of the max_pool output by 2 to get the input dimensions, which would be off with a naive implementation like this.\r\n\r\nWhen using this to implement an unpooling operation (for instance for SegNet), this will cause every line of the image to shift (by 1 pixel) if padding is applied to the width of an image,  basically slightly tilting an image. It's especially obvious with multiple argmax&unpool in succession.\r\n\r\nIt also means that, with zero-padding, if the whole tensor is negative values (in which case the zero-padding would be the highest value in the tensor), it won't return the coordinates of the padding. Basically, it doesn't actually add any padding to the input tensor, it just pretends it does to make dimensions line up, but doesn't consider the actual values/zeros in the padding. This might be intended behaviour, though it would strike me as odd given the usual understanding of padding. If so, the documentation should be changed to make it clear that this is the way the operation works. But I think this would be against the principle of least astonishment, since I assume most people would think that an op talking about padding would actually add padding values.\r\n\r\nSee also #2169 for a use-case for this with additional discussion"}