{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7949", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7949/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7949/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7949/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7949", "id": 210884146, "node_id": "MDU6SXNzdWUyMTA4ODQxNDY=", "number": 7949, "title": "Can't do \"weights\" quantization on an LSTM RNN", "user": {"login": "reuben", "id": 477142, "node_id": "MDQ6VXNlcjQ3NzE0Mg==", "avatar_url": "https://avatars3.githubusercontent.com/u/477142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reuben", "html_url": "https://github.com/reuben", "followers_url": "https://api.github.com/users/reuben/followers", "following_url": "https://api.github.com/users/reuben/following{/other_user}", "gists_url": "https://api.github.com/users/reuben/gists{/gist_id}", "starred_url": "https://api.github.com/users/reuben/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reuben/subscriptions", "organizations_url": "https://api.github.com/users/reuben/orgs", "repos_url": "https://api.github.com/users/reuben/repos", "events_url": "https://api.github.com/users/reuben/events{/privacy}", "received_events_url": "https://api.github.com/users/reuben/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2017-02-28T19:37:58Z", "updated_at": "2018-03-22T01:22:48Z", "closed_at": "2017-03-01T17:29:57Z", "author_association": "NONE", "body_html": "<p>On TensorFlow 1.0, trying to do quantization on the following graph:</p>\n<div class=\"highlight highlight-source-python\"><pre>cell <span class=\"pl-k\">=</span> tf.contrib.rnn.BasicLSTMCell(<span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">64</span>)\n\noutputs, _ <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(\n    <span class=\"pl-v\">cell</span><span class=\"pl-k\">=</span>cell,\n    <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32,\n    <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>tf.constant([<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>]),\n    <span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>tf.constant([[[<span class=\"pl-c1\">1</span>.,<span class=\"pl-c1\">1</span>.,<span class=\"pl-c1\">1</span>.]], [[<span class=\"pl-c1\">1</span>.,<span class=\"pl-c1\">1</span>.,<span class=\"pl-c1\">0</span>.]]]))\n\noutputs <span class=\"pl-k\">=</span> tf.identity(outputs, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>)</pre></div>\n<p>Fails with:</p>\n<pre><code>Traceback (most recent call last):\n  File \"import.py\", line 18, in &lt;module&gt;\n    result = sess.run(outputs)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'rnn/while/Select_1' has inputs from different frames. The input 'rnn/while/basic_lstm_cell/add_1' is in frame ''. The input 'rnn/while/GreaterEqual_1' is in frame 'rnn/while/rnn/while/'.\n</code></pre>\n<p>Instructions to reproduce here: <a href=\"https://github.com/reuben/tf-export-test\">https://github.com/reuben/tf-export-test</a> (just run reproduce.sh)</p>\n<p>It seems like the quantization code is unaware of control flow contexts, so it creates a broken graph when adding dequantize nodes. Interestingly, using a BasicRNNCell instead of a BasicLSTMCell works. Maybe because BasicRNNCell only does a single matmul + bias_add, and those have quantized kernel implementations and don't need dequantize nodes? It's not clear to me.</p>", "body_text": "On TensorFlow 1.0, trying to do quantization on the following graph:\ncell = tf.contrib.rnn.BasicLSTMCell(num_units=64)\n\noutputs, _ = tf.nn.dynamic_rnn(\n    cell=cell,\n    dtype=tf.float32,\n    sequence_length=tf.constant([3, 2]),\n    inputs=tf.constant([[[1.,1.,1.]], [[1.,1.,0.]]]))\n\noutputs = tf.identity(outputs, name=\"y\")\nFails with:\nTraceback (most recent call last):\n  File \"import.py\", line 18, in <module>\n    result = sess.run(outputs)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\n    run_metadata_ptr)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\n    feed_dict_string, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\n    target_list, options, run_metadata)\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'rnn/while/Select_1' has inputs from different frames. The input 'rnn/while/basic_lstm_cell/add_1' is in frame ''. The input 'rnn/while/GreaterEqual_1' is in frame 'rnn/while/rnn/while/'.\n\nInstructions to reproduce here: https://github.com/reuben/tf-export-test (just run reproduce.sh)\nIt seems like the quantization code is unaware of control flow contexts, so it creates a broken graph when adding dequantize nodes. Interestingly, using a BasicRNNCell instead of a BasicLSTMCell works. Maybe because BasicRNNCell only does a single matmul + bias_add, and those have quantized kernel implementations and don't need dequantize nodes? It's not clear to me.", "body": "On TensorFlow 1.0, trying to do quantization on the following graph:\r\n\r\n```python\r\ncell = tf.contrib.rnn.BasicLSTMCell(num_units=64)\r\n\r\noutputs, _ = tf.nn.dynamic_rnn(\r\n    cell=cell,\r\n    dtype=tf.float32,\r\n    sequence_length=tf.constant([3, 2]),\r\n    inputs=tf.constant([[[1.,1.,1.]], [[1.,1.,0.]]]))\r\n\r\noutputs = tf.identity(outputs, name=\"y\")\r\n```\r\n\r\nFails with:\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"import.py\", line 18, in <module>\r\n    result = sess.run(outputs)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 767, in run\r\n    run_metadata_ptr)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 965, in _run\r\n    feed_dict_string, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1015, in _do_run\r\n    target_list, options, run_metadata)\r\n  File \"/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 1035, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: The node 'rnn/while/Select_1' has inputs from different frames. The input 'rnn/while/basic_lstm_cell/add_1' is in frame ''. The input 'rnn/while/GreaterEqual_1' is in frame 'rnn/while/rnn/while/'.\r\n```\r\n\r\nInstructions to reproduce here: https://github.com/reuben/tf-export-test (just run reproduce.sh)\r\n\r\nIt seems like the quantization code is unaware of control flow contexts, so it creates a broken graph when adding dequantize nodes. Interestingly, using a BasicRNNCell instead of a BasicLSTMCell works. Maybe because BasicRNNCell only does a single matmul + bias_add, and those have quantized kernel implementations and don't need dequantize nodes? It's not clear to me."}