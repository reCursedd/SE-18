{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22709", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22709/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22709/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22709/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22709", "id": 366577256, "node_id": "MDU6SXNzdWUzNjY1NzcyNTY=", "number": 22709, "title": "[Cloud TPU] `dot` with non-standard layout operands produces incorrect output", "user": {"login": "Keno", "id": 1291671, "node_id": "MDQ6VXNlcjEyOTE2NzE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1291671?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Keno", "html_url": "https://github.com/Keno", "followers_url": "https://api.github.com/users/Keno/followers", "following_url": "https://api.github.com/users/Keno/following{/other_user}", "gists_url": "https://api.github.com/users/Keno/gists{/gist_id}", "starred_url": "https://api.github.com/users/Keno/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Keno/subscriptions", "organizations_url": "https://api.github.com/users/Keno/orgs", "repos_url": "https://api.github.com/users/Keno/repos", "events_url": "https://api.github.com/users/Keno/events{/privacy}", "received_events_url": "https://api.github.com/users/Keno/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "michaelisard", "id": 5376757, "node_id": "MDQ6VXNlcjUzNzY3NTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/5376757?v=4", "gravatar_id": "", "url": "https://api.github.com/users/michaelisard", "html_url": "https://github.com/michaelisard", "followers_url": "https://api.github.com/users/michaelisard/followers", "following_url": "https://api.github.com/users/michaelisard/following{/other_user}", "gists_url": "https://api.github.com/users/michaelisard/gists{/gist_id}", "starred_url": "https://api.github.com/users/michaelisard/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/michaelisard/subscriptions", "organizations_url": "https://api.github.com/users/michaelisard/orgs", "repos_url": "https://api.github.com/users/michaelisard/repos", "events_url": "https://api.github.com/users/michaelisard/events{/privacy}", "received_events_url": "https://api.github.com/users/michaelisard/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-10-04T00:20:06Z", "updated_at": "2018-10-18T23:33:39Z", "closed_at": "2018-10-18T23:33:39Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 18.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.11</li>\n<li><strong>Python version</strong>: N/A</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.16.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 7.2.0</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>We are seeing incorrect answers from VGG19 from Cloud TPUs using the Julia frontend (VGG19 source here: <a href=\"https://github.com/FluxML/Metalhead.jl/blob/master/src/vgg19.jl\">https://github.com/FluxML/Metalhead.jl/blob/master/src/vgg19.jl</a>). The Julia frontend generates XLA and submits it to the Cloud TPU using xrt. Reducing the test case, we find that there seems to be a problem with the <code>dot</code> operation when operating on arrays of non-standard data layout (julia is column major by default so tends to encounter this more, although the Julia-&gt;XLA compiler pass heuristically changes the layout of inputs/outputs when targeting TPUs to avoid padding if profitable). Example output showing the problem:</p>\n<pre><code>julia&gt; W\u2032\u2032 = XRTArray(sess, W\u2032)\n10\u00d710 XRTArray{Float32,(10, 10),2}:\n  0.0117621   -0.0111235    0.0105805   -0.00403011   -0.00355716  -0.00718089  -0.0134884    -0.00459831    0.0197616    -0.00417151\n -0.00695795  -0.00393374   0.0090458   -0.00566363    0.0121302    0.00420088  -0.00750095    0.000967382   0.0220296     0.00428639\n -0.00687687  -0.00229628  -0.007712    -0.00997357   -0.00680831   0.00262956   0.000473469  -0.00876678   -0.011446     -0.00720634\n  0.00530032   0.00782367  -0.00427682   0.00691867   -0.00343559   0.00912634  -0.0111816    -0.00729117    0.00522226   -0.00822838\n -0.00600947  -0.00683288  -0.0122932    0.000471149  -0.00142788   0.0201943    0.000141306   0.0052818     0.00387177   -0.00254829\n  0.00345886   0.0106005   -3.07181e-5   0.0217212     0.00135223  -0.00411474   0.00345068   -0.00515104    0.00140949   -0.0132586\n -0.00233608  -0.0117845   -0.00938103   0.0190475    -0.00731929  -0.00104086  -0.0144478     0.00959422    0.00774611   -0.00810476\n -0.00620138  -0.00673539  -0.00782188   0.00821768    0.00439068   0.00334791  -0.00190924   -0.00682392   -0.00805664    0.0150204\n -0.00116647  -0.00305227   0.00475127   9.85245e-7    0.0024828    0.013036     0.00340338   -0.00387893    0.000677002   0.00680005\n -0.00499851   0.0115887   -0.00762315   0.0148181     0.0112692    0.00543651   0.0137934    -0.00666574    0.00736139    0.00202298\n\njulia&gt; x\u2032\u2032 = XRTArray(sess, x\u2032)\n10\u00d71 XRTArray{Float32,(10, 1),2}:\n 0.0\n 6.706906\n 0.0\n 0.0\n 2.0117874\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia&gt; W\u2032\u2032 * x\u2032\u2032\n10\u00d71 XRTArray{Float32,(10, 1),2}:\n -0.081760176\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n</code></pre>\n<p>The first value is correct, the rest of them are not (however I do believe I've seen cases where the first value was incorrect but non-zero also) - for python folks remember that <code>*</code> is matrix multiply in Julia. Dumping the XLA generated during the above session, we get:</p>\n<pre><code>ENTRY comp {\n  comp0_parameter0 = f32[10,10]{0,1} parameter(0)\n  comp0_parameter1 = f32[10,1]{0,1} parameter(1)\n  ROOT comp0_dot3 = f32[10,1]{0,1} dot(comp0_parameter0, comp0_parameter1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n</code></pre>\n<p>Notice in particular the layout being <code>{0,1}</code> rather than the standard <code>{1,0}</code>. If I change the above mentioned layout heuristic to prefer the XLA standard layout, the issue disappears. The issue also disappears if the RHS is a plain vector, i.e. <code>f32[10]{0}</code> rather than <code>f32[10,1]{0,1}</code>. Further note that this issue was reduced from a case where the <code>dot</code> was not the final operation, but the zeros it generated nonetheless propagated. To me that would indicate that the output values are being written to the wrong place by the <code>dot</code> operation (as opposed to being a problem with the way that xrt retrieves the data from the device for example).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5376757\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/michaelisard\">@michaelisard</a> remarked that this sounded like a padding issue.<br>\ncc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1130906\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/eliben\">@eliben</a></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 18.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): r1.11\nPython version: N/A\nBazel version (if compiling from source): 0.16.1\nGCC/Compiler version (if compiling from source): 7.2.0\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nWe are seeing incorrect answers from VGG19 from Cloud TPUs using the Julia frontend (VGG19 source here: https://github.com/FluxML/Metalhead.jl/blob/master/src/vgg19.jl). The Julia frontend generates XLA and submits it to the Cloud TPU using xrt. Reducing the test case, we find that there seems to be a problem with the dot operation when operating on arrays of non-standard data layout (julia is column major by default so tends to encounter this more, although the Julia->XLA compiler pass heuristically changes the layout of inputs/outputs when targeting TPUs to avoid padding if profitable). Example output showing the problem:\njulia> W\u2032\u2032 = XRTArray(sess, W\u2032)\n10\u00d710 XRTArray{Float32,(10, 10),2}:\n  0.0117621   -0.0111235    0.0105805   -0.00403011   -0.00355716  -0.00718089  -0.0134884    -0.00459831    0.0197616    -0.00417151\n -0.00695795  -0.00393374   0.0090458   -0.00566363    0.0121302    0.00420088  -0.00750095    0.000967382   0.0220296     0.00428639\n -0.00687687  -0.00229628  -0.007712    -0.00997357   -0.00680831   0.00262956   0.000473469  -0.00876678   -0.011446     -0.00720634\n  0.00530032   0.00782367  -0.00427682   0.00691867   -0.00343559   0.00912634  -0.0111816    -0.00729117    0.00522226   -0.00822838\n -0.00600947  -0.00683288  -0.0122932    0.000471149  -0.00142788   0.0201943    0.000141306   0.0052818     0.00387177   -0.00254829\n  0.00345886   0.0106005   -3.07181e-5   0.0217212     0.00135223  -0.00411474   0.00345068   -0.00515104    0.00140949   -0.0132586\n -0.00233608  -0.0117845   -0.00938103   0.0190475    -0.00731929  -0.00104086  -0.0144478     0.00959422    0.00774611   -0.00810476\n -0.00620138  -0.00673539  -0.00782188   0.00821768    0.00439068   0.00334791  -0.00190924   -0.00682392   -0.00805664    0.0150204\n -0.00116647  -0.00305227   0.00475127   9.85245e-7    0.0024828    0.013036     0.00340338   -0.00387893    0.000677002   0.00680005\n -0.00499851   0.0115887   -0.00762315   0.0148181     0.0112692    0.00543651   0.0137934    -0.00666574    0.00736139    0.00202298\n\njulia> x\u2032\u2032 = XRTArray(sess, x\u2032)\n10\u00d71 XRTArray{Float32,(10, 1),2}:\n 0.0\n 6.706906\n 0.0\n 0.0\n 2.0117874\n 0.0\n 0.0\n 0.0\n 0.0\n 0.0\n\njulia> W\u2032\u2032 * x\u2032\u2032\n10\u00d71 XRTArray{Float32,(10, 1),2}:\n -0.081760176\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n  0.0\n\nThe first value is correct, the rest of them are not (however I do believe I've seen cases where the first value was incorrect but non-zero also) - for python folks remember that * is matrix multiply in Julia. Dumping the XLA generated during the above session, we get:\nENTRY comp {\n  comp0_parameter0 = f32[10,10]{0,1} parameter(0)\n  comp0_parameter1 = f32[10,1]{0,1} parameter(1)\n  ROOT comp0_dot3 = f32[10,1]{0,1} dot(comp0_parameter0, comp0_parameter1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\n}\n\nNotice in particular the layout being {0,1} rather than the standard {1,0}. If I change the above mentioned layout heuristic to prefer the XLA standard layout, the issue disappears. The issue also disappears if the RHS is a plain vector, i.e. f32[10]{0} rather than f32[10,1]{0,1}. Further note that this issue was reduced from a case where the dot was not the final operation, but the zeros it generated nonetheless propagated. To me that would indicate that the output values are being written to the wrong place by the dot operation (as opposed to being a problem with the way that xrt retrieves the data from the device for example).\n@michaelisard remarked that this sounded like a padding issue.\ncc @eliben", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 18.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: r1.11\r\n- **Python version**: N/A\r\n- **Bazel version (if compiling from source)**: 0.16.1\r\n- **GCC/Compiler version (if compiling from source)**: 7.2.0\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\n\r\nWe are seeing incorrect answers from VGG19 from Cloud TPUs using the Julia frontend (VGG19 source here: https://github.com/FluxML/Metalhead.jl/blob/master/src/vgg19.jl). The Julia frontend generates XLA and submits it to the Cloud TPU using xrt. Reducing the test case, we find that there seems to be a problem with the `dot` operation when operating on arrays of non-standard data layout (julia is column major by default so tends to encounter this more, although the Julia->XLA compiler pass heuristically changes the layout of inputs/outputs when targeting TPUs to avoid padding if profitable). Example output showing the problem:\r\n\r\n```\r\njulia> W\u2032\u2032 = XRTArray(sess, W\u2032)\r\n10\u00d710 XRTArray{Float32,(10, 10),2}:\r\n  0.0117621   -0.0111235    0.0105805   -0.00403011   -0.00355716  -0.00718089  -0.0134884    -0.00459831    0.0197616    -0.00417151\r\n -0.00695795  -0.00393374   0.0090458   -0.00566363    0.0121302    0.00420088  -0.00750095    0.000967382   0.0220296     0.00428639\r\n -0.00687687  -0.00229628  -0.007712    -0.00997357   -0.00680831   0.00262956   0.000473469  -0.00876678   -0.011446     -0.00720634\r\n  0.00530032   0.00782367  -0.00427682   0.00691867   -0.00343559   0.00912634  -0.0111816    -0.00729117    0.00522226   -0.00822838\r\n -0.00600947  -0.00683288  -0.0122932    0.000471149  -0.00142788   0.0201943    0.000141306   0.0052818     0.00387177   -0.00254829\r\n  0.00345886   0.0106005   -3.07181e-5   0.0217212     0.00135223  -0.00411474   0.00345068   -0.00515104    0.00140949   -0.0132586\r\n -0.00233608  -0.0117845   -0.00938103   0.0190475    -0.00731929  -0.00104086  -0.0144478     0.00959422    0.00774611   -0.00810476\r\n -0.00620138  -0.00673539  -0.00782188   0.00821768    0.00439068   0.00334791  -0.00190924   -0.00682392   -0.00805664    0.0150204\r\n -0.00116647  -0.00305227   0.00475127   9.85245e-7    0.0024828    0.013036     0.00340338   -0.00387893    0.000677002   0.00680005\r\n -0.00499851   0.0115887   -0.00762315   0.0148181     0.0112692    0.00543651   0.0137934    -0.00666574    0.00736139    0.00202298\r\n\r\njulia> x\u2032\u2032 = XRTArray(sess, x\u2032)\r\n10\u00d71 XRTArray{Float32,(10, 1),2}:\r\n 0.0\r\n 6.706906\r\n 0.0\r\n 0.0\r\n 2.0117874\r\n 0.0\r\n 0.0\r\n 0.0\r\n 0.0\r\n 0.0\r\n\r\njulia> W\u2032\u2032 * x\u2032\u2032\r\n10\u00d71 XRTArray{Float32,(10, 1),2}:\r\n -0.081760176\r\n  0.0\r\n  0.0\r\n  0.0\r\n  0.0\r\n  0.0\r\n  0.0\r\n  0.0\r\n  0.0\r\n  0.0\r\n```\r\n\r\nThe first value is correct, the rest of them are not (however I do believe I've seen cases where the first value was incorrect but non-zero also) - for python folks remember that `*` is matrix multiply in Julia. Dumping the XLA generated during the above session, we get:\r\n\r\n```\r\nENTRY comp {\r\n  comp0_parameter0 = f32[10,10]{0,1} parameter(0)\r\n  comp0_parameter1 = f32[10,1]{0,1} parameter(1)\r\n  ROOT comp0_dot3 = f32[10,1]{0,1} dot(comp0_parameter0, comp0_parameter1), lhs_contracting_dims={1}, rhs_contracting_dims={0}\r\n}\r\n```\r\n\r\nNotice in particular the layout being `{0,1}` rather than the standard `{1,0}`. If I change the above mentioned layout heuristic to prefer the XLA standard layout, the issue disappears. The issue also disappears if the RHS is a plain vector, i.e. `f32[10]{0}` rather than `f32[10,1]{0,1}`. Further note that this issue was reduced from a case where the `dot` was not the final operation, but the zeros it generated nonetheless propagated. To me that would indicate that the output values are being written to the wrong place by the `dot` operation (as opposed to being a problem with the way that xrt retrieves the data from the device for example).\r\n\r\n@michaelisard remarked that this sounded like a padding issue.\r\ncc @eliben "}