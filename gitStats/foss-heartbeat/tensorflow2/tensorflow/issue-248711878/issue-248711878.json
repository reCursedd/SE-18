{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12107", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12107/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12107/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12107/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/12107", "id": 248711878, "node_id": "MDExOlB1bGxSZXF1ZXN0MTM0NjU3ODcz", "number": 12107, "title": "Corrected dimension notes in attention_wrapper.py", "user": {"login": "rubenvereecken", "id": 5216553, "node_id": "MDQ6VXNlcjUyMTY1NTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/5216553?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rubenvereecken", "html_url": "https://github.com/rubenvereecken", "followers_url": "https://api.github.com/users/rubenvereecken/followers", "following_url": "https://api.github.com/users/rubenvereecken/following{/other_user}", "gists_url": "https://api.github.com/users/rubenvereecken/gists{/gist_id}", "starred_url": "https://api.github.com/users/rubenvereecken/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rubenvereecken/subscriptions", "organizations_url": "https://api.github.com/users/rubenvereecken/orgs", "repos_url": "https://api.github.com/users/rubenvereecken/repos", "events_url": "https://api.github.com/users/rubenvereecken/events{/privacy}", "received_events_url": "https://api.github.com/users/rubenvereecken/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-08-08T13:17:20Z", "updated_at": "2017-08-10T20:14:23Z", "closed_at": "2017-08-10T17:49:35Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/12107", "html_url": "https://github.com/tensorflow/tensorflow/pull/12107", "diff_url": "https://github.com/tensorflow/tensorflow/pull/12107.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/12107.patch"}, "body_html": "<p>I was following along your Bahdanau implementation while doing my own and noticed a bug in the documentation. While it's true you can project memory and query to <code>attention.num_inputs</code>, the context is of shape <code>[B, memory_size]</code>.</p>\n<p>To illustrate the lines around the ones I edited:</p>\n<pre><code># D_attention: num_units\n# D_encoded: memory size, encoder dimensionality, ...\n# T: time\n\nalignments # B x T x D_attention\nalignments_reduced # B x T\nmemory     # B x T x D_encoded\n\nalignments_reduced x memory # B x D_encoded\n</code></pre>\n<p>The <code>alignments_reduced</code> is computed in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L447\">attention_wrapper.py#L447</a>. <del>Additionally, I do not see mention of this step in the original paper but can see how it might be needed to make calculation of <code>context</code> possible.</del></p>\n<p><del>Maybe this is an incorrect interpretation of the original paper? It does not seem to mention a projection to a common dimensionality of both memory and query (<code>D_attention</code> above), denoted by <code>num_inputs</code> in the <code>BahdanauAttention</code> constructor. Say now we leave <code>D_attention = D_encoded</code>. The result would be that <code>alignments</code> and <code>memory</code> are of the same dimensionality, can safely be multiplied component-wise and then sum-reduced along the time dimension. This is an alternative interpretation of the paper, an approach that does not introduce <code>num_inputs</code> for attention and does not compute the sum along the last <code>num_inputs</code> axis (<code>alignments_reduced</code>).</del></p>", "body_text": "I was following along your Bahdanau implementation while doing my own and noticed a bug in the documentation. While it's true you can project memory and query to attention.num_inputs, the context is of shape [B, memory_size].\nTo illustrate the lines around the ones I edited:\n# D_attention: num_units\n# D_encoded: memory size, encoder dimensionality, ...\n# T: time\n\nalignments # B x T x D_attention\nalignments_reduced # B x T\nmemory     # B x T x D_encoded\n\nalignments_reduced x memory # B x D_encoded\n\nThe alignments_reduced is computed in attention_wrapper.py#L447. Additionally, I do not see mention of this step in the original paper but can see how it might be needed to make calculation of context possible.\nMaybe this is an incorrect interpretation of the original paper? It does not seem to mention a projection to a common dimensionality of both memory and query (D_attention above), denoted by num_inputs in the BahdanauAttention constructor. Say now we leave D_attention = D_encoded. The result would be that alignments and memory are of the same dimensionality, can safely be multiplied component-wise and then sum-reduced along the time dimension. This is an alternative interpretation of the paper, an approach that does not introduce num_inputs for attention and does not compute the sum along the last num_inputs axis (alignments_reduced).", "body": "I was following along your Bahdanau implementation while doing my own and noticed a bug in the documentation. While it's true you can project memory and query to `attention.num_inputs`, the context is of shape `[B, memory_size]`.\r\n\r\nTo illustrate the lines around the ones I edited:\r\n```\r\n# D_attention: num_units\r\n# D_encoded: memory size, encoder dimensionality, ...\r\n# T: time\r\n\r\nalignments # B x T x D_attention\r\nalignments_reduced # B x T\r\nmemory     # B x T x D_encoded\r\n\r\nalignments_reduced x memory # B x D_encoded\r\n```\r\n\r\nThe `alignments_reduced` is computed in [attention_wrapper.py#L447](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py#L447). ~~Additionally, I do not see mention of this step in the original paper but can see how it might be needed to make calculation of `context` possible.~~\r\n\r\n~~Maybe this is an incorrect interpretation of the original paper? It does not seem to mention a projection to a common dimensionality of both memory and query (`D_attention` above), denoted by `num_inputs` in the `BahdanauAttention` constructor. Say now we leave `D_attention = D_encoded`. The result would be that `alignments` and `memory` are of the same dimensionality, can safely be multiplied component-wise and then sum-reduced along the time dimension. This is an alternative interpretation of the paper, an approach that does not introduce `num_inputs` for attention and does not compute the sum along the last `num_inputs` axis (`alignments_reduced`).~~\r\n"}