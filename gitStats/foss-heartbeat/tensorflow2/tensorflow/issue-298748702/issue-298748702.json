{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17158", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17158/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17158/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17158/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17158", "id": 298748702, "node_id": "MDU6SXNzdWUyOTg3NDg3MDI=", "number": 17158, "title": "Feature Request: Better error reporting for tflite conversion", "user": {"login": "smoosh911", "id": 15962464, "node_id": "MDQ6VXNlcjE1OTYyNDY0", "avatar_url": "https://avatars2.githubusercontent.com/u/15962464?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smoosh911", "html_url": "https://github.com/smoosh911", "followers_url": "https://api.github.com/users/smoosh911/followers", "following_url": "https://api.github.com/users/smoosh911/following{/other_user}", "gists_url": "https://api.github.com/users/smoosh911/gists{/gist_id}", "starred_url": "https://api.github.com/users/smoosh911/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smoosh911/subscriptions", "organizations_url": "https://api.github.com/users/smoosh911/orgs", "repos_url": "https://api.github.com/users/smoosh911/repos", "events_url": "https://api.github.com/users/smoosh911/events{/privacy}", "received_events_url": "https://api.github.com/users/smoosh911/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aselle", "id": 326106, "node_id": "MDQ6VXNlcjMyNjEwNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/326106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aselle", "html_url": "https://github.com/aselle", "followers_url": "https://api.github.com/users/aselle/followers", "following_url": "https://api.github.com/users/aselle/following{/other_user}", "gists_url": "https://api.github.com/users/aselle/gists{/gist_id}", "starred_url": "https://api.github.com/users/aselle/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aselle/subscriptions", "organizations_url": "https://api.github.com/users/aselle/orgs", "repos_url": "https://api.github.com/users/aselle/repos", "events_url": "https://api.github.com/users/aselle/events{/privacy}", "received_events_url": "https://api.github.com/users/aselle/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-02-20T21:07:30Z", "updated_at": "2018-11-20T07:52:05Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:MacOS</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:source</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.5.0</li>\n<li><strong>Python version</strong>: 2.7.10</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: Not using</li>\n<li><strong>GPU model and memory</strong>: Radeon Pro 455</li>\n<li><strong>Exact command to reproduce</strong>: In macOS terminal: bazel-bin/tensorflow/contrib/lite/toco/toco <br>\n--input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb <br>\n--input_format=TENSORFLOW_GRAPHDEF <br>\n--output_format=TFLITE <br>\n--output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.lite <br>\n--inference_type=FLOAT <br>\n--input_arrays=input <br>\n--output_arrays=output <br>\n--input_shapes=1 <br>\n--output_shapes=1</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p>Overview<br>\nI have been trying to convert a word2vec custom model into tensorflow lite. I am using the code in the tensorflow documentation that was provided as a simple word2vec example.The conversion process up to the freeze_graph.py works fine and I can use that file to run inference no problem in a python script on my desktop. My problem is when I try to convert to lite.</p>\n<p>Model conversation seems to work<br>\nWhen I run the command I listed above I get a .tflite file. I can even visualize this file using bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec_model_viz.html. The model looks fine in the visualizer too.</p>\n<p>Use on mobile doesn't work<br>\nWhen I implement my tflite model into an iOS app and try to grab a node with</p>\n<pre><code>float* out = interpreter-&gt;typed_tensor&lt;float&gt;();\n</code></pre>\n<p>I sometimes get a valid pointer back but other times I get a null pointer depending on the node.</p>\n<p>After retraining my model with different shapes of inputs and outputs I finally noticed documentation at the end of the page <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md</a> that states some operations that are present but not ready for custom models. I am using a couple of these operations and I'm guessing this is the reason my lite model isn't working.</p>\n<p>My reqeust<br>\nPlease update the errors available for tflite conversion. I don't understand why it would have been difficult to throw an error on tflite conversion that specifies a node and states that tflite doesn't support its operation. Videos posted by google employees and the introduction to tflite make it sound like this product is ready to make life easier when converting for mobile which is what encouraged me to give it a shot. Now I see that this isn't the case yet. If nothing else I would ask for the sanity of your users that tensorflow documentation more clearly warns about the limitations on mobile.</p>\n<p>Tensorflow is a cool concept and the parts that are ready are awesome. Thanks for the great work!</p>\n<p>My graph code:</p>\n<pre><code>valid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=\"input\")\n\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\n\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True),name=\"norm\")\nnormalized_embeddings = tf.div(embeddings, norm, 'normalized_embeddings')\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset,name=\"valid_embeddings\")\nsimilarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings), transpose_b=False,name=\"similarity\")\noutput = tf.reshape(similarity, [-1], name=\"output\")\n</code></pre>\n<h3>Source code / logs</h3>\n<p>Include any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p>Output from tflite conversion:</p>\n<pre><code>(adventures-in-ml-code) miperry-macOS:tensorflow miperry$ bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite   --inference_type=FLOAT   --input_arrays=input   --output_arrays=output   --input_shapes=1   --output_shapes=1000\n2018-02-20 13:06:59.037245: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 21 arrays (0 quantized)\n2018-02-20 13:06:59.038319: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 21 arrays (0 quantized)\n2018-02-20 13:06:59.072631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 5 operators, 11 arrays (0 quantized)\n2018-02-20 13:06:59.072687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)\n2018-02-20 13:06:59.072731: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 2401216 bytes, theoretical optimal value: 240000\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):MacOS\nTensorFlow installed from (source or binary):source\nTensorFlow version (use command below):1.5.0\nPython version: 2.7.10\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: Not using\nGPU model and memory: Radeon Pro 455\nExact command to reproduce: In macOS terminal: bazel-bin/tensorflow/contrib/lite/toco/toco \n--input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb \n--input_format=TENSORFLOW_GRAPHDEF \n--output_format=TFLITE \n--output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.lite \n--inference_type=FLOAT \n--input_arrays=input \n--output_arrays=output \n--input_shapes=1 \n--output_shapes=1\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\nOverview\nI have been trying to convert a word2vec custom model into tensorflow lite. I am using the code in the tensorflow documentation that was provided as a simple word2vec example.The conversion process up to the freeze_graph.py works fine and I can use that file to run inference no problem in a python script on my desktop. My problem is when I try to convert to lite.\nModel conversation seems to work\nWhen I run the command I listed above I get a .tflite file. I can even visualize this file using bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec_model_viz.html. The model looks fine in the visualizer too.\nUse on mobile doesn't work\nWhen I implement my tflite model into an iOS app and try to grab a node with\nfloat* out = interpreter->typed_tensor<float>();\n\nI sometimes get a valid pointer back but other times I get a null pointer depending on the node.\nAfter retraining my model with different shapes of inputs and outputs I finally noticed documentation at the end of the page https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md that states some operations that are present but not ready for custom models. I am using a couple of these operations and I'm guessing this is the reason my lite model isn't working.\nMy reqeust\nPlease update the errors available for tflite conversion. I don't understand why it would have been difficult to throw an error on tflite conversion that specifies a node and states that tflite doesn't support its operation. Videos posted by google employees and the introduction to tflite make it sound like this product is ready to make life easier when converting for mobile which is what encouraged me to give it a shot. Now I see that this isn't the case yet. If nothing else I would ask for the sanity of your users that tensorflow documentation more clearly warns about the limitations on mobile.\nTensorflow is a cool concept and the parts that are ready are awesome. Thanks for the great work!\nMy graph code:\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=\"input\")\n\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\n\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True),name=\"norm\")\nnormalized_embeddings = tf.div(embeddings, norm, 'normalized_embeddings')\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset,name=\"valid_embeddings\")\nsimilarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings), transpose_b=False,name=\"similarity\")\noutput = tf.reshape(similarity, [-1], name=\"output\")\n\nSource code / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\nOutput from tflite conversion:\n(adventures-in-ml-code) miperry-macOS:tensorflow miperry$ bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite   --inference_type=FLOAT   --input_arrays=input   --output_arrays=output   --input_shapes=1   --output_shapes=1000\n2018-02-20 13:06:59.037245: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 21 arrays (0 quantized)\n2018-02-20 13:06:59.038319: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 21 arrays (0 quantized)\n2018-02-20 13:06:59.072631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 5 operators, 11 arrays (0 quantized)\n2018-02-20 13:06:59.072687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)\n2018-02-20 13:06:59.072731: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 2401216 bytes, theoretical optimal value: 240000", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:MacOS\r\n- **TensorFlow installed from (source or binary)**:source\r\n- **TensorFlow version (use command below)**:1.5.0\r\n- **Python version**: 2.7.10\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: Not using\r\n- **GPU model and memory**: Radeon Pro 455\r\n- **Exact command to reproduce**: In macOS terminal: bazel-bin/tensorflow/contrib/lite/toco/toco \\\r\n  --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb \\\r\n  --input_format=TENSORFLOW_GRAPHDEF \\\r\n  --output_format=TFLITE \\\r\n  --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.lite \\\r\n  --inference_type=FLOAT \\\r\n  --input_arrays=input \\\r\n  --output_arrays=output \\\r\n  --input_shapes=1 \\\r\n  --output_shapes=1\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n\r\nOverview\r\nI have been trying to convert a word2vec custom model into tensorflow lite. I am using the code in the tensorflow documentation that was provided as a simple word2vec example.The conversion process up to the freeze_graph.py works fine and I can use that file to run inference no problem in a python script on my desktop. My problem is when I try to convert to lite. \r\n\r\nModel conversation seems to work\r\nWhen I run the command I listed above I get a .tflite file. I can even visualize this file using bazel-bin/tensorflow/contrib/lite/tools/visualize /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite /Users/miperry/Documents/adventures-in-ml-code/tensorflow_word2vec/word2vec_model_viz.html. The model looks fine in the visualizer too.\r\n\r\nUse on mobile doesn't work\r\nWhen I implement my tflite model into an iOS app and try to grab a node with \r\n````\r\nfloat* out = interpreter->typed_tensor<float>();\r\n````\r\nI sometimes get a valid pointer back but other times I get a null pointer depending on the node. \r\n\r\nAfter retraining my model with different shapes of inputs and outputs I finally noticed documentation at the end of the page https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/tf_ops_compatibility.md that states some operations that are present but not ready for custom models. I am using a couple of these operations and I'm guessing this is the reason my lite model isn't working. \r\n\r\nMy reqeust\r\nPlease update the errors available for tflite conversion. I don't understand why it would have been difficult to throw an error on tflite conversion that specifies a node and states that tflite doesn't support its operation. Videos posted by google employees and the introduction to tflite make it sound like this product is ready to make life easier when converting for mobile which is what encouraged me to give it a shot. Now I see that this isn't the case yet. If nothing else I would ask for the sanity of your users that tensorflow documentation more clearly warns about the limitations on mobile. \r\n\r\nTensorflow is a cool concept and the parts that are ready are awesome. Thanks for the great work!\r\n\r\nMy graph code:\r\n````\r\nvalid_dataset = tf.constant(valid_examples, dtype=tf.int32,name=\"input\")\r\n\r\nembeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0),name=\"embeddings\")\r\n\r\nnorm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True),name=\"norm\")\r\nnormalized_embeddings = tf.div(embeddings, norm, 'normalized_embeddings')\r\nvalid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset,name=\"valid_embeddings\")\r\nsimilarity = tf.matmul(valid_embeddings, tf.transpose(normalized_embeddings), transpose_b=False,name=\"similarity\")\r\noutput = tf.reshape(similarity, [-1], name=\"output\")\r\n````\r\n\r\n### Source code / logs\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached. Try to provide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\nOutput from tflite conversion:\r\n````\r\n(adventures-in-ml-code) miperry-macOS:tensorflow miperry$ bazel-bin/tensorflow/contrib/lite/toco/toco   --input_file=../adventures-in-ml-code/tensorflow_word2vec/frozen_graph.pb   --input_format=TENSORFLOW_GRAPHDEF   --output_format=TFLITE   --output_file=../adventures-in-ml-code/tensorflow_word2vec/word2vec.tflite   --inference_type=FLOAT   --input_arrays=input   --output_arrays=output   --input_shapes=1   --output_shapes=1000\r\n2018-02-20 13:06:59.037245: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before Removing unused ops: 14 operators, 21 arrays (0 quantized)\r\n2018-02-20 13:06:59.038319: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before general graph transformations: 14 operators, 21 arrays (0 quantized)\r\n2018-02-20 13:06:59.072631: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] After general graph transformations pass 1: 5 operators, 11 arrays (0 quantized)\r\n2018-02-20 13:06:59.072687: I tensorflow/contrib/lite/toco/graph_transformations/graph_transformations.cc:39] Before dequantization graph transformations: 5 operators, 11 arrays (0 quantized)\r\n2018-02-20 13:06:59.072731: I tensorflow/contrib/lite/toco/allocate_transient_arrays.cc:311] Total transient array allocated size: 2401216 bytes, theoretical optimal value: 240000\r\n````"}