{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22009", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22009/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22009/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22009/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22009", "id": 356239873, "node_id": "MDU6SXNzdWUzNTYyMzk4NzM=", "number": 22009, "title": "tf.get_variable returns Tensor instead of Variable", "user": {"login": "mpekalski", "id": 2975068, "node_id": "MDQ6VXNlcjI5NzUwNjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2975068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpekalski", "html_url": "https://github.com/mpekalski", "followers_url": "https://api.github.com/users/mpekalski/followers", "following_url": "https://api.github.com/users/mpekalski/following{/other_user}", "gists_url": "https://api.github.com/users/mpekalski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpekalski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpekalski/subscriptions", "organizations_url": "https://api.github.com/users/mpekalski/orgs", "repos_url": "https://api.github.com/users/mpekalski/repos", "events_url": "https://api.github.com/users/mpekalski/events{/privacy}", "received_events_url": "https://api.github.com/users/mpekalski/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-09-01T23:28:51Z", "updated_at": "2018-09-02T19:42:33Z", "closed_at": "2018-09-02T19:42:33Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:</li>\n</ul>\n<p>TF checkpoint I have built</p>\n<pre><code>/tmp/tensorflow# git log   \ncommit 09792df012c22622324f085f46edde33006c7355\nAuthor: A. Unique TensorFlower &lt;gardener@tensorflow.org&gt;\nDate:   Sun Aug 26 02:07:11 2018 -0700\n\n    compat: Update forward compatibility horizon to 2018-08-26\n    \n    PiperOrigin-RevId: 210266798\n</code></pre>\n<pre><code>== cat /etc/issue ===============================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nYes\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.5)\nprotobuf (3.6.1)\ntensorflow (1.10.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.10.0\ntf.GIT_VERSION = b'unknown'\ntf.COMPILER_VERSION = b'unknown'\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nWed Aug 29 19:57:14 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\n\n</code></pre>\n<p><strong>Bazel</strong> version</p>\n<pre><code>$ bazel version\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\nBuild label: 0.16.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\nBuild timestamp: 1533056484\nBuild timestamp as int: 1533056484\n</code></pre>\n<p><strong>CUDNN</strong> version:</p>\n<pre><code>$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2018 NVIDIA Corporation\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\nCuda compilation tools, release 9.2, V9.2.148\n</code></pre>\n<p><strong>GPU</strong>: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS</p>\n<ul>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                               , trainable=False, use_resource=True)             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, 1, name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        #with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                                   , trainable=False, use_resource=True)         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            \n            assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\n            with tf.control_dependencies([assign_two]):\n                return y, label\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [0,0,0,0,0]))\n                    .map(scope_2)\n                    .batch(2)\n                    .repeat(10)        \n                    )\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1\n\nwith tf.Session() as sess:\n    dataset_fn()\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    print(sess.graph.get_operations())    \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n\n</code></pre>\n<h3>Describe the problem</h3>\n<p>I am trying to create a function that would modify a Tensor within a pipeline of Dataset API.<br>\nThe scoping may seem weird, but that is minimal example that shows the problem that I created from my project.but somehow it is using <code>ReadVariableOp</code> that only</p>\n<p>Looks like when I fetch a variable using <code>tf.get_variable</code> (within def scope_2) it returns immutable Tensor, whereas the original Variable is mutable. In <code>scope_1</code> I am able to run <code>x.assign(0)</code> but then within <code>scope_2</code> (that is invoked in dataset.map) it throws an <code>AttributeError: 'Tensor' object has no attribute 'assign'</code></p>\n<p>According to the docs <a href=\"https://www.tensorflow.org/api_docs/python/tf/get_variable\" rel=\"nofollow\">https://www.tensorflow.org/api_docs/python/tf/get_variable</a> <code>tf.get_variable</code> should return a Variable not a Tensor.</p>\n<pre><code>Returns:\nThe created or existing Variable (or PartitionedVariable, if a partitioner was used).\n</code></pre>\n<h3>Source code / logs</h3>\n<pre><code>2018-09-01 23:26:58.452631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-01 23:26:58.453216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 9.98GiB\n2018-09-01 23:26:58.453237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-09-01 23:26:58.653750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-01 23:26:58.653789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-09-01 23:26:58.653796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-09-01 23:26:58.654003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9644 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-09-01 23:26:58.758539: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nDS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400&gt;\nscope: scope_1\n name: scope_1/x:0\n  var: &lt;tf.Variable 'scope_1/x:0' shape=() dtype=float32&gt;\ninitial scope: \nDS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400&gt;\nscope: scope_1\n name: ReadVariableOp:0\n  var: Tensor(\"ReadVariableOp:0\", shape=(), dtype=float32)\n=============\nTraceback (most recent call last):\n  File \"bug.py\", line 50, in &lt;module&gt;\n    dataset_fn()\n  File \"bug.py\", line 40, in scope_1\n    .map(scope_2)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\n    return MapDataset(self, map_func)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\n    map_func, \"Dataset.map()\", input_dataset)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\n    self._function.add_to_graph(ops.get_default_graph())\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\n    self._create_definition_if_needed()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\n    self._create_definition_if_needed_impl()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\n    self._capture_by_value, self._caller_device)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\n    outputs = func(*func_graph.inputs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\n    ret = func(*nested_args)\n  File \"bug.py\", line 32, in scope_2\n    assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 222, in assign\n    return ref.assign(value, name=name)\nAttributeError: 'Tensor' object has no attribute 'assign'\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n\nTF checkpoint I have built\n/tmp/tensorflow# git log   \ncommit 09792df012c22622324f085f46edde33006c7355\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\nDate:   Sun Aug 26 02:07:11 2018 -0700\n\n    compat: Update forward compatibility horizon to 2018-08-26\n    \n    PiperOrigin-RevId: 210266798\n\n== cat /etc/issue ===============================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\nVERSION_ID=\"16.04\"\nVERSION_CODENAME=xenial\n\n== are we in docker =============================================\nYes\n\n== compiler =====================================================\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n\n== uname -a =====================================================\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\n\n== check pips ===================================================\nnumpy (1.14.5)\nprotobuf (3.6.1)\ntensorflow (1.10.0)\n\n== check for virtualenv =========================================\nFalse\n\n== tensorflow import ============================================\ntf.VERSION = 1.10.0\ntf.GIT_VERSION = b'unknown'\ntf.COMPILER_VERSION = b'unknown'\nSanity check: array([1], dtype=int32)\n\n== env ==========================================================\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\nDYLD_LIBRARY_PATH is unset\n\n== nvidia-smi ===================================================\nWed Aug 29 19:57:14 2018       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n== cuda libs  ===================================================\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\n\n\nBazel version\n$ bazel version\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\nBuild label: 0.16.0\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\nBuild timestamp: 1533056484\nBuild timestamp as int: 1533056484\n\nCUDNN version:\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2018 NVIDIA Corporation\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\nCuda compilation tools, release 9.2, V9.2.148\n\nGPU: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\n\nExact command to reproduce:\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                               , trainable=False, use_resource=True)             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, 1, name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        #with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                                   , trainable=False, use_resource=True)         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            \n            assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\n            with tf.control_dependencies([assign_two]):\n                return y, label\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [0,0,0,0,0]))\n                    .map(scope_2)\n                    .batch(2)\n                    .repeat(10)        \n                    )\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1\n\nwith tf.Session() as sess:\n    dataset_fn()\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    print(sess.graph.get_operations())    \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n\n\nDescribe the problem\nI am trying to create a function that would modify a Tensor within a pipeline of Dataset API.\nThe scoping may seem weird, but that is minimal example that shows the problem that I created from my project.but somehow it is using ReadVariableOp that only\nLooks like when I fetch a variable using tf.get_variable (within def scope_2) it returns immutable Tensor, whereas the original Variable is mutable. In scope_1 I am able to run x.assign(0) but then within scope_2 (that is invoked in dataset.map) it throws an AttributeError: 'Tensor' object has no attribute 'assign'\nAccording to the docs https://www.tensorflow.org/api_docs/python/tf/get_variable tf.get_variable should return a Variable not a Tensor.\nReturns:\nThe created or existing Variable (or PartitionedVariable, if a partitioner was used).\n\nSource code / logs\n2018-09-01 23:26:58.452631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-01 23:26:58.453216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 9.98GiB\n2018-09-01 23:26:58.453237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-09-01 23:26:58.653750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-01 23:26:58.653789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-09-01 23:26:58.653796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-09-01 23:26:58.654003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9644 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-09-01 23:26:58.758539: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>\nscope: scope_1\n name: scope_1/x:0\n  var: <tf.Variable 'scope_1/x:0' shape=() dtype=float32>\ninitial scope: \nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>\nscope: scope_1\n name: ReadVariableOp:0\n  var: Tensor(\"ReadVariableOp:0\", shape=(), dtype=float32)\n=============\nTraceback (most recent call last):\n  File \"bug.py\", line 50, in <module>\n    dataset_fn()\n  File \"bug.py\", line 40, in scope_1\n    .map(scope_2)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\n    return MapDataset(self, map_func)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\n    map_func, \"Dataset.map()\", input_dataset)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\n    self._function.add_to_graph(ops.get_default_graph())\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\n    self._create_definition_if_needed()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\n    self._create_definition_if_needed_impl()\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\n    self._capture_by_value, self._caller_device)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\n    outputs = func(*func_graph.inputs)\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\n    ret = func(*nested_args)\n  File \"bug.py\", line 32, in scope_2\n    assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 222, in assign\n    return ref.assign(value, name=name)\nAttributeError: 'Tensor' object has no attribute 'assign'", "body": "-----------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\n- **TensorFlow installed from (source or binary)**:\r\nsource\r\n- **TensorFlow version (use command below)**:\r\n\r\nTF checkpoint I have built\r\n```\r\n/tmp/tensorflow# git log   \r\ncommit 09792df012c22622324f085f46edde33006c7355\r\nAuthor: A. Unique TensorFlower <gardener@tensorflow.org>\r\nDate:   Sun Aug 26 02:07:11 2018 -0700\r\n\r\n    compat: Update forward compatibility horizon to 2018-08-26\r\n    \r\n    PiperOrigin-RevId: 210266798\r\n```\r\n\r\n```\r\n== cat /etc/issue ===============================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"16.04.5 LTS (Xenial Xerus)\"\r\nVERSION_ID=\"16.04\"\r\nVERSION_CODENAME=xenial\r\n\r\n== are we in docker =============================================\r\nYes\r\n\r\n== compiler =====================================================\r\nc++ (Ubuntu 5.4.0-6ubuntu1~16.04.10) 5.4.0 20160609\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux 3bed2f328777 4.13.0-38-generic #43~16.04.1-Ubuntu SMP Wed Mar 14 17:48:43 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.14.5)\r\nprotobuf (3.6.1)\r\ntensorflow (1.10.0)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.10.0\r\ntf.GIT_VERSION = b'unknown'\r\ntf.COMPILER_VERSION = b'unknown'\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH /usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/lib64/stubs:/usr/local/cuda/lib64/stubs:/usr/local/cuda/extras/CUPTI/lib64:/lib/amd64/server/:/usr/lib/jvm/java-8-openjdk-amd64/jre/lib/amd64/server:/opt/boost/lib:/opt/conda/lib/:/usr/local/cuda/lib64/:/opt/conda/lib/R/lib/:/usr/local/nvidia/lib64/:/usr/local/nvidia/lib:/lib/x86_64-linux-gnu:/usr/local/cuda/extras/CUPTI/lib64:/usr/lib/x86_64-linux-gnu:/opt/opencv/lib\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\nWed Aug 29 19:57:14 2018       \r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 396.26                 Driver Version: 396.26                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  GeForce GTX 108...  Off  | 00000000:01:00.0  On |                  N/A |\r\n|  0%   41C    P0    76W / 250W |    708MiB / 11177MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n                                                                               \r\n+-----------------------------------------------------------------------------+\r\n| Processes:                                                       GPU Memory |\r\n|  GPU       PID   Type   Process name                             Usage      |\r\n|=============================================================================|\r\n+-----------------------------------------------------------------------------+\r\n\r\n== cuda libs  ===================================================\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart.so.9.2.148\r\n/usr/local/cuda-9.2/targets/x86_64-linux/lib/libcudart_static.a\r\n\r\n```\r\n**Bazel** version\r\n```\r\n$ bazel version\r\nWARNING: --batch mode is deprecated. Please instead explicitly shut down your Bazel server using the command \"bazel shutdown\".\r\nBuild label: 0.16.0\r\nBuild target: bazel-out/k8-opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Tue Jul 31 17:01:24 2018 (1533056484)\r\nBuild timestamp: 1533056484\r\nBuild timestamp as int: 1533056484\r\n```\r\n\r\n**CUDNN** version:\r\n```\r\n$ nvcc --version\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2018 NVIDIA Corporation\r\nBuilt on Tue_Jun_12_23:07:04_CDT_2018\r\nCuda compilation tools, release 9.2, V9.2.148\r\n```\r\n\r\n**GPU**: GEFORCE GTX 1080Ti, 11GB, GIGABYTE AORUS\r\n\r\n- **Exact command to reproduce**:\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        #with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                                   , trainable=False, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            \r\n            assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\r\n            with tf.control_dependencies([assign_two]):\r\n                return y, label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [0,0,0,0,0]))\r\n                    .map(scope_2)\r\n                    .batch(2)\r\n                    .repeat(10)        \r\n                    )\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1\r\n\r\nwith tf.Session() as sess:\r\n    dataset_fn()\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    print(sess.graph.get_operations())    \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n\r\n```\r\n\r\n### Describe the problem\r\nI am trying to create a function that would modify a Tensor within a pipeline of Dataset API.\r\nThe scoping may seem weird, but that is minimal example that shows the problem that I created from my project.but somehow it is using `ReadVariableOp` that only \r\n\r\nLooks like when I fetch a variable using `tf.get_variable` (within def scope_2) it returns immutable Tensor, whereas the original Variable is mutable. In `scope_1` I am able to run `x.assign(0)` but then within `scope_2` (that is invoked in dataset.map) it throws an `AttributeError: 'Tensor' object has no attribute 'assign'`\r\n\r\nAccording to the docs https://www.tensorflow.org/api_docs/python/tf/get_variable `tf.get_variable` should return a Variable not a Tensor.\r\n```\r\nReturns:\r\nThe created or existing Variable (or PartitionedVariable, if a partitioner was used).\r\n```\r\n\r\n### Source code / logs\r\n\r\n```\r\n2018-09-01 23:26:58.452631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-01 23:26:58.453216: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 9.98GiB\r\n2018-09-01 23:26:58.453237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-01 23:26:58.653750: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-01 23:26:58.653789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-01 23:26:58.653796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-01 23:26:58.654003: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9644 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-01 23:26:58.758539: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>\r\nscope: scope_1\r\n name: scope_1/x:0\r\n  var: <tf.Variable 'scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f1ebf351400>\r\nscope: scope_1\r\n name: ReadVariableOp:0\r\n  var: Tensor(\"ReadVariableOp:0\", shape=(), dtype=float32)\r\n=============\r\nTraceback (most recent call last):\r\n  File \"bug.py\", line 50, in <module>\r\n    dataset_fn()\r\n  File \"bug.py\", line 40, in scope_1\r\n    .map(scope_2)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1005, in map\r\n    return MapDataset(self, map_func)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 2216, in __init__\r\n    map_func, \"Dataset.map()\", input_dataset)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1473, in __init__\r\n    self._function.add_to_graph(ops.get_default_graph())\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 479, in add_to_graph\r\n    self._create_definition_if_needed()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 335, in _create_definition_if_needed\r\n    self._create_definition_if_needed_impl()\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 344, in _create_definition_if_needed_impl\r\n    self._capture_by_value, self._caller_device)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/function.py\", line 865, in func_graph_from_py_func\r\n    outputs = func(*func_graph.inputs)\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py\", line 1411, in tf_data_structured_function_wrapper\r\n    ret = func(*nested_args)\r\n  File \"bug.py\", line 32, in scope_2\r\n    assign_two = tf.assign(y, inputs+1, name=\"inputs_plus_1\")\r\n  File \"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/state_ops.py\", line 222, in assign\r\n    return ref.assign(value, name=name)\r\nAttributeError: 'Tensor' object has no attribute 'assign'\r\n\r\n```"}