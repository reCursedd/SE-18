{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417954968", "html_url": "https://github.com/tensorflow/tensorflow/issues/22009#issuecomment-417954968", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22009", "id": 417954968, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzk1NDk2OA==", "user": {"login": "mpekalski", "id": 2975068, "node_id": "MDQ6VXNlcjI5NzUwNjg=", "avatar_url": "https://avatars1.githubusercontent.com/u/2975068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mpekalski", "html_url": "https://github.com/mpekalski", "followers_url": "https://api.github.com/users/mpekalski/followers", "following_url": "https://api.github.com/users/mpekalski/following{/other_user}", "gists_url": "https://api.github.com/users/mpekalski/gists{/gist_id}", "starred_url": "https://api.github.com/users/mpekalski/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mpekalski/subscriptions", "organizations_url": "https://api.github.com/users/mpekalski/orgs", "repos_url": "https://api.github.com/users/mpekalski/repos", "events_url": "https://api.github.com/users/mpekalski/events{/privacy}", "received_events_url": "https://api.github.com/users/mpekalski/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-02T19:42:33Z", "updated_at": "2018-09-02T19:42:33Z", "author_association": "NONE", "body_html": "<p>I got it working with the piece of code I have commented out in the original issue. I do not know why it did not work for me before. Either way, the solution was to use the same scope again but with reference to it by variable not name, so use <code>current_scope</code> not <code>\"scope_1\"</code>.</p>\n<p>I find it strange that although I use <code>tf.get_variable</code> a new variable is created with the same name as original but followed by <code>_1</code>.</p>\n<pre><code>from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                               , trainable=False, use_resource=True)             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                                   , trainable=False, use_resource=True)         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            \n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))\n            with tf.control_dependencies([assign_two]):\n                return y.read_value(), label\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                    .map(scope_2)\n                    .batch(1)\n                    .repeat(1)        \n                    )\n    return dataset\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1()\n\nwith tf.variable_scope(\"iterator\"):\n    # Define iterator from_string_handle. In general it is useful to have\n    # this kind of iterator if one wants to switch between train and validation\n    # within the training loop.        \n    iterator_t = dataset_fn.make_initializable_iterator()\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                iterator_t.output_types,\n                                                iterator_t.output_shapes)\n    \n    def get_next_item():\n        next_elem = iterator.get_next(name=\"next_element\")\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n        return x, y    \nwith tf.Session() as sess:\n    handle_t = sess.run(iterator_t.string_handle())\n    # Run data iterator initialisation\n    sess.run(iterator_t.initializer)\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    print(sess.graph.get_operations()) \n    while True:\n        try:\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n        except tf.errors.OutOfRangeError:\n                        print(\"End of training dataset.\")\n                        break        \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n</code></pre>\n<p>LOG</p>\n<pre><code>DS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;\ninitial scope: \nDS1 SCOPE =============\ngraph: &lt;tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470&gt;\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;\n=============\n2018-09-02 19:37:43.456845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-02 19:37:43.457271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 10.29GiB\n2018-09-02 19:37:43.457291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-09-02 19:37:43.662214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-02 19:37:43.662252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-09-02 19:37:43.662260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-09-02 19:37:43.662470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9951 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-09-02 19:37:43.759025: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n[&lt;tf.Operation 'Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Initializer/initial_value' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/Const' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0/tensors/component_0' type=Const&gt;, &lt;tf.Operation 'scope_0/tensors/component_1' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Initializer/initial_value' type=Const&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp&gt;, &lt;tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp&gt;, &lt;tf.Operation 'scope_0/batch_size' type=Const&gt;, &lt;tf.Operation 'scope_0/drop_remainder' type=Const&gt;, &lt;tf.Operation 'scope_0/count' type=Const&gt;, &lt;tf.Operation 'iterator/IteratorV2' type=IteratorV2&gt;, &lt;tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset&gt;, &lt;tf.Operation 'iterator/MapDataset' type=MapDataset&gt;, &lt;tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2&gt;, &lt;tf.Operation 'iterator/RepeatDataset' type=RepeatDataset&gt;, &lt;tf.Operation 'iterator/MakeIterator' type=MakeIterator&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'iterator/iterator_handle' type=Placeholder&gt;, &lt;tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2&gt;, &lt;tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle&gt;, &lt;tf.Operation 'init' type=NoOp&gt;, &lt;tf.Operation 'init_1' type=NoOp&gt;]\n(array([2.], dtype=float32), array([-1], dtype=int32))\n(array([3.], dtype=float32), array([-2], dtype=int32))\n(array([4.], dtype=float32), array([-3], dtype=int32))\n(array([5.], dtype=float32), array([-4], dtype=int32))\n(array([6.], dtype=float32), array([-5], dtype=int32))\nEnd of training dataset.\n\nglobal vars: [&lt;tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32&gt;, &lt;tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32&gt;]\nlocal vars: []\n</code></pre>", "body_text": "I got it working with the piece of code I have commented out in the original issue. I do not know why it did not work for me before. Either way, the solution was to use the same scope again but with reference to it by variable not name, so use current_scope not \"scope_1\".\nI find it strange that although I use tf.get_variable a new variable is created with the same name as original but followed by _1.\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow as tf\n\ndef scope_1():\n    print(\"DS1 SCOPE =============\")\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                               , trainable=False, use_resource=True)             \n        print(\"graph: {}\".format(x.graph))\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\n        print(\" name: {}\".format(x.name))\n        print(\"  var: {}\".format(str(x)))\n        current_scope = tf.get_variable_scope()       \n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\n    \n    def scope_2(inputs, label):        \n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\n        print(\"DS1 SCOPE =============\")\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\n                                   , trainable=False, use_resource=True)         \n            print(\"graph: {}\".format(y.graph))\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\n            print(\" name: {}\".format(y.name))\n            print(\"  var: {}\".format(str(y)))\n            print(\"=============\")\n            \n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))\n            with tf.control_dependencies([assign_two]):\n                return y.read_value(), label\n            #return x,label\n    \n    # test that original x is mutable\n    with tf.control_dependencies([assign_one]):\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\n                    .map(scope_2)\n                    .batch(1)\n                    .repeat(1)        \n                    )\n    return dataset\n    \n                \nwith tf.variable_scope(\"scope_0\"):\n        dataset_fn = scope_1()\n\nwith tf.variable_scope(\"iterator\"):\n    # Define iterator from_string_handle. In general it is useful to have\n    # this kind of iterator if one wants to switch between train and validation\n    # within the training loop.        \n    iterator_t = dataset_fn.make_initializable_iterator()\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \n                                                iterator_t.output_types,\n                                                iterator_t.output_shapes)\n    \n    def get_next_item():\n        next_elem = iterator.get_next(name=\"next_element\")\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\n        return x, y    \nwith tf.Session() as sess:\n    handle_t = sess.run(iterator_t.string_handle())\n    # Run data iterator initialisation\n    sess.run(iterator_t.initializer)\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\n    print(sess.graph.get_operations()) \n    while True:\n        try:\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\n        except tf.errors.OutOfRangeError:\n                        print(\"End of training dataset.\")\n                        break        \n    print()\n    print(\"global vars: {}\".format(tf.global_variables()))\n    print(\"local vars: {}\".format(tf.local_variables()))\n    print(tf.get_default_graph().get_name_scope())\n\nLOG\nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x:0\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\ninitial scope: \nDS1 SCOPE =============\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470>\nscope: scope_0/scope_1\n name: scope_0/scope_1/x_1:0\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\n=============\n2018-09-02 19:37:43.456845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-09-02 19:37:43.457271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\npciBusID: 0000:01:00.0\ntotalMemory: 10.92GiB freeMemory: 10.29GiB\n2018-09-02 19:37:43.457291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\n2018-09-02 19:37:43.662214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-09-02 19:37:43.662252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \n2018-09-02 19:37:43.662260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \n2018-09-02 19:37:43.662470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9951 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-09-02 19:37:43.759025: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n[<tf.Operation 'Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/initial_value' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/initial_value' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\n(array([2.], dtype=float32), array([-1], dtype=int32))\n(array([3.], dtype=float32), array([-2], dtype=int32))\n(array([4.], dtype=float32), array([-3], dtype=int32))\n(array([5.], dtype=float32), array([-4], dtype=int32))\n(array([6.], dtype=float32), array([-5], dtype=int32))\nEnd of training dataset.\n\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]\nlocal vars: []", "body": "I got it working with the piece of code I have commented out in the original issue. I do not know why it did not work for me before. Either way, the solution was to use the same scope again but with reference to it by variable not name, so use `current_scope` not `\"scope_1\"`.\r\n\r\nI find it strange that although I use `tf.get_variable` a new variable is created with the same name as original but followed by `_1`. \r\n\r\n```\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport tensorflow as tf\r\n\r\ndef scope_1():\r\n    print(\"DS1 SCOPE =============\")\r\n    with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        x = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                               , trainable=False, use_resource=True)             \r\n        print(\"graph: {}\".format(x.graph))\r\n        print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\" name: {}\".format(x.name))\r\n        print(\"  var: {}\".format(str(x)))\r\n        current_scope = tf.get_variable_scope()       \r\n        assign_one = tf.assign(x, 1.0, name=\"x_is_one\")\r\n    \r\n    def scope_2(inputs, label):        \r\n        print(\"initial scope: {}\".format(tf.get_variable_scope().name))\r\n        print(\"DS1 SCOPE =============\")\r\n        #with tf.variable_scope(\"scope_1\", reuse=tf.AUTO_REUSE):\r\n        with tf.variable_scope(current_scope, reuse=tf.AUTO_REUSE):\r\n            y = tf.get_variable(\"x\", initializer=0.0, dtype=tf.float32\r\n                                   , trainable=False, use_resource=True)         \r\n            print(\"graph: {}\".format(y.graph))\r\n            print(\"scope: {}\".format(tf.get_variable_scope().name))\r\n            print(\" name: {}\".format(y.name))\r\n            print(\"  var: {}\".format(str(y)))\r\n            print(\"=============\")\r\n            \r\n            #assign_two = tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32),1.0), name=\"inputs_plus_1\")\r\n            assign_two = tf.identity(tf.assign(y, tf.add(tf.cast(inputs, dtype=tf.float32), 1.0)))\r\n            with tf.control_dependencies([assign_two]):\r\n                return y.read_value(), label\r\n            #return x,label\r\n    \r\n    # test that original x is mutable\r\n    with tf.control_dependencies([assign_one]):\r\n        dataset = (tf.data.Dataset.from_tensor_slices(([1,2,3,4,5], [-1,-2,-3,-4,-5]))\r\n                    .map(scope_2)\r\n                    .batch(1)\r\n                    .repeat(1)        \r\n                    )\r\n    return dataset\r\n    \r\n                \r\nwith tf.variable_scope(\"scope_0\"):\r\n        dataset_fn = scope_1()\r\n\r\nwith tf.variable_scope(\"iterator\"):\r\n    # Define iterator from_string_handle. In general it is useful to have\r\n    # this kind of iterator if one wants to switch between train and validation\r\n    # within the training loop.        \r\n    iterator_t = dataset_fn.make_initializable_iterator()\r\n    iterator_handle = tf.placeholder(tf.string, shape=[], name=\"iterator_handle\")\r\n    iterator = tf.data.Iterator.from_string_handle(iterator_handle, \r\n                                                iterator_t.output_types,\r\n                                                iterator_t.output_shapes)\r\n    \r\n    def get_next_item():\r\n        next_elem = iterator.get_next(name=\"next_element\")\r\n        x, y = tf.cast(next_elem[0], tf.float32), next_elem[1]# tf.cast(next_elem[1], tf.int32)\r\n        return x, y    \r\nwith tf.Session() as sess:\r\n    handle_t = sess.run(iterator_t.string_handle())\r\n    # Run data iterator initialisation\r\n    sess.run(iterator_t.initializer)\r\n    sess.run([tf.global_variables_initializer(),tf.local_variables_initializer()])\r\n    print(sess.graph.get_operations()) \r\n    while True:\r\n        try:\r\n            print(sess.run(get_next_item(), feed_dict={iterator_handle:handle_t}))\r\n        except tf.errors.OutOfRangeError:\r\n                        print(\"End of training dataset.\")\r\n                        break        \r\n    print()\r\n    print(\"global vars: {}\".format(tf.global_variables()))\r\n    print(\"local vars: {}\".format(tf.local_variables()))\r\n    print(tf.get_default_graph().get_name_scope())\r\n```\r\nLOG\r\n```\r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x:0\r\n  var: <tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>\r\ninitial scope: \r\nDS1 SCOPE =============\r\ngraph: <tensorflow.python.framework.ops.Graph object at 0x7f56a3a98470>\r\nscope: scope_0/scope_1\r\n name: scope_0/scope_1/x_1:0\r\n  var: <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>\r\n=============\r\n2018-09-02 19:37:43.456845: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-09-02 19:37:43.457271: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Found device 0 with properties: \r\nname: GeForce GTX 1080 Ti major: 6 minor: 1 memoryClockRate(GHz): 1.721\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 10.92GiB freeMemory: 10.29GiB\r\n2018-09-02 19:37:43.457291: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1485] Adding visible gpu devices: 0\r\n2018-09-02 19:37:43.662214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:966] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-09-02 19:37:43.662252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:972]      0 \r\n2018-09-02 19:37:43.662260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:985] 0:   N \r\n2018-09-02 19:37:43.662470: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1098] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9951 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-09-02 19:37:43.759025: I tensorflow/core/common_runtime/process_util.cc:69] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\r\n[<tf.Operation 'Const' type=Const>, <tf.Operation 'scope_0/scope_1/x/Initializer/initial_value' type=Const>, <tf.Operation 'scope_0/scope_1/x' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/scope_1/Const' type=Const>, <tf.Operation 'scope_0/scope_1/x_is_one' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/tensors/component_0' type=Const>, <tf.Operation 'scope_0/tensors/component_1' type=Const>, <tf.Operation 'scope_0/scope_1/x_1/Initializer/initial_value' type=Const>, <tf.Operation 'scope_0/scope_1/x_1' type=VarHandleOp>, <tf.Operation 'scope_0/scope_1/x_1/IsInitialized/VarIsInitializedOp' type=VarIsInitializedOp>, <tf.Operation 'scope_0/scope_1/x_1/Assign' type=AssignVariableOp>, <tf.Operation 'scope_0/scope_1/x_1/Read/ReadVariableOp' type=ReadVariableOp>, <tf.Operation 'scope_0/batch_size' type=Const>, <tf.Operation 'scope_0/drop_remainder' type=Const>, <tf.Operation 'scope_0/count' type=Const>, <tf.Operation 'iterator/IteratorV2' type=IteratorV2>, <tf.Operation 'iterator/TensorSliceDataset' type=TensorSliceDataset>, <tf.Operation 'iterator/MapDataset' type=MapDataset>, <tf.Operation 'iterator/BatchDatasetV2' type=BatchDatasetV2>, <tf.Operation 'iterator/RepeatDataset' type=RepeatDataset>, <tf.Operation 'iterator/MakeIterator' type=MakeIterator>, <tf.Operation 'iterator/IteratorToStringHandle' type=IteratorToStringHandle>, <tf.Operation 'iterator/iterator_handle' type=Placeholder>, <tf.Operation 'iterator/IteratorFromStringHandleV2' type=IteratorFromStringHandleV2>, <tf.Operation 'iterator/IteratorToStringHandle_1' type=IteratorToStringHandle>, <tf.Operation 'init' type=NoOp>, <tf.Operation 'init_1' type=NoOp>]\r\n(array([2.], dtype=float32), array([-1], dtype=int32))\r\n(array([3.], dtype=float32), array([-2], dtype=int32))\r\n(array([4.], dtype=float32), array([-3], dtype=int32))\r\n(array([5.], dtype=float32), array([-4], dtype=int32))\r\n(array([6.], dtype=float32), array([-5], dtype=int32))\r\nEnd of training dataset.\r\n\r\nglobal vars: [<tf.Variable 'scope_0/scope_1/x:0' shape=() dtype=float32>, <tf.Variable 'scope_0/scope_1/x_1:0' shape=() dtype=float32>]\r\nlocal vars: []\r\n```"}