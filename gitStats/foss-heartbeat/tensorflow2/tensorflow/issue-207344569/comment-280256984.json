{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/280256984", "html_url": "https://github.com/tensorflow/tensorflow/issues/7476#issuecomment-280256984", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7476", "id": 280256984, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MDI1Njk4NA==", "user": {"login": "shiyemin", "id": 1501158, "node_id": "MDQ6VXNlcjE1MDExNTg=", "avatar_url": "https://avatars3.githubusercontent.com/u/1501158?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shiyemin", "html_url": "https://github.com/shiyemin", "followers_url": "https://api.github.com/users/shiyemin/followers", "following_url": "https://api.github.com/users/shiyemin/following{/other_user}", "gists_url": "https://api.github.com/users/shiyemin/gists{/gist_id}", "starred_url": "https://api.github.com/users/shiyemin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shiyemin/subscriptions", "organizations_url": "https://api.github.com/users/shiyemin/orgs", "repos_url": "https://api.github.com/users/shiyemin/repos", "events_url": "https://api.github.com/users/shiyemin/events{/privacy}", "received_events_url": "https://api.github.com/users/shiyemin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-16T07:47:09Z", "updated_at": "2017-02-17T04:51:38Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=51059\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cancan101\">@cancan101</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5112846\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hengck23\">@hengck23</a> <a href=\"https://github.com/shiyemin/shuttleNet/blob/master/ops/ops.py#L55-L308\">Here</a> is my implementation. I also provide my code for setting the RMAX and DMAX.</p>\n<p>Basic usage:<br>\nbatch_renorm_decay=0.9<br>\nbatch_renorm_epsilon=0.001</p>\n<p>RMAX_decay = ops.adjust_max(200, 800, 1, 3, name='RMAXDECAY')<br>\nDMAX_decay = ops.adjust_max(200, 600, 1, 5, name='DMAXDECAY')<br>\nbatch_renorm_params = {<br>\n'renorm': True,<br>\n'RMAX': RMAX_decay,<br>\n'DMAX': DMAX_decay,<br>\n'decay': batch_renorm_decay,<br>\n'epsilon': batch_renorm_epsilon,<br>\n}<br>\nwith slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,<br>\nnormalizer_fn=ops.fused_batch_norm,<br>\nnormalizer_params=batch_renorm_params) as scope:</p>\n<p>BTW, my implementation is modified based on TF's fused_batch_norm and should be fully compatible with it.</p>", "body_text": "@cancan101 @hengck23 Here is my implementation. I also provide my code for setting the RMAX and DMAX.\nBasic usage:\nbatch_renorm_decay=0.9\nbatch_renorm_epsilon=0.001\nRMAX_decay = ops.adjust_max(200, 800, 1, 3, name='RMAXDECAY')\nDMAX_decay = ops.adjust_max(200, 600, 1, 5, name='DMAXDECAY')\nbatch_renorm_params = {\n'renorm': True,\n'RMAX': RMAX_decay,\n'DMAX': DMAX_decay,\n'decay': batch_renorm_decay,\n'epsilon': batch_renorm_epsilon,\n}\nwith slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,\nnormalizer_fn=ops.fused_batch_norm,\nnormalizer_params=batch_renorm_params) as scope:\nBTW, my implementation is modified based on TF's fused_batch_norm and should be fully compatible with it.", "body": "@cancan101 @hengck23 [Here](https://github.com/shiyemin/shuttleNet/blob/master/ops/ops.py#L55-L308) is my implementation. I also provide my code for setting the RMAX and DMAX.\r\n\r\nBasic usage:\r\nbatch_renorm_decay=0.9\r\nbatch_renorm_epsilon=0.001\r\n\r\nRMAX_decay = ops.adjust_max(200, 800, 1, 3, name='RMAXDECAY')\r\nDMAX_decay = ops.adjust_max(200, 600, 1, 5, name='DMAXDECAY')\r\nbatch_renorm_params = {\r\n    'renorm': True,\r\n    'RMAX': RMAX_decay,\r\n    'DMAX': DMAX_decay,\r\n    'decay': batch_renorm_decay,\r\n    'epsilon': batch_renorm_epsilon,\r\n}\r\nwith slim.arg_scope([slim.conv2d, slim.fully_connected], activation_fn=tf.nn.relu,\r\n                    normalizer_fn=ops.fused_batch_norm,\r\n                    normalizer_params=batch_renorm_params) as scope:\r\n\r\n\r\nBTW, my implementation is modified based on TF's fused_batch_norm and should be fully compatible with it."}