{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18943", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18943/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18943/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18943/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18943", "id": 318602490, "node_id": "MDU6SXNzdWUzMTg2MDI0OTA=", "number": 18943, "title": "ssd_mobilenet_v2 was slower than ssd_mobilenet_v1 in the tflite ?", "user": {"login": "WenguoLi", "id": 31765154, "node_id": "MDQ6VXNlcjMxNzY1MTU0", "avatar_url": "https://avatars0.githubusercontent.com/u/31765154?v=4", "gravatar_id": "", "url": "https://api.github.com/users/WenguoLi", "html_url": "https://github.com/WenguoLi", "followers_url": "https://api.github.com/users/WenguoLi/followers", "following_url": "https://api.github.com/users/WenguoLi/following{/other_user}", "gists_url": "https://api.github.com/users/WenguoLi/gists{/gist_id}", "starred_url": "https://api.github.com/users/WenguoLi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/WenguoLi/subscriptions", "organizations_url": "https://api.github.com/users/WenguoLi/orgs", "repos_url": "https://api.github.com/users/WenguoLi/repos", "events_url": "https://api.github.com/users/WenguoLi/events{/privacy}", "received_events_url": "https://api.github.com/users/WenguoLi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-28T05:34:07Z", "updated_at": "2018-06-07T10:35:54Z", "closed_at": "2018-05-02T02:36:44Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:Source</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.8.0rc0</li>\n<li><strong>Python version</strong>: 2.7.12</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.12.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>:cuda-9.0/7.0</li>\n<li><strong>GPU model and memory</strong>:GeForce GTX 1080/8105MiB</li>\n<li><strong>Phone</strong>: xiaomi5 (Snapdragon 820)</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using tflite's <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tools/benchmark_model.cc\">benchmark_model</a> tool, I tested the performance of the ssd_mobilenet_v1 and ssd_mobilenet_v2  models , and found that ssd_mobilenet_v2 was slower than ssd_mobilenet_v1, about 10ms, when setting to 4 threads.</p>\n<p>But SSDLite presented in <a href=\"https://arxiv.org/abs/1801.04381\" rel=\"nofollow\">MobileNetV2: Inverted Residuals and Linear Bottlenecks</a> is 35% faster than Mobilenet V1 SSD on a Google Pixel phone CPU (200ms vs. 270ms) at the same accuracy.</p>\n<p>Who can explain why ?</p>\n<h3>Source code / logs</h3>\n<p>ssd_mobilenet_v1 :</p>\n<pre><code> ./benchmark_model \\\n&gt;   --graph=\"mobilenet_ssd_v1_quan.tflite\" \\\n&gt;   --input_layer=\"Preprocessor/sub\" \\\n&gt;   --input_layer_shape=\"1,300,300,3\" \\\n&gt;   --input_layer_type=\"uint8\" \\\n&gt;   --run_delay=\"-1.0\" \\\n&gt;   --output_layer=\"concat,concat_1\" \\\n&gt;   --num_runs=50 \\\n&gt;   --num_threads=4 \\\n&gt;   --warmup_runs=1 \\\n&gt;   --use_nnapi=false\nWARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e\nGraph: [mobilenet_ssd_v1_quan.tflite]\nInput layers: [Preprocessor/sub]\nInput shapes: [1,300,300,3]\nInput types: [uint8]\nOutput layers: [concat,concat_1]\nNum runs: [50]\nInter-run delay (seconds): [-1.0]\nNum threads: [4]\nWarmup runs: [1]\nUse nnapi : [0]\nEnable profiling : [0]\nnnapi error: unable to open library libneuralnetworks.so\nInitialized session in 0.02646s\nRunning benchmark for 1 iterations: \ncount=1 min=220752 max=220752 avg=220752 std=0\nRunning benchmark for 50 iterations: \ncount=50 min=75066 max=193842 avg=82848.9 std=19942\nAverage inference timings in us: 82848 , Warmup: 220752,\n</code></pre>\n<p>ssd_mobilenet_v2 :</p>\n<pre><code>./benchmark_model \\\n&gt;   --graph=\"mobilenet_ssd_v2_quan.tflite\" \\\n&gt;   --input_layer=\"Preprocessor/sub\" \\\n&gt;   --input_layer_shape=\"1,300,300,3\" \\\n&gt;   --input_layer_type=\"uint8\" \\\n&gt;   --run_delay=\"-1.0\" \\\n&gt;   --output_layer=\"concat,concat_1\" \\\n&gt;   --num_runs=50 \\\n&gt;   --num_threads=4 \\\n&gt;   --warmup_runs=1 \\\n&gt;   --use_nnapi=false\nWARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e\nGraph: [mobilenet_ssd_v2_quan.tflite]\nInput layers: [Preprocessor/sub]\nInput shapes: [1,300,300,3]\nInput types: [uint8]\nOutput layers: [concat,concat_1]\nNum runs: [50]\nInter-run delay (seconds): [-1.0]\nNum threads: [4]\nWarmup runs: [1]\nUse nnapi : [0]\nEnable profiling : [0]\nnnapi error: unable to open library libneuralnetworks.so\nInitialized session in 0.033135s\nRunning benchmark for 1 iterations: \ncount=1 min=110434 max=110434 avg=110434 std=0\nRunning benchmark for 50 iterations: \ncount=50 min=78189 max=285788 avg=90162.9 std=29644\nAverage inference timings in us: 90162 , Warmup: 110434, \n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Ubuntu 16.04\nTensorFlow installed from (source or binary):Source\nTensorFlow version (use command below):1.8.0rc0\nPython version: 2.7.12\nBazel version (if compiling from source): 0.12.0\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version:cuda-9.0/7.0\nGPU model and memory:GeForce GTX 1080/8105MiB\nPhone: xiaomi5 (Snapdragon 820)\n\nDescribe the problem\nUsing tflite's benchmark_model tool, I tested the performance of the ssd_mobilenet_v1 and ssd_mobilenet_v2  models , and found that ssd_mobilenet_v2 was slower than ssd_mobilenet_v1, about 10ms, when setting to 4 threads.\nBut SSDLite presented in MobileNetV2: Inverted Residuals and Linear Bottlenecks is 35% faster than Mobilenet V1 SSD on a Google Pixel phone CPU (200ms vs. 270ms) at the same accuracy.\nWho can explain why ?\nSource code / logs\nssd_mobilenet_v1 :\n ./benchmark_model \\\n>   --graph=\"mobilenet_ssd_v1_quan.tflite\" \\\n>   --input_layer=\"Preprocessor/sub\" \\\n>   --input_layer_shape=\"1,300,300,3\" \\\n>   --input_layer_type=\"uint8\" \\\n>   --run_delay=\"-1.0\" \\\n>   --output_layer=\"concat,concat_1\" \\\n>   --num_runs=50 \\\n>   --num_threads=4 \\\n>   --warmup_runs=1 \\\n>   --use_nnapi=false\nWARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e\nGraph: [mobilenet_ssd_v1_quan.tflite]\nInput layers: [Preprocessor/sub]\nInput shapes: [1,300,300,3]\nInput types: [uint8]\nOutput layers: [concat,concat_1]\nNum runs: [50]\nInter-run delay (seconds): [-1.0]\nNum threads: [4]\nWarmup runs: [1]\nUse nnapi : [0]\nEnable profiling : [0]\nnnapi error: unable to open library libneuralnetworks.so\nInitialized session in 0.02646s\nRunning benchmark for 1 iterations: \ncount=1 min=220752 max=220752 avg=220752 std=0\nRunning benchmark for 50 iterations: \ncount=50 min=75066 max=193842 avg=82848.9 std=19942\nAverage inference timings in us: 82848 , Warmup: 220752,\n\nssd_mobilenet_v2 :\n./benchmark_model \\\n>   --graph=\"mobilenet_ssd_v2_quan.tflite\" \\\n>   --input_layer=\"Preprocessor/sub\" \\\n>   --input_layer_shape=\"1,300,300,3\" \\\n>   --input_layer_type=\"uint8\" \\\n>   --run_delay=\"-1.0\" \\\n>   --output_layer=\"concat,concat_1\" \\\n>   --num_runs=50 \\\n>   --num_threads=4 \\\n>   --warmup_runs=1 \\\n>   --use_nnapi=false\nWARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e\nGraph: [mobilenet_ssd_v2_quan.tflite]\nInput layers: [Preprocessor/sub]\nInput shapes: [1,300,300,3]\nInput types: [uint8]\nOutput layers: [concat,concat_1]\nNum runs: [50]\nInter-run delay (seconds): [-1.0]\nNum threads: [4]\nWarmup runs: [1]\nUse nnapi : [0]\nEnable profiling : [0]\nnnapi error: unable to open library libneuralnetworks.so\nInitialized session in 0.033135s\nRunning benchmark for 1 iterations: \ncount=1 min=110434 max=110434 avg=110434 std=0\nRunning benchmark for 50 iterations: \ncount=50 min=78189 max=285788 avg=90162.9 std=29644\nAverage inference timings in us: 90162 , Warmup: 110434,", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:Source\r\n- **TensorFlow version (use command below)**:1.8.0rc0\r\n- **Python version**: 2.7.12\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**:cuda-9.0/7.0\r\n- **GPU model and memory**:GeForce GTX 1080/8105MiB\r\n- **Phone**: xiaomi5 (Snapdragon 820)\r\n\r\n### Describe the problem\r\nUsing tflite's [benchmark_model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/tools/benchmark_model.cc) tool, I tested the performance of the ssd_mobilenet_v1 and ssd_mobilenet_v2  models , and found that ssd_mobilenet_v2 was slower than ssd_mobilenet_v1, about 10ms, when setting to 4 threads.\r\n\r\nBut SSDLite presented in [MobileNetV2: Inverted Residuals and Linear Bottlenecks](https://arxiv.org/abs/1801.04381) is 35% faster than Mobilenet V1 SSD on a Google Pixel phone CPU (200ms vs. 270ms) at the same accuracy.\r\n\r\nWho can explain why ?\r\n### Source code / logs\r\nssd_mobilenet_v1 : \r\n```\r\n ./benchmark_model \\\r\n>   --graph=\"mobilenet_ssd_v1_quan.tflite\" \\\r\n>   --input_layer=\"Preprocessor/sub\" \\\r\n>   --input_layer_shape=\"1,300,300,3\" \\\r\n>   --input_layer_type=\"uint8\" \\\r\n>   --run_delay=\"-1.0\" \\\r\n>   --output_layer=\"concat,concat_1\" \\\r\n>   --num_runs=50 \\\r\n>   --num_threads=4 \\\r\n>   --warmup_runs=1 \\\r\n>   --use_nnapi=false\r\nWARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e\r\nGraph: [mobilenet_ssd_v1_quan.tflite]\r\nInput layers: [Preprocessor/sub]\r\nInput shapes: [1,300,300,3]\r\nInput types: [uint8]\r\nOutput layers: [concat,concat_1]\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1.0]\r\nNum threads: [4]\r\nWarmup runs: [1]\r\nUse nnapi : [0]\r\nEnable profiling : [0]\r\nnnapi error: unable to open library libneuralnetworks.so\r\nInitialized session in 0.02646s\r\nRunning benchmark for 1 iterations: \r\ncount=1 min=220752 max=220752 avg=220752 std=0\r\nRunning benchmark for 50 iterations: \r\ncount=50 min=75066 max=193842 avg=82848.9 std=19942\r\nAverage inference timings in us: 82848 , Warmup: 220752,\r\n``` \r\nssd_mobilenet_v2 :\r\n```\r\n./benchmark_model \\\r\n>   --graph=\"mobilenet_ssd_v2_quan.tflite\" \\\r\n>   --input_layer=\"Preprocessor/sub\" \\\r\n>   --input_layer_shape=\"1,300,300,3\" \\\r\n>   --input_layer_type=\"uint8\" \\\r\n>   --run_delay=\"-1.0\" \\\r\n>   --output_layer=\"concat,concat_1\" \\\r\n>   --num_runs=50 \\\r\n>   --num_threads=4 \\\r\n>   --warmup_runs=1 \\\r\n>   --use_nnapi=false\r\nWARNING: linker: /data/local/tmp/benchmark_model: unused DT entry: type 0xf arg 0x82e\r\nGraph: [mobilenet_ssd_v2_quan.tflite]\r\nInput layers: [Preprocessor/sub]\r\nInput shapes: [1,300,300,3]\r\nInput types: [uint8]\r\nOutput layers: [concat,concat_1]\r\nNum runs: [50]\r\nInter-run delay (seconds): [-1.0]\r\nNum threads: [4]\r\nWarmup runs: [1]\r\nUse nnapi : [0]\r\nEnable profiling : [0]\r\nnnapi error: unable to open library libneuralnetworks.so\r\nInitialized session in 0.033135s\r\nRunning benchmark for 1 iterations: \r\ncount=1 min=110434 max=110434 avg=110434 std=0\r\nRunning benchmark for 50 iterations: \r\ncount=50 min=78189 max=285788 avg=90162.9 std=29644\r\nAverage inference timings in us: 90162 , Warmup: 110434, \r\n```"}