{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21679", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21679/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21679/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21679/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21679", "id": 351587931, "node_id": "MDU6SXNzdWUzNTE1ODc5MzE=", "number": 21679, "title": "How to clip weights of a dense layer between every step of training op?", "user": {"login": "jingwb222", "id": 42470353, "node_id": "MDQ6VXNlcjQyNDcwMzUz", "avatar_url": "https://avatars3.githubusercontent.com/u/42470353?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jingwb222", "html_url": "https://github.com/jingwb222", "followers_url": "https://api.github.com/users/jingwb222/followers", "following_url": "https://api.github.com/users/jingwb222/following{/other_user}", "gists_url": "https://api.github.com/users/jingwb222/gists{/gist_id}", "starred_url": "https://api.github.com/users/jingwb222/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jingwb222/subscriptions", "organizations_url": "https://api.github.com/users/jingwb222/orgs", "repos_url": "https://api.github.com/users/jingwb222/repos", "events_url": "https://api.github.com/users/jingwb222/events{/privacy}", "received_events_url": "https://api.github.com/users/jingwb222/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-17T13:09:27Z", "updated_at": "2018-10-05T21:57:53Z", "closed_at": "2018-10-05T21:57:53Z", "author_association": "NONE", "body_html": "<p>Is there any possible way to do a custom op (for example, to clip by values of the weights of a dense layer manually every training step)</p>\n<pre><code>if mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='logits/kernel')[0]\n    clip_op = tf.assign(weights, tf.clip_by_value(weights, 0.01, 0.1))\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n</code></pre>\n<p>Above, I want to clip the weights of <code>logits/kernel</code> layer between every training op, but it does not work as intended, the weights of that layer would still drift outside the range of (0.01, 0.1). I wonder what I'm missing here.</p>", "body_text": "Is there any possible way to do a custom op (for example, to clip by values of the weights of a dense layer manually every training step)\nif mode == tf.estimator.ModeKeys.TRAIN:\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n    weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='logits/kernel')[0]\n    clip_op = tf.assign(weights, tf.clip_by_value(weights, 0.01, 0.1))\n    train_op = optimizer.minimize(\n        loss=loss,\n        global_step=tf.train.get_global_step())\n    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n\nAbove, I want to clip the weights of logits/kernel layer between every training op, but it does not work as intended, the weights of that layer would still drift outside the range of (0.01, 0.1). I wonder what I'm missing here.", "body": "Is there any possible way to do a custom op (for example, to clip by values of the weights of a dense layer manually every training step)\r\n\r\n    if mode == tf.estimator.ModeKeys.TRAIN:\r\n        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\r\n        weights = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='logits/kernel')[0]\r\n        clip_op = tf.assign(weights, tf.clip_by_value(weights, 0.01, 0.1))\r\n        train_op = optimizer.minimize(\r\n            loss=loss,\r\n            global_step=tf.train.get_global_step())\r\n        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\r\n\r\nAbove, I want to clip the weights of `logits/kernel` layer between every training op, but it does not work as intended, the weights of that layer would still drift outside the range of (0.01, 0.1). I wonder what I'm missing here."}