{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18144", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18144/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18144/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18144/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18144", "id": 310239520, "node_id": "MDU6SXNzdWUzMTAyMzk1MjA=", "number": 18144, "title": "tf.data.apply does not return output shape", "user": {"login": "nthakor", "id": 19538934, "node_id": "MDQ6VXNlcjE5NTM4OTM0", "avatar_url": "https://avatars3.githubusercontent.com/u/19538934?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nthakor", "html_url": "https://github.com/nthakor", "followers_url": "https://api.github.com/users/nthakor/followers", "following_url": "https://api.github.com/users/nthakor/following{/other_user}", "gists_url": "https://api.github.com/users/nthakor/gists{/gist_id}", "starred_url": "https://api.github.com/users/nthakor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nthakor/subscriptions", "organizations_url": "https://api.github.com/users/nthakor/orgs", "repos_url": "https://api.github.com/users/nthakor/repos", "events_url": "https://api.github.com/users/nthakor/events{/privacy}", "received_events_url": "https://api.github.com/users/nthakor/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-03-31T13:07:23Z", "updated_at": "2018-04-17T15:33:23Z", "closed_at": "2018-04-17T15:33:23Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS High Sierra</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.8.0-dev20180328</li>\n<li><strong>Python version</strong>:  3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am using tf.data API to transform my dataset. Following is the code Transform:</p>\n<h3>Transformation Function</h3>\n<div class=\"highlight highlight-source-python\"><pre>win_len <span class=\"pl-k\">=</span> <span class=\"pl-c1\">32</span>\nlook_back <span class=\"pl-k\">=</span> <span class=\"pl-c1\">11</span>\npad <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(look_back<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span>)\nslen <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1344</span>\nsh <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(slen <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span><span class=\"pl-k\">*</span>pad)\nbatch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">840</span>\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_parse_function</span>(<span class=\"pl-smi\">example_proto</span>):\n    keys_to_features <span class=\"pl-k\">=</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>:tf.FixedLenFeature((slen), tf.float32),\n                        <span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>:tf.FixedLenFeature((slen), tf.float32)}\n    parsed_features <span class=\"pl-k\">=</span> tf.parse_single_example(example_proto, keys_to_features)\n    <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>pure<span class=\"pl-pds\">\"</span></span>:parsed_features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>], <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>mix<span class=\"pl-pds\">\"</span></span>:parsed_features[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>]}\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_dict_pad</span>(<span class=\"pl-smi\">sig</span>):\n    <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>:tf.pad(sig[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>],[[pad,pad]]),<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>:tf.pad(sig[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>],[[pad,pad]])}\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_dict_slide_reduce</span>(<span class=\"pl-smi\">sig</span>,<span class=\"pl-smi\">sh</span>):\n    <span class=\"pl-k\">return</span> sig.apply(sliding_window_batch(<span class=\"pl-v\">window_size</span><span class=\"pl-k\">=</span>look_back,<span class=\"pl-v\">stride</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>))\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_pure_reduce</span>(<span class=\"pl-smi\">sig</span>):\n    <span class=\"pl-k\">return</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>: sig[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>][:,pad],<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>:sig[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>] }</pre></div>\n<h3>Data Reading:</h3>\n<div class=\"highlight highlight-source-python\"><pre>train_data <span class=\"pl-k\">=</span> tf.data.TFRecordDataset(train_files)\\\n            .map(_parse_function)\\\n            .map(_dict_pad)\\\n            .apply(unbatch())\\\n            .apply((group_by_window(<span class=\"pl-v\">key_func</span><span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">sig</span> : <span class=\"pl-c1\">1</span>,<span class=\"pl-v\">reduce_func</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">key</span>,<span class=\"pl-smi\">sig</span> : _dict_slide_reduce(sig,sh),<span class=\"pl-v\">window_size</span><span class=\"pl-k\">=</span>sh)))\\\n            .batch(win_len)\\\n            .map(_pure_reduce)\\\n            .batch(batch_size) \ntrain_data <span class=\"pl-k\">=</span> train_data.prefetch(<span class=\"pl-c1\">100</span>)\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\niterator_train <span class=\"pl-k\">=</span> train_data.make_one_shot_iterator()\ntr_sig <span class=\"pl-k\">=</span> iterator_train.get_next()</pre></div>\n<p>output of <code>print(train_data.output_shapes) </code> is following:</p>\n<p><code>{'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])} </code></p>\n<p>output shape after each transformation:</p>\n<div class=\"highlight highlight-source-python\"><pre>train_data <span class=\"pl-k\">=</span> tf.data.TFRecordDataset(train_files)\ntrain_data <span class=\"pl-k\">=</span> train_data.map(_parse_function)\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([Dimension(1344)]), 'mix': TensorShape([Dimension(1344)])}</span>\ntrain_data <span class=\"pl-k\">=</span> train_data.map(_dict_pad)\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([Dimension(1354)]), 'mix': TensorShape([Dimension(1354)])}</span>\ntrain_data <span class=\"pl-k\">=</span> train_data.apply(tf.contrib.data.unbatch())\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([]), 'mix': TensorShape([])}</span>\ntrain_data <span class=\"pl-k\">=</span> train_data.apply((group_by_window(<span class=\"pl-v\">key_func</span><span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">sig</span> : <span class=\"pl-c1\">1</span>,<span class=\"pl-v\">reduce_func</span> <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">key</span>,<span class=\"pl-smi\">sig</span> : _dict_slide_reduce(sig,sh),<span class=\"pl-v\">window_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">32</span>)))\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None)])}</span>\ntrain_data <span class=\"pl-k\">=</span> train_data.batch(win_len)\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}</span>\ntrain_data <span class=\"pl-k\">=</span> train_data.map(_pure_reduce)\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}</span>\ntrain_data <span class=\"pl-k\">=</span> train_data.batch(batch_size)\n<span class=\"pl-c1\">print</span>(train_data.output_shapes)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])}</span></pre></div>\n<p>The expected output is <code>{'mix': TensorShape([Dimension(None), Dimension(32), Dimension(11)]), 'pure': TensorShape([Dimension(None), Dimension(32)])} </code></p>\n<div class=\"highlight highlight-source-python\"><pre>iterator_train <span class=\"pl-k\">=</span> train_data.make_one_shot_iterator()\ntr_sig <span class=\"pl-k\">=</span> iterator_train.get_next()\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    data <span class=\"pl-k\">=</span> sess.run(tr_sig)\n    <span class=\"pl-c1\">print</span>(data[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>mix<span class=\"pl-pds\">'</span></span>].shape,data[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>pure<span class=\"pl-pds\">'</span></span>].shape)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> (840, 32, 11) (840, 32)</span></pre></div>\n<p>Because of this issue I am getting following error in  dynamic_rnn:</p>\n<p><code>ValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.</code></p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS High Sierra\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.8.0-dev20180328\nPython version:  3.6\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce:\n\nDescribe the problem\nI am using tf.data API to transform my dataset. Following is the code Transform:\nTransformation Function\nwin_len = 32\nlook_back = 11\npad = int(look_back/2)\nslen = 1344\nsh = int(slen + 2*pad)\nbatch_size = 840\n\ndef _parse_function(example_proto):\n    keys_to_features = {'mix':tf.FixedLenFeature((slen), tf.float32),\n                        'pure':tf.FixedLenFeature((slen), tf.float32)}\n    parsed_features = tf.parse_single_example(example_proto, keys_to_features)\n    return {\"pure\":parsed_features['pure'], \"mix\":parsed_features['mix']}\ndef _dict_pad(sig):\n    return {'pure':tf.pad(sig['pure'],[[pad,pad]]),'mix':tf.pad(sig['mix'],[[pad,pad]])}\ndef _dict_slide_reduce(sig,sh):\n    return sig.apply(sliding_window_batch(window_size=look_back,stride=1))\ndef _pure_reduce(sig):\n    return {'pure': sig['pure'][:,pad],'mix':sig['mix'] }\nData Reading:\ntrain_data = tf.data.TFRecordDataset(train_files)\\\n            .map(_parse_function)\\\n            .map(_dict_pad)\\\n            .apply(unbatch())\\\n            .apply((group_by_window(key_func= lambda sig : 1,reduce_func = lambda key,sig : _dict_slide_reduce(sig,sh),window_size=sh)))\\\n            .batch(win_len)\\\n            .map(_pure_reduce)\\\n            .batch(batch_size) \ntrain_data = train_data.prefetch(100)\nprint(train_data.output_shapes)\niterator_train = train_data.make_one_shot_iterator()\ntr_sig = iterator_train.get_next()\noutput of print(train_data.output_shapes)  is following:\n{'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])} \noutput shape after each transformation:\ntrain_data = tf.data.TFRecordDataset(train_files)\ntrain_data = train_data.map(_parse_function)\nprint(train_data.output_shapes)\n# {'pure': TensorShape([Dimension(1344)]), 'mix': TensorShape([Dimension(1344)])}\ntrain_data = train_data.map(_dict_pad)\nprint(train_data.output_shapes)\n# {'pure': TensorShape([Dimension(1354)]), 'mix': TensorShape([Dimension(1354)])}\ntrain_data = train_data.apply(tf.contrib.data.unbatch())\nprint(train_data.output_shapes)\n# {'pure': TensorShape([]), 'mix': TensorShape([])}\ntrain_data = train_data.apply((group_by_window(key_func= lambda sig : 1,reduce_func = lambda key,sig : _dict_slide_reduce(sig,sh),window_size=32)))\nprint(train_data.output_shapes)\n# {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None)])}\ntrain_data = train_data.batch(win_len)\nprint(train_data.output_shapes)\n# {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}\ntrain_data = train_data.map(_pure_reduce)\nprint(train_data.output_shapes)\n# {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}\ntrain_data = train_data.batch(batch_size)\nprint(train_data.output_shapes)\n# {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])}\nThe expected output is {'mix': TensorShape([Dimension(None), Dimension(32), Dimension(11)]), 'pure': TensorShape([Dimension(None), Dimension(32)])} \niterator_train = train_data.make_one_shot_iterator()\ntr_sig = iterator_train.get_next()\nwith tf.Session() as sess:\n    data = sess.run(tr_sig)\n    print(data['mix'].shape,data['pure'].shape)\n# (840, 32, 11) (840, 32)\nBecause of this issue I am getting following error in  dynamic_rnn:\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS High Sierra\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.8.0-dev20180328\r\n- **Python version**:  3.6\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI am using tf.data API to transform my dataset. Following is the code Transform:\r\n\r\n### Transformation Function\r\n```python\r\n\r\nwin_len = 32\r\nlook_back = 11\r\npad = int(look_back/2)\r\nslen = 1344\r\nsh = int(slen + 2*pad)\r\nbatch_size = 840\r\n\r\ndef _parse_function(example_proto):\r\n    keys_to_features = {'mix':tf.FixedLenFeature((slen), tf.float32),\r\n                        'pure':tf.FixedLenFeature((slen), tf.float32)}\r\n    parsed_features = tf.parse_single_example(example_proto, keys_to_features)\r\n    return {\"pure\":parsed_features['pure'], \"mix\":parsed_features['mix']}\r\ndef _dict_pad(sig):\r\n    return {'pure':tf.pad(sig['pure'],[[pad,pad]]),'mix':tf.pad(sig['mix'],[[pad,pad]])}\r\ndef _dict_slide_reduce(sig,sh):\r\n    return sig.apply(sliding_window_batch(window_size=look_back,stride=1))\r\ndef _pure_reduce(sig):\r\n    return {'pure': sig['pure'][:,pad],'mix':sig['mix'] }\r\n```\r\n\r\n### Data Reading:\r\n\r\n```python\r\ntrain_data = tf.data.TFRecordDataset(train_files)\\\r\n            .map(_parse_function)\\\r\n            .map(_dict_pad)\\\r\n            .apply(unbatch())\\\r\n            .apply((group_by_window(key_func= lambda sig : 1,reduce_func = lambda key,sig : _dict_slide_reduce(sig,sh),window_size=sh)))\\\r\n            .batch(win_len)\\\r\n            .map(_pure_reduce)\\\r\n            .batch(batch_size) \r\ntrain_data = train_data.prefetch(100)\r\nprint(train_data.output_shapes)\r\niterator_train = train_data.make_one_shot_iterator()\r\ntr_sig = iterator_train.get_next()\r\n```\r\n\r\noutput of `print(train_data.output_shapes) ` is following:\r\n\r\n`{'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])}\r\n`\r\n\r\noutput shape after each transformation:\r\n```python\r\ntrain_data = tf.data.TFRecordDataset(train_files)\r\ntrain_data = train_data.map(_parse_function)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(1344)]), 'mix': TensorShape([Dimension(1344)])}\r\ntrain_data = train_data.map(_dict_pad)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(1354)]), 'mix': TensorShape([Dimension(1354)])}\r\ntrain_data = train_data.apply(tf.contrib.data.unbatch())\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([]), 'mix': TensorShape([])}\r\ntrain_data = train_data.apply((group_by_window(key_func= lambda sig : 1,reduce_func = lambda key,sig : _dict_slide_reduce(sig,sh),window_size=32)))\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None)])}\r\ntrain_data = train_data.batch(win_len)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}\r\ntrain_data = train_data.map(_pure_reduce)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None)])}\r\ntrain_data = train_data.batch(batch_size)\r\nprint(train_data.output_shapes)\r\n# {'pure': TensorShape([Dimension(None), Dimension(None)]), 'mix': TensorShape([Dimension(None), Dimension(None), Dimension(None)])}\r\n```\r\n\r\nThe expected output is `{'mix': TensorShape([Dimension(None), Dimension(32), Dimension(11)]), 'pure': TensorShape([Dimension(None), Dimension(32)])}\r\n`\r\n```python\r\niterator_train = train_data.make_one_shot_iterator()\r\ntr_sig = iterator_train.get_next()\r\nwith tf.Session() as sess:\r\n    data = sess.run(tr_sig)\r\n    print(data['mix'].shape,data['pure'].shape)\r\n# (840, 32, 11) (840, 32)\r\n```\r\nBecause of this issue I am getting following error in  dynamic_rnn:\r\n\r\n`\r\nValueError: Input size (depth of inputs) must be accessible via shape inference, but saw value None.\r\n`"}