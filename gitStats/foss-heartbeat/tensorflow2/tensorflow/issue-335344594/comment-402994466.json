{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/402994466", "html_url": "https://github.com/tensorflow/tensorflow/issues/20273#issuecomment-402994466", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20273", "id": 402994466, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjk5NDQ2Ng==", "user": {"login": "danielwatson6", "id": 3270063, "node_id": "MDQ6VXNlcjMyNzAwNjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/3270063?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danielwatson6", "html_url": "https://github.com/danielwatson6", "followers_url": "https://api.github.com/users/danielwatson6/followers", "following_url": "https://api.github.com/users/danielwatson6/following{/other_user}", "gists_url": "https://api.github.com/users/danielwatson6/gists{/gist_id}", "starred_url": "https://api.github.com/users/danielwatson6/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danielwatson6/subscriptions", "organizations_url": "https://api.github.com/users/danielwatson6/orgs", "repos_url": "https://api.github.com/users/danielwatson6/repos", "events_url": "https://api.github.com/users/danielwatson6/events{/privacy}", "received_events_url": "https://api.github.com/users/danielwatson6/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-06T10:23:45Z", "updated_at": "2018-07-06T10:23:45Z", "author_association": "NONE", "body_html": "<p>I found the following markdown documentation within the source code. It's in the docstring of the private class <code>_CudnnRNN </code> in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py\">this file</a>.</p>\n<p>Could someone add this to the website API docs so this issue can be resolved? Many thanks.</p>\n<p>Abstract class for RNN layers with Cudnn implementation.<br>\nCudnn RNNs have two major differences from other platform-independent RNNs tf<br>\nprovides:</p>\n<ul>\n<li>Cudnn LSTM and GRU are mathematically different from their tf counterparts.<br>\n(e.g. @{tf.contrib.rnn.LSTMBlockCell} and @{tf.nn.rnn_cell.GRUCell}.</li>\n<li>Cudnn-trained checkpoints are not directly compatible with tf RNNs:\n<ul>\n<li>They use a single opaque parameter buffer for the entire (possibly)<br>\nmulti-layer multi-directional RNN; Whereas tf RNN weights are per-cell and<br>\nlayer.</li>\n<li>The size and layout of the parameter buffers may change between<br>\nCUDA/CuDNN/GPU generations. Because of that, the opaque parameter variable<br>\ndoes not have a static shape and is not partitionable. Instead of using<br>\npartitioning to alleviate the PS's traffic load, try building a<br>\nmulti-tower model and do gradient aggregation locally within the host<br>\nbefore updating the PS. See <a href=\"https://www.tensorflow.org/performance/performance_models#parameter_server_variables\" rel=\"nofollow\">https://www.tensorflow.org/performance/performance_models#parameter_server_variables</a><br>\nfor a detailed performance guide.<br>\nConsequently, if one plans to use Cudnn trained models on both GPU and CPU<br>\nfor inference and training, one needs to:</li>\n</ul>\n</li>\n<li>Create a CudnnOpaqueParamsSaveable subclass object to save RNN params in<br>\ncanonical format. (This is done for you automatically during layer building<br>\nprocess.)</li>\n<li>When not using a Cudnn RNN class, use CudnnCompatibleRNN classes to load the<br>\ncheckpoints. These classes are platform-independent and perform the same<br>\ncomputation as Cudnn for training and inference.<br>\nSimilarly, CudnnCompatibleRNN-trained checkpoints can be loaded by CudnnRNN<br>\nclasses seamlessly.<br>\nBelow is a typical workflow(using LSTM as an example):<br>\nfor detailed performance guide.</li>\n</ul>\n<h1>Use Cudnn-trained checkpoints with CudnnCompatibleRNNs</h1>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">with</span> tf.Graph().as_default():\n  lstm <span class=\"pl-k\">=</span> CudnnLSTM(num_layers, num_units, direction, <span class=\"pl-c1\">...</span>)\n  outputs, output_states <span class=\"pl-k\">=</span> lstm(inputs, initial_states, <span class=\"pl-v\">training</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> If user plans to delay calling the cell with inputs, one can do</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> lstm.build(input_shape)</span>\n  saver <span class=\"pl-k\">=</span> Saver()\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> training subgraph</span>\n  <span class=\"pl-c1\">...</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Once in a while save the model.</span>\n  saver.save(save_path)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Inference subgraph for unidirectional RNN on, e.g., CPU or mobile.</span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n  single_cell <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span>: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">NOTE</span>: Even if there's only one layer, the cell needs to be wrapped in</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> MultiRNNCell.</span>\n  cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell(\n    [single_cell() <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_layers)])\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Leave the scope arg unset.</span>\n  outputs, final_state <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(cell, inputs, initial_state, <span class=\"pl-c1\">...</span>)\n  saver <span class=\"pl-k\">=</span> Saver()\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create session</span>\n  sess <span class=\"pl-k\">=</span> <span class=\"pl-c1\">...</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Restores</span>\n  saver.restore(sess, save_path)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Inference subgraph for bidirectional RNN</span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default():\n  single_cell <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span>: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\n  cells_fw <span class=\"pl-k\">=</span> [single_cell() <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_layers)]\n  cells_bw <span class=\"pl-k\">=</span> [single_cell() <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_layers)]\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Leave the scope arg unset.</span>\n  (outputs, output_state_fw,\n   output_state_bw) <span class=\"pl-k\">=</span> tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n       cells_fw, cells_bw, inputs, <span class=\"pl-c1\">...</span>)\n  saver <span class=\"pl-k\">=</span> Saver()\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create session</span>\n  sess <span class=\"pl-k\">=</span> <span class=\"pl-c1\">...</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Restores</span>\n  saver.restore(sess, save_path)</pre></div>", "body_text": "I found the following markdown documentation within the source code. It's in the docstring of the private class _CudnnRNN  in this file.\nCould someone add this to the website API docs so this issue can be resolved? Many thanks.\nAbstract class for RNN layers with Cudnn implementation.\nCudnn RNNs have two major differences from other platform-independent RNNs tf\nprovides:\n\nCudnn LSTM and GRU are mathematically different from their tf counterparts.\n(e.g. @{tf.contrib.rnn.LSTMBlockCell} and @{tf.nn.rnn_cell.GRUCell}.\nCudnn-trained checkpoints are not directly compatible with tf RNNs:\n\nThey use a single opaque parameter buffer for the entire (possibly)\nmulti-layer multi-directional RNN; Whereas tf RNN weights are per-cell and\nlayer.\nThe size and layout of the parameter buffers may change between\nCUDA/CuDNN/GPU generations. Because of that, the opaque parameter variable\ndoes not have a static shape and is not partitionable. Instead of using\npartitioning to alleviate the PS's traffic load, try building a\nmulti-tower model and do gradient aggregation locally within the host\nbefore updating the PS. See https://www.tensorflow.org/performance/performance_models#parameter_server_variables\nfor a detailed performance guide.\nConsequently, if one plans to use Cudnn trained models on both GPU and CPU\nfor inference and training, one needs to:\n\n\nCreate a CudnnOpaqueParamsSaveable subclass object to save RNN params in\ncanonical format. (This is done for you automatically during layer building\nprocess.)\nWhen not using a Cudnn RNN class, use CudnnCompatibleRNN classes to load the\ncheckpoints. These classes are platform-independent and perform the same\ncomputation as Cudnn for training and inference.\nSimilarly, CudnnCompatibleRNN-trained checkpoints can be loaded by CudnnRNN\nclasses seamlessly.\nBelow is a typical workflow(using LSTM as an example):\nfor detailed performance guide.\n\nUse Cudnn-trained checkpoints with CudnnCompatibleRNNs\nwith tf.Graph().as_default():\n  lstm = CudnnLSTM(num_layers, num_units, direction, ...)\n  outputs, output_states = lstm(inputs, initial_states, training=True)\n  # If user plans to delay calling the cell with inputs, one can do\n  # lstm.build(input_shape)\n  saver = Saver()\n  # training subgraph\n  ...\n  # Once in a while save the model.\n  saver.save(save_path)\n# Inference subgraph for unidirectional RNN on, e.g., CPU or mobile.\nwith tf.Graph().as_default():\n  single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\n  # NOTE: Even if there's only one layer, the cell needs to be wrapped in\n  # MultiRNNCell.\n  cell = tf.nn.rnn_cell.MultiRNNCell(\n    [single_cell() for _ in range(num_layers)])\n  # Leave the scope arg unset.\n  outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state, ...)\n  saver = Saver()\n  # Create session\n  sess = ...\n  # Restores\n  saver.restore(sess, save_path)\n# Inference subgraph for bidirectional RNN\nwith tf.Graph().as_default():\n  single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\n  cells_fw = [single_cell() for _ in range(num_layers)]\n  cells_bw = [single_cell() for _ in range(num_layers)]\n  # Leave the scope arg unset.\n  (outputs, output_state_fw,\n   output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n       cells_fw, cells_bw, inputs, ...)\n  saver = Saver()\n  # Create session\n  sess = ...\n  # Restores\n  saver.restore(sess, save_path)", "body": "I found the following markdown documentation within the source code. It's in the docstring of the private class `_CudnnRNN ` in [this file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/cudnn_rnn/python/layers/cudnn_rnn.py).\r\n\r\nCould someone add this to the website API docs so this issue can be resolved? Many thanks.\r\n\r\n\r\nAbstract class for RNN layers with Cudnn implementation.\r\n  Cudnn RNNs have two major differences from other platform-independent RNNs tf\r\n  provides:\r\n  * Cudnn LSTM and GRU are mathematically different from their tf counterparts.\r\n    (e.g. @{tf.contrib.rnn.LSTMBlockCell} and @{tf.nn.rnn_cell.GRUCell}.\r\n  * Cudnn-trained checkpoints are not directly compatible with tf RNNs:\r\n    * They use a single opaque parameter buffer for the entire (possibly)\r\n      multi-layer multi-directional RNN; Whereas tf RNN weights are per-cell and\r\n      layer.\r\n    * The size and layout of the parameter buffers may change between\r\n      CUDA/CuDNN/GPU generations. Because of that, the opaque parameter variable\r\n      does not have a static shape and is not partitionable. Instead of using\r\n      partitioning to alleviate the PS's traffic load, try building a\r\n      multi-tower model and do gradient aggregation locally within the host\r\n      before updating the PS. See https://www.tensorflow.org/performance/performance_models#parameter_server_variables\r\n      for a detailed performance guide.\r\n  Consequently, if one plans to use Cudnn trained models on both GPU and CPU\r\n  for inference and training, one needs to:\r\n  * Create a CudnnOpaqueParamsSaveable subclass object to save RNN params in\r\n    canonical format. (This is done for you automatically during layer building\r\n    process.)\r\n  * When not using a Cudnn RNN class, use CudnnCompatibleRNN classes to load the\r\n    checkpoints. These classes are platform-independent and perform the same\r\n    computation as Cudnn for training and inference.\r\n  Similarly, CudnnCompatibleRNN-trained checkpoints can be loaded by CudnnRNN\r\n  classes seamlessly.\r\n  Below is a typical workflow(using LSTM as an example):\r\n  for detailed performance guide.\r\n  # Use Cudnn-trained checkpoints with CudnnCompatibleRNNs\r\n  ```python\r\n  with tf.Graph().as_default():\r\n    lstm = CudnnLSTM(num_layers, num_units, direction, ...)\r\n    outputs, output_states = lstm(inputs, initial_states, training=True)\r\n    # If user plans to delay calling the cell with inputs, one can do\r\n    # lstm.build(input_shape)\r\n    saver = Saver()\r\n    # training subgraph\r\n    ...\r\n    # Once in a while save the model.\r\n    saver.save(save_path)\r\n  # Inference subgraph for unidirectional RNN on, e.g., CPU or mobile.\r\n  with tf.Graph().as_default():\r\n    single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\r\n    # NOTE: Even if there's only one layer, the cell needs to be wrapped in\r\n    # MultiRNNCell.\r\n    cell = tf.nn.rnn_cell.MultiRNNCell(\r\n      [single_cell() for _ in range(num_layers)])\r\n    # Leave the scope arg unset.\r\n    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, initial_state, ...)\r\n    saver = Saver()\r\n    # Create session\r\n    sess = ...\r\n    # Restores\r\n    saver.restore(sess, save_path)\r\n  # Inference subgraph for bidirectional RNN\r\n  with tf.Graph().as_default():\r\n    single_cell = lambda: tf.contrib.cudnn_rnn.CudnnCompatibleLSTM(num_units)\r\n    cells_fw = [single_cell() for _ in range(num_layers)]\r\n    cells_bw = [single_cell() for _ in range(num_layers)]\r\n    # Leave the scope arg unset.\r\n    (outputs, output_state_fw,\r\n     output_state_bw) = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\r\n         cells_fw, cells_bw, inputs, ...)\r\n    saver = Saver()\r\n    # Create session\r\n    sess = ...\r\n    # Restores\r\n    saver.restore(sess, save_path)\r\n  ```"}