{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10535", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10535/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10535/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10535/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10535", "id": 234534261, "node_id": "MDU6SXNzdWUyMzQ1MzQyNjE=", "number": 10535, "title": "tf.nn.conv3d_transpose really slow on i7 CPU with 100+G free memory", "user": {"login": "laoreja", "id": 9369143, "node_id": "MDQ6VXNlcjkzNjkxNDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/9369143?v=4", "gravatar_id": "", "url": "https://api.github.com/users/laoreja", "html_url": "https://github.com/laoreja", "followers_url": "https://api.github.com/users/laoreja/followers", "following_url": "https://api.github.com/users/laoreja/following{/other_user}", "gists_url": "https://api.github.com/users/laoreja/gists{/gist_id}", "starred_url": "https://api.github.com/users/laoreja/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/laoreja/subscriptions", "organizations_url": "https://api.github.com/users/laoreja/orgs", "repos_url": "https://api.github.com/users/laoreja/repos", "events_url": "https://api.github.com/users/laoreja/events{/privacy}", "received_events_url": "https://api.github.com/users/laoreja/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-06-08T13:50:49Z", "updated_at": "2018-02-08T22:29:30Z", "closed_at": "2018-02-08T22:29:29Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: virtualenv pip</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.1.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0</li>\n<li><strong>GPU model and memory</strong>: on CPU</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>It takes about 10 mins to do the tf.nn.conv3d_transpose computation.<br>\nThe input size is [1, 97, 128, 256, 32], kernel size is 3<em>3</em>3, strides is [1, 2, 2, 2, 1], and the output size is [1, 193, 256, 512, 1].</p>\n<p>At first my model runs with normal speed. Before this layer, there is several tf.nn.conv3d and tf.nn.conv3d_transpose computation.<br>\nAfter this step's computation, it seems the computation become really slow down.</p>\n<p>But my model runs smoothly on 12G GPU, if using 4 GPUs, 1.8 examples/sec.</p>\n<p>I notice a similar issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"163229319\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/3128\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/3128/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/3128\">#3128</a>. That issue also reports some problem on CPU, but not on GPU. Is that issue resolved?</p>\n<h3>Source code / logs</h3>\n<p>Since I am training a large model on a large dataset. I will only include the model's code. If is needed, I will include more codes.</p>\n<p>The computationally cost layer is the last layer in the variable_scope learning_regularization, right before the scope soft_argmin in <code>def _build_model</code>.</p>\n<p>For more details, I am reimplementing <a href=\"https://arxiv.org/pdf/1703.04309.pdf\" rel=\"nofollow\">https://arxiv.org/pdf/1703.04309.pdf</a>.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> collections <span class=\"pl-k\">import</span> namedtuple\n\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> six\n\n<span class=\"pl-k\">from</span> tensorflow.python.training <span class=\"pl-k\">import</span> moving_averages\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If a model is trained using multiple GPUs, prefix all Op names with tower_name</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> to differentiate the operations. Note that this prefix is removed from the</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> names of the summaries when visualizing a model.</span>\n<span class=\"pl-c1\">TOWER_NAME</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tower<span class=\"pl-pds\">'</span></span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Batch normalization. Constant governing the exponential moving average of</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> the 'global' mean and variance for all activations.</span>\n<span class=\"pl-c1\">BATCHNORM_MOVING_AVERAGE_DECAY</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.9997</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> The decay to use for the moving average.</span>\n<span class=\"pl-c1\">MOVING_AVERAGE_DECAY</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0.9999</span>\n\n\nHParams <span class=\"pl-k\">=</span> namedtuple(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>HParams<span class=\"pl-pds\">'</span></span>,\n                     [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>batch_size<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>lrn_rate<span class=\"pl-pds\">'</span></span>,\n                     <span class=\"pl-s\"><span class=\"pl-pds\">'</span>weight_decay_rate<span class=\"pl-pds\">'</span></span>,\n                     <span class=\"pl-s\"><span class=\"pl-pds\">'</span>relu_leakiness<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>optimizer<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>max_disparity<span class=\"pl-pds\">'</span></span>])\n\n\n<span class=\"pl-k\">class</span> <span class=\"pl-en\">GCNet</span>(<span class=\"pl-c1\">object</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>GCNet model.<span class=\"pl-pds\">\"\"\"</span></span>\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">hps</span>, <span class=\"pl-smi\">left_images</span>, <span class=\"pl-smi\">right_images</span>, <span class=\"pl-smi\">gt_disparity</span>, <span class=\"pl-smi\">mask</span>, <span class=\"pl-smi\">mode</span>): \n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>ResNet constructor.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">      hps: Hyperparameters.</span>\n<span class=\"pl-s\">      images: Batches of images. [batch_size, image_size, image_size, 3]</span>\n<span class=\"pl-s\">      labels: Batches of labels. [batch_size, num_classes]</span>\n<span class=\"pl-s\">      mode: One of 'train', 'eval' and 'predict'.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">self</span>.hps <span class=\"pl-k\">=</span> hps\n    <span class=\"pl-c1\">self</span>._left_images <span class=\"pl-k\">=</span> left_images\n    <span class=\"pl-c1\">self</span>._right_images <span class=\"pl-k\">=</span> right_images\n    <span class=\"pl-c1\">self</span>.gt_disparity <span class=\"pl-k\">=</span> gt_disparity\n    <span class=\"pl-c1\">self</span>.mask <span class=\"pl-k\">=</span> mask\n    <span class=\"pl-c1\">self</span>.mode <span class=\"pl-k\">=</span> mode\n    <span class=\"pl-c1\">self</span>.debug_op_list <span class=\"pl-k\">=</span> []  \n\n    <span class=\"pl-c1\">self</span>._extra_train_ops <span class=\"pl-k\">=</span> []\n    \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">build_graph_to_loss</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-c1\">self</span>._build_model()\n    <span class=\"pl-c1\">self</span>._build_loss_op()\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_stride_arr</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">stride</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Map a stride scalar to the stride array for tf.nn.conv2d.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> [<span class=\"pl-c1\">1</span>, stride, stride, <span class=\"pl-c1\">1</span>]\n    \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_stride_3d_arr</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">stride</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Map a stride scalar to the stride array for tf.nn.conv2d.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> [<span class=\"pl-c1\">1</span>, stride, stride, stride, <span class=\"pl-c1\">1</span>]\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_build_model</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Build the core model within the graph.<span class=\"pl-pds\">\"\"\"</span></span>\n\n    layer_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>unary_features<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>):\n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n        layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        left_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._left_images\n        left_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-pds\">'</span></span>, left_x, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">self</span>._stride_arr(<span class=\"pl-c1\">2</span>))\n        left_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(left_x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n        left_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, left_x)\n      tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(left_x))\n        \n      <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">8</span>):\n        left_x, layer_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._unary_feat_residual(left_x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">self</span>._stride_arr(<span class=\"pl-c1\">1</span>), layer_idx)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(left_x))\n    \n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n        layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        left_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-pds\">'</span></span>, left_x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">self</span>._stride_arr(<span class=\"pl-c1\">1</span>))\n      tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(left_x))\n    \n    layer_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>    \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>unary_features<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">reuse</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n        layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        right_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._left_images\n        right_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-pds\">'</span></span>, right_x, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">self</span>._stride_arr(<span class=\"pl-c1\">2</span>))\n        right_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(right_x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n        right_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, right_x)\n        \n      <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">8</span>):\n        right_x, layer_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._unary_feat_residual(right_x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">self</span>._stride_arr(<span class=\"pl-c1\">1</span>), layer_idx)\n\n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n        layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        right_x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-pds\">'</span></span>, right_x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">self</span>._stride_arr(<span class=\"pl-c1\">1</span>))\n      \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>cost_volumn<span class=\"pl-pds\">'</span></span>):\n      left_cost_volume <span class=\"pl-k\">=</span> tf.stack([tf.identity(left_x)] <span class=\"pl-k\">*</span> (<span class=\"pl-c1\">self</span>.hps.max_disparity<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>), <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>left_stack<span class=\"pl-pds\">'</span></span>)\n      right_cost_volume <span class=\"pl-k\">=</span> []\n      cur_width <span class=\"pl-k\">=</span> tf.shape(right_x)[<span class=\"pl-c1\">2</span>]\n\n      <span class=\"pl-k\">for</span> depth <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">self</span>.hps.max_disparity<span class=\"pl-k\">/</span><span class=\"pl-c1\">2</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>):\n        right_cost_volume.append(tf.pad(tf.slice(right_x, [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, cur_width <span class=\"pl-k\">-</span> depth, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>right_slice_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(depth)),\n                                        [[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>], [depth, <span class=\"pl-c1\">0</span>], [<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>]],\n                                        <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>right_pad_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(depth)\n                                        ))\n      right_cost_volume <span class=\"pl-k\">=</span> tf.stack(right_cost_volume, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>right_stack<span class=\"pl-pds\">'</span></span>)\n      x <span class=\"pl-k\">=</span> tf.concat([left_cost_volume, right_cost_volume], <span class=\"pl-c1\">4</span>)\n      tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n      \n          \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_regularization<span class=\"pl-pds\">'</span></span>):\n      stored_features <span class=\"pl-k\">=</span> []\n\n      in_filters <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>]\n      out_filters <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>]\n      in_filters_stride_2 <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>]\n      out_filters_stride_2 <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">128</span>]\n      <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">4</span>):\n        tmp_x, layer_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._regularization_subsample(x, <span class=\"pl-c1\">3</span>, in_filters[i], out_filters[i], <span class=\"pl-c1\">self</span>._stride_3d_arr(<span class=\"pl-c1\">1</span>), layer_idx)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(tmp_x))\n        stored_features.append(tmp_x)\n        \n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n          layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n          x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv3d(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv3d<span class=\"pl-pds\">'</span></span>, x, <span class=\"pl-c1\">3</span>, in_filters_stride_2[i], out_filters_stride_2[i], <span class=\"pl-c1\">self</span>._stride_3d_arr(<span class=\"pl-c1\">2</span>))\n          x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n          x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, x)\n          tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n\n      \n      <span class=\"pl-k\">assert</span> stored_features[<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> stored_features[<span class=\"pl-c1\">1</span>]\n\n      <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">2</span>):\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n          layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n          x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv3d(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv3d<span class=\"pl-pds\">'</span></span>, x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">self</span>._stride_3d_arr(<span class=\"pl-c1\">1</span>))\n          x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n          x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, x)\n          tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n\n      transposed_in_filters <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>]\n      transposed_out_filters <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">64</span>, <span class=\"pl-c1\">32</span>]\n      \n      <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">4</span>):\n        x, layer_idx <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._regularization_upsample(x, stored_features[<span class=\"pl-k\">-</span>i<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], <span class=\"pl-c1\">3</span>, transposed_in_filters[i], transposed_out_filters[i], <span class=\"pl-c1\">self</span>._stride_3d_arr(<span class=\"pl-c1\">2</span>), layer_idx)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n      \n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n        layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        input_shape <span class=\"pl-k\">=</span> tf.shape(<span class=\"pl-c1\">self</span>.gt_disparity)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv3d_trans(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv_trans<span class=\"pl-pds\">'</span></span>, x, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">32</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">self</span>._stride_3d_arr(<span class=\"pl-c1\">2</span>), [input_shape[<span class=\"pl-c1\">0</span>], <span class=\"pl-c1\">self</span>.hps.max_disparity<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, input_shape[<span class=\"pl-c1\">1</span>], input_shape[<span class=\"pl-c1\">2</span>], <span class=\"pl-c1\">1</span>])\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n        <span class=\"pl-c1\">self</span>.debug_op_list.append(tf.shape(x))\n\n    \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>soft_argmin<span class=\"pl-pds\">'</span></span>):\n        x <span class=\"pl-k\">=</span> tf.squeeze(x, <span class=\"pl-v\">squeeze_dims</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">4</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>squeeze<span class=\"pl-pds\">'</span></span>)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n        x <span class=\"pl-k\">=</span> tf.transpose(x, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>transpose<span class=\"pl-pds\">'</span></span>)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n        x <span class=\"pl-k\">=</span> tf.nn.softmax(x, <span class=\"pl-v\">dim</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>softmax<span class=\"pl-pds\">'</span></span>)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n\n        multiplier <span class=\"pl-k\">=</span> tf.range(<span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">self</span>.hps.max_disparity<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>depth_range<span class=\"pl-pds\">'</span></span>)\n        x <span class=\"pl-k\">=</span> tf.multiply(x, multiplier, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>softmax_mul_depth<span class=\"pl-pds\">'</span></span>)\n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(x))\n        <span class=\"pl-c1\">self</span>.predicted_disparity <span class=\"pl-k\">=</span> tf.reduce_sum(x, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>reduce_sum<span class=\"pl-pds\">'</span></span>)       \n        tf.add_to_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>, tf.shape(<span class=\"pl-c1\">self</span>.predicted_disparity))\n    <span class=\"pl-c1\">self</span>.shapes <span class=\"pl-k\">=</span> tf.get_collection(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>shapes<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">self</span>.debug_op_list.append(<span class=\"pl-c1\">self</span>.shapes)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_build_loss_op</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>):\n      <span class=\"pl-c1\">self</span>.abs_loss <span class=\"pl-k\">=</span> tf.reduce_mean(tf.abs((<span class=\"pl-c1\">self</span>.gt_disparity <span class=\"pl-k\">-</span> <span class=\"pl-c1\">self</span>.predicted_disparity) <span class=\"pl-k\">*</span> <span class=\"pl-c1\">self</span>.mask), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>abs_loss<span class=\"pl-pds\">'</span></span>)\n      <span class=\"pl-c1\">self</span>.total_loss <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.abs_loss <span class=\"pl-k\">+</span> <span class=\"pl-c1\">self</span>._decay()\n\n      \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_add_loss_summaries</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Add summaries for losses in CIFAR-10 model.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Generates moving average for all losses and associated summaries for</span>\n<span class=\"pl-s\">    visualizing the performance of the network.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">      total_loss: Total loss from loss().</span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">      loss_averages_op: op for generating moving averages of losses.</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Compute the moving average of all individual losses and the total loss.</span>\n    loss_averages <span class=\"pl-k\">=</span> tf.train.ExponentialMovingAverage(<span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss_avg<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">self</span>.loss_averages_op <span class=\"pl-k\">=</span> loss_averages.apply([<span class=\"pl-c1\">self</span>.abs_loss, <span class=\"pl-c1\">self</span>.total_loss])\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Attach a scalar summary to all individual losses and the total loss; do the</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> same for the averaged version of the losses.</span>\n    <span class=\"pl-k\">for</span> l <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">self</span>.abs_loss, <span class=\"pl-c1\">self</span>.total_loss]:\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> Name each loss as '(raw)' and name the moving average version of the loss</span>\n      <span class=\"pl-c\"><span class=\"pl-c\">#</span> as the original loss name.</span>\n      tf.summary.scalar(l.op.name <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span> (raw)<span class=\"pl-pds\">'</span></span>, l)\n      tf.summary.scalar(l.op.name, loss_averages.average(l))\n    \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_build_train_op</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">global_step</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Build training specific ops for the graph.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-c1\">self</span>.lrn_rate <span class=\"pl-k\">=</span> tf.constant(<span class=\"pl-c1\">self</span>.hps.lrn_rate, tf.float32)\n    tf.summary.scalar(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_rate<span class=\"pl-pds\">'</span></span>, <span class=\"pl-c1\">self</span>.lrn_rate)\n\n    loss_averages_op <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._add_loss_summaries()\n    \n    <span class=\"pl-k\">with</span> tf.control_dependencies([loss_averages_op]):\n      <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.hps.optimizer <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>sgd<span class=\"pl-pds\">'</span></span>:\n        optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-c1\">self</span>.lrn_rate)\n      <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">self</span>.hps.optimizer <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>mom<span class=\"pl-pds\">'</span></span>:\n        optimizer <span class=\"pl-k\">=</span> tf.train.MomentumOptimizer(<span class=\"pl-c1\">self</span>.lrn_rate, <span class=\"pl-c1\">0.9</span>)\n      <span class=\"pl-k\">elif</span> <span class=\"pl-c1\">self</span>.hps.optimizer <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>RMSProp<span class=\"pl-pds\">'</span></span>:\n        optimizer <span class=\"pl-k\">=</span> tf.train.RMSPropOptimizer(<span class=\"pl-c1\">self</span>.lrn_rate, <span class=\"pl-v\">decay</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">momentum</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.9</span>, <span class=\"pl-v\">epsilon</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        \n        trainable_variables <span class=\"pl-k\">=</span> tf.trainable_variables()\n        grads <span class=\"pl-k\">=</span> optimizer.compute_gradients(<span class=\"pl-c1\">self</span>.total_loss, trainable_variables)\n\n\n    apply_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(\n        grads,\n        <span class=\"pl-v\">global_step</span><span class=\"pl-k\">=</span>global_step, \n        <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>train_step<span class=\"pl-pds\">'</span></span>)\n        \n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Track the moving averages of all trainable variables.</span>\n    variable_averages <span class=\"pl-k\">=</span> tf.train.ExponentialMovingAverage(\n        <span class=\"pl-c1\">MOVING_AVERAGE_DECAY</span>, global_step)\n    variables_averages_op <span class=\"pl-k\">=</span> variable_averages.apply(tf.trainable_variables())\n\n    <span class=\"pl-k\">with</span> tf.control_dependencies([apply_op, variables_averages_op]):\n      <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> tf.no_op(<span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_regularization_upsample</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">feature</span>, <span class=\"pl-smi\">filter_size</span>, <span class=\"pl-smi\">in_filter</span>, <span class=\"pl-smi\">out_filter</span>, <span class=\"pl-smi\">stride</span>, <span class=\"pl-smi\">layer_idx</span>):\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n      layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv3d_trans(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv_trans<span class=\"pl-pds\">'</span></span>, x, filter_size, in_filter, out_filter, stride, tf.shape(feature))\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, x)\n      \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>residual_after_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)):\n      x <span class=\"pl-k\">+=</span> feature\n\n    tf.logging.debug(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image after unit <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span>, x.get_shape())\n    <span class=\"pl-k\">return</span> x, layer_idx\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_regularization_subsample</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">filter_size</span>, <span class=\"pl-smi\">in_filter</span>, <span class=\"pl-smi\">out_filter</span>, <span class=\"pl-smi\">stride</span>, <span class=\"pl-smi\">layer_idx</span>):\n\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n      layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv3d(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv3d<span class=\"pl-pds\">'</span></span>, x, filter_size, in_filter, out_filter, stride)\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, x)\n\n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n      layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv3d(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv3d<span class=\"pl-pds\">'</span></span>, x, filter_size, out_filter, out_filter, stride)\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n      x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, x)\n      \n    tf.logging.debug(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image after unit <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span>, x.get_shape())\n    <span class=\"pl-k\">return</span> x, layer_idx\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_unary_feat_residual</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">filter_size</span>, <span class=\"pl-smi\">in_filter</span>, <span class=\"pl-smi\">out_filter</span>, <span class=\"pl-smi\">stride</span>, <span class=\"pl-smi\">layer_idx</span>):\n    orig_x <span class=\"pl-k\">=</span> x\n    orig_layer_idx <span class=\"pl-k\">=</span> layer_idx <span class=\"pl-k\">-</span> <span class=\"pl-c1\">1</span>\n    \n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> six.moves.range(<span class=\"pl-c1\">2</span>):\n      <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>layer_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx)):\n        layer_idx <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._conv(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>conv<span class=\"pl-pds\">'</span></span>, x, <span class=\"pl-c1\">3</span>, in_filter, out_filter, stride)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._relu(x, <span class=\"pl-c1\">self</span>.hps.relu_leakiness)\n        x <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._batch_norm(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>bn<span class=\"pl-pds\">'</span></span>, x)\n          \n    <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>residual_btw_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(layer_idx<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>_<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(orig_layer_idx)):\n      x <span class=\"pl-k\">+=</span> orig_x\n\n    tf.logging.debug(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>image after unit <span class=\"pl-c1\">%s</span><span class=\"pl-pds\">'</span></span>, x.get_shape())\n    <span class=\"pl-k\">return</span> x, layer_idx\n\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_decay</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>L2 weight decay loss.<span class=\"pl-pds\">\"\"\"</span></span>\n    costs <span class=\"pl-k\">=</span> []\n    <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> tf.trainable_variables():\n      <span class=\"pl-k\">if</span> var.op.name.find(<span class=\"pl-sr\"><span class=\"pl-k\">r</span><span class=\"pl-pds\">'</span>DW<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n        costs.append(tf.nn.l2_loss(var))\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.summary.histogram(var.op.name, var)</span>\n\n    <span class=\"pl-k\">return</span> tf.multiply(<span class=\"pl-c1\">self</span>.hps.weight_decay_rate, tf.add_n(costs))\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_conv</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">filter_size</span>, <span class=\"pl-smi\">in_filters</span>, <span class=\"pl-smi\">out_filters</span>, <span class=\"pl-smi\">strides</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Convolution.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(name):\n      n <span class=\"pl-k\">=</span> filter_size <span class=\"pl-k\">*</span> filter_size <span class=\"pl-k\">*</span> out_filters\n      kernel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>DW<span class=\"pl-pds\">'</span></span>, [filter_size, filter_size, in_filters, out_filters],\n          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(\n              <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span>np.sqrt(<span class=\"pl-c1\">2.0</span><span class=\"pl-k\">/</span>n)))\n      <span class=\"pl-k\">return</span> tf.nn.conv2d(x, kernel, strides, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n      \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_conv3d</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">filter_size</span>, <span class=\"pl-smi\">in_filters</span>, <span class=\"pl-smi\">out_filters</span>, <span class=\"pl-smi\">strides</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Convolution.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(name):\n      n <span class=\"pl-k\">=</span> filter_size <span class=\"pl-k\">*</span> filter_size <span class=\"pl-k\">*</span> filter_size <span class=\"pl-k\">*</span> out_filters\n      kernel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>DW<span class=\"pl-pds\">'</span></span>, [filter_size, filter_size, filter_size, in_filters, out_filters],\n           <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(\n              <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span>np.sqrt(<span class=\"pl-c1\">2.0</span><span class=\"pl-k\">/</span>n)))\n      <span class=\"pl-k\">return</span> tf.nn.conv3d(x, kernel, strides, <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n      \n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_conv3d_trans</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">filter_size</span>, <span class=\"pl-smi\">in_filters</span>, <span class=\"pl-smi\">out_filters</span>, <span class=\"pl-smi\">strides</span>, <span class=\"pl-smi\">output_shape</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Convolution.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(name):\n      n <span class=\"pl-k\">=</span> filter_size <span class=\"pl-k\">*</span> filter_size <span class=\"pl-k\">*</span> filter_size <span class=\"pl-k\">*</span> out_filters\n      kernel <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>DW<span class=\"pl-pds\">'</span></span>, [filter_size, filter_size, filter_size, out_filters, in_filters],\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal_initializer(\n              <span class=\"pl-v\">stddev</span><span class=\"pl-k\">=</span>np.sqrt(<span class=\"pl-c1\">2.0</span><span class=\"pl-k\">/</span>n)))\n      x_shape <span class=\"pl-k\">=</span> tf.shape(x)\n      <span class=\"pl-c1\">self</span>.debug_op_list.append(tf.shape(kernel))\n      <span class=\"pl-k\">return</span> tf.nn.conv3d_transpose(\n                x, \n                kernel, \n                output_shape,\n                strides, \n                <span class=\"pl-v\">padding</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>SAME<span class=\"pl-pds\">'</span></span>)\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_relu</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">x</span>, <span class=\"pl-smi\">leakiness</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.0</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Relu, with optional leaky support.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">return</span> tf.where(tf.less(x, <span class=\"pl-c1\">0.0</span>), leakiness <span class=\"pl-k\">*</span> x, x, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>leaky_relu<span class=\"pl-pds\">'</span></span>)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py</span>\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_batch_norm</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">x</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Batch normalization.<span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.variable_scope(name):\n      params_shape <span class=\"pl-k\">=</span> [x.get_shape()[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]]\n\n      beta <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>beta<span class=\"pl-pds\">'</span></span>, params_shape,\n          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>, tf.float32))\n      gamma <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n          <span class=\"pl-s\"><span class=\"pl-pds\">'</span>gamma<span class=\"pl-pds\">'</span></span>, params_shape,\n          <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">1.0</span>, tf.float32))\n\n      <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.mode <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>train<span class=\"pl-pds\">'</span></span>:\n        mean, variance <span class=\"pl-k\">=</span> tf.nn.moments(x, <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(x.get_shape())<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>moments<span class=\"pl-pds\">'</span></span>)\n\n        moving_mean <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>moving_mean<span class=\"pl-pds\">'</span></span>, params_shape,\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>, tf.float32),\n            <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        moving_variance <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>moving_variance<span class=\"pl-pds\">'</span></span>, params_shape,\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">1.0</span>, tf.float32),\n            <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n        <span class=\"pl-c1\">self</span>._extra_train_ops.append(moving_averages.assign_moving_average(\n            moving_mean, mean, <span class=\"pl-c1\">BATCHNORM_MOVING_AVERAGE_DECAY</span>))\n        <span class=\"pl-c1\">self</span>._extra_train_ops.append(moving_averages.assign_moving_average(\n            moving_variance, variance, <span class=\"pl-c1\">BATCHNORM_MOVING_AVERAGE_DECAY</span>))\n      <span class=\"pl-k\">else</span>:\n        mean <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>moving_mean<span class=\"pl-pds\">'</span></span>, params_shape,\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">0.0</span>, tf.float32),\n            <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n        variance <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._variable_on_cpu(\n            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>moving_variance<span class=\"pl-pds\">'</span></span>, params_shape,\n            <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.constant_initializer(<span class=\"pl-c1\">1.0</span>, tf.float32),\n            <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n      y <span class=\"pl-k\">=</span> tf.nn.batch_normalization(\n          x, mean, variance, beta, gamma, <span class=\"pl-c1\">0.001</span>)\n      y.set_shape(x.get_shape())\n      <span class=\"pl-k\">return</span> y\n\n  <span class=\"pl-k\">def</span> <span class=\"pl-en\">_variable_on_cpu</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">name</span>, <span class=\"pl-smi\">shape</span>, <span class=\"pl-smi\">initializer</span>, <span class=\"pl-smi\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-smi\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Helper to create a Variable stored on CPU memory.</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Args:</span>\n<span class=\"pl-s\">      name: name of the variable</span>\n<span class=\"pl-s\">      shape: list of ints</span>\n<span class=\"pl-s\">      initializer: initializer for Variable</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">    Returns:</span>\n<span class=\"pl-s\">      Variable Tensor</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">\"\"\"</span></span>\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu:0<span class=\"pl-pds\">'</span></span>):\n      var <span class=\"pl-k\">=</span> tf.get_variable(name, shape, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>initializer, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span>trainable)\n    <span class=\"pl-k\">return</span> var</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): virtualenv pip\nTensorFlow version (use command below): 1.1.0\nBazel version (if compiling from source):\nCUDA/cuDNN version: 8.0\nGPU model and memory: on CPU\nExact command to reproduce:\n\nDescribe the problem\nIt takes about 10 mins to do the tf.nn.conv3d_transpose computation.\nThe input size is [1, 97, 128, 256, 32], kernel size is 333, strides is [1, 2, 2, 2, 1], and the output size is [1, 193, 256, 512, 1].\nAt first my model runs with normal speed. Before this layer, there is several tf.nn.conv3d and tf.nn.conv3d_transpose computation.\nAfter this step's computation, it seems the computation become really slow down.\nBut my model runs smoothly on 12G GPU, if using 4 GPUs, 1.8 examples/sec.\nI notice a similar issue #3128. That issue also reports some problem on CPU, but not on GPU. Is that issue resolved?\nSource code / logs\nSince I am training a large model on a large dataset. I will only include the model's code. If is needed, I will include more codes.\nThe computationally cost layer is the last layer in the variable_scope learning_regularization, right before the scope soft_argmin in def _build_model.\nFor more details, I am reimplementing https://arxiv.org/pdf/1703.04309.pdf.\nfrom collections import namedtuple\n\nimport numpy as np\nimport tensorflow as tf\nimport six\n\nfrom tensorflow.python.training import moving_averages\n\n# If a model is trained using multiple GPUs, prefix all Op names with tower_name\n# to differentiate the operations. Note that this prefix is removed from the\n# names of the summaries when visualizing a model.\nTOWER_NAME = 'tower'\n\n# Batch normalization. Constant governing the exponential moving average of\n# the 'global' mean and variance for all activations.\nBATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\n\n# The decay to use for the moving average.\nMOVING_AVERAGE_DECAY = 0.9999\n\n\nHParams = namedtuple('HParams',\n                     ['batch_size', 'lrn_rate',\n                     'weight_decay_rate',\n                     'relu_leakiness', 'optimizer', 'max_disparity'])\n\n\nclass GCNet(object):\n  \"\"\"GCNet model.\"\"\"\n\n  def __init__(self, hps, left_images, right_images, gt_disparity, mask, mode): \n    \"\"\"ResNet constructor.\n\n    Args:\n      hps: Hyperparameters.\n      images: Batches of images. [batch_size, image_size, image_size, 3]\n      labels: Batches of labels. [batch_size, num_classes]\n      mode: One of 'train', 'eval' and 'predict'.\n    \"\"\"\n    self.hps = hps\n    self._left_images = left_images\n    self._right_images = right_images\n    self.gt_disparity = gt_disparity\n    self.mask = mask\n    self.mode = mode\n    self.debug_op_list = []  \n\n    self._extra_train_ops = []\n    \n  def build_graph_to_loss(self):\n    self._build_model()\n    self._build_loss_op()\n\n  def _stride_arr(self, stride):\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\n    return [1, stride, stride, 1]\n    \n  def _stride_3d_arr(self, stride):\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\n    return [1, stride, stride, stride, 1]\n\n  def _build_model(self):\n    \"\"\"Build the core model within the graph.\"\"\"\n\n    layer_idx = 1\n    with tf.variable_scope('unary_features', reuse=False):\n      with tf.variable_scope('layer_'+str(layer_idx)):\n        layer_idx += 1\n        left_x = self._left_images\n        left_x = self._conv('conv', left_x, 5, 3, 32, self._stride_arr(2))\n        left_x = self._relu(left_x, self.hps.relu_leakiness)\n        left_x = self._batch_norm('bn', left_x)\n      tf.add_to_collection('shapes', tf.shape(left_x))\n        \n      for i in six.moves.range(8):\n        left_x, layer_idx = self._unary_feat_residual(left_x, 3, 32, 32, self._stride_arr(1), layer_idx)\n        tf.add_to_collection('shapes', tf.shape(left_x))\n    \n      with tf.variable_scope('layer_'+str(layer_idx)):\n        layer_idx += 1\n        left_x = self._conv('conv', left_x, 3, 32, 32, self._stride_arr(1))\n      tf.add_to_collection('shapes', tf.shape(left_x))\n    \n    layer_idx = 1    \n    with tf.variable_scope('unary_features', reuse=True):\n      with tf.variable_scope('layer_'+str(layer_idx)):\n        layer_idx += 1\n        right_x = self._left_images\n        right_x = self._conv('conv', right_x, 5, 3, 32, self._stride_arr(2))\n        right_x = self._relu(right_x, self.hps.relu_leakiness)\n        right_x = self._batch_norm('bn', right_x)\n        \n      for i in six.moves.range(8):\n        right_x, layer_idx = self._unary_feat_residual(right_x, 3, 32, 32, self._stride_arr(1), layer_idx)\n\n      with tf.variable_scope('layer_'+str(layer_idx)):\n        layer_idx += 1\n        right_x = self._conv('conv', right_x, 3, 32, 32, self._stride_arr(1))\n      \n    with tf.variable_scope('cost_volumn'):\n      left_cost_volume = tf.stack([tf.identity(left_x)] * (self.hps.max_disparity/2+1), axis=1, name='left_stack')\n      right_cost_volume = []\n      cur_width = tf.shape(right_x)[2]\n\n      for depth in six.moves.range(self.hps.max_disparity/2+1):\n        right_cost_volume.append(tf.pad(tf.slice(right_x, [0, 0, 0, 0], [-1, -1, cur_width - depth, -1], name='right_slice_'+str(depth)),\n                                        [[0, 0], [0, 0], [depth, 0], [0, 0]],\n                                        name='right_pad_'+str(depth)\n                                        ))\n      right_cost_volume = tf.stack(right_cost_volume, axis=1, name='right_stack')\n      x = tf.concat([left_cost_volume, right_cost_volume], 4)\n      tf.add_to_collection('shapes', tf.shape(x))\n      \n          \n    with tf.variable_scope('learning_regularization'):\n      stored_features = []\n\n      in_filters = [64, 64, 64, 64]\n      out_filters = [32, 64, 64, 64]\n      in_filters_stride_2 = [64, 64, 64, 64]\n      out_filters_stride_2 = [64, 64, 64, 128]\n      for i in six.moves.range(4):\n        tmp_x, layer_idx = self._regularization_subsample(x, 3, in_filters[i], out_filters[i], self._stride_3d_arr(1), layer_idx)\n        tf.add_to_collection('shapes', tf.shape(tmp_x))\n        stored_features.append(tmp_x)\n        \n        with tf.variable_scope('layer_'+str(layer_idx)):\n          layer_idx += 1\n          x = self._conv3d('conv3d', x, 3, in_filters_stride_2[i], out_filters_stride_2[i], self._stride_3d_arr(2))\n          x = self._relu(x, self.hps.relu_leakiness)\n          x = self._batch_norm('bn', x)\n          tf.add_to_collection('shapes', tf.shape(x))\n\n      \n      assert stored_features[0] is not stored_features[1]\n\n      for i in six.moves.range(2):\n        with tf.variable_scope('layer_'+str(layer_idx)):\n          layer_idx += 1\n          x = self._conv3d('conv3d', x, 3, 128, 128, self._stride_3d_arr(1))\n          x = self._relu(x, self.hps.relu_leakiness)\n          x = self._batch_norm('bn', x)\n          tf.add_to_collection('shapes', tf.shape(x))\n\n      transposed_in_filters = [128, 64, 64, 64]\n      transposed_out_filters = [64, 64, 64, 32]\n      \n      for i in six.moves.range(4):\n        x, layer_idx = self._regularization_upsample(x, stored_features[-i-1], 3, transposed_in_filters[i], transposed_out_filters[i], self._stride_3d_arr(2), layer_idx)\n        tf.add_to_collection('shapes', tf.shape(x))\n      \n      with tf.variable_scope('layer_'+str(layer_idx)):\n        layer_idx += 1\n        input_shape = tf.shape(self.gt_disparity)\n        x = self._conv3d_trans('conv_trans', x, 3, 32, 1, self._stride_3d_arr(2), [input_shape[0], self.hps.max_disparity+1, input_shape[1], input_shape[2], 1])\n        tf.add_to_collection('shapes', tf.shape(x))\n        self.debug_op_list.append(tf.shape(x))\n\n    \n    with tf.variable_scope('soft_argmin'):\n        x = tf.squeeze(x, squeeze_dims=[4], name='squeeze')\n        tf.add_to_collection('shapes', tf.shape(x))\n        x = tf.transpose(x, perm=[0, 2, 3, 1], name='transpose')\n        tf.add_to_collection('shapes', tf.shape(x))\n        x = tf.nn.softmax(x, dim=-1, name='softmax')\n        tf.add_to_collection('shapes', tf.shape(x))\n\n        multiplier = tf.range(0, self.hps.max_disparity+1, dtype=tf.float32, name='depth_range')\n        x = tf.multiply(x, multiplier, name='softmax_mul_depth')\n        tf.add_to_collection('shapes', tf.shape(x))\n        self.predicted_disparity = tf.reduce_sum(x, axis=3, name='reduce_sum')       \n        tf.add_to_collection('shapes', tf.shape(self.predicted_disparity))\n    self.shapes = tf.get_collection('shapes')\n    self.debug_op_list.append(self.shapes)\n\n  def _build_loss_op(self):\n    with tf.variable_scope('loss'):\n      self.abs_loss = tf.reduce_mean(tf.abs((self.gt_disparity - self.predicted_disparity) * self.mask), name='abs_loss')\n      self.total_loss = self.abs_loss + self._decay()\n\n      \n  def _add_loss_summaries(self):\n    \"\"\"Add summaries for losses in CIFAR-10 model.\n\n    Generates moving average for all losses and associated summaries for\n    visualizing the performance of the network.\n\n    Args:\n      total_loss: Total loss from loss().\n    Returns:\n      loss_averages_op: op for generating moving averages of losses.\n    \"\"\"\n    # Compute the moving average of all individual losses and the total loss.\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='loss_avg')\n    self.loss_averages_op = loss_averages.apply([self.abs_loss, self.total_loss])\n\n    # Attach a scalar summary to all individual losses and the total loss; do the\n    # same for the averaged version of the losses.\n    for l in [self.abs_loss, self.total_loss]:\n      # Name each loss as '(raw)' and name the moving average version of the loss\n      # as the original loss name.\n      tf.summary.scalar(l.op.name + ' (raw)', l)\n      tf.summary.scalar(l.op.name, loss_averages.average(l))\n    \n  def _build_train_op(self, global_step):\n    \"\"\"Build training specific ops for the graph.\"\"\"\n    self.lrn_rate = tf.constant(self.hps.lrn_rate, tf.float32)\n    tf.summary.scalar('learning_rate', self.lrn_rate)\n\n    loss_averages_op = self._add_loss_summaries()\n    \n    with tf.control_dependencies([loss_averages_op]):\n      if self.hps.optimizer == 'sgd':\n        optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\n      elif self.hps.optimizer == 'mom':\n        optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\n      elif self.hps.optimizer == 'RMSProp':\n        optimizer = tf.train.RMSPropOptimizer(self.lrn_rate, decay=0.9, momentum=0.9, epsilon=1)\n        \n        trainable_variables = tf.trainable_variables()\n        grads = optimizer.compute_gradients(self.total_loss, trainable_variables)\n\n\n    apply_op = optimizer.apply_gradients(\n        grads,\n        global_step=global_step, \n        name='train_step')\n        \n    # Track the moving averages of all trainable variables.\n    variable_averages = tf.train.ExponentialMovingAverage(\n        MOVING_AVERAGE_DECAY, global_step)\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n\n    with tf.control_dependencies([apply_op, variables_averages_op]):\n      self.train_op = tf.no_op(name='train')\n\n  def _regularization_upsample(self, x, feature, filter_size, in_filter, out_filter, stride, layer_idx):\n    with tf.variable_scope('layer_'+str(layer_idx)):\n      layer_idx += 1\n      x = self._conv3d_trans('conv_trans', x, filter_size, in_filter, out_filter, stride, tf.shape(feature))\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._batch_norm('bn', x)\n      \n    with tf.variable_scope('residual_after_'+str(layer_idx-1)):\n      x += feature\n\n    tf.logging.debug('image after unit %s', x.get_shape())\n    return x, layer_idx\n\n  def _regularization_subsample(self, x, filter_size, in_filter, out_filter, stride, layer_idx):\n\n    with tf.variable_scope('layer_'+str(layer_idx)):\n      layer_idx += 1\n      x = self._conv3d('conv3d', x, filter_size, in_filter, out_filter, stride)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._batch_norm('bn', x)\n\n    with tf.variable_scope('layer_'+str(layer_idx)):\n      layer_idx += 1\n      x = self._conv3d('conv3d', x, filter_size, out_filter, out_filter, stride)\n      x = self._relu(x, self.hps.relu_leakiness)\n      x = self._batch_norm('bn', x)\n      \n    tf.logging.debug('image after unit %s', x.get_shape())\n    return x, layer_idx\n\n  def _unary_feat_residual(self, x, filter_size, in_filter, out_filter, stride, layer_idx):\n    orig_x = x\n    orig_layer_idx = layer_idx - 1\n    \n    for i in six.moves.range(2):\n      with tf.variable_scope('layer_'+str(layer_idx)):\n        layer_idx += 1\n        x = self._conv('conv', x, 3, in_filter, out_filter, stride)\n        x = self._relu(x, self.hps.relu_leakiness)\n        x = self._batch_norm('bn', x)\n          \n    with tf.variable_scope('residual_btw_'+str(layer_idx-1)+'_'+str(orig_layer_idx)):\n      x += orig_x\n\n    tf.logging.debug('image after unit %s', x.get_shape())\n    return x, layer_idx\n\n\n  def _decay(self):\n    \"\"\"L2 weight decay loss.\"\"\"\n    costs = []\n    for var in tf.trainable_variables():\n      if var.op.name.find(r'DW') > 0:\n        costs.append(tf.nn.l2_loss(var))\n        # tf.summary.histogram(var.op.name, var)\n\n    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\n\n  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\n    \"\"\"Convolution.\"\"\"\n    with tf.variable_scope(name):\n      n = filter_size * filter_size * out_filters\n      kernel = self._variable_on_cpu(\n          'DW', [filter_size, filter_size, in_filters, out_filters],\n          initializer=tf.random_normal_initializer(\n              stddev=np.sqrt(2.0/n)))\n      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\n      \n  def _conv3d(self, name, x, filter_size, in_filters, out_filters, strides):\n    \"\"\"Convolution.\"\"\"\n    with tf.variable_scope(name):\n      n = filter_size * filter_size * filter_size * out_filters\n      kernel = self._variable_on_cpu(\n          'DW', [filter_size, filter_size, filter_size, in_filters, out_filters],\n           initializer=tf.random_normal_initializer(\n              stddev=np.sqrt(2.0/n)))\n      return tf.nn.conv3d(x, kernel, strides, padding='SAME')\n      \n  def _conv3d_trans(self, name, x, filter_size, in_filters, out_filters, strides, output_shape):\n    \"\"\"Convolution.\"\"\"\n    with tf.variable_scope(name):\n      n = filter_size * filter_size * filter_size * out_filters\n      kernel = self._variable_on_cpu(\n          'DW', [filter_size, filter_size, filter_size, out_filters, in_filters],\n            initializer=tf.random_normal_initializer(\n              stddev=np.sqrt(2.0/n)))\n      x_shape = tf.shape(x)\n      self.debug_op_list.append(tf.shape(kernel))\n      return tf.nn.conv3d_transpose(\n                x, \n                kernel, \n                output_shape,\n                strides, \n                padding='SAME')\n\n  def _relu(self, x, leakiness=0.0):\n    \"\"\"Relu, with optional leaky support.\"\"\"\n    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\n\n  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\n  def _batch_norm(self, name, x):\n    \"\"\"Batch normalization.\"\"\"\n    with tf.variable_scope(name):\n      params_shape = [x.get_shape()[-1]]\n\n      beta = self._variable_on_cpu(\n          'beta', params_shape,\n          initializer=tf.constant_initializer(0.0, tf.float32))\n      gamma = self._variable_on_cpu(\n          'gamma', params_shape,\n          initializer=tf.constant_initializer(1.0, tf.float32))\n\n      if self.mode == 'train':\n        mean, variance = tf.nn.moments(x, range(len(x.get_shape())-1), name='moments')\n\n        moving_mean = self._variable_on_cpu(\n            'moving_mean', params_shape,\n            initializer=tf.constant_initializer(0.0, tf.float32),\n            trainable=False)\n        moving_variance = self._variable_on_cpu(\n            'moving_variance', params_shape,\n            initializer=tf.constant_initializer(1.0, tf.float32),\n            trainable=False)\n\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\n            moving_mean, mean, BATCHNORM_MOVING_AVERAGE_DECAY))\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\n            moving_variance, variance, BATCHNORM_MOVING_AVERAGE_DECAY))\n      else:\n        mean = self._variable_on_cpu(\n            'moving_mean', params_shape,\n            initializer=tf.constant_initializer(0.0, tf.float32),\n            trainable=False)\n        variance = self._variable_on_cpu(\n            'moving_variance', params_shape,\n            initializer=tf.constant_initializer(1.0, tf.float32),\n            trainable=False)\n      y = tf.nn.batch_normalization(\n          x, mean, variance, beta, gamma, 0.001)\n      y.set_shape(x.get_shape())\n      return y\n\n  def _variable_on_cpu(self, name, shape, initializer, dtype=tf.float32, trainable=True):\n    \"\"\"Helper to create a Variable stored on CPU memory.\n\n    Args:\n      name: name of the variable\n      shape: list of ints\n      initializer: initializer for Variable\n\n    Returns:\n      Variable Tensor\n    \"\"\"\n    with tf.device('/cpu:0'):\n      var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\n    return var", "body": "\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: virtualenv pip\r\n- **TensorFlow version (use command below)**: 1.1.0\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 8.0\r\n- **GPU model and memory**: on CPU\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nIt takes about 10 mins to do the tf.nn.conv3d_transpose computation.\r\nThe input size is [1, 97, 128, 256, 32], kernel size is 3*3*3, strides is [1, 2, 2, 2, 1], and the output size is [1, 193, 256, 512, 1].\r\n\r\nAt first my model runs with normal speed. Before this layer, there is several tf.nn.conv3d and tf.nn.conv3d_transpose computation.\r\nAfter this step's computation, it seems the computation become really slow down.\r\n\r\nBut my model runs smoothly on 12G GPU, if using 4 GPUs, 1.8 examples/sec.\r\n\r\nI notice a similar issue #3128. That issue also reports some problem on CPU, but not on GPU. Is that issue resolved?\r\n\r\n### Source code / logs\r\nSince I am training a large model on a large dataset. I will only include the model's code. If is needed, I will include more codes. \r\n\r\nThe computationally cost layer is the last layer in the variable_scope learning_regularization, right before the scope soft_argmin in `def _build_model`.\r\n\r\nFor more details, I am reimplementing https://arxiv.org/pdf/1703.04309.pdf.\r\n\r\n```python\r\nfrom collections import namedtuple\r\n\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport six\r\n\r\nfrom tensorflow.python.training import moving_averages\r\n\r\n# If a model is trained using multiple GPUs, prefix all Op names with tower_name\r\n# to differentiate the operations. Note that this prefix is removed from the\r\n# names of the summaries when visualizing a model.\r\nTOWER_NAME = 'tower'\r\n\r\n# Batch normalization. Constant governing the exponential moving average of\r\n# the 'global' mean and variance for all activations.\r\nBATCHNORM_MOVING_AVERAGE_DECAY = 0.9997\r\n\r\n# The decay to use for the moving average.\r\nMOVING_AVERAGE_DECAY = 0.9999\r\n\r\n\r\nHParams = namedtuple('HParams',\r\n                     ['batch_size', 'lrn_rate',\r\n                     'weight_decay_rate',\r\n                     'relu_leakiness', 'optimizer', 'max_disparity'])\r\n\r\n\r\nclass GCNet(object):\r\n  \"\"\"GCNet model.\"\"\"\r\n\r\n  def __init__(self, hps, left_images, right_images, gt_disparity, mask, mode): \r\n    \"\"\"ResNet constructor.\r\n\r\n    Args:\r\n      hps: Hyperparameters.\r\n      images: Batches of images. [batch_size, image_size, image_size, 3]\r\n      labels: Batches of labels. [batch_size, num_classes]\r\n      mode: One of 'train', 'eval' and 'predict'.\r\n    \"\"\"\r\n    self.hps = hps\r\n    self._left_images = left_images\r\n    self._right_images = right_images\r\n    self.gt_disparity = gt_disparity\r\n    self.mask = mask\r\n    self.mode = mode\r\n    self.debug_op_list = []  \r\n\r\n    self._extra_train_ops = []\r\n    \r\n  def build_graph_to_loss(self):\r\n    self._build_model()\r\n    self._build_loss_op()\r\n\r\n  def _stride_arr(self, stride):\r\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\r\n    return [1, stride, stride, 1]\r\n    \r\n  def _stride_3d_arr(self, stride):\r\n    \"\"\"Map a stride scalar to the stride array for tf.nn.conv2d.\"\"\"\r\n    return [1, stride, stride, stride, 1]\r\n\r\n  def _build_model(self):\r\n    \"\"\"Build the core model within the graph.\"\"\"\r\n\r\n    layer_idx = 1\r\n    with tf.variable_scope('unary_features', reuse=False):\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        left_x = self._left_images\r\n        left_x = self._conv('conv', left_x, 5, 3, 32, self._stride_arr(2))\r\n        left_x = self._relu(left_x, self.hps.relu_leakiness)\r\n        left_x = self._batch_norm('bn', left_x)\r\n      tf.add_to_collection('shapes', tf.shape(left_x))\r\n        \r\n      for i in six.moves.range(8):\r\n        left_x, layer_idx = self._unary_feat_residual(left_x, 3, 32, 32, self._stride_arr(1), layer_idx)\r\n        tf.add_to_collection('shapes', tf.shape(left_x))\r\n    \r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        left_x = self._conv('conv', left_x, 3, 32, 32, self._stride_arr(1))\r\n      tf.add_to_collection('shapes', tf.shape(left_x))\r\n    \r\n    layer_idx = 1    \r\n    with tf.variable_scope('unary_features', reuse=True):\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        right_x = self._left_images\r\n        right_x = self._conv('conv', right_x, 5, 3, 32, self._stride_arr(2))\r\n        right_x = self._relu(right_x, self.hps.relu_leakiness)\r\n        right_x = self._batch_norm('bn', right_x)\r\n        \r\n      for i in six.moves.range(8):\r\n        right_x, layer_idx = self._unary_feat_residual(right_x, 3, 32, 32, self._stride_arr(1), layer_idx)\r\n\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        right_x = self._conv('conv', right_x, 3, 32, 32, self._stride_arr(1))\r\n      \r\n    with tf.variable_scope('cost_volumn'):\r\n      left_cost_volume = tf.stack([tf.identity(left_x)] * (self.hps.max_disparity/2+1), axis=1, name='left_stack')\r\n      right_cost_volume = []\r\n      cur_width = tf.shape(right_x)[2]\r\n\r\n      for depth in six.moves.range(self.hps.max_disparity/2+1):\r\n        right_cost_volume.append(tf.pad(tf.slice(right_x, [0, 0, 0, 0], [-1, -1, cur_width - depth, -1], name='right_slice_'+str(depth)),\r\n                                        [[0, 0], [0, 0], [depth, 0], [0, 0]],\r\n                                        name='right_pad_'+str(depth)\r\n                                        ))\r\n      right_cost_volume = tf.stack(right_cost_volume, axis=1, name='right_stack')\r\n      x = tf.concat([left_cost_volume, right_cost_volume], 4)\r\n      tf.add_to_collection('shapes', tf.shape(x))\r\n      \r\n          \r\n    with tf.variable_scope('learning_regularization'):\r\n      stored_features = []\r\n\r\n      in_filters = [64, 64, 64, 64]\r\n      out_filters = [32, 64, 64, 64]\r\n      in_filters_stride_2 = [64, 64, 64, 64]\r\n      out_filters_stride_2 = [64, 64, 64, 128]\r\n      for i in six.moves.range(4):\r\n        tmp_x, layer_idx = self._regularization_subsample(x, 3, in_filters[i], out_filters[i], self._stride_3d_arr(1), layer_idx)\r\n        tf.add_to_collection('shapes', tf.shape(tmp_x))\r\n        stored_features.append(tmp_x)\r\n        \r\n        with tf.variable_scope('layer_'+str(layer_idx)):\r\n          layer_idx += 1\r\n          x = self._conv3d('conv3d', x, 3, in_filters_stride_2[i], out_filters_stride_2[i], self._stride_3d_arr(2))\r\n          x = self._relu(x, self.hps.relu_leakiness)\r\n          x = self._batch_norm('bn', x)\r\n          tf.add_to_collection('shapes', tf.shape(x))\r\n\r\n      \r\n      assert stored_features[0] is not stored_features[1]\r\n\r\n      for i in six.moves.range(2):\r\n        with tf.variable_scope('layer_'+str(layer_idx)):\r\n          layer_idx += 1\r\n          x = self._conv3d('conv3d', x, 3, 128, 128, self._stride_3d_arr(1))\r\n          x = self._relu(x, self.hps.relu_leakiness)\r\n          x = self._batch_norm('bn', x)\r\n          tf.add_to_collection('shapes', tf.shape(x))\r\n\r\n      transposed_in_filters = [128, 64, 64, 64]\r\n      transposed_out_filters = [64, 64, 64, 32]\r\n      \r\n      for i in six.moves.range(4):\r\n        x, layer_idx = self._regularization_upsample(x, stored_features[-i-1], 3, transposed_in_filters[i], transposed_out_filters[i], self._stride_3d_arr(2), layer_idx)\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n      \r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        input_shape = tf.shape(self.gt_disparity)\r\n        x = self._conv3d_trans('conv_trans', x, 3, 32, 1, self._stride_3d_arr(2), [input_shape[0], self.hps.max_disparity+1, input_shape[1], input_shape[2], 1])\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        self.debug_op_list.append(tf.shape(x))\r\n\r\n    \r\n    with tf.variable_scope('soft_argmin'):\r\n        x = tf.squeeze(x, squeeze_dims=[4], name='squeeze')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        x = tf.transpose(x, perm=[0, 2, 3, 1], name='transpose')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        x = tf.nn.softmax(x, dim=-1, name='softmax')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n\r\n        multiplier = tf.range(0, self.hps.max_disparity+1, dtype=tf.float32, name='depth_range')\r\n        x = tf.multiply(x, multiplier, name='softmax_mul_depth')\r\n        tf.add_to_collection('shapes', tf.shape(x))\r\n        self.predicted_disparity = tf.reduce_sum(x, axis=3, name='reduce_sum')       \r\n        tf.add_to_collection('shapes', tf.shape(self.predicted_disparity))\r\n    self.shapes = tf.get_collection('shapes')\r\n    self.debug_op_list.append(self.shapes)\r\n\r\n  def _build_loss_op(self):\r\n    with tf.variable_scope('loss'):\r\n      self.abs_loss = tf.reduce_mean(tf.abs((self.gt_disparity - self.predicted_disparity) * self.mask), name='abs_loss')\r\n      self.total_loss = self.abs_loss + self._decay()\r\n\r\n      \r\n  def _add_loss_summaries(self):\r\n    \"\"\"Add summaries for losses in CIFAR-10 model.\r\n\r\n    Generates moving average for all losses and associated summaries for\r\n    visualizing the performance of the network.\r\n\r\n    Args:\r\n      total_loss: Total loss from loss().\r\n    Returns:\r\n      loss_averages_op: op for generating moving averages of losses.\r\n    \"\"\"\r\n    # Compute the moving average of all individual losses and the total loss.\r\n    loss_averages = tf.train.ExponentialMovingAverage(0.9, name='loss_avg')\r\n    self.loss_averages_op = loss_averages.apply([self.abs_loss, self.total_loss])\r\n\r\n    # Attach a scalar summary to all individual losses and the total loss; do the\r\n    # same for the averaged version of the losses.\r\n    for l in [self.abs_loss, self.total_loss]:\r\n      # Name each loss as '(raw)' and name the moving average version of the loss\r\n      # as the original loss name.\r\n      tf.summary.scalar(l.op.name + ' (raw)', l)\r\n      tf.summary.scalar(l.op.name, loss_averages.average(l))\r\n    \r\n  def _build_train_op(self, global_step):\r\n    \"\"\"Build training specific ops for the graph.\"\"\"\r\n    self.lrn_rate = tf.constant(self.hps.lrn_rate, tf.float32)\r\n    tf.summary.scalar('learning_rate', self.lrn_rate)\r\n\r\n    loss_averages_op = self._add_loss_summaries()\r\n    \r\n    with tf.control_dependencies([loss_averages_op]):\r\n      if self.hps.optimizer == 'sgd':\r\n        optimizer = tf.train.GradientDescentOptimizer(self.lrn_rate)\r\n      elif self.hps.optimizer == 'mom':\r\n        optimizer = tf.train.MomentumOptimizer(self.lrn_rate, 0.9)\r\n      elif self.hps.optimizer == 'RMSProp':\r\n        optimizer = tf.train.RMSPropOptimizer(self.lrn_rate, decay=0.9, momentum=0.9, epsilon=1)\r\n        \r\n        trainable_variables = tf.trainable_variables()\r\n        grads = optimizer.compute_gradients(self.total_loss, trainable_variables)\r\n\r\n\r\n    apply_op = optimizer.apply_gradients(\r\n        grads,\r\n        global_step=global_step, \r\n        name='train_step')\r\n        \r\n    # Track the moving averages of all trainable variables.\r\n    variable_averages = tf.train.ExponentialMovingAverage(\r\n        MOVING_AVERAGE_DECAY, global_step)\r\n    variables_averages_op = variable_averages.apply(tf.trainable_variables())\r\n\r\n    with tf.control_dependencies([apply_op, variables_averages_op]):\r\n      self.train_op = tf.no_op(name='train')\r\n\r\n  def _regularization_upsample(self, x, feature, filter_size, in_filter, out_filter, stride, layer_idx):\r\n    with tf.variable_scope('layer_'+str(layer_idx)):\r\n      layer_idx += 1\r\n      x = self._conv3d_trans('conv_trans', x, filter_size, in_filter, out_filter, stride, tf.shape(feature))\r\n      x = self._relu(x, self.hps.relu_leakiness)\r\n      x = self._batch_norm('bn', x)\r\n      \r\n    with tf.variable_scope('residual_after_'+str(layer_idx-1)):\r\n      x += feature\r\n\r\n    tf.logging.debug('image after unit %s', x.get_shape())\r\n    return x, layer_idx\r\n\r\n  def _regularization_subsample(self, x, filter_size, in_filter, out_filter, stride, layer_idx):\r\n\r\n    with tf.variable_scope('layer_'+str(layer_idx)):\r\n      layer_idx += 1\r\n      x = self._conv3d('conv3d', x, filter_size, in_filter, out_filter, stride)\r\n      x = self._relu(x, self.hps.relu_leakiness)\r\n      x = self._batch_norm('bn', x)\r\n\r\n    with tf.variable_scope('layer_'+str(layer_idx)):\r\n      layer_idx += 1\r\n      x = self._conv3d('conv3d', x, filter_size, out_filter, out_filter, stride)\r\n      x = self._relu(x, self.hps.relu_leakiness)\r\n      x = self._batch_norm('bn', x)\r\n      \r\n    tf.logging.debug('image after unit %s', x.get_shape())\r\n    return x, layer_idx\r\n\r\n  def _unary_feat_residual(self, x, filter_size, in_filter, out_filter, stride, layer_idx):\r\n    orig_x = x\r\n    orig_layer_idx = layer_idx - 1\r\n    \r\n    for i in six.moves.range(2):\r\n      with tf.variable_scope('layer_'+str(layer_idx)):\r\n        layer_idx += 1\r\n        x = self._conv('conv', x, 3, in_filter, out_filter, stride)\r\n        x = self._relu(x, self.hps.relu_leakiness)\r\n        x = self._batch_norm('bn', x)\r\n          \r\n    with tf.variable_scope('residual_btw_'+str(layer_idx-1)+'_'+str(orig_layer_idx)):\r\n      x += orig_x\r\n\r\n    tf.logging.debug('image after unit %s', x.get_shape())\r\n    return x, layer_idx\r\n\r\n\r\n  def _decay(self):\r\n    \"\"\"L2 weight decay loss.\"\"\"\r\n    costs = []\r\n    for var in tf.trainable_variables():\r\n      if var.op.name.find(r'DW') > 0:\r\n        costs.append(tf.nn.l2_loss(var))\r\n        # tf.summary.histogram(var.op.name, var)\r\n\r\n    return tf.multiply(self.hps.weight_decay_rate, tf.add_n(costs))\r\n\r\n  def _conv(self, name, x, filter_size, in_filters, out_filters, strides):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * out_filters\r\n      kernel = self._variable_on_cpu(\r\n          'DW', [filter_size, filter_size, in_filters, out_filters],\r\n          initializer=tf.random_normal_initializer(\r\n              stddev=np.sqrt(2.0/n)))\r\n      return tf.nn.conv2d(x, kernel, strides, padding='SAME')\r\n      \r\n  def _conv3d(self, name, x, filter_size, in_filters, out_filters, strides):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * filter_size * out_filters\r\n      kernel = self._variable_on_cpu(\r\n          'DW', [filter_size, filter_size, filter_size, in_filters, out_filters],\r\n           initializer=tf.random_normal_initializer(\r\n              stddev=np.sqrt(2.0/n)))\r\n      return tf.nn.conv3d(x, kernel, strides, padding='SAME')\r\n      \r\n  def _conv3d_trans(self, name, x, filter_size, in_filters, out_filters, strides, output_shape):\r\n    \"\"\"Convolution.\"\"\"\r\n    with tf.variable_scope(name):\r\n      n = filter_size * filter_size * filter_size * out_filters\r\n      kernel = self._variable_on_cpu(\r\n          'DW', [filter_size, filter_size, filter_size, out_filters, in_filters],\r\n            initializer=tf.random_normal_initializer(\r\n              stddev=np.sqrt(2.0/n)))\r\n      x_shape = tf.shape(x)\r\n      self.debug_op_list.append(tf.shape(kernel))\r\n      return tf.nn.conv3d_transpose(\r\n                x, \r\n                kernel, \r\n                output_shape,\r\n                strides, \r\n                padding='SAME')\r\n\r\n  def _relu(self, x, leakiness=0.0):\r\n    \"\"\"Relu, with optional leaky support.\"\"\"\r\n    return tf.where(tf.less(x, 0.0), leakiness * x, x, name='leaky_relu')\r\n\r\n  # TODO(xpan): Consider batch_norm in contrib/layers/python/layers/layers.py\r\n  def _batch_norm(self, name, x):\r\n    \"\"\"Batch normalization.\"\"\"\r\n    with tf.variable_scope(name):\r\n      params_shape = [x.get_shape()[-1]]\r\n\r\n      beta = self._variable_on_cpu(\r\n          'beta', params_shape,\r\n          initializer=tf.constant_initializer(0.0, tf.float32))\r\n      gamma = self._variable_on_cpu(\r\n          'gamma', params_shape,\r\n          initializer=tf.constant_initializer(1.0, tf.float32))\r\n\r\n      if self.mode == 'train':\r\n        mean, variance = tf.nn.moments(x, range(len(x.get_shape())-1), name='moments')\r\n\r\n        moving_mean = self._variable_on_cpu(\r\n            'moving_mean', params_shape,\r\n            initializer=tf.constant_initializer(0.0, tf.float32),\r\n            trainable=False)\r\n        moving_variance = self._variable_on_cpu(\r\n            'moving_variance', params_shape,\r\n            initializer=tf.constant_initializer(1.0, tf.float32),\r\n            trainable=False)\r\n\r\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\r\n            moving_mean, mean, BATCHNORM_MOVING_AVERAGE_DECAY))\r\n        self._extra_train_ops.append(moving_averages.assign_moving_average(\r\n            moving_variance, variance, BATCHNORM_MOVING_AVERAGE_DECAY))\r\n      else:\r\n        mean = self._variable_on_cpu(\r\n            'moving_mean', params_shape,\r\n            initializer=tf.constant_initializer(0.0, tf.float32),\r\n            trainable=False)\r\n        variance = self._variable_on_cpu(\r\n            'moving_variance', params_shape,\r\n            initializer=tf.constant_initializer(1.0, tf.float32),\r\n            trainable=False)\r\n      y = tf.nn.batch_normalization(\r\n          x, mean, variance, beta, gamma, 0.001)\r\n      y.set_shape(x.get_shape())\r\n      return y\r\n\r\n  def _variable_on_cpu(self, name, shape, initializer, dtype=tf.float32, trainable=True):\r\n    \"\"\"Helper to create a Variable stored on CPU memory.\r\n\r\n    Args:\r\n      name: name of the variable\r\n      shape: list of ints\r\n      initializer: initializer for Variable\r\n\r\n    Returns:\r\n      Variable Tensor\r\n    \"\"\"\r\n    with tf.device('/cpu:0'):\r\n      var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype, trainable=trainable)\r\n    return var\r\n```"}