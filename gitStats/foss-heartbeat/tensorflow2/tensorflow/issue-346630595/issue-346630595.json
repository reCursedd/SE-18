{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21310", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21310/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21310/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21310/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21310", "id": 346630595, "node_id": "MDU6SXNzdWUzNDY2MzA1OTU=", "number": 21310, "title": "Getting slow video straming while testing a model and by using ROS ", "user": {"login": "jadhm", "id": 25588361, "node_id": "MDQ6VXNlcjI1NTg4MzYx", "avatar_url": "https://avatars0.githubusercontent.com/u/25588361?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jadhm", "html_url": "https://github.com/jadhm", "followers_url": "https://api.github.com/users/jadhm/followers", "following_url": "https://api.github.com/users/jadhm/following{/other_user}", "gists_url": "https://api.github.com/users/jadhm/gists{/gist_id}", "starred_url": "https://api.github.com/users/jadhm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jadhm/subscriptions", "organizations_url": "https://api.github.com/users/jadhm/orgs", "repos_url": "https://api.github.com/users/jadhm/repos", "events_url": "https://api.github.com/users/jadhm/events{/privacy}", "received_events_url": "https://api.github.com/users/jadhm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-08-01T14:41:06Z", "updated_at": "2018-08-22T12:07:13Z", "closed_at": "2018-08-22T12:07:13Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:</li>\n<li><strong>TensorFlow version (use command below)</strong>:('v1.9.0-0-g25c197e023', '1.9.0')</li>\n<li><strong>GPU model and memory</strong>:Quadro M1000M , 4 Gb<br>\nBazel version: NA<br>\nCUDA/cuDNN version:NA<br>\nExact command to reproduce: NA<br>\nMobile device : NA</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I trained a model in order to detect a hopper in a bakery using realsense camera D435 but  when i wanted to test and validate the model I wrote a program to stream the frames and to test the model but the stram was so slow so i was getting all the frames but with a big delay despite i am using my gpu and this is the code :<br>\nPm: I am using also ROS for that.</p>\n<h3>Source code / logs</h3>\n<p>#!/usr/bin/env python<br>\nimport numpy as np<br>\nimport os<br>\nimport six.moves.urllib as urllib<br>\nimport sys<br>\nimport tarfile<br>\nimport tensorflow as tf<br>\nimport zipfile</p>\n<p>from collections import defaultdict<br>\nfrom io import StringIO<br>\n#from matplotlib import pyplot as plt<br>\n#from PIL import Image<br>\nimport matplotlib.pyplot as plt<br>\nimport matplotlib.image as mpimg</p>\n<p>import rospkg<br>\nimport rospy<br>\nfrom sensor_msgs.msg import Image<br>\nfrom std_msgs.msg import String<br>\nfrom cv_bridge import CvBridge<br>\nimport cv2</p>\n<h1>SET FRACTION OF GPU YOU WANT TO USE HERE</h1>\n<p>#GPU_FRACTION = 0.4</p>\n<p>if tf.<strong>version</strong> &lt; '1.4.0':<br>\nraise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')</p>\n<h1>get an instance of RosPack with the default search paths</h1>\n<p>rospack = rospkg.RosPack()</p>\n<h1>get the file path for rospy_tutorials</h1>\n<p>path_to_learn_pkg = rospack.get_path('object-detection')<br>\nresearch_module_path = os.path.join(path_to_learn_pkg,\"models/research\")<br>\nobject_detection_module_path = os.path.join(path_to_learn_pkg,\"models/research/object_detection\")<br>\nsys.path.append(object_detection_module_path)</p>\n<p>#print(sys.path)</p>\n<p>from object_detection.utils import ops as utils_ops<br>\nfrom utils import label_map_util</p>\n<p>from utils import visualization_utils as vis_util</p>\n<h1>What model to download.</h1>\n<p>MODEL_NAME = 'learned_model'</p>\n<h1>Path to frozen detection graph. This is the actual model that is used for the object detection.</h1>\n<p>PATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'</p>\n<p>#scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")<br>\nfinal_path_to_ckpt = os.path.join(path_to_learn_pkg,PATH_TO_CKPT)</p>\n<h1>List of the strings that is used to add correct label for each box. In our case mira_robot</h1>\n<p>PATH_TO_LABELS = os.path.join(path_to_learn_pkg, 'training/object-detection.pbtxt')</p>\n<p>NUM_CLASSES = 1</p>\n<p>detection_graph = tf.Graph()</p>\n<p>with detection_graph.as_default():<br>\nod_graph_def = tf.GraphDef()<br>\nwith tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:<br>\nserialized_graph = fid.read()<br>\nod_graph_def.ParseFromString(serialized_graph)<br>\ntf.import_graph_def(od_graph_def, name='')</p>\n<p>label_map = label_map_util.load_labelmap(PATH_TO_LABELS)<br>\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)<br>\ncategory_index = label_map_util.create_category_index(categories)</p>\n<p>def load_image_into_numpy_array(image):<br>\n(im_width, im_height) = image.size<br>\nreturn np.array(image.getdata()).reshape(<br>\n(im_height, im_width, 3)).astype(np.uint8)</p>\n<h1>Size, in inches, of the output images.</h1>\n<p>IMAGE_SIZE = (12, 8)<br>\n#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))<br>\ndef run_inference_for_single_image(image, graph):<br>\nwith graph.as_default():<br>\nwith tf.Session() as sess:<br>\n# Get handles to input and output tensors<br>\nops = tf.get_default_graph().get_operations()<br>\nall_tensor_names = {output.name for op in ops for output in op.outputs}<br>\ntensor_dict = {}<br>\nfor key in [<br>\n'num_detections', 'detection_boxes', 'detection_scores',<br>\n'detection_classes', 'detection_masks'<br>\n]:<br>\ntensor_name = key + ':0'<br>\nif tensor_name in all_tensor_names:<br>\ntensor_dict[key] = tf.get_default_graph().get_tensor_by_name(<br>\ntensor_name)<br>\nif 'detection_masks' in tensor_dict:<br>\n# The following processing is only for a single image<br>\ndetection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])<br>\ndetection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])<br>\n# Reframing is required to translate the mask from box coordinates to image coordinates and fit the image size.<br>\nreal_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)<br>\ndetection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])<br>\ndetection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])<br>\ndetection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(<br>\ndetection_masks, detection_boxes, image.shape[0], image.shape[1])<br>\ndetection_masks_reframed = tf.cast(<br>\ntf.greater(detection_masks_reframed, 0.5), tf.uint8)<br>\n# Follow the convention by adding back the batch dimension<br>\ntensor_dict['detection_masks'] = tf.expand_dims(<br>\ndetection_masks_reframed, 0)<br>\nimage_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')</p>\n<pre><code>      # Run inference\n      output_dict = sess.run(tensor_dict,\n\t                     feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n      # all outputs are float32 numpy arrays, so convert types as appropriate\n      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n      output_dict['detection_classes'] = output_dict[\n\t  'detection_classes'][0].astype(np.uint8)\n      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n      if 'detection_masks' in output_dict:\n\toutput_dict['detection_masks'] = output_dict['detection_masks'][0]\n</code></pre>\n<p>return output_dict</p>\n<p>class RosTensorFlow():<br>\ndef <strong>init</strong>(self):<br>\n# Processing the variable to process only half of the frame's lower load<br>\nself._process_this_frame = True<br>\nself._cv_bridge = CvBridge()</p>\n<pre><code>    self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\n    self._pub = rospy.Publisher('result', String, queue_size=1)\n    self.score_threshold = rospy.get_param('~score_threshold', 0.1)\n    self.use_top_k = rospy.get_param('~use_top_k', 5)\n    \n    \n\ndef callback(self, image_msg):\n    if (self._process_this_frame):\n        \n        image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\n\n        # Expand dimensions since the model expects images to have shapes: [1, None, None, 3]\n        image_np_expanded = np.expand_dims(image_np, axis=0)\n        # Actual detection.\n        output_dict = run_inference_for_single_image(image_np, detection_graph)\n        # Visualization of the results of a detection.\n        vis_util.visualize_boxes_and_labels_on_image_array(\n            image_np,\n            output_dict['detection_boxes'],\n            output_dict['detection_classes'],\n            output_dict['detection_scores'],\n            category_index,\n            instance_masks=output_dict.get('detection_masks'),\n            use_normalized_coordinates=True,\n            line_thickness=8)\n        cv2.imshow(\"Image window\", image_np)\n        cv2.waitKey(1)\n    else:\n        pass\n    # We invert it\n    self._process_this_frame = not self._process_this_frame\n    \n    \n    \ndef main(self):\n    rospy.spin()\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\nrospy.init_node('search_hopper_node')<br>\ntensor = RosTensorFlow()<br>\ntensor.main()</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):Linux Ubuntu 16.04\nTensorFlow installed from (source or binary):\nTensorFlow version (use command below):('v1.9.0-0-g25c197e023', '1.9.0')\nGPU model and memory:Quadro M1000M , 4 Gb\nBazel version: NA\nCUDA/cuDNN version:NA\nExact command to reproduce: NA\nMobile device : NA\n\nDescribe the problem\nI trained a model in order to detect a hopper in a bakery using realsense camera D435 but  when i wanted to test and validate the model I wrote a program to stream the frames and to test the model but the stram was so slow so i was getting all the frames but with a big delay despite i am using my gpu and this is the code :\nPm: I am using also ROS for that.\nSource code / logs\n#!/usr/bin/env python\nimport numpy as np\nimport os\nimport six.moves.urllib as urllib\nimport sys\nimport tarfile\nimport tensorflow as tf\nimport zipfile\nfrom collections import defaultdict\nfrom io import StringIO\n#from matplotlib import pyplot as plt\n#from PIL import Image\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport rospkg\nimport rospy\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nSET FRACTION OF GPU YOU WANT TO USE HERE\n#GPU_FRACTION = 0.4\nif tf.version < '1.4.0':\nraise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\nget an instance of RosPack with the default search paths\nrospack = rospkg.RosPack()\nget the file path for rospy_tutorials\npath_to_learn_pkg = rospack.get_path('object-detection')\nresearch_module_path = os.path.join(path_to_learn_pkg,\"models/research\")\nobject_detection_module_path = os.path.join(path_to_learn_pkg,\"models/research/object_detection\")\nsys.path.append(object_detection_module_path)\n#print(sys.path)\nfrom object_detection.utils import ops as utils_ops\nfrom utils import label_map_util\nfrom utils import visualization_utils as vis_util\nWhat model to download.\nMODEL_NAME = 'learned_model'\nPath to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n#scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")\nfinal_path_to_ckpt = os.path.join(path_to_learn_pkg,PATH_TO_CKPT)\nList of the strings that is used to add correct label for each box. In our case mira_robot\nPATH_TO_LABELS = os.path.join(path_to_learn_pkg, 'training/object-detection.pbtxt')\nNUM_CLASSES = 1\ndetection_graph = tf.Graph()\nwith detection_graph.as_default():\nod_graph_def = tf.GraphDef()\nwith tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\nserialized_graph = fid.read()\nod_graph_def.ParseFromString(serialized_graph)\ntf.import_graph_def(od_graph_def, name='')\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)\ndef load_image_into_numpy_array(image):\n(im_width, im_height) = image.size\nreturn np.array(image.getdata()).reshape(\n(im_height, im_width, 3)).astype(np.uint8)\nSize, in inches, of the output images.\nIMAGE_SIZE = (12, 8)\n#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\ndef run_inference_for_single_image(image, graph):\nwith graph.as_default():\nwith tf.Session() as sess:\n# Get handles to input and output tensors\nops = tf.get_default_graph().get_operations()\nall_tensor_names = {output.name for op in ops for output in op.outputs}\ntensor_dict = {}\nfor key in [\n'num_detections', 'detection_boxes', 'detection_scores',\n'detection_classes', 'detection_masks'\n]:\ntensor_name = key + ':0'\nif tensor_name in all_tensor_names:\ntensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\ntensor_name)\nif 'detection_masks' in tensor_dict:\n# The following processing is only for a single image\ndetection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\ndetection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\n# Reframing is required to translate the mask from box coordinates to image coordinates and fit the image size.\nreal_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\ndetection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\ndetection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\ndetection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\ndetection_masks, detection_boxes, image.shape[0], image.shape[1])\ndetection_masks_reframed = tf.cast(\ntf.greater(detection_masks_reframed, 0.5), tf.uint8)\n# Follow the convention by adding back the batch dimension\ntensor_dict['detection_masks'] = tf.expand_dims(\ndetection_masks_reframed, 0)\nimage_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n      # Run inference\n      output_dict = sess.run(tensor_dict,\n\t                     feed_dict={image_tensor: np.expand_dims(image, 0)})\n\n      # all outputs are float32 numpy arrays, so convert types as appropriate\n      output_dict['num_detections'] = int(output_dict['num_detections'][0])\n      output_dict['detection_classes'] = output_dict[\n\t  'detection_classes'][0].astype(np.uint8)\n      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n      output_dict['detection_scores'] = output_dict['detection_scores'][0]\n      if 'detection_masks' in output_dict:\n\toutput_dict['detection_masks'] = output_dict['detection_masks'][0]\n\nreturn output_dict\nclass RosTensorFlow():\ndef init(self):\n# Processing the variable to process only half of the frame's lower load\nself._process_this_frame = True\nself._cv_bridge = CvBridge()\n    self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\n    self._pub = rospy.Publisher('result', String, queue_size=1)\n    self.score_threshold = rospy.get_param('~score_threshold', 0.1)\n    self.use_top_k = rospy.get_param('~use_top_k', 5)\n    \n    \n\ndef callback(self, image_msg):\n    if (self._process_this_frame):\n        \n        image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\n\n        # Expand dimensions since the model expects images to have shapes: [1, None, None, 3]\n        image_np_expanded = np.expand_dims(image_np, axis=0)\n        # Actual detection.\n        output_dict = run_inference_for_single_image(image_np, detection_graph)\n        # Visualization of the results of a detection.\n        vis_util.visualize_boxes_and_labels_on_image_array(\n            image_np,\n            output_dict['detection_boxes'],\n            output_dict['detection_classes'],\n            output_dict['detection_scores'],\n            category_index,\n            instance_masks=output_dict.get('detection_masks'),\n            use_normalized_coordinates=True,\n            line_thickness=8)\n        cv2.imshow(\"Image window\", image_np)\n        cv2.waitKey(1)\n    else:\n        pass\n    # We invert it\n    self._process_this_frame = not self._process_this_frame\n    \n    \n    \ndef main(self):\n    rospy.spin()\n\nif name == 'main':\nrospy.init_node('search_hopper_node')\ntensor = RosTensorFlow()\ntensor.main()", "body": "\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\n- **TensorFlow version (use command below)**:('v1.9.0-0-g25c197e023', '1.9.0')\r\n- **GPU model and memory**:Quadro M1000M , 4 Gb \r\n     Bazel version: NA\r\n     CUDA/cuDNN version:NA\r\n     Exact command to reproduce: NA\r\n     Mobile device : NA\r\n\r\n \r\n### Describe the problem\r\nI trained a model in order to detect a hopper in a bakery using realsense camera D435 but  when i wanted to test and validate the model I wrote a program to stream the frames and to test the model but the stram was so slow so i was getting all the frames but with a big delay despite i am using my gpu and this is the code : \r\nPm: I am using also ROS for that. \r\n\r\n### Source code / logs\r\n#!/usr/bin/env python\r\nimport numpy as np\r\nimport os\r\nimport six.moves.urllib as urllib\r\nimport sys\r\nimport tarfile\r\nimport tensorflow as tf\r\nimport zipfile\r\n\r\nfrom collections import defaultdict\r\nfrom io import StringIO\r\n#from matplotlib import pyplot as plt\r\n#from PIL import Image\r\nimport matplotlib.pyplot as plt\r\nimport matplotlib.image as mpimg\r\n\r\nimport rospkg\r\nimport rospy\r\nfrom sensor_msgs.msg import Image\r\nfrom std_msgs.msg import String\r\nfrom cv_bridge import CvBridge\r\nimport cv2\r\n# SET FRACTION OF GPU YOU WANT TO USE HERE\r\n#GPU_FRACTION = 0.4\r\n\r\nif tf.__version__ < '1.4.0':\r\n  raise ImportError('Please upgrade your tensorflow installation to v1.4.* or later!')\r\n  \r\n# get an instance of RosPack with the default search paths\r\nrospack = rospkg.RosPack()\r\n# get the file path for rospy_tutorials\r\n\r\npath_to_learn_pkg = rospack.get_path('object-detection')\r\nresearch_module_path = os.path.join(path_to_learn_pkg,\"models/research\")\r\nobject_detection_module_path = os.path.join(path_to_learn_pkg,\"models/research/object_detection\")\r\nsys.path.append(object_detection_module_path)\r\n\r\n#print(sys.path)\r\n\r\nfrom object_detection.utils import ops as utils_ops\r\nfrom utils import label_map_util\r\n\r\nfrom utils import visualization_utils as vis_util\r\n\r\n# What model to download.\r\nMODEL_NAME = 'learned_model'\r\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\r\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\r\n\r\n#scripts_module_path = os.path.join(path_to_learn_pkg,\"scripts/\")\r\nfinal_path_to_ckpt = os.path.join(path_to_learn_pkg,PATH_TO_CKPT)\r\n\r\n# List of the strings that is used to add correct label for each box. In our case mira_robot\r\nPATH_TO_LABELS = os.path.join(path_to_learn_pkg, 'training/object-detection.pbtxt')\r\n\r\nNUM_CLASSES = 1\r\n\r\n\r\ndetection_graph = tf.Graph()\r\n\t\r\nwith detection_graph.as_default():\r\n  od_graph_def = tf.GraphDef()\r\n  with tf.gfile.GFile(final_path_to_ckpt, 'rb') as fid:\r\n    serialized_graph = fid.read()\r\n    od_graph_def.ParseFromString(serialized_graph)\r\n    tf.import_graph_def(od_graph_def, name='')\r\n    \r\nlabel_map = label_map_util.load_labelmap(PATH_TO_LABELS)\r\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\r\ncategory_index = label_map_util.create_category_index(categories)\r\n\r\n\r\ndef load_image_into_numpy_array(image):\r\n  (im_width, im_height) = image.size\r\n  return np.array(image.getdata()).reshape(\r\n      (im_height, im_width, 3)).astype(np.uint8)\r\n      \r\n# Size, in inches, of the output images.\r\nIMAGE_SIZE = (12, 8)\r\n#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\r\ndef run_inference_for_single_image(image, graph):\r\n  with graph.as_default():\r\n\t    with tf.Session() as sess:\r\n\t      # Get handles to input and output tensors\r\n\t      ops = tf.get_default_graph().get_operations()\r\n\t      all_tensor_names = {output.name for op in ops for output in op.outputs}\r\n\t      tensor_dict = {}\r\n\t      for key in [\r\n\t\t  'num_detections', 'detection_boxes', 'detection_scores',\r\n\t\t  'detection_classes', 'detection_masks'\r\n\t      ]:\r\n\t\ttensor_name = key + ':0'\r\n\t\tif tensor_name in all_tensor_names:\r\n\t\t  tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\r\n\t\t      tensor_name)\r\n\t      if 'detection_masks' in tensor_dict:\r\n\t\t# The following processing is only for a single image\r\n\t\tdetection_boxes = tf.squeeze(tensor_dict['detection_boxes'], [0])\r\n\t\tdetection_masks = tf.squeeze(tensor_dict['detection_masks'], [0])\r\n\t\t# Reframing is required to translate the mask from box coordinates to image coordinates and fit the image size.\r\n\t\treal_num_detection = tf.cast(tensor_dict['num_detections'][0], tf.int32)\r\n\t\tdetection_boxes = tf.slice(detection_boxes, [0, 0], [real_num_detection, -1])\r\n\t\tdetection_masks = tf.slice(detection_masks, [0, 0, 0], [real_num_detection, -1, -1])\r\n\t\tdetection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\r\n\t\t    detection_masks, detection_boxes, image.shape[0], image.shape[1])\r\n\t\tdetection_masks_reframed = tf.cast(\r\n\t\t    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\r\n\t\t# Follow the convention by adding back the batch dimension\r\n\t\ttensor_dict['detection_masks'] = tf.expand_dims(\r\n\t\t    detection_masks_reframed, 0)\r\n\t      image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\r\n\r\n\t      # Run inference\r\n\t      output_dict = sess.run(tensor_dict,\r\n\t\t                     feed_dict={image_tensor: np.expand_dims(image, 0)})\r\n\r\n\t      # all outputs are float32 numpy arrays, so convert types as appropriate\r\n\t      output_dict['num_detections'] = int(output_dict['num_detections'][0])\r\n\t      output_dict['detection_classes'] = output_dict[\r\n\t\t  'detection_classes'][0].astype(np.uint8)\r\n\t      output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\r\n\t      output_dict['detection_scores'] = output_dict['detection_scores'][0]\r\n\t      if 'detection_masks' in output_dict:\r\n\t\toutput_dict['detection_masks'] = output_dict['detection_masks'][0]\r\n  return output_dict\r\n  \r\n\r\nclass RosTensorFlow():\r\n    def __init__(self):\r\n        # Processing the variable to process only half of the frame's lower load\r\n        self._process_this_frame = True\r\n        self._cv_bridge = CvBridge()\r\n\t\r\n        self._sub = rospy.Subscriber('image', Image, self.callback, queue_size=1)\r\n        self._pub = rospy.Publisher('result', String, queue_size=1)\r\n        self.score_threshold = rospy.get_param('~score_threshold', 0.1)\r\n        self.use_top_k = rospy.get_param('~use_top_k', 5)\r\n        \r\n        \r\n\r\n    def callback(self, image_msg):\r\n        if (self._process_this_frame):\r\n            \r\n            image_np = self._cv_bridge.imgmsg_to_cv2(image_msg, \"bgr8\")\r\n    \r\n            # Expand dimensions since the model expects images to have shapes: [1, None, None, 3]\r\n            image_np_expanded = np.expand_dims(image_np, axis=0)\r\n            # Actual detection.\r\n            output_dict = run_inference_for_single_image(image_np, detection_graph)\r\n            # Visualization of the results of a detection.\r\n            vis_util.visualize_boxes_and_labels_on_image_array(\r\n                image_np,\r\n                output_dict['detection_boxes'],\r\n                output_dict['detection_classes'],\r\n                output_dict['detection_scores'],\r\n                category_index,\r\n                instance_masks=output_dict.get('detection_masks'),\r\n                use_normalized_coordinates=True,\r\n                line_thickness=8)\r\n            cv2.imshow(\"Image window\", image_np)\r\n            cv2.waitKey(1)\r\n        else:\r\n            pass\r\n        # We invert it\r\n        self._process_this_frame = not self._process_this_frame\r\n        \r\n        \r\n        \r\n    def main(self):\r\n        rospy.spin()\r\n\r\nif __name__ == '__main__':\r\n    rospy.init_node('search_hopper_node')\r\n    tensor = RosTensorFlow()\r\n    tensor.main()"}