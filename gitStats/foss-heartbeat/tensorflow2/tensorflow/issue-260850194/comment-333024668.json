{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333024668", "html_url": "https://github.com/tensorflow/tensorflow/issues/13331#issuecomment-333024668", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13331", "id": 333024668, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzAyNDY2OA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-29T03:52:48Z", "updated_at": "2017-09-29T03:52:48Z", "author_association": "MEMBER", "body_html": "<p>Ok, let's get more specific.  You're trying to do something that probably wasn't anticipated in the early phases of design, but may be possible anyway.  An Op is supposed to allocate memory for its output tensors by calling OpKernelContext::allocate_output().  Two of the definitions take an AllocatorAttributes parameter, e.g. <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h#L808\">this one</a>.  If the op is assigned to execute on a GPU (i.e. 'placed on' a GPU), then the allocator will by default allocate memory from GPU RAM, but AllocatorAttributes has an <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/allocator.h#L352\">on_host bit</a> which if set should cause a GPU allocator to allocate from CPU RAM instead.</p>\n<p>Are you allocating memory from the OpKernelContext::allocate_output function or some other way?</p>\n<p>If from allocate_output, are you setting the on_host attribute of AllocatorAttributes in the request?</p>\n<p>Can you confirm that you've actually allocated a CPU resident tensor within your Op prior to returning the value?</p>\n<p>Is it the case that you successfully allocate CPU RAM and set it, but that the framework automatically converts it to a GPU value, or just fails to return it?</p>", "body_text": "Ok, let's get more specific.  You're trying to do something that probably wasn't anticipated in the early phases of design, but may be possible anyway.  An Op is supposed to allocate memory for its output tensors by calling OpKernelContext::allocate_output().  Two of the definitions take an AllocatorAttributes parameter, e.g. this one.  If the op is assigned to execute on a GPU (i.e. 'placed on' a GPU), then the allocator will by default allocate memory from GPU RAM, but AllocatorAttributes has an on_host bit which if set should cause a GPU allocator to allocate from CPU RAM instead.\nAre you allocating memory from the OpKernelContext::allocate_output function or some other way?\nIf from allocate_output, are you setting the on_host attribute of AllocatorAttributes in the request?\nCan you confirm that you've actually allocated a CPU resident tensor within your Op prior to returning the value?\nIs it the case that you successfully allocate CPU RAM and set it, but that the framework automatically converts it to a GPU value, or just fails to return it?", "body": "Ok, let's get more specific.  You're trying to do something that probably wasn't anticipated in the early phases of design, but may be possible anyway.  An Op is supposed to allocate memory for its output tensors by calling OpKernelContext::allocate_output().  Two of the definitions take an AllocatorAttributes parameter, e.g. [this one](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/op_kernel.h#L808).  If the op is assigned to execute on a GPU (i.e. 'placed on' a GPU), then the allocator will by default allocate memory from GPU RAM, but AllocatorAttributes has an [on_host bit](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/framework/allocator.h#L352) which if set should cause a GPU allocator to allocate from CPU RAM instead.\r\n\r\nAre you allocating memory from the OpKernelContext::allocate_output function or some other way?\r\n\r\nIf from allocate_output, are you setting the on_host attribute of AllocatorAttributes in the request?\r\n\r\nCan you confirm that you've actually allocated a CPU resident tensor within your Op prior to returning the value?\r\n\r\nIs it the case that you successfully allocate CPU RAM and set it, but that the framework automatically converts it to a GPU value, or just fails to return it?"}