{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333030054", "html_url": "https://github.com/tensorflow/tensorflow/issues/13331#issuecomment-333030054", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13331", "id": 333030054, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzAzMDA1NA==", "user": {"login": "andrei-pokrovsky", "id": 11221446, "node_id": "MDQ6VXNlcjExMjIxNDQ2", "avatar_url": "https://avatars3.githubusercontent.com/u/11221446?v=4", "gravatar_id": "", "url": "https://api.github.com/users/andrei-pokrovsky", "html_url": "https://github.com/andrei-pokrovsky", "followers_url": "https://api.github.com/users/andrei-pokrovsky/followers", "following_url": "https://api.github.com/users/andrei-pokrovsky/following{/other_user}", "gists_url": "https://api.github.com/users/andrei-pokrovsky/gists{/gist_id}", "starred_url": "https://api.github.com/users/andrei-pokrovsky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/andrei-pokrovsky/subscriptions", "organizations_url": "https://api.github.com/users/andrei-pokrovsky/orgs", "repos_url": "https://api.github.com/users/andrei-pokrovsky/repos", "events_url": "https://api.github.com/users/andrei-pokrovsky/events{/privacy}", "received_events_url": "https://api.github.com/users/andrei-pokrovsky/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-29T04:48:34Z", "updated_at": "2017-09-29T05:14:34Z", "author_association": "NONE", "body_html": "<p>Thank you, I did try AllocatorAttributes but the next custom op that consumes the output of the previous op seemed to be getting a GPU address in a tensor that was allocated with on_host attribute in a previous op. So i assumed the framework transfered the tensor to GPU automatically. I didn't step through the framework code because I didn't have a debug build atm. I also tried to explicitly allocate a tensor in python as tf.Variable with a context with tf.device(\"/cpu:0\") but the same thing - GPU ops seem to get all tensors converted to their respective devices. I realize this might be a consequence of propagating a placement spec of an op to all it's I/Os or requiring a per-input/output device spec which can complicate things, just wanted to confirm that it is indeed the case or perhaps there's a workaround that I couldn't find.</p>", "body_text": "Thank you, I did try AllocatorAttributes but the next custom op that consumes the output of the previous op seemed to be getting a GPU address in a tensor that was allocated with on_host attribute in a previous op. So i assumed the framework transfered the tensor to GPU automatically. I didn't step through the framework code because I didn't have a debug build atm. I also tried to explicitly allocate a tensor in python as tf.Variable with a context with tf.device(\"/cpu:0\") but the same thing - GPU ops seem to get all tensors converted to their respective devices. I realize this might be a consequence of propagating a placement spec of an op to all it's I/Os or requiring a per-input/output device spec which can complicate things, just wanted to confirm that it is indeed the case or perhaps there's a workaround that I couldn't find.", "body": "Thank you, I did try AllocatorAttributes but the next custom op that consumes the output of the previous op seemed to be getting a GPU address in a tensor that was allocated with on_host attribute in a previous op. So i assumed the framework transfered the tensor to GPU automatically. I didn't step through the framework code because I didn't have a debug build atm. I also tried to explicitly allocate a tensor in python as tf.Variable with a context with tf.device(\"/cpu:0\") but the same thing - GPU ops seem to get all tensors converted to their respective devices. I realize this might be a consequence of propagating a placement spec of an op to all it's I/Os or requiring a per-input/output device spec which can complicate things, just wanted to confirm that it is indeed the case or perhaps there's a workaround that I couldn't find."}