{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333032851", "html_url": "https://github.com/tensorflow/tensorflow/issues/13331#issuecomment-333032851", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13331", "id": 333032851, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzAzMjg1MQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-29T05:15:29Z", "updated_at": "2017-09-29T05:15:29Z", "author_association": "MEMBER", "body_html": "<p>So, you're allocating from allocate_output() with the on_host bit, and you've confirmed that the tensor is allocated from CPU RAM when you access it in your Op.  Then the next Op in the flow graph (let's call it B) is also a GPU-placed Op, and when B reads the tensor via OpKernelContext::inputs() you find that it's allocated from GPU memory, correct?</p>\n<p>The TF execution environment will copy a CPU tensor to GPU RAM when the consuming Op is placed on a GPU, unless the Op specifies that that particular input is expected to be in CPU RAM.  This specification is static, done when the Op is registered.   I don't have access to an example of how, at the moment, but the OpKernel definition will end of with a HOST_MEMORY MemoryType attribute for that input.  Maybe you can figure out by looking for how <a href=\"https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/framework/op_kernel.h?q=op_kernel.h&amp;dr=CSs&amp;l=276\" rel=\"nofollow\">this</a> gets set.</p>\n<p>So the next question is, has op B declared that the corresponding input is HOST_MEMORY?</p>", "body_text": "So, you're allocating from allocate_output() with the on_host bit, and you've confirmed that the tensor is allocated from CPU RAM when you access it in your Op.  Then the next Op in the flow graph (let's call it B) is also a GPU-placed Op, and when B reads the tensor via OpKernelContext::inputs() you find that it's allocated from GPU memory, correct?\nThe TF execution environment will copy a CPU tensor to GPU RAM when the consuming Op is placed on a GPU, unless the Op specifies that that particular input is expected to be in CPU RAM.  This specification is static, done when the Op is registered.   I don't have access to an example of how, at the moment, but the OpKernel definition will end of with a HOST_MEMORY MemoryType attribute for that input.  Maybe you can figure out by looking for how this gets set.\nSo the next question is, has op B declared that the corresponding input is HOST_MEMORY?", "body": "So, you're allocating from allocate_output() with the on_host bit, and you've confirmed that the tensor is allocated from CPU RAM when you access it in your Op.  Then the next Op in the flow graph (let's call it B) is also a GPU-placed Op, and when B reads the tensor via OpKernelContext::inputs() you find that it's allocated from GPU memory, correct?\r\n\r\nThe TF execution environment will copy a CPU tensor to GPU RAM when the consuming Op is placed on a GPU, unless the Op specifies that that particular input is expected to be in CPU RAM.  This specification is static, done when the Op is registered.   I don't have access to an example of how, at the moment, but the OpKernel definition will end of with a HOST_MEMORY MemoryType attribute for that input.  Maybe you can figure out by looking for how [this](https://cs.corp.google.com/piper///depot/google3/third_party/tensorflow/core/framework/op_kernel.h?q=op_kernel.h&dr=CSs&l=276) gets set.\r\n\r\nSo the next question is, has op B declared that the corresponding input is HOST_MEMORY?"}