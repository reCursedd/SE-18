{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146071231", "pull_request_review_id": 70970705, "id": 146071231, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0NjA3MTIzMQ==", "diff_hunk": "@@ -0,0 +1,107 @@\n+/* Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#if GOOGLE_CUDA\n+\n+#define EIGEN_USE_GPU\n+\n+#include \"tensorflow/core/kernels/bincount_op.h\"\n+#include \"external/cub_archive/cub/device/device_histogram.cuh\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+#include \"tensorflow/core/util/cuda_kernel_helper.h\"\n+\n+namespace tensorflow {\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+namespace functor {\n+\n+template <typename T>\n+struct BincountFunctor<GPUDevice, T> {\n+  static Status Compute(OpKernelContext* context,\n+                        const typename TTypes<int32, 1>::ConstTensor& arr,\n+                        typename TTypes<T, 1>::Tensor& output) {\n+    if (output.size() == 0) {\n+      return Status::OK();\n+    }\n+    // In case weight.size() == 0, use CUB\n+    size_t temp_storage_bytes = 0;\n+    const int32* d_samples = arr.data();\n+    T* d_histogram = output.data();\n+    int num_levels = output.size() + 1;\n+    int32 lower_level = 0;\n+    int32 upper_level = output.size();\n+    int num_samples = arr.size();\n+    const cudaStream_t& stream = GetCudaStream(context);\n+\n+    // The first HistogramEven is to obtain the temp storage size required\n+    // with d_temp_storage = NULL passed to the call.\n+    auto err = cub::DeviceHistogram::HistogramEven(\n+        /* d_temp_storage */ NULL,\n+        /* temp_storage_bytes */ temp_storage_bytes,\n+        /* d_samples */ d_samples,\n+        /* d_histogram */ d_histogram,\n+        /* num_levels */ num_levels,\n+        /* lower_level */ lower_level,\n+        /* upper_level */ upper_level,\n+        /* num_samples */ num_samples,\n+        /* stream */ stream);\n+    if (err != cudaSuccess) {\n+      return errors::Internal(\n+          \"Could not launch HistogramEven to get temp storage: \",\n+          cudaGetErrorString(err), \".\");\n+    }\n+    Tensor temp_storage;\n+    TF_RETURN_IF_ERROR(context->allocate_temp(\n+        DataTypeToEnum<int8>::value,\n+        TensorShape({static_cast<int64>(temp_storage_bytes)}), &temp_storage));\n+\n+    void* d_temp_storage = temp_storage.flat<int8>().data();\n+    // The second HistogramEven is to actual run with d_temp_storage\n+    // allocated with temp_storage_bytes.\n+    err = cub::DeviceHistogram::HistogramEven(\n+        /* d_temp_storage */ d_temp_storage,\n+        /* temp_storage_bytes */ temp_storage_bytes,\n+        /* d_samples */ d_samples,\n+        /* d_histogram */ d_histogram,\n+        /* num_levels */ num_levels,\n+        /* lower_level */ lower_level,\n+        /* upper_level */ upper_level,\n+        /* num_samples */ num_samples,\n+        /* stream */ stream);\n+    if (err != cudaSuccess) {\n+      return errors::Internal(\"Could not launch HistogramEven: \",\n+                              cudaGetErrorString(err), \".\");\n+    }\n+    return Status::OK();\n+  }\n+};\n+\n+}  // end namespace functor\n+\n+#define REGISTER_GPU_SPEC(type) \\\n+  template struct functor::BincountFunctor<GPUDevice, type>;\n+\n+TF_CALL_float(REGISTER_GPU_SPEC)", "path": "tensorflow/core/kernels/bincount_op_gpu.cu.cc", "position": null, "original_position": 102, "commit_id": "95ec3de3c5b4baeed1d6db825f48289f7a4d1bbf", "original_commit_id": "a1efcaaf6d1a783170eae7387e19957667e24145", "user": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "body": "again, I think the most useful types will be integer ones, not float.  I think not working on half or double is not a problem.", "created_at": "2017-10-20T21:31:44Z", "updated_at": "2017-11-05T16:19:59Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13813#discussion_r146071231", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13813", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/146071231"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13813#discussion_r146071231"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13813"}}, "body_html": "<p>again, I think the most useful types will be integer ones, not float.  I think not working on half or double is not a problem.</p>", "body_text": "again, I think the most useful types will be integer ones, not float.  I think not working on half or double is not a problem."}