{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/245785938", "html_url": "https://github.com/tensorflow/tensorflow/issues/4218#issuecomment-245785938", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4218", "id": 245785938, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NTc4NTkzOA==", "user": {"login": "zhaopku", "id": 20232241, "node_id": "MDQ6VXNlcjIwMjMyMjQx", "avatar_url": "https://avatars1.githubusercontent.com/u/20232241?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhaopku", "html_url": "https://github.com/zhaopku", "followers_url": "https://api.github.com/users/zhaopku/followers", "following_url": "https://api.github.com/users/zhaopku/following{/other_user}", "gists_url": "https://api.github.com/users/zhaopku/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhaopku/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhaopku/subscriptions", "organizations_url": "https://api.github.com/users/zhaopku/orgs", "repos_url": "https://api.github.com/users/zhaopku/repos", "events_url": "https://api.github.com/users/zhaopku/events{/privacy}", "received_events_url": "https://api.github.com/users/zhaopku/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-09T00:40:02Z", "updated_at": "2016-09-09T00:40:02Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> Actually the original code is like this:</p>\n<pre><code>    with tf.device(getDevice()):\n        model = Model(args, textData)\n\n    writer = tf.train.SummaryWriter(_getSummaryName())\n    saver = tf.train.Saver(max_to_keep=200)  # Arbitrary limit ?\n\n    sess = tf.Session()  # TODO: Replace all sess by sess (not necessary a good idea) ?\n\n    embedding_key_0 = 'embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding:0'\n    embedding_key_1 = 'embedding_rnn_seq2seq/embedding_rnn_decoder/embedding:0'\n\n    var_list = []\n    embedding_list = []\n    cnt = 0\n    for v in tf.all_variables():\n        print(v.name)\n        if v.name != embedding_key_0 and v.name != embedding_key_1:\n            cnt += 1\n            var_list.append(v)\n        else:\n            cnt += 1\n            embedding_list.append(v)\n    print(cnt)\n    print('Initialize variables...')\n\n    sess.run(tf.initialize_all_variables())\n\n\n    print('embedding_0 shape')\n    print(embedding_list[0].get_shape())\n    print('embedding_1 shape')\n    print(embedding_list[1].get_shape())\n\n    print('initializing embeddings from pre-trained word embeddings')\n    embed = loadEmbeddings()\n    embed = np.asarray(embed).reshape([vocab_size, 300])\n\n    assign_op_0 = tf.assign(embedding_list[0], embed)\n    assign_op_1 = tf.assign(embedding_list[1], embed)\n\n\n    sess.run(assign_op_0)\n    sess.run(assign_op_1)\n</code></pre>\n<p>The code in my first post is part of the <code>Model</code>function in the second line. I rewrite the embeddings with pretrained word embeddings.</p>", "body_text": "@lukaszkaiser Actually the original code is like this:\n    with tf.device(getDevice()):\n        model = Model(args, textData)\n\n    writer = tf.train.SummaryWriter(_getSummaryName())\n    saver = tf.train.Saver(max_to_keep=200)  # Arbitrary limit ?\n\n    sess = tf.Session()  # TODO: Replace all sess by sess (not necessary a good idea) ?\n\n    embedding_key_0 = 'embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding:0'\n    embedding_key_1 = 'embedding_rnn_seq2seq/embedding_rnn_decoder/embedding:0'\n\n    var_list = []\n    embedding_list = []\n    cnt = 0\n    for v in tf.all_variables():\n        print(v.name)\n        if v.name != embedding_key_0 and v.name != embedding_key_1:\n            cnt += 1\n            var_list.append(v)\n        else:\n            cnt += 1\n            embedding_list.append(v)\n    print(cnt)\n    print('Initialize variables...')\n\n    sess.run(tf.initialize_all_variables())\n\n\n    print('embedding_0 shape')\n    print(embedding_list[0].get_shape())\n    print('embedding_1 shape')\n    print(embedding_list[1].get_shape())\n\n    print('initializing embeddings from pre-trained word embeddings')\n    embed = loadEmbeddings()\n    embed = np.asarray(embed).reshape([vocab_size, 300])\n\n    assign_op_0 = tf.assign(embedding_list[0], embed)\n    assign_op_1 = tf.assign(embedding_list[1], embed)\n\n\n    sess.run(assign_op_0)\n    sess.run(assign_op_1)\n\nThe code in my first post is part of the Modelfunction in the second line. I rewrite the embeddings with pretrained word embeddings.", "body": "@lukaszkaiser Actually the original code is like this:\n\n```\n    with tf.device(getDevice()):\n        model = Model(args, textData)\n\n    writer = tf.train.SummaryWriter(_getSummaryName())\n    saver = tf.train.Saver(max_to_keep=200)  # Arbitrary limit ?\n\n    sess = tf.Session()  # TODO: Replace all sess by sess (not necessary a good idea) ?\n\n    embedding_key_0 = 'embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding:0'\n    embedding_key_1 = 'embedding_rnn_seq2seq/embedding_rnn_decoder/embedding:0'\n\n    var_list = []\n    embedding_list = []\n    cnt = 0\n    for v in tf.all_variables():\n        print(v.name)\n        if v.name != embedding_key_0 and v.name != embedding_key_1:\n            cnt += 1\n            var_list.append(v)\n        else:\n            cnt += 1\n            embedding_list.append(v)\n    print(cnt)\n    print('Initialize variables...')\n\n    sess.run(tf.initialize_all_variables())\n\n\n    print('embedding_0 shape')\n    print(embedding_list[0].get_shape())\n    print('embedding_1 shape')\n    print(embedding_list[1].get_shape())\n\n    print('initializing embeddings from pre-trained word embeddings')\n    embed = loadEmbeddings()\n    embed = np.asarray(embed).reshape([vocab_size, 300])\n\n    assign_op_0 = tf.assign(embedding_list[0], embed)\n    assign_op_1 = tf.assign(embedding_list[1], embed)\n\n\n    sess.run(assign_op_0)\n    sess.run(assign_op_1)\n```\n\nThe code in my first post is part of the `Model`function in the second line. I rewrite the embeddings with pretrained word embeddings.\n"}