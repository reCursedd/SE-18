{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19281", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19281/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19281/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19281/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19281", "id": 323038982, "node_id": "MDU6SXNzdWUzMjMwMzg5ODI=", "number": 19281, "title": "Tensorflow takes up all system memory in distributed training ", "user": {"login": "oliverhu", "id": 1659910, "node_id": "MDQ6VXNlcjE2NTk5MTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/1659910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oliverhu", "html_url": "https://github.com/oliverhu", "followers_url": "https://api.github.com/users/oliverhu/followers", "following_url": "https://api.github.com/users/oliverhu/following{/other_user}", "gists_url": "https://api.github.com/users/oliverhu/gists{/gist_id}", "starred_url": "https://api.github.com/users/oliverhu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oliverhu/subscriptions", "organizations_url": "https://api.github.com/users/oliverhu/orgs", "repos_url": "https://api.github.com/users/oliverhu/repos", "events_url": "https://api.github.com/users/oliverhu/events{/privacy}", "received_events_url": "https://api.github.com/users/oliverhu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-15T01:52:26Z", "updated_at": "2018-05-30T09:12:33Z", "closed_at": "2018-05-30T09:12:33Z", "author_association": "NONE", "body_html": "<p><strong>System Information:</strong></p>\n<ul>\n<li>OS Platform: Redhat 6.6</li>\n<li>TensorFlow installed from binary</li>\n<li>TensorFlow version: 1.6.1</li>\n<li>CUDA version: 9.1</li>\n<li>cuDNN version: 9.1</li>\n</ul>\n<p>output for print(tf.GIT_VERSION, tf.VERSION):<br>\n('v1.6.0-rc1-3-g690ce9c6cc', '1.6.0-rc0')</p>\n<p><strong>Problem:</strong><br>\nI'm running distributed training with TensorFlow GPU build in a managed cluster. Each box has 4 GPUs and multi tenancy is enabled in this cluster. The problem is each TensorFlow job running inside a container inside the node would allocate <strong>ALL</strong> available system RAM to the process and ends in exhausting available virtual memory. For example, the box has 4 GPUs and 400 GB mem. When we allocate 4 jobs in the same node each asking for a single GPU, each container running the tf job would allocate a 400GB vm, in total 2TB mem and blow away the box :(</p>\n<p><strong>Stack trace:</strong></p>\n<pre><code>INFO:root: Before calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 159284 kB, vmem 1228872 kB\n2018-05-15 01:38:39.088741: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-05-15 01:38:44.083881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:05:00.0\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\n2018-05-15 01:38:44.083950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-05-15 01:38:44.386139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-05-15 01:38:44.386178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \n2018-05-15 01:38:44.386187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \n2018-05-15 01:38:44.390272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10763 MB memory) -&gt; physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)\n2018-05-15 01:38:44.589474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -&gt; {0 -&gt; ltx1-hcl4471.grid.linkedin.com:9521}\n2018-05-15 01:38:44.589514: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -&gt; {0 -&gt; ltx1-hcl4471.grid.linkedin.com:10011, 1 -&gt; localhost:16944}\n2018-05-15 01:38:44.591463: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:16944\nINFO:root: After calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 838408 kB, vmem 292753900 kB\n</code></pre>\n<p>This is killing the multi-tenancy for a managed cluster, please help!</p>", "body_text": "System Information:\n\nOS Platform: Redhat 6.6\nTensorFlow installed from binary\nTensorFlow version: 1.6.1\nCUDA version: 9.1\ncuDNN version: 9.1\n\noutput for print(tf.GIT_VERSION, tf.VERSION):\n('v1.6.0-rc1-3-g690ce9c6cc', '1.6.0-rc0')\nProblem:\nI'm running distributed training with TensorFlow GPU build in a managed cluster. Each box has 4 GPUs and multi tenancy is enabled in this cluster. The problem is each TensorFlow job running inside a container inside the node would allocate ALL available system RAM to the process and ends in exhausting available virtual memory. For example, the box has 4 GPUs and 400 GB mem. When we allocate 4 jobs in the same node each asking for a single GPU, each container running the tf job would allocate a 400GB vm, in total 2TB mem and blow away the box :(\nStack trace:\nINFO:root: Before calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 159284 kB, vmem 1228872 kB\n2018-05-15 01:38:39.088741: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n2018-05-15 01:38:44.083881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:05:00.0\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\n2018-05-15 01:38:44.083950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\n2018-05-15 01:38:44.386139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-05-15 01:38:44.386178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \n2018-05-15 01:38:44.386187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \n2018-05-15 01:38:44.390272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10763 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)\n2018-05-15 01:38:44.589474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ltx1-hcl4471.grid.linkedin.com:9521}\n2018-05-15 01:38:44.589514: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> ltx1-hcl4471.grid.linkedin.com:10011, 1 -> localhost:16944}\n2018-05-15 01:38:44.591463: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:16944\nINFO:root: After calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 838408 kB, vmem 292753900 kB\n\nThis is killing the multi-tenancy for a managed cluster, please help!", "body": "**System Information:**\r\n\r\n- OS Platform: Redhat 6.6\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.6.1\r\n- CUDA version: 9.1\r\n- cuDNN version: 9.1\r\n\r\noutput for print(tf.GIT_VERSION, tf.VERSION):\r\n('v1.6.0-rc1-3-g690ce9c6cc', '1.6.0-rc0')\r\n\r\n**Problem:**\r\nI'm running distributed training with TensorFlow GPU build in a managed cluster. Each box has 4 GPUs and multi tenancy is enabled in this cluster. The problem is each TensorFlow job running inside a container inside the node would allocate **ALL** available system RAM to the process and ends in exhausting available virtual memory. For example, the box has 4 GPUs and 400 GB mem. When we allocate 4 jobs in the same node each asking for a single GPU, each container running the tf job would allocate a 400GB vm, in total 2TB mem and blow away the box :(\r\n\r\n**Stack trace:**\r\n```\r\nINFO:root: Before calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 159284 kB, vmem 1228872 kB\r\n2018-05-15 01:38:39.088741: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n2018-05-15 01:38:44.083881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1344] Found device 0 with properties: \r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:05:00.0\r\ntotalMemory: 11.17GiB freeMemory: 11.10GiB\r\n2018-05-15 01:38:44.083950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1423] Adding visible gpu devices: 0\r\n2018-05-15 01:38:44.386139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:911] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-05-15 01:38:44.386178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:917]      0 \r\n2018-05-15 01:38:44.386187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:930] 0:   N \r\n2018-05-15 01:38:44.390272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1041] Created TensorFlow device (/job:worker/replica:0/task:1/device:GPU:0 with 10763 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7)\r\n2018-05-15 01:38:44.589474: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> ltx1-hcl4471.grid.linkedin.com:9521}\r\n2018-05-15 01:38:44.589514: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> ltx1-hcl4471.grid.linkedin.com:10011, 1 -> localhost:16944}\r\n2018-05-15 01:38:44.591463: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:333] Started server with target: grpc://localhost:16944\r\nINFO:root: After calling tf.train.Server(cluster_spec, job_name, task_index): mem usage: 838408 kB, vmem 292753900 kB\r\n```\r\n\r\nThis is killing the multi-tenancy for a managed cluster, please help!"}