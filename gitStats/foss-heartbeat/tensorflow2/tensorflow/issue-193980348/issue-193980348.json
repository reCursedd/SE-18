{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6148", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6148/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6148/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6148/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6148", "id": 193980348, "node_id": "MDU6SXNzdWUxOTM5ODAzNDg=", "number": 6148, "title": "Data Parallel, multi GPU (minimal) example?", "user": {"login": "sachinruk", "id": 1410927, "node_id": "MDQ6VXNlcjE0MTA5Mjc=", "avatar_url": "https://avatars2.githubusercontent.com/u/1410927?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sachinruk", "html_url": "https://github.com/sachinruk", "followers_url": "https://api.github.com/users/sachinruk/followers", "following_url": "https://api.github.com/users/sachinruk/following{/other_user}", "gists_url": "https://api.github.com/users/sachinruk/gists{/gist_id}", "starred_url": "https://api.github.com/users/sachinruk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sachinruk/subscriptions", "organizations_url": "https://api.github.com/users/sachinruk/orgs", "repos_url": "https://api.github.com/users/sachinruk/repos", "events_url": "https://api.github.com/users/sachinruk/events{/privacy}", "received_events_url": "https://api.github.com/users/sachinruk/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2016-12-07T07:51:39Z", "updated_at": "2017-11-30T21:24:41Z", "closed_at": "2017-01-16T03:26:13Z", "author_association": "NONE", "body_html": "<p>I am trying to get a simple example of tensorflow that parallelises the data over multiple GPUs to train. I have looked at <code>cifar10_multi_gpu.py</code> and its associated input files etc. but it seems to be a bit beyond me at this stage. Especially since I'm not sure why multiple GPUs would speed up the process in that example. If I understand correctly in that example all GPUs get the same copy of the data and the model. The only advantage that I see from this is that you can search a more spread out weight/parameter space.</p>\n<p>What I expected was a line that did something similar to:</p>\n<pre><code>with tf.Session() as sess:\n    for i in range(4): #for 4 gpus\n        with tf.device('/gpu:%d' % i):\n            tf.feed_dict({x: data[i*step:(i+1)*step], y: labels[i*step:(i+1)*step])\n</code></pre>\n<p>with the emphasis of the feed dict feeding different data to the four different gpus. I haven't however thought how I would go about getting the gradients and averaging them yet.</p>\n<p>Any code examples would be highly appreciated. If not thoughts or any direction on how to proceed is also welcome.</p>", "body_text": "I am trying to get a simple example of tensorflow that parallelises the data over multiple GPUs to train. I have looked at cifar10_multi_gpu.py and its associated input files etc. but it seems to be a bit beyond me at this stage. Especially since I'm not sure why multiple GPUs would speed up the process in that example. If I understand correctly in that example all GPUs get the same copy of the data and the model. The only advantage that I see from this is that you can search a more spread out weight/parameter space.\nWhat I expected was a line that did something similar to:\nwith tf.Session() as sess:\n    for i in range(4): #for 4 gpus\n        with tf.device('/gpu:%d' % i):\n            tf.feed_dict({x: data[i*step:(i+1)*step], y: labels[i*step:(i+1)*step])\n\nwith the emphasis of the feed dict feeding different data to the four different gpus. I haven't however thought how I would go about getting the gradients and averaging them yet.\nAny code examples would be highly appreciated. If not thoughts or any direction on how to proceed is also welcome.", "body": "I am trying to get a simple example of tensorflow that parallelises the data over multiple GPUs to train. I have looked at `cifar10_multi_gpu.py` and its associated input files etc. but it seems to be a bit beyond me at this stage. Especially since I'm not sure why multiple GPUs would speed up the process in that example. If I understand correctly in that example all GPUs get the same copy of the data and the model. The only advantage that I see from this is that you can search a more spread out weight/parameter space.\r\n\r\nWhat I expected was a line that did something similar to:\r\n```\r\nwith tf.Session() as sess:\r\n    for i in range(4): #for 4 gpus\r\n        with tf.device('/gpu:%d' % i):\r\n            tf.feed_dict({x: data[i*step:(i+1)*step], y: labels[i*step:(i+1)*step])\r\n```\r\nwith the emphasis of the feed dict feeding different data to the four different gpus. I haven't however thought how I would go about getting the gradients and averaging them yet. \r\n\r\nAny code examples would be highly appreciated. If not thoughts or any direction on how to proceed is also welcome."}