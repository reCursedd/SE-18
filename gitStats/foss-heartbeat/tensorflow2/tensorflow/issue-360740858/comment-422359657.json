{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422359657", "html_url": "https://github.com/tensorflow/tensorflow/pull/22308#issuecomment-422359657", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22308", "id": 422359657, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjM1OTY1Nw==", "user": {"login": "bstriner", "id": 12462956, "node_id": "MDQ6VXNlcjEyNDYyOTU2", "avatar_url": "https://avatars3.githubusercontent.com/u/12462956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bstriner", "html_url": "https://github.com/bstriner", "followers_url": "https://api.github.com/users/bstriner/followers", "following_url": "https://api.github.com/users/bstriner/following{/other_user}", "gists_url": "https://api.github.com/users/bstriner/gists{/gist_id}", "starred_url": "https://api.github.com/users/bstriner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bstriner/subscriptions", "organizations_url": "https://api.github.com/users/bstriner/orgs", "repos_url": "https://api.github.com/users/bstriner/repos", "events_url": "https://api.github.com/users/bstriner/events{/privacy}", "received_events_url": "https://api.github.com/users/bstriner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T11:39:24Z", "updated_at": "2018-09-18T11:39:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Packing operations seem to be calculating everything correctly. Would appreciate any tips on making the kernels faster, maybe adding in some shared memory. Here is how they work currently.</p>\n<ul>\n<li>Pack extracts just the valid data from a sequence</li>\n<li>Unpack pads that data back into the original sequence shape. Padding regions are zero.</li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.cudnn_rnn.python.ops <span class=\"pl-k\">import</span> packing_ops\n\nsequence <span class=\"pl-k\">=</span> tf.constant(np.arange(<span class=\"pl-c1\">6</span> <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>).reshape((<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">1</span>)), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\nsequence_lengths <span class=\"pl-k\">=</span> tf.constant(np.array([<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>]), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32)\nalignments, batch_sizes <span class=\"pl-k\">=</span> packing_ops.packed_sequence_alignment(sequence_lengths, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>align<span class=\"pl-pds\">'</span></span>)\npacked <span class=\"pl-k\">=</span> packing_ops.pack_sequence(sequence, alignments, batch_sizes)\nunpacked <span class=\"pl-k\">=</span> packing_ops.unpack_sequence(packed, alignments, batch_sizes)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    _sequence_lengths, _sequence, _packed, _unpacked <span class=\"pl-k\">=</span> sess.run([\n        sequence_lengths, sequence, packed, unpacked\n    ])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Sequence lengths: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_sequence_lengths))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Sequence: <span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_sequence[:, :, <span class=\"pl-c1\">0</span>]))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Packed: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_packed[:, <span class=\"pl-c1\">0</span>]))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Unpacked: <span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_unpacked[:, :, <span class=\"pl-c1\">0</span>]))</pre></div>\n<blockquote>\n<p>Sequence lengths: [6 3 2]<br>\nSequence:<br>\n[[ 1.  2.  3.]<br>\n[ 4.  5.  6.]<br>\n[ 7.  8.  9.]<br>\n[10. 11. 12.]<br>\n[13. 14. 15.]<br>\n[16. 17. 18.]]<br>\nPacked: [ 1.  2.  3.  4.  5.  6.  7.  8. 10. 13. 16.]<br>\nUnpacked:<br>\n[[ 1.  2.  3.]<br>\n[ 4.  5.  6.]<br>\n[ 7.  8.  0.]<br>\n[10.  0.  0.]<br>\n[13.  0.  0.]<br>\n[16.  0.  0.]]</p>\n</blockquote>", "body_text": "Packing operations seem to be calculating everything correctly. Would appreciate any tips on making the kernels faster, maybe adding in some shared memory. Here is how they work currently.\n\nPack extracts just the valid data from a sequence\nUnpack pads that data back into the original sequence shape. Padding regions are zero.\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.ops import packing_ops\n\nsequence = tf.constant(np.arange(6 * 3).reshape((6, 3, 1)), dtype=tf.float32) + 1\nsequence_lengths = tf.constant(np.array([6, 3, 2]), dtype=tf.int32)\nalignments, batch_sizes = packing_ops.packed_sequence_alignment(sequence_lengths, name='align')\npacked = packing_ops.pack_sequence(sequence, alignments, batch_sizes)\nunpacked = packing_ops.unpack_sequence(packed, alignments, batch_sizes)\n\nwith tf.Session() as sess:\n    _sequence_lengths, _sequence, _packed, _unpacked = sess.run([\n        sequence_lengths, sequence, packed, unpacked\n    ])\n    print(\"Sequence lengths: {}\".format(_sequence_lengths))\n    print(\"Sequence: \\n{}\".format(_sequence[:, :, 0]))\n    print(\"Packed: {}\".format(_packed[:, 0]))\n    print(\"Unpacked: \\n{}\".format(_unpacked[:, :, 0]))\n\nSequence lengths: [6 3 2]\nSequence:\n[[ 1.  2.  3.]\n[ 4.  5.  6.]\n[ 7.  8.  9.]\n[10. 11. 12.]\n[13. 14. 15.]\n[16. 17. 18.]]\nPacked: [ 1.  2.  3.  4.  5.  6.  7.  8. 10. 13. 16.]\nUnpacked:\n[[ 1.  2.  3.]\n[ 4.  5.  6.]\n[ 7.  8.  0.]\n[10.  0.  0.]\n[13.  0.  0.]\n[16.  0.  0.]]", "body": "Packing operations seem to be calculating everything correctly. Would appreciate any tips on making the kernels faster, maybe adding in some shared memory. Here is how they work currently.\r\n* Pack extracts just the valid data from a sequence\r\n* Unpack pads that data back into the original sequence shape. Padding regions are zero.\r\n\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import packing_ops\r\n\r\nsequence = tf.constant(np.arange(6 * 3).reshape((6, 3, 1)), dtype=tf.float32) + 1\r\nsequence_lengths = tf.constant(np.array([6, 3, 2]), dtype=tf.int32)\r\nalignments, batch_sizes = packing_ops.packed_sequence_alignment(sequence_lengths, name='align')\r\npacked = packing_ops.pack_sequence(sequence, alignments, batch_sizes)\r\nunpacked = packing_ops.unpack_sequence(packed, alignments, batch_sizes)\r\n\r\nwith tf.Session() as sess:\r\n    _sequence_lengths, _sequence, _packed, _unpacked = sess.run([\r\n        sequence_lengths, sequence, packed, unpacked\r\n    ])\r\n    print(\"Sequence lengths: {}\".format(_sequence_lengths))\r\n    print(\"Sequence: \\n{}\".format(_sequence[:, :, 0]))\r\n    print(\"Packed: {}\".format(_packed[:, 0]))\r\n    print(\"Unpacked: \\n{}\".format(_unpacked[:, :, 0]))\r\n```\r\n\r\n> Sequence lengths: [6 3 2]\r\n> Sequence: \r\n> [[ 1.  2.  3.]\r\n>  [ 4.  5.  6.]\r\n>  [ 7.  8.  9.]\r\n>  [10. 11. 12.]\r\n>  [13. 14. 15.]\r\n>  [16. 17. 18.]]\r\n> Packed: [ 1.  2.  3.  4.  5.  6.  7.  8. 10. 13. 16.]\r\n> Unpacked: \r\n> [[ 1.  2.  3.]\r\n>  [ 4.  5.  6.]\r\n>  [ 7.  8.  0.]\r\n>  [10.  0.  0.]\r\n>  [13.  0.  0.]\r\n>  [16.  0.  0.]]"}