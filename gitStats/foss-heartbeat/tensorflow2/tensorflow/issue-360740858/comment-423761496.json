{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/423761496", "html_url": "https://github.com/tensorflow/tensorflow/pull/22308#issuecomment-423761496", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22308", "id": 423761496, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMzc2MTQ5Ng==", "user": {"login": "bstriner", "id": 12462956, "node_id": "MDQ6VXNlcjEyNDYyOTU2", "avatar_url": "https://avatars3.githubusercontent.com/u/12462956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bstriner", "html_url": "https://github.com/bstriner", "followers_url": "https://api.github.com/users/bstriner/followers", "following_url": "https://api.github.com/users/bstriner/following{/other_user}", "gists_url": "https://api.github.com/users/bstriner/gists{/gist_id}", "starred_url": "https://api.github.com/users/bstriner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bstriner/subscriptions", "organizations_url": "https://api.github.com/users/bstriner/orgs", "repos_url": "https://api.github.com/users/bstriner/repos", "events_url": "https://api.github.com/users/bstriner/events{/privacy}", "received_events_url": "https://api.github.com/users/bstriner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-22T17:50:05Z", "updated_at": "2018-09-22T17:50:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=59132\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/albertz\">@albertz</a> yes, but that uses ops in tensorflow already. An actual example goes like this. Will definitely need some examples for people to get it right.</p>\n<div class=\"highlight highlight-source-python\"><pre>        u_order <span class=\"pl-k\">=</span> argsort(utterance_lengths, <span class=\"pl-v\">direction</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>DESCENDING<span class=\"pl-pds\">'</span></span>)\n        u_backorder <span class=\"pl-k\">=</span> argsort(u_order)\n        sorted_lengths <span class=\"pl-k\">=</span> tf.gather(utterance_lengths, <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>u_order, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n        sorted_utterances <span class=\"pl-k\">=</span> tf.gather(utterances, <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>u_order, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n        u_align <span class=\"pl-k\">=</span> packed_sequence_alignment(sorted_lengths)\n        u_packed <span class=\"pl-k\">=</span> pack_sequence(sorted_utterances, <span class=\"pl-k\">*</span>u_align)\n\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Listener<span class=\"pl-pds\">\"</span></span>) <span class=\"pl-k\">as</span> listener_scope:\n            h, _ <span class=\"pl-k\">=</span> CudnnLSTM(<span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">3</span>, <span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>params.listener_dim, <span class=\"pl-v\">direction</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">CUDNN_RNN_BIDIRECTION</span>)(\n                u_packed, <span class=\"pl-v\">sequence_lengths</span><span class=\"pl-k\">=</span>sorted_lengths\n            )\n            packed_logits <span class=\"pl-k\">=</span> slim.fully_connected(h, <span class=\"pl-v\">num_outputs</span><span class=\"pl-k\">=</span>vocab_size<span class=\"pl-k\">+</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation_fn</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-v\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>logits<span class=\"pl-pds\">'</span></span>)\n            logits <span class=\"pl-k\">=</span> unpack_sequence(packed_logits, <span class=\"pl-k\">*</span>u_align)\n            logits <span class=\"pl-k\">=</span> tf.gather(logits, <span class=\"pl-v\">indices</span><span class=\"pl-k\">=</span>u_backorder, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)</pre></div>", "body_text": "@albertz yes, but that uses ops in tensorflow already. An actual example goes like this. Will definitely need some examples for people to get it right.\n        u_order = argsort(utterance_lengths, direction='DESCENDING')\n        u_backorder = argsort(u_order)\n        sorted_lengths = tf.gather(utterance_lengths, indices=u_order, axis=0)\n        sorted_utterances = tf.gather(utterances, indices=u_order, axis=1)\n        u_align = packed_sequence_alignment(sorted_lengths)\n        u_packed = pack_sequence(sorted_utterances, *u_align)\n\n        with tf.variable_scope(\"Listener\") as listener_scope:\n            h, _ = CudnnLSTM(num_layers=3, num_units=params.listener_dim, direction=CUDNN_RNN_BIDIRECTION)(\n                u_packed, sequence_lengths=sorted_lengths\n            )\n            packed_logits = slim.fully_connected(h, num_outputs=vocab_size+1, activation_fn=None, scope='logits')\n            logits = unpack_sequence(packed_logits, *u_align)\n            logits = tf.gather(logits, indices=u_backorder, axis=1)", "body": "@albertz yes, but that uses ops in tensorflow already. An actual example goes like this. Will definitely need some examples for people to get it right.\r\n\r\n```python\r\n        u_order = argsort(utterance_lengths, direction='DESCENDING')\r\n        u_backorder = argsort(u_order)\r\n        sorted_lengths = tf.gather(utterance_lengths, indices=u_order, axis=0)\r\n        sorted_utterances = tf.gather(utterances, indices=u_order, axis=1)\r\n        u_align = packed_sequence_alignment(sorted_lengths)\r\n        u_packed = pack_sequence(sorted_utterances, *u_align)\r\n\r\n        with tf.variable_scope(\"Listener\") as listener_scope:\r\n            h, _ = CudnnLSTM(num_layers=3, num_units=params.listener_dim, direction=CUDNN_RNN_BIDIRECTION)(\r\n                u_packed, sequence_lengths=sorted_lengths\r\n            )\r\n            packed_logits = slim.fully_connected(h, num_outputs=vocab_size+1, activation_fn=None, scope='logits')\r\n            logits = unpack_sequence(packed_logits, *u_align)\r\n            logits = tf.gather(logits, indices=u_backorder, axis=1)\r\n```"}