{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/422582340", "html_url": "https://github.com/tensorflow/tensorflow/pull/22308#issuecomment-422582340", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22308", "id": 422582340, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMjU4MjM0MA==", "user": {"login": "bstriner", "id": 12462956, "node_id": "MDQ6VXNlcjEyNDYyOTU2", "avatar_url": "https://avatars3.githubusercontent.com/u/12462956?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bstriner", "html_url": "https://github.com/bstriner", "followers_url": "https://api.github.com/users/bstriner/followers", "following_url": "https://api.github.com/users/bstriner/following{/other_user}", "gists_url": "https://api.github.com/users/bstriner/gists{/gist_id}", "starred_url": "https://api.github.com/users/bstriner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bstriner/subscriptions", "organizations_url": "https://api.github.com/users/bstriner/orgs", "repos_url": "https://api.github.com/users/bstriner/repos", "events_url": "https://api.github.com/users/bstriner/events{/privacy}", "received_events_url": "https://api.github.com/users/bstriner/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-18T22:42:03Z", "updated_at": "2018-09-18T22:42:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5118881\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/qlzh727\">@qlzh727</a> Just got everything working! Could use some polish if anyone has time to give it a review, but the code seems to be working correctly and is enough to get started training. Remaining tasks (can be follow-up issues):</p>\n<ul>\n<li>Maybe refactor the packing kernels. Could probably make them faster. Lots of ways to write that kernel.</li>\n<li>Only made the variable length rnn for int32 indices. Should probably template that out.</li>\n<li>I added tests but can always add more. Only wrote tests for forward, not for backward yet.</li>\n<li>Should setup some benchmarking</li>\n<li>I'm pretty sure the descriptors are being deleted correctly but I would appreciate if anyone can take a look</li>\n</ul>\n<p>Anyways, here is a worked example of a bidirectional CuDNN LSTM with variable length sequences in tensorflow. You can see the variable lengths are working correctly in the output below.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn <span class=\"pl-k\">import</span> CudnnLSTM, <span class=\"pl-c1\">CUDNN_RNN_BIDIRECTION</span>\n<span class=\"pl-k\">from</span> tensorflow.contrib.cudnn_rnn.python.ops <span class=\"pl-k\">import</span> packing_ops\n\nlengths <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">6</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">2</span>]\nl <span class=\"pl-k\">=</span> lengths[<span class=\"pl-c1\">0</span>]\nn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">len</span>(lengths)\nm <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>\ndata_dtype <span class=\"pl-k\">=</span> tf.float32\nindex_dtype <span class=\"pl-k\">=</span> tf.int32\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Create standard CudnnLSTM</span>\nlstm <span class=\"pl-k\">=</span> CudnnLSTM(\n    <span class=\"pl-v\">num_layers</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">num_units</span><span class=\"pl-k\">=</span>m, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>data_dtype, <span class=\"pl-v\">direction</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">CUDNN_RNN_BIDIRECTION</span>,\n    <span class=\"pl-v\">kernel_initializer</span><span class=\"pl-k\">=</span>tf.initializers.constant(<span class=\"pl-c1\">0.2</span>),\n    <span class=\"pl-v\">bias_initializer</span><span class=\"pl-k\">=</span>tf.initializers.constant(<span class=\"pl-c1\">0.2</span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Standard 3d sequence inputs</span>\ninputs <span class=\"pl-k\">=</span> tf.ones(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(l, n, m), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>data_dtype)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Length of each sequence in decreasing order</span>\nsequence_lengths <span class=\"pl-k\">=</span> tf.constant(lengths, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>index_dtype)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Calculate alignments</span>\nalign <span class=\"pl-k\">=</span> packing_ops.packed_sequence_alignment(sequence_lengths)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Pack the sequence</span>\npacked_inputs <span class=\"pl-k\">=</span> packing_ops.pack_sequence(inputs, <span class=\"pl-k\">*</span>align)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the LSTM</span>\npacked_outputs, _ <span class=\"pl-k\">=</span> lstm(packed_inputs, <span class=\"pl-v\">sequence_lengths</span><span class=\"pl-k\">=</span>sequence_lengths)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Unpack the outputs</span>\nunpacked_outputs <span class=\"pl-k\">=</span> packing_ops.unpack_sequence(packed_outputs, <span class=\"pl-k\">*</span>align)\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    _sequence_lengths, _packed_outputs, _unpacked_outputs <span class=\"pl-k\">=</span> sess.run([\n        sequence_lengths, packed_outputs, unpacked_outputs])\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Sequence lengths: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_sequence_lengths))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Unpacked outputs forward: <span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_unpacked_outputs[:, :, <span class=\"pl-c1\">0</span>]))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Unpacked outputs backward: <span class=\"pl-cce\">\\n</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(_unpacked_outputs[:, :, <span class=\"pl-c1\">1</span>]))</pre></div>\n<blockquote>\n<p>Sequence lengths: [6 3 2]<br>\nUnpacked outputs forward:<br>\n[[0.21531965 0.21531965 0.21531965]<br>\n[0.35149407 0.35149407 0.35149407]<br>\n[0.43308067 0.43308067 0.        ]<br>\n[0.4820674  0.         0.        ]<br>\n[0.5121958  0.         0.        ]<br>\n[0.53126556 0.         0.        ]]<br>\nUnpacked outputs backward:<br>\n[[0.53126556 0.43308067 0.35149407]<br>\n[0.5121958  0.35149407 0.21531965]<br>\n[0.4820674  0.21531965 0.        ]<br>\n[0.43308067 0.         0.        ]<br>\n[0.35149407 0.         0.        ]<br>\n[0.21531965 0.         0.        ]]</p>\n</blockquote>", "body_text": "@qlzh727 Just got everything working! Could use some polish if anyone has time to give it a review, but the code seems to be working correctly and is enough to get started training. Remaining tasks (can be follow-up issues):\n\nMaybe refactor the packing kernels. Could probably make them faster. Lots of ways to write that kernel.\nOnly made the variable length rnn for int32 indices. Should probably template that out.\nI added tests but can always add more. Only wrote tests for forward, not for backward yet.\nShould setup some benchmarking\nI'm pretty sure the descriptors are being deleted correctly but I would appreciate if anyone can take a look\n\nAnyways, here is a worked example of a bidirectional CuDNN LSTM with variable length sequences in tensorflow. You can see the variable lengths are working correctly in the output below.\nimport tensorflow as tf\nfrom tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import CudnnLSTM, CUDNN_RNN_BIDIRECTION\nfrom tensorflow.contrib.cudnn_rnn.python.ops import packing_ops\n\nlengths = [6,3,2]\nl = lengths[0]\nn = len(lengths)\nm = 1\ndata_dtype = tf.float32\nindex_dtype = tf.int32\n\n# Create standard CudnnLSTM\nlstm = CudnnLSTM(\n    num_layers=1, num_units=m, dtype=data_dtype, direction=CUDNN_RNN_BIDIRECTION,\n    kernel_initializer=tf.initializers.constant(0.2),\n    bias_initializer=tf.initializers.constant(0.2))\n\n# Standard 3d sequence inputs\ninputs = tf.ones(shape=(l, n, m), dtype=data_dtype)\n# Length of each sequence in decreasing order\nsequence_lengths = tf.constant(lengths, dtype=index_dtype)\n# Calculate alignments\nalign = packing_ops.packed_sequence_alignment(sequence_lengths)\n# Pack the sequence\npacked_inputs = packing_ops.pack_sequence(inputs, *align)\n# Run the LSTM\npacked_outputs, _ = lstm(packed_inputs, sequence_lengths=sequence_lengths)\n# Unpack the outputs\nunpacked_outputs = packing_ops.unpack_sequence(packed_outputs, *align)\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    _sequence_lengths, _packed_outputs, _unpacked_outputs = sess.run([\n        sequence_lengths, packed_outputs, unpacked_outputs])\n    print(\"Sequence lengths: {}\".format(_sequence_lengths))\n    print(\"Unpacked outputs forward: \\n{}\".format(_unpacked_outputs[:, :, 0]))\n    print(\"Unpacked outputs backward: \\n{}\".format(_unpacked_outputs[:, :, 1]))\n\nSequence lengths: [6 3 2]\nUnpacked outputs forward:\n[[0.21531965 0.21531965 0.21531965]\n[0.35149407 0.35149407 0.35149407]\n[0.43308067 0.43308067 0.        ]\n[0.4820674  0.         0.        ]\n[0.5121958  0.         0.        ]\n[0.53126556 0.         0.        ]]\nUnpacked outputs backward:\n[[0.53126556 0.43308067 0.35149407]\n[0.5121958  0.35149407 0.21531965]\n[0.4820674  0.21531965 0.        ]\n[0.43308067 0.         0.        ]\n[0.35149407 0.         0.        ]\n[0.21531965 0.         0.        ]]", "body": "@qlzh727 Just got everything working! Could use some polish if anyone has time to give it a review, but the code seems to be working correctly and is enough to get started training. Remaining tasks (can be follow-up issues):\r\n* Maybe refactor the packing kernels. Could probably make them faster. Lots of ways to write that kernel.\r\n* Only made the variable length rnn for int32 indices. Should probably template that out.\r\n* I added tests but can always add more. Only wrote tests for forward, not for backward yet.\r\n* Should setup some benchmarking\r\n* I'm pretty sure the descriptors are being deleted correctly but I would appreciate if anyone can take a look\r\n\r\nAnyways, here is a worked example of a bidirectional CuDNN LSTM with variable length sequences in tensorflow. You can see the variable lengths are working correctly in the output below.\r\n\r\n```python\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.cudnn_rnn.python.layers.cudnn_rnn import CudnnLSTM, CUDNN_RNN_BIDIRECTION\r\nfrom tensorflow.contrib.cudnn_rnn.python.ops import packing_ops\r\n\r\nlengths = [6,3,2]\r\nl = lengths[0]\r\nn = len(lengths)\r\nm = 1\r\ndata_dtype = tf.float32\r\nindex_dtype = tf.int32\r\n\r\n# Create standard CudnnLSTM\r\nlstm = CudnnLSTM(\r\n    num_layers=1, num_units=m, dtype=data_dtype, direction=CUDNN_RNN_BIDIRECTION,\r\n    kernel_initializer=tf.initializers.constant(0.2),\r\n    bias_initializer=tf.initializers.constant(0.2))\r\n\r\n# Standard 3d sequence inputs\r\ninputs = tf.ones(shape=(l, n, m), dtype=data_dtype)\r\n# Length of each sequence in decreasing order\r\nsequence_lengths = tf.constant(lengths, dtype=index_dtype)\r\n# Calculate alignments\r\nalign = packing_ops.packed_sequence_alignment(sequence_lengths)\r\n# Pack the sequence\r\npacked_inputs = packing_ops.pack_sequence(inputs, *align)\r\n# Run the LSTM\r\npacked_outputs, _ = lstm(packed_inputs, sequence_lengths=sequence_lengths)\r\n# Unpack the outputs\r\nunpacked_outputs = packing_ops.unpack_sequence(packed_outputs, *align)\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    _sequence_lengths, _packed_outputs, _unpacked_outputs = sess.run([\r\n        sequence_lengths, packed_outputs, unpacked_outputs])\r\n    print(\"Sequence lengths: {}\".format(_sequence_lengths))\r\n    print(\"Unpacked outputs forward: \\n{}\".format(_unpacked_outputs[:, :, 0]))\r\n    print(\"Unpacked outputs backward: \\n{}\".format(_unpacked_outputs[:, :, 1]))\r\n```\r\n\r\n> Sequence lengths: [6 3 2]\r\n> Unpacked outputs forward: \r\n> [[0.21531965 0.21531965 0.21531965]\r\n>  [0.35149407 0.35149407 0.35149407]\r\n>  [0.43308067 0.43308067 0.        ]\r\n>  [0.4820674  0.         0.        ]\r\n>  [0.5121958  0.         0.        ]\r\n>  [0.53126556 0.         0.        ]]\r\n> Unpacked outputs backward: \r\n> [[0.53126556 0.43308067 0.35149407]\r\n>  [0.5121958  0.35149407 0.21531965]\r\n>  [0.4820674  0.21531965 0.        ]\r\n>  [0.43308067 0.         0.        ]\r\n>  [0.35149407 0.         0.        ]\r\n>  [0.21531965 0.         0.        ]]"}