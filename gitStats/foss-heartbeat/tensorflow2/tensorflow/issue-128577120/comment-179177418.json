{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/179177418", "html_url": "https://github.com/tensorflow/tensorflow/pull/882#issuecomment-179177418", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/882", "id": 179177418, "node_id": "MDEyOklzc3VlQ29tbWVudDE3OTE3NzQxOA==", "user": {"login": "manipopopo", "id": 14799222, "node_id": "MDQ6VXNlcjE0Nzk5MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/14799222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manipopopo", "html_url": "https://github.com/manipopopo", "followers_url": "https://api.github.com/users/manipopopo/followers", "following_url": "https://api.github.com/users/manipopopo/following{/other_user}", "gists_url": "https://api.github.com/users/manipopopo/gists{/gist_id}", "starred_url": "https://api.github.com/users/manipopopo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manipopopo/subscriptions", "organizations_url": "https://api.github.com/users/manipopopo/orgs", "repos_url": "https://api.github.com/users/manipopopo/repos", "events_url": "https://api.github.com/users/manipopopo/events{/privacy}", "received_events_url": "https://api.github.com/users/manipopopo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-03T11:29:48Z", "updated_at": "2016-02-03T23:46:11Z", "author_association": "CONTRIBUTOR", "body_html": "<p>If the gradients don't propagate down to the embeddings, the input of the decoder RNN will be the random <em>initial values</em> throughout the training. This might be weird, for example, when the embeddings for all words have the same initial values (word 'ils' corresponds to <code>[1, 0, 0, 0, 0]</code>, word 'personnes' corresponds to <code>[1, 0, 0, 0, 0]</code> too, ...).<br>\nMaybe it will be better if the embeddings could learn something during the training. And if it is not the case, maybe we could replace the return value of <code>extract_argmax_and_embed</code>, which is now <code>return embedding_ops.embedding_lookup(embedding, prev_symbol)</code>, with <code>return array_ops.stop_gradient(embedding_ops.embedding_lookup(embedding, prev_symbol))</code>.</p>\n<p>I am not sure, but I think that gradients don't propagate down to the second parameter of <code>embedding_ops.embedding_lookup</code>. Could we replace <code>prev_symbol = array_ops.stop_gradient(math_ops.argmax(prev, 1))</code> in <code>extract_argmax_and_embed</code> with <code>prev_symbol = math_ops.argmax(prev, 1)</code> and add a comment on this?</p>\n<hr>\n<p>(Edit)</p>\n<blockquote>\n<p>these functions will not work at all without stop_gradients (as there is no gradient for argmax).</p>\n</blockquote>\n<p>If we don't want to propagate gradients through the sampling decisions, why the absence of the gradient for <code>argmax</code> makes these functions fail?</p>", "body_text": "If the gradients don't propagate down to the embeddings, the input of the decoder RNN will be the random initial values throughout the training. This might be weird, for example, when the embeddings for all words have the same initial values (word 'ils' corresponds to [1, 0, 0, 0, 0], word 'personnes' corresponds to [1, 0, 0, 0, 0] too, ...).\nMaybe it will be better if the embeddings could learn something during the training. And if it is not the case, maybe we could replace the return value of extract_argmax_and_embed, which is now return embedding_ops.embedding_lookup(embedding, prev_symbol), with return array_ops.stop_gradient(embedding_ops.embedding_lookup(embedding, prev_symbol)).\nI am not sure, but I think that gradients don't propagate down to the second parameter of embedding_ops.embedding_lookup. Could we replace prev_symbol = array_ops.stop_gradient(math_ops.argmax(prev, 1)) in extract_argmax_and_embed with prev_symbol = math_ops.argmax(prev, 1) and add a comment on this?\n\n(Edit)\n\nthese functions will not work at all without stop_gradients (as there is no gradient for argmax).\n\nIf we don't want to propagate gradients through the sampling decisions, why the absence of the gradient for argmax makes these functions fail?", "body": "If the gradients don't propagate down to the embeddings, the input of the decoder RNN will be the random _initial values_ throughout the training. This might be weird, for example, when the embeddings for all words have the same initial values (word 'ils' corresponds to `[1, 0, 0, 0, 0]`, word 'personnes' corresponds to `[1, 0, 0, 0, 0]` too, ...).\nMaybe it will be better if the embeddings could learn something during the training. And if it is not the case, maybe we could replace the return value of `extract_argmax_and_embed`, which is now `return embedding_ops.embedding_lookup(embedding, prev_symbol)`, with `return array_ops.stop_gradient(embedding_ops.embedding_lookup(embedding, prev_symbol))`.\n\nI am not sure, but I think that gradients don't propagate down to the second parameter of `embedding_ops.embedding_lookup`. Could we replace `prev_symbol = array_ops.stop_gradient(math_ops.argmax(prev, 1))` in `extract_argmax_and_embed` with `prev_symbol = math_ops.argmax(prev, 1)` and add a comment on this?\n\n---\n\n(Edit)\n\n> these functions will not work at all without stop_gradients (as there is no gradient for argmax).\n\nIf we don't want to propagate gradients through the sampling decisions, why the absence of the gradient for `argmax` makes these functions fail?\n"}