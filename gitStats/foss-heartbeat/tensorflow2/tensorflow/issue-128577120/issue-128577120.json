{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/882", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/882/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/882/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/882/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/882", "id": 128577120, "node_id": "MDExOlB1bGxSZXF1ZXN0NTcwOTQ0ODU=", "number": 882, "title": "Fix training of decoder embeddings.", "user": {"login": "manipopopo", "id": 14799222, "node_id": "MDQ6VXNlcjE0Nzk5MjIy", "avatar_url": "https://avatars2.githubusercontent.com/u/14799222?v=4", "gravatar_id": "", "url": "https://api.github.com/users/manipopopo", "html_url": "https://github.com/manipopopo", "followers_url": "https://api.github.com/users/manipopopo/followers", "following_url": "https://api.github.com/users/manipopopo/following{/other_user}", "gists_url": "https://api.github.com/users/manipopopo/gists{/gist_id}", "starred_url": "https://api.github.com/users/manipopopo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/manipopopo/subscriptions", "organizations_url": "https://api.github.com/users/manipopopo/orgs", "repos_url": "https://api.github.com/users/manipopopo/repos", "events_url": "https://api.github.com/users/manipopopo/events{/privacy}", "received_events_url": "https://api.github.com/users/manipopopo/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 18, "created_at": "2016-01-25T16:29:50Z", "updated_at": "2018-06-09T03:29:44Z", "closed_at": "2016-02-22T17:55:34Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/882", "html_url": "https://github.com/tensorflow/tensorflow/pull/882", "diff_url": "https://github.com/tensorflow/tensorflow/pull/882.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/882.patch"}, "body_html": "<p>When <code>loop_function</code> is provided, <code>rnn_decoder</code> and <code>attention_decoder</code> do not propagate gradients over the loop function. However,  when <code>feed_previous</code> for <code>embedding_rnn_decoder</code>, <code>embedding_tied_rnn_seq2seq</code> and <code>embedding_attention_decoder</code> is <code>True</code>, the output symbols are embedded by the loop function <code>extract_argmax_and_embed</code>. The embedding for output symbols will learning nothing if there is no gradient propagating over the loop function.</p>\n<p>I think it will be better if the propagation of gradients is specified within the loop function itself instead of <code>rnn_decoder</code> and <code>attention_decoder</code>.</p>", "body_text": "When loop_function is provided, rnn_decoder and attention_decoder do not propagate gradients over the loop function. However,  when feed_previous for embedding_rnn_decoder, embedding_tied_rnn_seq2seq and embedding_attention_decoder is True, the output symbols are embedded by the loop function extract_argmax_and_embed. The embedding for output symbols will learning nothing if there is no gradient propagating over the loop function.\nI think it will be better if the propagation of gradients is specified within the loop function itself instead of rnn_decoder and attention_decoder.", "body": "When `loop_function` is provided, `rnn_decoder` and `attention_decoder` do not propagate gradients over the loop function. However,  when `feed_previous` for `embedding_rnn_decoder`, `embedding_tied_rnn_seq2seq` and `embedding_attention_decoder` is `True`, the output symbols are embedded by the loop function `extract_argmax_and_embed`. The embedding for output symbols will learning nothing if there is no gradient propagating over the loop function.\n\nI think it will be better if the propagation of gradients is specified within the loop function itself instead of `rnn_decoder` and `attention_decoder`.\n"}