{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21745", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21745/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21745/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21745/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21745", "id": 352330438, "node_id": "MDU6SXNzdWUzNTIzMzA0Mzg=", "number": 21745, "title": "Distributed tensorflow worker hangs at TF_CloseSession() when using MonitoredTrainingSession", "user": {"login": "leewyang", "id": 3676078, "node_id": "MDQ6VXNlcjM2NzYwNzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/3676078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leewyang", "html_url": "https://github.com/leewyang", "followers_url": "https://api.github.com/users/leewyang/followers", "following_url": "https://api.github.com/users/leewyang/following{/other_user}", "gists_url": "https://api.github.com/users/leewyang/gists{/gist_id}", "starred_url": "https://api.github.com/users/leewyang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leewyang/subscriptions", "organizations_url": "https://api.github.com/users/leewyang/orgs", "repos_url": "https://api.github.com/users/leewyang/repos", "events_url": "https://api.github.com/users/leewyang/events{/privacy}", "received_events_url": "https://api.github.com/users/leewyang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "yuefengz", "id": 1647833, "node_id": "MDQ6VXNlcjE2NDc4MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1647833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuefengz", "html_url": "https://github.com/yuefengz", "followers_url": "https://api.github.com/users/yuefengz/followers", "following_url": "https://api.github.com/users/yuefengz/following{/other_user}", "gists_url": "https://api.github.com/users/yuefengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuefengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuefengz/subscriptions", "organizations_url": "https://api.github.com/users/yuefengz/orgs", "repos_url": "https://api.github.com/users/yuefengz/repos", "events_url": "https://api.github.com/users/yuefengz/events{/privacy}", "received_events_url": "https://api.github.com/users/yuefengz/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuefengz", "id": 1647833, "node_id": "MDQ6VXNlcjE2NDc4MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1647833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuefengz", "html_url": "https://github.com/yuefengz", "followers_url": "https://api.github.com/users/yuefengz/followers", "following_url": "https://api.github.com/users/yuefengz/following{/other_user}", "gists_url": "https://api.github.com/users/yuefengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuefengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuefengz/subscriptions", "organizations_url": "https://api.github.com/users/yuefengz/orgs", "repos_url": "https://api.github.com/users/yuefengz/repos", "events_url": "https://api.github.com/users/yuefengz/events{/privacy}", "received_events_url": "https://api.github.com/users/yuefengz/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2018-08-20T23:22:27Z", "updated_at": "2018-11-09T13:08:17Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: N/A</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:  RHEL7 and also Mac OS X 10.13.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary (pip install)</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0+</li>\n<li><strong>Python version</strong>: 2.7 (RHEL7), 3.6 (Mac)</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: See below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When using <code>MonitoredTrainingSession</code> in TensorFlow version 1.9 or higher, I'm seeing the following deadlock/hang (as reported by the <code>hanging-threads</code> pip package) when the context manager exits.  Note: I do not see this hang for versions 1.8 or earlier.  Also, note that this does not occur if using the older <code>tf.train.Supervisor</code> API.</p>\n<pre><code>----------     Thread 140682110711616 hangs       ----------\n\tFile \"trainer.py\", line 102, in &lt;module&gt;\n\t\ttf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n\t\t_sys.exit(main(argv))\n\tFile \"trainer.py\", line 67, in main\n\t\tprint(\"step: {}\".format(step))\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 689, in __exit__\n\t\tself._close_internal(exception_type)\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 726, in _close_internal\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1121, in close\n\t\t_WrappedSession.close(self)\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 690, in close\n\t\ttf_session.TF_CloseSession(self._session)\n</code></pre>\n<p>The <a href=\"https://gist.github.com/leewyang/7bf6d0df0328fbc4e2e97275d741a044\">code</a> that generates this is based on the <a href=\"https://www.tensorflow.org/deploy/distributed\" rel=\"nofollow\">Distributed Tensorflow</a> documentation (with a trivial/dummy model).  I start one PS node and two worker nodes on a single box as follows:</p>\n<pre><code>rm -rf /tmp/train_logs; \\\npython trainer.py \\\n     --ps_hosts=localhost:2222 \\\n     --worker_hosts=localhost:2223,localhost:2224 \\\n     --job_name=ps --task_index=0\n\npython trainer.py \\\n     --ps_hosts=localhost:2222 \\\n     --worker_hosts=localhost:2223,localhost:2224 \\\n     --job_name=worker --task_index=0\n\npython trainer.py \\\n     --ps_hosts=localhost:2222 \\\n     --worker_hosts=localhost:2223,localhost:2224 \\\n     --job_name=worker --task_index=1\n</code></pre>\n<p>I've been able to reproduce this quite consistently on:</p>\n<ul>\n<li>Mac 10.13.6, Python 3.6, TensorFlow 1.10</li>\n<li>RHEL7, Python2.7, TensorFlow 1.9</li>\n</ul>\n<p>And the symptom goes away when switching to 1.8 or earlier.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: N/A\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):  RHEL7 and also Mac OS X 10.13.6\nTensorFlow installed from (source or binary): binary (pip install)\nTensorFlow version (use command below): 1.9.0+\nPython version: 2.7 (RHEL7), 3.6 (Mac)\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: See below\n\nDescribe the problem\nWhen using MonitoredTrainingSession in TensorFlow version 1.9 or higher, I'm seeing the following deadlock/hang (as reported by the hanging-threads pip package) when the context manager exits.  Note: I do not see this hang for versions 1.8 or earlier.  Also, note that this does not occur if using the older tf.train.Supervisor API.\n----------     Thread 140682110711616 hangs       ----------\n\tFile \"trainer.py\", line 102, in <module>\n\t\ttf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\n\t\t_sys.exit(main(argv))\n\tFile \"trainer.py\", line 67, in main\n\t\tprint(\"step: {}\".format(step))\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 689, in __exit__\n\t\tself._close_internal(exception_type)\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 726, in _close_internal\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1121, in close\n\t\t_WrappedSession.close(self)\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\n\t\tself._sess.close()\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 690, in close\n\t\ttf_session.TF_CloseSession(self._session)\n\nThe code that generates this is based on the Distributed Tensorflow documentation (with a trivial/dummy model).  I start one PS node and two worker nodes on a single box as follows:\nrm -rf /tmp/train_logs; \\\npython trainer.py \\\n     --ps_hosts=localhost:2222 \\\n     --worker_hosts=localhost:2223,localhost:2224 \\\n     --job_name=ps --task_index=0\n\npython trainer.py \\\n     --ps_hosts=localhost:2222 \\\n     --worker_hosts=localhost:2223,localhost:2224 \\\n     --job_name=worker --task_index=0\n\npython trainer.py \\\n     --ps_hosts=localhost:2222 \\\n     --worker_hosts=localhost:2223,localhost:2224 \\\n     --job_name=worker --task_index=1\n\nI've been able to reproduce this quite consistently on:\n\nMac 10.13.6, Python 3.6, TensorFlow 1.10\nRHEL7, Python2.7, TensorFlow 1.9\n\nAnd the symptom goes away when switching to 1.8 or earlier.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:  RHEL7 and also Mac OS X 10.13.6\r\n- **TensorFlow installed from (source or binary)**: binary (pip install)\r\n- **TensorFlow version (use command below)**: 1.9.0+\r\n- **Python version**: 2.7 (RHEL7), 3.6 (Mac)\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: See below\r\n\r\n### Describe the problem\r\nWhen using `MonitoredTrainingSession` in TensorFlow version 1.9 or higher, I'm seeing the following deadlock/hang (as reported by the `hanging-threads` pip package) when the context manager exits.  Note: I do not see this hang for versions 1.8 or earlier.  Also, note that this does not occur if using the older `tf.train.Supervisor` API.\r\n```\r\n----------     Thread 140682110711616 hangs       ----------\r\n\tFile \"trainer.py\", line 102, in <module>\r\n\t\ttf.app.run(main=main, argv=[sys.argv[0]] + unparsed)\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/platform/app.py\", line 125, in run\r\n\t\t_sys.exit(main(argv))\r\n\tFile \"trainer.py\", line 67, in main\r\n\t\tprint(\"step: {}\".format(step))\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 689, in __exit__\r\n\t\tself._close_internal(exception_type)\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 726, in _close_internal\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 1121, in close\r\n\t\t_WrappedSession.close(self)\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/training/monitored_session.py\", line 974, in close\r\n\t\tself._sess.close()\r\n\tFile \"/usr/lib/python2.7/site-packages/tensorflow/python/client/session.py\", line 690, in close\r\n\t\ttf_session.TF_CloseSession(self._session)\r\n```\r\n\r\nThe [code](https://gist.github.com/leewyang/7bf6d0df0328fbc4e2e97275d741a044) that generates this is based on the [Distributed Tensorflow](https://www.tensorflow.org/deploy/distributed) documentation (with a trivial/dummy model).  I start one PS node and two worker nodes on a single box as follows:\r\n```\r\nrm -rf /tmp/train_logs; \\\r\npython trainer.py \\\r\n     --ps_hosts=localhost:2222 \\\r\n     --worker_hosts=localhost:2223,localhost:2224 \\\r\n     --job_name=ps --task_index=0\r\n\r\npython trainer.py \\\r\n     --ps_hosts=localhost:2222 \\\r\n     --worker_hosts=localhost:2223,localhost:2224 \\\r\n     --job_name=worker --task_index=0\r\n\r\npython trainer.py \\\r\n     --ps_hosts=localhost:2222 \\\r\n     --worker_hosts=localhost:2223,localhost:2224 \\\r\n     --job_name=worker --task_index=1\r\n```\r\n\r\nI've been able to reproduce this quite consistently on:\r\n- Mac 10.13.6, Python 3.6, TensorFlow 1.10\r\n- RHEL7, Python2.7, TensorFlow 1.9\r\n\r\nAnd the symptom goes away when switching to 1.8 or earlier."}