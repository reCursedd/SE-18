{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17902", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17902/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17902/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17902/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17902", "id": 307354308, "node_id": "MDU6SXNzdWUzMDczNTQzMDg=", "number": 17902, "title": "Allocating reusable memory in Tensorflow custom operator", "user": {"login": "zhangjiong724", "id": 11007902, "node_id": "MDQ6VXNlcjExMDA3OTAy", "avatar_url": "https://avatars1.githubusercontent.com/u/11007902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangjiong724", "html_url": "https://github.com/zhangjiong724", "followers_url": "https://api.github.com/users/zhangjiong724/followers", "following_url": "https://api.github.com/users/zhangjiong724/following{/other_user}", "gists_url": "https://api.github.com/users/zhangjiong724/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangjiong724/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangjiong724/subscriptions", "organizations_url": "https://api.github.com/users/zhangjiong724/orgs", "repos_url": "https://api.github.com/users/zhangjiong724/repos", "events_url": "https://api.github.com/users/zhangjiong724/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangjiong724/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "angersson", "id": 32465472, "node_id": "MDQ6VXNlcjMyNDY1NDcy", "avatar_url": "https://avatars2.githubusercontent.com/u/32465472?v=4", "gravatar_id": "", "url": "https://api.github.com/users/angersson", "html_url": "https://github.com/angersson", "followers_url": "https://api.github.com/users/angersson/followers", "following_url": "https://api.github.com/users/angersson/following{/other_user}", "gists_url": "https://api.github.com/users/angersson/gists{/gist_id}", "starred_url": "https://api.github.com/users/angersson/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/angersson/subscriptions", "organizations_url": "https://api.github.com/users/angersson/orgs", "repos_url": "https://api.github.com/users/angersson/repos", "events_url": "https://api.github.com/users/angersson/events{/privacy}", "received_events_url": "https://api.github.com/users/angersson/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-03-21T17:52:36Z", "updated_at": "2018-04-03T17:09:10Z", "closed_at": "2018-04-03T17:09:10Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nMaxOS 10.13 and Linux version 2.6.32-696.3.2.el6.x86_64</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nsource</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.5</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\n0.11.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\ngcc 4.4.7</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\n8.0/5.1</li>\n<li><strong>GPU model and memory</strong>:<br>\nTesla K40M</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hi, I'm having a memory related question when defining a Tensorflow custom operator with cuBLAS/C++.</p>\n<p>I defined a custom operator which needs to be called multiple times during training, computed with a BLAS level 3 algorithm.<br>\nMy algorithm (as well as many other BLAS3 algorithms) requires a 'workspace' memory to exploit the hardware efficiently. However, I cannot seem to find a way to allocate such memory and keep it throughout the training process. Instead, I have to allocate a new chunk of workspace memory and free it during each call to my operator.</p>\n<p>I've also tried to define a workspace variable in Tensorflow and feed it to the operator as an input. But I found that TF treats it as a const pointer and does not allow me to write to it.</p>\n<p>So I am wondering if there is an efficient way to allocate reusable GPU memory for the operator at the beginning, and keep it throughout the training process. If there is not, would it be better to include such features in the future versions so that many well-developed linear algebra algorithms could avoid such memory allocating issue.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nMaxOS 10.13 and Linux version 2.6.32-696.3.2.el6.x86_64\nTensorFlow installed from (source or binary):\nsource\nTensorFlow version (use command below):\n1.5\nPython version:\n2.7\nBazel version (if compiling from source):\n0.11.0\nGCC/Compiler version (if compiling from source):\ngcc 4.4.7\nCUDA/cuDNN version:\n8.0/5.1\nGPU model and memory:\nTesla K40M\n\nDescribe the problem\nHi, I'm having a memory related question when defining a Tensorflow custom operator with cuBLAS/C++.\nI defined a custom operator which needs to be called multiple times during training, computed with a BLAS level 3 algorithm.\nMy algorithm (as well as many other BLAS3 algorithms) requires a 'workspace' memory to exploit the hardware efficiently. However, I cannot seem to find a way to allocate such memory and keep it throughout the training process. Instead, I have to allocate a new chunk of workspace memory and free it during each call to my operator.\nI've also tried to define a workspace variable in Tensorflow and feed it to the operator as an input. But I found that TF treats it as a const pointer and does not allow me to write to it.\nSo I am wondering if there is an efficient way to allocate reusable GPU memory for the operator at the beginning, and keep it throughout the training process. If there is not, would it be better to include such features in the future versions so that many well-developed linear algebra algorithms could avoid such memory allocating issue.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: \r\nMaxOS 10.13 and Linux version 2.6.32-696.3.2.el6.x86_64\r\n- **TensorFlow installed from (source or binary)**: \r\nsource\r\n- **TensorFlow version (use command below)**: \r\n1.5\r\n- **Python version**: \r\n2.7\r\n- **Bazel version (if compiling from source)**:  \r\n0.11.0\r\n- **GCC/Compiler version (if compiling from source)**: \r\ngcc 4.4.7\r\n- **CUDA/cuDNN version**: \r\n8.0/5.1\r\n- **GPU model and memory**: \r\nTesla K40M\r\n\r\n\r\n### Describe the problem\r\nHi, I'm having a memory related question when defining a Tensorflow custom operator with cuBLAS/C++.\r\n\r\nI defined a custom operator which needs to be called multiple times during training, computed with a BLAS level 3 algorithm.\r\nMy algorithm (as well as many other BLAS3 algorithms) requires a 'workspace' memory to exploit the hardware efficiently. However, I cannot seem to find a way to allocate such memory and keep it throughout the training process. Instead, I have to allocate a new chunk of workspace memory and free it during each call to my operator. \r\n\r\nI've also tried to define a workspace variable in Tensorflow and feed it to the operator as an input. But I found that TF treats it as a const pointer and does not allow me to write to it.\r\n\r\nSo I am wondering if there is an efficient way to allocate reusable GPU memory for the operator at the beginning, and keep it throughout the training process. If there is not, would it be better to include such features in the future versions so that many well-developed linear algebra algorithms could avoid such memory allocating issue."}