{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5928", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5928/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5928/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5928/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5928", "id": 192208242, "node_id": "MDU6SXNzdWUxOTIyMDgyNDI=", "number": 5928, "title": "Support distributed aggregation in embedding_lookup_sparse", "user": {"login": "llhe", "id": 192829, "node_id": "MDQ6VXNlcjE5MjgyOQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/192829?v=4", "gravatar_id": "", "url": "https://api.github.com/users/llhe", "html_url": "https://github.com/llhe", "followers_url": "https://api.github.com/users/llhe/followers", "following_url": "https://api.github.com/users/llhe/following{/other_user}", "gists_url": "https://api.github.com/users/llhe/gists{/gist_id}", "starred_url": "https://api.github.com/users/llhe/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/llhe/subscriptions", "organizations_url": "https://api.github.com/users/llhe/orgs", "repos_url": "https://api.github.com/users/llhe/repos", "events_url": "https://api.github.com/users/llhe/events{/privacy}", "received_events_url": "https://api.github.com/users/llhe/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-11-29T07:33:00Z", "updated_at": "2017-04-25T02:56:06Z", "closed_at": "2017-04-25T02:56:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The current code just gather the slice (co-located with the partition) and send back to the caller for the further aggregation with the specified combiner. In some cases, making the aggregation co-located with the variable partition can reduce latency (both computing time and transfer time).</p>\n<p>For example, the embedding weights has shape <code>10000000 * 1024</code> with <code>10</code> partitions. The batch size is <code>256</code>. And the average number non-zero value of each feature tensor is <code>100</code>. Without aggregation, the transfer size out of each partition is around <code>256 * 100/10 * 1024 * 4 = 10MB</code>. If we make the aggregation first, the data size can be reduced to around <code>256 * 1024 * 4 = 1MB</code>. There are two benefits for the latency, first is the reduced transfer size, the second is partial of the aggregation is done in parallel.</p>\n<p>It's suggested to add an option with this feature default to false (whether to use this feature depends on the devices speed up ratio and transfer bandwidth).</p>", "body_text": "The current code just gather the slice (co-located with the partition) and send back to the caller for the further aggregation with the specified combiner. In some cases, making the aggregation co-located with the variable partition can reduce latency (both computing time and transfer time).\nFor example, the embedding weights has shape 10000000 * 1024 with 10 partitions. The batch size is 256. And the average number non-zero value of each feature tensor is 100. Without aggregation, the transfer size out of each partition is around 256 * 100/10 * 1024 * 4 = 10MB. If we make the aggregation first, the data size can be reduced to around 256 * 1024 * 4 = 1MB. There are two benefits for the latency, first is the reduced transfer size, the second is partial of the aggregation is done in parallel.\nIt's suggested to add an option with this feature default to false (whether to use this feature depends on the devices speed up ratio and transfer bandwidth).", "body": "The current code just gather the slice (co-located with the partition) and send back to the caller for the further aggregation with the specified combiner. In some cases, making the aggregation co-located with the variable partition can reduce latency (both computing time and transfer time).\r\n\r\nFor example, the embedding weights has shape `10000000 * 1024` with `10` partitions. The batch size is `256`. And the average number non-zero value of each feature tensor is `100`. Without aggregation, the transfer size out of each partition is around `256 * 100/10 * 1024 * 4 = 10MB`. If we make the aggregation first, the data size can be reduced to around `256 * 1024 * 4 = 1MB`. There are two benefits for the latency, first is the reduced transfer size, the second is partial of the aggregation is done in parallel.\r\n\r\nIt's suggested to add an option with this feature default to false (whether to use this feature depends on the devices speed up ratio and transfer bandwidth)."}