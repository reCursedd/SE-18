{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4203", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4203/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4203/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4203/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4203", "id": 175030284, "node_id": "MDU6SXNzdWUxNzUwMzAyODQ=", "number": 4203, "title": "BUG in graph_actions.py", "user": {"login": "david-liu", "id": 1133159, "node_id": "MDQ6VXNlcjExMzMxNTk=", "avatar_url": "https://avatars1.githubusercontent.com/u/1133159?v=4", "gravatar_id": "", "url": "https://api.github.com/users/david-liu", "html_url": "https://github.com/david-liu", "followers_url": "https://api.github.com/users/david-liu/followers", "following_url": "https://api.github.com/users/david-liu/following{/other_user}", "gists_url": "https://api.github.com/users/david-liu/gists{/gist_id}", "starred_url": "https://api.github.com/users/david-liu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/david-liu/subscriptions", "organizations_url": "https://api.github.com/users/david-liu/orgs", "repos_url": "https://api.github.com/users/david-liu/repos", "events_url": "https://api.github.com/users/david-liu/events{/privacy}", "received_events_url": "https://api.github.com/users/david-liu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-09-05T09:23:28Z", "updated_at": "2017-01-27T18:14:15Z", "closed_at": "2017-01-27T18:14:15Z", "author_association": "NONE", "body_html": "<p>Hello I am using <code>tf.contrib.learn.Estimator</code> to train my model, and size of validation sample is small,(validation sample size=32, eval_batch_size=16)</p>\n<p>I have created an EvaluationMonitor to evaluate the validation sample</p>\n<pre><code>class EvaluationMonitor(tf.contrib.learn.monitors.EveryN):\n        def every_n_step_end(self, step, outputs):\n            eval_results = self._estimator.evaluate(\n                input_fn=input_fn_eval,\n                metrics=eval_metrics,\n                steps=None)\n            print \"Evaluation Result: %s\" % eval_results\n</code></pre>\n<p>and the <code>eval_metrics</code> is is a list of <code>tf.contrib.metrics.streaming_sparse_recall_at_k</code>, <code>input_fn_eval</code> is a function to read TFRecord from file</p>\n<p>but in the evaluation step, I always get the exception:</p>\n<blockquote>\n<p>File \"...../tensorflow/contrib/learn/python/learn/graph_actions.py\", line 610, in _eval_results_to_str<br>\nreturn ', '.join('%s = %s' % (k, v) for k, v in eval_results.items())</p>\n</blockquote>\n<p>When i looks inside the evaluate method(Line 632- Line 779) in the source of <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L632\"><code>graph_actions.py</code></a></p>\n<pre><code>def evaluate(graph,\n             output_dir,\n             checkpoint_path,\n             eval_dict,\n             update_op=None,\n             global_step_tensor=None,\n             supervisor_master='',\n             log_every_steps=10,\n             feed_fn=None,\n             max_steps=None):\n......\ntry:\n      try:\n        while (max_steps is None) or (step &lt; max_steps):\n          step += 1\n          start_time = time.time()\n          feed_dict = feed_fn() if feed_fn is not None else None\n          if update_op is not None:\n            session.run(update_op, feed_dict=feed_dict)\n          else:\n            eval_results = session.run(eval_dict, feed_dict=feed_dict)\n            eval_step = step\n\n          if step % log_every_steps == 0:\n            if eval_step is None or step != eval_step:\n              eval_results = session.run(eval_dict, feed_dict=feed_dict)\n              eval_step = step\n            duration = time.time() - start_time\n            logging.info('Results after %d steps (%.3f sec/batch): %s.',\n                         step, float(duration),\n                         _eval_results_to_str(eval_results))\n      finally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n\n......\n\n # Save summaries for this evaluation.\n  _write_summary_results(output_dir, eval_results, current_global_step)\n\n</code></pre>\n<p>when my code call the function,   the <code>max step=2</code>(validation sample size=32, eval_batch_size=16) will be less than the default parameter( <code>log_every_steps=10</code>), therefore the code only can run the evaluation in the <code>finally</code> block,</p>\n<pre><code> ....\nfinally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n...\n</code></pre>\n<p>but for the metrics of t<code>tf.contrib.metrics.streaming_sparse_recall_at_k,</code> the <code>update_op</code> is not Not, which means the line</p>\n<pre><code>if update_op is not None\n    session.run(update_op, feed_dict=feed_dict)\n</code></pre>\n<p>will alway be invoked, and the batched_data in the queue will be <em>empty</em> when the code jump into <code>finally</code> block, therefore an exception of <code>OutOfRangeError</code> will be raised, and <code>eval_results</code> will be None when the code execute at line</p>\n<pre><code>_write_summary_results(output_dir, eval_results, current_global_step)\n</code></pre>\n<p>I think it is caused by the default value of <code>log_every_steps</code>, when the <code>_evaluate_model</code> method in <code>tf.contrib.learn.Estimator</code>  invoke the method in line <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L673\">679</a>,</p>\n<pre><code>eval_results, current_global_step = graph_actions.evaluate(\n          graph=g,\n          output_dir=eval_dir,\n          checkpoint_path=checkpoint_path,\n          eval_dict=eval_dict,\n          update_op=update_op,\n          global_step_tensor=global_step,\n          supervisor_master=self._config.master,\n          feed_fn=feed_fn,\n          max_steps=steps)\n\n</code></pre>\n<p>it did not set the value of <code>log_every_steps</code> and  the default value of   <code>log_every_steps</code> will be 10, so the exception will be raised when the validation sample size is small (or total evaluation steps is less than 10)</p>\n<p>When I set the hyper parameter <code>eval_batch_size</code> to a small value (eg. 2, which ensure total evaluation 32/2 = 16 &gt; 10), my code is ok</p>", "body_text": "Hello I am using tf.contrib.learn.Estimator to train my model, and size of validation sample is small,(validation sample size=32, eval_batch_size=16)\nI have created an EvaluationMonitor to evaluate the validation sample\nclass EvaluationMonitor(tf.contrib.learn.monitors.EveryN):\n        def every_n_step_end(self, step, outputs):\n            eval_results = self._estimator.evaluate(\n                input_fn=input_fn_eval,\n                metrics=eval_metrics,\n                steps=None)\n            print \"Evaluation Result: %s\" % eval_results\n\nand the eval_metrics is is a list of tf.contrib.metrics.streaming_sparse_recall_at_k, input_fn_eval is a function to read TFRecord from file\nbut in the evaluation step, I always get the exception:\n\nFile \"...../tensorflow/contrib/learn/python/learn/graph_actions.py\", line 610, in _eval_results_to_str\nreturn ', '.join('%s = %s' % (k, v) for k, v in eval_results.items())\n\nWhen i looks inside the evaluate method(Line 632- Line 779) in the source of graph_actions.py\ndef evaluate(graph,\n             output_dir,\n             checkpoint_path,\n             eval_dict,\n             update_op=None,\n             global_step_tensor=None,\n             supervisor_master='',\n             log_every_steps=10,\n             feed_fn=None,\n             max_steps=None):\n......\ntry:\n      try:\n        while (max_steps is None) or (step < max_steps):\n          step += 1\n          start_time = time.time()\n          feed_dict = feed_fn() if feed_fn is not None else None\n          if update_op is not None:\n            session.run(update_op, feed_dict=feed_dict)\n          else:\n            eval_results = session.run(eval_dict, feed_dict=feed_dict)\n            eval_step = step\n\n          if step % log_every_steps == 0:\n            if eval_step is None or step != eval_step:\n              eval_results = session.run(eval_dict, feed_dict=feed_dict)\n              eval_step = step\n            duration = time.time() - start_time\n            logging.info('Results after %d steps (%.3f sec/batch): %s.',\n                         step, float(duration),\n                         _eval_results_to_str(eval_results))\n      finally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n\n......\n\n # Save summaries for this evaluation.\n  _write_summary_results(output_dir, eval_results, current_global_step)\n\n\nwhen my code call the function,   the max step=2(validation sample size=32, eval_batch_size=16) will be less than the default parameter( log_every_steps=10), therefore the code only can run the evaluation in the finally block,\n ....\nfinally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n...\n\nbut for the metrics of ttf.contrib.metrics.streaming_sparse_recall_at_k, the update_op is not Not, which means the line\nif update_op is not None\n    session.run(update_op, feed_dict=feed_dict)\n\nwill alway be invoked, and the batched_data in the queue will be empty when the code jump into finally block, therefore an exception of OutOfRangeError will be raised, and eval_results will be None when the code execute at line\n_write_summary_results(output_dir, eval_results, current_global_step)\n\nI think it is caused by the default value of log_every_steps, when the _evaluate_model method in tf.contrib.learn.Estimator  invoke the method in line 679,\neval_results, current_global_step = graph_actions.evaluate(\n          graph=g,\n          output_dir=eval_dir,\n          checkpoint_path=checkpoint_path,\n          eval_dict=eval_dict,\n          update_op=update_op,\n          global_step_tensor=global_step,\n          supervisor_master=self._config.master,\n          feed_fn=feed_fn,\n          max_steps=steps)\n\n\nit did not set the value of log_every_steps and  the default value of   log_every_steps will be 10, so the exception will be raised when the validation sample size is small (or total evaluation steps is less than 10)\nWhen I set the hyper parameter eval_batch_size to a small value (eg. 2, which ensure total evaluation 32/2 = 16 > 10), my code is ok", "body": "Hello I am using `tf.contrib.learn.Estimator` to train my model, and size of validation sample is small,(validation sample size=32, eval_batch_size=16)\n\nI have created an EvaluationMonitor to evaluate the validation sample\n\n```\nclass EvaluationMonitor(tf.contrib.learn.monitors.EveryN):\n        def every_n_step_end(self, step, outputs):\n            eval_results = self._estimator.evaluate(\n                input_fn=input_fn_eval,\n                metrics=eval_metrics,\n                steps=None)\n            print \"Evaluation Result: %s\" % eval_results\n```\n\nand the `eval_metrics` is is a list of `tf.contrib.metrics.streaming_sparse_recall_at_k`, `input_fn_eval` is a function to read TFRecord from file\n\nbut in the evaluation step, I always get the exception:\n\n> File \"...../tensorflow/contrib/learn/python/learn/graph_actions.py\", line 610, in _eval_results_to_str\n>     return ', '.join('%s = %s' % (k, v) for k, v in eval_results.items())\n\nWhen i looks inside the evaluate method(Line 632- Line 779) in the source of [`graph_actions.py`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/graph_actions.py#L632)\n\n```\ndef evaluate(graph,\n             output_dir,\n             checkpoint_path,\n             eval_dict,\n             update_op=None,\n             global_step_tensor=None,\n             supervisor_master='',\n             log_every_steps=10,\n             feed_fn=None,\n             max_steps=None):\n......\ntry:\n      try:\n        while (max_steps is None) or (step < max_steps):\n          step += 1\n          start_time = time.time()\n          feed_dict = feed_fn() if feed_fn is not None else None\n          if update_op is not None:\n            session.run(update_op, feed_dict=feed_dict)\n          else:\n            eval_results = session.run(eval_dict, feed_dict=feed_dict)\n            eval_step = step\n\n          if step % log_every_steps == 0:\n            if eval_step is None or step != eval_step:\n              eval_results = session.run(eval_dict, feed_dict=feed_dict)\n              eval_step = step\n            duration = time.time() - start_time\n            logging.info('Results after %d steps (%.3f sec/batch): %s.',\n                         step, float(duration),\n                         _eval_results_to_str(eval_results))\n      finally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n\n......\n\n # Save summaries for this evaluation.\n  _write_summary_results(output_dir, eval_results, current_global_step)\n\n```\n\nwhen my code call the function,   the `max step=2`(validation sample size=32, eval_batch_size=16) will be less than the default parameter( `log_every_steps=10`), therefore the code only can run the evaluation in the `finally` block, \n\n```\n ....\nfinally:\n        if eval_results is None or step != eval_step:\n          eval_results = session.run(eval_dict, feed_dict=feed_dict)\n          eval_step = step\n...\n```\n\nbut for the metrics of t`tf.contrib.metrics.streaming_sparse_recall_at_k,` the `update_op` is not Not, which means the line\n\n```\nif update_op is not None\n    session.run(update_op, feed_dict=feed_dict)\n```\n\nwill alway be invoked, and the batched_data in the queue will be _empty_ when the code jump into `finally` block, therefore an exception of `OutOfRangeError` will be raised, and `eval_results` will be None when the code execute at line\n\n```\n_write_summary_results(output_dir, eval_results, current_global_step)\n```\n\nI think it is caused by the default value of `log_every_steps`, when the `_evaluate_model` method in `tf.contrib.learn.Estimator`  invoke the method in line [679](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/learn/python/learn/estimators/estimator.py#L673),\n\n```\neval_results, current_global_step = graph_actions.evaluate(\n          graph=g,\n          output_dir=eval_dir,\n          checkpoint_path=checkpoint_path,\n          eval_dict=eval_dict,\n          update_op=update_op,\n          global_step_tensor=global_step,\n          supervisor_master=self._config.master,\n          feed_fn=feed_fn,\n          max_steps=steps)\n\n```\n\nit did not set the value of `log_every_steps` and  the default value of   `log_every_steps` will be 10, so the exception will be raised when the validation sample size is small (or total evaluation steps is less than 10)\n\nWhen I set the hyper parameter `eval_batch_size` to a small value (eg. 2, which ensure total evaluation 32/2 = 16 > 10), my code is ok\n"}