{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23783", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23783/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23783/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23783/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23783", "id": 381365089, "node_id": "MDU6SXNzdWUzODEzNjUwODk=", "number": 23783, "title": "CudaRoot() during compilation should not be used ", "user": {"login": "ppwwyyxx", "id": 1381301, "node_id": "MDQ6VXNlcjEzODEzMDE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1381301?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ppwwyyxx", "html_url": "https://github.com/ppwwyyxx", "followers_url": "https://api.github.com/users/ppwwyyxx/followers", "following_url": "https://api.github.com/users/ppwwyyxx/following{/other_user}", "gists_url": "https://api.github.com/users/ppwwyyxx/gists{/gist_id}", "starred_url": "https://api.github.com/users/ppwwyyxx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ppwwyyxx/subscriptions", "organizations_url": "https://api.github.com/users/ppwwyyxx/orgs", "repos_url": "https://api.github.com/users/ppwwyyxx/repos", "events_url": "https://api.github.com/users/ppwwyyxx/events{/privacy}", "received_events_url": "https://api.github.com/users/ppwwyyxx/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-11-15T22:23:21Z", "updated_at": "2018-11-15T22:28:58Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a</li>\n<li>TensorFlow installed from (source or binary):binary</li>\n<li>TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0</li>\n<li>Python version:3.6</li>\n<li>Bazel version (if compiling from source):n/a</li>\n<li>GCC/Compiler version (if compiling from source):n/a</li>\n<li>CUDA/cuDNN version:9.0/7.2</li>\n<li>GPU model and memory:n/a</li>\n</ul>\n<p><strong>Describe the current behavior</strong><br>\nIt can be seen from <a href=\"https://github.com/tensorflow/tensorflow/search?q=CudaRoot&amp;unscoped_q=CudaRoot\">https://github.com/tensorflow/tensorflow/search?q=CudaRoot&amp;unscoped_q=CudaRoot</a>, that the <code>CudaRoot()</code> function is used by tensorflow to:</p>\n<ol>\n<li>Get the path to libdevice library</li>\n<li>Get the path to ptxas binary</li>\n</ol>\n<p>However, the return value of <code>CudaRoot()</code> function is a constant that's determined during compilation.<br>\nAs a result, tensorflow's XLA cannot work properly, if it is used on a machine where the path to CUDA is different from the compilation machine.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>The path to cuda should be dynamically obtained (by configuration or environment variables).</p>\n<p><strong>Code to reproduce the issue</strong><br>\nUsing the command in the end of <a href=\"https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473\" rel=\"nofollow\">https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473</a>, posted yesterday by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23486130\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tfboyd\">@tfboyd</a> .</p>\n<p><strong>Other info / logs</strong><br>\nWith XLA, saw errors like:</p>\n<pre><code>2018-11-15 14:07:40.696214: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:794] Failed to compile ptx to cubin.  Will attempt to let GPU driver compile the ptx. Not found: /usr/local/cuda\n-9.0/bin/ptxas not found\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\nTensorFlow installed from (source or binary):binary\nTensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0\nPython version:3.6\nBazel version (if compiling from source):n/a\nGCC/Compiler version (if compiling from source):n/a\nCUDA/cuDNN version:9.0/7.2\nGPU model and memory:n/a\n\nDescribe the current behavior\nIt can be seen from https://github.com/tensorflow/tensorflow/search?q=CudaRoot&unscoped_q=CudaRoot, that the CudaRoot() function is used by tensorflow to:\n\nGet the path to libdevice library\nGet the path to ptxas binary\n\nHowever, the return value of CudaRoot() function is a constant that's determined during compilation.\nAs a result, tensorflow's XLA cannot work properly, if it is used on a machine where the path to CUDA is different from the compilation machine.\nDescribe the expected behavior\nThe path to cuda should be dynamically obtained (by configuration or environment variables).\nCode to reproduce the issue\nUsing the command in the end of https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473, posted yesterday by @tfboyd .\nOther info / logs\nWith XLA, saw errors like:\n2018-11-15 14:07:40.696214: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:794] Failed to compile ptx to cubin.  Will attempt to let GPU driver compile the ptx. Not found: /usr/local/cuda\n-9.0/bin/ptxas not found", "body": "**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):no\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):linux\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:n/a\r\n- TensorFlow installed from (source or binary):binary\r\n- TensorFlow version (use command below):v1.12.0-0-ga6d8ffae09 1.12.0\r\n- Python version:3.6\r\n- Bazel version (if compiling from source):n/a\r\n- GCC/Compiler version (if compiling from source):n/a\r\n- CUDA/cuDNN version:9.0/7.2\r\n- GPU model and memory:n/a\r\n\r\n\r\n**Describe the current behavior**\r\nIt can be seen from https://github.com/tensorflow/tensorflow/search?q=CudaRoot&unscoped_q=CudaRoot, that the `CudaRoot()` function is used by tensorflow to:\r\n\r\n1. Get the path to libdevice library\r\n2. Get the path to ptxas binary \r\n\r\nHowever, the return value of `CudaRoot()` function is a constant that's determined during compilation.\r\nAs a result, tensorflow's XLA cannot work properly, if it is used on a machine where the path to CUDA is different from the compilation machine.\r\n\r\n**Describe the expected behavior**\r\n\r\nThe path to cuda should be dynamically obtained (by configuration or environment variables).\r\n\r\n**Code to reproduce the issue**\r\nUsing the command in the end of https://medium.com/tensorflow/pushing-the-limits-of-gpu-performance-with-xla-53559db8e473, posted yesterday by @tfboyd .\r\n\r\n**Other info / logs**\r\nWith XLA, saw errors like:\r\n```\r\n2018-11-15 14:07:40.696214: W tensorflow/compiler/xla/service/gpu/nvptx_compiler.cc:794] Failed to compile ptx to cubin.  Will attempt to let GPU driver compile the ptx. Not found: /usr/local/cuda\r\n-9.0/bin/ptxas not found\r\n```\r\n"}