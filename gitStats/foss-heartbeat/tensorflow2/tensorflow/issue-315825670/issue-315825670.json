{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18687", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18687/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18687/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18687/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18687", "id": 315825670, "node_id": "MDU6SXNzdWUzMTU4MjU2NzA=", "number": 18687, "title": "Absence of 'tanh()' operation in the computation of attention vector", "user": {"login": "Yunzhi-b", "id": 10114898, "node_id": "MDQ6VXNlcjEwMTE0ODk4", "avatar_url": "https://avatars2.githubusercontent.com/u/10114898?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Yunzhi-b", "html_url": "https://github.com/Yunzhi-b", "followers_url": "https://api.github.com/users/Yunzhi-b/followers", "following_url": "https://api.github.com/users/Yunzhi-b/following{/other_user}", "gists_url": "https://api.github.com/users/Yunzhi-b/gists{/gist_id}", "starred_url": "https://api.github.com/users/Yunzhi-b/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Yunzhi-b/subscriptions", "organizations_url": "https://api.github.com/users/Yunzhi-b/orgs", "repos_url": "https://api.github.com/users/Yunzhi-b/repos", "events_url": "https://api.github.com/users/Yunzhi-b/events{/privacy}", "received_events_url": "https://api.github.com/users/Yunzhi-b/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-04-19T10:52:18Z", "updated_at": "2018-04-26T10:23:31Z", "closed_at": "2018-04-26T10:23:31Z", "author_association": "NONE", "body_html": "<p>Tanh() operation is missed in the computation of attention vector (<a href=\"https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py\">https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py</a>), which is mentioned useful in the definition of Attetntion vector(<a href=\"https://www.tensorflow.org/tutorials/seq2seq\" rel=\"nofollow\">https://www.tensorflow.org/tutorials/seq2seq</a>).</p>\n<p>OS : Ubuntu 14.04.4 LTS<br>\nTensorFlow installed from: (<a href=\"https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl\" rel=\"nofollow\">https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl</a>)   via pip<br>\nTensorFlow version: tensorflow-gpu 1.4.1<br>\nCUDA/cuDNN version: 8.0<br>\nGPU model and memory: Tesla K80</p>\n<p>The problem is found when I compare the attention vector value, computed with Numpy and network's weights, to those retrieved from Attention Vector tensor. These two vectors are the same when I did not apply tanh(), meaning a tanh() opertion is missed with regards to the Attention vector's definition.</p>", "body_text": "Tanh() operation is missed in the computation of attention vector (https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py), which is mentioned useful in the definition of Attetntion vector(https://www.tensorflow.org/tutorials/seq2seq).\nOS : Ubuntu 14.04.4 LTS\nTensorFlow installed from: (https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl)   via pip\nTensorFlow version: tensorflow-gpu 1.4.1\nCUDA/cuDNN version: 8.0\nGPU model and memory: Tesla K80\nThe problem is found when I compare the attention vector value, computed with Numpy and network's weights, to those retrieved from Attention Vector tensor. These two vectors are the same when I did not apply tanh(), meaning a tanh() opertion is missed with regards to the Attention vector's definition.", "body": "Tanh() operation is missed in the computation of attention vector (https://github.com/tensorflow/tensorflow/blob/r1.7/tensorflow/contrib/seq2seq/python/ops/attention_wrapper.py), which is mentioned useful in the definition of Attetntion vector(https://www.tensorflow.org/tutorials/seq2seq).\r\n\r\nOS : Ubuntu 14.04.4 LTS\r\nTensorFlow installed from: (https://files.pythonhosted.org/packages/f7/52/4f78cc674775bd36a28223fd63d98260eb7130128298f7c70edbdcb34075/tensorflow-1.4.1-cp34-cp34m-manylinux1_x86_64.whl)   via pip \r\nTensorFlow version: tensorflow-gpu 1.4.1 \r\nCUDA/cuDNN version: 8.0\r\nGPU model and memory: Tesla K80\r\n\r\nThe problem is found when I compare the attention vector value, computed with Numpy and network's weights, to those retrieved from Attention Vector tensor. These two vectors are the same when I did not apply tanh(), meaning a tanh() opertion is missed with regards to the Attention vector's definition."}