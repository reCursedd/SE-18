{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/382837893", "html_url": "https://github.com/tensorflow/tensorflow/issues/18687#issuecomment-382837893", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18687", "id": 382837893, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MjgzNzg5Mw==", "user": {"login": "guillaumekln", "id": 4805513, "node_id": "MDQ6VXNlcjQ4MDU1MTM=", "avatar_url": "https://avatars3.githubusercontent.com/u/4805513?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guillaumekln", "html_url": "https://github.com/guillaumekln", "followers_url": "https://api.github.com/users/guillaumekln/followers", "following_url": "https://api.github.com/users/guillaumekln/following{/other_user}", "gists_url": "https://api.github.com/users/guillaumekln/gists{/gist_id}", "starred_url": "https://api.github.com/users/guillaumekln/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guillaumekln/subscriptions", "organizations_url": "https://api.github.com/users/guillaumekln/orgs", "repos_url": "https://api.github.com/users/guillaumekln/repos", "events_url": "https://api.github.com/users/guillaumekln/events{/privacy}", "received_events_url": "https://api.github.com/users/guillaumekln/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-19T18:35:28Z", "updated_at": "2018-04-19T18:35:28Z", "author_association": "CONTRIBUTOR", "body_html": "<p>This is fixed on the master branch with <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/0586c57292a7bd1a79b4a03270c0f1c32d02a4af/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/0586c57292a7bd1a79b4a03270c0f1c32d02a4af\"><tt>0586c57</tt></a>.</p>\n<p>The new <code>attention_layer</code> argument allows the use of an arbitrary layer to produce the attention vector. For example, to replicate the equation (3) from <a href=\"https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism\" rel=\"nofollow\">Background on the Attention Mechanism</a>:</p>\n<div class=\"highlight highlight-source-python\"><pre>cell <span class=\"pl-k\">=</span> tf.contrib.seq2seq.AttentionWrapper(\n    cell,\n    attention_mechanism,\n    <span class=\"pl-v\">initial_cell_state</span><span class=\"pl-k\">=</span>initial_cell_state,\n    <span class=\"pl-v\">attention_layer</span><span class=\"pl-k\">=</span>tf.layers.Dense(<span class=\"pl-c1\">512</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span>tf.tanh, <span class=\"pl-v\">use_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))</pre></div>", "body_text": "This is fixed on the master branch with 0586c57.\nThe new attention_layer argument allows the use of an arbitrary layer to produce the attention vector. For example, to replicate the equation (3) from Background on the Attention Mechanism:\ncell = tf.contrib.seq2seq.AttentionWrapper(\n    cell,\n    attention_mechanism,\n    initial_cell_state=initial_cell_state,\n    attention_layer=tf.layers.Dense(512, activation=tf.tanh, use_bias=False))", "body": "This is fixed on the master branch with 0586c572.\r\n\r\nThe new `attention_layer` argument allows the use of an arbitrary layer to produce the attention vector. For example, to replicate the equation (3) from [Background on the Attention Mechanism](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism):\r\n\r\n```python\r\ncell = tf.contrib.seq2seq.AttentionWrapper(\r\n    cell,\r\n    attention_mechanism,\r\n    initial_cell_state=initial_cell_state,\r\n    attention_layer=tf.layers.Dense(512, activation=tf.tanh, use_bias=False))\r\n```"}