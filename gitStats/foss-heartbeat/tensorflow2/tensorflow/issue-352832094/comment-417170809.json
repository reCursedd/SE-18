{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/417170809", "html_url": "https://github.com/tensorflow/tensorflow/issues/21787#issuecomment-417170809", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21787", "id": 417170809, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNzE3MDgwOQ==", "user": {"login": "jiarenyf", "id": 12405578, "node_id": "MDQ6VXNlcjEyNDA1NTc4", "avatar_url": "https://avatars2.githubusercontent.com/u/12405578?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jiarenyf", "html_url": "https://github.com/jiarenyf", "followers_url": "https://api.github.com/users/jiarenyf/followers", "following_url": "https://api.github.com/users/jiarenyf/following{/other_user}", "gists_url": "https://api.github.com/users/jiarenyf/gists{/gist_id}", "starred_url": "https://api.github.com/users/jiarenyf/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jiarenyf/subscriptions", "organizations_url": "https://api.github.com/users/jiarenyf/orgs", "repos_url": "https://api.github.com/users/jiarenyf/repos", "events_url": "https://api.github.com/users/jiarenyf/events{/privacy}", "received_events_url": "https://api.github.com/users/jiarenyf/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-30T02:40:22Z", "updated_at": "2018-08-30T02:58:49Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1162712\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/shashishekhar\">@shashishekhar</a> Thanks.</p>\n<p>I use the benchmark tool to test performance tf-mobile and tf-lite on <code>desktop</code>. And the performance of the tf-mobile and tf-lite are recoded in <code>benchmark_on_mobile.txt</code> and <code>benchmark_on_lite.txt</code>.</p>\n<pre><code>../../benchmark_model \\\n  --graph='./cnn+ctc.pb' \\\n  --input_layer='images_placeholder' \\\n  --input_layer_shape='1,48,480,1' \\\n  --input_layer_type='float' \\\n  --output_layer='Reshape_1' \\\n  --num_threads=4\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2334382/benchmark_on_mobile.txt\">benchmark_on_mobile.txt</a></p>\n<pre><code>../../benchmark_model_tflite \\\n  --graph='./cnn+ctc.tflite' \\\n  --input_layer='images_placeholder' \\\n  --input_layer_shape='1,48,480,1' \\\n  --num_threads=4\n</code></pre>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/2334381/benchmark_on_lite.txt\">benchmark_on_lite.txt</a></p>\n<p>I wonder:</p>\n<ol>\n<li>\n<p>In the <code>benchmark_on_lite.txt</code> there is <code>8 conv2d + 1 DEPTHWISE_CONV_2D</code>, while in <code>benchmark_on_mobile.txt</code> there is <code>9 conv2d</code>. I use the <code>pb</code> file to generate the <code>tflite</code> file, so why the network architecture is different ?</p>\n</li>\n<li>\n<p>In the <code>benchmark_on_mobile.txt</code>, <code>Conv2D</code> takes <code>305ms</code> and <code>MatMul</code> takes <code>36ms</code>, while in the <code>benchmark_on_tflite.txt</code>,  <code>Conv2D</code> takes <code>377ms</code> and <code>fully-connected</code> takes <code>382ms</code>. Why the time of <code>fc-layer</code> is different ?</p>\n</li>\n</ol>\n<p>All the files are provided here: <a href=\"https://share.weiyun.com/5t3ZFCX\" rel=\"nofollow\">All the files: *.txt, *.pb, *.tflie</a></p>\n<p>Thank you.</p>", "body_text": "@shashishekhar Thanks.\nI use the benchmark tool to test performance tf-mobile and tf-lite on desktop. And the performance of the tf-mobile and tf-lite are recoded in benchmark_on_mobile.txt and benchmark_on_lite.txt.\n../../benchmark_model \\\n  --graph='./cnn+ctc.pb' \\\n  --input_layer='images_placeholder' \\\n  --input_layer_shape='1,48,480,1' \\\n  --input_layer_type='float' \\\n  --output_layer='Reshape_1' \\\n  --num_threads=4\n\nbenchmark_on_mobile.txt\n../../benchmark_model_tflite \\\n  --graph='./cnn+ctc.tflite' \\\n  --input_layer='images_placeholder' \\\n  --input_layer_shape='1,48,480,1' \\\n  --num_threads=4\n\nbenchmark_on_lite.txt\nI wonder:\n\n\nIn the benchmark_on_lite.txt there is 8 conv2d + 1 DEPTHWISE_CONV_2D, while in benchmark_on_mobile.txt there is 9 conv2d. I use the pb file to generate the tflite file, so why the network architecture is different ?\n\n\nIn the benchmark_on_mobile.txt, Conv2D takes 305ms and MatMul takes 36ms, while in the benchmark_on_tflite.txt,  Conv2D takes 377ms and fully-connected takes 382ms. Why the time of fc-layer is different ?\n\n\nAll the files are provided here: All the files: *.txt, *.pb, *.tflie\nThank you.", "body": "@shashishekhar Thanks.\r\n\r\nI use the benchmark tool to test performance tf-mobile and tf-lite on `desktop`. And the performance of the tf-mobile and tf-lite are recoded in `benchmark_on_mobile.txt` and `benchmark_on_lite.txt`.\r\n\r\n```\r\n../../benchmark_model \\\r\n  --graph='./cnn+ctc.pb' \\\r\n  --input_layer='images_placeholder' \\\r\n  --input_layer_shape='1,48,480,1' \\\r\n  --input_layer_type='float' \\\r\n  --output_layer='Reshape_1' \\\r\n  --num_threads=4\r\n```\r\n\r\n[benchmark_on_mobile.txt](https://github.com/tensorflow/tensorflow/files/2334382/benchmark_on_mobile.txt)\r\n\r\n```\r\n../../benchmark_model_tflite \\\r\n  --graph='./cnn+ctc.tflite' \\\r\n  --input_layer='images_placeholder' \\\r\n  --input_layer_shape='1,48,480,1' \\\r\n  --num_threads=4\r\n```\r\n\r\n[benchmark_on_lite.txt](https://github.com/tensorflow/tensorflow/files/2334381/benchmark_on_lite.txt)\r\n\r\nI wonder:\r\n\r\n1. In the `benchmark_on_lite.txt` there is `8 conv2d + 1 DEPTHWISE_CONV_2D`, while in `benchmark_on_mobile.txt` there is `9 conv2d`. I use the `pb` file to generate the `tflite` file, so why the network architecture is different ?\r\n\r\n2. In the `benchmark_on_mobile.txt`, `Conv2D` takes `305ms` and `MatMul` takes `36ms`, while in the `benchmark_on_tflite.txt`,  `Conv2D` takes `377ms` and `fully-connected` takes `382ms`. Why the time of `fc-layer` is different ?\r\n\r\nAll the files are provided here: [All the files: *.txt, *.pb, *.tflie](https://share.weiyun.com/5t3ZFCX)\r\n\r\nThank you.\r\n"}