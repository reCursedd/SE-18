{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237335608", "html_url": "https://github.com/tensorflow/tensorflow/issues/367#issuecomment-237335608", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/367", "id": 237335608, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzMzNTYwOA==", "user": {"login": "harpone", "id": 5112840, "node_id": "MDQ6VXNlcjUxMTI4NDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/5112840?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harpone", "html_url": "https://github.com/harpone", "followers_url": "https://api.github.com/users/harpone/followers", "following_url": "https://api.github.com/users/harpone/following{/other_user}", "gists_url": "https://api.github.com/users/harpone/gists{/gist_id}", "starred_url": "https://api.github.com/users/harpone/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harpone/subscriptions", "organizations_url": "https://api.github.com/users/harpone/orgs", "repos_url": "https://api.github.com/users/harpone/repos", "events_url": "https://api.github.com/users/harpone/events{/privacy}", "received_events_url": "https://api.github.com/users/harpone/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-03T18:55:15Z", "updated_at": "2016-08-03T18:55:15Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7140902\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/RuiShu\">@RuiShu</a> (and others): are you aware that <code>grad(logdet(A)) = transpose(inv(A))</code>? Unless it's actually faster to compute the grad of the cholesky decomp (and to the other operations), then IMO it might make sense to use the cholesky decomp in getting the <em>value</em> of the logdet and to use the inverse-transpose form for the gradient?</p>", "body_text": "@RuiShu (and others): are you aware that grad(logdet(A)) = transpose(inv(A))? Unless it's actually faster to compute the grad of the cholesky decomp (and to the other operations), then IMO it might make sense to use the cholesky decomp in getting the value of the logdet and to use the inverse-transpose form for the gradient?", "body": "@RuiShu (and others): are you aware that `grad(logdet(A)) = transpose(inv(A))`? Unless it's actually faster to compute the grad of the cholesky decomp (and to the other operations), then IMO it might make sense to use the cholesky decomp in getting the _value_ of the logdet and to use the inverse-transpose form for the gradient?\n"}