{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21811", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21811/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21811/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21811/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21811", "id": 353140813, "node_id": "MDU6SXNzdWUzNTMxNDA4MTM=", "number": 21811, "title": "Estimators created from Keras models expect unconsumed outputs to be provided with data in training/evaluation.", "user": {"login": "zmjjmz", "id": 1694612, "node_id": "MDQ6VXNlcjE2OTQ2MTI=", "avatar_url": "https://avatars2.githubusercontent.com/u/1694612?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zmjjmz", "html_url": "https://github.com/zmjjmz", "followers_url": "https://api.github.com/users/zmjjmz/followers", "following_url": "https://api.github.com/users/zmjjmz/following{/other_user}", "gists_url": "https://api.github.com/users/zmjjmz/gists{/gist_id}", "starred_url": "https://api.github.com/users/zmjjmz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zmjjmz/subscriptions", "organizations_url": "https://api.github.com/users/zmjjmz/orgs", "repos_url": "https://api.github.com/users/zmjjmz/repos", "events_url": "https://api.github.com/users/zmjjmz/events{/privacy}", "received_events_url": "https://api.github.com/users/zmjjmz/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-22T21:56:25Z", "updated_at": "2018-11-13T18:56:46Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Debian Jessie</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9</li>\n<li><strong>Python version</strong>: 2.7.9</li>\n<li><strong>Exact command to reproduce</strong>: See gist <a href=\"https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6\">here</a></li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Mobile device</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Sorry, this is a bit arcane/edge-casey. Basically I have a Keras model that I want to convert into an Estimator for use with the <code>train_and_evaluate</code> framework. One aspect of these models is that I have layers that are outputs but do not contribute to the loss that are part of the deployed model's contract -- e.g. an <code>oov_code</code> that indicates if the input (text) was out-of-vocabulary.</p>\n<p>The way I've been doing this <em>before</em> switching to a <code>keras.model_to_estimator</code> method was to have the Keras model's loss and metrics be set to just the parts contribute to the loss in the <code>ModeKeys.TRAIN</code> or <code>ModeKeys.EVAL</code> modes and then have the <code>export_outputs</code> argument take all of them in the <code>PREDICT</code> mode, e.g. in this <a href=\"https://gist.github.com/zmjjmz/b84248a8a1ff21afd1c172e0abbba2e8\">gist</a>. I can't do this fine-grained thing in <code>model_to_estimator</code>, which is sort of the first problem.</p>\n<p>However, theoretically the <code>Model.compile</code> function allows for the <code>loss</code> and <code>metrics</code> arguments to be dictionaries of output names (derived from the layer names) to their respective loss / metric functions. When using <code>Model.fit</code>, one can (as in line 49 of the provided reproduction) provide a dictionary of <code>output:labels</code> which are then appropriately routed to their respective losses / metrics (as can be seen by running it - note the <code>classes_accuracy</code> and <code>classes_loss</code> printouts.</p>\n<p>Unfortunately, when using <code>model_to_estimator</code> followed by <code>train_and_evaluate</code> my eternal enemy <code>_create_ordered_io</code> expects to find a value in <code>y</code> for all outputs, irrespective of the <code>loss</code> and <code>metrics</code> not specifying any such requirements.</p>\n<p>Confusingly I am promised separately that <code>we will not be expecting any data to be passed to \"reporting\" during training.</code> as shown in the provided logs.</p>\n<p>Basically, it would be nice if <code>_create_ordered_io</code> had some way to 'know' that these outputs should be ignored for these steps, OR for a way to specify different outputs according to the <code>ModeKeys</code> in <code>model_to_estimator</code>. I can workaround this at the moment by providing a fake output in the <code>numpy_input_fn</code>'s <code>y</code> argument (e.g. what happens if you set <code>incl_reporting=True</code> in the <code>get_input_fn</code> function in the reproduction script), however this is really not ideal for a variety of reasons.</p>\n<h3>Source code / logs</h3>\n<p>The gist provided <a href=\"https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6\">here</a> has both reproduction code and comments with the outputs.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Debian Jessie\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.9\nPython version: 2.7.9\nExact command to reproduce: See gist here\nBazel version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nMobile device: N/A\n\nDescribe the problem\nSorry, this is a bit arcane/edge-casey. Basically I have a Keras model that I want to convert into an Estimator for use with the train_and_evaluate framework. One aspect of these models is that I have layers that are outputs but do not contribute to the loss that are part of the deployed model's contract -- e.g. an oov_code that indicates if the input (text) was out-of-vocabulary.\nThe way I've been doing this before switching to a keras.model_to_estimator method was to have the Keras model's loss and metrics be set to just the parts contribute to the loss in the ModeKeys.TRAIN or ModeKeys.EVAL modes and then have the export_outputs argument take all of them in the PREDICT mode, e.g. in this gist. I can't do this fine-grained thing in model_to_estimator, which is sort of the first problem.\nHowever, theoretically the Model.compile function allows for the loss and metrics arguments to be dictionaries of output names (derived from the layer names) to their respective loss / metric functions. When using Model.fit, one can (as in line 49 of the provided reproduction) provide a dictionary of output:labels which are then appropriately routed to their respective losses / metrics (as can be seen by running it - note the classes_accuracy and classes_loss printouts.\nUnfortunately, when using model_to_estimator followed by train_and_evaluate my eternal enemy _create_ordered_io expects to find a value in y for all outputs, irrespective of the loss and metrics not specifying any such requirements.\nConfusingly I am promised separately that we will not be expecting any data to be passed to \"reporting\" during training. as shown in the provided logs.\nBasically, it would be nice if _create_ordered_io had some way to 'know' that these outputs should be ignored for these steps, OR for a way to specify different outputs according to the ModeKeys in model_to_estimator. I can workaround this at the moment by providing a fake output in the numpy_input_fn's y argument (e.g. what happens if you set incl_reporting=True in the get_input_fn function in the reproduction script), however this is really not ideal for a variety of reasons.\nSource code / logs\nThe gist provided here has both reproduction code and comments with the outputs.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Debian Jessie\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.9\r\n- **Python version**: 2.7.9\r\n- **Exact command to reproduce**: See gist [here](https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6)\r\n- **Bazel version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Mobile device**: N/A\r\n\r\n### Describe the problem\r\nSorry, this is a bit arcane/edge-casey. Basically I have a Keras model that I want to convert into an Estimator for use with the `train_and_evaluate` framework. One aspect of these models is that I have layers that are outputs but do not contribute to the loss that are part of the deployed model's contract -- e.g. an `oov_code` that indicates if the input (text) was out-of-vocabulary. \r\n\r\nThe way I've been doing this _before_ switching to a `keras.model_to_estimator` method was to have the Keras model's loss and metrics be set to just the parts contribute to the loss in the `ModeKeys.TRAIN` or `ModeKeys.EVAL` modes and then have the `export_outputs` argument take all of them in the `PREDICT` mode, e.g. in this [gist](https://gist.github.com/zmjjmz/b84248a8a1ff21afd1c172e0abbba2e8). I can't do this fine-grained thing in `model_to_estimator`, which is sort of the first problem.\r\n\r\nHowever, theoretically the `Model.compile` function allows for the `loss` and `metrics` arguments to be dictionaries of output names (derived from the layer names) to their respective loss / metric functions. When using `Model.fit`, one can (as in line 49 of the provided reproduction) provide a dictionary of `output:labels` which are then appropriately routed to their respective losses / metrics (as can be seen by running it - note the `classes_accuracy` and `classes_loss` printouts.\r\n\r\nUnfortunately, when using `model_to_estimator` followed by `train_and_evaluate` my eternal enemy `_create_ordered_io` expects to find a value in `y` for all outputs, irrespective of the `loss` and `metrics` not specifying any such requirements. \r\n\r\nConfusingly I am promised separately that `we will not be expecting any data to be passed to \"reporting\" during training.` as shown in the provided logs.\r\n\r\nBasically, it would be nice if `_create_ordered_io` had some way to 'know' that these outputs should be ignored for these steps, OR for a way to specify different outputs according to the `ModeKeys` in `model_to_estimator`. I can workaround this at the moment by providing a fake output in the `numpy_input_fn`'s `y` argument (e.g. what happens if you set `incl_reporting=True` in the `get_input_fn` function in the reproduction script), however this is really not ideal for a variety of reasons.\r\n\r\n### Source code / logs\r\nThe gist provided [here](https://gist.github.com/zmjjmz/3f621aaafc5683238ade6224e5dedcb6) has both reproduction code and comments with the outputs.\r\n"}