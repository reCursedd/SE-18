{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316360545", "html_url": "https://github.com/tensorflow/tensorflow/issues/6268#issuecomment-316360545", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6268", "id": 316360545, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjM2MDU0NQ==", "user": {"login": "fferroni", "id": 16327442, "node_id": "MDQ6VXNlcjE2MzI3NDQy", "avatar_url": "https://avatars1.githubusercontent.com/u/16327442?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fferroni", "html_url": "https://github.com/fferroni", "followers_url": "https://api.github.com/users/fferroni/followers", "following_url": "https://api.github.com/users/fferroni/following{/other_user}", "gists_url": "https://api.github.com/users/fferroni/gists{/gist_id}", "starred_url": "https://api.github.com/users/fferroni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fferroni/subscriptions", "organizations_url": "https://api.github.com/users/fferroni/orgs", "repos_url": "https://api.github.com/users/fferroni/repos", "events_url": "https://api.github.com/users/fferroni/events{/privacy}", "received_events_url": "https://api.github.com/users/fferroni/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-19T11:48:06Z", "updated_at": "2017-07-19T11:48:06Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a> Hi, I am trying to find ways of getting gradients for the various variables in my network, so I can use these with ApplyGradientDescent or similar. This is for example how I define a single layer MLP:</p>\n<p>I am a bit lost with the various C or C++ APIs. I just relied on the r1.2 API guide for the ops etc. I presume that currently it's possible to define an optimization routine?</p>\n<p>Thanks!</p>\n<pre><code>tensorflow::Scope *root;\n...\nstd::string scope = \"test\";\nauto curr = root-&gt;NewSubScope(scope);\n\n// input\nauto input_scope = curr.NewSubScope(\"input\");\nauto state_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"state\"), tensorflow::DT_FLOAT);\nauto action_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"action\"), tensorflow::DT_FLOAT);\n\n// operations\n// layer 1\nint layer_1_neurons = 400;\nauto layer_1_scope = curr.NewSubScope(\"dense_1\");\nauto W1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"W\"), {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT);\nvars_to_fetch.push_back(scope+\"/dense_1/W\");\nauto assignW1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"W_init\"), W1, tensorflow::ops::TruncatedNormal(layer_1_scope, {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT));\nvar_init_list.push_back(scope+\"/dense_1/W_init\");\nauto b1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"b\"), {layer_1_neurons}, tensorflow::DT_FLOAT);\nvars_to_fetch.push_back(scope+\"/dense_1/b\");\nauto assignb1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"b_init\"), b1, tensorflow::ops::ZerosLike(layer_1_scope, tensorflow::Tensor(tensorflow::DT_FLOAT, {layer_1_neurons})));\nvar_init_list.push_back(scope+\"/dense_1/b_init\");\nauto m1 = tensorflow::ops::MatMul(layer_1_scope, state_placeholder, W1);\nauto a1 = tensorflow::ops::BiasAdd(layer_1_scope, m1, b1);\n</code></pre>", "body_text": "@suharshs Hi, I am trying to find ways of getting gradients for the various variables in my network, so I can use these with ApplyGradientDescent or similar. This is for example how I define a single layer MLP:\nI am a bit lost with the various C or C++ APIs. I just relied on the r1.2 API guide for the ops etc. I presume that currently it's possible to define an optimization routine?\nThanks!\ntensorflow::Scope *root;\n...\nstd::string scope = \"test\";\nauto curr = root->NewSubScope(scope);\n\n// input\nauto input_scope = curr.NewSubScope(\"input\");\nauto state_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"state\"), tensorflow::DT_FLOAT);\nauto action_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"action\"), tensorflow::DT_FLOAT);\n\n// operations\n// layer 1\nint layer_1_neurons = 400;\nauto layer_1_scope = curr.NewSubScope(\"dense_1\");\nauto W1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"W\"), {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT);\nvars_to_fetch.push_back(scope+\"/dense_1/W\");\nauto assignW1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"W_init\"), W1, tensorflow::ops::TruncatedNormal(layer_1_scope, {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT));\nvar_init_list.push_back(scope+\"/dense_1/W_init\");\nauto b1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"b\"), {layer_1_neurons}, tensorflow::DT_FLOAT);\nvars_to_fetch.push_back(scope+\"/dense_1/b\");\nauto assignb1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"b_init\"), b1, tensorflow::ops::ZerosLike(layer_1_scope, tensorflow::Tensor(tensorflow::DT_FLOAT, {layer_1_neurons})));\nvar_init_list.push_back(scope+\"/dense_1/b_init\");\nauto m1 = tensorflow::ops::MatMul(layer_1_scope, state_placeholder, W1);\nauto a1 = tensorflow::ops::BiasAdd(layer_1_scope, m1, b1);", "body": "@suharshs Hi, I am trying to find ways of getting gradients for the various variables in my network, so I can use these with ApplyGradientDescent or similar. This is for example how I define a single layer MLP:\r\n\r\nI am a bit lost with the various C or C++ APIs. I just relied on the r1.2 API guide for the ops etc. I presume that currently it's possible to define an optimization routine?\r\n\r\nThanks!\r\n\r\n```\r\ntensorflow::Scope *root;\r\n...\r\nstd::string scope = \"test\";\r\nauto curr = root->NewSubScope(scope);\r\n\r\n// input\r\nauto input_scope = curr.NewSubScope(\"input\");\r\nauto state_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"state\"), tensorflow::DT_FLOAT);\r\nauto action_placeholder = tensorflow::ops::Placeholder(input_scope.WithOpName(\"action\"), tensorflow::DT_FLOAT);\r\n\r\n// operations\r\n// layer 1\r\nint layer_1_neurons = 400;\r\nauto layer_1_scope = curr.NewSubScope(\"dense_1\");\r\nauto W1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"W\"), {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT);\r\nvars_to_fetch.push_back(scope+\"/dense_1/W\");\r\nauto assignW1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"W_init\"), W1, tensorflow::ops::TruncatedNormal(layer_1_scope, {state_dim, layer_1_neurons}, tensorflow::DT_FLOAT));\r\nvar_init_list.push_back(scope+\"/dense_1/W_init\");\r\nauto b1 = tensorflow::ops::Variable(layer_1_scope.WithOpName(\"b\"), {layer_1_neurons}, tensorflow::DT_FLOAT);\r\nvars_to_fetch.push_back(scope+\"/dense_1/b\");\r\nauto assignb1 = tensorflow::ops::Assign(layer_1_scope.WithOpName(\"b_init\"), b1, tensorflow::ops::ZerosLike(layer_1_scope, tensorflow::Tensor(tensorflow::DT_FLOAT, {layer_1_neurons})));\r\nvar_init_list.push_back(scope+\"/dense_1/b_init\");\r\nauto m1 = tensorflow::ops::MatMul(layer_1_scope, state_placeholder, W1);\r\nauto a1 = tensorflow::ops::BiasAdd(layer_1_scope, m1, b1);\r\n```"}