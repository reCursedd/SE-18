{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300225381", "html_url": "https://github.com/tensorflow/tensorflow/issues/9579#issuecomment-300225381", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9579", "id": 300225381, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDIyNTM4MQ==", "user": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-09T16:40:02Z", "updated_at": "2017-05-09T16:40:02Z", "author_association": "MEMBER", "body_html": "<p>I think this is working as intended. By default, only the GPU JIT will be enabled by the --xla flag to that script. We do not enable the CPU JIT by default at the moment because it does not support inter-operator parallelism. And you do not have a GPU installed or are not compiling with CUDA.</p>\n<p>If you wish to enable XLA on the CPU, you will need to modify the mnist_softmax_xla.py script to use a <code>with tf.device(\"device:XLA_CPU:0\"):</code> scope to force CPU operators to be compiled with XLA. The \"Placing operators on XLA devices\" section of the tutorial describes how to do this.</p>\n<p>(I cannot speak to what is happening with your custom \"Xpu\" device --- if I cannot see the source code, there is no way for me to debug it.)</p>", "body_text": "I think this is working as intended. By default, only the GPU JIT will be enabled by the --xla flag to that script. We do not enable the CPU JIT by default at the moment because it does not support inter-operator parallelism. And you do not have a GPU installed or are not compiling with CUDA.\nIf you wish to enable XLA on the CPU, you will need to modify the mnist_softmax_xla.py script to use a with tf.device(\"device:XLA_CPU:0\"): scope to force CPU operators to be compiled with XLA. The \"Placing operators on XLA devices\" section of the tutorial describes how to do this.\n(I cannot speak to what is happening with your custom \"Xpu\" device --- if I cannot see the source code, there is no way for me to debug it.)", "body": "I think this is working as intended. By default, only the GPU JIT will be enabled by the --xla flag to that script. We do not enable the CPU JIT by default at the moment because it does not support inter-operator parallelism. And you do not have a GPU installed or are not compiling with CUDA.\r\n\r\nIf you wish to enable XLA on the CPU, you will need to modify the mnist_softmax_xla.py script to use a `with tf.device(\"device:XLA_CPU:0\"):` scope to force CPU operators to be compiled with XLA. The \"Placing operators on XLA devices\" section of the tutorial describes how to do this.\r\n\r\n(I cannot speak to what is happening with your custom \"Xpu\" device --- if I cannot see the source code, there is no way for me to debug it.)"}