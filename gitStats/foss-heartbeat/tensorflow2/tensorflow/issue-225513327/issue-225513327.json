{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9579", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9579/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9579/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9579/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9579", "id": 225513327, "node_id": "MDU6SXNzdWUyMjU1MTMzMjc=", "number": 9579, "title": "MNIST Tutorial Appears to Not Toggle XLA Compilation", "user": {"login": "lwogulis", "id": 13294147, "node_id": "MDQ6VXNlcjEzMjk0MTQ3", "avatar_url": "https://avatars0.githubusercontent.com/u/13294147?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lwogulis", "html_url": "https://github.com/lwogulis", "followers_url": "https://api.github.com/users/lwogulis/followers", "following_url": "https://api.github.com/users/lwogulis/following{/other_user}", "gists_url": "https://api.github.com/users/lwogulis/gists{/gist_id}", "starred_url": "https://api.github.com/users/lwogulis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lwogulis/subscriptions", "organizations_url": "https://api.github.com/users/lwogulis/orgs", "repos_url": "https://api.github.com/users/lwogulis/repos", "events_url": "https://api.github.com/users/lwogulis/events{/privacy}", "received_events_url": "https://api.github.com/users/lwogulis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "hawkinsp", "id": 348932, "node_id": "MDQ6VXNlcjM0ODkzMg==", "avatar_url": "https://avatars0.githubusercontent.com/u/348932?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hawkinsp", "html_url": "https://github.com/hawkinsp", "followers_url": "https://api.github.com/users/hawkinsp/followers", "following_url": "https://api.github.com/users/hawkinsp/following{/other_user}", "gists_url": "https://api.github.com/users/hawkinsp/gists{/gist_id}", "starred_url": "https://api.github.com/users/hawkinsp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hawkinsp/subscriptions", "organizations_url": "https://api.github.com/users/hawkinsp/orgs", "repos_url": "https://api.github.com/users/hawkinsp/repos", "events_url": "https://api.github.com/users/hawkinsp/events{/privacy}", "received_events_url": "https://api.github.com/users/hawkinsp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 24, "created_at": "2017-05-01T20:04:20Z", "updated_at": "2018-08-06T14:27:19Z", "closed_at": "2018-08-06T14:27:19Z", "author_association": "NONE", "body_html": "<p>I have been using the JIT compilation/XLA tutorial (<a href=\"https://www.tensorflow.org/performance/xla/jit#step_3_run_with_xla\" rel=\"nofollow\">https://www.tensorflow.org/performance/xla/jit#step_3_run_with_xla</a>), and it seems that whether or not XLA compilation happens doesn't depend on the statement on line 63 in mnist_softmax_xla.py. The comment above it says that line will turn on XLA JIT compilation. When I run with the two options explained on the page (--xla='' for compiling without XLA, and TF_XLA_FLAGS=--xla_generate_hlo_graph=.* for compiling with XLA), the second executes line 63 and the first does not. But, both seem to compute in exactly the same way, going through compiler/xla/service. The output for both runs is:</p>\n<pre><code>Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n2017-05-01 12:50:25.203870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-01 12:50:25.203904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-01 12:50:25.232661: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-01 12:50:25.232702: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\n2017-05-01 12:50:25.233155: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n2017-05-01 12:50:25.233165: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.233894: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-01 12:50:25.233903: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\n2017-05-01 12:50:25.234360: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Xpu. Devices:\n2017-05-01 12:50:25.234369: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234372: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (1): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234376: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (2): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234379: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (3): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234382: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (4): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234385: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (5): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234389: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (6): &lt;undefined&gt;, &lt;undefined&gt;\n2017-05-01 12:50:25.234392: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (7): &lt;undefined&gt;, &lt;undefined&gt;\n0.9206\n</code></pre>", "body_text": "I have been using the JIT compilation/XLA tutorial (https://www.tensorflow.org/performance/xla/jit#step_3_run_with_xla), and it seems that whether or not XLA compilation happens doesn't depend on the statement on line 63 in mnist_softmax_xla.py. The comment above it says that line will turn on XLA JIT compilation. When I run with the two options explained on the page (--xla='' for compiling without XLA, and TF_XLA_FLAGS=--xla_generate_hlo_graph=.* for compiling with XLA), the second executes line 63 and the first does not. But, both seem to compute in exactly the same way, going through compiler/xla/service. The output for both runs is:\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n2017-05-01 12:50:25.203870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-01 12:50:25.203904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n2017-05-01 12:50:25.232661: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-01 12:50:25.232702: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\n2017-05-01 12:50:25.233155: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\n2017-05-01 12:50:25.233165: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n2017-05-01 12:50:25.233894: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\n2017-05-01 12:50:25.233903: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\n2017-05-01 12:50:25.234360: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Xpu. Devices:\n2017-05-01 12:50:25.234369: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\n2017-05-01 12:50:25.234372: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (1): <undefined>, <undefined>\n2017-05-01 12:50:25.234376: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (2): <undefined>, <undefined>\n2017-05-01 12:50:25.234379: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (3): <undefined>, <undefined>\n2017-05-01 12:50:25.234382: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (4): <undefined>, <undefined>\n2017-05-01 12:50:25.234385: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (5): <undefined>, <undefined>\n2017-05-01 12:50:25.234389: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (6): <undefined>, <undefined>\n2017-05-01 12:50:25.234392: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (7): <undefined>, <undefined>\n0.9206", "body": "I have been using the JIT compilation/XLA tutorial (https://www.tensorflow.org/performance/xla/jit#step_3_run_with_xla), and it seems that whether or not XLA compilation happens doesn't depend on the statement on line 63 in mnist_softmax_xla.py. The comment above it says that line will turn on XLA JIT compilation. When I run with the two options explained on the page (--xla='' for compiling without XLA, and TF_XLA_FLAGS=--xla_generate_hlo_graph=.* for compiling with XLA), the second executes line 63 and the first does not. But, both seem to compute in exactly the same way, going through compiler/xla/service. The output for both runs is:\r\n```\r\nExtracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\r\nExtracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\r\n2017-05-01 12:50:25.203870: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-01 12:50:25.203904: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\r\n2017-05-01 12:50:25.232661: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-01 12:50:25.232702: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\r\n2017-05-01 12:50:25.233155: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Host. Devices:\r\n2017-05-01 12:50:25.233165: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-01 12:50:25.233894: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Host present with 8 visible devices\r\n2017-05-01 12:50:25.233903: I tensorflow/compiler/xla/service/platform_util.cc:58] platform Xpu present with 8 visible devices\r\n2017-05-01 12:50:25.234360: I tensorflow/compiler/xla/service/service.cc:180] XLA service executing computations on platform Xpu. Devices:\r\n2017-05-01 12:50:25.234369: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (0): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234372: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (1): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234376: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (2): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234379: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (3): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234382: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (4): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234385: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (5): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234389: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (6): <undefined>, <undefined>\r\n2017-05-01 12:50:25.234392: I tensorflow/compiler/xla/service/service.cc:187]   StreamExecutor device (7): <undefined>, <undefined>\r\n0.9206\r\n```"}