{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2859", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2859/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2859/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2859/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2859", "id": 160254230, "node_id": "MDU6SXNzdWUxNjAyNTQyMzA=", "number": 2859, "title": "tf.train.exponential_decay examples use 32 bit number for batch", "user": {"login": "jwise", "id": 87427, "node_id": "MDQ6VXNlcjg3NDI3", "avatar_url": "https://avatars3.githubusercontent.com/u/87427?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jwise", "html_url": "https://github.com/jwise", "followers_url": "https://api.github.com/users/jwise/followers", "following_url": "https://api.github.com/users/jwise/following{/other_user}", "gists_url": "https://api.github.com/users/jwise/gists{/gist_id}", "starred_url": "https://api.github.com/users/jwise/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jwise/subscriptions", "organizations_url": "https://api.github.com/users/jwise/orgs", "repos_url": "https://api.github.com/users/jwise/repos", "events_url": "https://api.github.com/users/jwise/events{/privacy}", "received_events_url": "https://api.github.com/users/jwise/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-06-14T18:41:44Z", "updated_at": "2016-06-14T20:54:48Z", "closed_at": "2016-06-14T20:48:01Z", "author_association": "NONE", "body_html": "<p>As far as I can tell, <code>tf.train.exponential_decay</code> examples seem to use a 32-bit signed number for global_step, because the batch number is also a <code>tf.int32</code>.  This means that long runs can result in <a href=\"http://nyus.joshuawise.com/fml-tensorflow.png\" rel=\"nofollow\">unpleasant surprises</a> with learning rates, which result in a frustrating experience for new users. [1]  Examples should be updated to initialize <code>batch</code> as a <code>dtype=tf.int64</code>.</p>\n<p>I originally believed this to be a <code>tf.train.exponential_decay</code> bug, and wrote some code to minimize the issue.  I now understand that this comes from the variable, but you can have the below repro case anyway, because I think it makes it a little more obvious as to the kind of thing that can happen.  When you run it, you'll note a discontinuity between epoch 85 and 86 in learning rate that comes from <code>batch * batch_size</code> overflowing (and results in losing an evening's training...).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>(<span class=\"pl-smi\">argv</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n  epoch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">25000000</span>\n  epoches <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span>\n  batch_size <span class=\"pl-k\">=</span> <span class=\"pl-c1\">16384</span>\n\n  batch <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span> &lt;-- this line, in examples, should be changed to have dtype=tf.int64, with explanation of why</span>\n  learning_rate <span class=\"pl-k\">=</span> tf.train.exponential_decay(\n      <span class=\"pl-c1\">0.01</span>,\n      batch <span class=\"pl-k\">*</span> batch_size,\n      epoch_size,\n      <span class=\"pl-c1\">0.95</span>,\n      <span class=\"pl-v\">staircase</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>)\n  loss <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">0.0</span>)\n  eval_frequency <span class=\"pl-k\">=</span> <span class=\"pl-c1\">int</span>(epoch_size <span class=\"pl-k\">/</span> batch_size <span class=\"pl-k\">/</span> <span class=\"pl-c1\">2</span>)\n\n  optimizer <span class=\"pl-k\">=</span> tf.train.MomentumOptimizer(learning_rate, <span class=\"pl-c1\">0.9</span>) \\\n     .minimize(loss, <span class=\"pl-v\">global_step</span> <span class=\"pl-k\">=</span> batch)\n\n  <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    tf.initialize_all_variables().run()\n\n    <span class=\"pl-k\">for</span> step <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(<span class=\"pl-c1\">int</span>(epoches <span class=\"pl-k\">*</span> epoch_size) <span class=\"pl-k\">//</span> batch_size):\n      _, b, lr <span class=\"pl-k\">=</span> sess.run([optimizer, batch, learning_rate])\n      <span class=\"pl-k\">if</span> step <span class=\"pl-k\">%</span> eval_frequency <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Step <span class=\"pl-c1\">%d</span> (epoch <span class=\"pl-c1\">%.2f</span>): batch <span class=\"pl-c1\">%d</span>, learning rate <span class=\"pl-c1\">%.6f</span><span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span>\n            (step, <span class=\"pl-c1\">float</span>(step) <span class=\"pl-k\">*</span> batch_size <span class=\"pl-k\">/</span> epoch_size, b, lr))\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n  tf.app.run()</pre></div>\n<p><em>[1] No, I did not checkpoint midway through.  Yes, I have learned my lesson.</em></p>", "body_text": "As far as I can tell, tf.train.exponential_decay examples seem to use a 32-bit signed number for global_step, because the batch number is also a tf.int32.  This means that long runs can result in unpleasant surprises with learning rates, which result in a frustrating experience for new users. [1]  Examples should be updated to initialize batch as a dtype=tf.int64.\nI originally believed this to be a tf.train.exponential_decay bug, and wrote some code to minimize the issue.  I now understand that this comes from the variable, but you can have the below repro case anyway, because I think it makes it a little more obvious as to the kind of thing that can happen.  When you run it, you'll note a discontinuity between epoch 85 and 86 in learning rate that comes from batch * batch_size overflowing (and results in losing an evening's training...).\nimport tensorflow as tf\n\ndef main(argv=None):\n  epoch_size = 25000000\n  epoches = 100\n  batch_size = 16384\n\n  batch = tf.Variable(0) # <-- this line, in examples, should be changed to have dtype=tf.int64, with explanation of why\n  learning_rate = tf.train.exponential_decay(\n      0.01,\n      batch * batch_size,\n      epoch_size,\n      0.95,\n      staircase = True)\n  loss = tf.Variable(0.0)\n  eval_frequency = int(epoch_size / batch_size / 2)\n\n  optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9) \\\n     .minimize(loss, global_step = batch)\n\n  with tf.Session() as sess:\n    tf.initialize_all_variables().run()\n\n    for step in xrange(int(epoches * epoch_size) // batch_size):\n      _, b, lr = sess.run([optimizer, batch, learning_rate])\n      if step % eval_frequency == 0:\n        print(\"Step %d (epoch %.2f): batch %d, learning rate %.6f\" %\n            (step, float(step) * batch_size / epoch_size, b, lr))\n\nif __name__ == '__main__':\n  tf.app.run()\n[1] No, I did not checkpoint midway through.  Yes, I have learned my lesson.", "body": "As far as I can tell, `tf.train.exponential_decay` examples seem to use a 32-bit signed number for global_step, because the batch number is also a `tf.int32`.  This means that long runs can result in [unpleasant surprises](http://nyus.joshuawise.com/fml-tensorflow.png) with learning rates, which result in a frustrating experience for new users. [1]  Examples should be updated to initialize `batch` as a `dtype=tf.int64`.\n\nI originally believed this to be a `tf.train.exponential_decay` bug, and wrote some code to minimize the issue.  I now understand that this comes from the variable, but you can have the below repro case anyway, because I think it makes it a little more obvious as to the kind of thing that can happen.  When you run it, you'll note a discontinuity between epoch 85 and 86 in learning rate that comes from `batch * batch_size` overflowing (and results in losing an evening's training...).\n\n``` python\n\nimport tensorflow as tf\n\ndef main(argv=None):\n  epoch_size = 25000000\n  epoches = 100\n  batch_size = 16384\n\n  batch = tf.Variable(0) # <-- this line, in examples, should be changed to have dtype=tf.int64, with explanation of why\n  learning_rate = tf.train.exponential_decay(\n      0.01,\n      batch * batch_size,\n      epoch_size,\n      0.95,\n      staircase = True)\n  loss = tf.Variable(0.0)\n  eval_frequency = int(epoch_size / batch_size / 2)\n\n  optimizer = tf.train.MomentumOptimizer(learning_rate, 0.9) \\\n     .minimize(loss, global_step = batch)\n\n  with tf.Session() as sess:\n    tf.initialize_all_variables().run()\n\n    for step in xrange(int(epoches * epoch_size) // batch_size):\n      _, b, lr = sess.run([optimizer, batch, learning_rate])\n      if step % eval_frequency == 0:\n        print(\"Step %d (epoch %.2f): batch %d, learning rate %.6f\" %\n            (step, float(step) * batch_size / epoch_size, b, lr))\n\nif __name__ == '__main__':\n  tf.app.run()\n```\n\n_[1] No, I did not checkpoint midway through.  Yes, I have learned my lesson._\n"}