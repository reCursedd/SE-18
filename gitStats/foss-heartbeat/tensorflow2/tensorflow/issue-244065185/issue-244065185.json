{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11610", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11610/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11610/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11610/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11610", "id": 244065185, "node_id": "MDU6SXNzdWUyNDQwNjUxODU=", "number": 11610, "title": "Changing batch size changes output for float32 matmuls elementwise by ~1e-8 (at least on CPU)", "user": {"login": "iamshang1", "id": 8942688, "node_id": "MDQ6VXNlcjg5NDI2ODg=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942688?v=4", "gravatar_id": "", "url": "https://api.github.com/users/iamshang1", "html_url": "https://github.com/iamshang1", "followers_url": "https://api.github.com/users/iamshang1/followers", "following_url": "https://api.github.com/users/iamshang1/following{/other_user}", "gists_url": "https://api.github.com/users/iamshang1/gists{/gist_id}", "starred_url": "https://api.github.com/users/iamshang1/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/iamshang1/subscriptions", "organizations_url": "https://api.github.com/users/iamshang1/orgs", "repos_url": "https://api.github.com/users/iamshang1/repos", "events_url": "https://api.github.com/users/iamshang1/events{/privacy}", "received_events_url": "https://api.github.com/users/iamshang1/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 20, "created_at": "2017-07-19T14:41:54Z", "updated_at": "2018-02-17T22:02:26Z", "closed_at": "2018-02-17T22:02:26Z", "author_association": "NONE", "body_html": "<p>While developing a Hierarchical Attention Network, we have discovered that changing the batch size of the input effects the output of dynamic RNNs (while keeping everything else constant). In other words, feeding in [[1,2,3,4,5]] and [[6,7,8,9,10]] individually with batch size 1 will give a different result than feeding in [[1,2,3,4,5],[6,7,8,9,10]] together with batch size 2. We are currently running Bidirectional Dynamic RNNs with GRUs on the CPU-version of Tensorflow 1.2.</p>\n<p>While the change in output is small, when a network has many layers of RNNs, the differences become amplified. In our case, changing the batch size from 1 to 10 changes the network accuracy on our test set from 50% to 46%.</p>\n<p>System information and shortened sample code below.</p>\n<h3>System information</h3>\n<p>== cat /etc/issue ===============================================<br>\nLinux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux<br>\nVERSION=\"7.3 (Maipo)\"<br>\nVERSION_ID=\"7.3\"<br>\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.3<br>\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.3\"</p>\n<p>== are we in docker =============================================<br>\nNo</p>\n<p>== compiler =====================================================<br>\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)<br>\nCopyright (C) 2015 Free Software Foundation, Inc.<br>\nThis is free software; see the source for copying conditions.  There is NO<br>\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.</p>\n<p>== uname -a =====================================================<br>\nLinux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a> SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux</p>\n<p>== check pips ===================================================<br>\nnumpy (1.13.1)<br>\nnumpydoc (0.6.0)<br>\nprotobuf (3.3.0)<br>\ntensorflow (1.2.1)</p>\n<p>== check for virtualenv =========================================<br>\nFalse</p>\n<p>== tensorflow import ============================================<br>\ntf.VERSION = 1.2.1<br>\ntf.GIT_VERSION = v1.2.0-5-g435cdfc<br>\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc<br>\nSanity check: array([1], dtype=int32)</p>\n<p>== env ==========================================================<br>\nLD_LIBRARY_PATH is unset<br>\nDYLD_LIBRARY_PATH is unset</p>\n<p>== nvidia-smi ===================================================<br>\ntf_env_collect.sh: line 105: nvidia-smi: command not found</p>\n<p>== cuda libs  ===================================================</p>\n<h3>Source code / logs</h3>\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell, GRUCell\n\nembeddings = np.random.rand(8000,350).astype(np.float32)\nembeddings -= embeddings.mean()\nembeddings /= (embeddings.std()*2.5)\n\n#doc input and line count\nline = tf.placeholder(tf.int32, shape=[None,10])\nnum_words = tf.reduce_sum(tf.cast(tf.greater(line,0),tf.int32),1)\nword_embeds = tf.nn.embedding_lookup(tf.get_variable('embeddings',\n              initializer=embeddings,dtype=tf.float32),line)\n\n[word_outputs_fw,word_outputs_bw],_ = \\\n        tf.nn.bidirectional_dynamic_rnn(GRUCell(5),GRUCell(5),\n        word_embeds,sequence_length=num_words,\n        dtype=tf.float32)\n\nword_outputs = tf.concat((word_outputs_fw, word_outputs_bw),2)\n\ninit_op = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init_op)\n\na = np.array([[1,2,3,4,5,0,0,0,0,0]])\nb = np.array([[6,7,8,9,10,11,12,13,14,15]])\nab = np.array([[1,2,3,4,5,0,0,0,0,0],[6,7,8,9,10,11,12,13,14,15]])\n\nfeed_dict = {line:a}\nprint sess.run(word_outputs,feed_dict=feed_dict)\nfeed_dict = {line:b}\nprint sess.run(word_outputs,feed_dict=feed_dict)\nfeed_dict = {line:ab}\nprint sess.run(word_outputs,feed_dict=feed_dict)\n</code></pre>\n<h3>Sample Output</h3>\n<p>Below, the first two matrices are the results of feeding in two inputs one at a time with batch size 1, while the second two matrices are the results of feeding in two inputs together with batch size 2. You can see that the outputs are not exactly the same. While the differences between the two are small, this becomes a major issue when there are multiple layers of RNNs as the differences become more pronounced after each layer.</p>\n<pre><code>[[[ 0.07946277 -0.09917585  0.01027258 -0.03145921 -0.06281948  0.27924815\n   -0.32083094 -0.18930595  0.17904316 -0.09718883]\n  [ 0.09456758 -0.06845391 -0.02745478 -0.10440759 -0.07491632  0.27888948\n   -0.27896836  0.03206063  0.09979809 -0.00771215]\n  [ 0.01643243  0.12345143  0.12964873  0.01598591 -0.18927756  0.37746075\n   -0.03456679 -0.01384296  0.03874877  0.06282371]\n  [ 0.04219431  0.02407469 -0.1588002   0.1497623  -0.17770161  0.3960323\n    0.16187154 -0.04393335 -0.02065297  0.10994863]\n  [-0.13827246 -0.07322901 -0.012384    0.12282669  0.07407188 -0.14240782\n    0.140168    0.02362901  0.06010906  0.05862212]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]]]\n[[[ 0.16048311 -0.02057735  0.18933752 -0.12690066 -0.04377137  0.32376432\n    0.13263705 -0.07457904  0.14895026 -0.18088266]\n  [ 0.16808736 -0.11579071 -0.11836589 -0.31363881  0.1567639  -0.08804542\n    0.1359456  -0.03568897  0.12253968 -0.08998561]\n  [ 0.19741508 -0.01034784 -0.03235145 -0.27677989  0.1338885  -0.14571345\n    0.08804264 -0.02352159  0.04717591 -0.37237346]\n  [ 0.24377933 -0.27160296 -0.11816068 -0.45893419 -0.09967859 -0.04910848\n    0.03985181 -0.01856269  0.04410465 -0.21198548]\n  [ 0.16746353 -0.20125373 -0.2098352  -0.36264825  0.02557869 -0.06599348\n   -0.11331714 -0.17118242 -0.08420456 -0.22979215]\n  [-0.09969822 -0.14207448  0.12536064 -0.22236535  0.11328859 -0.09342889\n   -0.02536193 -0.28028104 -0.11790876 -0.10144062]\n  [-0.09796695 -0.14415297 -0.19729097 -0.25542045 -0.15568495 -0.12689842\n   -0.14712927 -0.35488427 -0.06447952 -0.19063833]\n  [-0.12240371 -0.07732555 -0.2645728   0.11042064 -0.19387801  0.07324903\n   -0.03920996  0.05104404 -0.09357925 -0.13582835]\n  [-0.07295815 -0.02809375 -0.24317381  0.04480781 -0.06040902  0.03428879\n    0.10196722 -0.06142509 -0.36903486 -0.16991363]\n  [-0.01382132 -0.09746805  0.13226555  0.19477166  0.02158988  0.09287433\n    0.01845972 -0.16030487 -0.2186746  -0.07543172]]]\n[[[ 0.07946277 -0.09917583  0.01027257 -0.03145922 -0.06281949  0.27924818\n   -0.32083094 -0.1893059   0.17904317 -0.09718874]\n  [ 0.09456757 -0.06845395 -0.02745485 -0.10440758 -0.07491633  0.27888948\n   -0.27896842  0.03206065  0.09979802 -0.00771213]\n  [ 0.01643244  0.12345135  0.1296487   0.01598606 -0.18927751  0.37746072\n   -0.0345668  -0.01384297  0.03874873  0.06282371]\n  [ 0.0421943   0.02407462 -0.15880026  0.14976241 -0.17770153  0.39603227\n    0.16187152 -0.04393341 -0.02065307  0.10994864]\n  [-0.13827249 -0.07322903 -0.01238406  0.12282679  0.07407197 -0.14240779\n    0.14016804  0.02362898  0.06010904  0.05862212]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]]\n\n [[ 0.16048311 -0.02057736  0.18933751 -0.12690073 -0.04377136  0.32376438\n    0.13263711 -0.07457898  0.14895013 -0.18088272]\n  [ 0.16808733 -0.11579067 -0.11836579 -0.31363881  0.15676391 -0.0880454\n    0.13594568 -0.03568894  0.12253958 -0.08998567]\n  [ 0.19741504 -0.01034782 -0.0323514  -0.27677989  0.13388851 -0.14571348\n    0.08804271 -0.02352155  0.0471758  -0.37237346]\n  [ 0.2437793  -0.27160296 -0.11816064 -0.45893413 -0.09967858 -0.04910852\n    0.03985184 -0.01856264  0.04410452 -0.21198554]\n  [ 0.16746351 -0.20125373 -0.20983508 -0.36264819  0.02557869 -0.06599346\n   -0.11331721 -0.17118247 -0.08420463 -0.22979221]\n  [-0.09969822 -0.14207451  0.12536073 -0.22236532  0.11328855 -0.09342889\n   -0.02536207 -0.28028107 -0.11790879 -0.10144066]\n  [-0.09796697 -0.14415301 -0.19729097 -0.25542039 -0.15568499 -0.12689844\n   -0.14712939 -0.35488427 -0.06447949 -0.19063836]\n  [-0.12240377 -0.07732558 -0.2645728   0.11042073 -0.19387813  0.07324899\n   -0.03921008  0.05104404 -0.0935792  -0.13582836]\n  [-0.07295817 -0.02809371 -0.24317381  0.04480787 -0.06040911  0.03428881\n    0.10196713 -0.06142508 -0.3690348  -0.16991359]\n  [-0.0138213  -0.09746802  0.13226555  0.19477174  0.02158987  0.09287442\n    0.0184597  -0.16030489 -0.21867451 -0.07543168]]]\n\n</code></pre>", "body_text": "While developing a Hierarchical Attention Network, we have discovered that changing the batch size of the input effects the output of dynamic RNNs (while keeping everything else constant). In other words, feeding in [[1,2,3,4,5]] and [[6,7,8,9,10]] individually with batch size 1 will give a different result than feeding in [[1,2,3,4,5],[6,7,8,9,10]] together with batch size 2. We are currently running Bidirectional Dynamic RNNs with GRUs on the CPU-version of Tensorflow 1.2.\nWhile the change in output is small, when a network has many layers of RNNs, the differences become amplified. In our case, changing the batch size from 1 to 10 changes the network accuracy on our test set from 50% to 46%.\nSystem information and shortened sample code below.\nSystem information\n== cat /etc/issue ===============================================\nLinux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 #1 SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\nVERSION=\"7.3 (Maipo)\"\nVERSION_ID=\"7.3\"\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.3\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.3\"\n== are we in docker =============================================\nNo\n== compiler =====================================================\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n== uname -a =====================================================\nLinux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 #1 SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\n== check pips ===================================================\nnumpy (1.13.1)\nnumpydoc (0.6.0)\nprotobuf (3.3.0)\ntensorflow (1.2.1)\n== check for virtualenv =========================================\nFalse\n== tensorflow import ============================================\ntf.VERSION = 1.2.1\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\nSanity check: array([1], dtype=int32)\n== env ==========================================================\nLD_LIBRARY_PATH is unset\nDYLD_LIBRARY_PATH is unset\n== nvidia-smi ===================================================\ntf_env_collect.sh: line 105: nvidia-smi: command not found\n== cuda libs  ===================================================\nSource code / logs\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn import LSTMCell, GRUCell\n\nembeddings = np.random.rand(8000,350).astype(np.float32)\nembeddings -= embeddings.mean()\nembeddings /= (embeddings.std()*2.5)\n\n#doc input and line count\nline = tf.placeholder(tf.int32, shape=[None,10])\nnum_words = tf.reduce_sum(tf.cast(tf.greater(line,0),tf.int32),1)\nword_embeds = tf.nn.embedding_lookup(tf.get_variable('embeddings',\n              initializer=embeddings,dtype=tf.float32),line)\n\n[word_outputs_fw,word_outputs_bw],_ = \\\n        tf.nn.bidirectional_dynamic_rnn(GRUCell(5),GRUCell(5),\n        word_embeds,sequence_length=num_words,\n        dtype=tf.float32)\n\nword_outputs = tf.concat((word_outputs_fw, word_outputs_bw),2)\n\ninit_op = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init_op)\n\na = np.array([[1,2,3,4,5,0,0,0,0,0]])\nb = np.array([[6,7,8,9,10,11,12,13,14,15]])\nab = np.array([[1,2,3,4,5,0,0,0,0,0],[6,7,8,9,10,11,12,13,14,15]])\n\nfeed_dict = {line:a}\nprint sess.run(word_outputs,feed_dict=feed_dict)\nfeed_dict = {line:b}\nprint sess.run(word_outputs,feed_dict=feed_dict)\nfeed_dict = {line:ab}\nprint sess.run(word_outputs,feed_dict=feed_dict)\n\nSample Output\nBelow, the first two matrices are the results of feeding in two inputs one at a time with batch size 1, while the second two matrices are the results of feeding in two inputs together with batch size 2. You can see that the outputs are not exactly the same. While the differences between the two are small, this becomes a major issue when there are multiple layers of RNNs as the differences become more pronounced after each layer.\n[[[ 0.07946277 -0.09917585  0.01027258 -0.03145921 -0.06281948  0.27924815\n   -0.32083094 -0.18930595  0.17904316 -0.09718883]\n  [ 0.09456758 -0.06845391 -0.02745478 -0.10440759 -0.07491632  0.27888948\n   -0.27896836  0.03206063  0.09979809 -0.00771215]\n  [ 0.01643243  0.12345143  0.12964873  0.01598591 -0.18927756  0.37746075\n   -0.03456679 -0.01384296  0.03874877  0.06282371]\n  [ 0.04219431  0.02407469 -0.1588002   0.1497623  -0.17770161  0.3960323\n    0.16187154 -0.04393335 -0.02065297  0.10994863]\n  [-0.13827246 -0.07322901 -0.012384    0.12282669  0.07407188 -0.14240782\n    0.140168    0.02362901  0.06010906  0.05862212]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]]]\n[[[ 0.16048311 -0.02057735  0.18933752 -0.12690066 -0.04377137  0.32376432\n    0.13263705 -0.07457904  0.14895026 -0.18088266]\n  [ 0.16808736 -0.11579071 -0.11836589 -0.31363881  0.1567639  -0.08804542\n    0.1359456  -0.03568897  0.12253968 -0.08998561]\n  [ 0.19741508 -0.01034784 -0.03235145 -0.27677989  0.1338885  -0.14571345\n    0.08804264 -0.02352159  0.04717591 -0.37237346]\n  [ 0.24377933 -0.27160296 -0.11816068 -0.45893419 -0.09967859 -0.04910848\n    0.03985181 -0.01856269  0.04410465 -0.21198548]\n  [ 0.16746353 -0.20125373 -0.2098352  -0.36264825  0.02557869 -0.06599348\n   -0.11331714 -0.17118242 -0.08420456 -0.22979215]\n  [-0.09969822 -0.14207448  0.12536064 -0.22236535  0.11328859 -0.09342889\n   -0.02536193 -0.28028104 -0.11790876 -0.10144062]\n  [-0.09796695 -0.14415297 -0.19729097 -0.25542045 -0.15568495 -0.12689842\n   -0.14712927 -0.35488427 -0.06447952 -0.19063833]\n  [-0.12240371 -0.07732555 -0.2645728   0.11042064 -0.19387801  0.07324903\n   -0.03920996  0.05104404 -0.09357925 -0.13582835]\n  [-0.07295815 -0.02809375 -0.24317381  0.04480781 -0.06040902  0.03428879\n    0.10196722 -0.06142509 -0.36903486 -0.16991363]\n  [-0.01382132 -0.09746805  0.13226555  0.19477166  0.02158988  0.09287433\n    0.01845972 -0.16030487 -0.2186746  -0.07543172]]]\n[[[ 0.07946277 -0.09917583  0.01027257 -0.03145922 -0.06281949  0.27924818\n   -0.32083094 -0.1893059   0.17904317 -0.09718874]\n  [ 0.09456757 -0.06845395 -0.02745485 -0.10440758 -0.07491633  0.27888948\n   -0.27896842  0.03206065  0.09979802 -0.00771213]\n  [ 0.01643244  0.12345135  0.1296487   0.01598606 -0.18927751  0.37746072\n   -0.0345668  -0.01384297  0.03874873  0.06282371]\n  [ 0.0421943   0.02407462 -0.15880026  0.14976241 -0.17770153  0.39603227\n    0.16187152 -0.04393341 -0.02065307  0.10994864]\n  [-0.13827249 -0.07322903 -0.01238406  0.12282679  0.07407197 -0.14240779\n    0.14016804  0.02362898  0.06010904  0.05862212]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]\n  [ 0.          0.          0.          0.          0.          0.          0.\n    0.          0.          0.        ]]\n\n [[ 0.16048311 -0.02057736  0.18933751 -0.12690073 -0.04377136  0.32376438\n    0.13263711 -0.07457898  0.14895013 -0.18088272]\n  [ 0.16808733 -0.11579067 -0.11836579 -0.31363881  0.15676391 -0.0880454\n    0.13594568 -0.03568894  0.12253958 -0.08998567]\n  [ 0.19741504 -0.01034782 -0.0323514  -0.27677989  0.13388851 -0.14571348\n    0.08804271 -0.02352155  0.0471758  -0.37237346]\n  [ 0.2437793  -0.27160296 -0.11816064 -0.45893413 -0.09967858 -0.04910852\n    0.03985184 -0.01856264  0.04410452 -0.21198554]\n  [ 0.16746351 -0.20125373 -0.20983508 -0.36264819  0.02557869 -0.06599346\n   -0.11331721 -0.17118247 -0.08420463 -0.22979221]\n  [-0.09969822 -0.14207451  0.12536073 -0.22236532  0.11328855 -0.09342889\n   -0.02536207 -0.28028107 -0.11790879 -0.10144066]\n  [-0.09796697 -0.14415301 -0.19729097 -0.25542039 -0.15568499 -0.12689844\n   -0.14712939 -0.35488427 -0.06447949 -0.19063836]\n  [-0.12240377 -0.07732558 -0.2645728   0.11042073 -0.19387813  0.07324899\n   -0.03921008  0.05104404 -0.0935792  -0.13582836]\n  [-0.07295817 -0.02809371 -0.24317381  0.04480787 -0.06040911  0.03428881\n    0.10196713 -0.06142508 -0.3690348  -0.16991359]\n  [-0.0138213  -0.09746802  0.13226555  0.19477174  0.02158987  0.09287442\n    0.0184597  -0.16030489 -0.21867451 -0.07543168]]]", "body": "While developing a Hierarchical Attention Network, we have discovered that changing the batch size of the input effects the output of dynamic RNNs (while keeping everything else constant). In other words, feeding in [[1,2,3,4,5]] and [[6,7,8,9,10]] individually with batch size 1 will give a different result than feeding in [[1,2,3,4,5],[6,7,8,9,10]] together with batch size 2. We are currently running Bidirectional Dynamic RNNs with GRUs on the CPU-version of Tensorflow 1.2.\r\n\r\nWhile the change in output is small, when a network has many layers of RNNs, the differences become amplified. In our case, changing the batch size from 1 to 10 changes the network accuracy on our test set from 50% to 46%.\r\n\r\nSystem information and shortened sample code below.\r\n\r\n### System information\r\n== cat /etc/issue ===============================================\r\nLinux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 #1 SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\nVERSION=\"7.3 (Maipo)\"\r\nVERSION_ID=\"7.3\"\r\nREDHAT_BUGZILLA_PRODUCT_VERSION=7.3\r\nREDHAT_SUPPORT_PRODUCT_VERSION=\"7.3\"\r\n\r\n== are we in docker =============================================\r\nNo\r\n\r\n== compiler =====================================================\r\nc++ (GCC) 4.8.5 20150623 (Red Hat 4.8.5-11)\r\nCopyright (C) 2015 Free Software Foundation, Inc.\r\nThis is free software; see the source for copying conditions.  There is NO\r\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\r\n\r\n\r\n== uname -a =====================================================\r\nLinux pc93071.ornl.gov 3.10.0-514.26.1.el7.x86_64 #1 SMP Tue Jun 20 01:16:02 EDT 2017 x86_64 x86_64 x86_64 GNU/Linux\r\n\r\n== check pips ===================================================\r\nnumpy (1.13.1)\r\nnumpydoc (0.6.0)\r\nprotobuf (3.3.0)\r\ntensorflow (1.2.1)\r\n\r\n== check for virtualenv =========================================\r\nFalse\r\n\r\n== tensorflow import ============================================\r\ntf.VERSION = 1.2.1\r\ntf.GIT_VERSION = v1.2.0-5-g435cdfc\r\ntf.COMPILER_VERSION = v1.2.0-5-g435cdfc\r\nSanity check: array([1], dtype=int32)\r\n\r\n== env ==========================================================\r\nLD_LIBRARY_PATH is unset\r\nDYLD_LIBRARY_PATH is unset\r\n\r\n== nvidia-smi ===================================================\r\ntf_env_collect.sh: line 105: nvidia-smi: command not found\r\n\r\n== cuda libs  ===================================================\r\n\r\n### Source code / logs\r\n\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn import LSTMCell, GRUCell\r\n\r\nembeddings = np.random.rand(8000,350).astype(np.float32)\r\nembeddings -= embeddings.mean()\r\nembeddings /= (embeddings.std()*2.5)\r\n\r\n#doc input and line count\r\nline = tf.placeholder(tf.int32, shape=[None,10])\r\nnum_words = tf.reduce_sum(tf.cast(tf.greater(line,0),tf.int32),1)\r\nword_embeds = tf.nn.embedding_lookup(tf.get_variable('embeddings',\r\n              initializer=embeddings,dtype=tf.float32),line)\r\n\r\n[word_outputs_fw,word_outputs_bw],_ = \\\r\n        tf.nn.bidirectional_dynamic_rnn(GRUCell(5),GRUCell(5),\r\n        word_embeds,sequence_length=num_words,\r\n        dtype=tf.float32)\r\n\r\nword_outputs = tf.concat((word_outputs_fw, word_outputs_bw),2)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nsess = tf.Session()\r\nsess.run(init_op)\r\n\r\na = np.array([[1,2,3,4,5,0,0,0,0,0]])\r\nb = np.array([[6,7,8,9,10,11,12,13,14,15]])\r\nab = np.array([[1,2,3,4,5,0,0,0,0,0],[6,7,8,9,10,11,12,13,14,15]])\r\n\r\nfeed_dict = {line:a}\r\nprint sess.run(word_outputs,feed_dict=feed_dict)\r\nfeed_dict = {line:b}\r\nprint sess.run(word_outputs,feed_dict=feed_dict)\r\nfeed_dict = {line:ab}\r\nprint sess.run(word_outputs,feed_dict=feed_dict)\r\n```\r\n\r\n### Sample Output\r\n\r\nBelow, the first two matrices are the results of feeding in two inputs one at a time with batch size 1, while the second two matrices are the results of feeding in two inputs together with batch size 2. You can see that the outputs are not exactly the same. While the differences between the two are small, this becomes a major issue when there are multiple layers of RNNs as the differences become more pronounced after each layer.\r\n\r\n```\r\n[[[ 0.07946277 -0.09917585  0.01027258 -0.03145921 -0.06281948  0.27924815\r\n   -0.32083094 -0.18930595  0.17904316 -0.09718883]\r\n  [ 0.09456758 -0.06845391 -0.02745478 -0.10440759 -0.07491632  0.27888948\r\n   -0.27896836  0.03206063  0.09979809 -0.00771215]\r\n  [ 0.01643243  0.12345143  0.12964873  0.01598591 -0.18927756  0.37746075\r\n   -0.03456679 -0.01384296  0.03874877  0.06282371]\r\n  [ 0.04219431  0.02407469 -0.1588002   0.1497623  -0.17770161  0.3960323\r\n    0.16187154 -0.04393335 -0.02065297  0.10994863]\r\n  [-0.13827246 -0.07322901 -0.012384    0.12282669  0.07407188 -0.14240782\r\n    0.140168    0.02362901  0.06010906  0.05862212]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]]]\r\n[[[ 0.16048311 -0.02057735  0.18933752 -0.12690066 -0.04377137  0.32376432\r\n    0.13263705 -0.07457904  0.14895026 -0.18088266]\r\n  [ 0.16808736 -0.11579071 -0.11836589 -0.31363881  0.1567639  -0.08804542\r\n    0.1359456  -0.03568897  0.12253968 -0.08998561]\r\n  [ 0.19741508 -0.01034784 -0.03235145 -0.27677989  0.1338885  -0.14571345\r\n    0.08804264 -0.02352159  0.04717591 -0.37237346]\r\n  [ 0.24377933 -0.27160296 -0.11816068 -0.45893419 -0.09967859 -0.04910848\r\n    0.03985181 -0.01856269  0.04410465 -0.21198548]\r\n  [ 0.16746353 -0.20125373 -0.2098352  -0.36264825  0.02557869 -0.06599348\r\n   -0.11331714 -0.17118242 -0.08420456 -0.22979215]\r\n  [-0.09969822 -0.14207448  0.12536064 -0.22236535  0.11328859 -0.09342889\r\n   -0.02536193 -0.28028104 -0.11790876 -0.10144062]\r\n  [-0.09796695 -0.14415297 -0.19729097 -0.25542045 -0.15568495 -0.12689842\r\n   -0.14712927 -0.35488427 -0.06447952 -0.19063833]\r\n  [-0.12240371 -0.07732555 -0.2645728   0.11042064 -0.19387801  0.07324903\r\n   -0.03920996  0.05104404 -0.09357925 -0.13582835]\r\n  [-0.07295815 -0.02809375 -0.24317381  0.04480781 -0.06040902  0.03428879\r\n    0.10196722 -0.06142509 -0.36903486 -0.16991363]\r\n  [-0.01382132 -0.09746805  0.13226555  0.19477166  0.02158988  0.09287433\r\n    0.01845972 -0.16030487 -0.2186746  -0.07543172]]]\r\n[[[ 0.07946277 -0.09917583  0.01027257 -0.03145922 -0.06281949  0.27924818\r\n   -0.32083094 -0.1893059   0.17904317 -0.09718874]\r\n  [ 0.09456757 -0.06845395 -0.02745485 -0.10440758 -0.07491633  0.27888948\r\n   -0.27896842  0.03206065  0.09979802 -0.00771213]\r\n  [ 0.01643244  0.12345135  0.1296487   0.01598606 -0.18927751  0.37746072\r\n   -0.0345668  -0.01384297  0.03874873  0.06282371]\r\n  [ 0.0421943   0.02407462 -0.15880026  0.14976241 -0.17770153  0.39603227\r\n    0.16187152 -0.04393341 -0.02065307  0.10994864]\r\n  [-0.13827249 -0.07322903 -0.01238406  0.12282679  0.07407197 -0.14240779\r\n    0.14016804  0.02362898  0.06010904  0.05862212]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]\r\n  [ 0.          0.          0.          0.          0.          0.          0.\r\n    0.          0.          0.        ]]\r\n\r\n [[ 0.16048311 -0.02057736  0.18933751 -0.12690073 -0.04377136  0.32376438\r\n    0.13263711 -0.07457898  0.14895013 -0.18088272]\r\n  [ 0.16808733 -0.11579067 -0.11836579 -0.31363881  0.15676391 -0.0880454\r\n    0.13594568 -0.03568894  0.12253958 -0.08998567]\r\n  [ 0.19741504 -0.01034782 -0.0323514  -0.27677989  0.13388851 -0.14571348\r\n    0.08804271 -0.02352155  0.0471758  -0.37237346]\r\n  [ 0.2437793  -0.27160296 -0.11816064 -0.45893413 -0.09967858 -0.04910852\r\n    0.03985184 -0.01856264  0.04410452 -0.21198554]\r\n  [ 0.16746351 -0.20125373 -0.20983508 -0.36264819  0.02557869 -0.06599346\r\n   -0.11331721 -0.17118247 -0.08420463 -0.22979221]\r\n  [-0.09969822 -0.14207451  0.12536073 -0.22236532  0.11328855 -0.09342889\r\n   -0.02536207 -0.28028107 -0.11790879 -0.10144066]\r\n  [-0.09796697 -0.14415301 -0.19729097 -0.25542039 -0.15568499 -0.12689844\r\n   -0.14712939 -0.35488427 -0.06447949 -0.19063836]\r\n  [-0.12240377 -0.07732558 -0.2645728   0.11042073 -0.19387813  0.07324899\r\n   -0.03921008  0.05104404 -0.0935792  -0.13582836]\r\n  [-0.07295817 -0.02809371 -0.24317381  0.04480787 -0.06040911  0.03428881\r\n    0.10196713 -0.06142508 -0.3690348  -0.16991359]\r\n  [-0.0138213  -0.09746802  0.13226555  0.19477174  0.02158987  0.09287442\r\n    0.0184597  -0.16030489 -0.21867451 -0.07543168]]]\r\n\r\n```"}