{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/327845745", "html_url": "https://github.com/tensorflow/tensorflow/issues/11610#issuecomment-327845745", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11610", "id": 327845745, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzg0NTc0NQ==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-07T16:03:44Z", "updated_at": "2017-09-07T16:05:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Looks like the matmul is the real culprit here.  It's not deterministic for f32, at least on CPUs:</p>\n<pre><code> z = tf.concat((word_embeds[:,0,:], init), axis=1)  # shaped [batch_size, 355]\nmm = tf.matmul(z, c.variables[0])  # matmul (batch_size, 355) . (355, 10) =&gt; (batch_size, 10)\nav = sess.run(mm, {line:a}) \nbv = sess.run(mm, {line:b}) \nabv = sess.run(mm, {line:ab}) \nnp.linalg.norm(av-abv[0])   \n4.1578312e-07\nnp.linalg.norm(bv-abv[1])   \n4.8896561e-07\n</code></pre>", "body_text": "Looks like the matmul is the real culprit here.  It's not deterministic for f32, at least on CPUs:\n z = tf.concat((word_embeds[:,0,:], init), axis=1)  # shaped [batch_size, 355]\nmm = tf.matmul(z, c.variables[0])  # matmul (batch_size, 355) . (355, 10) => (batch_size, 10)\nav = sess.run(mm, {line:a}) \nbv = sess.run(mm, {line:b}) \nabv = sess.run(mm, {line:ab}) \nnp.linalg.norm(av-abv[0])   \n4.1578312e-07\nnp.linalg.norm(bv-abv[1])   \n4.8896561e-07", "body": "Looks like the matmul is the real culprit here.  It's not deterministic for f32, at least on CPUs:\r\n\r\n```\r\n z = tf.concat((word_embeds[:,0,:], init), axis=1)  # shaped [batch_size, 355]\r\nmm = tf.matmul(z, c.variables[0])  # matmul (batch_size, 355) . (355, 10) => (batch_size, 10)\r\nav = sess.run(mm, {line:a}) \r\nbv = sess.run(mm, {line:b}) \r\nabv = sess.run(mm, {line:ab}) \r\nnp.linalg.norm(av-abv[0])   \r\n4.1578312e-07\r\nnp.linalg.norm(bv-abv[1])   \r\n4.8896561e-07\r\n```\r\n"}