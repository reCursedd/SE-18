{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/421936135", "html_url": "https://github.com/tensorflow/tensorflow/issues/13463#issuecomment-421936135", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13463", "id": 421936135, "node_id": "MDEyOklzc3VlQ29tbWVudDQyMTkzNjEzNQ==", "user": {"login": "eliorc", "id": 17727283, "node_id": "MDQ6VXNlcjE3NzI3Mjgz", "avatar_url": "https://avatars2.githubusercontent.com/u/17727283?v=4", "gravatar_id": "", "url": "https://api.github.com/users/eliorc", "html_url": "https://github.com/eliorc", "followers_url": "https://api.github.com/users/eliorc/followers", "following_url": "https://api.github.com/users/eliorc/following{/other_user}", "gists_url": "https://api.github.com/users/eliorc/gists{/gist_id}", "starred_url": "https://api.github.com/users/eliorc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/eliorc/subscriptions", "organizations_url": "https://api.github.com/users/eliorc/orgs", "repos_url": "https://api.github.com/users/eliorc/repos", "events_url": "https://api.github.com/users/eliorc/events{/privacy}", "received_events_url": "https://api.github.com/users/eliorc/received_events", "type": "User", "site_admin": false}, "created_at": "2018-09-17T09:05:08Z", "updated_at": "2018-09-17T09:05:08Z", "author_association": "NONE", "body_html": "<p>Having the same problem</p>\n<p>The setup is as follows.<br>\nI'm running inside a nvidia-docker container, TF version 1.10, Ubuntu 16.04.<br>\nMy TFRecords are 161GB in size.</p>\n<p>Since the code is sensitive I can't post it but I'll explain what goes on and how I can repeat this exception.</p>\n<p>After restarting the machine and rebuilding the image (still TF 1.10) before training I'll actually go over the entire dataset, and count its size using <code>train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))</code> - so far no problems - notice this is a full iteration over that tfrecords.<br>\nThen while iterating through a session that accepts the handle string, training will start normally and will suddenly fail indicating <code>DataLossError: corrupted record at X</code>.<br>\nIf then I try to run the same script again, it will fail on the <code>train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))</code> immediately  indicating the corrupted record at exactly same value (same <code>X</code>)</p>\n<p>This is now repeatable forever.</p>\n<p>If I restart the machine it the whole process starts over, the first run will randomly fail during training, at an unexpected record number (this time a different number) and then no matter how many times I rerun the script I will fail on the first call on the same record.</p>\n<p>I think this should be reopened and solved - TFRecords are very important for big data training.</p>", "body_text": "Having the same problem\nThe setup is as follows.\nI'm running inside a nvidia-docker container, TF version 1.10, Ubuntu 16.04.\nMy TFRecords are 161GB in size.\nSince the code is sensitive I can't post it but I'll explain what goes on and how I can repeat this exception.\nAfter restarting the machine and rebuilding the image (still TF 1.10) before training I'll actually go over the entire dataset, and count its size using train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path'])) - so far no problems - notice this is a full iteration over that tfrecords.\nThen while iterating through a session that accepts the handle string, training will start normally and will suddenly fail indicating DataLossError: corrupted record at X.\nIf then I try to run the same script again, it will fail on the train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path'])) immediately  indicating the corrupted record at exactly same value (same X)\nThis is now repeatable forever.\nIf I restart the machine it the whole process starts over, the first run will randomly fail during training, at an unexpected record number (this time a different number) and then no matter how many times I rerun the script I will fail on the first call on the same record.\nI think this should be reopened and solved - TFRecords are very important for big data training.", "body": "Having the same problem\r\n\r\nThe setup is as follows. \r\nI'm running inside a nvidia-docker container, TF version 1.10, Ubuntu 16.04.\r\nMy TFRecords are 161GB in size.\r\n\r\nSince the code is sensitive I can't post it but I'll explain what goes on and how I can repeat this exception.\r\n\r\nAfter restarting the machine and rebuilding the image (still TF 1.10) before training I'll actually go over the entire dataset, and count its size using `train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))` - so far no problems - notice this is a full iteration over that tfrecords.\r\nThen while iterating through a session that accepts the handle string, training will start normally and will suddenly fail indicating `DataLossError: corrupted record at X`.\r\nIf then I try to run the same script again, it will fail on the `train_size = sum(1 for _ in tf.python_io.tf_record_iterator(meta['train_tfr_path']))` immediately  indicating the corrupted record at exactly same value (same `X`)\r\n\r\nThis is now repeatable forever.\r\n\r\nIf I restart the machine it the whole process starts over, the first run will randomly fail during training, at an unexpected record number (this time a different number) and then no matter how many times I rerun the script I will fail on the first call on the same record.\r\n\r\nI think this should be reopened and solved - TFRecords are very important for big data training.\r\n"}