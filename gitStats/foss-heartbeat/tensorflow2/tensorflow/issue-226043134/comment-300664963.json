{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/300664963", "html_url": "https://github.com/tensorflow/tensorflow/issues/9635#issuecomment-300664963", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9635", "id": 300664963, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMDY2NDk2Mw==", "user": {"login": "Songweiping", "id": 11703156, "node_id": "MDQ6VXNlcjExNzAzMTU2", "avatar_url": "https://avatars2.githubusercontent.com/u/11703156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Songweiping", "html_url": "https://github.com/Songweiping", "followers_url": "https://api.github.com/users/Songweiping/followers", "following_url": "https://api.github.com/users/Songweiping/following{/other_user}", "gists_url": "https://api.github.com/users/Songweiping/gists{/gist_id}", "starred_url": "https://api.github.com/users/Songweiping/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Songweiping/subscriptions", "organizations_url": "https://api.github.com/users/Songweiping/orgs", "repos_url": "https://api.github.com/users/Songweiping/repos", "events_url": "https://api.github.com/users/Songweiping/events{/privacy}", "received_events_url": "https://api.github.com/users/Songweiping/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-11T02:26:07Z", "updated_at": "2017-05-11T02:26:07Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=396613\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lmthang\">@lmthang</a> ,</p>\n<p>I am wondering if it is desirable to support personalized activation function in attention layer. In current version, no activation is applied after concatenating context vector and target hidden value, while there is a <em>tanh</em> in Luong's paper <a href=\"https://arxiv.org/abs/1508.04025\" rel=\"nofollow\">https://arxiv.org/abs/1508.04025</a>.<br>\nI have tested this on some tasks, it seems that it does make some difference, but not significant. Therefore, this is just a kind suggestion.</p>\n<p>Thanks!</p>", "body_text": "Hi @ebrevdo @lmthang ,\nI am wondering if it is desirable to support personalized activation function in attention layer. In current version, no activation is applied after concatenating context vector and target hidden value, while there is a tanh in Luong's paper https://arxiv.org/abs/1508.04025.\nI have tested this on some tasks, it seems that it does make some difference, but not significant. Therefore, this is just a kind suggestion.\nThanks!", "body": "Hi @ebrevdo @lmthang ,\r\n\r\nI am wondering if it is desirable to support personalized activation function in attention layer. In current version, no activation is applied after concatenating context vector and target hidden value, while there is a _tanh_ in Luong's paper https://arxiv.org/abs/1508.04025.\r\nI have tested this on some tasks, it seems that it does make some difference, but not significant. Therefore, this is just a kind suggestion.\r\n\r\nThanks!"}