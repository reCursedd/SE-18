{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9635", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9635/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9635/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9635/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9635", "id": 226043134, "node_id": "MDU6SXNzdWUyMjYwNDMxMzQ=", "number": 9635, "title": "Implement of attention mechanisms", "user": {"login": "Songweiping", "id": 11703156, "node_id": "MDQ6VXNlcjExNzAzMTU2", "avatar_url": "https://avatars2.githubusercontent.com/u/11703156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Songweiping", "html_url": "https://github.com/Songweiping", "followers_url": "https://api.github.com/users/Songweiping/followers", "following_url": "https://api.github.com/users/Songweiping/following{/other_user}", "gists_url": "https://api.github.com/users/Songweiping/gists{/gist_id}", "starred_url": "https://api.github.com/users/Songweiping/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Songweiping/subscriptions", "organizations_url": "https://api.github.com/users/Songweiping/orgs", "repos_url": "https://api.github.com/users/Songweiping/repos", "events_url": "https://api.github.com/users/Songweiping/events{/privacy}", "received_events_url": "https://api.github.com/users/Songweiping/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 16, "created_at": "2017-05-03T16:28:41Z", "updated_at": "2018-08-30T23:08:24Z", "closed_at": "2017-06-16T22:25:59Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Here is a question(maybe bug) about implements of different attention mechanisms, i.e. LuongAttention &amp; BahdanauAttention, in contrib/seq2seq/python/ops/attention_wrapper.py. It seems that the only difference is their score functions(alignment models).</p>\n<p>In BahdanauAttention, the alignment model is as described in <a href=\"https://arxiv.org/abs/1412.7449\" rel=\"nofollow\">https://arxiv.org/abs/1412.7449</a></p>\n<pre><code> score = math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query),\n                                    [2])\n</code></pre>\n<p>In LuongAttention, the alignment model is a dot function:<br>\n<code>score = math_ops.matmul(query, self.keys, transpose_b=True)</code></p>\n<p>Then, the context vector is used all in same way. However, there should be more differences between these two attention mechanisms. Is this a modified version of Bahdanau attention proposed in <a href=\"https://arxiv.org/abs/1409.0473\" rel=\"nofollow\">https://arxiv.org/abs/1409.0473</a>?</p>\n<p>Any response would be appreciated. Thanks!</p>", "body_text": "Here is a question(maybe bug) about implements of different attention mechanisms, i.e. LuongAttention & BahdanauAttention, in contrib/seq2seq/python/ops/attention_wrapper.py. It seems that the only difference is their score functions(alignment models).\nIn BahdanauAttention, the alignment model is as described in https://arxiv.org/abs/1412.7449\n score = math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query),\n                                    [2])\n\nIn LuongAttention, the alignment model is a dot function:\nscore = math_ops.matmul(query, self.keys, transpose_b=True)\nThen, the context vector is used all in same way. However, there should be more differences between these two attention mechanisms. Is this a modified version of Bahdanau attention proposed in https://arxiv.org/abs/1409.0473?\nAny response would be appreciated. Thanks!", "body": "Here is a question(maybe bug) about implements of different attention mechanisms, i.e. LuongAttention & BahdanauAttention, in contrib/seq2seq/python/ops/attention_wrapper.py. It seems that the only difference is their score functions(alignment models).\r\n\r\n In BahdanauAttention, the alignment model is as described in https://arxiv.org/abs/1412.7449\r\n\r\n```\r\n score = math_ops.reduce_sum(v * math_ops.tanh(keys + processed_query),\r\n                                    [2])\r\n```\r\n\r\nIn LuongAttention, the alignment model is a dot function:\r\n`score = math_ops.matmul(query, self.keys, transpose_b=True)`\r\n\r\nThen, the context vector is used all in same way. However, there should be more differences between these two attention mechanisms. Is this a modified version of Bahdanau attention proposed in https://arxiv.org/abs/1409.0473?\r\n\r\nAny response would be appreciated. Thanks!"}