{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299653586", "html_url": "https://github.com/tensorflow/tensorflow/issues/9635#issuecomment-299653586", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9635", "id": 299653586, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTY1MzU4Ng==", "user": {"login": "lmthang", "id": 396613, "node_id": "MDQ6VXNlcjM5NjYxMw==", "avatar_url": "https://avatars3.githubusercontent.com/u/396613?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lmthang", "html_url": "https://github.com/lmthang", "followers_url": "https://api.github.com/users/lmthang/followers", "following_url": "https://api.github.com/users/lmthang/following{/other_user}", "gists_url": "https://api.github.com/users/lmthang/gists{/gist_id}", "starred_url": "https://api.github.com/users/lmthang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lmthang/subscriptions", "organizations_url": "https://api.github.com/users/lmthang/orgs", "repos_url": "https://api.github.com/users/lmthang/repos", "events_url": "https://api.github.com/users/lmthang/events{/privacy}", "received_events_url": "https://api.github.com/users/lmthang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-06T17:13:33Z", "updated_at": "2017-05-06T17:18:16Z", "author_association": "NONE", "body_html": "<p>Hi guys,</p>\n<p>Thanks for discussing. I developed the early version of attention_wrapper.py. To me, we replicate most details in Bahdanau and Luong's attention mechanisms, except from the fact that Songweiping mentioned \"In Bahdanau papers, the path is h(t-1)-&gt;a(t)-&gt;c(t)-&gt;h(t), while in Luong it should be h(t)-&gt;a(t)-&gt;c(t)-&gt;h\u02dc(t).\" I believe similar to yhg0112 that there're no significant difference. Besides, I'm gonna release detailed results on which attentions work best and under which settings this month, so stay tune! [ but give scaled LuongAttention a try ;) ]<br>\nps: For the scaled version of LuongAttention and the normalized version of BahdanauAttention, see this paper <a href=\"https://arxiv.org/abs/1704.00784\" rel=\"nofollow\">https://arxiv.org/abs/1704.00784</a>. We simplified scaled LuongAttention a bit when implementing though.</p>", "body_text": "Hi guys,\nThanks for discussing. I developed the early version of attention_wrapper.py. To me, we replicate most details in Bahdanau and Luong's attention mechanisms, except from the fact that Songweiping mentioned \"In Bahdanau papers, the path is h(t-1)->a(t)->c(t)->h(t), while in Luong it should be h(t)->a(t)->c(t)->h\u02dc(t).\" I believe similar to yhg0112 that there're no significant difference. Besides, I'm gonna release detailed results on which attentions work best and under which settings this month, so stay tune! [ but give scaled LuongAttention a try ;) ]\nps: For the scaled version of LuongAttention and the normalized version of BahdanauAttention, see this paper https://arxiv.org/abs/1704.00784. We simplified scaled LuongAttention a bit when implementing though.", "body": "Hi guys,\r\n\r\nThanks for discussing. I developed the early version of attention_wrapper.py. To me, we replicate most details in Bahdanau and Luong's attention mechanisms, except from the fact that Songweiping mentioned \"In Bahdanau papers, the path is h(t-1)->a(t)->c(t)->h(t), while in Luong it should be h(t)->a(t)->c(t)->h\u02dc(t).\" I believe similar to yhg0112 that there're no significant difference. Besides, I'm gonna release detailed results on which attentions work best and under which settings this month, so stay tune! [ but give scaled LuongAttention a try ;) ] \r\nps: For the scaled version of LuongAttention and the normalized version of BahdanauAttention, see this paper https://arxiv.org/abs/1704.00784. We simplified scaled LuongAttention a bit when implementing though.\r\n"}