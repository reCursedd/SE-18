{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/354964602", "html_url": "https://github.com/tensorflow/tensorflow/pull/15612#issuecomment-354964602", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15612", "id": 354964602, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NDk2NDYwMg==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-03T09:07:24Z", "updated_at": "2018-01-03T09:07:24Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for the contributions!</p>\n<p>Adding <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3979096\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/benbarsdell\">@benbarsdell</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1873655\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/nluehr\">@nluehr</a> from NVIDIA here. How reasonable is it that future cudnn can follow this convention that backward batch-norm can happen in place?</p>\n<p>Also for real models, such as resnet50 or inceptionv3, how much is the performance and maximum batch size improvement? If they are quite significant, we could condition this on cudnn-versions. But that is a lot of maintenance overhead, and we could only consider it if the improvement is sizable in real models. Thanks!</p>", "body_text": "Thanks for the contributions!\nAdding @benbarsdell and @nluehr from NVIDIA here. How reasonable is it that future cudnn can follow this convention that backward batch-norm can happen in place?\nAlso for real models, such as resnet50 or inceptionv3, how much is the performance and maximum batch size improvement? If they are quite significant, we could condition this on cudnn-versions. But that is a lot of maintenance overhead, and we could only consider it if the improvement is sizable in real models. Thanks!", "body": "Thanks for the contributions!\r\n\r\nAdding @benbarsdell and @nluehr from NVIDIA here. How reasonable is it that future cudnn can follow this convention that backward batch-norm can happen in place?\r\n\r\nAlso for real models, such as resnet50 or inceptionv3, how much is the performance and maximum batch size improvement? If they are quite significant, we could condition this on cudnn-versions. But that is a lot of maintenance overhead, and we could only consider it if the improvement is sizable in real models. Thanks!"}