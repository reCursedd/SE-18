{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/355179373", "html_url": "https://github.com/tensorflow/tensorflow/pull/15612#issuecomment-355179373", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15612", "id": 355179373, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTE3OTM3Mw==", "user": {"login": "nluehr", "id": 1873655, "node_id": "MDQ6VXNlcjE4NzM2NTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1873655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nluehr", "html_url": "https://github.com/nluehr", "followers_url": "https://api.github.com/users/nluehr/followers", "following_url": "https://api.github.com/users/nluehr/following{/other_user}", "gists_url": "https://api.github.com/users/nluehr/gists{/gist_id}", "starred_url": "https://api.github.com/users/nluehr/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nluehr/subscriptions", "organizations_url": "https://api.github.com/users/nluehr/orgs", "repos_url": "https://api.github.com/users/nluehr/repos", "events_url": "https://api.github.com/users/nluehr/events{/privacy}", "received_events_url": "https://api.github.com/users/nluehr/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-04T01:58:36Z", "updated_at": "2018-01-04T01:58:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>In-place backward batch norm certainly works by accident rather than design. Also, execution can follow a variety of code paths depending on input size, GPU architecture, etc. I wouldn't be surprised if in-place breaks down in some corner cases. As noted above, it certainly could in future releases.</p>\n<p>My assumption is that the memory savings should be modest. The out-of-place buffer is only needed during the actual cudnn call, and would be the size of one batch of activations. I would be interested to learn about any significant gains in important use cases. We might be able to add an explicitly in-place-safe variant.</p>", "body_text": "In-place backward batch norm certainly works by accident rather than design. Also, execution can follow a variety of code paths depending on input size, GPU architecture, etc. I wouldn't be surprised if in-place breaks down in some corner cases. As noted above, it certainly could in future releases.\nMy assumption is that the memory savings should be modest. The out-of-place buffer is only needed during the actual cudnn call, and would be the size of one batch of activations. I would be interested to learn about any significant gains in important use cases. We might be able to add an explicitly in-place-safe variant.", "body": "In-place backward batch norm certainly works by accident rather than design. Also, execution can follow a variety of code paths depending on input size, GPU architecture, etc. I wouldn't be surprised if in-place breaks down in some corner cases. As noted above, it certainly could in future releases.\r\n\r\nMy assumption is that the memory savings should be modest. The out-of-place buffer is only needed during the actual cudnn call, and would be the size of one batch of activations. I would be interested to learn about any significant gains in important use cases. We might be able to add an explicitly in-place-safe variant."}