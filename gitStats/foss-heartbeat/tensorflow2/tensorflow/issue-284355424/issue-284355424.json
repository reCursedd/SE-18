{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15612", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15612/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15612/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15612/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/15612", "id": 284355424, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU5OTk1NDI0", "number": 15612, "title": "Optimize FusedBatchNormGrad.", "user": {"login": "codrut3", "id": 10788581, "node_id": "MDQ6VXNlcjEwNzg4NTgx", "avatar_url": "https://avatars1.githubusercontent.com/u/10788581?v=4", "gravatar_id": "", "url": "https://api.github.com/users/codrut3", "html_url": "https://github.com/codrut3", "followers_url": "https://api.github.com/users/codrut3/followers", "following_url": "https://api.github.com/users/codrut3/following{/other_user}", "gists_url": "https://api.github.com/users/codrut3/gists{/gist_id}", "starred_url": "https://api.github.com/users/codrut3/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/codrut3/subscriptions", "organizations_url": "https://api.github.com/users/codrut3/orgs", "repos_url": "https://api.github.com/users/codrut3/repos", "events_url": "https://api.github.com/users/codrut3/events{/privacy}", "received_events_url": "https://api.github.com/users/codrut3/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 390482148, "node_id": "MDU6TGFiZWwzOTA0ODIxNDg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20review", "name": "awaiting review", "color": "fef2c0", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-12-24T10:38:20Z", "updated_at": "2018-01-13T02:08:45Z", "closed_at": "2018-01-13T02:08:45Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15612", "html_url": "https://github.com/tensorflow/tensorflow/pull/15612", "diff_url": "https://github.com/tensorflow/tensorflow/pull/15612.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/15612.patch"}, "body_html": "<p>Reuse the output buffer and allocate only one temporary tensor, when data format is NHWC and<br>\nGPU is used. This is based on the observation that cudnn can perform the backward computation<br>\nin place. The same idea is used in PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"284306364\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/15601\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/15601/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/15601\">#15601</a> .</p>\n<p>This lowers GPU memory consumption and may improve performance, because fewer distinct memory addresses are accessed. It also permits a higher batch size.</p>\n<p>I've also added a new test for the gradient computation.</p>", "body_text": "Reuse the output buffer and allocate only one temporary tensor, when data format is NHWC and\nGPU is used. This is based on the observation that cudnn can perform the backward computation\nin place. The same idea is used in PR #15601 .\nThis lowers GPU memory consumption and may improve performance, because fewer distinct memory addresses are accessed. It also permits a higher batch size.\nI've also added a new test for the gradient computation.", "body": "Reuse the output buffer and allocate only one temporary tensor, when data format is NHWC and\r\nGPU is used. This is based on the observation that cudnn can perform the backward computation\r\nin place. The same idea is used in PR #15601 . \r\n\r\nThis lowers GPU memory consumption and may improve performance, because fewer distinct memory addresses are accessed. It also permits a higher batch size.\r\n\r\nI've also added a new test for the gradient computation."}