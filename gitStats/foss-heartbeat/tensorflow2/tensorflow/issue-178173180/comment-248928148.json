{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/248928148", "html_url": "https://github.com/tensorflow/tensorflow/issues/4498#issuecomment-248928148", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4498", "id": 248928148, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODkyODE0OA==", "user": {"login": "JianbangZ", "id": 15835199, "node_id": "MDQ6VXNlcjE1ODM1MTk5", "avatar_url": "https://avatars1.githubusercontent.com/u/15835199?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JianbangZ", "html_url": "https://github.com/JianbangZ", "followers_url": "https://api.github.com/users/JianbangZ/followers", "following_url": "https://api.github.com/users/JianbangZ/following{/other_user}", "gists_url": "https://api.github.com/users/JianbangZ/gists{/gist_id}", "starred_url": "https://api.github.com/users/JianbangZ/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JianbangZ/subscriptions", "organizations_url": "https://api.github.com/users/JianbangZ/orgs", "repos_url": "https://api.github.com/users/JianbangZ/repos", "events_url": "https://api.github.com/users/JianbangZ/events{/privacy}", "received_events_url": "https://api.github.com/users/JianbangZ/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-22T14:55:50Z", "updated_at": "2016-09-22T15:19:37Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> I have tried different things to ease the CPU work such as only allowing one worker in ps and get rid of the image preprocessing on ps CPU, but the results is the same. My code is a mimic of the inception distributed example, the code to control training op is the same as this <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\">https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py</a><br>\nHas the team seen data transfer slowness with this implementation?<br>\n<a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> I don't think I am fetching large tensors. As I mentioned to mrry, my code is mostly a mimic of <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\">https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py</a></p>", "body_text": "@mrry I have tried different things to ease the CPU work such as only allowing one worker in ps and get rid of the image preprocessing on ps CPU, but the results is the same. My code is a mimic of the inception distributed example, the code to control training op is the same as this https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\nHas the team seen data transfer slowness with this implementation?\n@yaroslavvb I don't think I am fetching large tensors. As I mentioned to mrry, my code is mostly a mimic of https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py", "body": "@mrry I have tried different things to ease the CPU work such as only allowing one worker in ps and get rid of the image preprocessing on ps CPU, but the results is the same. My code is a mimic of the inception distributed example, the code to control training op is the same as this https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\nHas the team seen data transfer slowness with this implementation?\n@yaroslavvb I don't think I am fetching large tensors. As I mentioned to mrry, my code is mostly a mimic of https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py\n"}