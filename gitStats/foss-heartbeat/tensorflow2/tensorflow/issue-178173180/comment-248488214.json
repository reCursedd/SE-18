{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/248488214", "html_url": "https://github.com/tensorflow/tensorflow/issues/4498#issuecomment-248488214", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4498", "id": 248488214, "node_id": "MDEyOklzc3VlQ29tbWVudDI0ODQ4ODIxNA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-21T01:49:33Z", "updated_at": "2016-09-21T01:49:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi Yaroslav, I think your program is bottlenecked by this line:</p>\n<div class=\"highlight highlight-source-python\"><pre>sess.run(add_op)</pre></div>\n<p>This ensures that the entire result is copied back from the worker to the master to the client, deserialized from a protobuf into Python and discarded. A simple change to the following makes the numbers more promising:</p>\n<div class=\"highlight highlight-source-python\"><pre>sess.run(add_op.op)</pre></div>\n<ul>\n<li>\n<p>Before (with <code>sess.run(add_op)</code>):</p>\n<pre><code>Adding data in 10 MB chunks\nLocal rate:       3693.02 MB per second\nDistributed rate: 115.94 MB per second\n</code></pre>\n</li>\n<li>\n<p>After (with <code>sess.run(add_op.op)</code>):</p>\n<pre><code>Adding data in 10 MB chunks\nLocal rate:       10565.81 MB per second\nDistributed rate: 6379.86 MB per second\n</code></pre>\n</li>\n</ul>\n<p>Indeed, I'm almost certain that the placer will put the constant in your program  on the same device as the <code>assign_add()</code> (because it has only one consumer), so this is purely measuring dispatch overhead. Changing <code>update</code> to be a <code>tf.Variable</code>, so that it is pinned to the different device gives a more realistic number that falls in between the two cases above:</p>\n<ul>\n<li>\n<p>With tensors pinned to different devices (and <code>sess.run(add_op.op)</code>):</p>\n<pre><code>Adding data in 10 MB chunks\nLocal rate:       10259.25 MB per second\nDistributed rate: 832.56 MB per second\n</code></pre>\n</li>\n</ul>\n<p>This throughput obviously still isn't as good as it could be, but I haven't tried very hard to saturate the \"network\" (e.g. by issuing parallel transfers). <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15835199\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/JianbangZ\">@JianbangZ</a>, there are many different reasons why your code might not be saturating your 40GbE network. Being CPU-bound is the most likely reason, if your network is doing anything non-trivial. It's also possible that there's contention on the parameter server, or in some part of the TensorFlow runtime. Without more details about the code you are running, it's impossible to pinpoint the root cause, so if you can share more details, that'd be great!</p>", "body_text": "Hi Yaroslav, I think your program is bottlenecked by this line:\nsess.run(add_op)\nThis ensures that the entire result is copied back from the worker to the master to the client, deserialized from a protobuf into Python and discarded. A simple change to the following makes the numbers more promising:\nsess.run(add_op.op)\n\n\nBefore (with sess.run(add_op)):\nAdding data in 10 MB chunks\nLocal rate:       3693.02 MB per second\nDistributed rate: 115.94 MB per second\n\n\n\nAfter (with sess.run(add_op.op)):\nAdding data in 10 MB chunks\nLocal rate:       10565.81 MB per second\nDistributed rate: 6379.86 MB per second\n\n\n\nIndeed, I'm almost certain that the placer will put the constant in your program  on the same device as the assign_add() (because it has only one consumer), so this is purely measuring dispatch overhead. Changing update to be a tf.Variable, so that it is pinned to the different device gives a more realistic number that falls in between the two cases above:\n\n\nWith tensors pinned to different devices (and sess.run(add_op.op)):\nAdding data in 10 MB chunks\nLocal rate:       10259.25 MB per second\nDistributed rate: 832.56 MB per second\n\n\n\nThis throughput obviously still isn't as good as it could be, but I haven't tried very hard to saturate the \"network\" (e.g. by issuing parallel transfers). @JianbangZ, there are many different reasons why your code might not be saturating your 40GbE network. Being CPU-bound is the most likely reason, if your network is doing anything non-trivial. It's also possible that there's contention on the parameter server, or in some part of the TensorFlow runtime. Without more details about the code you are running, it's impossible to pinpoint the root cause, so if you can share more details, that'd be great!", "body": "Hi Yaroslav, I think your program is bottlenecked by this line:\n\n``` python\nsess.run(add_op)\n```\n\nThis ensures that the entire result is copied back from the worker to the master to the client, deserialized from a protobuf into Python and discarded. A simple change to the following makes the numbers more promising:\n\n``` python\nsess.run(add_op.op)\n```\n- Before (with `sess.run(add_op)`):\n  \n  ```\n  Adding data in 10 MB chunks\n  Local rate:       3693.02 MB per second\n  Distributed rate: 115.94 MB per second\n  ```\n- After (with `sess.run(add_op.op)`):\n  \n  ```\n  Adding data in 10 MB chunks\n  Local rate:       10565.81 MB per second\n  Distributed rate: 6379.86 MB per second\n  ```\n\nIndeed, I'm almost certain that the placer will put the constant in your program  on the same device as the `assign_add()` (because it has only one consumer), so this is purely measuring dispatch overhead. Changing `update` to be a `tf.Variable`, so that it is pinned to the different device gives a more realistic number that falls in between the two cases above:\n- With tensors pinned to different devices (and `sess.run(add_op.op)`):\n  \n  ```\n  Adding data in 10 MB chunks\n  Local rate:       10259.25 MB per second\n  Distributed rate: 832.56 MB per second\n  ```\n\nThis throughput obviously still isn't as good as it could be, but I haven't tried very hard to saturate the \"network\" (e.g. by issuing parallel transfers). @JianbangZ, there are many different reasons why your code might not be saturating your 40GbE network. Being CPU-bound is the most likely reason, if your network is doing anything non-trivial. It's also possible that there's contention on the parameter server, or in some part of the TensorFlow runtime. Without more details about the code you are running, it's impossible to pinpoint the root cause, so if you can share more details, that'd be great!\n"}