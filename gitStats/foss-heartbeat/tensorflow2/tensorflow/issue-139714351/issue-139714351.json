{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1444", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1444/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1444/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1444/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1444", "id": 139714351, "node_id": "MDU6SXNzdWUxMzk3MTQzNTE=", "number": 1444, "title": "SummaryWriter writes constants with the graph definition", "user": {"login": "vgatto", "id": 32080, "node_id": "MDQ6VXNlcjMyMDgw", "avatar_url": "https://avatars3.githubusercontent.com/u/32080?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vgatto", "html_url": "https://github.com/vgatto", "followers_url": "https://api.github.com/users/vgatto/followers", "following_url": "https://api.github.com/users/vgatto/following{/other_user}", "gists_url": "https://api.github.com/users/vgatto/gists{/gist_id}", "starred_url": "https://api.github.com/users/vgatto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vgatto/subscriptions", "organizations_url": "https://api.github.com/users/vgatto/orgs", "repos_url": "https://api.github.com/users/vgatto/repos", "events_url": "https://api.github.com/users/vgatto/events{/privacy}", "received_events_url": "https://api.github.com/users/vgatto/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 8, "created_at": "2016-03-09T22:03:49Z", "updated_at": "2016-06-06T22:48:07Z", "closed_at": "2016-06-06T22:48:06Z", "author_association": "NONE", "body_html": "<p>It looks like SummaryWriter includes the values of constants when writing the graph definition. I'm currently loading word embeddings using python and using the numpy array to initialize a variable. This results in the ~300M of embeddings to be dumped to disk during summarization.  I understand why a constant would be part of the model graph if the goal was to save and restore the graph, but I don't believe this is the goal of the SummaryWriter. Is it possible to strip these out to save disk space?</p>\n<p>Here's a trivial example. If the declaration of <code>random_b</code> is removed, the resulting summary is ~4.6K, but with it, it's about 7.6M.</p>\n<pre><code>import numpy as np\nimport os\nimport tensorflow as tf\n\nsession = tf.Session()\n\nlogdir = \"/tmp/tflogs\"\n\nrandom_a = tf.Variable(tf.random_normal([1000000]))\nrandom_b = tf.Variable(np.random.rand(1000000))\ntf.histogram_summary(\"random_var\", random_a)\nos.makedirs(logdir)\nwriter = tf.train.SummaryWriter(logdir, session.graph_def)\ninit = tf.initialize_all_variables()\nmerged_summary_op = tf.merge_all_summaries()\nsession.run(init)\n\nsummary = session.run(merged_summary_op)\nwriter.add_summary(summary, 0)\n</code></pre>", "body_text": "It looks like SummaryWriter includes the values of constants when writing the graph definition. I'm currently loading word embeddings using python and using the numpy array to initialize a variable. This results in the ~300M of embeddings to be dumped to disk during summarization.  I understand why a constant would be part of the model graph if the goal was to save and restore the graph, but I don't believe this is the goal of the SummaryWriter. Is it possible to strip these out to save disk space?\nHere's a trivial example. If the declaration of random_b is removed, the resulting summary is ~4.6K, but with it, it's about 7.6M.\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nsession = tf.Session()\n\nlogdir = \"/tmp/tflogs\"\n\nrandom_a = tf.Variable(tf.random_normal([1000000]))\nrandom_b = tf.Variable(np.random.rand(1000000))\ntf.histogram_summary(\"random_var\", random_a)\nos.makedirs(logdir)\nwriter = tf.train.SummaryWriter(logdir, session.graph_def)\ninit = tf.initialize_all_variables()\nmerged_summary_op = tf.merge_all_summaries()\nsession.run(init)\n\nsummary = session.run(merged_summary_op)\nwriter.add_summary(summary, 0)", "body": "It looks like SummaryWriter includes the values of constants when writing the graph definition. I'm currently loading word embeddings using python and using the numpy array to initialize a variable. This results in the ~300M of embeddings to be dumped to disk during summarization.  I understand why a constant would be part of the model graph if the goal was to save and restore the graph, but I don't believe this is the goal of the SummaryWriter. Is it possible to strip these out to save disk space?\n\nHere's a trivial example. If the declaration of `random_b` is removed, the resulting summary is ~4.6K, but with it, it's about 7.6M.\n\n```\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nsession = tf.Session()\n\nlogdir = \"/tmp/tflogs\"\n\nrandom_a = tf.Variable(tf.random_normal([1000000]))\nrandom_b = tf.Variable(np.random.rand(1000000))\ntf.histogram_summary(\"random_var\", random_a)\nos.makedirs(logdir)\nwriter = tf.train.SummaryWriter(logdir, session.graph_def)\ninit = tf.initialize_all_variables()\nmerged_summary_op = tf.merge_all_summaries()\nsession.run(init)\n\nsummary = session.run(merged_summary_op)\nwriter.add_summary(summary, 0)\n```\n"}