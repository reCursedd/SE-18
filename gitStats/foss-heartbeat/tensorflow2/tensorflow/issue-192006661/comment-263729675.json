{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263729675", "html_url": "https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-263729675", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5902", "id": 263729675, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzcyOTY3NQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-29T23:07:14Z", "updated_at": "2016-11-29T23:07:14Z", "author_association": "MEMBER", "body_html": "<p>OK, if your real question is \"How do I get a GPU-using TensorFlow program to cooperate effectively with a non-TF GPU program?\" that's a whole collection of difficult issues.</p>\n<p>The short, easy answer is to pipeline them, with the inputs/output staged through CPU RAM, as it sounds like you're doing.  Anything beyond that gets into tricky stuff that I probably shouldn't even suggest.</p>\n<p>Just for illustrative purposes,  here are some thoughts about some of the problems.</p>\n<p>Suppose you're willing to invoke the programs as separate processes, and just want to avoid the CPU RAM staging of values passed between them.  TensorFlow allocates all GPU RAM by default, because that allows us to do memory management more efficiently than by using cudaMalloc.  Via an option one can request that TF only allocate part of the total.  TF can only apply operations to Tensor typed memory regions that it has allocated.  It should be possible to declare a Var to be resident to a GPU, which provides a long-term storage with a static location, but there's no way to specify an address at which that Var should allocate.  Once the Var has been established, a TF graph program can read and store its value.  With considerably hackery on your part, it might be possible to modify the TF runtime to capture the GPU RAM address of an allocated Var in which you're interested, and make it available to another process.  While the TF program is still live, holding the allocated memory, but idle, it <em>might</em> be possible to start another program which uses the GPU and can read/write a location it hasn't allocated itself via pointer.  (GPUs are not (yet) virtualizable, so I think a second process will see the same memory contents and address space as the first.)  So this might point to a crude way of alternating the actions of two separate GPU-using programs on some shared memory, so long as only one of them is a TF program.</p>\n<p>Alternatively, you might want to call a non-TF program in the middle of a TF graph, e.g. by wrapping it in a new Op.  This could work, if you're willing and able to rewrite that non-TF program to obey the TF GPU runtime assumptions about use of stream executor contexts, memory allocators, etc. which are pretty non-standard in the CUDA world.  Simply trying to link and call a pre-existing GPU utility is unlikely to work.</p>\n<p>Hope this clarifies things.</p>", "body_text": "OK, if your real question is \"How do I get a GPU-using TensorFlow program to cooperate effectively with a non-TF GPU program?\" that's a whole collection of difficult issues.\nThe short, easy answer is to pipeline them, with the inputs/output staged through CPU RAM, as it sounds like you're doing.  Anything beyond that gets into tricky stuff that I probably shouldn't even suggest.\nJust for illustrative purposes,  here are some thoughts about some of the problems.\nSuppose you're willing to invoke the programs as separate processes, and just want to avoid the CPU RAM staging of values passed between them.  TensorFlow allocates all GPU RAM by default, because that allows us to do memory management more efficiently than by using cudaMalloc.  Via an option one can request that TF only allocate part of the total.  TF can only apply operations to Tensor typed memory regions that it has allocated.  It should be possible to declare a Var to be resident to a GPU, which provides a long-term storage with a static location, but there's no way to specify an address at which that Var should allocate.  Once the Var has been established, a TF graph program can read and store its value.  With considerably hackery on your part, it might be possible to modify the TF runtime to capture the GPU RAM address of an allocated Var in which you're interested, and make it available to another process.  While the TF program is still live, holding the allocated memory, but idle, it might be possible to start another program which uses the GPU and can read/write a location it hasn't allocated itself via pointer.  (GPUs are not (yet) virtualizable, so I think a second process will see the same memory contents and address space as the first.)  So this might point to a crude way of alternating the actions of two separate GPU-using programs on some shared memory, so long as only one of them is a TF program.\nAlternatively, you might want to call a non-TF program in the middle of a TF graph, e.g. by wrapping it in a new Op.  This could work, if you're willing and able to rewrite that non-TF program to obey the TF GPU runtime assumptions about use of stream executor contexts, memory allocators, etc. which are pretty non-standard in the CUDA world.  Simply trying to link and call a pre-existing GPU utility is unlikely to work.\nHope this clarifies things.", "body": "OK, if your real question is \"How do I get a GPU-using TensorFlow program to cooperate effectively with a non-TF GPU program?\" that's a whole collection of difficult issues.  \r\n\r\nThe short, easy answer is to pipeline them, with the inputs/output staged through CPU RAM, as it sounds like you're doing.  Anything beyond that gets into tricky stuff that I probably shouldn't even suggest. \r\n\r\nJust for illustrative purposes,  here are some thoughts about some of the problems.\r\n\r\nSuppose you're willing to invoke the programs as separate processes, and just want to avoid the CPU RAM staging of values passed between them.  TensorFlow allocates all GPU RAM by default, because that allows us to do memory management more efficiently than by using cudaMalloc.  Via an option one can request that TF only allocate part of the total.  TF can only apply operations to Tensor typed memory regions that it has allocated.  It should be possible to declare a Var to be resident to a GPU, which provides a long-term storage with a static location, but there's no way to specify an address at which that Var should allocate.  Once the Var has been established, a TF graph program can read and store its value.  With considerably hackery on your part, it might be possible to modify the TF runtime to capture the GPU RAM address of an allocated Var in which you're interested, and make it available to another process.  While the TF program is still live, holding the allocated memory, but idle, it *might* be possible to start another program which uses the GPU and can read/write a location it hasn't allocated itself via pointer.  (GPUs are not (yet) virtualizable, so I think a second process will see the same memory contents and address space as the first.)  So this might point to a crude way of alternating the actions of two separate GPU-using programs on some shared memory, so long as only one of them is a TF program.\r\n\r\nAlternatively, you might want to call a non-TF program in the middle of a TF graph, e.g. by wrapping it in a new Op.  This could work, if you're willing and able to rewrite that non-TF program to obey the TF GPU runtime assumptions about use of stream executor contexts, memory allocators, etc. which are pretty non-standard in the CUDA world.  Simply trying to link and call a pre-existing GPU utility is unlikely to work.\r\n\r\nHope this clarifies things.\r\n"}