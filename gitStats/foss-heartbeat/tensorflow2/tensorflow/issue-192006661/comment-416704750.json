{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/416704750", "html_url": "https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-416704750", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5902", "id": 416704750, "node_id": "MDEyOklzc3VlQ29tbWVudDQxNjcwNDc1MA==", "user": {"login": "kidtronnix", "id": 4520386, "node_id": "MDQ6VXNlcjQ1MjAzODY=", "avatar_url": "https://avatars3.githubusercontent.com/u/4520386?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kidtronnix", "html_url": "https://github.com/kidtronnix", "followers_url": "https://api.github.com/users/kidtronnix/followers", "following_url": "https://api.github.com/users/kidtronnix/following{/other_user}", "gists_url": "https://api.github.com/users/kidtronnix/gists{/gist_id}", "starred_url": "https://api.github.com/users/kidtronnix/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kidtronnix/subscriptions", "organizations_url": "https://api.github.com/users/kidtronnix/orgs", "repos_url": "https://api.github.com/users/kidtronnix/repos", "events_url": "https://api.github.com/users/kidtronnix/events{/privacy}", "received_events_url": "https://api.github.com/users/kidtronnix/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-28T19:06:46Z", "updated_at": "2018-08-28T19:41:23Z", "author_association": "NONE", "body_html": "<p>+1 on this feature, especially for python.</p>\n<p>Let me give a definitive use case, for same data different parameter / architecture jobs, we have the same data-set being used to train and evaluate over and over again. If we have 100 different combinations to try, this means we will copy from disk -&gt; cpu mem -&gt; gpu mem 100 times. ALL nvidia documentation suggests limiting data transfer between device and host as the primary optimisation.</p>\n<p>Currently TF is a blocker for us using in certain production use cases because of this.</p>", "body_text": "+1 on this feature, especially for python.\nLet me give a definitive use case, for same data different parameter / architecture jobs, we have the same data-set being used to train and evaluate over and over again. If we have 100 different combinations to try, this means we will copy from disk -> cpu mem -> gpu mem 100 times. ALL nvidia documentation suggests limiting data transfer between device and host as the primary optimisation.\nCurrently TF is a blocker for us using in certain production use cases because of this.", "body": "+1 on this feature, especially for python.\r\n\r\nLet me give a definitive use case, for same data different parameter / architecture jobs, we have the same data-set being used to train and evaluate over and over again. If we have 100 different combinations to try, this means we will copy from disk -> cpu mem -> gpu mem 100 times. ALL nvidia documentation suggests limiting data transfer between device and host as the primary optimisation.\r\n\r\nCurrently TF is a blocker for us using in certain production use cases because of this."}