{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263999874", "html_url": "https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-263999874", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5902", "id": 263999874, "node_id": "MDEyOklzc3VlQ29tbWVudDI2Mzk5OTg3NA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-30T21:24:35Z", "updated_at": "2016-11-30T21:24:35Z", "author_association": "MEMBER", "body_html": "<p>The session-&gt;Run() function computes the the subgraph closure induced by the specified outputs, in other words, it computes forward from consts, Vars or fed Nodes only along paths that lead to the requested outputs.   Given this situation, if you already have figured out how to copy the values computed by A into a TF allocated tensor, why not copy them into a GPU resident Var, instead of into some other kind of tensor, and skip the explicit feed argument to Run()?  If you're working in C++, you can use DMAHelper to get the address of the backing buffer for the memcpy.</p>\n<p>I agree it could be handy to have Session::Run() recognize that a feed tensor may already be GPU resident.  I guess that whoever wrote that code assumed it would never happen, so it could take a while to discover everything that needs to be fixed.</p>", "body_text": "The session->Run() function computes the the subgraph closure induced by the specified outputs, in other words, it computes forward from consts, Vars or fed Nodes only along paths that lead to the requested outputs.   Given this situation, if you already have figured out how to copy the values computed by A into a TF allocated tensor, why not copy them into a GPU resident Var, instead of into some other kind of tensor, and skip the explicit feed argument to Run()?  If you're working in C++, you can use DMAHelper to get the address of the backing buffer for the memcpy.\nI agree it could be handy to have Session::Run() recognize that a feed tensor may already be GPU resident.  I guess that whoever wrote that code assumed it would never happen, so it could take a while to discover everything that needs to be fixed.", "body": "The session->Run() function computes the the subgraph closure induced by the specified outputs, in other words, it computes forward from consts, Vars or fed Nodes only along paths that lead to the requested outputs.   Given this situation, if you already have figured out how to copy the values computed by A into a TF allocated tensor, why not copy them into a GPU resident Var, instead of into some other kind of tensor, and skip the explicit feed argument to Run()?  If you're working in C++, you can use DMAHelper to get the address of the backing buffer for the memcpy.\r\n\r\nI agree it could be handy to have Session::Run() recognize that a feed tensor may already be GPU resident.  I guess that whoever wrote that code assumed it would never happen, so it could take a while to discover everything that needs to be fixed."}