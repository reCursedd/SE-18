{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263944891", "html_url": "https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-263944891", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5902", "id": 263944891, "node_id": "MDEyOklzc3VlQ29tbWVudDI2Mzk0NDg5MQ==", "user": {"login": "larsmennen", "id": 1162951, "node_id": "MDQ6VXNlcjExNjI5NTE=", "avatar_url": "https://avatars2.githubusercontent.com/u/1162951?v=4", "gravatar_id": "", "url": "https://api.github.com/users/larsmennen", "html_url": "https://github.com/larsmennen", "followers_url": "https://api.github.com/users/larsmennen/followers", "following_url": "https://api.github.com/users/larsmennen/following{/other_user}", "gists_url": "https://api.github.com/users/larsmennen/gists{/gist_id}", "starred_url": "https://api.github.com/users/larsmennen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/larsmennen/subscriptions", "organizations_url": "https://api.github.com/users/larsmennen/orgs", "repos_url": "https://api.github.com/users/larsmennen/repos", "events_url": "https://api.github.com/users/larsmennen/events{/privacy}", "received_events_url": "https://api.github.com/users/larsmennen/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-30T17:54:37Z", "updated_at": "2016-11-30T17:54:55Z", "author_association": "NONE", "body_html": "<p>Thank you again for your quick and detailed reply.</p>\n<p>That is exactly the question. My problem does not relate to calling a non-TF program in the middle of a TF graph.</p>\n<p>Let's say program A runs on the GPU (non-TF) and I want to feed its output (which is in GPU memory) into a TF network (program B, which uses the TF C++ API).<br>\nI understand that the easiest way is to do: copy output of A back to CPU memory -&gt; run program B. TF will then internally copy the data to GPU memory and start running the graph.</p>\n<p>Now, I'm looking for a way to bypass the process of staging it through CPU RAM, because from a performance point of view this seems to be unnecessary. It would be more efficient if we could do a memory copy from GPU memory to GPU memory, i.e. copying the output of program A to the allocated memory for the input tensor of program B, both living on GPU memory.</p>\n<p>As you mention there are some problems there.<br>\nI understand that TensorFlow cannot operate on memory that it didn't allocate, but I am using a TF allocator:</p>\n<div class=\"highlight highlight-source-c++\"><pre>tensorflow::GPUBFCAllocator* allocator = <span class=\"pl-k\">new</span> tensorflow::GPUBFCAllocator(<span class=\"pl-c1\">0</span>, <span class=\"pl-k\">sizeof</span>(<span class=\"pl-k\">float</span>) * height * width * <span class=\"pl-c1\">3</span>);\ntensorflow::Tensor input_tensor = tensorflow::Tensor(allocator, tensorflow::DataType::DT_FLOAT, tensorflow::TensorShape( { <span class=\"pl-c1\">1</span>, height, width, <span class=\"pl-c1\">3</span> }));\n&lt;copy output data from program A into the GPU memory allocated by input_tensor <span class=\"pl-k\">using</span> a GPU-&gt;GPU copy&gt;</pre></div>\n<p>So TF allocated a Tensor on GPU memory, holding our input data (i.e. the output of program A), and I do not see why TF will now still insist on staging this through CPU RAM. All of this we can do at runtime.</p>\n<p>I think it would be a good feature addition to skip staging through CPU RAM if the input tensor has its memory allocated on the GPU using a TF GPU allocator.<br>\nSimilarly, there could be a flag to keep output tensors on GPU memory and not stage them back to CPU RAM. It would then be the users responsibility of course to deal with this, but it will allow users to not waste any time copying back and forth to CPU RAM in performance-critical environments.</p>\n<p>The solution using Vars could work I think, but I think it is a bit of a workaround and making this a feature could be useful to more users who are trying to optimize setups where TF is getting input/output from other programs operating with GPU memory.</p>\n<p>What are your thoughts on this?</p>", "body_text": "Thank you again for your quick and detailed reply.\nThat is exactly the question. My problem does not relate to calling a non-TF program in the middle of a TF graph.\nLet's say program A runs on the GPU (non-TF) and I want to feed its output (which is in GPU memory) into a TF network (program B, which uses the TF C++ API).\nI understand that the easiest way is to do: copy output of A back to CPU memory -> run program B. TF will then internally copy the data to GPU memory and start running the graph.\nNow, I'm looking for a way to bypass the process of staging it through CPU RAM, because from a performance point of view this seems to be unnecessary. It would be more efficient if we could do a memory copy from GPU memory to GPU memory, i.e. copying the output of program A to the allocated memory for the input tensor of program B, both living on GPU memory.\nAs you mention there are some problems there.\nI understand that TensorFlow cannot operate on memory that it didn't allocate, but I am using a TF allocator:\ntensorflow::GPUBFCAllocator* allocator = new tensorflow::GPUBFCAllocator(0, sizeof(float) * height * width * 3);\ntensorflow::Tensor input_tensor = tensorflow::Tensor(allocator, tensorflow::DataType::DT_FLOAT, tensorflow::TensorShape( { 1, height, width, 3 }));\n<copy output data from program A into the GPU memory allocated by input_tensor using a GPU->GPU copy>\nSo TF allocated a Tensor on GPU memory, holding our input data (i.e. the output of program A), and I do not see why TF will now still insist on staging this through CPU RAM. All of this we can do at runtime.\nI think it would be a good feature addition to skip staging through CPU RAM if the input tensor has its memory allocated on the GPU using a TF GPU allocator.\nSimilarly, there could be a flag to keep output tensors on GPU memory and not stage them back to CPU RAM. It would then be the users responsibility of course to deal with this, but it will allow users to not waste any time copying back and forth to CPU RAM in performance-critical environments.\nThe solution using Vars could work I think, but I think it is a bit of a workaround and making this a feature could be useful to more users who are trying to optimize setups where TF is getting input/output from other programs operating with GPU memory.\nWhat are your thoughts on this?", "body": "Thank you again for your quick and detailed reply.\r\n\r\nThat is exactly the question. My problem does not relate to calling a non-TF program in the middle of a TF graph.\r\n\r\nLet's say program A runs on the GPU (non-TF) and I want to feed its output (which is in GPU memory) into a TF network (program B, which uses the TF C++ API).\r\nI understand that the easiest way is to do: copy output of A back to CPU memory -> run program B. TF will then internally copy the data to GPU memory and start running the graph.\r\n\r\nNow, I'm looking for a way to bypass the process of staging it through CPU RAM, because from a performance point of view this seems to be unnecessary. It would be more efficient if we could do a memory copy from GPU memory to GPU memory, i.e. copying the output of program A to the allocated memory for the input tensor of program B, both living on GPU memory. \r\n\r\nAs you mention there are some problems there.\r\nI understand that TensorFlow cannot operate on memory that it didn't allocate, but I am using a TF allocator:\r\n```cpp\r\ntensorflow::GPUBFCAllocator* allocator = new tensorflow::GPUBFCAllocator(0, sizeof(float) * height * width * 3);\r\ntensorflow::Tensor input_tensor = tensorflow::Tensor(allocator, tensorflow::DataType::DT_FLOAT, tensorflow::TensorShape( { 1, height, width, 3 }));\r\n<copy output data from program A into the GPU memory allocated by input_tensor using a GPU->GPU copy>\r\n```\r\nSo TF allocated a Tensor on GPU memory, holding our input data (i.e. the output of program A), and I do not see why TF will now still insist on staging this through CPU RAM. All of this we can do at runtime.\r\n\r\nI think it would be a good feature addition to skip staging through CPU RAM if the input tensor has its memory allocated on the GPU using a TF GPU allocator.\r\nSimilarly, there could be a flag to keep output tensors on GPU memory and not stage them back to CPU RAM. It would then be the users responsibility of course to deal with this, but it will allow users to not waste any time copying back and forth to CPU RAM in performance-critical environments.\r\n\r\nThe solution using Vars could work I think, but I think it is a bit of a workaround and making this a feature could be useful to more users who are trying to optimize setups where TF is getting input/output from other programs operating with GPU memory.\r\n\r\nWhat are your thoughts on this?"}