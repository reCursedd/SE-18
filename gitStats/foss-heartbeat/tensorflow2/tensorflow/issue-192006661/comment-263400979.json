{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263400979", "html_url": "https://github.com/tensorflow/tensorflow/issues/5902#issuecomment-263400979", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5902", "id": 263400979, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzQwMDk3OQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-28T21:32:34Z", "updated_at": "2016-11-28T21:32:34Z", "author_association": "MEMBER", "body_html": "<p>Thanks for your clear and detailed issue report.</p>\n<p>Your analysis is correct.  GPU tensors are always fed by establishing a CPU tensor and copying its value to the GPU.  In the other direction, when reading the output of a graph computation, if the value you want to read is produced on the GPU, it must be copied back to the CPU before producing I/O e.g. writing to a file or window.</p>\n<p>The basic reason is that the GPU is simply an auxillary processor with very limited capability to engage in  I/O operations.   It is not feasible to e.g. read from local disk directly into GPU memory without first staging through CPU memory.  In principle this should not pose a performance problem, so long as you're able to to buffer input into CPU RAM ahead of it's being needed on the GPU.</p>\n<p>If you want to combine a TF program with a non-TF program that both run on GPU, without copying data back and forth to CPU RAM, I think you will need to arrange for input/output values to be in Vars that are local to the GPU, and somehow make them accessible to the non-TF program.  Alternatively, maybe you can package your non-TF program as a single TF Op.</p>", "body_text": "Thanks for your clear and detailed issue report.\nYour analysis is correct.  GPU tensors are always fed by establishing a CPU tensor and copying its value to the GPU.  In the other direction, when reading the output of a graph computation, if the value you want to read is produced on the GPU, it must be copied back to the CPU before producing I/O e.g. writing to a file or window.\nThe basic reason is that the GPU is simply an auxillary processor with very limited capability to engage in  I/O operations.   It is not feasible to e.g. read from local disk directly into GPU memory without first staging through CPU memory.  In principle this should not pose a performance problem, so long as you're able to to buffer input into CPU RAM ahead of it's being needed on the GPU.\nIf you want to combine a TF program with a non-TF program that both run on GPU, without copying data back and forth to CPU RAM, I think you will need to arrange for input/output values to be in Vars that are local to the GPU, and somehow make them accessible to the non-TF program.  Alternatively, maybe you can package your non-TF program as a single TF Op.", "body": "Thanks for your clear and detailed issue report.\r\n\r\nYour analysis is correct.  GPU tensors are always fed by establishing a CPU tensor and copying its value to the GPU.  In the other direction, when reading the output of a graph computation, if the value you want to read is produced on the GPU, it must be copied back to the CPU before producing I/O e.g. writing to a file or window.\r\n\r\nThe basic reason is that the GPU is simply an auxillary processor with very limited capability to engage in  I/O operations.   It is not feasible to e.g. read from local disk directly into GPU memory without first staging through CPU memory.  In principle this should not pose a performance problem, so long as you're able to to buffer input into CPU RAM ahead of it's being needed on the GPU.\r\n\r\nIf you want to combine a TF program with a non-TF program that both run on GPU, without copying data back and forth to CPU RAM, I think you will need to arrange for input/output values to be in Vars that are local to the GPU, and somehow make them accessible to the non-TF program.  Alternatively, maybe you can package your non-TF program as a single TF Op.\r\n\r\n\r\n\r\n\r\n"}