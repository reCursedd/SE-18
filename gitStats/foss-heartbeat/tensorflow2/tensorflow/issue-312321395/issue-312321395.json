{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18326", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18326/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18326/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18326/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18326", "id": 312321395, "node_id": "MDU6SXNzdWUzMTIzMjEzOTU=", "number": 18326, "title": "Input shape Error, bidirectional rnn", "user": {"login": "XMaster96", "id": 28674439, "node_id": "MDQ6VXNlcjI4Njc0NDM5", "avatar_url": "https://avatars2.githubusercontent.com/u/28674439?v=4", "gravatar_id": "", "url": "https://api.github.com/users/XMaster96", "html_url": "https://github.com/XMaster96", "followers_url": "https://api.github.com/users/XMaster96/followers", "following_url": "https://api.github.com/users/XMaster96/following{/other_user}", "gists_url": "https://api.github.com/users/XMaster96/gists{/gist_id}", "starred_url": "https://api.github.com/users/XMaster96/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/XMaster96/subscriptions", "organizations_url": "https://api.github.com/users/XMaster96/orgs", "repos_url": "https://api.github.com/users/XMaster96/repos", "events_url": "https://api.github.com/users/XMaster96/events{/privacy}", "received_events_url": "https://api.github.com/users/XMaster96/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-04-08T16:58:02Z", "updated_at": "2018-09-15T18:37:43Z", "closed_at": "2018-09-15T18:29:26Z", "author_association": "NONE", "body_html": "<p>System Info:<br>\n- OS: Windows 7<br>\n- Cude: 9, 9.1<br>\n- Tensorflow: 1.7.0<br>\n- GPU Mode</p>\n<p>I have some problems with bidirectional rnn inputs. After my anderstanging, a bidirectional  rnn needs a input from the shape \"[batch_size, input_size]\".<br>\nMay input \"hidden\" has first the shape (?, 512), I then reshape is to (1, 512) and expande the dims to<br>\n(1, 1, 512). I treid to pass all three versions to the bidirectional  rnn (and many more).  I have already used the last shap successful, with a normal rnn.</p>\n<p>Error:</p>\n<pre><code>Traceback (most recent call last):\n  File \"X:/PyProjects/Domm_Lerner/A3C_Arthur_Julianis_Doom.py\", line 572, in &lt;module&gt;\n    master_network = AC_Network(s_size, a_size, cnn_size, 'global', None)  # Generate global network\n  File \"X:/PyProjects/Domm_Lerner/A3C_Arthur_Julianis_Doom.py\", line 159, in __init__\n    lstm_1_outputs, lstm_1_state_fw, lstm_1_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, rnn_in,         initial_state_fw=state_1_in_fw, initial_state_bw=state_1_in_bw, sequence_length=step_size)\n  File \"X:\\PyProjects\\Domm_Lerner\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 1478, in static_bidirectional_rnn\n    raise TypeError(\"inputs must be a sequence\")\nTypeError: inputs must be a sequence\n</code></pre>\n<p>Code:</p>\n<pre><code>            hidden = tf.reshape(hidden, shape=[1, cnn_size])\n            rnn_in = tf.expand_dims(hidden, [0])\n\n            step_size = tf.shape(self.imageIn)[:1]\n\n            lstm_1_outputs, lstm_1_state_fw, lstm_1_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, rnn_in,         initial_state_fw=state_1_in_fw, initial_state_bw=state_1_in_bw, sequence_length=step_size)\n            lstm_2_outputs, lstm_2_state_fw, lstm_2_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_1_outputs, initial_state_fw=state_2_in_fw, initial_state_bw=state_2_in_bw, sequence_length=step_size)\n            lstm_3_outputs, lstm_3_state_fw, lstm_3_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_2_outputs, initial_state_fw=state_3_in_fw, initial_state_bw=state_3_in_bw, sequence_length=step_size)\n            lstm_4_outputs, lstm_4_state_fw, lstm_4_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_3_outputs, initial_state_fw=state_4_in_fw, initial_state_bw=state_4_in_bw, sequence_length=step_size)\n            lstm_5_outputs, lstm_5_state_fw, lstm_5_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_4_outputs, initial_state_fw=state_5_in_fw, initial_state_bw=state_5_in_bw, sequence_length=step_size)\n            lstm_6_outputs, lstm_6_state_fw, lstm_6_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_5_outputs, initial_state_fw=state_6_in_fw, initial_state_bw=state_6_in_bw, sequence_length=step_size)\n</code></pre>", "body_text": "System Info:\n- OS: Windows 7\n- Cude: 9, 9.1\n- Tensorflow: 1.7.0\n- GPU Mode\nI have some problems with bidirectional rnn inputs. After my anderstanging, a bidirectional  rnn needs a input from the shape \"[batch_size, input_size]\".\nMay input \"hidden\" has first the shape (?, 512), I then reshape is to (1, 512) and expande the dims to\n(1, 1, 512). I treid to pass all three versions to the bidirectional  rnn (and many more).  I have already used the last shap successful, with a normal rnn.\nError:\nTraceback (most recent call last):\n  File \"X:/PyProjects/Domm_Lerner/A3C_Arthur_Julianis_Doom.py\", line 572, in <module>\n    master_network = AC_Network(s_size, a_size, cnn_size, 'global', None)  # Generate global network\n  File \"X:/PyProjects/Domm_Lerner/A3C_Arthur_Julianis_Doom.py\", line 159, in __init__\n    lstm_1_outputs, lstm_1_state_fw, lstm_1_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, rnn_in,         initial_state_fw=state_1_in_fw, initial_state_bw=state_1_in_bw, sequence_length=step_size)\n  File \"X:\\PyProjects\\Domm_Lerner\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 1478, in static_bidirectional_rnn\n    raise TypeError(\"inputs must be a sequence\")\nTypeError: inputs must be a sequence\n\nCode:\n            hidden = tf.reshape(hidden, shape=[1, cnn_size])\n            rnn_in = tf.expand_dims(hidden, [0])\n\n            step_size = tf.shape(self.imageIn)[:1]\n\n            lstm_1_outputs, lstm_1_state_fw, lstm_1_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, rnn_in,         initial_state_fw=state_1_in_fw, initial_state_bw=state_1_in_bw, sequence_length=step_size)\n            lstm_2_outputs, lstm_2_state_fw, lstm_2_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_1_outputs, initial_state_fw=state_2_in_fw, initial_state_bw=state_2_in_bw, sequence_length=step_size)\n            lstm_3_outputs, lstm_3_state_fw, lstm_3_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_2_outputs, initial_state_fw=state_3_in_fw, initial_state_bw=state_3_in_bw, sequence_length=step_size)\n            lstm_4_outputs, lstm_4_state_fw, lstm_4_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_3_outputs, initial_state_fw=state_4_in_fw, initial_state_bw=state_4_in_bw, sequence_length=step_size)\n            lstm_5_outputs, lstm_5_state_fw, lstm_5_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_4_outputs, initial_state_fw=state_5_in_fw, initial_state_bw=state_5_in_bw, sequence_length=step_size)\n            lstm_6_outputs, lstm_6_state_fw, lstm_6_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_5_outputs, initial_state_fw=state_6_in_fw, initial_state_bw=state_6_in_bw, sequence_length=step_size)", "body": "System Info:\r\n    - OS: Windows 7\r\n    - Cude: 9, 9.1\r\n    - Tensorflow: 1.7.0\r\n    - GPU Mode\r\n\r\n\r\nI have some problems with bidirectional rnn inputs. After my anderstanging, a bidirectional  rnn needs a input from the shape \"[batch_size, input_size]\".  \r\nMay input \"hidden\" has first the shape (?, 512), I then reshape is to (1, 512) and expande the dims to\r\n(1, 1, 512). I treid to pass all three versions to the bidirectional  rnn (and many more).  I have already used the last shap successful, with a normal rnn.\r\n\r\n\r\n\r\n\r\nError:\r\n```\r\nTraceback (most recent call last):\r\n  File \"X:/PyProjects/Domm_Lerner/A3C_Arthur_Julianis_Doom.py\", line 572, in <module>\r\n    master_network = AC_Network(s_size, a_size, cnn_size, 'global', None)  # Generate global network\r\n  File \"X:/PyProjects/Domm_Lerner/A3C_Arthur_Julianis_Doom.py\", line 159, in __init__\r\n    lstm_1_outputs, lstm_1_state_fw, lstm_1_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, rnn_in,         initial_state_fw=state_1_in_fw, initial_state_bw=state_1_in_bw, sequence_length=step_size)\r\n  File \"X:\\PyProjects\\Domm_Lerner\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py\", line 1478, in static_bidirectional_rnn\r\n    raise TypeError(\"inputs must be a sequence\")\r\nTypeError: inputs must be a sequence\r\n```\r\n \r\n \r\n\r\nCode:\r\n```\r\n            hidden = tf.reshape(hidden, shape=[1, cnn_size])\r\n            rnn_in = tf.expand_dims(hidden, [0])\r\n\r\n            step_size = tf.shape(self.imageIn)[:1]\r\n\r\n            lstm_1_outputs, lstm_1_state_fw, lstm_1_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, rnn_in,         initial_state_fw=state_1_in_fw, initial_state_bw=state_1_in_bw, sequence_length=step_size)\r\n            lstm_2_outputs, lstm_2_state_fw, lstm_2_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_1_outputs, initial_state_fw=state_2_in_fw, initial_state_bw=state_2_in_bw, sequence_length=step_size)\r\n            lstm_3_outputs, lstm_3_state_fw, lstm_3_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_2_outputs, initial_state_fw=state_3_in_fw, initial_state_bw=state_3_in_bw, sequence_length=step_size)\r\n            lstm_4_outputs, lstm_4_state_fw, lstm_4_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_3_outputs, initial_state_fw=state_4_in_fw, initial_state_bw=state_4_in_bw, sequence_length=step_size)\r\n            lstm_5_outputs, lstm_5_state_fw, lstm_5_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_4_outputs, initial_state_fw=state_5_in_fw, initial_state_bw=state_5_in_bw, sequence_length=step_size)\r\n            lstm_6_outputs, lstm_6_state_fw, lstm_6_state_bw = tf.nn.static_bidirectional_rnn(lstm_cell, lstm_cell, lstm_5_outputs, initial_state_fw=state_6_in_fw, initial_state_bw=state_6_in_bw, sequence_length=step_size)\r\n```\r\n"}