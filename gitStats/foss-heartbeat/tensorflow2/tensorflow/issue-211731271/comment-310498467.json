{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310498467", "html_url": "https://github.com/tensorflow/tensorflow/issues/8057#issuecomment-310498467", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8057", "id": 310498467, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDQ5ODQ2Nw==", "user": {"login": "RylanSchaeffer", "id": 8942987, "node_id": "MDQ6VXNlcjg5NDI5ODc=", "avatar_url": "https://avatars3.githubusercontent.com/u/8942987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/RylanSchaeffer", "html_url": "https://github.com/RylanSchaeffer", "followers_url": "https://api.github.com/users/RylanSchaeffer/followers", "following_url": "https://api.github.com/users/RylanSchaeffer/following{/other_user}", "gists_url": "https://api.github.com/users/RylanSchaeffer/gists{/gist_id}", "starred_url": "https://api.github.com/users/RylanSchaeffer/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/RylanSchaeffer/subscriptions", "organizations_url": "https://api.github.com/users/RylanSchaeffer/orgs", "repos_url": "https://api.github.com/users/RylanSchaeffer/repos", "events_url": "https://api.github.com/users/RylanSchaeffer/events{/privacy}", "received_events_url": "https://api.github.com/users/RylanSchaeffer/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-22T20:49:24Z", "updated_at": "2017-06-22T20:56:30Z", "author_association": "NONE", "body_html": "<p>I'm happy to post all my code, but tell me if this is sufficient. Here's my main function, where execution starts.</p>\n<pre><code>def main(_):\n    x_minibatch, y_minibatch, y_lengths_minibatch = construct_data_pipeline()\n    model = import_model()\n    train(model=model, x_minibatch=x_minibatch, y_minibatch=y_minibatch, y_lengths_minibatch=y_lengths_minibatch)\n</code></pre>\n<p><code>import_model()</code> imports my <code>Model</code> class, which has the following constructor:</p>\n<pre><code>class Model(object):\n    def __init__(self, batch_size, initial_learning_rate):\n        self.x, self.y_actual, self.y_actual_lengths = self._define_inputs(batch_size)\n        self.encoder_outputs, self.encoder_final_states, self.decoder_outputs, self.y_predicted_logits = self._define_model()\n        self.loss = self._define_loss()\n        self.metrics = self._define_metrics()\n        self.optimizer = self._define_optimizer(initial_learning_rate)\n</code></pre>\n<p><code>_define_optimizer(initial_learning_rate)</code> is defined as follows:</p>\n<pre><code>    @staticmethod\n    def _define_optimizer(initial_learning_rate):\n        with tf.name_scope('define_optimizer'):\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=initial_learning_rate)\n            # optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate)\n        return optimizer\n</code></pre>\n<p>Then, <code>main</code> calls <code>train()</code>, which is defined as follows:</p>\n<pre><code>def train(model, x_minibatch, y_minibatch, y_lengths_minibatch):\n    with tf.Session() as sess:\n\n        # create coordinator to handle threading\n        coord = tf.train.Coordinator()\n\n        # start threads to enqueue input minibatches for training\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        # initialize all variables and ops\n        sess.run(tf.global_variables_initializer())\n\n        start_time = time.time()\n\n        # train\n        for step in range(tf.app.flags.FLAGS.training_steps):\n\n            train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch)\n\n            # every 100 steps, evaluate model metrics and write summaries to disk\n            if (step + 1) % 10 == 0 or step == tf.app.flags.FLAGS.training_steps - 1:\n                eval_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch, step, start_time)\n                start_time = time.time()\n\n        # when done, ask the threads to stop\n        coord.request_stop()\n\n        # wait for threads to finish\n        coord.join(threads)\n</code></pre>\n<p>My <code>train_op</code> is defined as follows:</p>\n<pre><code>def train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch):\n    x_values, y_values, y_lengths_values = sess.run([x_minibatch, y_minibatch, y_lengths_minibatch])\n\n    # minimize loss\n    sess.run([model.optimizer.minimize(model.loss), model.y_predicted_logits],\n                                     feed_dict={model.x: x_values,\n                                                model.y_actual: y_values,\n                                                model.y_actual_lengths: y_lengths_values})\n</code></pre>\n<p>As you can see, the model is constructed before I call <code>sess.run(tf.global_variables_initializer())</code> and isn't touched afterwards.</p>", "body_text": "I'm happy to post all my code, but tell me if this is sufficient. Here's my main function, where execution starts.\ndef main(_):\n    x_minibatch, y_minibatch, y_lengths_minibatch = construct_data_pipeline()\n    model = import_model()\n    train(model=model, x_minibatch=x_minibatch, y_minibatch=y_minibatch, y_lengths_minibatch=y_lengths_minibatch)\n\nimport_model() imports my Model class, which has the following constructor:\nclass Model(object):\n    def __init__(self, batch_size, initial_learning_rate):\n        self.x, self.y_actual, self.y_actual_lengths = self._define_inputs(batch_size)\n        self.encoder_outputs, self.encoder_final_states, self.decoder_outputs, self.y_predicted_logits = self._define_model()\n        self.loss = self._define_loss()\n        self.metrics = self._define_metrics()\n        self.optimizer = self._define_optimizer(initial_learning_rate)\n\n_define_optimizer(initial_learning_rate) is defined as follows:\n    @staticmethod\n    def _define_optimizer(initial_learning_rate):\n        with tf.name_scope('define_optimizer'):\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=initial_learning_rate)\n            # optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate)\n        return optimizer\n\nThen, main calls train(), which is defined as follows:\ndef train(model, x_minibatch, y_minibatch, y_lengths_minibatch):\n    with tf.Session() as sess:\n\n        # create coordinator to handle threading\n        coord = tf.train.Coordinator()\n\n        # start threads to enqueue input minibatches for training\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\n        # initialize all variables and ops\n        sess.run(tf.global_variables_initializer())\n\n        start_time = time.time()\n\n        # train\n        for step in range(tf.app.flags.FLAGS.training_steps):\n\n            train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch)\n\n            # every 100 steps, evaluate model metrics and write summaries to disk\n            if (step + 1) % 10 == 0 or step == tf.app.flags.FLAGS.training_steps - 1:\n                eval_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch, step, start_time)\n                start_time = time.time()\n\n        # when done, ask the threads to stop\n        coord.request_stop()\n\n        # wait for threads to finish\n        coord.join(threads)\n\nMy train_op is defined as follows:\ndef train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch):\n    x_values, y_values, y_lengths_values = sess.run([x_minibatch, y_minibatch, y_lengths_minibatch])\n\n    # minimize loss\n    sess.run([model.optimizer.minimize(model.loss), model.y_predicted_logits],\n                                     feed_dict={model.x: x_values,\n                                                model.y_actual: y_values,\n                                                model.y_actual_lengths: y_lengths_values})\n\nAs you can see, the model is constructed before I call sess.run(tf.global_variables_initializer()) and isn't touched afterwards.", "body": "I'm happy to post all my code, but tell me if this is sufficient. Here's my main function, where execution starts.\r\n\r\n```\r\ndef main(_):\r\n    x_minibatch, y_minibatch, y_lengths_minibatch = construct_data_pipeline()\r\n    model = import_model()\r\n    train(model=model, x_minibatch=x_minibatch, y_minibatch=y_minibatch, y_lengths_minibatch=y_lengths_minibatch)\r\n```\r\n\r\n`import_model()` imports my `Model` class, which has the following constructor:\r\n\r\n```\r\nclass Model(object):\r\n    def __init__(self, batch_size, initial_learning_rate):\r\n        self.x, self.y_actual, self.y_actual_lengths = self._define_inputs(batch_size)\r\n        self.encoder_outputs, self.encoder_final_states, self.decoder_outputs, self.y_predicted_logits = self._define_model()\r\n        self.loss = self._define_loss()\r\n        self.metrics = self._define_metrics()\r\n        self.optimizer = self._define_optimizer(initial_learning_rate)\r\n```\r\n\r\n`_define_optimizer(initial_learning_rate)` is defined as follows:\r\n\r\n```\r\n    @staticmethod\r\n    def _define_optimizer(initial_learning_rate):\r\n        with tf.name_scope('define_optimizer'):\r\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=initial_learning_rate)\r\n            # optimizer = tf.train.AdamOptimizer(learning_rate=initial_learning_rate)\r\n        return optimizer\r\n```\r\n\r\nThen, `main` calls `train()`, which is defined as follows:\r\n\r\n```\r\ndef train(model, x_minibatch, y_minibatch, y_lengths_minibatch):\r\n    with tf.Session() as sess:\r\n\r\n        # create coordinator to handle threading\r\n        coord = tf.train.Coordinator()\r\n\r\n        # start threads to enqueue input minibatches for training\r\n        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n        # initialize all variables and ops\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n        start_time = time.time()\r\n\r\n        # train\r\n        for step in range(tf.app.flags.FLAGS.training_steps):\r\n\r\n            train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch)\r\n\r\n            # every 100 steps, evaluate model metrics and write summaries to disk\r\n            if (step + 1) % 10 == 0 or step == tf.app.flags.FLAGS.training_steps - 1:\r\n                eval_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch, step, start_time)\r\n                start_time = time.time()\r\n\r\n        # when done, ask the threads to stop\r\n        coord.request_stop()\r\n\r\n        # wait for threads to finish\r\n        coord.join(threads)\r\n```\r\n\r\nMy `train_op` is defined as follows:\r\n\r\n```\r\ndef train_op(sess, model, x_minibatch, y_minibatch, y_lengths_minibatch):\r\n    x_values, y_values, y_lengths_values = sess.run([x_minibatch, y_minibatch, y_lengths_minibatch])\r\n\r\n    # minimize loss\r\n    sess.run([model.optimizer.minimize(model.loss), model.y_predicted_logits],\r\n                                     feed_dict={model.x: x_values,\r\n                                                model.y_actual: y_values,\r\n                                                model.y_actual_lengths: y_lengths_values})\r\n```\r\n\r\nAs you can see, the model is constructed before I call `sess.run(tf.global_variables_initializer())` and isn't touched afterwards."}