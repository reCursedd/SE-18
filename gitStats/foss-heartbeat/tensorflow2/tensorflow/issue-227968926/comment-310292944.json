{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310292944", "html_url": "https://github.com/tensorflow/tensorflow/issues/9837#issuecomment-310292944", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9837", "id": 310292944, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDI5Mjk0NA==", "user": {"login": "theflofly", "id": 3902382, "node_id": "MDQ6VXNlcjM5MDIzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3902382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theflofly", "html_url": "https://github.com/theflofly", "followers_url": "https://api.github.com/users/theflofly/followers", "following_url": "https://api.github.com/users/theflofly/following{/other_user}", "gists_url": "https://api.github.com/users/theflofly/gists{/gist_id}", "starred_url": "https://api.github.com/users/theflofly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theflofly/subscriptions", "organizations_url": "https://api.github.com/users/theflofly/orgs", "repos_url": "https://api.github.com/users/theflofly/repos", "events_url": "https://api.github.com/users/theflofly/events{/privacy}", "received_events_url": "https://api.github.com/users/theflofly/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-22T06:56:14Z", "updated_at": "2017-06-22T06:56:14Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15677695\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/AhmetCanSolak\">@AhmetCanSolak</a>, I am working on it and I made good progress even if it is not as easy as it sounds.</p>\n<p>I don't see a way to split the task unfortunately. Especially at this point. Once the optimizer part is added, there will be plenty of work. Namely adding all the Optimizers (adam, adagrad, momentum, etc.). If you want I can ping you at this time and we'll share the work.</p>\n<p>In the meantime, there is always the gradient operations that need to be ported.</p>", "body_text": "Hi @AhmetCanSolak, I am working on it and I made good progress even if it is not as easy as it sounds.\nI don't see a way to split the task unfortunately. Especially at this point. Once the optimizer part is added, there will be plenty of work. Namely adding all the Optimizers (adam, adagrad, momentum, etc.). If you want I can ping you at this time and we'll share the work.\nIn the meantime, there is always the gradient operations that need to be ported.", "body": "Hi @AhmetCanSolak, I am working on it and I made good progress even if it is not as easy as it sounds. \r\n\r\nI don't see a way to split the task unfortunately. Especially at this point. Once the optimizer part is added, there will be plenty of work. Namely adding all the Optimizers (adam, adagrad, momentum, etc.). If you want I can ping you at this time and we'll share the work.\r\n\r\nIn the meantime, there is always the gradient operations that need to be ported."}