{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317173911", "html_url": "https://github.com/tensorflow/tensorflow/issues/9837#issuecomment-317173911", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9837", "id": 317173911, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzE3MzkxMQ==", "user": {"login": "theflofly", "id": 3902382, "node_id": "MDQ6VXNlcjM5MDIzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3902382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theflofly", "html_url": "https://github.com/theflofly", "followers_url": "https://api.github.com/users/theflofly/followers", "following_url": "https://api.github.com/users/theflofly/following{/other_user}", "gists_url": "https://api.github.com/users/theflofly/gists{/gist_id}", "starred_url": "https://api.github.com/users/theflofly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theflofly/subscriptions", "organizations_url": "https://api.github.com/users/theflofly/orgs", "repos_url": "https://api.github.com/users/theflofly/repos", "events_url": "https://api.github.com/users/theflofly/events{/privacy}", "received_events_url": "https://api.github.com/users/theflofly/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-22T10:23:29Z", "updated_at": "2017-07-22T10:24:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16327442\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fferroni\">@fferroni</a> Indeed. The function is already existing and was created by someone at Google. In this issue I basically wrap existing stuffs into a cleaner API similar to the python.</p>\n<p>I already explained above but basically you can use:</p>\n<pre><code>auto x = Const(scope_, {{1.0f, 2.0f}, {3.0f, 4.0f}, {5.0f, 6.0f}});\nauto y = Const(scope_, {{1.0f}, {2.0f}, {3.0f}});\n\nauto w1 = Variable(scope_, {2, 1}, DT_FLOAT);\nauto assign_w1 = Assign(scope_, w1, Const(scope_, {{0.1f}, {0.2f}}));\n\nauto layer_1 = Tanh(scope_, MatMul(scope_, x, w1));\n\nstd::vector&lt;Output&gt; grad_outputs;\nTF_CHECK_OK(AddSymbolicGradients(scope_, {layer_1}, {assign_w1}, &amp;grad_outputs));\n\nauto apply = ApplyGradientDescent(\n      scope_, {w1},\n      Cast(scope_, 0,01,  static_cast&lt;DataType&gt;(w1.type() - 100)),\n      {grad_outputs[0]});\n\nClientSession session(scope_);\nTF_CHECK_OK(session.Run({assign_w1}, nullptr));\nTF_CHECK_OK(session.Run({apply}, nullptr));\n\n</code></pre>\n<p>The equivalent to gradients from python is AddSymbolicGradients. Not sure the above code is working as I can't test it, but you get the idea on how to put things together.</p>\n<p>Keep in mind that you first have to call session run on assign nodes, also the AddSymbolicGradients will not work if you pass the var w1 and not the assign node whereas to apply gradient descent you have to pass the var node and not the assign node. I edited AddSymbolicGradients on my PR to solve this issue.</p>\n<p>With more than one var the above code increases in complexity. That's why I replaced it by: <code>auto train = GradientDescentOptimizer(0.01).Minimize(scope, {loss});</code>.</p>", "body_text": "@fferroni Indeed. The function is already existing and was created by someone at Google. In this issue I basically wrap existing stuffs into a cleaner API similar to the python.\nI already explained above but basically you can use:\nauto x = Const(scope_, {{1.0f, 2.0f}, {3.0f, 4.0f}, {5.0f, 6.0f}});\nauto y = Const(scope_, {{1.0f}, {2.0f}, {3.0f}});\n\nauto w1 = Variable(scope_, {2, 1}, DT_FLOAT);\nauto assign_w1 = Assign(scope_, w1, Const(scope_, {{0.1f}, {0.2f}}));\n\nauto layer_1 = Tanh(scope_, MatMul(scope_, x, w1));\n\nstd::vector<Output> grad_outputs;\nTF_CHECK_OK(AddSymbolicGradients(scope_, {layer_1}, {assign_w1}, &grad_outputs));\n\nauto apply = ApplyGradientDescent(\n      scope_, {w1},\n      Cast(scope_, 0,01,  static_cast<DataType>(w1.type() - 100)),\n      {grad_outputs[0]});\n\nClientSession session(scope_);\nTF_CHECK_OK(session.Run({assign_w1}, nullptr));\nTF_CHECK_OK(session.Run({apply}, nullptr));\n\n\nThe equivalent to gradients from python is AddSymbolicGradients. Not sure the above code is working as I can't test it, but you get the idea on how to put things together.\nKeep in mind that you first have to call session run on assign nodes, also the AddSymbolicGradients will not work if you pass the var w1 and not the assign node whereas to apply gradient descent you have to pass the var node and not the assign node. I edited AddSymbolicGradients on my PR to solve this issue.\nWith more than one var the above code increases in complexity. That's why I replaced it by: auto train = GradientDescentOptimizer(0.01).Minimize(scope, {loss});.", "body": "@fferroni Indeed. The function is already existing and was created by someone at Google. In this issue I basically wrap existing stuffs into a cleaner API similar to the python.\r\n\r\nI already explained above but basically you can use:\r\n\r\n```\r\nauto x = Const(scope_, {{1.0f, 2.0f}, {3.0f, 4.0f}, {5.0f, 6.0f}});\r\nauto y = Const(scope_, {{1.0f}, {2.0f}, {3.0f}});\r\n\r\nauto w1 = Variable(scope_, {2, 1}, DT_FLOAT);\r\nauto assign_w1 = Assign(scope_, w1, Const(scope_, {{0.1f}, {0.2f}}));\r\n\r\nauto layer_1 = Tanh(scope_, MatMul(scope_, x, w1));\r\n\r\nstd::vector<Output> grad_outputs;\r\nTF_CHECK_OK(AddSymbolicGradients(scope_, {layer_1}, {assign_w1}, &grad_outputs));\r\n\r\nauto apply = ApplyGradientDescent(\r\n      scope_, {w1},\r\n      Cast(scope_, 0,01,  static_cast<DataType>(w1.type() - 100)),\r\n      {grad_outputs[0]});\r\n\r\nClientSession session(scope_);\r\nTF_CHECK_OK(session.Run({assign_w1}, nullptr));\r\nTF_CHECK_OK(session.Run({apply}, nullptr));\r\n\r\n```\r\nThe equivalent to gradients from python is AddSymbolicGradients. Not sure the above code is working as I can't test it, but you get the idea on how to put things together.\r\n\r\nKeep in mind that you first have to call session run on assign nodes, also the AddSymbolicGradients will not work if you pass the var w1 and not the assign node whereas to apply gradient descent you have to pass the var node and not the assign node. I edited AddSymbolicGradients on my PR to solve this issue.\r\n\r\nWith more than one var the above code increases in complexity. That's why I replaced it by: `auto train = GradientDescentOptimizer(0.01).Minimize(scope, {loss});`."}