{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2382", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2382/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2382/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2382/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2382", "id": 154947902, "node_id": "MDU6SXNzdWUxNTQ5NDc5MDI=", "number": 2382, "title": "ValueError: GraphDef cannot be larger than 2GB", "user": {"login": "wangbm", "id": 18027987, "node_id": "MDQ6VXNlcjE4MDI3OTg3", "avatar_url": "https://avatars3.githubusercontent.com/u/18027987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wangbm", "html_url": "https://github.com/wangbm", "followers_url": "https://api.github.com/users/wangbm/followers", "following_url": "https://api.github.com/users/wangbm/following{/other_user}", "gists_url": "https://api.github.com/users/wangbm/gists{/gist_id}", "starred_url": "https://api.github.com/users/wangbm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wangbm/subscriptions", "organizations_url": "https://api.github.com/users/wangbm/orgs", "repos_url": "https://api.github.com/users/wangbm/repos", "events_url": "https://api.github.com/users/wangbm/events{/privacy}", "received_events_url": "https://api.github.com/users/wangbm/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-05-16T02:03:52Z", "updated_at": "2018-10-23T02:59:48Z", "closed_at": "2016-06-07T16:30:30Z", "author_association": "NONE", "body_html": "<p>I'm trying to implement a deep autoencoder with tensorflow. The RBM pretraining works just fine, but when it comes to fine tuning, it raises the error: 'ValueError: GraphDef cannot be larger than 2GB'. My input is an array in the shape of [12396, 8192], and here is my layers: [8192 16384 8192 4096 2048 1024 512 256 512 1024 2048 4096 8192 16384 8192].<br>\nI know where the problem is, but I have no idea how to fix it. I have thought about using multiple graph, but what if my input is too big to even store one layer? Besides I don't know how many graph I should use. Set up a graph for every layer? That would be too slow and unnecessary.<br>\nThank you!<br>\n`  ...</p>\n<pre><code>def __init__(self, input_num, layers, rbm_learning_rate, deepnn_learning_rate, rbm_num_epoch, \n             deepnn_num_epoch, momentum=0, batch_size=128, data_type='float32'):\n    self.input_num = input_num\n    self.layers = layers\n    self.n_layers = len(self.layers)\n    self.rbm_learning_rate = rbm_learning_rate\n    self.deepnn_learning_rate = deepnn_learning_rate\n    if momentum == 0:\n        self.momentum = []            \n        for _ in range(self.n_layers):            \n            self.momentum.append(1)\n    self.rbm_num_epoch = rbm_num_epoch\n    self.deepnn_num_epoch = deepnn_num_epoch\n    self.batch_size = batch_size\n    self.data_type = data_type\n    self.rbm_list = []\n    self.rbm_list.append(RBM(self.input_num, self.layers[0], self.rbm_num_epoch, \n                            self.momentum[0], self.rbm_learning_rate[0], self.batch_size, self.data_type))            \n    for i in range(self.n_layers-1):\n        self.rbm_list.append(RBM(self.layers[i], self.layers[i+1], self.rbm_num_epoch,\n                            self.momentum[i], self.rbm_learning_rate[i], self.batch_size, self.data_type))\n\ndef pretrain(self, train_set):\n    self.W_list = []\n    self.b_list = []\n    self.a_list = []   \n    if not cmp(train_set.dtype, self.data_type):\n        train_set.dtype = self.data_type        \n    next_train = train_set        \n    for i, rboltz in enumerate(self.rbm_list):\n        next_train = self._pretrain_and_get_para(rboltz, next_train)\n\ndef _pretrain_and_get_para(self, rboltz, next_train):\n    output, W_out, a_out, b_out = rboltz.fit(next_train)\n    self.W_list.append(W_out)\n    self.a_list.append(a_out)\n    self.b_list.append(b_out)\n    return output\n\ndef fine_tune(self, train_set):\n    m, _ = train_set.shape\n    self.num_per_epoch = m / self.batch_size         \n    train_batch = tf.placeholder(self.data_type, [None, self.input_num])        \n    logits = self._build_model(train_batch)\n    loss = self._loss(logits, train_batch)\n    train_op = self._training(loss)\n    init = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init)\n        for _ in range(self.deepnn_num_epoch):\n            for i in range(self.num_per_epoch):\n                _, cost = sess.run([train_op, loss], feed_dict = self._feed_build(train_batch, train_set, i))\n            print cost    \n\ndef _feed_build(self, train_batch, train_set, i):\n    batch = prepare_data.next_batch(train_set, i, self.batch_size)\n    feed_dict = {train_batch: batch}\n    return feed_dict    \n\ndef _build_model(self, train_batch):      \n    middle_layer = self._make_encoder(train_batch)\n    last_layer = self._make_decoder(middle_layer)\n    return last_layer\n\ndef _make_encoder(self, train_batch):\n    encoder = []\n    encoder.append(train_batch)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('encoder'+str(i)):\n            W = tf.Variable(self.W_list[i], name = 'weights')\n            b = tf.Variable(self.b_list[i], name = 'biases')\n            encoder.append(tf.sigmoid(b + tf.matmul(encoder[i], W)))\n    return encoder[self.n_layers]\n\n\ndef _make_decoder(self, middle_layer):\n    decoder = []\n    decoder.append(middle_layer)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('decoder'+str(i)):\n            W = tf.Variable(self.W_list[self.n_layers-i-1], name = 'weights')\n            a = tf.Variable(self.a_list[self.n_layers-i-1], name = 'biases')\n            decoder.append(tf.sigmoid(a + tf.matmul(decoder[i], W, transpose_b = True)))\n    return decoder[self.n_layers]        \n\ndef _loss(self, logits, labels):\n    loss = tf.nn.l2_loss(logits-labels)\n    return loss\n\ndef _training(self, loss):\n    optimizer = tf.train.GradientDescentOptimizer(self.deepnn_learning_rate)\n    train_op = optimizer.minimize(loss)\n    return train_op `\n</code></pre>", "body_text": "I'm trying to implement a deep autoencoder with tensorflow. The RBM pretraining works just fine, but when it comes to fine tuning, it raises the error: 'ValueError: GraphDef cannot be larger than 2GB'. My input is an array in the shape of [12396, 8192], and here is my layers: [8192 16384 8192 4096 2048 1024 512 256 512 1024 2048 4096 8192 16384 8192].\nI know where the problem is, but I have no idea how to fix it. I have thought about using multiple graph, but what if my input is too big to even store one layer? Besides I don't know how many graph I should use. Set up a graph for every layer? That would be too slow and unnecessary.\nThank you!\n`  ...\ndef __init__(self, input_num, layers, rbm_learning_rate, deepnn_learning_rate, rbm_num_epoch, \n             deepnn_num_epoch, momentum=0, batch_size=128, data_type='float32'):\n    self.input_num = input_num\n    self.layers = layers\n    self.n_layers = len(self.layers)\n    self.rbm_learning_rate = rbm_learning_rate\n    self.deepnn_learning_rate = deepnn_learning_rate\n    if momentum == 0:\n        self.momentum = []            \n        for _ in range(self.n_layers):            \n            self.momentum.append(1)\n    self.rbm_num_epoch = rbm_num_epoch\n    self.deepnn_num_epoch = deepnn_num_epoch\n    self.batch_size = batch_size\n    self.data_type = data_type\n    self.rbm_list = []\n    self.rbm_list.append(RBM(self.input_num, self.layers[0], self.rbm_num_epoch, \n                            self.momentum[0], self.rbm_learning_rate[0], self.batch_size, self.data_type))            \n    for i in range(self.n_layers-1):\n        self.rbm_list.append(RBM(self.layers[i], self.layers[i+1], self.rbm_num_epoch,\n                            self.momentum[i], self.rbm_learning_rate[i], self.batch_size, self.data_type))\n\ndef pretrain(self, train_set):\n    self.W_list = []\n    self.b_list = []\n    self.a_list = []   \n    if not cmp(train_set.dtype, self.data_type):\n        train_set.dtype = self.data_type        \n    next_train = train_set        \n    for i, rboltz in enumerate(self.rbm_list):\n        next_train = self._pretrain_and_get_para(rboltz, next_train)\n\ndef _pretrain_and_get_para(self, rboltz, next_train):\n    output, W_out, a_out, b_out = rboltz.fit(next_train)\n    self.W_list.append(W_out)\n    self.a_list.append(a_out)\n    self.b_list.append(b_out)\n    return output\n\ndef fine_tune(self, train_set):\n    m, _ = train_set.shape\n    self.num_per_epoch = m / self.batch_size         \n    train_batch = tf.placeholder(self.data_type, [None, self.input_num])        \n    logits = self._build_model(train_batch)\n    loss = self._loss(logits, train_batch)\n    train_op = self._training(loss)\n    init = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init)\n        for _ in range(self.deepnn_num_epoch):\n            for i in range(self.num_per_epoch):\n                _, cost = sess.run([train_op, loss], feed_dict = self._feed_build(train_batch, train_set, i))\n            print cost    \n\ndef _feed_build(self, train_batch, train_set, i):\n    batch = prepare_data.next_batch(train_set, i, self.batch_size)\n    feed_dict = {train_batch: batch}\n    return feed_dict    \n\ndef _build_model(self, train_batch):      \n    middle_layer = self._make_encoder(train_batch)\n    last_layer = self._make_decoder(middle_layer)\n    return last_layer\n\ndef _make_encoder(self, train_batch):\n    encoder = []\n    encoder.append(train_batch)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('encoder'+str(i)):\n            W = tf.Variable(self.W_list[i], name = 'weights')\n            b = tf.Variable(self.b_list[i], name = 'biases')\n            encoder.append(tf.sigmoid(b + tf.matmul(encoder[i], W)))\n    return encoder[self.n_layers]\n\n\ndef _make_decoder(self, middle_layer):\n    decoder = []\n    decoder.append(middle_layer)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('decoder'+str(i)):\n            W = tf.Variable(self.W_list[self.n_layers-i-1], name = 'weights')\n            a = tf.Variable(self.a_list[self.n_layers-i-1], name = 'biases')\n            decoder.append(tf.sigmoid(a + tf.matmul(decoder[i], W, transpose_b = True)))\n    return decoder[self.n_layers]        \n\ndef _loss(self, logits, labels):\n    loss = tf.nn.l2_loss(logits-labels)\n    return loss\n\ndef _training(self, loss):\n    optimizer = tf.train.GradientDescentOptimizer(self.deepnn_learning_rate)\n    train_op = optimizer.minimize(loss)\n    return train_op `", "body": "I'm trying to implement a deep autoencoder with tensorflow. The RBM pretraining works just fine, but when it comes to fine tuning, it raises the error: 'ValueError: GraphDef cannot be larger than 2GB'. My input is an array in the shape of [12396, 8192], and here is my layers: [8192 16384 8192 4096 2048 1024 512 256 512 1024 2048 4096 8192 16384 8192]. \nI know where the problem is, but I have no idea how to fix it. I have thought about using multiple graph, but what if my input is too big to even store one layer? Besides I don't know how many graph I should use. Set up a graph for every layer? That would be too slow and unnecessary.\nThank you!\n`  ...\n\n```\ndef __init__(self, input_num, layers, rbm_learning_rate, deepnn_learning_rate, rbm_num_epoch, \n             deepnn_num_epoch, momentum=0, batch_size=128, data_type='float32'):\n    self.input_num = input_num\n    self.layers = layers\n    self.n_layers = len(self.layers)\n    self.rbm_learning_rate = rbm_learning_rate\n    self.deepnn_learning_rate = deepnn_learning_rate\n    if momentum == 0:\n        self.momentum = []            \n        for _ in range(self.n_layers):            \n            self.momentum.append(1)\n    self.rbm_num_epoch = rbm_num_epoch\n    self.deepnn_num_epoch = deepnn_num_epoch\n    self.batch_size = batch_size\n    self.data_type = data_type\n    self.rbm_list = []\n    self.rbm_list.append(RBM(self.input_num, self.layers[0], self.rbm_num_epoch, \n                            self.momentum[0], self.rbm_learning_rate[0], self.batch_size, self.data_type))            \n    for i in range(self.n_layers-1):\n        self.rbm_list.append(RBM(self.layers[i], self.layers[i+1], self.rbm_num_epoch,\n                            self.momentum[i], self.rbm_learning_rate[i], self.batch_size, self.data_type))\n\ndef pretrain(self, train_set):\n    self.W_list = []\n    self.b_list = []\n    self.a_list = []   \n    if not cmp(train_set.dtype, self.data_type):\n        train_set.dtype = self.data_type        \n    next_train = train_set        \n    for i, rboltz in enumerate(self.rbm_list):\n        next_train = self._pretrain_and_get_para(rboltz, next_train)\n\ndef _pretrain_and_get_para(self, rboltz, next_train):\n    output, W_out, a_out, b_out = rboltz.fit(next_train)\n    self.W_list.append(W_out)\n    self.a_list.append(a_out)\n    self.b_list.append(b_out)\n    return output\n\ndef fine_tune(self, train_set):\n    m, _ = train_set.shape\n    self.num_per_epoch = m / self.batch_size         \n    train_batch = tf.placeholder(self.data_type, [None, self.input_num])        \n    logits = self._build_model(train_batch)\n    loss = self._loss(logits, train_batch)\n    train_op = self._training(loss)\n    init = tf.initialize_all_variables()\n    with tf.Session() as sess:\n        sess.run(init)\n        for _ in range(self.deepnn_num_epoch):\n            for i in range(self.num_per_epoch):\n                _, cost = sess.run([train_op, loss], feed_dict = self._feed_build(train_batch, train_set, i))\n            print cost    \n\ndef _feed_build(self, train_batch, train_set, i):\n    batch = prepare_data.next_batch(train_set, i, self.batch_size)\n    feed_dict = {train_batch: batch}\n    return feed_dict    \n\ndef _build_model(self, train_batch):      \n    middle_layer = self._make_encoder(train_batch)\n    last_layer = self._make_decoder(middle_layer)\n    return last_layer\n\ndef _make_encoder(self, train_batch):\n    encoder = []\n    encoder.append(train_batch)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('encoder'+str(i)):\n            W = tf.Variable(self.W_list[i], name = 'weights')\n            b = tf.Variable(self.b_list[i], name = 'biases')\n            encoder.append(tf.sigmoid(b + tf.matmul(encoder[i], W)))\n    return encoder[self.n_layers]\n\n\ndef _make_decoder(self, middle_layer):\n    decoder = []\n    decoder.append(middle_layer)        \n    for i, layer in enumerate(self.layers):\n        with tf.name_scope('decoder'+str(i)):\n            W = tf.Variable(self.W_list[self.n_layers-i-1], name = 'weights')\n            a = tf.Variable(self.a_list[self.n_layers-i-1], name = 'biases')\n            decoder.append(tf.sigmoid(a + tf.matmul(decoder[i], W, transpose_b = True)))\n    return decoder[self.n_layers]        \n\ndef _loss(self, logits, labels):\n    loss = tf.nn.l2_loss(logits-labels)\n    return loss\n\ndef _training(self, loss):\n    optimizer = tf.train.GradientDescentOptimizer(self.deepnn_learning_rate)\n    train_op = optimizer.minimize(loss)\n    return train_op `\n```\n"}