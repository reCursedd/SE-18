{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19503", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19503/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19503/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19503/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19503", "id": 325780219, "node_id": "MDU6SXNzdWUzMjU3ODAyMTk=", "number": 19503, "title": "TOCO Quantized InceptionV3 Error: \"tensorflow/contrib/lite/kernels/pooling.cc:116 input->params.scale != output->params.scale\"", "user": {"login": "parvizp", "id": 926261, "node_id": "MDQ6VXNlcjkyNjI2MQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/926261?v=4", "gravatar_id": "", "url": "https://api.github.com/users/parvizp", "html_url": "https://github.com/parvizp", "followers_url": "https://api.github.com/users/parvizp/followers", "following_url": "https://api.github.com/users/parvizp/following{/other_user}", "gists_url": "https://api.github.com/users/parvizp/gists{/gist_id}", "starred_url": "https://api.github.com/users/parvizp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/parvizp/subscriptions", "organizations_url": "https://api.github.com/users/parvizp/orgs", "repos_url": "https://api.github.com/users/parvizp/repos", "events_url": "https://api.github.com/users/parvizp/events{/privacy}", "received_events_url": "https://api.github.com/users/parvizp/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-05-23T16:20:56Z", "updated_at": "2018-05-30T17:56:22Z", "closed_at": "2018-05-30T17:56:22Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Minor custom code for quantization but using provided InceptionV3 checkpoints and models/research/slim scripts.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 17.10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.8.0-2169-gb84878e63e', '1.8.0')</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.12.0</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: gcc version 7.2.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.1/7.1.3</li>\n<li><strong>GPU model and memory</strong>: NVIDIA P40 (24 GB)</li>\n<li><strong>Exact command to reproduce</strong>: I link to my script on GitHub below.</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I've been trying to produce a quantized InceptionV3 model since one is not provided (<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md</a>) but it currently fails in the <code>label_image</code> demo application due to a MaxPool that has different input/output quantization ranges.</p>\n<p>In this TensorFlow Lite announcement video, quantized InceptionV3 is shown to be ~3x faster than floating point (<a href=\"https://youtu.be/FAMfy7izB6A?t=8m40s\" rel=\"nofollow\">https://youtu.be/FAMfy7izB6A?t=8m40s</a>), so I thought it might work \"out of the box\".<br>\nAdditionally, in this Google quantization paper the accuracy is shown to be fairly high (<a href=\"https://arxiv.org/abs/1712.05877\" rel=\"nofollow\">https://arxiv.org/abs/1712.05877</a>), which is pretty motivating.</p>\n<h3>Source code / logs</h3>\n<p>I have a branch of <code>tensorflow/models</code> with very minor changes to support quantization: <a href=\"https://github.com/parvizp/models/tree/quantize\">https://github.com/parvizp/models/tree/quantize</a><br>\nYou can run the whole script to see the training, TOCO call and call to <code>label_image</code>: <a href=\"https://github.com/parvizp/models/blob/279e458ac99da67e405ac74bc5e4583d5111c1bb/research/slim/scripts/quantize_inception_v3_on_imagenet.sh\">https://github.com/parvizp/models/blob/279e458ac99da67e405ac74bc5e4583d5111c1bb/research/slim/scripts/quantize_inception_v3_on_imagenet.sh</a></p>\n<pre><code>INFO: Running command line: bazel-bin/tensorflow/contrib/lite/examples/label_image/label_image '--tflite_model=/tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite' '--image=/tmp/tensorflow/tensorflow/contrib/lite/examples/label_image/testdata/grace_hopper.bmp' '--labels=/tmp/imagenet/labels.txt'\nnnapi error: unable to open library libneuralnetworks.so\nLoaded model /tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite\nresolved reporter\ntensorflow/contrib/lite/kernels/pooling.cc:116 input-&gt;params.scale != output-&gt;params.scale (-1454358704 != 734155808)\nFailed to allocate tensors!\n</code></pre>\n<p>The error seems to stem from this MaxPool in the center of:<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/926261/40436998-d7f6ff38-5e79-11e8-9b1e-6e65f2f20ec9.png\"><img width=\"850\" alt=\"inception_v3 quantized maxpool-issue\" src=\"https://user-images.githubusercontent.com/926261/40436998-d7f6ff38-5e79-11e8-9b1e-6e65f2f20ec9.png\" style=\"max-width:100%;\"></a><br>\nTo reach the accuracy reported in (<a href=\"https://arxiv.org/abs/1712.05877\" rel=\"nofollow\">https://arxiv.org/abs/1712.05877</a>) should we:</p>\n<ul>\n<li>Add support for TF-Lite MaxPool kernel so it can perform requantization as needed (i.e. for this case)?</li>\n<li>Make TOCO nudge the ranges so all things forking from the cancat and joining have the same range? Could also make the graph re-writer emulate this with shared ranges?</li>\n</ul>\n<p>Seems to preserve more precision and easier to do the first option. I'm happy to provide a patch if you can provide some insight into which direction to pursue.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Minor custom code for quantization but using provided InceptionV3 checkpoints and models/research/slim scripts.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 17.10\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): ('v1.8.0-2169-gb84878e63e', '1.8.0')\nPython version: 2.7\nBazel version (if compiling from source): 0.12.0\nGCC/Compiler version (if compiling from source): gcc version 7.2.0\nCUDA/cuDNN version: 9.1/7.1.3\nGPU model and memory: NVIDIA P40 (24 GB)\nExact command to reproduce: I link to my script on GitHub below.\n\nDescribe the problem\nI've been trying to produce a quantized InceptionV3 model since one is not provided (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md) but it currently fails in the label_image demo application due to a MaxPool that has different input/output quantization ranges.\nIn this TensorFlow Lite announcement video, quantized InceptionV3 is shown to be ~3x faster than floating point (https://youtu.be/FAMfy7izB6A?t=8m40s), so I thought it might work \"out of the box\".\nAdditionally, in this Google quantization paper the accuracy is shown to be fairly high (https://arxiv.org/abs/1712.05877), which is pretty motivating.\nSource code / logs\nI have a branch of tensorflow/models with very minor changes to support quantization: https://github.com/parvizp/models/tree/quantize\nYou can run the whole script to see the training, TOCO call and call to label_image: https://github.com/parvizp/models/blob/279e458ac99da67e405ac74bc5e4583d5111c1bb/research/slim/scripts/quantize_inception_v3_on_imagenet.sh\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/examples/label_image/label_image '--tflite_model=/tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite' '--image=/tmp/tensorflow/tensorflow/contrib/lite/examples/label_image/testdata/grace_hopper.bmp' '--labels=/tmp/imagenet/labels.txt'\nnnapi error: unable to open library libneuralnetworks.so\nLoaded model /tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite\nresolved reporter\ntensorflow/contrib/lite/kernels/pooling.cc:116 input->params.scale != output->params.scale (-1454358704 != 734155808)\nFailed to allocate tensors!\n\nThe error seems to stem from this MaxPool in the center of:\n\nTo reach the accuracy reported in (https://arxiv.org/abs/1712.05877) should we:\n\nAdd support for TF-Lite MaxPool kernel so it can perform requantization as needed (i.e. for this case)?\nMake TOCO nudge the ranges so all things forking from the cancat and joining have the same range? Could also make the graph re-writer emulate this with shared ranges?\n\nSeems to preserve more precision and easier to do the first option. I'm happy to provide a patch if you can provide some insight into which direction to pursue.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Minor custom code for quantization but using provided InceptionV3 checkpoints and models/research/slim scripts.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 17.10\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: ('v1.8.0-2169-gb84878e63e', '1.8.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.12.0\r\n- **GCC/Compiler version (if compiling from source)**: gcc version 7.2.0\r\n- **CUDA/cuDNN version**: 9.1/7.1.3\r\n- **GPU model and memory**: NVIDIA P40 (24 GB)\r\n- **Exact command to reproduce**: I link to my script on GitHub below.\r\n\r\n### Describe the problem\r\nI've been trying to produce a quantized InceptionV3 model since one is not provided (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/lite/g3doc/models.md) but it currently fails in the `label_image` demo application due to a MaxPool that has different input/output quantization ranges.\r\n\r\nIn this TensorFlow Lite announcement video, quantized InceptionV3 is shown to be ~3x faster than floating point (https://youtu.be/FAMfy7izB6A?t=8m40s), so I thought it might work \"out of the box\".\r\nAdditionally, in this Google quantization paper the accuracy is shown to be fairly high (https://arxiv.org/abs/1712.05877), which is pretty motivating. \r\n\r\n### Source code / logs\r\nI have a branch of `tensorflow/models` with very minor changes to support quantization: https://github.com/parvizp/models/tree/quantize\r\nYou can run the whole script to see the training, TOCO call and call to `label_image`: https://github.com/parvizp/models/blob/279e458ac99da67e405ac74bc5e4583d5111c1bb/research/slim/scripts/quantize_inception_v3_on_imagenet.sh\r\n```\r\nINFO: Running command line: bazel-bin/tensorflow/contrib/lite/examples/label_image/label_image '--tflite_model=/tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite' '--image=/tmp/tensorflow/tensorflow/contrib/lite/examples/label_image/testdata/grace_hopper.bmp' '--labels=/tmp/imagenet/labels.txt'\r\nnnapi error: unable to open library libneuralnetworks.so\r\nLoaded model /tmp/imagenet-models/inception_v3/inception_v3.quantized.tflite\r\nresolved reporter\r\ntensorflow/contrib/lite/kernels/pooling.cc:116 input->params.scale != output->params.scale (-1454358704 != 734155808)\r\nFailed to allocate tensors!\r\n```\r\nThe error seems to stem from this MaxPool in the center of:\r\n<img width=\"850\" alt=\"inception_v3 quantized maxpool-issue\" src=\"https://user-images.githubusercontent.com/926261/40436998-d7f6ff38-5e79-11e8-9b1e-6e65f2f20ec9.png\">\r\nTo reach the accuracy reported in (https://arxiv.org/abs/1712.05877) should we:\r\n- Add support for TF-Lite MaxPool kernel so it can perform requantization as needed (i.e. for this case)?\r\n- Make TOCO nudge the ranges so all things forking from the cancat and joining have the same range? Could also make the graph re-writer emulate this with shared ranges? \r\n\r\nSeems to preserve more precision and easier to do the first option. I'm happy to provide a patch if you can provide some insight into which direction to pursue."}