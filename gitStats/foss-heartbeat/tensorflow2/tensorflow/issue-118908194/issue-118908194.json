{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/352", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/352/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/352/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/352/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/352", "id": 118908194, "node_id": "MDU6SXNzdWUxMTg5MDgxOTQ=", "number": 352, "title": "Out of GPU Memory -- Only 1 LSTM layer on 980 TI", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2015-11-25T19:36:26Z", "updated_at": "2016-07-16T20:04:44Z", "closed_at": "2015-12-03T14:12:17Z", "author_association": "NONE", "body_html": "<p>Hey TF,</p>\n<p>I have been using your translate model from your seq2seq tutorial and everything seems to work great.</p>\n<p>However, I have encountered a substantial problem. On my 980 TI with 6gb of memory:</p>\n<ul>\n<li>5 GRU layers, 512 cells, batch size 32 -- <em>works</em></li>\n<li>1 GRU layer, 1024 cells, batch size 2 -- <em>barely works</em> (tried batch size of 2, 4, 8, 16)</li>\n</ul>\n<p>In Keras, I could run 2 GRU layers each with 2048 cells on my the 6gb memory. So the question I have is: how is this possible? What is taking up so much memory when you increase the cell size?</p>\n<p>I have a second 980 TI as well. I was really hoping I could put one layer 2048 cells on each card. Thanks again!</p>", "body_text": "Hey TF,\nI have been using your translate model from your seq2seq tutorial and everything seems to work great.\nHowever, I have encountered a substantial problem. On my 980 TI with 6gb of memory:\n\n5 GRU layers, 512 cells, batch size 32 -- works\n1 GRU layer, 1024 cells, batch size 2 -- barely works (tried batch size of 2, 4, 8, 16)\n\nIn Keras, I could run 2 GRU layers each with 2048 cells on my the 6gb memory. So the question I have is: how is this possible? What is taking up so much memory when you increase the cell size?\nI have a second 980 TI as well. I was really hoping I could put one layer 2048 cells on each card. Thanks again!", "body": "Hey TF,\n\nI have been using your translate model from your seq2seq tutorial and everything seems to work great. \n\nHowever, I have encountered a substantial problem. On my 980 TI with 6gb of memory:\n- 5 GRU layers, 512 cells, batch size 32 -- _works_\n- 1 GRU layer, 1024 cells, batch size 2 -- _barely works_ (tried batch size of 2, 4, 8, 16)\n\nIn Keras, I could run 2 GRU layers each with 2048 cells on my the 6gb memory. So the question I have is: how is this possible? What is taking up so much memory when you increase the cell size? \n\nI have a second 980 TI as well. I was really hoping I could put one layer 2048 cells on each card. Thanks again! \n"}