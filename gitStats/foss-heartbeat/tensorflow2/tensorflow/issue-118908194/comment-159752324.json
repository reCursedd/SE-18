{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159752324", "html_url": "https://github.com/tensorflow/tensorflow/issues/352#issuecomment-159752324", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/352", "id": 159752324, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTc1MjMyNA==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-25T23:11:02Z", "updated_at": "2015-11-25T23:11:02Z", "author_association": "NONE", "body_html": "<p>Hey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a> thanks for your help. I just built it from source and it did help some. Now, I can run a 1536 GRU layer at a batch size of 8 -- which is definitely an improvement! However, I can't run the desired 2048 unless I decrease the batch size to 4.</p>\n<p>I'll close this issue as things are certainly better. Hopefully, the gpu allocator can be further improved upon? It seems that multiple layers is great (I can use 6 GRU layers with 512 cells/layer with a batch size of 16). It just seems that these large layer sizes really sucks up memory compared to Theano.</p>", "body_text": "Hey @vrv thanks for your help. I just built it from source and it did help some. Now, I can run a 1536 GRU layer at a batch size of 8 -- which is definitely an improvement! However, I can't run the desired 2048 unless I decrease the batch size to 4.\nI'll close this issue as things are certainly better. Hopefully, the gpu allocator can be further improved upon? It seems that multiple layers is great (I can use 6 GRU layers with 512 cells/layer with a batch size of 16). It just seems that these large layer sizes really sucks up memory compared to Theano.", "body": "Hey @vrv thanks for your help. I just built it from source and it did help some. Now, I can run a 1536 GRU layer at a batch size of 8 -- which is definitely an improvement! However, I can't run the desired 2048 unless I decrease the batch size to 4. \n\nI'll close this issue as things are certainly better. Hopefully, the gpu allocator can be further improved upon? It seems that multiple layers is great (I can use 6 GRU layers with 512 cells/layer with a batch size of 16). It just seems that these large layer sizes really sucks up memory compared to Theano. \n"}