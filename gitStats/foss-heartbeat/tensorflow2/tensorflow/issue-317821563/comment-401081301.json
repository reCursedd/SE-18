{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/401081301", "html_url": "https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-401081301", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18880", "id": 401081301, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMTA4MTMwMQ==", "user": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-28T15:47:15Z", "updated_at": "2018-06-28T15:47:15Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7325\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ajbouh\">@ajbouh</a> You're asking some excellent questions!</p>\n<p>Unfortunately, I'm not sure that detecting worker failure can be done as an op. Ops (by design) are not allowed to know of each other, and only communicate through input and output tensors. So, if we're trying to detect failures at the (arbitrary) op level, we can't necessarily do it as a custom op. But, that's probably too hard a problem; let's scope down to just detecting worker failures, which we could detect by having an op right after the recv op (which handles the RPC communication between workers). If we wanted to go down this route, we could augment the graph re-write pass that adds in the send's and recv's to also add in our custom loaded failure detecting op. But, then two issues arise:</p>\n<p>(1) how does the custom failure detector op know when a failure has happened? A standard timeout? If so, what's too long? It's not clear to me, given the huge variety of workloads running in TF. Even within a workload, some recv's should complete within seconds, whereas others are expected to take many minutes or even longer.</p>\n<p>(2) Assuming we've solved (1), we now have an op (which runs on a worker) that knows a failure has occurred. The next question becomes what can be done about the failure. We'd then need to define an RPC protocol for the worker to either (a) communicate with the master to request a new graph to be executed on the failed worker to re-initialize it, or (b) have the (potentially many) workers directly re-initialize it after it comes back up, and the associated coordination required. Because RPC protocols can't be added to a gRPC server after it's been started, this would require some non-trivial re-architecting of the RPC stack. (And while this is doable, the question becomes what's the value? How many workloads can we unlock with this capability? Note that the capability itself does not solve the problem; it only allows others to write subtle and complicated C++ ops that could attempt to perform failure handling. Would any of these be sharable between TF users? Between different workloads?)</p>\n<p>As an alternative, we could augment the TF Master to allow some custom failure handling instead of as an op. But unfortunately, it doesn't necessarily solve all the problems either (what's the expected time a worker should wait for an recv?), and introduces new challenges (e.g. we currently have no path for dynamically loading plugins into the master currently). Again, all of these are solveable with sufficient engineering resources. But there's an opportunity cost (in addition to the maintenance cost), so we have to understand the value. Given that the MTBF of machines (especially Google Compute Engine CPU VMs) is so high (a year or more), even a large cluster training for a couple days is not likely to experience a failure in the normal case. The question thus becomes, what workloads can we not serve today?</p>\n<p>Does that make sense?</p>", "body_text": "@ajbouh You're asking some excellent questions!\nUnfortunately, I'm not sure that detecting worker failure can be done as an op. Ops (by design) are not allowed to know of each other, and only communicate through input and output tensors. So, if we're trying to detect failures at the (arbitrary) op level, we can't necessarily do it as a custom op. But, that's probably too hard a problem; let's scope down to just detecting worker failures, which we could detect by having an op right after the recv op (which handles the RPC communication between workers). If we wanted to go down this route, we could augment the graph re-write pass that adds in the send's and recv's to also add in our custom loaded failure detecting op. But, then two issues arise:\n(1) how does the custom failure detector op know when a failure has happened? A standard timeout? If so, what's too long? It's not clear to me, given the huge variety of workloads running in TF. Even within a workload, some recv's should complete within seconds, whereas others are expected to take many minutes or even longer.\n(2) Assuming we've solved (1), we now have an op (which runs on a worker) that knows a failure has occurred. The next question becomes what can be done about the failure. We'd then need to define an RPC protocol for the worker to either (a) communicate with the master to request a new graph to be executed on the failed worker to re-initialize it, or (b) have the (potentially many) workers directly re-initialize it after it comes back up, and the associated coordination required. Because RPC protocols can't be added to a gRPC server after it's been started, this would require some non-trivial re-architecting of the RPC stack. (And while this is doable, the question becomes what's the value? How many workloads can we unlock with this capability? Note that the capability itself does not solve the problem; it only allows others to write subtle and complicated C++ ops that could attempt to perform failure handling. Would any of these be sharable between TF users? Between different workloads?)\nAs an alternative, we could augment the TF Master to allow some custom failure handling instead of as an op. But unfortunately, it doesn't necessarily solve all the problems either (what's the expected time a worker should wait for an recv?), and introduces new challenges (e.g. we currently have no path for dynamically loading plugins into the master currently). Again, all of these are solveable with sufficient engineering resources. But there's an opportunity cost (in addition to the maintenance cost), so we have to understand the value. Given that the MTBF of machines (especially Google Compute Engine CPU VMs) is so high (a year or more), even a large cluster training for a couple days is not likely to experience a failure in the normal case. The question thus becomes, what workloads can we not serve today?\nDoes that make sense?", "body": "@ajbouh You're asking some excellent questions!\r\n\r\nUnfortunately, I'm not sure that detecting worker failure can be done as an op. Ops (by design) are not allowed to know of each other, and only communicate through input and output tensors. So, if we're trying to detect failures at the (arbitrary) op level, we can't necessarily do it as a custom op. But, that's probably too hard a problem; let's scope down to just detecting worker failures, which we could detect by having an op right after the recv op (which handles the RPC communication between workers). If we wanted to go down this route, we could augment the graph re-write pass that adds in the send's and recv's to also add in our custom loaded failure detecting op. But, then two issues arise:\r\n\r\n(1) how does the custom failure detector op know when a failure has happened? A standard timeout? If so, what's too long? It's not clear to me, given the huge variety of workloads running in TF. Even within a workload, some recv's should complete within seconds, whereas others are expected to take many minutes or even longer.\r\n\r\n(2) Assuming we've solved (1), we now have an op (which runs on a worker) that knows a failure has occurred. The next question becomes what can be done about the failure. We'd then need to define an RPC protocol for the worker to either (a) communicate with the master to request a new graph to be executed on the failed worker to re-initialize it, or (b) have the (potentially many) workers directly re-initialize it after it comes back up, and the associated coordination required. Because RPC protocols can't be added to a gRPC server after it's been started, this would require some non-trivial re-architecting of the RPC stack. (And while this is doable, the question becomes what's the value? How many workloads can we unlock with this capability? Note that the capability itself does not solve the problem; it only allows others to write subtle and complicated C++ ops that could attempt to perform failure handling. Would any of these be sharable between TF users? Between different workloads?)\r\n\r\nAs an alternative, we could augment the TF Master to allow some custom failure handling instead of as an op. But unfortunately, it doesn't necessarily solve all the problems either (what's the expected time a worker should wait for an recv?), and introduces new challenges (e.g. we currently have no path for dynamically loading plugins into the master currently). Again, all of these are solveable with sufficient engineering resources. But there's an opportunity cost (in addition to the maintenance cost), so we have to understand the value. Given that the MTBF of machines (especially Google Compute Engine CPU VMs) is so high (a year or more), even a large cluster training for a couple days is not likely to experience a failure in the normal case. The question thus becomes, what workloads can we not serve today?\r\n\r\nDoes that make sense?"}