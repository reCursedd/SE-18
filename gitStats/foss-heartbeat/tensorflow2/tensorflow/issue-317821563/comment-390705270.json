{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/390705270", "html_url": "https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-390705270", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18880", "id": 390705270, "node_id": "MDEyOklzc3VlQ29tbWVudDM5MDcwNTI3MA==", "user": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-21T16:22:04Z", "updated_at": "2018-05-21T16:22:04Z", "author_association": "MEMBER", "body_html": "<p>Sorry <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=19718982\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/make1980\">@make1980</a> for the delayed reply. This thread got lost in my inbox. I apologize!</p>\n<p>In general whenever there is a machine failure within a TensorFlow cluster, we have to be extra careful. This is because ops can be stateful. In short given a node failure at an arbitrary point after session creation, it's not correct to just re-try running those ops. Concretely in your scenario about 1 worker and 1 ps, if the parameter server fails after the variables have been initialized, it is not correct to begin executing the python training loop once the ps server comes back online. (Because the variables that are placed on it have not been initialized.) Even if we special-cased variables, there are many other stateful ops that also would need explicit handling. Because TensorFlow users can define custom operations, it's not possible to teach the C++-runtime how to automatically handle the failures within a session.</p>\n<p>As a result, the failure handling strategy we adopt within TensorFlow is to close the session and create a new session (and optionally restoring from a checkpoint). When starting the new session, client code (e.g. python) can construct a new ClusterSpec and propagate it to all the nodes within the cluster. This also has the advantage of cleanly separating the core TensorFlow runtime from having to integrate with custom cluster-coordination systems (e.g. etcd, zookeeper, internal Google systems, Netflix's Eureka, k8s, etc.).</p>\n<p>Hope this helps!</p>", "body_text": "Sorry @make1980 for the delayed reply. This thread got lost in my inbox. I apologize!\nIn general whenever there is a machine failure within a TensorFlow cluster, we have to be extra careful. This is because ops can be stateful. In short given a node failure at an arbitrary point after session creation, it's not correct to just re-try running those ops. Concretely in your scenario about 1 worker and 1 ps, if the parameter server fails after the variables have been initialized, it is not correct to begin executing the python training loop once the ps server comes back online. (Because the variables that are placed on it have not been initialized.) Even if we special-cased variables, there are many other stateful ops that also would need explicit handling. Because TensorFlow users can define custom operations, it's not possible to teach the C++-runtime how to automatically handle the failures within a session.\nAs a result, the failure handling strategy we adopt within TensorFlow is to close the session and create a new session (and optionally restoring from a checkpoint). When starting the new session, client code (e.g. python) can construct a new ClusterSpec and propagate it to all the nodes within the cluster. This also has the advantage of cleanly separating the core TensorFlow runtime from having to integrate with custom cluster-coordination systems (e.g. etcd, zookeeper, internal Google systems, Netflix's Eureka, k8s, etc.).\nHope this helps!", "body": "Sorry @make1980 for the delayed reply. This thread got lost in my inbox. I apologize!\r\n\r\nIn general whenever there is a machine failure within a TensorFlow cluster, we have to be extra careful. This is because ops can be stateful. In short given a node failure at an arbitrary point after session creation, it's not correct to just re-try running those ops. Concretely in your scenario about 1 worker and 1 ps, if the parameter server fails after the variables have been initialized, it is not correct to begin executing the python training loop once the ps server comes back online. (Because the variables that are placed on it have not been initialized.) Even if we special-cased variables, there are many other stateful ops that also would need explicit handling. Because TensorFlow users can define custom operations, it's not possible to teach the C++-runtime how to automatically handle the failures within a session.\r\n\r\nAs a result, the failure handling strategy we adopt within TensorFlow is to close the session and create a new session (and optionally restoring from a checkpoint). When starting the new session, client code (e.g. python) can construct a new ClusterSpec and propagate it to all the nodes within the cluster. This also has the advantage of cleanly separating the core TensorFlow runtime from having to integrate with custom cluster-coordination systems (e.g. etcd, zookeeper, internal Google systems, Netflix's Eureka, k8s, etc.).\r\n\r\nHope this helps!"}