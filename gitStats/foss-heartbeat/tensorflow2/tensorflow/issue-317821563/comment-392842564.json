{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/392842564", "html_url": "https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-392842564", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18880", "id": 392842564, "node_id": "MDEyOklzc3VlQ29tbWVudDM5Mjg0MjU2NA==", "user": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-29T16:32:04Z", "updated_at": "2018-05-29T16:32:04Z", "author_association": "MEMBER", "body_html": "<p>Hmm, this is a very nuanced topic that deserves careful thought. I guess I see 2 approaches to failure handling: either (1) we adopt the \"crash-only\" philosophy, or (2) we attempt some sort of \"online\" recovery to handle partial (e.g. single node) failures. Unfortunately, they don't mix terribly well.</p>\n<p>Advantages of the first is that it dramatically simplifies the semantics as well as the system implementation. Because fewer code paths are required (and they are exercised constantly), software projects with this fault tolerance model can sometimes be more robust.</p>\n<p>That said, in some domains this approach cannot meet certain system goals, and thus partial failures must be tolerated and handled correctly. (e.g. A distributed database.) Unfortunately, these systems are very complicated and are generally designed around handling failures. These systems have uptime measured over months / years and are distributed across hundreds, thousands, or more servers. By contrast, TensorFlow is instead focused on training jobs that last hours to days (weeks at most), and typically don't scale beyond hundreds of servers. Because of these differing requirements, TensorFlow has historically taken the crash-only approach. (Note: TensorFlow has actually been moving in the opposite direction; new distribution strategies centered around all-reduce and other HPC-style primitives are known to be higher performance (e.g. than parameter-server style configurations) but are easier to ensure they are working reliably in crash-only systems. Parameter-server style configurations running in an async approach are pretty resilient to workers going down.)</p>\n<p>If we are to deviate from this, one way to think about it would be to answer the following questions:</p>\n<ol>\n<li>What are the overall system goals? What application are we trying to meet that we can't meet today?</li>\n<li>What is the end-to-end design? Are certain layers crash-only? If so, how does that mix with online recovery of other layers?</li>\n<li>What is the expected implementation cost of the approach? Does it prevent easy implementation of other features on our roadmap?</li>\n<li>Does the value of the applications outweigh the above costs?</li>\n</ol>\n<p>Dropping down a level (if the above response is focused on the larger discussion, this is on a more detailed technical level), the cost of creating a new session is typically pretty cheap (assuming the graph is not gigantic). State can be stored in containers outside of specific sessions, so [some] more complicated partial-failure recovery can be implemented (e.g. in Python) on top of the core TF primitives today.</p>", "body_text": "Hmm, this is a very nuanced topic that deserves careful thought. I guess I see 2 approaches to failure handling: either (1) we adopt the \"crash-only\" philosophy, or (2) we attempt some sort of \"online\" recovery to handle partial (e.g. single node) failures. Unfortunately, they don't mix terribly well.\nAdvantages of the first is that it dramatically simplifies the semantics as well as the system implementation. Because fewer code paths are required (and they are exercised constantly), software projects with this fault tolerance model can sometimes be more robust.\nThat said, in some domains this approach cannot meet certain system goals, and thus partial failures must be tolerated and handled correctly. (e.g. A distributed database.) Unfortunately, these systems are very complicated and are generally designed around handling failures. These systems have uptime measured over months / years and are distributed across hundreds, thousands, or more servers. By contrast, TensorFlow is instead focused on training jobs that last hours to days (weeks at most), and typically don't scale beyond hundreds of servers. Because of these differing requirements, TensorFlow has historically taken the crash-only approach. (Note: TensorFlow has actually been moving in the opposite direction; new distribution strategies centered around all-reduce and other HPC-style primitives are known to be higher performance (e.g. than parameter-server style configurations) but are easier to ensure they are working reliably in crash-only systems. Parameter-server style configurations running in an async approach are pretty resilient to workers going down.)\nIf we are to deviate from this, one way to think about it would be to answer the following questions:\n\nWhat are the overall system goals? What application are we trying to meet that we can't meet today?\nWhat is the end-to-end design? Are certain layers crash-only? If so, how does that mix with online recovery of other layers?\nWhat is the expected implementation cost of the approach? Does it prevent easy implementation of other features on our roadmap?\nDoes the value of the applications outweigh the above costs?\n\nDropping down a level (if the above response is focused on the larger discussion, this is on a more detailed technical level), the cost of creating a new session is typically pretty cheap (assuming the graph is not gigantic). State can be stored in containers outside of specific sessions, so [some] more complicated partial-failure recovery can be implemented (e.g. in Python) on top of the core TF primitives today.", "body": "Hmm, this is a very nuanced topic that deserves careful thought. I guess I see 2 approaches to failure handling: either (1) we adopt the \"crash-only\" philosophy, or (2) we attempt some sort of \"online\" recovery to handle partial (e.g. single node) failures. Unfortunately, they don't mix terribly well.\r\n\r\nAdvantages of the first is that it dramatically simplifies the semantics as well as the system implementation. Because fewer code paths are required (and they are exercised constantly), software projects with this fault tolerance model can sometimes be more robust.\r\n\r\nThat said, in some domains this approach cannot meet certain system goals, and thus partial failures must be tolerated and handled correctly. (e.g. A distributed database.) Unfortunately, these systems are very complicated and are generally designed around handling failures. These systems have uptime measured over months / years and are distributed across hundreds, thousands, or more servers. By contrast, TensorFlow is instead focused on training jobs that last hours to days (weeks at most), and typically don't scale beyond hundreds of servers. Because of these differing requirements, TensorFlow has historically taken the crash-only approach. (Note: TensorFlow has actually been moving in the opposite direction; new distribution strategies centered around all-reduce and other HPC-style primitives are known to be higher performance (e.g. than parameter-server style configurations) but are easier to ensure they are working reliably in crash-only systems. Parameter-server style configurations running in an async approach are pretty resilient to workers going down.)\r\n\r\nIf we are to deviate from this, one way to think about it would be to answer the following questions:\r\n 1. What are the overall system goals? What application are we trying to meet that we can't meet today?\r\n 2. What is the end-to-end design? Are certain layers crash-only? If so, how does that mix with online recovery of other layers?\r\n 3. What is the expected implementation cost of the approach? Does it prevent easy implementation of other features on our roadmap?\r\n 4. Does the value of the applications outweigh the above costs?\r\n\r\nDropping down a level (if the above response is focused on the larger discussion, this is on a more detailed technical level), the cost of creating a new session is typically pretty cheap (assuming the graph is not gigantic). State can be stored in containers outside of specific sessions, so [some] more complicated partial-failure recovery can be implemented (e.g. in Python) on top of the core TF primitives today."}