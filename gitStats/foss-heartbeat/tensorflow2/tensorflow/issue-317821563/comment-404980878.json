{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404980878", "html_url": "https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-404980878", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18880", "id": 404980878, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDk4MDg3OA==", "user": {"login": "ajbouh", "id": 7325, "node_id": "MDQ6VXNlcjczMjU=", "avatar_url": "https://avatars2.githubusercontent.com/u/7325?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ajbouh", "html_url": "https://github.com/ajbouh", "followers_url": "https://api.github.com/users/ajbouh/followers", "following_url": "https://api.github.com/users/ajbouh/following{/other_user}", "gists_url": "https://api.github.com/users/ajbouh/gists{/gist_id}", "starred_url": "https://api.github.com/users/ajbouh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ajbouh/subscriptions", "organizations_url": "https://api.github.com/users/ajbouh/orgs", "repos_url": "https://api.github.com/users/ajbouh/repos", "events_url": "https://api.github.com/users/ajbouh/events{/privacy}", "received_events_url": "https://api.github.com/users/ajbouh/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-13T23:49:20Z", "updated_at": "2018-07-13T23:49:20Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Brennan, there are many ways to detect failure that should be adequate to\nimprove the situation. These criteria are orthogonal to the rest of the\ndesign. Let us assume for now that upon \"failure\" a worker cleanly shuts\ndown its TCP connections and rejects any future connection attempts.\n\nI assume that any waiting receive will raise an error of some kind. It\nmight be enough to modifying the receive op to *not* raise this error and\n*instead* evaluate some second tensor and return 2 outputs (1 output for\nthe error text/type, 1 output for the second tensor).\n\nAs for how frequently this would be needed, the dropping price for hardware\nand the availability of *preemptible* hardware are two trends that will\ngenerate unique training scenarios that push the limits of today's training\narchitectures much more than the MBTF of underlying hardware. The design\noutlined above is an initial step in that direction.\n\nTensorFlow is uniquely suited to enable these modes. I hope we succeed in\ndoing just that!\n\nThanks for the critical feedback! :)</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Thu, Jun 28, 2018, 08:51 Brennan Saeta ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ajbouh\">@ajbouh</a> &lt;<a href=\"https://github.com/ajbouh\">https://github.com/ajbouh</a>&gt; You're asking some excellent\n questions!\n\n Unfortunately, I'm not sure that detecting worker failure can be done as\n an op. Ops (by design) are not allowed to know of each other, and only\n communicate through input and output tensors. So, if we're trying to detect\n failures at the (arbitrary) op level, we can't necessarily do it as a\n custom op. But, that's probably too hard a problem; let's scope down to\n just detecting worker failures, which we could detect by having an op right\n after the recv op (which handles the RPC communication between workers). If\n we wanted to go down this route, we could augment the graph re-write pass\n that adds in the send's and recv's to also add in our custom loaded failure\n detecting op. But, then two issues arise:\n\n (1) how does the custom failure detector op know when a failure has\n happened? A standard timeout? If so, what's too long? It's not clear to me,\n given the huge variety of workloads running in TF. Even within a workload,\n some recv's should complete within seconds, whereas others are expected to\n take many minutes or even longer.\n\n (2) Assuming we've solved (1), we now have an op (which runs on a worker)\n that knows a failure has occurred. The next question becomes what can be\n done about the failure. We'd then need to define an RPC protocol for the\n worker to either (a) communicate with the master to request a new graph to\n be executed on the failed worker to re-initialize it, or (b) have the\n (potentially many) workers directly re-initialize it after it comes back\n up, and the associated coordination required. Because RPC protocols can't\n be added to a gRPC server after it's been started, this would require some\n non-trivial re-architecting of the RPC stack. (And while this is doable,\n the question becomes what's the value? How many workloads can we unlock\n with this capability? Note that the capability itself does not solve the\n problem; it only allows others to write subtle and complicated C++ ops that\n could attempt to perform failure handling. Would any of these be sharable\n between TF users? Between different workloads?)\n\n As an alternative, we could augment the TF Master to allow some custom\n failure handling instead of as an op. But unfortunately, it doesn't\n necessarily solve all the problems either (what's the expected time a\n worker should wait for an recv?), and introduces new challenges (e.g. we\n currently have no path for dynamically loading plugins into the master\n currently). Again, all of these are solveable with sufficient engineering\n resources. But there's an opportunity cost (in addition to the maintenance\n cost), so we have to understand the value. Given that the MTBF of machines\n (especially Google Compute Engine CPU VMs) is so high (a year or more),\n even a large cluster training for a couple days is not likely to experience\n a failure in the normal case. The question thus becomes, what workloads can\n we not serve today?\n\n Does that make sense?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"317821563\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/18880\" href=\"https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-401081301\">#18880 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAAcnSTnsamo5KB_6WSAgvM7537T2L8lks5uBPuUgaJpZM4TkRsG\">https://github.com/notifications/unsubscribe-auth/AAAcnSTnsamo5KB_6WSAgvM7537T2L8lks5uBPuUgaJpZM4TkRsG</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "Brennan, there are many ways to detect failure that should be adequate to\nimprove the situation. These criteria are orthogonal to the rest of the\ndesign. Let us assume for now that upon \"failure\" a worker cleanly shuts\ndown its TCP connections and rejects any future connection attempts.\n\nI assume that any waiting receive will raise an error of some kind. It\nmight be enough to modifying the receive op to *not* raise this error and\n*instead* evaluate some second tensor and return 2 outputs (1 output for\nthe error text/type, 1 output for the second tensor).\n\nAs for how frequently this would be needed, the dropping price for hardware\nand the availability of *preemptible* hardware are two trends that will\ngenerate unique training scenarios that push the limits of today's training\narchitectures much more than the MBTF of underlying hardware. The design\noutlined above is an initial step in that direction.\n\nTensorFlow is uniquely suited to enable these modes. I hope we succeed in\ndoing just that!\n\nThanks for the critical feedback! :)\n\u2026\nOn Thu, Jun 28, 2018, 08:51 Brennan Saeta ***@***.***> wrote:\n @ajbouh <https://github.com/ajbouh> You're asking some excellent\n questions!\n\n Unfortunately, I'm not sure that detecting worker failure can be done as\n an op. Ops (by design) are not allowed to know of each other, and only\n communicate through input and output tensors. So, if we're trying to detect\n failures at the (arbitrary) op level, we can't necessarily do it as a\n custom op. But, that's probably too hard a problem; let's scope down to\n just detecting worker failures, which we could detect by having an op right\n after the recv op (which handles the RPC communication between workers). If\n we wanted to go down this route, we could augment the graph re-write pass\n that adds in the send's and recv's to also add in our custom loaded failure\n detecting op. But, then two issues arise:\n\n (1) how does the custom failure detector op know when a failure has\n happened? A standard timeout? If so, what's too long? It's not clear to me,\n given the huge variety of workloads running in TF. Even within a workload,\n some recv's should complete within seconds, whereas others are expected to\n take many minutes or even longer.\n\n (2) Assuming we've solved (1), we now have an op (which runs on a worker)\n that knows a failure has occurred. The next question becomes what can be\n done about the failure. We'd then need to define an RPC protocol for the\n worker to either (a) communicate with the master to request a new graph to\n be executed on the failed worker to re-initialize it, or (b) have the\n (potentially many) workers directly re-initialize it after it comes back\n up, and the associated coordination required. Because RPC protocols can't\n be added to a gRPC server after it's been started, this would require some\n non-trivial re-architecting of the RPC stack. (And while this is doable,\n the question becomes what's the value? How many workloads can we unlock\n with this capability? Note that the capability itself does not solve the\n problem; it only allows others to write subtle and complicated C++ ops that\n could attempt to perform failure handling. Would any of these be sharable\n between TF users? Between different workloads?)\n\n As an alternative, we could augment the TF Master to allow some custom\n failure handling instead of as an op. But unfortunately, it doesn't\n necessarily solve all the problems either (what's the expected time a\n worker should wait for an recv?), and introduces new challenges (e.g. we\n currently have no path for dynamically loading plugins into the master\n currently). Again, all of these are solveable with sufficient engineering\n resources. But there's an opportunity cost (in addition to the maintenance\n cost), so we have to understand the value. Given that the MTBF of machines\n (especially Google Compute Engine CPU VMs) is so high (a year or more),\n even a large cluster training for a couple days is not likely to experience\n a failure in the normal case. The question thus becomes, what workloads can\n we not serve today?\n\n Does that make sense?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#18880 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAAcnSTnsamo5KB_6WSAgvM7537T2L8lks5uBPuUgaJpZM4TkRsG>\n .", "body": "Brennan, there are many ways to detect failure that should be adequate to\nimprove the situation. These criteria are orthogonal to the rest of the\ndesign. Let us assume for now that upon \"failure\" a worker cleanly shuts\ndown its TCP connections and rejects any future connection attempts.\n\nI assume that any waiting receive will raise an error of some kind. It\nmight be enough to modifying the receive op to *not* raise this error and\n*instead* evaluate some second tensor and return 2 outputs (1 output for\nthe error text/type, 1 output for the second tensor).\n\nAs for how frequently this would be needed, the dropping price for hardware\nand the availability of *preemptible* hardware are two trends that will\ngenerate unique training scenarios that push the limits of today's training\narchitectures much more than the MBTF of underlying hardware. The design\noutlined above is an initial step in that direction.\n\nTensorFlow is uniquely suited to enable these modes. I hope we succeed in\ndoing just that!\n\nThanks for the critical feedback! :)\n\n\nOn Thu, Jun 28, 2018, 08:51 Brennan Saeta <notifications@github.com> wrote:\n\n> @ajbouh <https://github.com/ajbouh> You're asking some excellent\n> questions!\n>\n> Unfortunately, I'm not sure that detecting worker failure can be done as\n> an op. Ops (by design) are not allowed to know of each other, and only\n> communicate through input and output tensors. So, if we're trying to detect\n> failures at the (arbitrary) op level, we can't necessarily do it as a\n> custom op. But, that's probably too hard a problem; let's scope down to\n> just detecting worker failures, which we could detect by having an op right\n> after the recv op (which handles the RPC communication between workers). If\n> we wanted to go down this route, we could augment the graph re-write pass\n> that adds in the send's and recv's to also add in our custom loaded failure\n> detecting op. But, then two issues arise:\n>\n> (1) how does the custom failure detector op know when a failure has\n> happened? A standard timeout? If so, what's too long? It's not clear to me,\n> given the huge variety of workloads running in TF. Even within a workload,\n> some recv's should complete within seconds, whereas others are expected to\n> take many minutes or even longer.\n>\n> (2) Assuming we've solved (1), we now have an op (which runs on a worker)\n> that knows a failure has occurred. The next question becomes what can be\n> done about the failure. We'd then need to define an RPC protocol for the\n> worker to either (a) communicate with the master to request a new graph to\n> be executed on the failed worker to re-initialize it, or (b) have the\n> (potentially many) workers directly re-initialize it after it comes back\n> up, and the associated coordination required. Because RPC protocols can't\n> be added to a gRPC server after it's been started, this would require some\n> non-trivial re-architecting of the RPC stack. (And while this is doable,\n> the question becomes what's the value? How many workloads can we unlock\n> with this capability? Note that the capability itself does not solve the\n> problem; it only allows others to write subtle and complicated C++ ops that\n> could attempt to perform failure handling. Would any of these be sharable\n> between TF users? Between different workloads?)\n>\n> As an alternative, we could augment the TF Master to allow some custom\n> failure handling instead of as an op. But unfortunately, it doesn't\n> necessarily solve all the problems either (what's the expected time a\n> worker should wait for an recv?), and introduces new challenges (e.g. we\n> currently have no path for dynamically loading plugins into the master\n> currently). Again, all of these are solveable with sufficient engineering\n> resources. But there's an opportunity cost (in addition to the maintenance\n> cost), so we have to understand the value. Given that the MTBF of machines\n> (especially Google Compute Engine CPU VMs) is so high (a year or more),\n> even a large cluster training for a couple days is not likely to experience\n> a failure in the normal case. The question thus becomes, what workloads can\n> we not serve today?\n>\n> Does that make sense?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18880#issuecomment-401081301>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAAcnSTnsamo5KB_6WSAgvM7537T2L8lks5uBPuUgaJpZM4TkRsG>\n> .\n>\n"}