{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5995", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5995/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5995/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5995/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5995", "id": 192725256, "node_id": "MDU6SXNzdWUxOTI3MjUyNTY=", "number": 5995, "title": "GPU MUCH slower than CPU", "user": {"login": "camj256", "id": 6239949, "node_id": "MDQ6VXNlcjYyMzk5NDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/6239949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/camj256", "html_url": "https://github.com/camj256", "followers_url": "https://api.github.com/users/camj256/followers", "following_url": "https://api.github.com/users/camj256/following{/other_user}", "gists_url": "https://api.github.com/users/camj256/gists{/gist_id}", "starred_url": "https://api.github.com/users/camj256/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/camj256/subscriptions", "organizations_url": "https://api.github.com/users/camj256/orgs", "repos_url": "https://api.github.com/users/camj256/repos", "events_url": "https://api.github.com/users/camj256/events{/privacy}", "received_events_url": "https://api.github.com/users/camj256/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-12-01T00:51:49Z", "updated_at": "2016-12-03T05:23:01Z", "closed_at": "2016-12-03T05:23:01Z", "author_association": "NONE", "body_html": "<p>GPU training is MUCH slower than CPU training. It's possible I'm doing something wrong. If I'm not I can gather more data on this. The data set is pretty small and it slows to a crawl. GPU usage is around 2-5%, It fills up the memory in the GPU pretty quickly to 90% but the PCIe Bandwidth Utilization is 1%. My CPU and Memory usage are otherwise minimal.</p>\n<p>My setup: 32gb ram, 8 core 4.3 Ghz processor, (2) GTX 660's, 367.57 Nvidia Driver, Cuda Toolkit 7.5, cudnn 7.5, Python 2.7. Tensorflow matches 7.5.</p>\n<p>I can take exact time measurements later if needed but I would guess GPU is about 10x slower if not more than the CPU training.</p>\n<pre><code>pprint.pprint(len(X))\npprint.pprint(len(Y))\n\nnet = tflearn.input_data(shape=[None, 7])\nnet = tflearn.fully_connected(net, 32)\nnet = tflearn.fully_connected(net, 32)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net)\n\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y, n_epoch=100, batch_size=16, show_metric=True)\n\nmodel.save(\"model.tfl\")\n</code></pre>\n<pre><code>camj256@camj256:~/PycharmProjects/DeepLearning$ python build.py \nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nExceptions: 0\n3003\n3003\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:02:00.0\nTotal memory: 1.99GiB\nFree memory: 1.27GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x2af5d00\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:01:00.0\nTotal memory: 1.99GiB\nFree memory: 1.24GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\n---------------------------------\nRun id: 17E6I9\nLog directory: /tmp/tflearn_logs/\n---------------------------------\nTraining samples: 3003\nValidation samples: 0\n</code></pre>", "body_text": "GPU training is MUCH slower than CPU training. It's possible I'm doing something wrong. If I'm not I can gather more data on this. The data set is pretty small and it slows to a crawl. GPU usage is around 2-5%, It fills up the memory in the GPU pretty quickly to 90% but the PCIe Bandwidth Utilization is 1%. My CPU and Memory usage are otherwise minimal.\nMy setup: 32gb ram, 8 core 4.3 Ghz processor, (2) GTX 660's, 367.57 Nvidia Driver, Cuda Toolkit 7.5, cudnn 7.5, Python 2.7. Tensorflow matches 7.5.\nI can take exact time measurements later if needed but I would guess GPU is about 10x slower if not more than the CPU training.\npprint.pprint(len(X))\npprint.pprint(len(Y))\n\nnet = tflearn.input_data(shape=[None, 7])\nnet = tflearn.fully_connected(net, 32)\nnet = tflearn.fully_connected(net, 32)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net)\n\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y, n_epoch=100, batch_size=16, show_metric=True)\n\nmodel.save(\"model.tfl\")\n\ncamj256@camj256:~/PycharmProjects/DeepLearning$ python build.py \nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\nExceptions: 0\n3003\n3003\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:02:00.0\nTotal memory: 1.99GiB\nFree memory: 1.27GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x2af5d00\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:01:00.0\nTotal memory: 1.99GiB\nFree memory: 1.24GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\n---------------------------------\nRun id: 17E6I9\nLog directory: /tmp/tflearn_logs/\n---------------------------------\nTraining samples: 3003\nValidation samples: 0", "body": "GPU training is MUCH slower than CPU training. It's possible I'm doing something wrong. If I'm not I can gather more data on this. The data set is pretty small and it slows to a crawl. GPU usage is around 2-5%, It fills up the memory in the GPU pretty quickly to 90% but the PCIe Bandwidth Utilization is 1%. My CPU and Memory usage are otherwise minimal.\r\n\r\nMy setup: 32gb ram, 8 core 4.3 Ghz processor, (2) GTX 660's, 367.57 Nvidia Driver, Cuda Toolkit 7.5, cudnn 7.5, Python 2.7. Tensorflow matches 7.5.\r\n\r\nI can take exact time measurements later if needed but I would guess GPU is about 10x slower if not more than the CPU training.\r\n\r\n```\r\npprint.pprint(len(X))\r\npprint.pprint(len(Y))\r\n\r\nnet = tflearn.input_data(shape=[None, 7])\r\nnet = tflearn.fully_connected(net, 32)\r\nnet = tflearn.fully_connected(net, 32)\r\nnet = tflearn.fully_connected(net, 2, activation='softmax')\r\nnet = tflearn.regression(net)\r\n\r\nmodel = tflearn.DNN(net)\r\nmodel.fit(X, Y, n_epoch=100, batch_size=16, show_metric=True)\r\n\r\nmodel.save(\"model.tfl\")\r\n```\r\n\r\n```\r\ncamj256@camj256:~/PycharmProjects/DeepLearning$ python build.py \r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally\r\nExceptions: 0\r\n3003\r\n3003\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\r\npciBusID 0000:02:00.0\r\nTotal memory: 1.99GiB\r\nFree memory: 1.27GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:572] creating context when one is currently active; existing: 0x2af5d00\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:925] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.99GiB\r\nFree memory: 1.24GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 0 to device ordinal 1\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:59] cannot enable peer access from device ordinal 1 to device ordinal 0\r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:838] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\r\n---------------------------------\r\nRun id: 17E6I9\r\nLog directory: /tmp/tflearn_logs/\r\n---------------------------------\r\nTraining samples: 3003\r\nValidation samples: 0\r\n```\r\n\r\n\r\n"}