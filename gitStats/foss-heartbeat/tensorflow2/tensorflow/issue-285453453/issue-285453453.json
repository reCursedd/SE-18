{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15788", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15788/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15788/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15788/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15788", "id": 285453453, "node_id": "MDU6SXNzdWUyODU0NTM0NTM=", "number": 15788, "title": "MultiRNNCell and reuse_variables()", "user": {"login": "HelloSeeing", "id": 25819371, "node_id": "MDQ6VXNlcjI1ODE5Mzcx", "avatar_url": "https://avatars0.githubusercontent.com/u/25819371?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HelloSeeing", "html_url": "https://github.com/HelloSeeing", "followers_url": "https://api.github.com/users/HelloSeeing/followers", "following_url": "https://api.github.com/users/HelloSeeing/following{/other_user}", "gists_url": "https://api.github.com/users/HelloSeeing/gists{/gist_id}", "starred_url": "https://api.github.com/users/HelloSeeing/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HelloSeeing/subscriptions", "organizations_url": "https://api.github.com/users/HelloSeeing/orgs", "repos_url": "https://api.github.com/users/HelloSeeing/repos", "events_url": "https://api.github.com/users/HelloSeeing/events{/privacy}", "received_events_url": "https://api.github.com/users/HelloSeeing/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2018-01-02T15:12:14Z", "updated_at": "2018-05-20T09:46:27Z", "closed_at": "2018-01-08T15:42:05Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 14.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.4</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 4.8.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/6.0</li>\n<li><strong>GPU model and memory</strong>: Geforce GTX TITAN X/12G</li>\n</ul>\n<h3>Describe the problem</h3>\n<p><code>tf.nn.dynamic_rnn()</code> and <code>tf.nn.rnn_cell.MultiRNNCell()</code> breaks down when add <strong><code>tf.get_variable_scope().reuse_variables()</code></strong> before defining a rnn architecture.</p>\n<h4>Correct Situation</h4>\n<ul>\n<li>When there is no <code>tf.get_variable_scope().reuse_variables()</code></li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nx <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">100</span>])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_lstm</span>(<span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">num_layers</span>, <span class=\"pl-smi\">batch_size</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">build_cell</span>(<span class=\"pl-smi\">num_units</span>):\n        <span class=\"pl-k\">return</span> tf.contrib.rnn.LSTMCell(num_units)\n    \n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>multi_cells<span class=\"pl-pds\">'</span></span>):\n        cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_layers)])\n    init_state <span class=\"pl-k\">=</span> cell.zero_state(batch_size, tf.float32)\n    \n    <span class=\"pl-k\">return</span> cell, init_state\n\nlstm_cell, lstm_init_state <span class=\"pl-k\">=</span> build_lstm(<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>)\nlstm, final_state <span class=\"pl-k\">=</span>  tf.nn.dynamic_rnn(lstm_cell, x, <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span>lstm_init_state, <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>It works fine!</p>\n<h4>Wrong Situation</h4>\n<ul>\n<li>When there is <code>tf.get_variable_scope().reuse_variables()</code></li>\n</ul>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\nx <span class=\"pl-k\">=</span> tf.random_normal([<span class=\"pl-c1\">6</span>, <span class=\"pl-c1\">5</span>, <span class=\"pl-c1\">100</span>])\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> add a magic sentence here</span>\ntf.get_variable_scope().reuse_variables()\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_lstm</span>(<span class=\"pl-smi\">num_units</span>, <span class=\"pl-smi\">num_layers</span>, <span class=\"pl-smi\">batch_size</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">build_cell</span>(<span class=\"pl-smi\">num_units</span>):\n        <span class=\"pl-k\">return</span> tf.contrib.rnn.LSTMCell(num_units)\n    \n    <span class=\"pl-k\">with</span> tf.name_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>multi_cells<span class=\"pl-pds\">'</span></span>):\n        cell <span class=\"pl-k\">=</span> tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) <span class=\"pl-k\">for</span> _ <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_layers)])\n    init_state <span class=\"pl-k\">=</span> cell.zero_state(batch_size, tf.float32)\n    \n    <span class=\"pl-k\">return</span> cell, init_state\n\nlstm_cell, lstm_init_state <span class=\"pl-k\">=</span> build_lstm(<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">5</span>)\nlstm, final_state <span class=\"pl-k\">=</span>  tf.nn.dynamic_rnn(lstm_cell, x, <span class=\"pl-v\">initial_state</span><span class=\"pl-k\">=</span>lstm_init_state, <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)</pre></div>\n<p>it returns:</p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-7-d333e19dbfd0&gt; in &lt;module&gt;()\n----&gt; 1 lstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n    612         swap_memory=swap_memory,\n    613         sequence_length=sequence_length,\n--&gt; 614         dtype=dtype)\n    615 \n    616     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n    775       loop_vars=(time, output_ta, state),\n    776       parallel_iterations=parallel_iterations,\n--&gt; 777       swap_memory=swap_memory)\n    778 \n    779   # Unpack final output if not using output tuples.\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\n-&gt; 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   2817     return result\n   2818 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n   2638       self.Enter()\n   2639       original_body_result, exit_vars = self._BuildLoop(\n-&gt; 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2641     finally:\n   2642       self.Exit()\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2588         structure=original_loop_vars,\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\n-&gt; 2590     body_result = body(*packed_vars_for_body)\n   2591     if not nest.is_sequence(body_result):\n   2592       body_result = [body_result]\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\n    760           skip_conditionals=True)\n    761     else:\n--&gt; 762       (output, new_state) = call_cell()\n    763 \n    764     # Pack state if using state tuples\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in &lt;lambda&gt;()\n    746 \n    747     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n--&gt; 748     call_cell = lambda: cell(input_t, state)\n    749 \n    750     if sequence_length is not None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\n    181       with vs.variable_scope(vs.get_variable_scope(),\n    182                              custom_getter=self._rnn_get_variable):\n--&gt; 183         return super(RNNCell, self).__call__(inputs, state)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\n    573         if in_graph_mode:\n    574           self._assert_input_compatibility(inputs)\n--&gt; 575         outputs = self.call(inputs, *args, **kwargs)\n    576 \n    577         if outputs is None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\n   1064                                       [-1, cell.state_size])\n   1065           cur_state_pos += cell.state_size\n-&gt; 1066         cur_inp, new_state = cell(cur_inp, cur_state)\n   1067         new_states.append(new_state)\n   1068 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\n    181       with vs.variable_scope(vs.get_variable_scope(),\n    182                              custom_getter=self._rnn_get_variable):\n--&gt; 183         return super(RNNCell, self).__call__(inputs, state)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\n    573         if in_graph_mode:\n    574           self._assert_input_compatibility(inputs)\n--&gt; 575         outputs = self.call(inputs, *args, **kwargs)\n    576 \n    577         if outputs is None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\n    606               partitioned_variables.fixed_size_partitioner(\n    607                   self._num_unit_shards))\n--&gt; 608         self._linear1 = _Linear([inputs, m_prev], 4 * self._num_units, True)\n    609 \n    610     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __init__(self, args, output_size, build_bias, bias_initializer, kernel_initializer)\n   1169           _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n   1170           dtype=dtype,\n-&gt; 1171           initializer=kernel_initializer)\n   1172       if build_bias:\n   1173         with vs.variable_scope(outer_scope) as inner_scope:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1201       partitioner=partitioner, validate_shape=validate_shape,\n   1202       use_resource=use_resource, custom_getter=custom_getter,\n-&gt; 1203       constraint=constraint)\n   1204 get_variable_or_local_docstring = (\n   1205     \"\"\"%s\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1090           partitioner=partitioner, validate_shape=validate_shape,\n   1091           use_resource=use_resource, custom_getter=custom_getter,\n-&gt; 1092           constraint=constraint)\n   1093 \n   1094   def _get_partitioned_variable(self,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n    415       if \"constraint\" in estimator_util.fn_args(custom_getter):\n    416         custom_getter_kwargs[\"constraint\"] = constraint\n--&gt; 417       return custom_getter(**custom_getter_kwargs)\n    418     else:\n    419       return _true_getter(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in wrapped_custom_getter(getter, *args, **kwargs)\n   1581     return custom_getter(\n   1582         functools.partial(old_getter, getter),\n-&gt; 1583         *args, **kwargs)\n   1584   return wrapped_custom_getter\n   1585 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n--&gt; 186     variable = getter(*args, **kwargs)\n    187     if context.in_graph_mode():\n    188       trainable = (variable in tf_variables.trainable_variables() or\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n--&gt; 186     variable = getter(*args, **kwargs)\n    187     if context.in_graph_mode():\n    188       trainable = (variable in tf_variables.trainable_variables() or\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\n    392           trainable=trainable, collections=collections,\n    393           caching_device=caching_device, validate_shape=validate_shape,\n--&gt; 394           use_resource=use_resource, constraint=constraint)\n    395 \n    396     if custom_getter is not None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\n    758       raise ValueError(\"Variable %s does not exist, or was not created with \"\n    759                        \"tf.get_variable(). Did you mean to set \"\n--&gt; 760                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n    761     if not shape.is_fully_defined() and not initializing_from_value:\n    762       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n\nValueError: Variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\n</code></pre>\n<h3>Conclusion</h3>\n<p>It seems that <code>tf.get_variable_scope().reuse_variables()</code> and <code>tf.nn.rnn_cell.MultiRNNCell()</code> are not compatiable</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 14.04\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): 1.4\nPython version: 2.7.6\nBazel version (if compiling from source): 0.5.4\nGCC/Compiler version (if compiling from source): 4.8.4\nCUDA/cuDNN version: 8.0/6.0\nGPU model and memory: Geforce GTX TITAN X/12G\n\nDescribe the problem\ntf.nn.dynamic_rnn() and tf.nn.rnn_cell.MultiRNNCell() breaks down when add tf.get_variable_scope().reuse_variables() before defining a rnn architecture.\nCorrect Situation\n\nWhen there is no tf.get_variable_scope().reuse_variables()\n\nimport tensorflow as tf\nx = tf.random_normal([6, 5, 100])\n\ndef build_lstm(num_units, num_layers, batch_size):\n    def build_cell(num_units):\n        return tf.contrib.rnn.LSTMCell(num_units)\n    \n    with tf.name_scope('multi_cells'):\n        cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\n    init_state = cell.zero_state(batch_size, tf.float32)\n    \n    return cell, init_state\n\nlstm_cell, lstm_init_state = build_lstm(200, 2, 5)\nlstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\nIt works fine!\nWrong Situation\n\nWhen there is tf.get_variable_scope().reuse_variables()\n\nimport tensorflow as tf\nx = tf.random_normal([6, 5, 100])\n\n# add a magic sentence here\ntf.get_variable_scope().reuse_variables()\n\ndef build_lstm(num_units, num_layers, batch_size):\n    def build_cell(num_units):\n        return tf.contrib.rnn.LSTMCell(num_units)\n    \n    with tf.name_scope('multi_cells'):\n        cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\n    init_state = cell.zero_state(batch_size, tf.float32)\n    \n    return cell, init_state\n\nlstm_cell, lstm_init_state = build_lstm(200, 2, 5)\nlstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\nit returns:\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-7-d333e19dbfd0> in <module>()\n----> 1 lstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\n    612         swap_memory=swap_memory,\n    613         sequence_length=sequence_length,\n--> 614         dtype=dtype)\n    615 \n    616     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\n    775       loop_vars=(time, output_ta, state),\n    776       parallel_iterations=parallel_iterations,\n--> 777       swap_memory=swap_memory)\n    778 \n    779   # Unpack final output if not using output tuples.\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\n-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\n   2817     return result\n   2818 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\n   2638       self.Enter()\n   2639       original_body_result, exit_vars = self._BuildLoop(\n-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2641     finally:\n   2642       self.Exit()\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\n   2588         structure=original_loop_vars,\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\n-> 2590     body_result = body(*packed_vars_for_body)\n   2591     if not nest.is_sequence(body_result):\n   2592       body_result = [body_result]\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\n    760           skip_conditionals=True)\n    761     else:\n--> 762       (output, new_state) = call_cell()\n    763 \n    764     # Pack state if using state tuples\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in <lambda>()\n    746 \n    747     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\n--> 748     call_cell = lambda: cell(input_t, state)\n    749 \n    750     if sequence_length is not None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\n    181       with vs.variable_scope(vs.get_variable_scope(),\n    182                              custom_getter=self._rnn_get_variable):\n--> 183         return super(RNNCell, self).__call__(inputs, state)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\n    573         if in_graph_mode:\n    574           self._assert_input_compatibility(inputs)\n--> 575         outputs = self.call(inputs, *args, **kwargs)\n    576 \n    577         if outputs is None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\n   1064                                       [-1, cell.state_size])\n   1065           cur_state_pos += cell.state_size\n-> 1066         cur_inp, new_state = cell(cur_inp, cur_state)\n   1067         new_states.append(new_state)\n   1068 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\n    181       with vs.variable_scope(vs.get_variable_scope(),\n    182                              custom_getter=self._rnn_get_variable):\n--> 183         return super(RNNCell, self).__call__(inputs, state)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\n    573         if in_graph_mode:\n    574           self._assert_input_compatibility(inputs)\n--> 575         outputs = self.call(inputs, *args, **kwargs)\n    576 \n    577         if outputs is None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\n    606               partitioned_variables.fixed_size_partitioner(\n    607                   self._num_unit_shards))\n--> 608         self._linear1 = _Linear([inputs, m_prev], 4 * self._num_units, True)\n    609 \n    610     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __init__(self, args, output_size, build_bias, bias_initializer, kernel_initializer)\n   1169           _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n   1170           dtype=dtype,\n-> 1171           initializer=kernel_initializer)\n   1172       if build_bias:\n   1173         with vs.variable_scope(outer_scope) as inner_scope:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1201       partitioner=partitioner, validate_shape=validate_shape,\n   1202       use_resource=use_resource, custom_getter=custom_getter,\n-> 1203       constraint=constraint)\n   1204 get_variable_or_local_docstring = (\n   1205     \"\"\"%s\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n   1090           partitioner=partitioner, validate_shape=validate_shape,\n   1091           use_resource=use_resource, custom_getter=custom_getter,\n-> 1092           constraint=constraint)\n   1093 \n   1094   def _get_partitioned_variable(self,\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\n    415       if \"constraint\" in estimator_util.fn_args(custom_getter):\n    416         custom_getter_kwargs[\"constraint\"] = constraint\n--> 417       return custom_getter(**custom_getter_kwargs)\n    418     else:\n    419       return _true_getter(\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in wrapped_custom_getter(getter, *args, **kwargs)\n   1581     return custom_getter(\n   1582         functools.partial(old_getter, getter),\n-> 1583         *args, **kwargs)\n   1584   return wrapped_custom_getter\n   1585 \n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n--> 186     variable = getter(*args, **kwargs)\n    187     if context.in_graph_mode():\n    188       trainable = (variable in tf_variables.trainable_variables() or\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\n    184 \n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\n--> 186     variable = getter(*args, **kwargs)\n    187     if context.in_graph_mode():\n    188       trainable = (variable in tf_variables.trainable_variables() or\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\n    392           trainable=trainable, collections=collections,\n    393           caching_device=caching_device, validate_shape=validate_shape,\n--> 394           use_resource=use_resource, constraint=constraint)\n    395 \n    396     if custom_getter is not None:\n\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\n    758       raise ValueError(\"Variable %s does not exist, or was not created with \"\n    759                        \"tf.get_variable(). Did you mean to set \"\n--> 760                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\n    761     if not shape.is_fully_defined() and not initializing_from_value:\n    762       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\n\nValueError: Variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\n\nConclusion\nIt seems that tf.get_variable_scope().reuse_variables() and tf.nn.rnn_cell.MultiRNNCell() are not compatiable", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 14.04\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.6\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: 4.8.4\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n- **GPU model and memory**: Geforce GTX TITAN X/12G\r\n\r\n### Describe the problem\r\n\r\n`tf.nn.dynamic_rnn()` and `tf.nn.rnn_cell.MultiRNNCell()` breaks down when add **`tf.get_variable_scope().reuse_variables()`** before defining a rnn architecture.\r\n\r\n#### Correct Situation\r\n\r\n- When there is no `tf.get_variable_scope().reuse_variables()`\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random_normal([6, 5, 100])\r\n\r\ndef build_lstm(num_units, num_layers, batch_size):\r\n    def build_cell(num_units):\r\n        return tf.contrib.rnn.LSTMCell(num_units)\r\n    \r\n    with tf.name_scope('multi_cells'):\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\r\n    init_state = cell.zero_state(batch_size, tf.float32)\r\n    \r\n    return cell, init_state\r\n\r\nlstm_cell, lstm_init_state = build_lstm(200, 2, 5)\r\nlstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\r\n```\r\nIt works fine!\r\n\r\n#### Wrong Situation\r\n\r\n- When there is `tf.get_variable_scope().reuse_variables()`\r\n\r\n```python\r\nimport tensorflow as tf\r\nx = tf.random_normal([6, 5, 100])\r\n\r\n# add a magic sentence here\r\ntf.get_variable_scope().reuse_variables()\r\n\r\ndef build_lstm(num_units, num_layers, batch_size):\r\n    def build_cell(num_units):\r\n        return tf.contrib.rnn.LSTMCell(num_units)\r\n    \r\n    with tf.name_scope('multi_cells'):\r\n        cell = tf.nn.rnn_cell.MultiRNNCell([build_cell(num_units) for _ in range(num_layers)])\r\n    init_state = cell.zero_state(batch_size, tf.float32)\r\n    \r\n    return cell, init_state\r\n\r\nlstm_cell, lstm_init_state = build_lstm(200, 2, 5)\r\nlstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\r\n```\r\nit returns:\r\n\r\n```\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-7-d333e19dbfd0> in <module>()\r\n----> 1 lstm, final_state =  tf.nn.dynamic_rnn(lstm_cell, x, initial_state=lstm_init_state, time_major=True)\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in dynamic_rnn(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\r\n    612         swap_memory=swap_memory,\r\n    613         sequence_length=sequence_length,\r\n--> 614         dtype=dtype)\r\n    615 \r\n    616     # Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _dynamic_rnn_loop(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\r\n    775       loop_vars=(time, output_ta, state),\r\n    776       parallel_iterations=parallel_iterations,\r\n--> 777       swap_memory=swap_memory)\r\n    778 \r\n    779   # Unpack final output if not using output tuples.\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in while_loop(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name)\r\n   2814     loop_context = WhileContext(parallel_iterations, back_prop, swap_memory)  # pylint: disable=redefined-outer-name\r\n   2815     ops.add_to_collection(ops.GraphKeys.WHILE_CONTEXT, loop_context)\r\n-> 2816     result = loop_context.BuildLoop(cond, body, loop_vars, shape_invariants)\r\n   2817     return result\r\n   2818 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in BuildLoop(self, pred, body, loop_vars, shape_invariants)\r\n   2638       self.Enter()\r\n   2639       original_body_result, exit_vars = self._BuildLoop(\r\n-> 2640           pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2641     finally:\r\n   2642       self.Exit()\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/control_flow_ops.pyc in _BuildLoop(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\r\n   2588         structure=original_loop_vars,\r\n   2589         flat_sequence=vars_for_body_with_tensor_arrays)\r\n-> 2590     body_result = body(*packed_vars_for_body)\r\n   2591     if not nest.is_sequence(body_result):\r\n   2592       body_result = [body_result]\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in _time_step(time, output_ta_t, state)\r\n    760           skip_conditionals=True)\r\n    761     else:\r\n--> 762       (output, new_state) = call_cell()\r\n    763 \r\n    764     # Pack state if using state tuples\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn.pyc in <lambda>()\r\n    746 \r\n    747     input_t = nest.pack_sequence_as(structure=inputs, flat_sequence=input_t)\r\n--> 748     call_cell = lambda: cell(input_t, state)\r\n    749 \r\n    750     if sequence_length is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n    181       with vs.variable_scope(vs.get_variable_scope(),\r\n    182                              custom_getter=self._rnn_get_variable):\r\n--> 183         return super(RNNCell, self).__call__(inputs, state)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\r\n   1064                                       [-1, cell.state_size])\r\n   1065           cur_state_pos += cell.state_size\r\n-> 1066         cur_inp, new_state = cell(cur_inp, cur_state)\r\n   1067         new_states.append(new_state)\r\n   1068 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __call__(self, inputs, state, scope)\r\n    181       with vs.variable_scope(vs.get_variable_scope(),\r\n    182                              custom_getter=self._rnn_get_variable):\r\n--> 183         return super(RNNCell, self).__call__(inputs, state)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/layers/base.pyc in __call__(self, inputs, *args, **kwargs)\r\n    573         if in_graph_mode:\r\n    574           self._assert_input_compatibility(inputs)\r\n--> 575         outputs = self.call(inputs, *args, **kwargs)\r\n    576 \r\n    577         if outputs is None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in call(self, inputs, state)\r\n    606               partitioned_variables.fixed_size_partitioner(\r\n    607                   self._num_unit_shards))\r\n--> 608         self._linear1 = _Linear([inputs, m_prev], 4 * self._num_units, True)\r\n    609 \r\n    610     # i = input_gate, j = new_input, f = forget_gate, o = output_gate\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in __init__(self, args, output_size, build_bias, bias_initializer, kernel_initializer)\r\n   1169           _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\r\n   1170           dtype=dtype,\r\n-> 1171           initializer=kernel_initializer)\r\n   1172       if build_bias:\r\n   1173         with vs.variable_scope(outer_scope) as inner_scope:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1201       partitioner=partitioner, validate_shape=validate_shape,\r\n   1202       use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1203       constraint=constraint)\r\n   1204 get_variable_or_local_docstring = (\r\n   1205     \"\"\"%s\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n   1090           partitioner=partitioner, validate_shape=validate_shape,\r\n   1091           use_resource=use_resource, custom_getter=custom_getter,\r\n-> 1092           constraint=constraint)\r\n   1093 \r\n   1094   def _get_partitioned_variable(self,\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in get_variable(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\r\n    415       if \"constraint\" in estimator_util.fn_args(custom_getter):\r\n    416         custom_getter_kwargs[\"constraint\"] = constraint\r\n--> 417       return custom_getter(**custom_getter_kwargs)\r\n    418     else:\r\n    419       return _true_getter(\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in wrapped_custom_getter(getter, *args, **kwargs)\r\n   1581     return custom_getter(\r\n   1582         functools.partial(old_getter, getter),\r\n-> 1583         *args, **kwargs)\r\n   1584   return wrapped_custom_getter\r\n   1585 \r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 186     variable = getter(*args, **kwargs)\r\n    187     if context.in_graph_mode():\r\n    188       trainable = (variable in tf_variables.trainable_variables() or\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/rnn_cell_impl.pyc in _rnn_get_variable(self, getter, *args, **kwargs)\r\n    184 \r\n    185   def _rnn_get_variable(self, getter, *args, **kwargs):\r\n--> 186     variable = getter(*args, **kwargs)\r\n    187     if context.in_graph_mode():\r\n    188       trainable = (variable in tf_variables.trainable_variables() or\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _true_getter(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\r\n    392           trainable=trainable, collections=collections,\r\n    393           caching_device=caching_device, validate_shape=validate_shape,\r\n--> 394           use_resource=use_resource, constraint=constraint)\r\n    395 \r\n    396     if custom_getter is not None:\r\n\r\n/usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/variable_scope.pyc in _get_single_variable(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\r\n    758       raise ValueError(\"Variable %s does not exist, or was not created with \"\r\n    759                        \"tf.get_variable(). Did you mean to set \"\r\n--> 760                        \"reuse=tf.AUTO_REUSE in VarScope?\" % name)\r\n    761     if not shape.is_fully_defined() and not initializing_from_value:\r\n    762       raise ValueError(\"Shape of a new variable (%s) must be fully defined, \"\r\n\r\nValueError: Variable rnn/multi_rnn_cell/cell_0/lstm_cell/kernel does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n```\r\n\r\n### Conclusion\r\nIt seems that `tf.get_variable_scope().reuse_variables()` and `tf.nn.rnn_cell.MultiRNNCell()` are not compatiable"}