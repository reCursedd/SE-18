{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/196139015", "pull_request_review_id": 129635263, "id": 196139015, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE5NjEzOTAxNQ==", "diff_hunk": "@@ -0,0 +1,992 @@\n+{\n+  \"nbformat\": 4,\n+  \"nbformat_minor\": 0,\n+  \"metadata\": {\n+    \"colab\": {\n+      \"name\": \"NMT with Attention.ipynb\",\n+      \"version\": \"0.3.2\",\n+      \"views\": {},\n+      \"default_view\": {},\n+      \"provenance\": [\n+        {\n+          \"file_id\": \"1C4fpM7_7IL8ZzF7Gc5abywqQjeQNS2-U\",\n+          \"timestamp\": 1527858391290\n+        },\n+        {\n+          \"file_id\": \"1pExo6aUuw0S6MISFWoinfJv0Ftm9V4qv\",\n+          \"timestamp\": 1527776041613\n+        }\n+      ],\n+      \"private_outputs\": true,\n+      \"collapsed_sections\": []\n+    },\n+    \"kernelspec\": {\n+      \"name\": \"python3\",\n+      \"display_name\": \"Python 3\"\n+    },\n+    \"accelerator\": \"GPU\"\n+  },\n+  \"cells\": [\n+    {\n+      \"metadata\": {\n+        \"id\": \"AOpGoE2T-YXS\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"##### Copyright 2018 The TensorFlow Authors.\\n\",\n+        \"\\n\",\n+        \"Licensed under the Apache License, Version 2.0 (the \\\"License\\\").\\n\",\n+        \"\\n\",\n+        \"# Neural Machine Translation with Attention\\n\",\n+        \"\\n\",\n+        \"This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). This is an advanced example for readers with prior background in sequence to sequence models.\\n\",\n+        \"\\n\",\n+        \"Here's an example output you'll see after running this notebook. After training the model, we'll translate the Spanish sentence \\\"\u00bftodavia estan en casa?\\\", and we'll see the output \\\"are you still at home ?\\\". \\n\",\n+        \"\\n\",\n+        \"The translation quality is reasonable for a toy example, but what's even cooler is the attention plot that will be generated:\\n\",\n+        \"\\n\",\n+        \"This shows which parts of the input sentence the model is attending to while translating. \\n\",\n+        \"\\n\",\n+        \"![alt text](https://tensorflow.org/images/spanish-english.png)\\n\",\n+        \"\\n\",\n+        \"\\n\",\n+        \"Ballpark, this example will take approximately 10 mintues to run on a single P100 GPU.\\n\",\n+        \"\\n\",\n+        \"This notebook requires Tensorflow version >= 1.9\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"tnxXKDjq3jEL\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Import TensorFlow and enable eager execution\\n\",\n+        \"import tensorflow as tf\\n\",\n+        \"import tensorflow.contrib.eager as tfe\\n\",\n+        \"tf.enable_eager_execution()\\n\",\n+        \"\\n\",\n+        \"# We'll generate plots of attention in order to see which parts of a sentence\\n\",\n+        \"# our model focuses on during translation\\n\",\n+        \"import matplotlib.pyplot as plt\\n\",\n+        \"\\n\",\n+        \"# Scikit-learn includes many handy utilities\\n\",\n+        \"from sklearn.model_selection import train_test_split\\n\",\n+        \"\\n\",\n+        \"import unicodedata\\n\",\n+        \"import re\\n\",\n+        \"import numpy as np\\n\",\n+        \"import os\\n\",\n+        \"import time\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"wfodePkj3jEa\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Download and prepare the dataset\\n\",\n+        \"\\n\",\n+        \"We'll use a dataset helpfully provided by http://www.manythings.org/anki/. This contains language translation pairs, in this format:\\n\",\n+        \"\\n\",\n+        \"```\\n\",\n+        \"May I borrow this book?\\t\u00bfPuedo tomar prestado este libro?\\n\",\n+        \"```\\n\",\n+        \"\\n\",\n+        \"There are a variety of such datasets you can explore. This notebook will download and use the English-Spanish dataset. \\n\",\n+        \"\\n\",\n+        \"We've hosted a copy on Google Cloud for convenience. Alternatively, you can download and use a similar dataset (like English -> German) from http://www.manythings.org/anki/ and use it instead without changing any other code.\\n\",\n+        \"\\n\",\n+        \"After we've downloaded it, here are the steps we'll use to prepare the data:\\n\",\n+        \"\\n\",\n+        \"* Add a start and end token to each sentence\\n\",\n+        \"* Clean the sentences by removing special characters\\n\",\n+        \"* Create a word index and reverse word index (dictionaries mapping from word -> id and id -> word)\\n\",\n+        \"* Pad each sentence to a maximum length\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"kRVATYOgJs1b\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Download the file\\n\",\n+        \"path_to_zip = tf.keras.utils.get_file(\\n\",\n+        \"    'spa-eng.zip', origin='http://download.tensorflow.org/data/spa-eng.zip', \\n\",\n+        \"    extract=True)\\n\",\n+        \"\\n\",\n+        \"path_to_file = os.path.dirname(path_to_zip)+\\\"/spa-eng/spa.txt\\\"\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"DzIS_cRu3jEb\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Converts the unicode file to ascii\\n\",\n+        \"def unicode_to_ascii(s):\\n\",\n+        \"    return ''.join(c for c in unicodedata.normalize('NFD', s)\\n\",\n+        \"        if unicodedata.category(c) != 'Mn')\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"rd0jw-eC3jEh\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"def preprocess_sentence(w):\\n\",\n+        \"    w = unicode_to_ascii(w.lower().strip())\\n\",\n+        \"    \\n\",\n+        \"    # creating a space between a word and the punctuation following it\\n\",\n+        \"    # eg: \\\"he is a boy.\\\" => \\\"he is a boy .\\\" \\n\",\n+        \"    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\\n\",\n+        \"    w = re.sub(r\\\"([?.!,\u00bf])\\\", r\\\" \\\\1 \\\", w)\\n\",\n+        \"    w = re.sub(r'[\\\" \\\"]+', \\\" \\\", w)\\n\",\n+        \"    \\n\",\n+        \"    # replacing everything with space except (a-z, A-Z, \\\".\\\", \\\"?\\\", \\\"!\\\", \\\",\\\")\\n\",\n+        \"    w = re.sub(r\\\"[^a-zA-Z?.!,\u00bf]+\\\", \\\" \\\", w)\\n\",\n+        \"    \\n\",\n+        \"    w = w.rstrip().strip()\\n\",\n+        \"    \\n\",\n+        \"    # adding a start and an end token to the sentence\\n\",\n+        \"    # so that the model know when to start and stop predicting.\\n\",\n+        \"    w = '<start> ' + w + ' <end>'\\n\",\n+        \"    return w\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"OHn4Dct23jEm\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# first we remove the pronumciations\\n\",\n+        \"# second we clean the sentences\\n\",\n+        \"# and third we return word pairs in [ENGLISH, SPANISH] format\\n\",\n+        \"def create_dataset(path, num_examples):\\n\",\n+        \"    lines = open(path, encoding='UTF-8').read().strip().split('\\\\n')\\n\",\n+        \"    \\n\",\n+        \"    word_pairs = [[preprocess_sentence(w) for w in l.split('\\\\t')]  for l in lines[:num_examples]]\\n\",\n+        \"    \\n\",\n+        \"    return word_pairs\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"9xbqO7Iie9bb\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# This class creates a word -> index mapping (e.g,. \\\"dad\\\" -> 5) and vice-versa \\n\",\n+        \"# (e.g., 5 -> \\\"dad\\\") for each language,\\n\",\n+        \"class LanguageIndex():\\n\",\n+        \"  def __init__(self, lang):\\n\",\n+        \"    self.lang = lang\\n\",\n+        \"    self.word2idx = {}\\n\",\n+        \"    self.idx2word = {}\\n\",\n+        \"    self.vocab = set()\\n\",\n+        \"    \\n\",\n+        \"    self.create_index()\\n\",\n+        \"    \\n\",\n+        \"  def create_index(self):\\n\",\n+        \"    for phrase in self.lang:\\n\",\n+        \"      self.vocab.update(phrase.split(' '))\\n\",\n+        \"    \\n\",\n+        \"    self.vocab = sorted(self.vocab)\\n\",\n+        \"\\n\",\n+        \"    for index, word in enumerate(self.vocab):\\n\",\n+        \"      self.word2idx[word] = index\\n\",\n+        \"      self.idx2word[index] = word\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"lU4fj_gG3jE6\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"def max_length(tensor):\\n\",\n+        \"    return max(len(t) for t in tensor)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"eAY9k49G3jE_\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"def load_dataset(path, num_examples):\\n\",\n+        \"    # creating cleaned input, output pairs\\n\",\n+        \"    pairs = create_dataset(path, num_examples)\\n\",\n+        \"\\n\",\n+        \"    # index language using the class defined above    \\n\",\n+        \"    inp_lang = LanguageIndex(sp for en, sp in pairs)\\n\",\n+        \"    targ_lang = LanguageIndex(en for en, sp in pairs)\\n\",\n+        \"    \\n\",\n+        \"    # Vectorize the input and target languages\\n\",\n+        \"    \\n\",\n+        \"    # Spanish sentences\\n\",\n+        \"    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\\n\",\n+        \"    \\n\",\n+        \"    # English sentences\\n\",\n+        \"    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\\n\",\n+        \"    \\n\",\n+        \"    # Calculate max_length of input and output tensor\\n\",\n+        \"    # Here, we'll set those to the longest sentence in the dataset\\n\",\n+        \"    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\\n\",\n+        \"    \\n\",\n+        \"    # Padding the input and output tensor to the maximum length\\n\",\n+        \"    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \\n\",\n+        \"                                                                 maxlen=max_length_inp,\\n\",\n+        \"                                                                 padding='post')\\n\",\n+        \"    \\n\",\n+        \"    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \\n\",\n+        \"                                                                  maxlen=max_length_tar, \\n\",\n+        \"                                                                  padding='post')\\n\",\n+        \"    \\n\",\n+        \"    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"GOi42V79Ydlr\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Limit the size of the dataset to experiment faster (optional)\\n\",\n+        \"\\n\",\n+        \"Training on the complete dataset of >100,000 sentences will take some time. Below, we'll limit the size of the dataset to 30,000 sentences, in order to experiment faster (of course, translation quality will improve with more data).\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"cnxC7q-j3jFD\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Try experimenting with the size of that dataset\\n\",\n+        \"num_examples = 30000\\n\",\n+        \"input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"4QILQkOs3jFG\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"# Creating training and validation sets using an 80-20 split\\n\",\n+        \"input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\\n\",\n+        \"len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"rgCLkfv5uO3d\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Create a tf.data dataset\"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"TqHsArVZ3jFS\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"BUFFER_SIZE = len(input_tensor_train)\\n\",\n+        \"BATCH_SIZE = 64\\n\",\n+        \"embedding_dim = 256\\n\",\n+        \"units = 1024\\n\",\n+        \"vocab_inp_size = len(inp_lang.vocab)\\n\",\n+        \"vocab_tar_size = len(targ_lang.vocab)\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"fYLzjawH3jFW\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\\n\",\n+        \"dataset = dataset.apply(tf.contrib.data.batch_and_drop_remainder(BATCH_SIZE))\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"TNfHIF71ulLu\",\n+        \"colab_type\": \"text\"\n+      },\n+      \"cell_type\": \"markdown\",\n+      \"source\": [\n+        \"## Write the encoder and decoder model with attention\\n\",\n+        \"Here, we'll implement an encoder-deocder model. For background on how these work, you can read more about them in this previous [tutorial](https://www.tensorflow.org/tutorials/seq2seq). In this example, we'll use a more recent (and much easier) set of APIs.\\n\",\n+        \"\\n\",\n+        \"![alt text](https://storage.googleapis.com/yashkatariya/attention_picture.png)\\n\",\n+        \"\\n\",\n+        \"The code below implements the attention [equations](https://www.tensorflow.org/tutorials/seq2seq#background_on_the_attention_mechanism) from the previous tutorial. In the above diagram, each of the input words is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\\n\",\n+        \"\\n\",\n+        \"The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \\n\",\n+        \"\\n\",\n+        \"Here are the equations we'll implement below:\\n\",\n+        \"\\n\",\n+        \"![alt text](https://storage.googleapis.com/yashkatariya/attention_eq1.png)\\n\",\n+        \"![alt text](https://storage.googleapis.com/yashkatariya/attention_eq2.png)\\n\",\n+        \"\\n\",\n+        \"We'll use *Bahdanau attention*. Lets decide on some notations before we write the simplified form:\\n\",\n+        \"\\n\",\n+        \"* FC = Fully connected (dense) layer\\n\",\n+        \"* EO = Encoder output\\n\",\n+        \"* H = hidden state\\n\",\n+        \"* X = input to the decoder\\n\",\n+        \"\\n\",\n+        \"Pseudo-code:\\n\",\n+        \"\\n\",\n+        \"  1. *score = FC(tanh(FC(EO) + FC(H)))*\\n\",\n+        \"  2. *attention weights = softmax(score, axis = 1)*. Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, hidden_size)*. Max_length is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.\\n\",\n+        \"  3. *context vector = sum(attention weights * EO, axis = 1)*. Same reason as above for choosing axis as 1.\\n\",\n+        \"  4. *embedding output = The input to the decoder X is passed through an embedding layer.*\\n\",\n+        \"  5. *merged vector = concat(embedding output, context vector)*\\n\",\n+        \"  6. *This merged vector is then given to the GRU*\\n\",\n+        \"  \\n\",\n+        \"The shapes of all the vectors at each step have been specified in the comments in the code.\\n\",\n+        \"  \\n\",\n+        \" \"\n+      ]\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"nZ2rI24i3jFg\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"class Encoder(tf.keras.Model):\\n\",\n+        \"    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\\n\",\n+        \"        super(Encoder, self).__init__()\\n\",\n+        \"        self.batch_sz = batch_sz\\n\",\n+        \"        self.enc_units = enc_units\\n\",\n+        \"        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\\n\",\n+        \"        \\n\",\n+        \"        # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\\n\",\n+        \"        # the code automatically does that.\\n\",\n+        \"        if tf.test.is_gpu_available():\\n\",\n+        \"          self.gru = tf.keras.layers.CuDNNGRU(self.enc_units, \\n\",\n+        \"                                              return_sequences=True, \\n\",\n+        \"                                              return_state=True, \\n\",\n+        \"                                              recurrent_initializer='glorot_uniform')\\n\",\n+        \"        else:\\n\",\n+        \"          self.gru = tf.keras.layers.GRU(self.enc_units, \\n\",\n+        \"                                         return_sequences=True, \\n\",\n+        \"                                         return_state=True, \\n\",\n+        \"                                         recurrent_activation='sigmoid', \\n\",\n+        \"                                         recurrent_initializer='glorot_uniform')\\n\",\n+        \"\\n\",\n+        \"    def call(self, x, hidden):\\n\",\n+        \"        x = self.embedding(x)\\n\",\n+        \"        output, state = self.gru(x, initial_state = hidden)        \\n\",\n+        \"        return output, state\\n\",\n+        \"    \\n\",\n+        \"    def initialize_hidden_state(self):\\n\",\n+        \"        return tf.zeros((self.batch_sz, self.enc_units))\"\n+      ],\n+      \"execution_count\": 0,\n+      \"outputs\": []\n+    },\n+    {\n+      \"metadata\": {\n+        \"id\": \"yJ_B3mhW3jFk\",\n+        \"colab_type\": \"code\",\n+        \"colab\": {\n+          \"autoexec\": {\n+            \"startup\": false,\n+            \"wait_interval\": 0\n+          }\n+        }\n+      },\n+      \"cell_type\": \"code\",\n+      \"source\": [\n+        \"class Decoder(tf.keras.Model):\\n\",\n+        \"    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\\n\",\n+        \"        super(Decoder, self).__init__()\\n\",\n+        \"        self.batch_sz = batch_sz\\n\",\n+        \"        self.dec_units = dec_units\\n\",\n+        \"        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\\n\",\n+        \"        \\n\",\n+        \"        # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\\n\",\n+        \"        # the code automatically does that.\\n\",\n+        \"        if tf.test.is_gpu_available():\\n\",", "path": "tensorflow/contrib/eager/python/examples/nmt_with_attention/NMT_with_Attention.ipynb", "position": null, "original_position": 541, "commit_id": "ce74f7362ee5161976f7c30777b88637be1d02b5", "original_commit_id": "4e0e0750b0cb6ba922503b8e543c378ea0ee937b", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "body": "Refactor this into a GRU function so you can remove the duplicates?", "created_at": "2018-06-18T16:16:48Z", "updated_at": "2018-06-18T19:59:58Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/20084#discussion_r196139015", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20084", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/196139015"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/20084#discussion_r196139015"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/20084"}}, "body_html": "<p>Refactor this into a GRU function so you can remove the duplicates?</p>", "body_text": "Refactor this into a GRU function so you can remove the duplicates?"}