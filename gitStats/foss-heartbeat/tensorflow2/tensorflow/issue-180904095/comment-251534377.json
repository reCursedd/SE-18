{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251534377", "html_url": "https://github.com/tensorflow/tensorflow/issues/4754#issuecomment-251534377", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4754", "id": 251534377, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTUzNDM3Nw==", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-04T22:39:33Z", "updated_at": "2016-10-04T22:51:25Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6200749\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/hholst80\">@hholst80</a> look forward  to see your progress.<br>\nTo add more info. My program use hinge loss with margin 0.5, before step 18000 the result is fine, and loss is far below 0.5(about 0.138 at step 17000), then at 180000 the train loss will suddenly increase to 0.5(just looks like random guessing), though the scores and loss value with no nan yet. Then after applying gradients the relevant embdding matix values turn to nan. I have also printed the gradient values before applying to embedding, one gradient is all zero, not sure if this cause nan ?<br>\ntf.Tensor 'rnn/gradients/AddN_5:0' shape=(1024, 1024) dtype=float32<br>\n[[ 0.  0.  0. ...,  0.  0.  0.]<br>\n[ 0.  0.  0. ...,  0.  0.  0.]<br>\n[ 0.  0.  0. ...,  0.  0.  0.]<br>\n...,<br>\n[ 0.  0.  0. ...,  0.  0.  0.]<br>\n[ 0.  0.  0. ...,  0.  0.  0.]<br>\n[ 0.  0.  0. ...,  0.  0.  0.]]</p>\n<p>I have also tried to retrain from step 17000.<br>\n2016-10-05 06:50:36 epoch:1.77 train_step:17001 duration:1.044 elapsed:1.044 train_avg_metrics:['loss:0.139']  ['loss:0.139']<br>\n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 train_avg_loss:['loss:0.139']<br>\n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 duration:0.002 elapsed:1.045 ratio:0.001<br>\n2016-10-05 06:50:40 epoch:1.77 train_step:17002 duration:0.512 elapsed:4.025 train_avg_metrics:['loss:0.549']  ['loss:0.549']</p>\n<p>after training step the loss increase to 0.5, and the gradients at 170000 also has all zero gradient.</p>", "body_text": "@hholst80 look forward  to see your progress.\nTo add more info. My program use hinge loss with margin 0.5, before step 18000 the result is fine, and loss is far below 0.5(about 0.138 at step 17000), then at 180000 the train loss will suddenly increase to 0.5(just looks like random guessing), though the scores and loss value with no nan yet. Then after applying gradients the relevant embdding matix values turn to nan. I have also printed the gradient values before applying to embedding, one gradient is all zero, not sure if this cause nan ?\ntf.Tensor 'rnn/gradients/AddN_5:0' shape=(1024, 1024) dtype=float32\n[[ 0.  0.  0. ...,  0.  0.  0.]\n[ 0.  0.  0. ...,  0.  0.  0.]\n[ 0.  0.  0. ...,  0.  0.  0.]\n...,\n[ 0.  0.  0. ...,  0.  0.  0.]\n[ 0.  0.  0. ...,  0.  0.  0.]\n[ 0.  0.  0. ...,  0.  0.  0.]]\nI have also tried to retrain from step 17000.\n2016-10-05 06:50:36 epoch:1.77 train_step:17001 duration:1.044 elapsed:1.044 train_avg_metrics:['loss:0.139']  ['loss:0.139']\n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 train_avg_loss:['loss:0.139']\n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 duration:0.002 elapsed:1.045 ratio:0.001\n2016-10-05 06:50:40 epoch:1.77 train_step:17002 duration:0.512 elapsed:4.025 train_avg_metrics:['loss:0.549']  ['loss:0.549']\nafter training step the loss increase to 0.5, and the gradients at 170000 also has all zero gradient.", "body": "@hholst80 look forward  to see your progress.\nTo add more info. My program use hinge loss with margin 0.5, before step 18000 the result is fine, and loss is far below 0.5(about 0.138 at step 17000), then at 180000 the train loss will suddenly increase to 0.5(just looks like random guessing), though the scores and loss value with no nan yet. Then after applying gradients the relevant embdding matix values turn to nan. I have also printed the gradient values before applying to embedding, one gradient is all zero, not sure if this cause nan ? \ntf.Tensor 'rnn/gradients/AddN_5:0' shape=(1024, 1024) dtype=float32\n[[ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n ..., \n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]\n [ 0.  0.  0. ...,  0.  0.  0.]]\n\nI have also tried to retrain from step 17000.\n2016-10-05 06:50:36 epoch:1.77 train_step:17001 duration:1.044 elapsed:1.044 train_avg_metrics:['loss:0.139']  ['loss:0.139']\n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 train_avg_loss:['loss:0.139'] \n2016-10-05 06:50:36 epoch:1.77 eval_step: 17001 duration:0.002 elapsed:1.045 ratio:0.001\n2016-10-05 06:50:40 epoch:1.77 train_step:17002 duration:0.512 elapsed:4.025 train_avg_metrics:['loss:0.549']  ['loss:0.549']\n\nafter training step the loss increase to 0.5, and the gradients at 170000 also has all zero gradient.\n"}