{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/229508899", "pull_request_review_id": 170027713, "id": 229508899, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyOTUwODg5OQ==", "diff_hunk": "@@ -398,8 +398,13 @@ Status GrpcServer::Stop() {\n       state_ = STOPPED;\n       return Status::OK();\n     case STARTED:\n-      return errors::Unimplemented(\n-          \"Clean shutdown is not currently implemented\");\n+      server_->Shutdown();\n+      master_service_->Shutdown();\n+      worker_service_->Shutdown();\n+      eager_service_->Shutdown();\n+      state_ = STOPPED;\n+      LOG(INFO) << \"Server stopped (target: \" << target() << \")\";\n+      return Status::OK();", "path": "tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc", "position": 12, "original_position": 12, "commit_id": "56576ad348dfb5a858864e95e87434e514693c81", "original_commit_id": "d7d33062ad24afa008962559c834a8b5eda83612", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "body": "Cleanly shutting down running servers is tricky, due to the work-in-flight problem.  One way might be to enter a 'lame duck' mode in which the server refuses to accept new requests but keeps processing existing requests until either a timeout elapses or a special control request is made to transition from lame-duck to shutdown.   Getting this to work requires being able to find and cancel outstanding RPCs and internal requests so everything cleans up correctly.   What are your requirements?", "created_at": "2018-10-30T22:27:07Z", "updated_at": "2018-10-30T22:27:07Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/23190#discussion_r229508899", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23190", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/229508899"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/23190#discussion_r229508899"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23190"}}, "body_html": "<p>Cleanly shutting down running servers is tricky, due to the work-in-flight problem.  One way might be to enter a 'lame duck' mode in which the server refuses to accept new requests but keeps processing existing requests until either a timeout elapses or a special control request is made to transition from lame-duck to shutdown.   Getting this to work requires being able to find and cancel outstanding RPCs and internal requests so everything cleans up correctly.   What are your requirements?</p>", "body_text": "Cleanly shutting down running servers is tricky, due to the work-in-flight problem.  One way might be to enter a 'lame duck' mode in which the server refuses to accept new requests but keeps processing existing requests until either a timeout elapses or a special control request is made to transition from lame-duck to shutdown.   Getting this to work requires being able to find and cancel outstanding RPCs and internal requests so everything cleans up correctly.   What are your requirements?", "in_reply_to_id": 227438700}