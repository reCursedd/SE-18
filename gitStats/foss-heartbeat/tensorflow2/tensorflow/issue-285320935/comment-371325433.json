{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/371325433", "html_url": "https://github.com/tensorflow/tensorflow/issues/15773#issuecomment-371325433", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15773", "id": 371325433, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MTMyNTQzMw==", "user": {"login": "developer-mayuan", "id": 13360081, "node_id": "MDQ6VXNlcjEzMzYwMDgx", "avatar_url": "https://avatars2.githubusercontent.com/u/13360081?v=4", "gravatar_id": "", "url": "https://api.github.com/users/developer-mayuan", "html_url": "https://github.com/developer-mayuan", "followers_url": "https://api.github.com/users/developer-mayuan/followers", "following_url": "https://api.github.com/users/developer-mayuan/following{/other_user}", "gists_url": "https://api.github.com/users/developer-mayuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/developer-mayuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/developer-mayuan/subscriptions", "organizations_url": "https://api.github.com/users/developer-mayuan/orgs", "repos_url": "https://api.github.com/users/developer-mayuan/repos", "events_url": "https://api.github.com/users/developer-mayuan/events{/privacy}", "received_events_url": "https://api.github.com/users/developer-mayuan/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-07T23:41:46Z", "updated_at": "2018-03-07T23:41:46Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7075077\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mdfry\">@mdfry</a> I didn't do the optimization on the total loss for all variables, instead I doing the optimization separately for each loss.</p>\n<pre><code># extract trainable variables\nres_variables = tf.get_collection(\n    tf.GraphKeys.GLOBAL_VARIABLESscope = 'resnet')\nfc_yaw_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                     scope='FC/yaw')\nfc_pitch_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                       scope='FC/pitch')\nfc_roll_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                      scope='FC/roll')\n\nfc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate * 5)\nres_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n\n# Batch norm requires update_ops to be added as a train_op dependency.\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    fc_yaw_train_op = fc_optimizer.minimize(yaw_total_loss,\n                                            var_list=fc_yaw_variables)\n    fc_pitch_train_op = fc_optimizer.minimize(pitch_total_loss,\n                                              var_list=fc_pitch_variables)\n    fc_roll_train_op = fc_optimizer.minimize(roll_total_loss,\n                                             var_list=fc_roll_variables)\n    res_train_op = res_optimizer.minimize(total_loss, global_step,\n                                          var_list=res_variables)\n</code></pre>", "body_text": "@mdfry I didn't do the optimization on the total loss for all variables, instead I doing the optimization separately for each loss.\n# extract trainable variables\nres_variables = tf.get_collection(\n    tf.GraphKeys.GLOBAL_VARIABLESscope = 'resnet')\nfc_yaw_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                     scope='FC/yaw')\nfc_pitch_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                       scope='FC/pitch')\nfc_roll_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n                                      scope='FC/roll')\n\nfc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate * 5)\nres_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n\n# Batch norm requires update_ops to be added as a train_op dependency.\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    fc_yaw_train_op = fc_optimizer.minimize(yaw_total_loss,\n                                            var_list=fc_yaw_variables)\n    fc_pitch_train_op = fc_optimizer.minimize(pitch_total_loss,\n                                              var_list=fc_pitch_variables)\n    fc_roll_train_op = fc_optimizer.minimize(roll_total_loss,\n                                             var_list=fc_roll_variables)\n    res_train_op = res_optimizer.minimize(total_loss, global_step,\n                                          var_list=res_variables)", "body": "@mdfry I didn't do the optimization on the total loss for all variables, instead I doing the optimization separately for each loss.\r\n\r\n```\r\n# extract trainable variables\r\nres_variables = tf.get_collection(\r\n    tf.GraphKeys.GLOBAL_VARIABLESscope = 'resnet')\r\nfc_yaw_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                     scope='FC/yaw')\r\nfc_pitch_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                       scope='FC/pitch')\r\nfc_roll_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\r\n                                      scope='FC/roll')\r\n\r\nfc_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate * 5)\r\nres_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\r\n\r\n# Batch norm requires update_ops to be added as a train_op dependency.\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    fc_yaw_train_op = fc_optimizer.minimize(yaw_total_loss,\r\n                                            var_list=fc_yaw_variables)\r\n    fc_pitch_train_op = fc_optimizer.minimize(pitch_total_loss,\r\n                                              var_list=fc_pitch_variables)\r\n    fc_roll_train_op = fc_optimizer.minimize(roll_total_loss,\r\n                                             var_list=fc_roll_variables)\r\n    res_train_op = res_optimizer.minimize(total_loss, global_step,\r\n                                          var_list=res_variables)\r\n``` "}