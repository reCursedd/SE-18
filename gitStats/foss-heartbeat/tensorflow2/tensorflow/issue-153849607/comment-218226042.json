{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/218226042", "html_url": "https://github.com/tensorflow/tensorflow/issues/2296#issuecomment-218226042", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2296", "id": 218226042, "node_id": "MDEyOklzc3VlQ29tbWVudDIxODIyNjA0Mg==", "user": {"login": "oweingrod", "id": 4645780, "node_id": "MDQ6VXNlcjQ2NDU3ODA=", "avatar_url": "https://avatars1.githubusercontent.com/u/4645780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oweingrod", "html_url": "https://github.com/oweingrod", "followers_url": "https://api.github.com/users/oweingrod/followers", "following_url": "https://api.github.com/users/oweingrod/following{/other_user}", "gists_url": "https://api.github.com/users/oweingrod/gists{/gist_id}", "starred_url": "https://api.github.com/users/oweingrod/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oweingrod/subscriptions", "organizations_url": "https://api.github.com/users/oweingrod/orgs", "repos_url": "https://api.github.com/users/oweingrod/repos", "events_url": "https://api.github.com/users/oweingrod/events{/privacy}", "received_events_url": "https://api.github.com/users/oweingrod/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-10T17:12:18Z", "updated_at": "2016-05-10T17:23:54Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> new information! I tried catching the error and continuing after skipping the image file in question.</p>\n<p><code>Creating bottleneck at ~/tmp/bottleneck/832764-001/3-output00000727.jpg.txt E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 190.44M (199688192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY</code></p>\n<p>So, looks like it was a memory issue causing the ValueError. How can I free up more GPU memory during the process? I have ~4GB.</p>\n<p>Should note this was after creating ~400k bottleneck caches, and with no other processes using GPU memory on the machine.</p>\n<p>EDIT: After leaving the crashed and out of memory Tensorflow process running, I saw this new error:</p>\n<p>EDIT 2: Well, that actually was a memory error. Clearing the memory and restarting seemed to fix, for now.</p>", "body_text": "@yaroslavvb new information! I tried catching the error and continuing after skipping the image file in question.\nCreating bottleneck at ~/tmp/bottleneck/832764-001/3-output00000727.jpg.txt E tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 190.44M (199688192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nSo, looks like it was a memory issue causing the ValueError. How can I free up more GPU memory during the process? I have ~4GB.\nShould note this was after creating ~400k bottleneck caches, and with no other processes using GPU memory on the machine.\nEDIT: After leaving the crashed and out of memory Tensorflow process running, I saw this new error:\nEDIT 2: Well, that actually was a memory error. Clearing the memory and restarting seemed to fix, for now.", "body": "@yaroslavvb new information! I tried catching the error and continuing after skipping the image file in question.\n\n`Creating bottleneck at ~/tmp/bottleneck/832764-001/3-output00000727.jpg.txt\nE tensorflow/stream_executor/cuda/cuda_driver.cc:932] failed to allocate 190.44M (199688192 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY`\n\nSo, looks like it was a memory issue causing the ValueError. How can I free up more GPU memory during the process? I have ~4GB.\n\nShould note this was after creating ~400k bottleneck caches, and with no other processes using GPU memory on the machine.\n\nEDIT: After leaving the crashed and out of memory Tensorflow process running, I saw this new error:\n\nEDIT 2: Well, that actually was a memory error. Clearing the memory and restarting seemed to fix, for now.\n"}