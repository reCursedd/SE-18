{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217987695", "html_url": "https://github.com/tensorflow/tensorflow/issues/2296#issuecomment-217987695", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2296", "id": 217987695, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzk4NzY5NQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-09T20:58:29Z", "updated_at": "2016-05-09T20:59:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There shouldn't be a theoretical cap, but there could be practical caps. In my own experiments I saw overfitting happening when using &lt;1M images. However, when increasing to 20M, I didn't see overfitting after hundreds of millions of steps, so I've been using it as personal rule of thumb for desired number of training images. I'm expecting GTX1080 to be 2x faster than TitanX so it's not a great panacea. It should go beyond 2x speedup for float16, but probably will require adjustments to training procedure.</p>", "body_text": "There shouldn't be a theoretical cap, but there could be practical caps. In my own experiments I saw overfitting happening when using <1M images. However, when increasing to 20M, I didn't see overfitting after hundreds of millions of steps, so I've been using it as personal rule of thumb for desired number of training images. I'm expecting GTX1080 to be 2x faster than TitanX so it's not a great panacea. It should go beyond 2x speedup for float16, but probably will require adjustments to training procedure.", "body": "There shouldn't be a theoretical cap, but there could be practical caps. In my own experiments I saw overfitting happening when using <1M images. However, when increasing to 20M, I didn't see overfitting after hundreds of millions of steps, so I've been using it as personal rule of thumb for desired number of training images. I'm expecting GTX1080 to be 2x faster than TitanX so it's not a great panacea. It should go beyond 2x speedup for float16, but probably will require adjustments to training procedure. \n"}