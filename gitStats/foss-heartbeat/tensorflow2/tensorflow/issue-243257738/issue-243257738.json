{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11536", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11536/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11536/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11536/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/11536", "id": 243257738, "node_id": "MDU6SXNzdWUyNDMyNTc3Mzg=", "number": 11536, "title": "[Feature Request] Feed tensors to feed_dict", "user": {"login": "AndreiCostinescu", "id": 9751090, "node_id": "MDQ6VXNlcjk3NTEwOTA=", "avatar_url": "https://avatars2.githubusercontent.com/u/9751090?v=4", "gravatar_id": "", "url": "https://api.github.com/users/AndreiCostinescu", "html_url": "https://github.com/AndreiCostinescu", "followers_url": "https://api.github.com/users/AndreiCostinescu/followers", "following_url": "https://api.github.com/users/AndreiCostinescu/following{/other_user}", "gists_url": "https://api.github.com/users/AndreiCostinescu/gists{/gist_id}", "starred_url": "https://api.github.com/users/AndreiCostinescu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/AndreiCostinescu/subscriptions", "organizations_url": "https://api.github.com/users/AndreiCostinescu/orgs", "repos_url": "https://api.github.com/users/AndreiCostinescu/repos", "events_url": "https://api.github.com/users/AndreiCostinescu/events{/privacy}", "received_events_url": "https://api.github.com/users/AndreiCostinescu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-07-16T18:39:26Z", "updated_at": "2017-07-16T21:26:27Z", "closed_at": "2017-07-16T20:53:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I think that a nice step towards \"dynamic computation graphs\" is the possibility to feed tensors to placeholders.</p>\n<p>I am building an application which creates a network.<br>\nI create my training and testing batches with tf.train.batch.<br>\nHowever, the training batches are unlimited (so no epoch limit to the tf.train.slice_input_producer) but I would like to also test during training so to only test one epoch on the testing data.<br>\nFor both the training and testing images, I have a preprocessing pipeline which also uses a computation graph, but is different for testing and for training.</p>\n<p>Now the problem is that I have to create the testing batch every time I want to test (because I want to test for 1 epoch, and I cannot otherwise limit the input producing...).<br>\nSo it would definitely help, to be able to feed the training and testing batch respectively to the network input.<br>\nMy current solution is to create the network every time I make the switch from training to testing with the new network inputs. But this takes quite an (unnecessary) long amount of time...</p>\n<p>Is there perhaps another way to \"link\" two graphs in tensorflow?</p>", "body_text": "I think that a nice step towards \"dynamic computation graphs\" is the possibility to feed tensors to placeholders.\nI am building an application which creates a network.\nI create my training and testing batches with tf.train.batch.\nHowever, the training batches are unlimited (so no epoch limit to the tf.train.slice_input_producer) but I would like to also test during training so to only test one epoch on the testing data.\nFor both the training and testing images, I have a preprocessing pipeline which also uses a computation graph, but is different for testing and for training.\nNow the problem is that I have to create the testing batch every time I want to test (because I want to test for 1 epoch, and I cannot otherwise limit the input producing...).\nSo it would definitely help, to be able to feed the training and testing batch respectively to the network input.\nMy current solution is to create the network every time I make the switch from training to testing with the new network inputs. But this takes quite an (unnecessary) long amount of time...\nIs there perhaps another way to \"link\" two graphs in tensorflow?", "body": "I think that a nice step towards \"dynamic computation graphs\" is the possibility to feed tensors to placeholders.\r\n\r\nI am building an application which creates a network.\r\nI create my training and testing batches with tf.train.batch. \r\nHowever, the training batches are unlimited (so no epoch limit to the tf.train.slice_input_producer) but I would like to also test during training so to only test one epoch on the testing data.\r\nFor both the training and testing images, I have a preprocessing pipeline which also uses a computation graph, but is different for testing and for training.\r\n\r\nNow the problem is that I have to create the testing batch every time I want to test (because I want to test for 1 epoch, and I cannot otherwise limit the input producing...).\r\nSo it would definitely help, to be able to feed the training and testing batch respectively to the network input.\r\nMy current solution is to create the network every time I make the switch from training to testing with the new network inputs. But this takes quite an (unnecessary) long amount of time...\r\n\r\nIs there perhaps another way to \"link\" two graphs in tensorflow?"}