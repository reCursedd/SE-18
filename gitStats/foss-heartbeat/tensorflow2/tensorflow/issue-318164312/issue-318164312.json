{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18906", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18906/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18906/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18906/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18906", "id": 318164312, "node_id": "MDU6SXNzdWUzMTgxNjQzMTI=", "number": 18906, "title": "Planning Ticket  CUDA 9.2 + cuDNN 7.1", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 57, "created_at": "2018-04-26T19:21:39Z", "updated_at": "2018-10-03T23:14:43Z", "closed_at": "2018-07-31T18:40:27Z", "author_association": "MEMBER", "body_html": "<p>Updated 02-JUNE-2018</p>\n<p>I have done some testing with CUDA 9.2/cuDNN 7.1.4/NCCL 2.x with tf_cnn_benchmarks.</p>\n<p>Below you will see that if you upgrade to cuDNN 7.1.4 and a newer device driver you get some pretty nice gains and no need to compile from source as well as using CUDA 9.0.  Compiling from source and using NCCL 2.x (Hierarchal Copy was almost the same) I was able to get 6,800+ 8xV100s ResNet50v1 FP16 with synthetic data and about 6,500+ real data sustained over a full run for under 5 hours.</p>\n<p>I suspect many of you saw NVIDIA announce 1K and 1.3K for 1xV100 ResNet50.  That was with <strong>unreleased</strong> libraries and we are working to ensure we can hit those numbers when the libraries are available or with our own tricks.</p>\n<p>I apologize for any short hand I use below in describing the runs.  I am happy to answer questions or make clarifications.  This testing was slightly informal but recent full scale testing yielded similar results.</p>\n<p><strong>CUDA 9.0</strong><br>\n[Recommended driver] 6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy<br>\n6,620 CUDA 9.0 + 390.59 (v1.8.0-1386-g2dc7575) NCCL<br>\n6,613.11 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) NCCL<br>\n6,564.9 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy<br>\n6,541.25 CUDA 9.0 + 390.59 (1.9.0.dev20180523) hierarchical copy<br>\n6,276.02 CUDA 9.0 + 384.13 (1.9.0.dev20180523) hierarchical copy<br>\n6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy</p>\n<p><strong>CUDA 9.1</strong><br>\n[Recommended driver]  6,227.64 CUDA 9.1 + 390.59 (v1.8.0-2215-gf528eba) hierarchical copy<br>\n6,210.28 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy<br>\n6,118.21 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) NCCL</p>\n<p><strong>CUDA 9.2</strong><br>\n[Recommended driver]  6,696.39 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL<br>\n6,606.57 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy<br>\n6,738.32 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL SGD</p>\n<p><strong>Full test runs with CUDA 9.2</strong><br>\ntop_1 ranges between 75.7% and 76% and does not seem to be based on the hyper parameters.  I was focused on testing NCCL vs. hierarchical copy.  I have a minor concern about my validation command, but this is still good info.<br>\nHierarchical copy:  6441.64  Accuracy @ 1 = 0.7584 Accuracy @ 5 = 0.9267 [49920 examples]<br>\nNCCL (repacking:2):  6490.62  Accuracy @ 1 = 0.7572 Accuracy @ 5 = 0.9265 [49920 examples]<br>\nNCCL (repacking:8):  6490.62  Accuracy @ 1 = 0.7582 Accuracy @ 5 = 0.9268 [49920 examples]<br>\nMy first hierarchical copy run was 76% exactly.</p>\n<p><strong>Note:</strong>  I would like to move to CUDA 9.2 as the default but the driver is not widely available for easy apt-get install.  I am working long-term to get the build team to support a secondary CUDA build.  I am also very open to feedback as I do not see any easy path forward to moving to newer CUDA versions faster.  You can always (usually) install a new cuDNN.  We are compiling with 7.0 but you see gains by installing 7.1.4 as you see above if you upgrade your driver.</p>", "body_text": "Updated 02-JUNE-2018\nI have done some testing with CUDA 9.2/cuDNN 7.1.4/NCCL 2.x with tf_cnn_benchmarks.\nBelow you will see that if you upgrade to cuDNN 7.1.4 and a newer device driver you get some pretty nice gains and no need to compile from source as well as using CUDA 9.0.  Compiling from source and using NCCL 2.x (Hierarchal Copy was almost the same) I was able to get 6,800+ 8xV100s ResNet50v1 FP16 with synthetic data and about 6,500+ real data sustained over a full run for under 5 hours.\nI suspect many of you saw NVIDIA announce 1K and 1.3K for 1xV100 ResNet50.  That was with unreleased libraries and we are working to ensure we can hit those numbers when the libraries are available or with our own tricks.\nI apologize for any short hand I use below in describing the runs.  I am happy to answer questions or make clarifications.  This testing was slightly informal but recent full scale testing yielded similar results.\nCUDA 9.0\n[Recommended driver] 6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy\n6,620 CUDA 9.0 + 390.59 (v1.8.0-1386-g2dc7575) NCCL\n6,613.11 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) NCCL\n6,564.9 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy\n6,541.25 CUDA 9.0 + 390.59 (1.9.0.dev20180523) hierarchical copy\n6,276.02 CUDA 9.0 + 384.13 (1.9.0.dev20180523) hierarchical copy\n6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy\nCUDA 9.1\n[Recommended driver]  6,227.64 CUDA 9.1 + 390.59 (v1.8.0-2215-gf528eba) hierarchical copy\n6,210.28 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy\n6,118.21 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) NCCL\nCUDA 9.2\n[Recommended driver]  6,696.39 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL\n6,606.57 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy\n6,738.32 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL SGD\nFull test runs with CUDA 9.2\ntop_1 ranges between 75.7% and 76% and does not seem to be based on the hyper parameters.  I was focused on testing NCCL vs. hierarchical copy.  I have a minor concern about my validation command, but this is still good info.\nHierarchical copy:  6441.64  Accuracy @ 1 = 0.7584 Accuracy @ 5 = 0.9267 [49920 examples]\nNCCL (repacking:2):  6490.62  Accuracy @ 1 = 0.7572 Accuracy @ 5 = 0.9265 [49920 examples]\nNCCL (repacking:8):  6490.62  Accuracy @ 1 = 0.7582 Accuracy @ 5 = 0.9268 [49920 examples]\nMy first hierarchical copy run was 76% exactly.\nNote:  I would like to move to CUDA 9.2 as the default but the driver is not widely available for easy apt-get install.  I am working long-term to get the build team to support a secondary CUDA build.  I am also very open to feedback as I do not see any easy path forward to moving to newer CUDA versions faster.  You can always (usually) install a new cuDNN.  We are compiling with 7.0 but you see gains by installing 7.1.4 as you see above if you upgrade your driver.", "body": "Updated 02-JUNE-2018\r\n\r\nI have done some testing with CUDA 9.2/cuDNN 7.1.4/NCCL 2.x with tf_cnn_benchmarks.\r\n\r\nBelow you will see that if you upgrade to cuDNN 7.1.4 and a newer device driver you get some pretty nice gains and no need to compile from source as well as using CUDA 9.0.  Compiling from source and using NCCL 2.x (Hierarchal Copy was almost the same) I was able to get 6,800+ 8xV100s ResNet50v1 FP16 with synthetic data and about 6,500+ real data sustained over a full run for under 5 hours.  \r\n\r\nI suspect many of you saw NVIDIA announce 1K and 1.3K for 1xV100 ResNet50.  That was with **unreleased** libraries and we are working to ensure we can hit those numbers when the libraries are available or with our own tricks.\r\n\r\nI apologize for any short hand I use below in describing the runs.  I am happy to answer questions or make clarifications.  This testing was slightly informal but recent full scale testing yielded similar results.  \r\n\r\n**CUDA 9.0**\r\n[Recommended driver] 6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy\r\n6,620 CUDA 9.0 + 390.59 (v1.8.0-1386-g2dc7575) NCCL\r\n6,613.11 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) NCCL\r\n6,564.9 CUDA 9.0 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy\r\n6,541.25 CUDA 9.0 + 390.59 (1.9.0.dev20180523) hierarchical copy\r\n6,276.02 CUDA 9.0 + 384.13 (1.9.0.dev20180523) hierarchical copy\r\n6,197.70 CUDA 9.0 + 384.13 (v1.8.0-1386-g2dc7575) hierarchical copy\r\n\r\n**CUDA 9.1**\r\n[Recommended driver]  6,227.64 CUDA 9.1 + 390.59 (v1.8.0-2215-gf528eba) hierarchical copy\r\n6,210.28 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy\r\n6,118.21 CUDA 9.1 + 396.26 (v1.8.0-2215-gf528eba) NCCL\r\n\r\n**CUDA 9.2**\r\n[Recommended driver]  6,696.39 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL\r\n6,606.57 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) hierarchical copy\r\n6,738.32 CUDA 9.2 + 396.26 (v1.8.0-2215-gf528eba) NCCL SGD\r\n\r\n**Full test runs with CUDA 9.2**\r\ntop_1 ranges between 75.7% and 76% and does not seem to be based on the hyper parameters.  I was focused on testing NCCL vs. hierarchical copy.  I have a minor concern about my validation command, but this is still good info.\r\nHierarchical copy:  6441.64  Accuracy @ 1 = 0.7584 Accuracy @ 5 = 0.9267 [49920 examples] \r\nNCCL (repacking:2):  6490.62  Accuracy @ 1 = 0.7572 Accuracy @ 5 = 0.9265 [49920 examples]\r\nNCCL (repacking:8):  6490.62  Accuracy @ 1 = 0.7582 Accuracy @ 5 = 0.9268 [49920 examples]\r\nMy first hierarchical copy run was 76% exactly. \r\n\r\n**Note:**  I would like to move to CUDA 9.2 as the default but the driver is not widely available for easy apt-get install.  I am working long-term to get the build team to support a secondary CUDA build.  I am also very open to feedback as I do not see any easy path forward to moving to newer CUDA versions faster.  You can always (usually) install a new cuDNN.  We are compiling with 7.0 but you see gains by installing 7.1.4 as you see above if you upgrade your driver."}