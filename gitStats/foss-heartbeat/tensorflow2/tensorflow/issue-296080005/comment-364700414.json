{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/364700414", "html_url": "https://github.com/tensorflow/tensorflow/issues/16907#issuecomment-364700414", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16907", "id": 364700414, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NDcwMDQxNA==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-10T22:21:33Z", "updated_at": "2018-02-10T22:21:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1836025\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/joeyearsley\">@joeyearsley</a> raises a good point about the loop structure here, which isn't representative of a real training job. In particular, it builds a huge graph that performs every iteration using a different <code>tf.matmul()</code> node, whereas it is more common to build a graph that works on a single mini-batch and then calls <code>sess.run()</code> multiple times in a loop. The <code>tf.matmul()</code> operations multiply two four-element vectors, which is a computation that is too small to benefit from GPU acceleration: the cost to copy the operands to the GPU would be much larger than the time to multiply them... which could potentially be achieved with a couple of SSE instructions. Finally, the code only measures the first time to execute the graph, which I expect will be dominated by the costs of building the large graph in both cases, and not substantially different based on how many GPUs you are using.</p>\n<p>I'd recommend that you take a look at the <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\"><code>tf_cnn_benchmarks.py</code></a> code for an example of how to write a well-optimized multi-GPU trainer using <code>tf.data</code>. In addition, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=667809\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/karmel\">@karmel</a> has just augmented the official MNIST example with support for <a href=\"https://github.com/tensorflow/models/blob/fd7b6887fb294e90356d5664724083d1f61671ef/official/mnist/mnist.py#L158\">multi-GPU processing</a>, and that example shows some of the easier-to-use library support that has been added for multi-GPU processing. We're continuing to develop this to improve performance and usability together.</p>", "body_text": "@joeyearsley raises a good point about the loop structure here, which isn't representative of a real training job. In particular, it builds a huge graph that performs every iteration using a different tf.matmul() node, whereas it is more common to build a graph that works on a single mini-batch and then calls sess.run() multiple times in a loop. The tf.matmul() operations multiply two four-element vectors, which is a computation that is too small to benefit from GPU acceleration: the cost to copy the operands to the GPU would be much larger than the time to multiply them... which could potentially be achieved with a couple of SSE instructions. Finally, the code only measures the first time to execute the graph, which I expect will be dominated by the costs of building the large graph in both cases, and not substantially different based on how many GPUs you are using.\nI'd recommend that you take a look at the tf_cnn_benchmarks.py code for an example of how to write a well-optimized multi-GPU trainer using tf.data. In addition, @karmel has just augmented the official MNIST example with support for multi-GPU processing, and that example shows some of the easier-to-use library support that has been added for multi-GPU processing. We're continuing to develop this to improve performance and usability together.", "body": "@joeyearsley raises a good point about the loop structure here, which isn't representative of a real training job. In particular, it builds a huge graph that performs every iteration using a different `tf.matmul()` node, whereas it is more common to build a graph that works on a single mini-batch and then calls `sess.run()` multiple times in a loop. The `tf.matmul()` operations multiply two four-element vectors, which is a computation that is too small to benefit from GPU acceleration: the cost to copy the operands to the GPU would be much larger than the time to multiply them... which could potentially be achieved with a couple of SSE instructions. Finally, the code only measures the first time to execute the graph, which I expect will be dominated by the costs of building the large graph in both cases, and not substantially different based on how many GPUs you are using.\r\n\r\nI'd recommend that you take a look at the [`tf_cnn_benchmarks.py`](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py) code for an example of how to write a well-optimized multi-GPU trainer using `tf.data`. In addition, @karmel has just augmented the official MNIST example with support for [multi-GPU processing](https://github.com/tensorflow/models/blob/fd7b6887fb294e90356d5664724083d1f61671ef/official/mnist/mnist.py#L158), and that example shows some of the easier-to-use library support that has been added for multi-GPU processing. We're continuing to develop this to improve performance and usability together."}