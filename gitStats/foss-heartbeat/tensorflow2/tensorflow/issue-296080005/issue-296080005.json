{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16907", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16907/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16907/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16907/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16907", "id": 296080005, "node_id": "MDU6SXNzdWUyOTYwODAwMDU=", "number": 16907, "title": "Multi-GPU could not provide performance improve with dataset API", "user": {"login": "Lancerchiang", "id": 35952525, "node_id": "MDQ6VXNlcjM1OTUyNTI1", "avatar_url": "https://avatars2.githubusercontent.com/u/35952525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Lancerchiang", "html_url": "https://github.com/Lancerchiang", "followers_url": "https://api.github.com/users/Lancerchiang/followers", "following_url": "https://api.github.com/users/Lancerchiang/following{/other_user}", "gists_url": "https://api.github.com/users/Lancerchiang/gists{/gist_id}", "starred_url": "https://api.github.com/users/Lancerchiang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Lancerchiang/subscriptions", "organizations_url": "https://api.github.com/users/Lancerchiang/orgs", "repos_url": "https://api.github.com/users/Lancerchiang/repos", "events_url": "https://api.github.com/users/Lancerchiang/events{/privacy}", "received_events_url": "https://api.github.com/users/Lancerchiang/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2018-02-10T06:59:27Z", "updated_at": "2018-02-19T23:30:45Z", "closed_at": "2018-02-10T22:21:33Z", "author_association": "NONE", "body_html": "<p>I just wrote a small piece of code in tensorflow to test its multi-gpu performance, with dataset API.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\nimport os\n\n#dataset with 1000 vectors\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))\n          \nprint(dataset.output_types)\nprint(dataset.output_shapes)\n\niterator = dataset.make_initializable_iterator()\n#next_element = iterator.get_next()\n\ntensor_results = []\n\n\nfor i in range(500):\n    for j in range(2):\n        with tf.device(\"/gpu:%d\" % j):\n            with tf.name_scope(\"Tower_%d\" % j) as scope:\n                operand = iterator.get_next()\n                tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))\n                tensor_results.append(tensor_result)\n\n\ntfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\ntfconfig.gpu_options.allow_growth=True\n\nsess = tf.Session(config=tfconfig)\n\nsess.run(iterator.initializer)\nt0 = time.time()\n\nresults = sess.run(tensor_results)\n\nt1 = time.time()\n\nelapsed_time = t1 - t0\nprint(elapsed_time)\nresults\n</code></pre>\n<p>I have 2 GPUs and this program takes 0.68 seconds to finish.<br>\nWhen I change to a single GPU execution mode:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport time\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))\n          \nprint(dataset.output_types)\nprint(dataset.output_shapes)\n\niterator = dataset.make_initializable_iterator()\n#next_element = iterator.get_next()\n\ntensor_results = []\n\nwith tf.device(\"/gpu:0\"):\n    for i in range(1000):\n        operand = iterator.get_next()\n        tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))\n        tensor_results.append(tensor_result)\n\n\ntfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\ntfconfig.gpu_options.allow_growth=True\n\nsess = tf.Session(config=tfconfig)\n\nsess.run(iterator.initializer)\nt0 = time.time()\n\nresults = sess.run(tensor_results)\n\nt1 = time.time()\n\nelapsed_time = t1 - t0\nprint(elapsed_time)\nresults\n</code></pre>\n<p>It takes the same time to finish (actually single GPU is even faster perhaps due to overhead reasons). Do anyone know where does the problem come from?</p>\n<p>python version: Python 2.7.12<br>\ntensorflow version: 1.4.0<br>\nCUDA version: 8.0<br>\nUbuntu version: Ubuntu 16.04 LTS</p>\n<p>Thanks!</p>", "body_text": "I just wrote a small piece of code in tensorflow to test its multi-gpu performance, with dataset API.\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport os\n\n#dataset with 1000 vectors\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))\n          \nprint(dataset.output_types)\nprint(dataset.output_shapes)\n\niterator = dataset.make_initializable_iterator()\n#next_element = iterator.get_next()\n\ntensor_results = []\n\n\nfor i in range(500):\n    for j in range(2):\n        with tf.device(\"/gpu:%d\" % j):\n            with tf.name_scope(\"Tower_%d\" % j) as scope:\n                operand = iterator.get_next()\n                tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))\n                tensor_results.append(tensor_result)\n\n\ntfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\ntfconfig.gpu_options.allow_growth=True\n\nsess = tf.Session(config=tfconfig)\n\nsess.run(iterator.initializer)\nt0 = time.time()\n\nresults = sess.run(tensor_results)\n\nt1 = time.time()\n\nelapsed_time = t1 - t0\nprint(elapsed_time)\nresults\n\nI have 2 GPUs and this program takes 0.68 seconds to finish.\nWhen I change to a single GPU execution mode:\nimport tensorflow as tf\nimport numpy as np\nimport time\nimport os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))\n          \nprint(dataset.output_types)\nprint(dataset.output_shapes)\n\niterator = dataset.make_initializable_iterator()\n#next_element = iterator.get_next()\n\ntensor_results = []\n\nwith tf.device(\"/gpu:0\"):\n    for i in range(1000):\n        operand = iterator.get_next()\n        tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))\n        tensor_results.append(tensor_result)\n\n\ntfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\ntfconfig.gpu_options.allow_growth=True\n\nsess = tf.Session(config=tfconfig)\n\nsess.run(iterator.initializer)\nt0 = time.time()\n\nresults = sess.run(tensor_results)\n\nt1 = time.time()\n\nelapsed_time = t1 - t0\nprint(elapsed_time)\nresults\n\nIt takes the same time to finish (actually single GPU is even faster perhaps due to overhead reasons). Do anyone know where does the problem come from?\npython version: Python 2.7.12\ntensorflow version: 1.4.0\nCUDA version: 8.0\nUbuntu version: Ubuntu 16.04 LTS\nThanks!", "body": "I just wrote a small piece of code in tensorflow to test its multi-gpu performance, with dataset API.\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\n#dataset with 1000 vectors\r\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))\r\n          \r\nprint(dataset.output_types)\r\nprint(dataset.output_shapes)\r\n\r\niterator = dataset.make_initializable_iterator()\r\n#next_element = iterator.get_next()\r\n\r\ntensor_results = []\r\n\r\n\r\nfor i in range(500):\r\n    for j in range(2):\r\n        with tf.device(\"/gpu:%d\" % j):\r\n            with tf.name_scope(\"Tower_%d\" % j) as scope:\r\n                operand = iterator.get_next()\r\n                tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))\r\n                tensor_results.append(tensor_result)\r\n\r\n\r\ntfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\r\ntfconfig.gpu_options.allow_growth=True\r\n\r\nsess = tf.Session(config=tfconfig)\r\n\r\nsess.run(iterator.initializer)\r\nt0 = time.time()\r\n\r\nresults = sess.run(tensor_results)\r\n\r\nt1 = time.time()\r\n\r\nelapsed_time = t1 - t0\r\nprint(elapsed_time)\r\nresults\r\n```\r\nI have 2 GPUs and this program takes 0.68 seconds to finish.\r\nWhen I change to a single GPU execution mode:\r\n\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport time\r\nimport os\r\n\r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\r\n\r\ndataset = tf.data.Dataset.from_tensor_slices(tf.random_uniform([1000,4], maxval=4, dtype=tf.int32))\r\n          \r\nprint(dataset.output_types)\r\nprint(dataset.output_shapes)\r\n\r\niterator = dataset.make_initializable_iterator()\r\n#next_element = iterator.get_next()\r\n\r\ntensor_results = []\r\n\r\nwith tf.device(\"/gpu:0\"):\r\n    for i in range(1000):\r\n        operand = iterator.get_next()\r\n        tensor_result = tf.matmul(tf.reshape(operand,shape=[1,4]), tf.reshape(operand,shape=[4,1]))\r\n        tensor_results.append(tensor_result)\r\n\r\n\r\ntfconfig = tf.ConfigProto(allow_soft_placement=True, log_device_placement=True)\r\ntfconfig.gpu_options.allow_growth=True\r\n\r\nsess = tf.Session(config=tfconfig)\r\n\r\nsess.run(iterator.initializer)\r\nt0 = time.time()\r\n\r\nresults = sess.run(tensor_results)\r\n\r\nt1 = time.time()\r\n\r\nelapsed_time = t1 - t0\r\nprint(elapsed_time)\r\nresults\r\n```\r\n\r\nIt takes the same time to finish (actually single GPU is even faster perhaps due to overhead reasons). Do anyone know where does the problem come from?\r\n\r\npython version: Python 2.7.12\r\ntensorflow version: 1.4.0\r\nCUDA version: 8.0\r\nUbuntu version: Ubuntu 16.04 LTS\r\n\r\nThanks!\r\n"}