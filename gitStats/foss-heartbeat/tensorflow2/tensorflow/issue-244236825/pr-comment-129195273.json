{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/129195273", "pull_request_review_id": 51940398, "id": 129195273, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEyOTE5NTI3Mw==", "diff_hunk": "@@ -54,6 +54,74 @@ __device__ __forceinline__ void AccumulateInto(\n   CudaAtomicAdd(dest_scalar + 1, value.imag());\n }\n \n+// SortedSegmentSumFunctor kernel reduces input data just as\n+// UnsortedSegmentSumCustomKernel does except that input data\n+// is partitioned along the outer reduction dimension. This is\n+// because consecutive rows (elements in a row share the same\n+// outer dimension index) in the flattened 2D input data likely\n+// belong to the same segment in sorted segment sum operation.\n+// Therefore such partitioning strategy has two advantages over\n+// the UnsortedSegmentSumFunctor kernel:\n+// 1. Each thread reduces across multiple rows before writing\n+// answers to the global memory, we can therefore\n+// write reduction results to global memory less often.\n+// 2. We may know that the current thread is the only contributor\n+// to an output element because of the increasing nature of segment\n+// ids. In such cases, we do not need to use atomic operations\n+// to write results to global memory.\n+// In the flattened view of input data (with only outer and inner\n+// dimension), every thread processes a strip of input data of\n+// size OUTER_DIM_TILE_SIZE x 1. This strip runs across multiple\n+// rows of input data and all reduction elements share one inner\n+// dimension index.\n+#define OUTER_DIM_TILE_SIZE 8\n+#define CEIL_DIV(x, y) (1 + (((x)-1) / (y)))", "path": "tensorflow/core/kernels/segment_reduction_ops_gpu.cu.cc", "position": null, "original_position": 25, "commit_id": "50517c902261289b8163ded0979d5ac28bfe9c6d", "original_commit_id": "aba87c28d57d9fd16aacce476b405ebeed50263e", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "body": "This produces the wrong answer when x == 0. The typical way is to write (x + y - 1) / y. \r\n\r\nIn TF, the convention is to use Eigen::divup", "created_at": "2017-07-25T01:41:14Z", "updated_at": "2017-08-29T00:31:24Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11630#discussion_r129195273", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11630", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/129195273"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11630#discussion_r129195273"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11630"}}, "body_html": "<p>This produces the wrong answer when x == 0. The typical way is to write (x + y - 1) / y.</p>\n<p>In TF, the convention is to use Eigen::divup</p>", "body_text": "This produces the wrong answer when x == 0. The typical way is to write (x + y - 1) / y.\nIn TF, the convention is to use Eigen::divup"}