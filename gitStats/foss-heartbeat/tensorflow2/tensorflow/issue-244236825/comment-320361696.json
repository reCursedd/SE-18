{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/320361696", "html_url": "https://github.com/tensorflow/tensorflow/pull/11630#issuecomment-320361696", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11630", "id": 320361696, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMDM2MTY5Ng==", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-04T21:54:41Z", "updated_at": "2017-08-04T22:12:30Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I have updated the pertinent code. Performance tests with more aspect ratios are conducted and recorded in this <a href=\"https://gist.github.com/tjingrant/3113562d6e9fc8738487ca91847ffd16\">gist</a> including the python code used in this process. The results of both segment_sum and unsorted_segment_sum are always compared for consistency and therefore this also serves as a correctness test for potential race conditions.</p>\n<p>In general, the execution time of sorted segment sum cuda kernel is uniformly shorter than that of unsorted segment sum cuda kernel when the problem size is medium or large. It is clear that tiling comes with an inherent cost and therefore when the problem size is small, the performance penalty imposed by tiling will be more noticeable but hopefully nevertheless small. Of all the problem sizes tested (small and large problem sizes alike), the average speed up factor is 1.17 for fp32 and 1.38 for fp64. In general, for fp32 typed segment summation, the break even point is around 512x512 and for any problem sizes beyond this point, using sorted segment sum will be profitable. For fp64, this point is very low and we actually did not see executions of any problem sizes during which sorted segment sum is slower.</p>\n<p>It is also worth noting that sorted segment sum requires a mem copy in order to retrieve the output size (a scalar) from device memory. Therefore the overall break even point is somewhat higher but since this cost is constant, it will not impact the execution time of kernels doing more significant work. Please let me know if there is any way to mitigate this problem. Specifically,</p>\n<ul>\n<li>is it possible to access the input array on the host before it is moved to the device?</li>\n<li>Or slightly worse, is it possible to have duplicate copies of the input array at both host and device memory?</li>\n</ul>", "body_text": "I have updated the pertinent code. Performance tests with more aspect ratios are conducted and recorded in this gist including the python code used in this process. The results of both segment_sum and unsorted_segment_sum are always compared for consistency and therefore this also serves as a correctness test for potential race conditions.\nIn general, the execution time of sorted segment sum cuda kernel is uniformly shorter than that of unsorted segment sum cuda kernel when the problem size is medium or large. It is clear that tiling comes with an inherent cost and therefore when the problem size is small, the performance penalty imposed by tiling will be more noticeable but hopefully nevertheless small. Of all the problem sizes tested (small and large problem sizes alike), the average speed up factor is 1.17 for fp32 and 1.38 for fp64. In general, for fp32 typed segment summation, the break even point is around 512x512 and for any problem sizes beyond this point, using sorted segment sum will be profitable. For fp64, this point is very low and we actually did not see executions of any problem sizes during which sorted segment sum is slower.\nIt is also worth noting that sorted segment sum requires a mem copy in order to retrieve the output size (a scalar) from device memory. Therefore the overall break even point is somewhat higher but since this cost is constant, it will not impact the execution time of kernels doing more significant work. Please let me know if there is any way to mitigate this problem. Specifically,\n\nis it possible to access the input array on the host before it is moved to the device?\nOr slightly worse, is it possible to have duplicate copies of the input array at both host and device memory?", "body": "I have updated the pertinent code. Performance tests with more aspect ratios are conducted and recorded in this [gist](https://gist.github.com/tjingrant/3113562d6e9fc8738487ca91847ffd16) including the python code used in this process. The results of both segment_sum and unsorted_segment_sum are always compared for consistency and therefore this also serves as a correctness test for potential race conditions. \r\n\r\nIn general, the execution time of sorted segment sum cuda kernel is uniformly shorter than that of unsorted segment sum cuda kernel when the problem size is medium or large. It is clear that tiling comes with an inherent cost and therefore when the problem size is small, the performance penalty imposed by tiling will be more noticeable but hopefully nevertheless small. Of all the problem sizes tested (small and large problem sizes alike), the average speed up factor is 1.17 for fp32 and 1.38 for fp64. In general, for fp32 typed segment summation, the break even point is around 512x512 and for any problem sizes beyond this point, using sorted segment sum will be profitable. For fp64, this point is very low and we actually did not see executions of any problem sizes during which sorted segment sum is slower.\r\n\r\nIt is also worth noting that sorted segment sum requires a mem copy in order to retrieve the output size (a scalar) from device memory. Therefore the overall break even point is somewhat higher but since this cost is constant, it will not impact the execution time of kernels doing more significant work. Please let me know if there is any way to mitigate this problem. Specifically, \r\n\r\n- is it possible to access the input array on the host before it is moved to the device? \r\n- Or slightly worse, is it possible to have duplicate copies of the input array at both host and device memory?\r\n\r\n"}