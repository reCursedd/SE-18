{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/294014551", "html_url": "https://github.com/tensorflow/tensorflow/pull/9175#issuecomment-294014551", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9175", "id": 294014551, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDAxNDU1MQ==", "user": {"login": "louiehelm", "id": 381635, "node_id": "MDQ6VXNlcjM4MTYzNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/381635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/louiehelm", "html_url": "https://github.com/louiehelm", "followers_url": "https://api.github.com/users/louiehelm/followers", "following_url": "https://api.github.com/users/louiehelm/following{/other_user}", "gists_url": "https://api.github.com/users/louiehelm/gists{/gist_id}", "starred_url": "https://api.github.com/users/louiehelm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/louiehelm/subscriptions", "organizations_url": "https://api.github.com/users/louiehelm/orgs", "repos_url": "https://api.github.com/users/louiehelm/repos", "events_url": "https://api.github.com/users/louiehelm/events{/privacy}", "received_events_url": "https://api.github.com/users/louiehelm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-13T20:34:54Z", "updated_at": "2017-04-13T20:34:54Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20959853\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/drpngx\">@drpngx</a> I agree <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8077637\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jitindua\">@jitindua</a>'s code looks more compact and has testing and maintenance advantages.</p>\n<p>One of my calculations may be more efficient in the CPU version of Nadam because of how I'm using Eigen. Perhaps <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8077637\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jitindua\">@jitindua</a> could try to adapt this efficiency if you're gonna go with his PR? See code near <a href=\"https://github.com/tensorflow/tensorflow/pull/8405/files#diff-fee33779233fb6ff133bbb93c7830880R301\">https://github.com/tensorflow/tensorflow/pull/8405/files#diff-fee33779233fb6ff133bbb93c7830880R301</a></p>\n<p>One of the subtle / critical issues I was fixing with my Nadam interface (which this can't fix) was setting saner default decay factors.</p>\n<p>Kingma &amp; Ba's paper on Adam only sets beta1=0.9 and beta2=0.999 to promote their pet initialization scheme (that no one uses). And if you don't use it, those factors are absurdly large + bad. I know in theory that developers could set good decay factors on their own. But in practice, 100 out of 100 developers incorrectly compensate for Adam's poorly set default decay factors by setting depressingly low learning rates to prevent it from diverging instantly.</p>\n<p>In some sense, Radam itself is a research \"discovery\" about how wrong the most widely-used learning rate / decay factors in Adam are. Its gamma factor is just a more convenient knob to help change beta1 and beta2 in the correct proportions so you can properly decrease them in tandem.</p>\n<p>How about you pull this PR, and I'll submit two thin, python-only, init-only front-ends into core (like my current Radam one) so that there's an explicit Radam call and a Nadam optimizer that doesn't have unconscionably bad default decay factors?</p>", "body_text": "@lukaszkaiser @drpngx I agree @jitindua's code looks more compact and has testing and maintenance advantages.\nOne of my calculations may be more efficient in the CPU version of Nadam because of how I'm using Eigen. Perhaps @jitindua could try to adapt this efficiency if you're gonna go with his PR? See code near https://github.com/tensorflow/tensorflow/pull/8405/files#diff-fee33779233fb6ff133bbb93c7830880R301\nOne of the subtle / critical issues I was fixing with my Nadam interface (which this can't fix) was setting saner default decay factors.\nKingma & Ba's paper on Adam only sets beta1=0.9 and beta2=0.999 to promote their pet initialization scheme (that no one uses). And if you don't use it, those factors are absurdly large + bad. I know in theory that developers could set good decay factors on their own. But in practice, 100 out of 100 developers incorrectly compensate for Adam's poorly set default decay factors by setting depressingly low learning rates to prevent it from diverging instantly.\nIn some sense, Radam itself is a research \"discovery\" about how wrong the most widely-used learning rate / decay factors in Adam are. Its gamma factor is just a more convenient knob to help change beta1 and beta2 in the correct proportions so you can properly decrease them in tandem.\nHow about you pull this PR, and I'll submit two thin, python-only, init-only front-ends into core (like my current Radam one) so that there's an explicit Radam call and a Nadam optimizer that doesn't have unconscionably bad default decay factors?", "body": "@lukaszkaiser @drpngx I agree @jitindua's code looks more compact and has testing and maintenance advantages.\r\n\r\nOne of my calculations may be more efficient in the CPU version of Nadam because of how I'm using Eigen. Perhaps @jitindua could try to adapt this efficiency if you're gonna go with his PR? See code near https://github.com/tensorflow/tensorflow/pull/8405/files#diff-fee33779233fb6ff133bbb93c7830880R301\r\n\r\nOne of the subtle / critical issues I was fixing with my Nadam interface (which this can't fix) was setting saner default decay factors.\r\n\r\nKingma & Ba's paper on Adam only sets beta1=0.9 and beta2=0.999 to promote their pet initialization scheme (that no one uses). And if you don't use it, those factors are absurdly large + bad. I know in theory that developers could set good decay factors on their own. But in practice, 100 out of 100 developers incorrectly compensate for Adam's poorly set default decay factors by setting depressingly low learning rates to prevent it from diverging instantly.\r\n\r\nIn some sense, Radam itself is a research \"discovery\" about how wrong the most widely-used learning rate / decay factors in Adam are. Its gamma factor is just a more convenient knob to help change beta1 and beta2 in the correct proportions so you can properly decrease them in tandem.\r\n\r\nHow about you pull this PR, and I'll submit two thin, python-only, init-only front-ends into core (like my current Radam one) so that there's an explicit Radam call and a Nadam optimizer that doesn't have unconscionably bad default decay factors? "}