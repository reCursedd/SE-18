{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/295395355", "html_url": "https://github.com/tensorflow/tensorflow/pull/9175#issuecomment-295395355", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9175", "id": 295395355, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NTM5NTM1NQ==", "user": {"login": "louiehelm", "id": 381635, "node_id": "MDQ6VXNlcjM4MTYzNQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/381635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/louiehelm", "html_url": "https://github.com/louiehelm", "followers_url": "https://api.github.com/users/louiehelm/followers", "following_url": "https://api.github.com/users/louiehelm/following{/other_user}", "gists_url": "https://api.github.com/users/louiehelm/gists{/gist_id}", "starred_url": "https://api.github.com/users/louiehelm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/louiehelm/subscriptions", "organizations_url": "https://api.github.com/users/louiehelm/orgs", "repos_url": "https://api.github.com/users/louiehelm/repos", "events_url": "https://api.github.com/users/louiehelm/events{/privacy}", "received_events_url": "https://api.github.com/users/louiehelm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-19T19:08:18Z", "updated_at": "2017-04-19T19:08:18Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I've had good performance with beta1 = 0.825, beta2 = 0.99685, lr = double.</p>\n<p>If you have a well-tuned model using Adam's stock decay params, this beta2 lets you simply double the learning rate, while still landing back in the same favorable gradient landscape you were navigating into before (but with more stability).</p>\n<p>I didn't do the explicit math, so there might be a more precise set of beta1 / beta2 that give this benefit. Given how many people are getting comfortable with well-tuned models that hum along with Adam, maybe this is a desirable target for future default decay factors, so that we can easily migrate researchers / developers to a new standard. At very least, figuring out decay factors that do this and then mentioning them in Adam's documentation like <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=55744\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ahundt\">@ahundt</a> suggests seems wise.</p>\n<p>I share <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a>'s intuition that epsilon should be higher than 1e-8 but never benefited from raising it myself in practice. However, that's a parameter you wouldn't see harming your results until it bites you... so I maybe just got lucky. I haven't done the proper data interrogation to see the warning signs if 1e-8 is concerningly low or not.</p>", "body_text": "I've had good performance with beta1 = 0.825, beta2 = 0.99685, lr = double.\nIf you have a well-tuned model using Adam's stock decay params, this beta2 lets you simply double the learning rate, while still landing back in the same favorable gradient landscape you were navigating into before (but with more stability).\nI didn't do the explicit math, so there might be a more precise set of beta1 / beta2 that give this benefit. Given how many people are getting comfortable with well-tuned models that hum along with Adam, maybe this is a desirable target for future default decay factors, so that we can easily migrate researchers / developers to a new standard. At very least, figuring out decay factors that do this and then mentioning them in Adam's documentation like @ahundt suggests seems wise.\nI share @lukaszkaiser's intuition that epsilon should be higher than 1e-8 but never benefited from raising it myself in practice. However, that's a parameter you wouldn't see harming your results until it bites you... so I maybe just got lucky. I haven't done the proper data interrogation to see the warning signs if 1e-8 is concerningly low or not.", "body": "I've had good performance with beta1 = 0.825, beta2 = 0.99685, lr = double.\r\n\r\nIf you have a well-tuned model using Adam's stock decay params, this beta2 lets you simply double the learning rate, while still landing back in the same favorable gradient landscape you were navigating into before (but with more stability).\r\n\r\nI didn't do the explicit math, so there might be a more precise set of beta1 / beta2 that give this benefit. Given how many people are getting comfortable with well-tuned models that hum along with Adam, maybe this is a desirable target for future default decay factors, so that we can easily migrate researchers / developers to a new standard. At very least, figuring out decay factors that do this and then mentioning them in Adam's documentation like @ahundt suggests seems wise.\r\n\r\nI share @lukaszkaiser's intuition that epsilon should be higher than 1e-8 but never benefited from raising it myself in practice. However, that's a parameter you wouldn't see harming your results until it bites you... so I maybe just got lucky. I haven't done the proper data interrogation to see the warning signs if 1e-8 is concerningly low or not."}