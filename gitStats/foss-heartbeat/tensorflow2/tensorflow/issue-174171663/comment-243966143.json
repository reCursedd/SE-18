{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243966143", "html_url": "https://github.com/tensorflow/tensorflow/issues/4114#issuecomment-243966143", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4114", "id": 243966143, "node_id": "MDEyOklzc3VlQ29tbWVudDI0Mzk2NjE0Mw==", "user": {"login": "tobegit3hub", "id": 2715000, "node_id": "MDQ6VXNlcjI3MTUwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobegit3hub", "html_url": "https://github.com/tobegit3hub", "followers_url": "https://api.github.com/users/tobegit3hub/followers", "following_url": "https://api.github.com/users/tobegit3hub/following{/other_user}", "gists_url": "https://api.github.com/users/tobegit3hub/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobegit3hub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobegit3hub/subscriptions", "organizations_url": "https://api.github.com/users/tobegit3hub/orgs", "repos_url": "https://api.github.com/users/tobegit3hub/repos", "events_url": "https://api.github.com/users/tobegit3hub/events{/privacy}", "received_events_url": "https://api.github.com/users/tobegit3hub/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-01T03:28:07Z", "updated_at": "2016-09-01T03:32:12Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py\">Wide and deep model</a> is the best example to handle sparse data and use <code>safe_embedding_lookup_sparse</code>. Notice that the input data doesn't have missing feature, which means each example has exactly same number of feature values.</p>\n<p>The model handles continues columns and categorical(sparse) columns in different ways. It will create one SparseTensor for each categorical column, which is quite different from what <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>  and I mentioned above. Now we have many SparseTensors such as \"education_num\", \"capital_gain\" and \"capital_loss\" and the shape is the same <code>[batch_size, 1]</code>. This is the precondition of using <code>safe_embedding_lookup_sparse</code> because it doesn't need to aggregate anything in this use case. In each train step, we need a <code>for</code> loop to iterate all columns' outputs and it will be much slower if we have more features.</p>\n<p>So I don't know <code>safe_embedding_lookup_sparse</code> is the solution for this problem. It works like <code>embedding_lookup</code> and we have to create many SparseTensors for each column.</p>\n<p>It would be better to know how Google uses TensorFlow for sparse inputs internally. It's general in recommendation system and CTR scenario. Because what I want is how to handle this use cause instead of using sparse matrix multiplication, I'm gonna change the tittle and please comment if you have any practices with TensorFlow.</p>", "body_text": "Wide and deep model is the best example to handle sparse data and use safe_embedding_lookup_sparse. Notice that the input data doesn't have missing feature, which means each example has exactly same number of feature values.\nThe model handles continues columns and categorical(sparse) columns in different ways. It will create one SparseTensor for each categorical column, which is quite different from what @ebrevdo  and I mentioned above. Now we have many SparseTensors such as \"education_num\", \"capital_gain\" and \"capital_loss\" and the shape is the same [batch_size, 1]. This is the precondition of using safe_embedding_lookup_sparse because it doesn't need to aggregate anything in this use case. In each train step, we need a for loop to iterate all columns' outputs and it will be much slower if we have more features.\nSo I don't know safe_embedding_lookup_sparse is the solution for this problem. It works like embedding_lookup and we have to create many SparseTensors for each column.\nIt would be better to know how Google uses TensorFlow for sparse inputs internally. It's general in recommendation system and CTR scenario. Because what I want is how to handle this use cause instead of using sparse matrix multiplication, I'm gonna change the tittle and please comment if you have any practices with TensorFlow.", "body": "[Wide and deep model](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/learn/wide_n_deep_tutorial.py) is the best example to handle sparse data and use `safe_embedding_lookup_sparse`. Notice that the input data doesn't have missing feature, which means each example has exactly same number of feature values.\n\nThe model handles continues columns and categorical(sparse) columns in different ways. It will create one SparseTensor for each categorical column, which is quite different from what @ebrevdo  and I mentioned above. Now we have many SparseTensors such as \"education_num\", \"capital_gain\" and \"capital_loss\" and the shape is the same `[batch_size, 1]`. This is the precondition of using `safe_embedding_lookup_sparse` because it doesn't need to aggregate anything in this use case. In each train step, we need a `for` loop to iterate all columns' outputs and it will be much slower if we have more features.\n\nSo I don't know `safe_embedding_lookup_sparse` is the solution for this problem. It works like `embedding_lookup` and we have to create many SparseTensors for each column.\n\nIt would be better to know how Google uses TensorFlow for sparse inputs internally. It's general in recommendation system and CTR scenario. Because what I want is how to handle this use cause instead of using sparse matrix multiplication, I'm gonna change the tittle and please comment if you have any practices with TensorFlow.\n"}