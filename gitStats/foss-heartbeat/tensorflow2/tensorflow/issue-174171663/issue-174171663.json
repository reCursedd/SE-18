{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4114", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4114/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4114/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4114/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4114", "id": 174171663, "node_id": "MDU6SXNzdWUxNzQxNzE2NjM=", "number": 4114, "title": "Train DNN model efficiently with sparse features and missing data", "user": {"login": "tobegit3hub", "id": 2715000, "node_id": "MDQ6VXNlcjI3MTUwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobegit3hub", "html_url": "https://github.com/tobegit3hub", "followers_url": "https://api.github.com/users/tobegit3hub/followers", "following_url": "https://api.github.com/users/tobegit3hub/following{/other_user}", "gists_url": "https://api.github.com/users/tobegit3hub/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobegit3hub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobegit3hub/subscriptions", "organizations_url": "https://api.github.com/users/tobegit3hub/orgs", "repos_url": "https://api.github.com/users/tobegit3hub/repos", "events_url": "https://api.github.com/users/tobegit3hub/events{/privacy}", "received_events_url": "https://api.github.com/users/tobegit3hub/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-08-31T02:42:51Z", "updated_at": "2018-02-26T06:48:31Z", "closed_at": "2016-09-12T18:00:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We have used TensorFlow for recommend systems to replace the logistic regression model. The train data is quite sparse and we have to do it efficiently. Now we have the problem to train the model without sparse tensor's multiplication.</p>\n<p>We have read <code>word2vec</code> and the example of <code>wide_n_deep</code> models. Their inputs are sparse but not suitable for general recommend systems. Our inputs are sparse and we need to encode the features like ages, colleges with one-hot encoding. The examples may have different number of valid features because of unknown values and the train data looks like this.</p>\n<pre><code>label         gender        age        college       \n------------------------------------------------\n0             2:1           8:1       (unknown)\n1             1:1           5:1         25:1\n0             1:1           8:1       (unknown)\n</code></pre>\n<p>With this dataset, we have to build <code>SparseTensor</code> object for each example data like this.</p>\n<pre><code>example1 = tf.SparseTensor(indices=[[2], [8]], values=[1, 1], shape=[100])\nexample2 = tf.SparseTensor(indices=[[1], [5], [25]], values=[1, 1, 1], shape=[100])\nexample3 = tf.SparseTensor(indices=[[1], [8]], values=[1, 1], shape=[100])\n</code></pre>\n<p>For <code>word2vec</code> and <code>wide_n_deep</code> models, we can use <code>tf.nn.embedding_lookup()</code> to lookup the variables to train and no need to fill up the whole matrix with zeros. This works for each example but we have problems if using batch because the valid <code>ids</code> has different shape to train. The code  looks like this.</p>\n<pre><code>vocabulary_size = 100\nembedding_size = 1\nembeddings = tf.Variable(tf.ones([vocabulary_size, embedding_size]))\n\nbatch_size = 3\nfeature_number = 3 # ERROR: should be 2 or 3\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size, feature_number])\n\nbatch_data = np.array([[2, 8], [1, 5, 25], [1, 8]]) # EROOR: should be dense\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(embed, feed_dict={train_inputs: batch_data}))\n</code></pre>\n<p>The code could not work until we make <code>example1</code>, <code>example2</code> and <code>example3</code> have the same number of valid featue, such as 3 in this case. This is one solution but we have to fill up the examples with zeros(notice that this is different from filling up the one-hot encoding).</p>\n<p>Maybe supporting sparse tensor's multiplication is quit more efficient. Now we have only <code>embedding_lookup</code> op for SpareTensor and I don't know how to use <code>embedding_lookup_sparse</code> or <code>safe_embedding_lookup_sparse</code> for this scenario. It could be great if anyone has any suggestion for this use case.</p>\n<p>Related to <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"135547405\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1241\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1241/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1241\">#1241</a></p>", "body_text": "We have used TensorFlow for recommend systems to replace the logistic regression model. The train data is quite sparse and we have to do it efficiently. Now we have the problem to train the model without sparse tensor's multiplication.\nWe have read word2vec and the example of wide_n_deep models. Their inputs are sparse but not suitable for general recommend systems. Our inputs are sparse and we need to encode the features like ages, colleges with one-hot encoding. The examples may have different number of valid features because of unknown values and the train data looks like this.\nlabel         gender        age        college       \n------------------------------------------------\n0             2:1           8:1       (unknown)\n1             1:1           5:1         25:1\n0             1:1           8:1       (unknown)\n\nWith this dataset, we have to build SparseTensor object for each example data like this.\nexample1 = tf.SparseTensor(indices=[[2], [8]], values=[1, 1], shape=[100])\nexample2 = tf.SparseTensor(indices=[[1], [5], [25]], values=[1, 1, 1], shape=[100])\nexample3 = tf.SparseTensor(indices=[[1], [8]], values=[1, 1], shape=[100])\n\nFor word2vec and wide_n_deep models, we can use tf.nn.embedding_lookup() to lookup the variables to train and no need to fill up the whole matrix with zeros. This works for each example but we have problems if using batch because the valid ids has different shape to train. The code  looks like this.\nvocabulary_size = 100\nembedding_size = 1\nembeddings = tf.Variable(tf.ones([vocabulary_size, embedding_size]))\n\nbatch_size = 3\nfeature_number = 3 # ERROR: should be 2 or 3\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size, feature_number])\n\nbatch_data = np.array([[2, 8], [1, 5, 25], [1, 8]]) # EROOR: should be dense\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(embed, feed_dict={train_inputs: batch_data}))\n\nThe code could not work until we make example1, example2 and example3 have the same number of valid featue, such as 3 in this case. This is one solution but we have to fill up the examples with zeros(notice that this is different from filling up the one-hot encoding).\nMaybe supporting sparse tensor's multiplication is quit more efficient. Now we have only embedding_lookup op for SpareTensor and I don't know how to use embedding_lookup_sparse or safe_embedding_lookup_sparse for this scenario. It could be great if anyone has any suggestion for this use case.\nRelated to #1241", "body": "We have used TensorFlow for recommend systems to replace the logistic regression model. The train data is quite sparse and we have to do it efficiently. Now we have the problem to train the model without sparse tensor's multiplication.\n\nWe have read `word2vec` and the example of `wide_n_deep` models. Their inputs are sparse but not suitable for general recommend systems. Our inputs are sparse and we need to encode the features like ages, colleges with one-hot encoding. The examples may have different number of valid features because of unknown values and the train data looks like this.\n\n```\nlabel         gender        age        college       \n------------------------------------------------\n0             2:1           8:1       (unknown)\n1             1:1           5:1         25:1\n0             1:1           8:1       (unknown)\n```\n\nWith this dataset, we have to build `SparseTensor` object for each example data like this.\n\n```\nexample1 = tf.SparseTensor(indices=[[2], [8]], values=[1, 1], shape=[100])\nexample2 = tf.SparseTensor(indices=[[1], [5], [25]], values=[1, 1, 1], shape=[100])\nexample3 = tf.SparseTensor(indices=[[1], [8]], values=[1, 1], shape=[100])\n```\n\nFor `word2vec` and `wide_n_deep` models, we can use `tf.nn.embedding_lookup()` to lookup the variables to train and no need to fill up the whole matrix with zeros. This works for each example but we have problems if using batch because the valid `ids` has different shape to train. The code  looks like this. \n\n```\nvocabulary_size = 100\nembedding_size = 1\nembeddings = tf.Variable(tf.ones([vocabulary_size, embedding_size]))\n\nbatch_size = 3\nfeature_number = 3 # ERROR: should be 2 or 3\ntrain_inputs = tf.placeholder(tf.int32, shape=[batch_size, feature_number])\n\nbatch_data = np.array([[2, 8], [1, 5, 25], [1, 8]]) # EROOR: should be dense\n\nwith tf.Session() as sess:\n    sess.run(tf.initialize_all_variables())\n    print(sess.run(embed, feed_dict={train_inputs: batch_data}))\n```\n\nThe code could not work until we make `example1`, `example2` and `example3` have the same number of valid featue, such as 3 in this case. This is one solution but we have to fill up the examples with zeros(notice that this is different from filling up the one-hot encoding).\n\nMaybe supporting sparse tensor's multiplication is quit more efficient. Now we have only `embedding_lookup` op for SpareTensor and I don't know how to use `embedding_lookup_sparse` or `safe_embedding_lookup_sparse` for this scenario. It could be great if anyone has any suggestion for this use case.\n\nRelated to https://github.com/tensorflow/tensorflow/issues/1241\n"}