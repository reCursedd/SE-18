{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243658612", "html_url": "https://github.com/tensorflow/tensorflow/issues/4114#issuecomment-243658612", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4114", "id": 243658612, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzY1ODYxMg==", "user": {"login": "tobegit3hub", "id": 2715000, "node_id": "MDQ6VXNlcjI3MTUwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobegit3hub", "html_url": "https://github.com/tobegit3hub", "followers_url": "https://api.github.com/users/tobegit3hub/followers", "following_url": "https://api.github.com/users/tobegit3hub/following{/other_user}", "gists_url": "https://api.github.com/users/tobegit3hub/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobegit3hub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobegit3hub/subscriptions", "organizations_url": "https://api.github.com/users/tobegit3hub/orgs", "repos_url": "https://api.github.com/users/tobegit3hub/repos", "events_url": "https://api.github.com/users/tobegit3hub/events{/privacy}", "received_events_url": "https://api.github.com/users/tobegit3hub/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-31T04:58:06Z", "updated_at": "2016-08-31T04:58:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> for the quick response. We have considered to use the rank-3 SparseTensor instead of a batch of rank-2 SparseTensors. It's the different way to represent the train data and of course we can use it.</p>\n<p>But the <code>safe_embedding_lookup_sparse</code> is not only the lookup method but also combining the weights sum/average of each row. I have read the <a href=\"https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#embedding_lookup_sparse\" rel=\"nofollow\">api docs</a> and trace the unit test of <code>safe_embedding_lookup_sparse</code>.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/2715000/18116453/89381de0-6f7a-11e6-9635-26ef332160d3.png\"><img src=\"https://cloud.githubusercontent.com/assets/2715000/18116453/89381de0-6f7a-11e6-9635-26ef332160d3.png\" alt=\"screen shot 2016-08-30 at 12 19 02\" style=\"max-width:100%;\"></a></p>\n<p>It's some kind of computation but not for our model. For dense data, we compute the logist with <code>w * x + b</code> where w, x and b are dense matrix. For sparse data, just like word2vev, we use <code>embedding_lookup</code> to lookup the related variables and apply to the similar function. Now we can do that but <code>embedding_lookup</code> need the dense <code>ids</code> and we can't satisfy this because we have missing features now.</p>\n<p>We have tried <code>safe_embedding_lookup_sparse</code> but don't know any way to do something like <code>w * x + b</code>. It would be better to have an example about using this to get the logis so that we can write the loss function to train.</p>\n<p>Not sure if <code>safe_embedding_lookup_sparse</code> can do this. If not, we need to lookup each \"reldated\"  variables and do the sparse multiplication.</p>", "body_text": "Thanks @ebrevdo for the quick response. We have considered to use the rank-3 SparseTensor instead of a batch of rank-2 SparseTensors. It's the different way to represent the train data and of course we can use it.\nBut the safe_embedding_lookup_sparse is not only the lookup method but also combining the weights sum/average of each row. I have read the api docs and trace the unit test of safe_embedding_lookup_sparse.\n\nIt's some kind of computation but not for our model. For dense data, we compute the logist with w * x + b where w, x and b are dense matrix. For sparse data, just like word2vev, we use embedding_lookup to lookup the related variables and apply to the similar function. Now we can do that but embedding_lookup need the dense ids and we can't satisfy this because we have missing features now.\nWe have tried safe_embedding_lookup_sparse but don't know any way to do something like w * x + b. It would be better to have an example about using this to get the logis so that we can write the loss function to train.\nNot sure if safe_embedding_lookup_sparse can do this. If not, we need to lookup each \"reldated\"  variables and do the sparse multiplication.", "body": "Thanks @ebrevdo for the quick response. We have considered to use the rank-3 SparseTensor instead of a batch of rank-2 SparseTensors. It's the different way to represent the train data and of course we can use it.\n\nBut the `safe_embedding_lookup_sparse` is not only the lookup method but also combining the weights sum/average of each row. I have read the [api docs](https://www.tensorflow.org/versions/r0.10/api_docs/python/nn.html#embedding_lookup_sparse) and trace the unit test of `safe_embedding_lookup_sparse`.\n\n![screen shot 2016-08-30 at 12 19 02](https://cloud.githubusercontent.com/assets/2715000/18116453/89381de0-6f7a-11e6-9635-26ef332160d3.png)\n\nIt's some kind of computation but not for our model. For dense data, we compute the logist with `w * x + b` where w, x and b are dense matrix. For sparse data, just like word2vev, we use `embedding_lookup` to lookup the related variables and apply to the similar function. Now we can do that but `embedding_lookup` need the dense `ids` and we can't satisfy this because we have missing features now.\n\nWe have tried `safe_embedding_lookup_sparse` but don't know any way to do something like `w * x + b`. It would be better to have an example about using this to get the logis so that we can write the loss function to train.\n\nNot sure if `safe_embedding_lookup_sparse` can do this. If not, we need to lookup each \"reldated\"  variables and do the sparse multiplication.\n"}