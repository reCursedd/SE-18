{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7785", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7785/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7785/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7785/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7785", "id": 209519793, "node_id": "MDU6SXNzdWUyMDk1MTk3OTM=", "number": 7785, "title": "Distributed Tensorflow fails : no device", "user": {"login": "volvador", "id": 15655730, "node_id": "MDQ6VXNlcjE1NjU1NzMw", "avatar_url": "https://avatars1.githubusercontent.com/u/15655730?v=4", "gravatar_id": "", "url": "https://api.github.com/users/volvador", "html_url": "https://github.com/volvador", "followers_url": "https://api.github.com/users/volvador/followers", "following_url": "https://api.github.com/users/volvador/following{/other_user}", "gists_url": "https://api.github.com/users/volvador/gists{/gist_id}", "starred_url": "https://api.github.com/users/volvador/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/volvador/subscriptions", "organizations_url": "https://api.github.com/users/volvador/orgs", "repos_url": "https://api.github.com/users/volvador/repos", "events_url": "https://api.github.com/users/volvador/events{/privacy}", "received_events_url": "https://api.github.com/users/volvador/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2017-02-22T17:11:30Z", "updated_at": "2017-10-15T17:30:16Z", "closed_at": "2017-02-23T16:02:56Z", "author_association": "NONE", "body_html": "<p>I am following the example in <a href=\"https://www.tensorflow.org/versions/r0.10/how_tos/distributed/\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.10/how_tos/distributed/</a>  to create a distributed tensorflow model with a parameter server and n workers. I do not have any GPU, all work is distributed on CPU</p>\n<p>In the chief worker, I want to save my variables every some steps, but invoking the saver results in the following exception :</p>\n<pre><code>Cannot assign a device to node 'save_1/RestoreV2_21': \nCould not satisfy explicit device specification \n'/job:ps/task:0/device:CPU:0' because no devices matching that \nspecification are registered in this process; available devices: \n/job:localhost/replica:0/task:0/cpu:0\n\n[[Node: save_1/RestoreV2_21 = RestoreV2[dtypes=[DT_INT32],\n_device=\"/job:ps/task:0/device:CPU:0\"](save_1/Const, \nsave_1/RestoreV2_21/tensor_names, save_1/RestoreV2_21/shape_and_slices)]]\n</code></pre>\n<p>I tried :</p>\n<pre><code>server = tf.train.Server(cluster,\n                         job_name=self.calib.params['job_name'],\n                         task_index=self.calib.params['task_index'],\n                         config=tf.ConfigProto(allow_soft_placement=True)\n</code></pre>\n<p>I am using a supervisor as in the example in the documentation:</p>\n<pre><code>sv = tf.train.Supervisor(\n                         is_chief=is_chief,\n                       ...)\n</code></pre>\n<p>and creating my session as follows :</p>\n<pre><code>sess = sv.prepare_or_wait_for_session(server.target) as sess:\n</code></pre>\n<p>but I am still having the exact same error.</p>\n<p>When I print server.target I obtain</p>\n<pre><code>grpc://localhost:xxxx\n</code></pre>\n<p>where xxxx is 2200 for my first worker, 2201 for my second worker and so on</p>", "body_text": "I am following the example in https://www.tensorflow.org/versions/r0.10/how_tos/distributed/  to create a distributed tensorflow model with a parameter server and n workers. I do not have any GPU, all work is distributed on CPU\nIn the chief worker, I want to save my variables every some steps, but invoking the saver results in the following exception :\nCannot assign a device to node 'save_1/RestoreV2_21': \nCould not satisfy explicit device specification \n'/job:ps/task:0/device:CPU:0' because no devices matching that \nspecification are registered in this process; available devices: \n/job:localhost/replica:0/task:0/cpu:0\n\n[[Node: save_1/RestoreV2_21 = RestoreV2[dtypes=[DT_INT32],\n_device=\"/job:ps/task:0/device:CPU:0\"](save_1/Const, \nsave_1/RestoreV2_21/tensor_names, save_1/RestoreV2_21/shape_and_slices)]]\n\nI tried :\nserver = tf.train.Server(cluster,\n                         job_name=self.calib.params['job_name'],\n                         task_index=self.calib.params['task_index'],\n                         config=tf.ConfigProto(allow_soft_placement=True)\n\nI am using a supervisor as in the example in the documentation:\nsv = tf.train.Supervisor(\n                         is_chief=is_chief,\n                       ...)\n\nand creating my session as follows :\nsess = sv.prepare_or_wait_for_session(server.target) as sess:\n\nbut I am still having the exact same error.\nWhen I print server.target I obtain\ngrpc://localhost:xxxx\n\nwhere xxxx is 2200 for my first worker, 2201 for my second worker and so on", "body": "I am following the example in https://www.tensorflow.org/versions/r0.10/how_tos/distributed/  to create a distributed tensorflow model with a parameter server and n workers. I do not have any GPU, all work is distributed on CPU\r\n\r\nIn the chief worker, I want to save my variables every some steps, but invoking the saver results in the following exception :\r\n\r\n    Cannot assign a device to node 'save_1/RestoreV2_21': \r\n    Could not satisfy explicit device specification \r\n    '/job:ps/task:0/device:CPU:0' because no devices matching that \r\n    specification are registered in this process; available devices: \r\n    /job:localhost/replica:0/task:0/cpu:0\r\n\r\n    [[Node: save_1/RestoreV2_21 = RestoreV2[dtypes=[DT_INT32],\r\n    _device=\"/job:ps/task:0/device:CPU:0\"](save_1/Const, \r\n    save_1/RestoreV2_21/tensor_names, save_1/RestoreV2_21/shape_and_slices)]]\r\n\r\nI tried :\r\n\r\n    server = tf.train.Server(cluster,\r\n                             job_name=self.calib.params['job_name'],\r\n                             task_index=self.calib.params['task_index'],\r\n                             config=tf.ConfigProto(allow_soft_placement=True)\r\n\r\nI am using a supervisor as in the example in the documentation:\r\n\r\n    sv = tf.train.Supervisor(\r\n                             is_chief=is_chief,\r\n                           ...)\r\n\r\nand creating my session as follows :\r\n\r\n    sess = sv.prepare_or_wait_for_session(server.target) as sess:\r\n\r\nbut I am still having the exact same error.\r\n\r\nWhen I print server.target I obtain \r\n\r\n    grpc://localhost:xxxx\r\n\r\nwhere xxxx is 2200 for my first worker, 2201 for my second worker and so on"}