{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/242350207", "html_url": "https://github.com/tensorflow/tensorflow/issues/4016#issuecomment-242350207", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4016", "id": 242350207, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MjM1MDIwNw==", "user": {"login": "ivankreso", "id": 2056432, "node_id": "MDQ6VXNlcjIwNTY0MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/2056432?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ivankreso", "html_url": "https://github.com/ivankreso", "followers_url": "https://api.github.com/users/ivankreso/followers", "following_url": "https://api.github.com/users/ivankreso/following{/other_user}", "gists_url": "https://api.github.com/users/ivankreso/gists{/gist_id}", "starred_url": "https://api.github.com/users/ivankreso/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ivankreso/subscriptions", "organizations_url": "https://api.github.com/users/ivankreso/orgs", "repos_url": "https://api.github.com/users/ivankreso/repos", "events_url": "https://api.github.com/users/ivankreso/events{/privacy}", "received_events_url": "https://api.github.com/users/ivankreso/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-25T11:03:20Z", "updated_at": "2016-08-25T11:05:00Z", "author_association": "NONE", "body_html": "<p>With lambda function initializing_from_value flag will stay false in _get_single_variable():</p>\n<pre><code>Traceback (most recent call last):\n...\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 281, in _init_from_args\n    self._initial_value = ops.convert_to_tensor(initial_value(),\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 526, in &lt;lambda&gt;\n    init_val = lambda: initializer(shape.as_list(), dtype=dtype)\nTypeError: &lt;lambda&gt;() got an unexpected keyword argument 'dtype'\n</code></pre>\n<p>I did what you suggested. variables.variable() is a better place for a fix. Below is the code which works for me. Is that ok, should I create a pull request for this?</p>\n<pre><code>@contrib_add_arg_scope\ndef variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n             regularizer=None, trainable=True, collections=None,\n             caching_device=None, device=None):\n  # Remove duplicates\n  collections = set(collections)\n  # If initializing from numpy array don't pass shape to get_variable()\n  if initializer is not None and not callable(initializer):\n    if shape != list(initializer.shape):\n        raise ValueError(\"Shape doesn't match initializer shape: %s != %s.\"\n                         % (shape, list(initializer.shape)))\n    shape = None\n  with ops.device(device or ''):\n    return variable_scope.get_variable(name, shape=shape, dtype=dtype,\n                                       initializer=initializer,\n                                       regularizer=regularizer,\n                                       trainable=trainable,\n                                       collections=collections,\n                                       caching_device=caching_device)\n</code></pre>", "body_text": "With lambda function initializing_from_value flag will stay false in _get_single_variable():\nTraceback (most recent call last):\n...\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 281, in _init_from_args\n    self._initial_value = ops.convert_to_tensor(initial_value(),\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 526, in <lambda>\n    init_val = lambda: initializer(shape.as_list(), dtype=dtype)\nTypeError: <lambda>() got an unexpected keyword argument 'dtype'\n\nI did what you suggested. variables.variable() is a better place for a fix. Below is the code which works for me. Is that ok, should I create a pull request for this?\n@contrib_add_arg_scope\ndef variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n             regularizer=None, trainable=True, collections=None,\n             caching_device=None, device=None):\n  # Remove duplicates\n  collections = set(collections)\n  # If initializing from numpy array don't pass shape to get_variable()\n  if initializer is not None and not callable(initializer):\n    if shape != list(initializer.shape):\n        raise ValueError(\"Shape doesn't match initializer shape: %s != %s.\"\n                         % (shape, list(initializer.shape)))\n    shape = None\n  with ops.device(device or ''):\n    return variable_scope.get_variable(name, shape=shape, dtype=dtype,\n                                       initializer=initializer,\n                                       regularizer=regularizer,\n                                       trainable=trainable,\n                                       collections=collections,\n                                       caching_device=caching_device)", "body": "With lambda function initializing_from_value flag will stay false in _get_single_variable():\n\n```\nTraceback (most recent call last):\n...\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variables.py\", line 281, in _init_from_args\n    self._initial_value = ops.convert_to_tensor(initial_value(),\n  File \"/usr/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\", line 526, in <lambda>\n    init_val = lambda: initializer(shape.as_list(), dtype=dtype)\nTypeError: <lambda>() got an unexpected keyword argument 'dtype'\n```\n\nI did what you suggested. variables.variable() is a better place for a fix. Below is the code which works for me. Is that ok, should I create a pull request for this?\n\n```\n@contrib_add_arg_scope\ndef variable(name, shape=None, dtype=dtypes.float32, initializer=None,\n             regularizer=None, trainable=True, collections=None,\n             caching_device=None, device=None):\n  # Remove duplicates\n  collections = set(collections)\n  # If initializing from numpy array don't pass shape to get_variable()\n  if initializer is not None and not callable(initializer):\n    if shape != list(initializer.shape):\n        raise ValueError(\"Shape doesn't match initializer shape: %s != %s.\"\n                         % (shape, list(initializer.shape)))\n    shape = None\n  with ops.device(device or ''):\n    return variable_scope.get_variable(name, shape=shape, dtype=dtype,\n                                       initializer=initializer,\n                                       regularizer=regularizer,\n                                       trainable=trainable,\n                                       collections=collections,\n                                       caching_device=caching_device)\n```\n"}