{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/197240766", "html_url": "https://github.com/tensorflow/tensorflow/issues/666#issuecomment-197240766", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/666", "id": 197240766, "node_id": "MDEyOklzc3VlQ29tbWVudDE5NzI0MDc2Ng==", "user": {"login": "hillelt", "id": 2479861, "node_id": "MDQ6VXNlcjI0Nzk4NjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/2479861?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hillelt", "html_url": "https://github.com/hillelt", "followers_url": "https://api.github.com/users/hillelt/followers", "following_url": "https://api.github.com/users/hillelt/following{/other_user}", "gists_url": "https://api.github.com/users/hillelt/gists{/gist_id}", "starred_url": "https://api.github.com/users/hillelt/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hillelt/subscriptions", "organizations_url": "https://api.github.com/users/hillelt/orgs", "repos_url": "https://api.github.com/users/hillelt/repos", "events_url": "https://api.github.com/users/hillelt/events{/privacy}", "received_events_url": "https://api.github.com/users/hillelt/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-16T10:05:15Z", "updated_at": "2016-05-06T19:50:24Z", "author_association": "NONE", "body_html": "<p>I get this error when trying to save a text classification model that uses pre-trained word embeddings.  I was able to reproduce it with the minimal snippet below. It definitely looks like a variable size issue since the example completes successfully once you switch from 3,000,000 rows to 300,000 rows.</p>\n<p>The issue looks important since since these models are common in NLP and not being able to save them means you can't really use them in production.</p>\n<pre><code># Reproducing the error:\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.device('/cpu:0'):\n    rows = 3000000\n    cols = 300\n    input_W = tf.placeholder(tf.float32, [rows, cols])\n    W = tf.Variable(input_W)\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables(), feed_dict={input_W: np.random.rand(rows, cols)})\n    saver = tf.train.Saver(tf.all_variables())\n    path = saver.save(sess, \"dump.bin\",  write_meta_graph=False)\n    print \"Done.\"\n\n</code></pre>", "body_text": "I get this error when trying to save a text classification model that uses pre-trained word embeddings.  I was able to reproduce it with the minimal snippet below. It definitely looks like a variable size issue since the example completes successfully once you switch from 3,000,000 rows to 300,000 rows.\nThe issue looks important since since these models are common in NLP and not being able to save them means you can't really use them in production.\n# Reproducing the error:\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.device('/cpu:0'):\n    rows = 3000000\n    cols = 300\n    input_W = tf.placeholder(tf.float32, [rows, cols])\n    W = tf.Variable(input_W)\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables(), feed_dict={input_W: np.random.rand(rows, cols)})\n    saver = tf.train.Saver(tf.all_variables())\n    path = saver.save(sess, \"dump.bin\",  write_meta_graph=False)\n    print \"Done.\"", "body": "I get this error when trying to save a text classification model that uses pre-trained word embeddings.  I was able to reproduce it with the minimal snippet below. It definitely looks like a variable size issue since the example completes successfully once you switch from 3,000,000 rows to 300,000 rows.\n\nThe issue looks important since since these models are common in NLP and not being able to save them means you can't really use them in production.\n\n```\n# Reproducing the error:\nimport tensorflow as tf\nimport numpy as np\n\nwith tf.device('/cpu:0'):\n    rows = 3000000\n    cols = 300\n    input_W = tf.placeholder(tf.float32, [rows, cols])\n    W = tf.Variable(input_W)\n    sess = tf.Session()\n    sess.run(tf.initialize_all_variables(), feed_dict={input_W: np.random.rand(rows, cols)})\n    saver = tf.train.Saver(tf.all_variables())\n    path = saver.save(sess, \"dump.bin\",  write_meta_graph=False)\n    print \"Done.\"\n\n```\n"}