{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/351909211", "html_url": "https://github.com/tensorflow/tensorflow/issues/14859#issuecomment-351909211", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14859", "id": 351909211, "node_id": "MDEyOklzc3VlQ29tbWVudDM1MTkwOTIxMQ==", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "created_at": "2017-12-15T04:16:46Z", "updated_at": "2017-12-15T06:55:00Z", "author_association": "NONE", "body_html": "<p>I solved this by removing all Batch Normalization layers in network , followed the tutorial process and froze model with official script , achieve the same accuracy between checkpoint and freeze_graph.pb .</p>\n<p>And I noticed that the more different inference images comparing to training images , the loss of accuracy gets more . Does this problem result from insufficient number and lack of variety of training images ? Since batch norm normalize and shift images , and get these parameter during training process .</p>\n<p>Is there any other insights can be provided ?</p>", "body_text": "I solved this by removing all Batch Normalization layers in network , followed the tutorial process and froze model with official script , achieve the same accuracy between checkpoint and freeze_graph.pb .\nAnd I noticed that the more different inference images comparing to training images , the loss of accuracy gets more . Does this problem result from insufficient number and lack of variety of training images ? Since batch norm normalize and shift images , and get these parameter during training process .\nIs there any other insights can be provided ?", "body": "I solved this by removing all Batch Normalization layers in network , followed the tutorial process and froze model with official script , achieve the same accuracy between checkpoint and freeze_graph.pb .\r\n\r\nAnd I noticed that the more different inference images comparing to training images , the loss of accuracy gets more . Does this problem result from insufficient number and lack of variety of training images ? Since batch norm normalize and shift images , and get these parameter during training process .\r\n\r\nIs there any other insights can be provided ?"}