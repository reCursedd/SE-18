{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/255620946", "html_url": "https://github.com/tensorflow/tensorflow/issues/5040#issuecomment-255620946", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5040", "id": 255620946, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NTYyMDk0Ng==", "user": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-23T23:00:23Z", "updated_at": "2016-10-23T23:00:23Z", "author_association": "MEMBER", "body_html": "<p>This is ok during inference (not training) becase, when building the graph, we set the forward_only parameter in seq2seq_model to true, which in turns sets feed_previous to true in tf.nn.seq2seq.embedding_attention_seq2seq. In this case, the decoder uses the previous-step output for all steps other than the first one (GO symbol) and doesn't use the decoder inputs at all.</p>", "body_text": "This is ok during inference (not training) becase, when building the graph, we set the forward_only parameter in seq2seq_model to true, which in turns sets feed_previous to true in tf.nn.seq2seq.embedding_attention_seq2seq. In this case, the decoder uses the previous-step output for all steps other than the first one (GO symbol) and doesn't use the decoder inputs at all.", "body": "This is ok during inference (not training) becase, when building the graph, we set the forward_only parameter in seq2seq_model to true, which in turns sets feed_previous to true in tf.nn.seq2seq.embedding_attention_seq2seq. In this case, the decoder uses the previous-step output for all steps other than the first one (GO symbol) and doesn't use the decoder inputs at all.\n"}