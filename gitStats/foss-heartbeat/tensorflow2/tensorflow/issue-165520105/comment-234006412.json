{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/234006412", "html_url": "https://github.com/tensorflow/tensorflow/issues/3308#issuecomment-234006412", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3308", "id": 234006412, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNDAwNjQxMg==", "user": {"login": "petewarden", "id": 161459, "node_id": "MDQ6VXNlcjE2MTQ1OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/161459?v=4", "gravatar_id": "", "url": "https://api.github.com/users/petewarden", "html_url": "https://github.com/petewarden", "followers_url": "https://api.github.com/users/petewarden/followers", "following_url": "https://api.github.com/users/petewarden/following{/other_user}", "gists_url": "https://api.github.com/users/petewarden/gists{/gist_id}", "starred_url": "https://api.github.com/users/petewarden/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/petewarden/subscriptions", "organizations_url": "https://api.github.com/users/petewarden/orgs", "repos_url": "https://api.github.com/users/petewarden/repos", "events_url": "https://api.github.com/users/petewarden/events{/privacy}", "received_events_url": "https://api.github.com/users/petewarden/received_events", "type": "User", "site_admin": false}, "created_at": "2016-07-20T16:38:41Z", "updated_at": "2016-07-20T16:38:41Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>No OpKernel was registered to support Op 'Inv'</p>\n</blockquote>\n<p>This is actually a different error. If you look at <code>tensorflow/contrib/makefile/tf_op_files.txt</code> you'll see a list of the kernel files that are included by default, which doesn't include the <code>tensorflow/core/kernels/cwise_op_inverse.cc</code> which defines the Inv kernel.</p>\n<p>This is actually by design, since we have picked a minimal subset of kernels that we expect to be used for inference, skipping those that are likely to only be used for training, since inference is the focus of our mobile build. That's not always an easy thing to estimate however, and here we've missed one that you need.</p>\n<p>The short-term answer is that you can make local changes to <code>tf_op_files.txt</code> to add <code>cwise_op_inverse.cc</code> to fix your immediate problem (and do something similar with any other kernels you're missing). The better solution is to document what ops are supported more effectively, and offer an option to have a full set of ops too, instead of the current subset.</p>\n<blockquote>\n<p>This linker flag increases the binary size for each of them by around 180 MB. It is huge compared to the minimal size I get without the flag.</p>\n</blockquote>\n<p>Is this still true if you supply -Os to the whole build process? We tend to see a smaller increase outside of debug builds. You're right though, as I mention above this is a pretty indiscriminate switch and it would be good to figure out a nicer solution.</p>", "body_text": "No OpKernel was registered to support Op 'Inv'\n\nThis is actually a different error. If you look at tensorflow/contrib/makefile/tf_op_files.txt you'll see a list of the kernel files that are included by default, which doesn't include the tensorflow/core/kernels/cwise_op_inverse.cc which defines the Inv kernel.\nThis is actually by design, since we have picked a minimal subset of kernels that we expect to be used for inference, skipping those that are likely to only be used for training, since inference is the focus of our mobile build. That's not always an easy thing to estimate however, and here we've missed one that you need.\nThe short-term answer is that you can make local changes to tf_op_files.txt to add cwise_op_inverse.cc to fix your immediate problem (and do something similar with any other kernels you're missing). The better solution is to document what ops are supported more effectively, and offer an option to have a full set of ops too, instead of the current subset.\n\nThis linker flag increases the binary size for each of them by around 180 MB. It is huge compared to the minimal size I get without the flag.\n\nIs this still true if you supply -Os to the whole build process? We tend to see a smaller increase outside of debug builds. You're right though, as I mention above this is a pretty indiscriminate switch and it would be good to figure out a nicer solution.", "body": "> No OpKernel was registered to support Op 'Inv'\n\nThis is actually a different error. If you look at `tensorflow/contrib/makefile/tf_op_files.txt` you'll see a list of the kernel files that are included by default, which doesn't include the `tensorflow/core/kernels/cwise_op_inverse.cc` which defines the Inv kernel.\n\nThis is actually by design, since we have picked a minimal subset of kernels that we expect to be used for inference, skipping those that are likely to only be used for training, since inference is the focus of our mobile build. That's not always an easy thing to estimate however, and here we've missed one that you need.\n\nThe short-term answer is that you can make local changes to `tf_op_files.txt` to add `cwise_op_inverse.cc` to fix your immediate problem (and do something similar with any other kernels you're missing). The better solution is to document what ops are supported more effectively, and offer an option to have a full set of ops too, instead of the current subset.\n\n> This linker flag increases the binary size for each of them by around 180 MB. It is huge compared to the minimal size I get without the flag.\n\nIs this still true if you supply -Os to the whole build process? We tend to see a smaller increase outside of debug builds. You're right though, as I mention above this is a pretty indiscriminate switch and it would be good to figure out a nicer solution.\n"}