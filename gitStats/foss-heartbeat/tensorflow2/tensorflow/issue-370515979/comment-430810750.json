{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430810750", "html_url": "https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430810750", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23014", "id": 430810750, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDgxMDc1MA==", "user": {"login": "Goldesel23", "id": 10371630, "node_id": "MDQ6VXNlcjEwMzcxNjMw", "avatar_url": "https://avatars3.githubusercontent.com/u/10371630?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Goldesel23", "html_url": "https://github.com/Goldesel23", "followers_url": "https://api.github.com/users/Goldesel23/followers", "following_url": "https://api.github.com/users/Goldesel23/following{/other_user}", "gists_url": "https://api.github.com/users/Goldesel23/gists{/gist_id}", "starred_url": "https://api.github.com/users/Goldesel23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Goldesel23/subscriptions", "organizations_url": "https://api.github.com/users/Goldesel23/orgs", "repos_url": "https://api.github.com/users/Goldesel23/repos", "events_url": "https://api.github.com/users/Goldesel23/events{/privacy}", "received_events_url": "https://api.github.com/users/Goldesel23/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T22:24:28Z", "updated_at": "2018-10-17T22:24:28Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D? We've seen issues where tf.layers layers leak in eager. If this replacing fixes your problem then we know where to look.<br>\n<a href=\"#\">\u2026</a><br>\nOn Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas ***@***.***&gt; wrote: Hi thanks for answering! This my model definition: class PiModel(tf.keras.Model): \"\"\" Class for defining eager compatible tfrecords file I did not use tfe.Network since it will be depracated in the future by tensorflow. \"\"\" def <strong>init</strong>(self): \"\"\" Init Set all the layers that need to be tracked in the process of gradients descent (pooling and dropout for example dont need to be stored) \"\"\" super(PiModel, self).<strong>init</strong>() self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3], padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) def __aditive_gaussian_noise(self, input, std): \"\"\" Function to add additive zero mean noise as described in the paper Arguments: input {tensor} -- image std {int} -- std to use in the random_normal Returns: {tensor} -- image with added noise \"\"\" noise = tf.random_normal(shape=tf.shape( input), mean=0.0, stddev=std, dtype=tf.float32) return input + noise def __apply_image_augmentation(self, image): \"\"\" Applies random transformation to the image Arguments: image {tensor} -- image Returns: {tensor} -- transformed image \"\"\" random_translation = tf.random_uniform( [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32) image = tf.contrib.image.translate( image, random_translation, 'NEAREST') return image def call(self, input, training=True): \"\"\" Function that allows running a tensor through the pi model Arguments: input {[tensor]} -- batch of images training {bool} -- if true applies augmentaton and additive noise Returns: [tensor] -- predictions \"\"\" if training: h = self.__aditive_gaussian_noise(input, 0.15) h = self.__apply_image_augmentation(h) else: h = input h = self._conv1a(h, training) h = self._conv1b(h, training) h = self._conv1c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv2a(h, training) h = self._conv2b(h, training) h = self._conv2c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv3a(h, training) h = self._conv3b(h, training) h = self._conv3c(h, training) # Average Pooling h = tf.reduce_mean(h, reduction_indices=[1, 2]) return self._dense(h, training) The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization. \u2014 You are receiving this because you were assigned. Reply to this email directly, view it on GitHub &lt;<a href=\"https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/23014/hovercard\">#23014 (comment)</a>&gt;, or mute the thread <a href=\"https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui\">https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui</a> .<br>\n-- - Alex</p>\n</blockquote>\n<p>After replacing the layers with tf.keras.layers.MaxPool2D the problem was not totally solved (the increase was still there but in a smaller magnitude). But after replacing the tf.layers.dropout by Keras one, the problem was solved.</p>\n<p>Thanks for the support. I think the issue was resolved!</p>", "body_text": "Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D? We've seen issues where tf.layers layers leak in eager. If this replacing fixes your problem then we know where to look.\n\u2026\nOn Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas ***@***.***> wrote: Hi thanks for answering! This my model definition: class PiModel(tf.keras.Model): \"\"\" Class for defining eager compatible tfrecords file I did not use tfe.Network since it will be depracated in the future by tensorflow. \"\"\" def init(self): \"\"\" Init Set all the layers that need to be tracked in the process of gradients descent (pooling and dropout for example dont need to be stored) \"\"\" super(PiModel, self).init() self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3], padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) def __aditive_gaussian_noise(self, input, std): \"\"\" Function to add additive zero mean noise as described in the paper Arguments: input {tensor} -- image std {int} -- std to use in the random_normal Returns: {tensor} -- image with added noise \"\"\" noise = tf.random_normal(shape=tf.shape( input), mean=0.0, stddev=std, dtype=tf.float32) return input + noise def __apply_image_augmentation(self, image): \"\"\" Applies random transformation to the image Arguments: image {tensor} -- image Returns: {tensor} -- transformed image \"\"\" random_translation = tf.random_uniform( [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32) image = tf.contrib.image.translate( image, random_translation, 'NEAREST') return image def call(self, input, training=True): \"\"\" Function that allows running a tensor through the pi model Arguments: input {[tensor]} -- batch of images training {bool} -- if true applies augmentaton and additive noise Returns: [tensor] -- predictions \"\"\" if training: h = self.__aditive_gaussian_noise(input, 0.15) h = self.__apply_image_augmentation(h) else: h = input h = self._conv1a(h, training) h = self._conv1b(h, training) h = self._conv1c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv2a(h, training) h = self._conv2b(h, training) h = self._conv2c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv3a(h, training) h = self._conv3b(h, training) h = self._conv3c(h, training) # Average Pooling h = tf.reduce_mean(h, reduction_indices=[1, 2]) return self._dense(h, training) The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization. \u2014 You are receiving this because you were assigned. Reply to this email directly, view it on GitHub <#23014 (comment)>, or mute the thread https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui .\n-- - Alex\n\nAfter replacing the layers with tf.keras.layers.MaxPool2D the problem was not totally solved (the increase was still there but in a smaller magnitude). But after replacing the tf.layers.dropout by Keras one, the problem was solved.\nThanks for the support. I think the issue was resolved!", "body": "> Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D? We've seen issues where tf.layers layers leak in eager. If this replacing fixes your problem then we know where to look.\r\n> [\u2026](#)\r\n> On Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas ***@***.***> wrote: Hi thanks for answering! This my model definition: class PiModel(tf.keras.Model): \"\"\" Class for defining eager compatible tfrecords file I did not use tfe.Network since it will be depracated in the future by tensorflow. \"\"\" def __init__(self): \"\"\" Init Set all the layers that need to be tracked in the process of gradients descent (pooling and dropout for example dont need to be stored) \"\"\" super(PiModel, self).__init__() self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3], padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1], padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, kernel_initializer=tf.keras.initializers.he_uniform(), bias_initializer=tf.keras.initializers.constant(0.1), weight_norm=True, mean_only_batch_norm=True) def __aditive_gaussian_noise(self, input, std): \"\"\" Function to add additive zero mean noise as described in the paper Arguments: input {tensor} -- image std {int} -- std to use in the random_normal Returns: {tensor} -- image with added noise \"\"\" noise = tf.random_normal(shape=tf.shape( input), mean=0.0, stddev=std, dtype=tf.float32) return input + noise def __apply_image_augmentation(self, image): \"\"\" Applies random transformation to the image Arguments: image {tensor} -- image Returns: {tensor} -- transformed image \"\"\" random_translation = tf.random_uniform( [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32) image = tf.contrib.image.translate( image, random_translation, 'NEAREST') return image def call(self, input, training=True): \"\"\" Function that allows running a tensor through the pi model Arguments: input {[tensor]} -- batch of images training {bool} -- if true applies augmentaton and additive noise Returns: [tensor] -- predictions \"\"\" if training: h = self.__aditive_gaussian_noise(input, 0.15) h = self.__apply_image_augmentation(h) else: h = input h = self._conv1a(h, training) h = self._conv1b(h, training) h = self._conv1c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv2a(h, training) h = self._conv2b(h, training) h = self._conv2c(h, training) h = tf.layers.max_pooling2d( inputs=h, pool_size=[2, 2], strides=[2, 2]) h = tf.layers.dropout(inputs=h, rate=0.5, training=training) h = self._conv3a(h, training) h = self._conv3b(h, training) h = self._conv3c(h, training) # Average Pooling h = tf.reduce_mean(h, reduction_indices=[1, 2]) return self._dense(h, training) The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization. \u2014 You are receiving this because you were assigned. Reply to this email directly, view it on GitHub <[#23014 (comment)](https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886)>, or mute the thread <https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui> .\r\n> -- - Alex\r\n\r\nAfter replacing the layers with tf.keras.layers.MaxPool2D the problem was not totally solved (the increase was still there but in a smaller magnitude). But after replacing the tf.layers.dropout by Keras one, the problem was solved. \r\n\r\nThanks for the support. I think the issue was resolved!"}