{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430695718", "html_url": "https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430695718", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23014", "id": 430695718, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDY5NTcxOA==", "user": {"login": "alextp", "id": 5061, "node_id": "MDQ6VXNlcjUwNjE=", "avatar_url": "https://avatars0.githubusercontent.com/u/5061?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alextp", "html_url": "https://github.com/alextp", "followers_url": "https://api.github.com/users/alextp/followers", "following_url": "https://api.github.com/users/alextp/following{/other_user}", "gists_url": "https://api.github.com/users/alextp/gists{/gist_id}", "starred_url": "https://api.github.com/users/alextp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alextp/subscriptions", "organizations_url": "https://api.github.com/users/alextp/orgs", "repos_url": "https://api.github.com/users/alextp/repos", "events_url": "https://api.github.com/users/alextp/events{/privacy}", "received_events_url": "https://api.github.com/users/alextp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T16:24:18Z", "updated_at": "2018-10-17T16:24:18Z", "author_association": "MEMBER", "body_html": "<div class=\"email-fragment\">Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D?\nWe've seen issues where tf.layers layers leak in eager. If this replacing\nfixes your problem then we know where to look.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas ***@***.***&gt; wrote:\n Hi thanks for answering! This my model definition:\n\n class PiModel(tf.keras.Model):\n     \"\"\" Class for defining eager compatible tfrecords file\n         I did not use tfe.Network since it will be depracated in the\n         future by tensorflow.\n     \"\"\"\n\n     def __init__(self):\n         \"\"\" Init\n             Set all the layers that need to be tracked in the process of\n             gradients descent (pooling and dropout for example dont need\n             to be stored)\n         \"\"\"\n\n         super(PiModel, self).__init__()\n         self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n\n         self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n\n         self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\n                                                         padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                          kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                          bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n\n         self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax,\n                                                      kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                      bias_initializer=tf.keras.initializers.constant(0.1),\n                                                      weight_norm=True, mean_only_batch_norm=True)\n\n     def __aditive_gaussian_noise(self, input, std):\n         \"\"\" Function to add additive zero mean noise as described in the paper\n         Arguments:\n             input {tensor} -- image\n             std {int} -- std to use in the random_normal\n         Returns:\n             {tensor} -- image with added noise\n         \"\"\"\n\n         noise = tf.random_normal(shape=tf.shape(\n             input), mean=0.0, stddev=std, dtype=tf.float32)\n         return input + noise\n\n     def __apply_image_augmentation(self, image):\n         \"\"\" Applies random transformation to the image\n         Arguments:\n             image {tensor} -- image\n         Returns:\n             {tensor} -- transformed image\n         \"\"\"\n\n         random_translation = tf.random_uniform(\n             [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\n         image = tf.contrib.image.translate(\n             image, random_translation, 'NEAREST')\n         return image\n\n     def call(self, input, training=True):\n         \"\"\" Function that allows running a tensor through the pi model\n         Arguments:\n             input {[tensor]} -- batch of images\n             training {bool} -- if true applies augmentaton and additive noise\n         Returns:\n             [tensor] -- predictions\n         \"\"\"\n\n         if training:\n             h = self.__aditive_gaussian_noise(input, 0.15)\n             h = self.__apply_image_augmentation(h)\n         else:\n             h = input\n\n         h = self._conv1a(h, training)\n         h = self._conv1b(h, training)\n         h = self._conv1c(h, training)\n         h = tf.layers.max_pooling2d(\n             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n         h = self._conv2a(h, training)\n         h = self._conv2b(h, training)\n         h = self._conv2c(h, training)\n         h = tf.layers.max_pooling2d(\n             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n         h = self._conv3a(h, training)\n         h = self._conv3b(h, training)\n         h = self._conv3c(h, training)\n\n         # Average Pooling\n         h = tf.reduce_mean(h, reduction_indices=[1, 2])\n         return self._dense(h, training)\n\n The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for\n weight and mean only batch normalization.\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"370515979\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/23014\" href=\"https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886\">#23014 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui\">https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n<div class=\"email-signature-reply\">-- \n - Alex</div>\n</div>", "body_text": "Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D?\nWe've seen issues where tf.layers layers leak in eager. If this replacing\nfixes your problem then we know where to look.\n\u2026\nOn Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas ***@***.***> wrote:\n Hi thanks for answering! This my model definition:\n\n class PiModel(tf.keras.Model):\n     \"\"\" Class for defining eager compatible tfrecords file\n         I did not use tfe.Network since it will be depracated in the\n         future by tensorflow.\n     \"\"\"\n\n     def __init__(self):\n         \"\"\" Init\n             Set all the layers that need to be tracked in the process of\n             gradients descent (pooling and dropout for example dont need\n             to be stored)\n         \"\"\"\n\n         super(PiModel, self).__init__()\n         self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n\n         self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n\n         self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\n                                                         padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                          kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                          bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n         self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\n                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                         weight_norm=True, mean_only_batch_norm=True)\n\n         self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax,\n                                                      kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                      bias_initializer=tf.keras.initializers.constant(0.1),\n                                                      weight_norm=True, mean_only_batch_norm=True)\n\n     def __aditive_gaussian_noise(self, input, std):\n         \"\"\" Function to add additive zero mean noise as described in the paper\n         Arguments:\n             input {tensor} -- image\n             std {int} -- std to use in the random_normal\n         Returns:\n             {tensor} -- image with added noise\n         \"\"\"\n\n         noise = tf.random_normal(shape=tf.shape(\n             input), mean=0.0, stddev=std, dtype=tf.float32)\n         return input + noise\n\n     def __apply_image_augmentation(self, image):\n         \"\"\" Applies random transformation to the image\n         Arguments:\n             image {tensor} -- image\n         Returns:\n             {tensor} -- transformed image\n         \"\"\"\n\n         random_translation = tf.random_uniform(\n             [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\n         image = tf.contrib.image.translate(\n             image, random_translation, 'NEAREST')\n         return image\n\n     def call(self, input, training=True):\n         \"\"\" Function that allows running a tensor through the pi model\n         Arguments:\n             input {[tensor]} -- batch of images\n             training {bool} -- if true applies augmentaton and additive noise\n         Returns:\n             [tensor] -- predictions\n         \"\"\"\n\n         if training:\n             h = self.__aditive_gaussian_noise(input, 0.15)\n             h = self.__apply_image_augmentation(h)\n         else:\n             h = input\n\n         h = self._conv1a(h, training)\n         h = self._conv1b(h, training)\n         h = self._conv1c(h, training)\n         h = tf.layers.max_pooling2d(\n             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n         h = self._conv2a(h, training)\n         h = self._conv2b(h, training)\n         h = self._conv2c(h, training)\n         h = tf.layers.max_pooling2d(\n             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n         h = self._conv3a(h, training)\n         h = self._conv3b(h, training)\n         h = self._conv3c(h, training)\n\n         # Average Pooling\n         h = tf.reduce_mean(h, reduction_indices=[1, 2])\n         return self._dense(h, training)\n\n The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for\n weight and mean only batch normalization.\n\n \u2014\n You are receiving this because you were assigned.\n Reply to this email directly, view it on GitHub\n <#23014 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui>\n .\n\n\n-- \n - Alex", "body": "Can you replace tf.layers.max_pooling2d with tf.keras.layers.MaxPool2D?\nWe've seen issues where tf.layers layers leak in eager. If this replacing\nfixes your problem then we know where to look.\n\nOn Wed, Oct 17, 2018 at 12:23 AM Tiago Freitas <notifications@github.com>\nwrote:\n\n> Hi thanks for answering! This my model definition:\n>\n> class PiModel(tf.keras.Model):\n>     \"\"\" Class for defining eager compatible tfrecords file\n>         I did not use tfe.Network since it will be depracated in the\n>         future by tensorflow.\n>     \"\"\"\n>\n>     def __init__(self):\n>         \"\"\" Init\n>             Set all the layers that need to be tracked in the process of\n>             gradients descent (pooling and dropout for example dont need\n>             to be stored)\n>         \"\"\"\n>\n>         super(PiModel, self).__init__()\n>         self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>\n>         self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>\n>         self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\n>                                                         padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                          kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                          bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>         self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\n>                                                         padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n>                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                         weight_norm=True, mean_only_batch_norm=True)\n>\n>         self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax,\n>                                                      kernel_initializer=tf.keras.initializers.he_uniform(),\n>                                                      bias_initializer=tf.keras.initializers.constant(0.1),\n>                                                      weight_norm=True, mean_only_batch_norm=True)\n>\n>     def __aditive_gaussian_noise(self, input, std):\n>         \"\"\" Function to add additive zero mean noise as described in the paper\n>         Arguments:\n>             input {tensor} -- image\n>             std {int} -- std to use in the random_normal\n>         Returns:\n>             {tensor} -- image with added noise\n>         \"\"\"\n>\n>         noise = tf.random_normal(shape=tf.shape(\n>             input), mean=0.0, stddev=std, dtype=tf.float32)\n>         return input + noise\n>\n>     def __apply_image_augmentation(self, image):\n>         \"\"\" Applies random transformation to the image\n>         Arguments:\n>             image {tensor} -- image\n>         Returns:\n>             {tensor} -- transformed image\n>         \"\"\"\n>\n>         random_translation = tf.random_uniform(\n>             [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\n>         image = tf.contrib.image.translate(\n>             image, random_translation, 'NEAREST')\n>         return image\n>\n>     def call(self, input, training=True):\n>         \"\"\" Function that allows running a tensor through the pi model\n>         Arguments:\n>             input {[tensor]} -- batch of images\n>             training {bool} -- if true applies augmentaton and additive noise\n>         Returns:\n>             [tensor] -- predictions\n>         \"\"\"\n>\n>         if training:\n>             h = self.__aditive_gaussian_noise(input, 0.15)\n>             h = self.__apply_image_augmentation(h)\n>         else:\n>             h = input\n>\n>         h = self._conv1a(h, training)\n>         h = self._conv1b(h, training)\n>         h = self._conv1c(h, training)\n>         h = tf.layers.max_pooling2d(\n>             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n>         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n>\n>         h = self._conv2a(h, training)\n>         h = self._conv2b(h, training)\n>         h = self._conv2c(h, training)\n>         h = tf.layers.max_pooling2d(\n>             inputs=h,  pool_size=[2, 2], strides=[2, 2])\n>         h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n>\n>         h = self._conv3a(h, training)\n>         h = self._conv3b(h, training)\n>         h = self._conv3c(h, training)\n>\n>         # Average Pooling\n>         h = tf.reduce_mean(h, reduction_indices=[1, 2])\n>         return self._dense(h, training)\n>\n> The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for\n> weight and mean only batch normalization.\n>\n> \u2014\n> You are receiving this because you were assigned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/AAATxQ99sw39bY2iuzVECUFQUcf4NrUaks5ultrtgaJpZM4Xd7Ui>\n> .\n>\n\n\n-- \n - Alex\n"}