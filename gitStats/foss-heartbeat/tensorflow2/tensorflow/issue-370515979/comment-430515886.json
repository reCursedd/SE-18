{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/430515886", "html_url": "https://github.com/tensorflow/tensorflow/issues/23014#issuecomment-430515886", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23014", "id": 430515886, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMDUxNTg4Ng==", "user": {"login": "Goldesel23", "id": 10371630, "node_id": "MDQ6VXNlcjEwMzcxNjMw", "avatar_url": "https://avatars3.githubusercontent.com/u/10371630?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Goldesel23", "html_url": "https://github.com/Goldesel23", "followers_url": "https://api.github.com/users/Goldesel23/followers", "following_url": "https://api.github.com/users/Goldesel23/following{/other_user}", "gists_url": "https://api.github.com/users/Goldesel23/gists{/gist_id}", "starred_url": "https://api.github.com/users/Goldesel23/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Goldesel23/subscriptions", "organizations_url": "https://api.github.com/users/Goldesel23/orgs", "repos_url": "https://api.github.com/users/Goldesel23/repos", "events_url": "https://api.github.com/users/Goldesel23/events{/privacy}", "received_events_url": "https://api.github.com/users/Goldesel23/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-17T07:12:52Z", "updated_at": "2018-10-17T07:12:52Z", "author_association": "NONE", "body_html": "<p>Hi thanks for answering! This my model definition:</p>\n<pre><code>class PiModel(tf.keras.Model):\n    \"\"\" Class for defining eager compatible tfrecords file\n        I did not use tfe.Network since it will be depracated in the\n        future by tensorflow.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\" Init\n            Set all the layers that need to be tracked in the process of\n            gradients descent (pooling and dropout for example dont need\n            to be stored)\n        \"\"\"\n\n        super(PiModel, self).__init__()\n        self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n\n        self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n\n        self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\n                                                        padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n\n        self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, \n                                                     kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                     bias_initializer=tf.keras.initializers.constant(0.1),\n                                                     weight_norm=True, mean_only_batch_norm=True)\n\n    def __aditive_gaussian_noise(self, input, std):\n        \"\"\" Function to add additive zero mean noise as described in the paper\n        Arguments:\n            input {tensor} -- image\n            std {int} -- std to use in the random_normal\n        Returns:\n            {tensor} -- image with added noise\n        \"\"\"\n\n        noise = tf.random_normal(shape=tf.shape(\n            input), mean=0.0, stddev=std, dtype=tf.float32)\n        return input + noise\n\n    def __apply_image_augmentation(self, image):\n        \"\"\" Applies random transformation to the image\n        Arguments:\n            image {tensor} -- image\n        Returns:\n            {tensor} -- transformed image\n        \"\"\"\n\n        random_translation = tf.random_uniform(\n            [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\n        image = tf.contrib.image.translate(\n            image, random_translation, 'NEAREST')\n        return image\n\n    def call(self, input, training=True):\n        \"\"\" Function that allows running a tensor through the pi model\n        Arguments:\n            input {[tensor]} -- batch of images\n            training {bool} -- if true applies augmentaton and additive noise\n        Returns:\n            [tensor] -- predictions\n        \"\"\"\n\n        if training:\n            h = self.__aditive_gaussian_noise(input, 0.15)\n            h = self.__apply_image_augmentation(h)\n        else:\n            h = input\n\n        h = self._conv1a(h, training)\n        h = self._conv1b(h, training)\n        h = self._conv1c(h, training)\n        h = tf.layers.max_pooling2d(\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n        h = self._conv2a(h, training)\n        h = self._conv2b(h, training)\n        h = self._conv2c(h, training)\n        h = tf.layers.max_pooling2d(\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n        h = self._conv3a(h, training)\n        h = self._conv3b(h, training)\n        h = self._conv3c(h, training)\n\n        # Average Pooling\n        h = tf.reduce_mean(h, reduction_indices=[1, 2])\n        return self._dense(h, training)\n</code></pre>\n<p>The weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization.</p>", "body_text": "Hi thanks for answering! This my model definition:\nclass PiModel(tf.keras.Model):\n    \"\"\" Class for defining eager compatible tfrecords file\n        I did not use tfe.Network since it will be depracated in the\n        future by tensorflow.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\" Init\n            Set all the layers that need to be tracked in the process of\n            gradients descent (pooling and dropout for example dont need\n            to be stored)\n        \"\"\"\n\n        super(PiModel, self).__init__()\n        self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n\n        self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n\n        self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\n                                                        padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n        self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\n                                                        weight_norm=True, mean_only_batch_norm=True)\n\n        self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, \n                                                     kernel_initializer=tf.keras.initializers.he_uniform(),\n                                                     bias_initializer=tf.keras.initializers.constant(0.1),\n                                                     weight_norm=True, mean_only_batch_norm=True)\n\n    def __aditive_gaussian_noise(self, input, std):\n        \"\"\" Function to add additive zero mean noise as described in the paper\n        Arguments:\n            input {tensor} -- image\n            std {int} -- std to use in the random_normal\n        Returns:\n            {tensor} -- image with added noise\n        \"\"\"\n\n        noise = tf.random_normal(shape=tf.shape(\n            input), mean=0.0, stddev=std, dtype=tf.float32)\n        return input + noise\n\n    def __apply_image_augmentation(self, image):\n        \"\"\" Applies random transformation to the image\n        Arguments:\n            image {tensor} -- image\n        Returns:\n            {tensor} -- transformed image\n        \"\"\"\n\n        random_translation = tf.random_uniform(\n            [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\n        image = tf.contrib.image.translate(\n            image, random_translation, 'NEAREST')\n        return image\n\n    def call(self, input, training=True):\n        \"\"\" Function that allows running a tensor through the pi model\n        Arguments:\n            input {[tensor]} -- batch of images\n            training {bool} -- if true applies augmentaton and additive noise\n        Returns:\n            [tensor] -- predictions\n        \"\"\"\n\n        if training:\n            h = self.__aditive_gaussian_noise(input, 0.15)\n            h = self.__apply_image_augmentation(h)\n        else:\n            h = input\n\n        h = self._conv1a(h, training)\n        h = self._conv1b(h, training)\n        h = self._conv1c(h, training)\n        h = tf.layers.max_pooling2d(\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n        h = self._conv2a(h, training)\n        h = self._conv2b(h, training)\n        h = self._conv2c(h, training)\n        h = tf.layers.max_pooling2d(\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\n\n        h = self._conv3a(h, training)\n        h = self._conv3b(h, training)\n        h = self._conv3c(h, training)\n\n        # Average Pooling\n        h = tf.reduce_mean(h, reduction_indices=[1, 2])\n        return self._dense(h, training)\n\nThe weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization.", "body": "Hi thanks for answering! This my model definition:\r\n\r\n```\r\nclass PiModel(tf.keras.Model):\r\n    \"\"\" Class for defining eager compatible tfrecords file\r\n        I did not use tfe.Network since it will be depracated in the\r\n        future by tensorflow.\r\n    \"\"\"\r\n\r\n    def __init__(self):\r\n        \"\"\" Init\r\n            Set all the layers that need to be tracked in the process of\r\n            gradients descent (pooling and dropout for example dont need\r\n            to be stored)\r\n        \"\"\"\r\n\r\n        super(PiModel, self).__init__()\r\n        self._conv1a = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv1b = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv1c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n\r\n        self._conv2a = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv2b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv2c = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[3, 3],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n\r\n        self._conv3a = weight_norm_layers.Conv2D.Conv2D(filters=512, kernel_size=[3, 3],\r\n                                                        padding=\"valid\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv3b = weight_norm_layers.Conv2D.Conv2D(filters=256, kernel_size=[1, 1],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1),\r\n                                                         kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                         bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n        self._conv3c = weight_norm_layers.Conv2D.Conv2D(filters=128, kernel_size=[1, 1],\r\n                                                        padding=\"same\", activation=tf.keras.layers.LeakyReLU(alpha=0.1), \r\n                                                        kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                        bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                        weight_norm=True, mean_only_batch_norm=True)\r\n\r\n        self._dense = weight_norm_layers.Dense.Dense(units=10, activation=tf.nn.softmax, \r\n                                                     kernel_initializer=tf.keras.initializers.he_uniform(),\r\n                                                     bias_initializer=tf.keras.initializers.constant(0.1),\r\n                                                     weight_norm=True, mean_only_batch_norm=True)\r\n\r\n    def __aditive_gaussian_noise(self, input, std):\r\n        \"\"\" Function to add additive zero mean noise as described in the paper\r\n        Arguments:\r\n            input {tensor} -- image\r\n            std {int} -- std to use in the random_normal\r\n        Returns:\r\n            {tensor} -- image with added noise\r\n        \"\"\"\r\n\r\n        noise = tf.random_normal(shape=tf.shape(\r\n            input), mean=0.0, stddev=std, dtype=tf.float32)\r\n        return input + noise\r\n\r\n    def __apply_image_augmentation(self, image):\r\n        \"\"\" Applies random transformation to the image\r\n        Arguments:\r\n            image {tensor} -- image\r\n        Returns:\r\n            {tensor} -- transformed image\r\n        \"\"\"\r\n\r\n        random_translation = tf.random_uniform(\r\n            [1, 2], minval=-2.0, maxval=2.0, dtype=tf.float32)\r\n        image = tf.contrib.image.translate(\r\n            image, random_translation, 'NEAREST')\r\n        return image\r\n\r\n    def call(self, input, training=True):\r\n        \"\"\" Function that allows running a tensor through the pi model\r\n        Arguments:\r\n            input {[tensor]} -- batch of images\r\n            training {bool} -- if true applies augmentaton and additive noise\r\n        Returns:\r\n            [tensor] -- predictions\r\n        \"\"\"\r\n\r\n        if training:\r\n            h = self.__aditive_gaussian_noise(input, 0.15)\r\n            h = self.__apply_image_augmentation(h)\r\n        else:\r\n            h = input\r\n\r\n        h = self._conv1a(h, training)\r\n        h = self._conv1b(h, training)\r\n        h = self._conv1c(h, training)\r\n        h = tf.layers.max_pooling2d(\r\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\r\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\r\n\r\n        h = self._conv2a(h, training)\r\n        h = self._conv2b(h, training)\r\n        h = self._conv2c(h, training)\r\n        h = tf.layers.max_pooling2d(\r\n            inputs=h,  pool_size=[2, 2], strides=[2, 2])\r\n        h = tf.layers.dropout(inputs=h, rate=0.5, training=training)\r\n\r\n        h = self._conv3a(h, training)\r\n        h = self._conv3b(h, training)\r\n        h = self._conv3c(h, training)\r\n\r\n        # Average Pooling\r\n        h = tf.reduce_mean(h, reduction_indices=[1, 2])\r\n        return self._dense(h, training)\r\n```\r\n\r\nThe weight_norm_layers.Conv2D.Conv2D is just the edited wrapper layer for weight and mean only batch normalization."}