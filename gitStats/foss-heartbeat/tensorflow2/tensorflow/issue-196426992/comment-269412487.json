{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/269412487", "html_url": "https://github.com/tensorflow/tensorflow/issues/6399#issuecomment-269412487", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6399", "id": 269412487, "node_id": "MDEyOklzc3VlQ29tbWVudDI2OTQxMjQ4Nw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-28T02:00:08Z", "updated_at": "2016-12-28T02:00:08Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Off the top of my head, there are a few different possibilities here (although I'm sure there are others...):</p>\n<ul>\n<li>\n<p>The easiest would be to implement a write-once version of <code>tf.Variable</code>. It would still have an explicit <code>initialize</code> op, but it could be treated as a constant subsequently (e.g. in graph optimizations constant folding, CSE, etc.). Depending on how you want to use it, it could have blocking read semantics (block until initialized) or it could raise an error (like a normal <code>tf.Variable</code>). With the blocking read semantics, it would essentially be an <a href=\"http://www.cse.chalmers.se/edu/year/2016/course/pfp/Papers/IStructures.pdf\" rel=\"nofollow\">I-Structure</a> (see Section 5 for details), but it would also be another potential source of deadlock :).</p>\n</li>\n<li>\n<p>You could build a constant-like construct using a TensorFlow function (or perhaps a <code>tf.Variable</code> and <code>tf.cond()</code>, if you're not worried about concurrent assignments). On the first execution, it would call the function and store the result in the \"constant\", then produce that value. Subsequent executions would produce the same value. We discussed doing something like this for <code>tf.Variable</code> initialization a long time ago, but neither functions nor control flow were stable enough at the time.</p>\n</li>\n</ul>\n<p>TensorFlow's push semantics make it tricky to implement a construct that takes a <code>tf.Tensor</code> and conditionally executes the subgraph needed to produce that tensor. I'm sure it could be done, but it would require changes in the core executor, so either of the above two options would be preferable.</p>", "body_text": "Off the top of my head, there are a few different possibilities here (although I'm sure there are others...):\n\n\nThe easiest would be to implement a write-once version of tf.Variable. It would still have an explicit initialize op, but it could be treated as a constant subsequently (e.g. in graph optimizations constant folding, CSE, etc.). Depending on how you want to use it, it could have blocking read semantics (block until initialized) or it could raise an error (like a normal tf.Variable). With the blocking read semantics, it would essentially be an I-Structure (see Section 5 for details), but it would also be another potential source of deadlock :).\n\n\nYou could build a constant-like construct using a TensorFlow function (or perhaps a tf.Variable and tf.cond(), if you're not worried about concurrent assignments). On the first execution, it would call the function and store the result in the \"constant\", then produce that value. Subsequent executions would produce the same value. We discussed doing something like this for tf.Variable initialization a long time ago, but neither functions nor control flow were stable enough at the time.\n\n\nTensorFlow's push semantics make it tricky to implement a construct that takes a tf.Tensor and conditionally executes the subgraph needed to produce that tensor. I'm sure it could be done, but it would require changes in the core executor, so either of the above two options would be preferable.", "body": "Off the top of my head, there are a few different possibilities here (although I'm sure there are others...):\r\n\r\n* The easiest would be to implement a write-once version of `tf.Variable`. It would still have an explicit `initialize` op, but it could be treated as a constant subsequently (e.g. in graph optimizations constant folding, CSE, etc.). Depending on how you want to use it, it could have blocking read semantics (block until initialized) or it could raise an error (like a normal `tf.Variable`). With the blocking read semantics, it would essentially be an [I-Structure](http://www.cse.chalmers.se/edu/year/2016/course/pfp/Papers/IStructures.pdf) (see Section 5 for details), but it would also be another potential source of deadlock :).\r\n\r\n* You could build a constant-like construct using a TensorFlow function (or perhaps a `tf.Variable` and `tf.cond()`, if you're not worried about concurrent assignments). On the first execution, it would call the function and store the result in the \"constant\", then produce that value. Subsequent executions would produce the same value. We discussed doing something like this for `tf.Variable` initialization a long time ago, but neither functions nor control flow were stable enough at the time.\r\n\r\nTensorFlow's push semantics make it tricky to implement a construct that takes a `tf.Tensor` and conditionally executes the subgraph needed to produce that tensor. I'm sure it could be done, but it would require changes in the core executor, so either of the above two options would be preferable."}