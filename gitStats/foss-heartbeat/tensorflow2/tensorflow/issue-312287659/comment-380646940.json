{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380646940", "html_url": "https://github.com/tensorflow/tensorflow/issues/18323#issuecomment-380646940", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18323", "id": 380646940, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDY0Njk0MA==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-12T01:36:53Z", "updated_at": "2018-04-12T01:36:53Z", "author_association": "CONTRIBUTOR", "body_html": "<div class=\"email-fragment\">print(tf.get_default_graph().as_graph_def()) to file between each session\nand diff the files to see what's out of order.</div>\n<span class=\"email-hidden-toggle\"><a href=\"#\">\u2026</a></span><div class=\"email-hidden-reply\">\n<div class=\"email-quoted-reply\">On Wed, Apr 11, 2018, 2:21 PM Hesam Moshiri ***@***.***&gt; wrote:\n <a class=\"user-mention\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> &lt;<a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a>&gt;\n\n I trained both LSTM and GRU for 10 times in a loop. both for 5 epochs.\n everything is identical for both.\n\n *GRU:*\n\n All 10 runs handle the same results as below and all are identical:\n\n Epoch 1/5\n  - 2s - loss: 0.0085 - val_loss: 0.0011\n Epoch 2/5\n  - 1s - loss: 0.0087 - val_loss: 1.4272e-04\n Epoch 3/5\n  - 1s - loss: 0.0049 - val_loss: 5.7577e-05\n Epoch 4/5\n  - 1s - loss: 0.0025 - val_loss: 7.7277e-04\n Epoch 5/5\n  - 1s - loss: 0.0013 - val_loss: 0.0019\n\n *LSTM:*\n\n Within 10 runs, sometimes it delivers the results like A and sometimes\n like B. it is something like rotary switch that randomly goes to A or to B.\n\n A:\n\n Epoch 1/5\n  - 2s - loss: 0.0146 - val_loss: 6.0151e-04\n Epoch 2/5\n  - 1s - loss: 0.0111 - val_loss: 1.2672e-04\n Epoch 3/5\n  - 1s - loss: 0.0071 - val_loss: 2.7325e-04\n Epoch 4/5\n  - 1s - loss: 0.0041 - val_loss: 6.2337e-04\n Epoch 5/5\n  - 1s - loss: 0.0021 - val_loss: 8.3224e-04\n\n B:\n\n Epoch 1/5\n  - 2s - loss: 0.0146 - val_loss: 6.0327e-04\n Epoch 2/5\n  - 1s - loss: 0.0111 - val_loss: 1.2939e-04\n Epoch 3/5\n  - 1s - loss: 0.0071 - val_loss: 2.7630e-04\n Epoch 4/5\n  - 1s - loss: 0.0041 - val_loss: 6.2675e-04\n Epoch 5/5\n  - 2s - loss: 0.0021 - val_loss: 8.3687e-04\n\n Can you use tf.reset_default_graph() between iterations?\n\n I tried it. unfortunately it did not change the situation.\n\n Can you compare the graphdefs generated for each entry of the loop and\n ensure the ops are created in the same order?\n\n I could not get the point. Would you please explain it more?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n &lt;<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"312287659\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/18323\" href=\"https://github.com/tensorflow/tensorflow/issues/18323#issuecomment-380599426\">#18323 (comment)</a>&gt;,\n or mute the thread\n &lt;<a href=\"https://github.com/notifications/unsubscribe-auth/ABtimw7NPqU6WCFl0edPxY8ckjqlneq3ks5tnnPsgaJpZM4TLdQt\">https://github.com/notifications/unsubscribe-auth/ABtimw7NPqU6WCFl0edPxY8ckjqlneq3ks5tnnPsgaJpZM4TLdQt</a>&gt;\n .\n</div>\n<div class=\"email-fragment\"></div>\n</div>", "body_text": "print(tf.get_default_graph().as_graph_def()) to file between each session\nand diff the files to see what's out of order.\n\u2026\nOn Wed, Apr 11, 2018, 2:21 PM Hesam Moshiri ***@***.***> wrote:\n @ebrevdo <https://github.com/ebrevdo>\n\n I trained both LSTM and GRU for 10 times in a loop. both for 5 epochs.\n everything is identical for both.\n\n *GRU:*\n\n All 10 runs handle the same results as below and all are identical:\n\n Epoch 1/5\n  - 2s - loss: 0.0085 - val_loss: 0.0011\n Epoch 2/5\n  - 1s - loss: 0.0087 - val_loss: 1.4272e-04\n Epoch 3/5\n  - 1s - loss: 0.0049 - val_loss: 5.7577e-05\n Epoch 4/5\n  - 1s - loss: 0.0025 - val_loss: 7.7277e-04\n Epoch 5/5\n  - 1s - loss: 0.0013 - val_loss: 0.0019\n\n *LSTM:*\n\n Within 10 runs, sometimes it delivers the results like A and sometimes\n like B. it is something like rotary switch that randomly goes to A or to B.\n\n A:\n\n Epoch 1/5\n  - 2s - loss: 0.0146 - val_loss: 6.0151e-04\n Epoch 2/5\n  - 1s - loss: 0.0111 - val_loss: 1.2672e-04\n Epoch 3/5\n  - 1s - loss: 0.0071 - val_loss: 2.7325e-04\n Epoch 4/5\n  - 1s - loss: 0.0041 - val_loss: 6.2337e-04\n Epoch 5/5\n  - 1s - loss: 0.0021 - val_loss: 8.3224e-04\n\n B:\n\n Epoch 1/5\n  - 2s - loss: 0.0146 - val_loss: 6.0327e-04\n Epoch 2/5\n  - 1s - loss: 0.0111 - val_loss: 1.2939e-04\n Epoch 3/5\n  - 1s - loss: 0.0071 - val_loss: 2.7630e-04\n Epoch 4/5\n  - 1s - loss: 0.0041 - val_loss: 6.2675e-04\n Epoch 5/5\n  - 2s - loss: 0.0021 - val_loss: 8.3687e-04\n\n Can you use tf.reset_default_graph() between iterations?\n\n I tried it. unfortunately it did not change the situation.\n\n Can you compare the graphdefs generated for each entry of the loop and\n ensure the ops are created in the same order?\n\n I could not get the point. Would you please explain it more?\n\n \u2014\n You are receiving this because you were mentioned.\n Reply to this email directly, view it on GitHub\n <#18323 (comment)>,\n or mute the thread\n <https://github.com/notifications/unsubscribe-auth/ABtimw7NPqU6WCFl0edPxY8ckjqlneq3ks5tnnPsgaJpZM4TLdQt>\n .", "body": "print(tf.get_default_graph().as_graph_def()) to file between each session\nand diff the files to see what's out of order.\n\nOn Wed, Apr 11, 2018, 2:21 PM Hesam Moshiri <notifications@github.com>\nwrote:\n\n> @ebrevdo <https://github.com/ebrevdo>\n>\n> I trained both LSTM and GRU for 10 times in a loop. both for 5 epochs.\n> everything is identical for both.\n>\n> *GRU:*\n>\n> All 10 runs handle the same results as below and all are identical:\n>\n> Epoch 1/5\n>  - 2s - loss: 0.0085 - val_loss: 0.0011\n> Epoch 2/5\n>  - 1s - loss: 0.0087 - val_loss: 1.4272e-04\n> Epoch 3/5\n>  - 1s - loss: 0.0049 - val_loss: 5.7577e-05\n> Epoch 4/5\n>  - 1s - loss: 0.0025 - val_loss: 7.7277e-04\n> Epoch 5/5\n>  - 1s - loss: 0.0013 - val_loss: 0.0019\n>\n> *LSTM:*\n>\n> Within 10 runs, sometimes it delivers the results like A and sometimes\n> like B. it is something like rotary switch that randomly goes to A or to B.\n>\n> A:\n>\n> Epoch 1/5\n>  - 2s - loss: 0.0146 - val_loss: 6.0151e-04\n> Epoch 2/5\n>  - 1s - loss: 0.0111 - val_loss: 1.2672e-04\n> Epoch 3/5\n>  - 1s - loss: 0.0071 - val_loss: 2.7325e-04\n> Epoch 4/5\n>  - 1s - loss: 0.0041 - val_loss: 6.2337e-04\n> Epoch 5/5\n>  - 1s - loss: 0.0021 - val_loss: 8.3224e-04\n>\n> B:\n>\n> Epoch 1/5\n>  - 2s - loss: 0.0146 - val_loss: 6.0327e-04\n> Epoch 2/5\n>  - 1s - loss: 0.0111 - val_loss: 1.2939e-04\n> Epoch 3/5\n>  - 1s - loss: 0.0071 - val_loss: 2.7630e-04\n> Epoch 4/5\n>  - 1s - loss: 0.0041 - val_loss: 6.2675e-04\n> Epoch 5/5\n>  - 2s - loss: 0.0021 - val_loss: 8.3687e-04\n>\n> Can you use tf.reset_default_graph() between iterations?\n>\n> I tried it. unfortunately it did not change the situation.\n>\n> Can you compare the graphdefs generated for each entry of the loop and\n> ensure the ops are created in the same order?\n>\n> I could not get the point. Would you please explain it more?\n>\n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> <https://github.com/tensorflow/tensorflow/issues/18323#issuecomment-380599426>,\n> or mute the thread\n> <https://github.com/notifications/unsubscribe-auth/ABtimw7NPqU6WCFl0edPxY8ckjqlneq3ks5tnnPsgaJpZM4TLdQt>\n> .\n>\n"}