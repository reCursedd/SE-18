{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/387226123", "html_url": "https://github.com/tensorflow/tensorflow/issues/18323#issuecomment-387226123", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18323", "id": 387226123, "node_id": "MDEyOklzc3VlQ29tbWVudDM4NzIyNjEyMw==", "user": {"login": "VanitarNordic", "id": 18719591, "node_id": "MDQ6VXNlcjE4NzE5NTkx", "avatar_url": "https://avatars0.githubusercontent.com/u/18719591?v=4", "gravatar_id": "", "url": "https://api.github.com/users/VanitarNordic", "html_url": "https://github.com/VanitarNordic", "followers_url": "https://api.github.com/users/VanitarNordic/followers", "following_url": "https://api.github.com/users/VanitarNordic/following{/other_user}", "gists_url": "https://api.github.com/users/VanitarNordic/gists{/gist_id}", "starred_url": "https://api.github.com/users/VanitarNordic/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/VanitarNordic/subscriptions", "organizations_url": "https://api.github.com/users/VanitarNordic/orgs", "repos_url": "https://api.github.com/users/VanitarNordic/repos", "events_url": "https://api.github.com/users/VanitarNordic/events{/privacy}", "received_events_url": "https://api.github.com/users/VanitarNordic/received_events", "type": "User", "site_admin": false}, "created_at": "2018-05-07T22:27:31Z", "updated_at": "2018-05-26T21:57:13Z", "author_association": "NONE", "body_html": "<p>Well, the problem still exists but now it happens when we add a <code>Dropout</code> layer. The results are all identical for the GRU, but switches between two values for the LSTM !!</p>\n<p>You don't want to hire me as a software tester?!</p>\n<p>========================================</p>\n<p>I attached the database as a numpy array. so you can import it to the <code>Spyder</code> variables explorer and run the code and consider the training results as I provided below, You can also switch the LSTM with GRU and see the results.</p>\n<ul>\n<li>Create an Anaconda python environment with python=3.6.4</li>\n<li>install Tensorflow (1.8), Keras (2.1.6) and other dependencies.</li>\n<li>install the Spyder</li>\n<li>unzip and import the attached numpy array to the Spyder variable explorer: <code>values.npy</code><br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/1981667/values.zip\">values.zip</a></li>\n</ul>\n<div class=\"highlight highlight-text-python-traceback\"><pre>import os\nos.environ['PYTHONHASHSEED'] = '0'\nimport numpy as np\nnp.random.seed(12)\nimport random as rn\nrn.seed(123)\nimport tensorflow as tf\ntf.set_random_seed(1234)\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nfrom keras import backend as K\n\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n\nK.set_session(sess)\n\nfrom keras import initializers\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, Dense\n\nlag = 5\n\ntrain = values[:(450 - lag - 1), :]\ntest = values[(450 - lag - 1):, :]\n\nnum_parameters = 5\nfeature = num_parameters - 1\n\ntrain_X, train_y = train[:, 0:num_parameters * lag], train[:, feature - num_parameters]\ntrain_X = train_X.reshape((train_X.shape[0], lag, num_parameters))\n\ntest_X, test_y = test[:, 0:num_parameters * lag], test[:, feature - num_parameters]\ntest_X = test_X.reshape((test_X.shape[0], lag, num_parameters))\n\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n\ntbCallBack = TensorBoard(log_dir='logs', histogram_freq=0, batch_size=32,\nwrite_graph=True, write_grads=False, write_images=True, embeddings_freq=0, \nembeddings_layer_names=None, embeddings_metadata=None)\n\n#===============================================\n\nfor counter in range(1, 11):    \n            \n    model <span class=\"pl-k\">=</span> Sequential()\n    model.add(LSTM(<span class=\"pl-c1\">60</span>, <span class=\"pl-v\">input_shape</span><span class=\"pl-k\">=</span>(lag, num_parameters), <span class=\"pl-v\">return_sequences</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>))\n    model.add(Dropout(<span class=\"pl-c1\">0.05</span>))\n    model.add(Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span> <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>linear<span class=\"pl-pds\">\"</span></span>))\n    model.compile(<span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>mse<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>)\n    model.summary()\n    history <span class=\"pl-k\">=</span> model.fit(train_X, train_y, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">validation_data</span><span class=\"pl-k\">=</span>(test_X, test_y), \n                    verbose <span class=\"pl-k\">=</span> <span class=\"pl-c1\">2</span>, shuffle <span class=\"pl-k\">=</span> <span class=\"pl-c1\">False</span>, callbacks<span class=\"pl-k\">=</span>[tbCallBack])    \n          \n    <span class=\"pl-k\">with</span> <span class=\"pl-c1\">open</span>(<span class=\"pl-c1\">str</span>(counter) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>.txt<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">as</span> f:        \n        <span class=\"pl-c1\">print</span>(tf.get_default_graph().as_graph_def(), <span class=\"pl-v\">file</span><span class=\"pl-k\">=</span>f)\n        \n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Counter=<span class=\"pl-pds\">\"</span></span>, counter)    \n    <span class=\"pl-k\">del</span> model\n    K.clear_session() \n    tf.reset_default_graph()\n    np.random.seed(<span class=\"pl-c1\">12</span>)    \n    rn.seed(<span class=\"pl-c1\">123</span>)    \n    tf.set_random_seed(<span class=\"pl-c1\">1234</span>)</pre></div>\n<p><strong>------------- The results you should get-----------------------</strong></p>\n<p><strong>GRU:</strong></p>\n<p>All 10 runs handle the same results as below and all are identical:</p>\n<pre><code>Epoch 1/5\n - 2s - loss: 0.0023 - val_loss: 0.0037\nEpoch 2/5\n - 2s - loss: 0.0034 - val_loss: 2.9823e-04\nEpoch 3/5\n - 1s - loss: 0.0022 - val_loss: 0.0025\nEpoch 4/5\n - 1s - loss: 0.0019 - val_loss: 0.0013\nEpoch 5/5\n - 1s - loss: 0.0016 - val_loss: 1.0047e-04\n</code></pre>\n<p><strong>LSTM:</strong></p>\n<p>Within 10 runs, sometimes it delivers the results like A and sometimes like B. it is something like a rotary switch that randomly goes to A or B.</p>\n<p><strong>A:</strong></p>\n<pre><code>Epoch 1/5\n - 2s - loss: 0.0021 - val_loss: 0.0101\nEpoch 2/5\n - 2s - loss: 0.0058 - val_loss: 2.3162e-04\nEpoch 3/5\n - 2s - loss: 0.0053 - val_loss: 3.7952e-04\nEpoch 4/5\n - 2s - loss: 0.0044 - val_loss: 3.2818e-04\nEpoch 5/5\n - 2s - loss: 0.0042 - val_loss: 1.4068e-04\n</code></pre>\n<p><strong>B:</strong></p>\n<pre><code>Epoch 1/5\n - 2s - loss: 0.0020 - val_loss: 0.0101\nEpoch 2/5\n - 2s - loss: 0.0057 - val_loss: 2.4354e-04\nEpoch 3/5\n - 2s - loss: 0.0052 - val_loss: 3.7617e-04\nEpoch 4/5\n - 2s - loss: 0.0044 - val_loss: 3.7726e-04\nEpoch 5/5\n - 2s - loss: 0.0042 - val_loss: 1.4422e-04\n</code></pre>", "body_text": "Well, the problem still exists but now it happens when we add a Dropout layer. The results are all identical for the GRU, but switches between two values for the LSTM !!\nYou don't want to hire me as a software tester?!\n========================================\nI attached the database as a numpy array. so you can import it to the Spyder variables explorer and run the code and consider the training results as I provided below, You can also switch the LSTM with GRU and see the results.\n\nCreate an Anaconda python environment with python=3.6.4\ninstall Tensorflow (1.8), Keras (2.1.6) and other dependencies.\ninstall the Spyder\nunzip and import the attached numpy array to the Spyder variable explorer: values.npy\nvalues.zip\n\nimport os\nos.environ['PYTHONHASHSEED'] = '0'\nimport numpy as np\nnp.random.seed(12)\nimport random as rn\nrn.seed(123)\nimport tensorflow as tf\ntf.set_random_seed(1234)\n\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\nfrom keras import backend as K\n\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n\nK.set_session(sess)\n\nfrom keras import initializers\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, GRU, Dense\n\nlag = 5\n\ntrain = values[:(450 - lag - 1), :]\ntest = values[(450 - lag - 1):, :]\n\nnum_parameters = 5\nfeature = num_parameters - 1\n\ntrain_X, train_y = train[:, 0:num_parameters * lag], train[:, feature - num_parameters]\ntrain_X = train_X.reshape((train_X.shape[0], lag, num_parameters))\n\ntest_X, test_y = test[:, 0:num_parameters * lag], test[:, feature - num_parameters]\ntest_X = test_X.reshape((test_X.shape[0], lag, num_parameters))\n\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\n\ntbCallBack = TensorBoard(log_dir='logs', histogram_freq=0, batch_size=32,\nwrite_graph=True, write_grads=False, write_images=True, embeddings_freq=0, \nembeddings_layer_names=None, embeddings_metadata=None)\n\n#===============================================\n\nfor counter in range(1, 11):    \n            \n    model = Sequential()\n    model.add(LSTM(60, input_shape=(lag, num_parameters), return_sequences=False))\n    model.add(Dropout(0.05))\n    model.add(Dense(1, activation = \"linear\"))\n    model.compile(loss='mse', optimizer='adam')\n    model.summary()\n    history = model.fit(train_X, train_y, epochs=5, batch_size=1, validation_data=(test_X, test_y), \n                    verbose = 2, shuffle = False, callbacks=[tbCallBack])    \n          \n    with open(str(counter) + \".txt\", 'w') as f:        \n        print(tf.get_default_graph().as_graph_def(), file=f)\n        \n    print(\"Counter=\", counter)    \n    del model\n    K.clear_session() \n    tf.reset_default_graph()\n    np.random.seed(12)    \n    rn.seed(123)    \n    tf.set_random_seed(1234)\n------------- The results you should get-----------------------\nGRU:\nAll 10 runs handle the same results as below and all are identical:\nEpoch 1/5\n - 2s - loss: 0.0023 - val_loss: 0.0037\nEpoch 2/5\n - 2s - loss: 0.0034 - val_loss: 2.9823e-04\nEpoch 3/5\n - 1s - loss: 0.0022 - val_loss: 0.0025\nEpoch 4/5\n - 1s - loss: 0.0019 - val_loss: 0.0013\nEpoch 5/5\n - 1s - loss: 0.0016 - val_loss: 1.0047e-04\n\nLSTM:\nWithin 10 runs, sometimes it delivers the results like A and sometimes like B. it is something like a rotary switch that randomly goes to A or B.\nA:\nEpoch 1/5\n - 2s - loss: 0.0021 - val_loss: 0.0101\nEpoch 2/5\n - 2s - loss: 0.0058 - val_loss: 2.3162e-04\nEpoch 3/5\n - 2s - loss: 0.0053 - val_loss: 3.7952e-04\nEpoch 4/5\n - 2s - loss: 0.0044 - val_loss: 3.2818e-04\nEpoch 5/5\n - 2s - loss: 0.0042 - val_loss: 1.4068e-04\n\nB:\nEpoch 1/5\n - 2s - loss: 0.0020 - val_loss: 0.0101\nEpoch 2/5\n - 2s - loss: 0.0057 - val_loss: 2.4354e-04\nEpoch 3/5\n - 2s - loss: 0.0052 - val_loss: 3.7617e-04\nEpoch 4/5\n - 2s - loss: 0.0044 - val_loss: 3.7726e-04\nEpoch 5/5\n - 2s - loss: 0.0042 - val_loss: 1.4422e-04", "body": "Well, the problem still exists but now it happens when we add a `Dropout` layer. The results are all identical for the GRU, but switches between two values for the LSTM !!\r\n\r\nYou don't want to hire me as a software tester?!\r\n\r\n========================================\r\n\r\nI attached the database as a numpy array. so you can import it to the `Spyder` variables explorer and run the code and consider the training results as I provided below, You can also switch the LSTM with GRU and see the results.\r\n\r\n- Create an Anaconda python environment with python=3.6.4\r\n- install Tensorflow (1.8), Keras (2.1.6) and other dependencies.\r\n- install the Spyder\r\n- unzip and import the attached numpy array to the Spyder variable explorer: `values.npy`\r\n[values.zip](https://github.com/tensorflow/tensorflow/files/1981667/values.zip)\r\n\r\n\r\n```python-traceback\r\nimport os\r\nos.environ['PYTHONHASHSEED'] = '0'\r\nimport numpy as np\r\nnp.random.seed(12)\r\nimport random as rn\r\nrn.seed(123)\r\nimport tensorflow as tf\r\ntf.set_random_seed(1234)\r\n\r\nsession_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\r\nfrom keras import backend as K\r\n\r\nsess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\r\n\r\nK.set_session(sess)\r\n\r\nfrom keras import initializers\r\nfrom keras.optimizers import Adam\r\nfrom keras.callbacks import EarlyStopping, TensorBoard\r\nfrom keras.models import Sequential\r\nfrom keras.layers import LSTM, GRU, Dense\r\n\r\nlag = 5\r\n\r\ntrain = values[:(450 - lag - 1), :]\r\ntest = values[(450 - lag - 1):, :]\r\n\r\nnum_parameters = 5\r\nfeature = num_parameters - 1\r\n\r\ntrain_X, train_y = train[:, 0:num_parameters * lag], train[:, feature - num_parameters]\r\ntrain_X = train_X.reshape((train_X.shape[0], lag, num_parameters))\r\n\r\ntest_X, test_y = test[:, 0:num_parameters * lag], test[:, feature - num_parameters]\r\ntest_X = test_X.reshape((test_X.shape[0], lag, num_parameters))\r\n\r\nprint(train_X.shape, train_y.shape, test_X.shape, test_y.shape)\r\n\r\ntbCallBack = TensorBoard(log_dir='logs', histogram_freq=0, batch_size=32,\r\nwrite_graph=True, write_grads=False, write_images=True, embeddings_freq=0, \r\nembeddings_layer_names=None, embeddings_metadata=None)\r\n\r\n#===============================================\r\n\r\nfor counter in range(1, 11):    \r\n            \r\n    model = Sequential()\r\n    model.add(LSTM(60, input_shape=(lag, num_parameters), return_sequences=False))\r\n    model.add(Dropout(0.05))\r\n    model.add(Dense(1, activation = \"linear\"))\r\n    model.compile(loss='mse', optimizer='adam')\r\n    model.summary()\r\n    history = model.fit(train_X, train_y, epochs=5, batch_size=1, validation_data=(test_X, test_y), \r\n                    verbose = 2, shuffle = False, callbacks=[tbCallBack])    \r\n          \r\n    with open(str(counter) + \".txt\", 'w') as f:        \r\n        print(tf.get_default_graph().as_graph_def(), file=f)\r\n        \r\n    print(\"Counter=\", counter)    \r\n    del model\r\n    K.clear_session() \r\n    tf.reset_default_graph()\r\n    np.random.seed(12)    \r\n    rn.seed(123)    \r\n    tf.set_random_seed(1234)\r\n```\r\n\r\n\r\n**------------- The results you should get-----------------------**\r\n\r\n**GRU:**\r\n\r\nAll 10 runs handle the same results as below and all are identical:\r\n\r\n```\r\nEpoch 1/5\r\n - 2s - loss: 0.0023 - val_loss: 0.0037\r\nEpoch 2/5\r\n - 2s - loss: 0.0034 - val_loss: 2.9823e-04\r\nEpoch 3/5\r\n - 1s - loss: 0.0022 - val_loss: 0.0025\r\nEpoch 4/5\r\n - 1s - loss: 0.0019 - val_loss: 0.0013\r\nEpoch 5/5\r\n - 1s - loss: 0.0016 - val_loss: 1.0047e-04\r\n```\r\n\r\n\r\n**LSTM:**\r\n\r\nWithin 10 runs, sometimes it delivers the results like A and sometimes like B. it is something like a rotary switch that randomly goes to A or B.\r\n\r\n**A:**\r\n\r\n```\r\nEpoch 1/5\r\n - 2s - loss: 0.0021 - val_loss: 0.0101\r\nEpoch 2/5\r\n - 2s - loss: 0.0058 - val_loss: 2.3162e-04\r\nEpoch 3/5\r\n - 2s - loss: 0.0053 - val_loss: 3.7952e-04\r\nEpoch 4/5\r\n - 2s - loss: 0.0044 - val_loss: 3.2818e-04\r\nEpoch 5/5\r\n - 2s - loss: 0.0042 - val_loss: 1.4068e-04\r\n```\r\n\r\n**B:**\r\n\r\n```\r\nEpoch 1/5\r\n - 2s - loss: 0.0020 - val_loss: 0.0101\r\nEpoch 2/5\r\n - 2s - loss: 0.0057 - val_loss: 2.4354e-04\r\nEpoch 3/5\r\n - 2s - loss: 0.0052 - val_loss: 3.7617e-04\r\nEpoch 4/5\r\n - 2s - loss: 0.0044 - val_loss: 3.7726e-04\r\nEpoch 5/5\r\n - 2s - loss: 0.0042 - val_loss: 1.4422e-04\r\n```"}