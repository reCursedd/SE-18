{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/307316326", "html_url": "https://github.com/tensorflow/tensorflow/issues/10491#issuecomment-307316326", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10491", "id": 307316326, "node_id": "MDEyOklzc3VlQ29tbWVudDMwNzMxNjMyNg==", "user": {"login": "Scitator", "id": 7606451, "node_id": "MDQ6VXNlcjc2MDY0NTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7606451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Scitator", "html_url": "https://github.com/Scitator", "followers_url": "https://api.github.com/users/Scitator/followers", "following_url": "https://api.github.com/users/Scitator/following{/other_user}", "gists_url": "https://api.github.com/users/Scitator/gists{/gist_id}", "starred_url": "https://api.github.com/users/Scitator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Scitator/subscriptions", "organizations_url": "https://api.github.com/users/Scitator/orgs", "repos_url": "https://api.github.com/users/Scitator/repos", "events_url": "https://api.github.com/users/Scitator/events{/privacy}", "received_events_url": "https://api.github.com/users/Scitator/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-09T07:25:23Z", "updated_at": "2017-06-09T07:25:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a></p>\n<blockquote>\n<p>This is not a limitation since all TensorFlow layers are in Keras (albeit the reverse is not true).</p>\n</blockquote>\n<p>No sure about this. TF have huge variety of RNN Cells and other important stuff (decoders) for seq2seq model, which I personally interested in. Nevertheless Keras is extremely helpful in prototyping feed-forward-like models with it's ConvLSTM2D and other primitives.</p>\n<p>PS. For the sake of interest: are there any best practices for TF-Keras seq2seq models other than <a href=\"https://github.com/farizrahman4u/seq2seq\">these one</a>?</p>", "body_text": "@fchollet\n\nThis is not a limitation since all TensorFlow layers are in Keras (albeit the reverse is not true).\n\nNo sure about this. TF have huge variety of RNN Cells and other important stuff (decoders) for seq2seq model, which I personally interested in. Nevertheless Keras is extremely helpful in prototyping feed-forward-like models with it's ConvLSTM2D and other primitives.\nPS. For the sake of interest: are there any best practices for TF-Keras seq2seq models other than these one?", "body": "@fchollet \r\n> This is not a limitation since all TensorFlow layers are in Keras (albeit the reverse is not true).\r\n\r\nNo sure about this. TF have huge variety of RNN Cells and other important stuff (decoders) for seq2seq model, which I personally interested in. Nevertheless Keras is extremely helpful in prototyping feed-forward-like models with it's ConvLSTM2D and other primitives.\r\n\r\nPS. For the sake of interest: are there any best practices for TF-Keras seq2seq models other than [these one](https://github.com/farizrahman4u/seq2seq)?"}