{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10491", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10491/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10491/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10491/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10491", "id": 234194127, "node_id": "MDU6SXNzdWUyMzQxOTQxMjc=", "number": 10491, "title": "TF-Keras dont see TF variables", "user": {"login": "Scitator", "id": 7606451, "node_id": "MDQ6VXNlcjc2MDY0NTE=", "avatar_url": "https://avatars0.githubusercontent.com/u/7606451?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Scitator", "html_url": "https://github.com/Scitator", "followers_url": "https://api.github.com/users/Scitator/followers", "following_url": "https://api.github.com/users/Scitator/following{/other_user}", "gists_url": "https://api.github.com/users/Scitator/gists{/gist_id}", "starred_url": "https://api.github.com/users/Scitator/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Scitator/subscriptions", "organizations_url": "https://api.github.com/users/Scitator/orgs", "repos_url": "https://api.github.com/users/Scitator/repos", "events_url": "https://api.github.com/users/Scitator/events{/privacy}", "received_events_url": "https://api.github.com/users/Scitator/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2017-06-07T12:35:13Z", "updated_at": "2017-06-09T07:25:42Z", "closed_at": "2017-06-08T17:40:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It is a feature or a bug?</p>\n<p>For example, I have 2 models:<br>\ntypical imports:</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib.keras import layers as klayers\nfrom tensorflow.contrib.keras import activations as kacts\nfrom tensorflow.contrib.keras import models as kmodels\nfrom tensorflow.contrib.keras import optimizers as kopts\nfrom tensorflow.contrib.keras import losses as kloss\nfrom tensorflow.contrib.keras import backend as kback\n</code></pre>\n<p>Keras model</p>\n<pre><code># keras model\nfeatures = klayers.Input(shape=(28, 28, 1))\nx = klayers.Conv2D(64, 3, strides=(2, 2))(features)\nx = klayers.MaxPool2D()(x)\nx = klayers.Conv2D(32, 3, strides=(2, 2))(x)\nx = klayers.Flatten()(x)\nprelogits = klayers.Dense(128, activation=kacts.elu)(x)\npred = klayers.Dense(10, activation=kacts.softmax)(prelogits)\n</code></pre>\n<p>and TF one:</p>\n<pre><code>features = klayers.Input(shape=(28, 28, 1))\ndef convolution_network(\n        states, n_filters=None, kernels=None, strides=None,\n        activation_fn=tf.nn.elu, use_bn=False, dropout=-1):\n    n_filters = n_filters or [64, 32]\n    kernels = kernels or [3, 3]\n    strides = strides or [2, 2]\n    x = states\n    for n_filter, kernel, stride in zip(n_filters, kernels, strides):\n        x = tf.layers.conv2d(x, n_filter, kernel, stride, activation=None)\n        if use_bn:\n            x = tf.layers.batch_normalization(x, training=is_training)\n        x = tf.layers.max_pooling2d(x, 2, 2)\n        x = activation_fn(x)\n        if dropout &gt; 0:\n            x = tf.layers.dropout(x, rate=dropout, training=is_training)\n    x = tf.contrib.layers.flatten(x)\n    return x\n\ndef simple_model(features):\n    features = convolution_network(features)\n    prelogits = tf.layers.dense(features, 128, activation=tf.nn.elu)\n    logits = tf.layers.dense(prelogits, 10, activation=tf.nn.softmax)\n    \n    return logits\n\npred = tf.contrib.keras.layers.Lambda(simple_model)(features)\n</code></pre>\n<p>Both of them then made with <code>model = kmodels.Model(inputs=features, outputs=pred)</code></p>\n<p>Nevertheless, when I use <code>model.summary()</code> , Kesar model give me correct output:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        640       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 2, 2, 32)          18464     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 36,906\nTrainable params: 36,906\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n<p><strong>but</strong> TF one, give only:</p>\n<pre><code>_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \n_________________________________________________________________\nlambda_1 (Lambda)            (None, 10)                0         \n=================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n_________________________________________________________________\n</code></pre>\n<p>Is it correct, that TF-Keras cannot see TF variables and optimize them? So we still have separated Tensorflow an Keras frameworks?<br>\nWhy cannot I use TF as Keras backend?<br>\nTF-versions: from 1.1.0 to 1.2.0rc1</p>", "body_text": "It is a feature or a bug?\nFor example, I have 2 models:\ntypical imports:\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nfrom tensorflow.contrib.keras import layers as klayers\nfrom tensorflow.contrib.keras import activations as kacts\nfrom tensorflow.contrib.keras import models as kmodels\nfrom tensorflow.contrib.keras import optimizers as kopts\nfrom tensorflow.contrib.keras import losses as kloss\nfrom tensorflow.contrib.keras import backend as kback\n\nKeras model\n# keras model\nfeatures = klayers.Input(shape=(28, 28, 1))\nx = klayers.Conv2D(64, 3, strides=(2, 2))(features)\nx = klayers.MaxPool2D()(x)\nx = klayers.Conv2D(32, 3, strides=(2, 2))(x)\nx = klayers.Flatten()(x)\nprelogits = klayers.Dense(128, activation=kacts.elu)(x)\npred = klayers.Dense(10, activation=kacts.softmax)(prelogits)\n\nand TF one:\nfeatures = klayers.Input(shape=(28, 28, 1))\ndef convolution_network(\n        states, n_filters=None, kernels=None, strides=None,\n        activation_fn=tf.nn.elu, use_bn=False, dropout=-1):\n    n_filters = n_filters or [64, 32]\n    kernels = kernels or [3, 3]\n    strides = strides or [2, 2]\n    x = states\n    for n_filter, kernel, stride in zip(n_filters, kernels, strides):\n        x = tf.layers.conv2d(x, n_filter, kernel, stride, activation=None)\n        if use_bn:\n            x = tf.layers.batch_normalization(x, training=is_training)\n        x = tf.layers.max_pooling2d(x, 2, 2)\n        x = activation_fn(x)\n        if dropout > 0:\n            x = tf.layers.dropout(x, rate=dropout, training=is_training)\n    x = tf.contrib.layers.flatten(x)\n    return x\n\ndef simple_model(features):\n    features = convolution_network(features)\n    prelogits = tf.layers.dense(features, 128, activation=tf.nn.elu)\n    logits = tf.layers.dense(prelogits, 10, activation=tf.nn.softmax)\n    \n    return logits\n\npred = tf.contrib.keras.layers.Lambda(simple_model)(features)\n\nBoth of them then made with model = kmodels.Model(inputs=features, outputs=pred)\nNevertheless, when I use model.summary() , Kesar model give me correct output:\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        640       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 2, 2, 32)          18464     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 128)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               16512     \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)                1290      \n=================================================================\nTotal params: 36,906\nTrainable params: 36,906\nNon-trainable params: 0\n_________________________________________________________________\n\nbut TF one, give only:\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \n_________________________________________________________________\nlambda_1 (Lambda)            (None, 10)                0         \n=================================================================\nTotal params: 0\nTrainable params: 0\nNon-trainable params: 0\n_________________________________________________________________\n\nIs it correct, that TF-Keras cannot see TF variables and optimize them? So we still have separated Tensorflow an Keras frameworks?\nWhy cannot I use TF as Keras backend?\nTF-versions: from 1.1.0 to 1.2.0rc1", "body": "It is a feature or a bug?\r\n\r\nFor example, I have 2 models:\r\ntypical imports:\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\nfrom tensorflow.contrib.keras import layers as klayers\r\nfrom tensorflow.contrib.keras import activations as kacts\r\nfrom tensorflow.contrib.keras import models as kmodels\r\nfrom tensorflow.contrib.keras import optimizers as kopts\r\nfrom tensorflow.contrib.keras import losses as kloss\r\nfrom tensorflow.contrib.keras import backend as kback\r\n```\r\n\r\nKeras model\r\n```\r\n# keras model\r\nfeatures = klayers.Input(shape=(28, 28, 1))\r\nx = klayers.Conv2D(64, 3, strides=(2, 2))(features)\r\nx = klayers.MaxPool2D()(x)\r\nx = klayers.Conv2D(32, 3, strides=(2, 2))(x)\r\nx = klayers.Flatten()(x)\r\nprelogits = klayers.Dense(128, activation=kacts.elu)(x)\r\npred = klayers.Dense(10, activation=kacts.softmax)(prelogits)\r\n```\r\n\r\nand TF one:\r\n```\r\nfeatures = klayers.Input(shape=(28, 28, 1))\r\ndef convolution_network(\r\n        states, n_filters=None, kernels=None, strides=None,\r\n        activation_fn=tf.nn.elu, use_bn=False, dropout=-1):\r\n    n_filters = n_filters or [64, 32]\r\n    kernels = kernels or [3, 3]\r\n    strides = strides or [2, 2]\r\n    x = states\r\n    for n_filter, kernel, stride in zip(n_filters, kernels, strides):\r\n        x = tf.layers.conv2d(x, n_filter, kernel, stride, activation=None)\r\n        if use_bn:\r\n            x = tf.layers.batch_normalization(x, training=is_training)\r\n        x = tf.layers.max_pooling2d(x, 2, 2)\r\n        x = activation_fn(x)\r\n        if dropout > 0:\r\n            x = tf.layers.dropout(x, rate=dropout, training=is_training)\r\n    x = tf.contrib.layers.flatten(x)\r\n    return x\r\n\r\ndef simple_model(features):\r\n    features = convolution_network(features)\r\n    prelogits = tf.layers.dense(features, 128, activation=tf.nn.elu)\r\n    logits = tf.layers.dense(prelogits, 10, activation=tf.nn.softmax)\r\n    \r\n    return logits\r\n\r\npred = tf.contrib.keras.layers.Lambda(simple_model)(features)\r\n```\r\n\r\nBoth of them then made with `model = kmodels.Model(inputs=features, outputs=pred)`\r\n\r\nNevertheless, when I use `model.summary()` , Kesar model give me correct output:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \r\n_________________________________________________________________\r\nconv2d_1 (Conv2D)            (None, 13, 13, 64)        640       \r\n_________________________________________________________________\r\nmax_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \r\n_________________________________________________________________\r\nconv2d_2 (Conv2D)            (None, 2, 2, 32)          18464     \r\n_________________________________________________________________\r\nflatten_1 (Flatten)          (None, 128)               0         \r\n_________________________________________________________________\r\ndense_1 (Dense)              (None, 128)               16512     \r\n_________________________________________________________________\r\ndense_2 (Dense)              (None, 10)                1290      \r\n=================================================================\r\nTotal params: 36,906\r\nTrainable params: 36,906\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n**but** TF one, give only:\r\n```\r\n_________________________________________________________________\r\nLayer (type)                 Output Shape              Param #   \r\n=================================================================\r\ninput_1 (InputLayer)         (None, 28, 28, 1)         0         \r\n_________________________________________________________________\r\nlambda_1 (Lambda)            (None, 10)                0         \r\n=================================================================\r\nTotal params: 0\r\nTrainable params: 0\r\nNon-trainable params: 0\r\n_________________________________________________________________\r\n```\r\n\r\nIs it correct, that TF-Keras cannot see TF variables and optimize them? So we still have separated Tensorflow an Keras frameworks?\r\nWhy cannot I use TF as Keras backend?\r\nTF-versions: from 1.1.0 to 1.2.0rc1"}