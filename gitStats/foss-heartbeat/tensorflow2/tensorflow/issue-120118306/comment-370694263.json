{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/370694263", "html_url": "https://github.com/tensorflow/tensorflow/issues/398#issuecomment-370694263", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/398", "id": 370694263, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MDY5NDI2Mw==", "user": {"login": "ahmedmaalej", "id": 4375237, "node_id": "MDQ6VXNlcjQzNzUyMzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/4375237?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ahmedmaalej", "html_url": "https://github.com/ahmedmaalej", "followers_url": "https://api.github.com/users/ahmedmaalej/followers", "following_url": "https://api.github.com/users/ahmedmaalej/following{/other_user}", "gists_url": "https://api.github.com/users/ahmedmaalej/gists{/gist_id}", "starred_url": "https://api.github.com/users/ahmedmaalej/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ahmedmaalej/subscriptions", "organizations_url": "https://api.github.com/users/ahmedmaalej/orgs", "repos_url": "https://api.github.com/users/ahmedmaalej/repos", "events_url": "https://api.github.com/users/ahmedmaalej/events{/privacy}", "received_events_url": "https://api.github.com/users/ahmedmaalej/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-06T07:53:56Z", "updated_at": "2018-03-06T07:53:56Z", "author_association": "NONE", "body_html": "<p>Similar problem I had to assign 16 to the batch size.<br>\nDoes the h5 file generation has to do something with this issue. Maybe using some compression options within create_dataset method would fixe the problem (didn't try it yet):<br>\nex: h5_fout.create_dataset( 'data', data=data,compression='gzip', compression_opts=4,       dtype=data_dtype)</p>", "body_text": "Similar problem I had to assign 16 to the batch size.\nDoes the h5 file generation has to do something with this issue. Maybe using some compression options within create_dataset method would fixe the problem (didn't try it yet):\nex: h5_fout.create_dataset( 'data', data=data,compression='gzip', compression_opts=4,       dtype=data_dtype)", "body": "Similar problem I had to assign 16 to the batch size. \r\nDoes the h5 file generation has to do something with this issue. Maybe using some compression options within create_dataset method would fixe the problem (didn't try it yet):\r\nex: h5_fout.create_dataset( 'data', data=data,compression='gzip', compression_opts=4,       dtype=data_dtype)"}