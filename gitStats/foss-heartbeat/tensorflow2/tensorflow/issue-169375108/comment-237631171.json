{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237631171", "html_url": "https://github.com/tensorflow/tensorflow/issues/3644#issuecomment-237631171", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3644", "id": 237631171, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzYzMTE3MQ==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-04T17:54:11Z", "updated_at": "2016-08-04T17:54:11Z", "author_association": "MEMBER", "body_html": "<p>Your observation is correct.  It is a deliberate design choice that a TF process will, unless instructed otherwise, use visible GPUs in order, and attempt to use all of the memory on each device.  This approach can work well when processes sharing a server are in a virtual environment that makes visible only the GPUs that the process is permitted to use.  As you note, setting a different value for <code>CUDA_VISIBLE_DEVICES</code> in each process is a similar solution.</p>\n<p>Unfortunately, there's no single best solution for distribution, and TF does not attempt to provide one.   Packing multiple ps shards and worker shards onto a single server is a convenient way of distributing, particularly if one has a large virtualized server farm, but if you really care about maximizing performance it may be necessary to customize your model to a particular server architecture, and run one process per server, where each process is aware of all the GPUs on that server and potentially takes advantage of local communication between them.  This kind of approach would involve the <code>with tf.device()</code> construct you noted.</p>\n<p>It's probably feasible to do a somewhat more effective job of scheduling Ops onto devices than TF does at the moment, but doing so is actually a pretty hard problem.  Although we've looked into this, we've continued to find that a little bit of attention from the programmer when setting up the model and execution plan is usually sufficient for a good solution.</p>", "body_text": "Your observation is correct.  It is a deliberate design choice that a TF process will, unless instructed otherwise, use visible GPUs in order, and attempt to use all of the memory on each device.  This approach can work well when processes sharing a server are in a virtual environment that makes visible only the GPUs that the process is permitted to use.  As you note, setting a different value for CUDA_VISIBLE_DEVICES in each process is a similar solution.\nUnfortunately, there's no single best solution for distribution, and TF does not attempt to provide one.   Packing multiple ps shards and worker shards onto a single server is a convenient way of distributing, particularly if one has a large virtualized server farm, but if you really care about maximizing performance it may be necessary to customize your model to a particular server architecture, and run one process per server, where each process is aware of all the GPUs on that server and potentially takes advantage of local communication between them.  This kind of approach would involve the with tf.device() construct you noted.\nIt's probably feasible to do a somewhat more effective job of scheduling Ops onto devices than TF does at the moment, but doing so is actually a pretty hard problem.  Although we've looked into this, we've continued to find that a little bit of attention from the programmer when setting up the model and execution plan is usually sufficient for a good solution.", "body": "Your observation is correct.  It is a deliberate design choice that a TF process will, unless instructed otherwise, use visible GPUs in order, and attempt to use all of the memory on each device.  This approach can work well when processes sharing a server are in a virtual environment that makes visible only the GPUs that the process is permitted to use.  As you note, setting a different value for `CUDA_VISIBLE_DEVICES` in each process is a similar solution.\n\nUnfortunately, there's no single best solution for distribution, and TF does not attempt to provide one.   Packing multiple ps shards and worker shards onto a single server is a convenient way of distributing, particularly if one has a large virtualized server farm, but if you really care about maximizing performance it may be necessary to customize your model to a particular server architecture, and run one process per server, where each process is aware of all the GPUs on that server and potentially takes advantage of local communication between them.  This kind of approach would involve the `with tf.device()` construct you noted.\n\nIt's probably feasible to do a somewhat more effective job of scheduling Ops onto devices than TF does at the moment, but doing so is actually a pretty hard problem.  Although we've looked into this, we've continued to find that a little bit of attention from the programmer when setting up the model and execution plan is usually sufficient for a good solution.\n"}