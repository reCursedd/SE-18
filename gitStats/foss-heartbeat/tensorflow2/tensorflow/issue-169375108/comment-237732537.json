{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/237732537", "html_url": "https://github.com/tensorflow/tensorflow/issues/3644#issuecomment-237732537", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3644", "id": 237732537, "node_id": "MDEyOklzc3VlQ29tbWVudDIzNzczMjUzNw==", "user": {"login": "tobegit3hub", "id": 2715000, "node_id": "MDQ6VXNlcjI3MTUwMDA=", "avatar_url": "https://avatars0.githubusercontent.com/u/2715000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tobegit3hub", "html_url": "https://github.com/tobegit3hub", "followers_url": "https://api.github.com/users/tobegit3hub/followers", "following_url": "https://api.github.com/users/tobegit3hub/following{/other_user}", "gists_url": "https://api.github.com/users/tobegit3hub/gists{/gist_id}", "starred_url": "https://api.github.com/users/tobegit3hub/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tobegit3hub/subscriptions", "organizations_url": "https://api.github.com/users/tobegit3hub/orgs", "repos_url": "https://api.github.com/users/tobegit3hub/repos", "events_url": "https://api.github.com/users/tobegit3hub/events{/privacy}", "received_events_url": "https://api.github.com/users/tobegit3hub/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-05T01:27:06Z", "updated_at": "2016-08-05T01:27:06Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for confirming and detailed explaination <g-emoji class=\"g-emoji\" alias=\"smiley\" fallback-src=\"https://assets-cdn.github.com/images/icons/emoji/unicode/1f603.png\">\ud83d\ude03</g-emoji></p>\n<p>TensorFlow is flexible enough with <code>CUDA_VISIBLE_DEVICES</code> and <code>with tf.device()</code> to archive any architecture. We are also looking forward to the optimization of scheduling. Maybe adding the rule of using CPU for ps and placing operations randomly in all GPUs for worker could help, especially for beginners.</p>", "body_text": "Thanks for confirming and detailed explaination \ud83d\ude03\nTensorFlow is flexible enough with CUDA_VISIBLE_DEVICES and with tf.device() to archive any architecture. We are also looking forward to the optimization of scheduling. Maybe adding the rule of using CPU for ps and placing operations randomly in all GPUs for worker could help, especially for beginners.", "body": "Thanks for confirming and detailed explaination \ud83d\ude03 \n\nTensorFlow is flexible enough with `CUDA_VISIBLE_DEVICES` and `with tf.device()` to archive any architecture. We are also looking forward to the optimization of scheduling. Maybe adding the rule of using CPU for ps and placing operations randomly in all GPUs for worker could help, especially for beginners.\n"}