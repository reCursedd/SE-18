{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/165185025", "html_url": "https://github.com/tensorflow/tensorflow/issues/456#issuecomment-165185025", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/456", "id": 165185025, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NTE4NTAyNQ==", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-16T17:29:55Z", "updated_at": "2015-12-16T17:29:55Z", "author_association": "NONE", "body_html": "<p>In case anyone is looking at this, here is the sampling function I use. Thanks <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a>  again for all your help.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">batch_sample_with_temperature</span>(<span class=\"pl-smi\">a</span>, <span class=\"pl-smi\">temperature</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>):\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>this function is like sample_with_temperature except it can handle batch input a of [batch_size x logits] </span>\n<span class=\"pl-s\">        this function takes logits input, and produces a specific number from the array. This is all done on the gpu</span>\n<span class=\"pl-s\">        because this function uses tensorflow</span>\n<span class=\"pl-s\">        As you increase the temperature, you will get more diversified output but with more errors (usually gramatical if you're </span>\n<span class=\"pl-s\">            doing text)</span>\n<span class=\"pl-s\">    args: </span>\n<span class=\"pl-s\">        Logits -- this must be a 2d array [batch_size x logits]</span>\n<span class=\"pl-s\">        Temperature -- how much variance you want in output</span>\n<span class=\"pl-s\">    returns:</span>\n<span class=\"pl-s\">        Selected number from distribution</span>\n<span class=\"pl-s\">    <span class=\"pl-pds\">'''</span></span>\n\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span></span>\n<span class=\"pl-s\">    Equation can be found here: https://en.wikipedia.org/wiki/Softmax_function (under reinforcement learning)</span>\n<span class=\"pl-s\">        Karpathy did it here as well: https://github.com/karpathy/char-rnn/blob/4297a9bf69726823d944ad971555e91204f12ca8/sample.lua<span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-s\"><span class=\"pl-pds\">'''</span>a is [batch_size x logits]<span class=\"pl-pds\">'''</span></span>\n    <span class=\"pl-k\">with</span> tf.op_scope(a<span class=\"pl-k\">+</span>temperature, <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>batch_sample_with_temperature<span class=\"pl-pds\">\"</span></span>):\n\n        exponent_raised <span class=\"pl-k\">=</span> tf.exp(tf.div(a, temperature)) <span class=\"pl-c\"><span class=\"pl-c\">#</span>start by reduction of temperature, and get rid of negative numbers with exponent</span>\n\n        matrix_X <span class=\"pl-k\">=</span> tf.div(exponent_raised, tf.reduce_sum(exponent_raised, <span class=\"pl-v\">reduction_indices</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>)) <span class=\"pl-c\"><span class=\"pl-c\">#</span>this will yield probabilities!</span>\n\n        matrix_U <span class=\"pl-k\">=</span> tf.random_uniform([batch_size, tf.shape(a)[<span class=\"pl-c1\">1</span>]], <span class=\"pl-v\">minval</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>, <span class=\"pl-v\">maxval</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>)\n\n        final_number <span class=\"pl-k\">=</span> tf.argmax(tf.sub(matrix_X <span class=\"pl-k\">-</span> matrix_U), <span class=\"pl-v\">dimension</span> <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1</span>) <span class=\"pl-c\"><span class=\"pl-c\">#</span>you want dimension = 1 because you are argmaxing across rows.</span>\n\n    <span class=\"pl-k\">return</span> final_number</pre></div>", "body_text": "In case anyone is looking at this, here is the sampling function I use. Thanks @zheng-xq  again for all your help.\ndef batch_sample_with_temperature(a, temperature=1.0):\n    '''this function is like sample_with_temperature except it can handle batch input a of [batch_size x logits] \n        this function takes logits input, and produces a specific number from the array. This is all done on the gpu\n        because this function uses tensorflow\n        As you increase the temperature, you will get more diversified output but with more errors (usually gramatical if you're \n            doing text)\n    args: \n        Logits -- this must be a 2d array [batch_size x logits]\n        Temperature -- how much variance you want in output\n    returns:\n        Selected number from distribution\n    '''\n\n    '''\n    Equation can be found here: https://en.wikipedia.org/wiki/Softmax_function (under reinforcement learning)\n        Karpathy did it here as well: https://github.com/karpathy/char-rnn/blob/4297a9bf69726823d944ad971555e91204f12ca8/sample.lua'''\n    '''a is [batch_size x logits]'''\n    with tf.op_scope(a+temperature, \"batch_sample_with_temperature\"):\n\n        exponent_raised = tf.exp(tf.div(a, temperature)) #start by reduction of temperature, and get rid of negative numbers with exponent\n\n        matrix_X = tf.div(exponent_raised, tf.reduce_sum(exponent_raised, reduction_indices = 1)) #this will yield probabilities!\n\n        matrix_U = tf.random_uniform([batch_size, tf.shape(a)[1]], minval = 0, maxval = 1)\n\n        final_number = tf.argmax(tf.sub(matrix_X - matrix_U), dimension = 1) #you want dimension = 1 because you are argmaxing across rows.\n\n    return final_number", "body": "In case anyone is looking at this, here is the sampling function I use. Thanks @zheng-xq  again for all your help.\n\n``` python\ndef batch_sample_with_temperature(a, temperature=1.0):\n    '''this function is like sample_with_temperature except it can handle batch input a of [batch_size x logits] \n        this function takes logits input, and produces a specific number from the array. This is all done on the gpu\n        because this function uses tensorflow\n        As you increase the temperature, you will get more diversified output but with more errors (usually gramatical if you're \n            doing text)\n    args: \n        Logits -- this must be a 2d array [batch_size x logits]\n        Temperature -- how much variance you want in output\n    returns:\n        Selected number from distribution\n    '''\n\n    '''\n    Equation can be found here: https://en.wikipedia.org/wiki/Softmax_function (under reinforcement learning)\n        Karpathy did it here as well: https://github.com/karpathy/char-rnn/blob/4297a9bf69726823d944ad971555e91204f12ca8/sample.lua'''\n    '''a is [batch_size x logits]'''\n    with tf.op_scope(a+temperature, \"batch_sample_with_temperature\"):\n\n        exponent_raised = tf.exp(tf.div(a, temperature)) #start by reduction of temperature, and get rid of negative numbers with exponent\n\n        matrix_X = tf.div(exponent_raised, tf.reduce_sum(exponent_raised, reduction_indices = 1)) #this will yield probabilities!\n\n        matrix_U = tf.random_uniform([batch_size, tf.shape(a)[1]], minval = 0, maxval = 1)\n\n        final_number = tf.argmax(tf.sub(matrix_X - matrix_U), dimension = 1) #you want dimension = 1 because you are argmaxing across rows.\n\n    return final_number\n```\n"}