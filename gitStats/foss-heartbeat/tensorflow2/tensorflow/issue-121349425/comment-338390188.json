{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338390188", "html_url": "https://github.com/tensorflow/tensorflow/issues/456#issuecomment-338390188", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/456", "id": 338390188, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODM5MDE4OA==", "user": {"login": "zxteloiv", "id": 517994, "node_id": "MDQ6VXNlcjUxNzk5NA==", "avatar_url": "https://avatars2.githubusercontent.com/u/517994?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zxteloiv", "html_url": "https://github.com/zxteloiv", "followers_url": "https://api.github.com/users/zxteloiv/followers", "following_url": "https://api.github.com/users/zxteloiv/following{/other_user}", "gists_url": "https://api.github.com/users/zxteloiv/gists{/gist_id}", "starred_url": "https://api.github.com/users/zxteloiv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zxteloiv/subscriptions", "organizations_url": "https://api.github.com/users/zxteloiv/orgs", "repos_url": "https://api.github.com/users/zxteloiv/repos", "events_url": "https://api.github.com/users/zxteloiv/events{/privacy}", "received_events_url": "https://api.github.com/users/zxteloiv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-21T12:25:07Z", "updated_at": "2017-10-21T12:25:07Z", "author_association": "NONE", "body_html": "<p>Thanks for sharing the useful information.<br>\nI'm trying to implement the same thing in chainer because it doesn't provide functions to sample a batch of multinomial distributions.</p>\n<p>However, in my experiments, the code provided in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"121349425\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/456\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/456/hovercard?comment_id=165185025&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/456#issuecomment-165185025\">#456 (comment)</a> is wrong, which doesn't give the correct samples. The main problem is that uniform noise is <strong>directly substracted</strong> from the softmax probabilities then take the argmax, differing to a <em>usual way</em> that finding index for a uniform noise from ranges defined by the multinomial CDF. By the way, the method provided by <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"121349425\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/456\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/456/hovercard?comment_id=164038802&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/456#issuecomment-164038802\">#456 (comment)</a> is exactly what we want.</p>\n<p>Since testing random number generators is not easy, I use the simplest way to sample the same distribution for a million times and count their occurrence. Here's the testing snippet.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">multinomial_issue_impl</span>(<span class=\"pl-smi\">logits</span>):\n    exponent <span class=\"pl-k\">=</span> np.exp(logits)\n    matrix_X <span class=\"pl-k\">=</span> exponent <span class=\"pl-k\">/</span> np.sum(exponent)\n    matrix_U <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-k\">*</span>matrix_X.shape)\n    final_num <span class=\"pl-k\">=</span> np.argmax(matrix_X <span class=\"pl-k\">-</span> matrix_U)\n    <span class=\"pl-k\">return</span> final_num\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">multinomial_gumbel_impl</span>(<span class=\"pl-smi\">logits</span>):\n    exponent <span class=\"pl-k\">=</span> np.exp(logits)\n    log_prob <span class=\"pl-k\">=</span> np.log(exponent <span class=\"pl-k\">/</span> np.sum(exponent))\n    U <span class=\"pl-k\">=</span> np.random.rand(<span class=\"pl-k\">*</span>log_prob.shape)\n    num <span class=\"pl-k\">=</span> np.argmax(log_prob <span class=\"pl-k\">-</span> np.log(<span class=\"pl-k\">-</span>np.log(U)))\n    <span class=\"pl-k\">return</span> num\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>(<span class=\"pl-smi\">func</span>):\n    logits <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">1</span>., <span class=\"pl-c1\">5</span>.]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> 0 ~ 5</span>\n    K <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1000000</span>\n\n    count <span class=\"pl-k\">=</span> [<span class=\"pl-c1\">0</span>] <span class=\"pl-k\">*</span> <span class=\"pl-c1\">6</span>\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(K):\n        num <span class=\"pl-k\">=</span> func(logits)\n        count[num] <span class=\"pl-k\">+=</span> <span class=\"pl-c1\">1</span>\n\n    <span class=\"pl-c1\">print</span> count\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>__main__<span class=\"pl-pds\">\"</span></span>:\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>test:<span class=\"pl-pds\">\"</span></span>\n    test(multinomial_issue_impl)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>gumbel:<span class=\"pl-pds\">\"</span></span>\n    test(multinomial_gumbel_impl)</pre></div>\n<p>which yields</p>\n<pre lang=\"text\"><code>test:\n[4525, 4415, 4449, 4395, 4451, 977765]\ngumbel:\n[16674, 16570, 16929, 16902, 16878, 916047]\n</code></pre>\n<p>Given the logits <code>[1., 1., 1., 1., 1., 5.]</code>, the probabilities should be <code>[0.01677904,  0.01677904,  0.01677904,  0.01677904,  0.01677904, 0.91610478]</code>. Thus the result tells that gumbel trick is correct.</p>", "body_text": "Thanks for sharing the useful information.\nI'm trying to implement the same thing in chainer because it doesn't provide functions to sample a batch of multinomial distributions.\nHowever, in my experiments, the code provided in #456 (comment) is wrong, which doesn't give the correct samples. The main problem is that uniform noise is directly substracted from the softmax probabilities then take the argmax, differing to a usual way that finding index for a uniform noise from ranges defined by the multinomial CDF. By the way, the method provided by #456 (comment) is exactly what we want.\nSince testing random number generators is not easy, I use the simplest way to sample the same distribution for a million times and count their occurrence. Here's the testing snippet.\nimport numpy as np\n\ndef multinomial_issue_impl(logits):\n    exponent = np.exp(logits)\n    matrix_X = exponent / np.sum(exponent)\n    matrix_U = np.random.rand(*matrix_X.shape)\n    final_num = np.argmax(matrix_X - matrix_U)\n    return final_num\n\n\ndef multinomial_gumbel_impl(logits):\n    exponent = np.exp(logits)\n    log_prob = np.log(exponent / np.sum(exponent))\n    U = np.random.rand(*log_prob.shape)\n    num = np.argmax(log_prob - np.log(-np.log(U)))\n    return num\n\n\ndef test(func):\n    logits = [1., 1., 1., 1., 1., 5.]  # 0 ~ 5\n    K = 1000000\n\n    count = [0] * 6\n    for i in xrange(K):\n        num = func(logits)\n        count[num] += 1\n\n    print count\n\n\nif __name__ == \"__main__\":\n    print \"test:\"\n    test(multinomial_issue_impl)\n    print \"gumbel:\"\n    test(multinomial_gumbel_impl)\nwhich yields\ntest:\n[4525, 4415, 4449, 4395, 4451, 977765]\ngumbel:\n[16674, 16570, 16929, 16902, 16878, 916047]\n\nGiven the logits [1., 1., 1., 1., 1., 5.], the probabilities should be [0.01677904,  0.01677904,  0.01677904,  0.01677904,  0.01677904, 0.91610478]. Thus the result tells that gumbel trick is correct.", "body": "Thanks for sharing the useful information.\r\nI'm trying to implement the same thing in chainer because it doesn't provide functions to sample a batch of multinomial distributions.\r\n\r\nHowever, in my experiments, the code provided in https://github.com/tensorflow/tensorflow/issues/456#issuecomment-165185025 is wrong, which doesn't give the correct samples. The main problem is that uniform noise is **directly substracted** from the softmax probabilities then take the argmax, differing to a *usual way* that finding index for a uniform noise from ranges defined by the multinomial CDF. By the way, the method provided by https://github.com/tensorflow/tensorflow/issues/456#issuecomment-164038802 is exactly what we want.\r\n\r\nSince testing random number generators is not easy, I use the simplest way to sample the same distribution for a million times and count their occurrence. Here's the testing snippet.\r\n\r\n```python\r\nimport numpy as np\r\n\r\ndef multinomial_issue_impl(logits):\r\n    exponent = np.exp(logits)\r\n    matrix_X = exponent / np.sum(exponent)\r\n    matrix_U = np.random.rand(*matrix_X.shape)\r\n    final_num = np.argmax(matrix_X - matrix_U)\r\n    return final_num\r\n\r\n\r\ndef multinomial_gumbel_impl(logits):\r\n    exponent = np.exp(logits)\r\n    log_prob = np.log(exponent / np.sum(exponent))\r\n    U = np.random.rand(*log_prob.shape)\r\n    num = np.argmax(log_prob - np.log(-np.log(U)))\r\n    return num\r\n\r\n\r\ndef test(func):\r\n    logits = [1., 1., 1., 1., 1., 5.]  # 0 ~ 5\r\n    K = 1000000\r\n\r\n    count = [0] * 6\r\n    for i in xrange(K):\r\n        num = func(logits)\r\n        count[num] += 1\r\n\r\n    print count\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    print \"test:\"\r\n    test(multinomial_issue_impl)\r\n    print \"gumbel:\"\r\n    test(multinomial_gumbel_impl)\r\n```\r\n\r\nwhich yields\r\n\r\n```text\r\ntest:\r\n[4525, 4415, 4449, 4395, 4451, 977765]\r\ngumbel:\r\n[16674, 16570, 16929, 16902, 16878, 916047]\r\n```\r\n\r\nGiven the logits `[1., 1., 1., 1., 1., 5.]`, the probabilities should be `[0.01677904,  0.01677904,  0.01677904,  0.01677904,  0.01677904, 0.91610478]`. Thus the result tells that gumbel trick is correct.\r\n"}