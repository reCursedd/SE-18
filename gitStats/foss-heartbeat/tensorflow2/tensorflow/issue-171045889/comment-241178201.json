{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/241178201", "html_url": "https://github.com/tensorflow/tensorflow/pull/3802#issuecomment-241178201", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3802", "id": 241178201, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MTE3ODIwMQ==", "user": {"login": "sahiliitm", "id": 5723372, "node_id": "MDQ6VXNlcjU3MjMzNzI=", "avatar_url": "https://avatars0.githubusercontent.com/u/5723372?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sahiliitm", "html_url": "https://github.com/sahiliitm", "followers_url": "https://api.github.com/users/sahiliitm/followers", "following_url": "https://api.github.com/users/sahiliitm/following{/other_user}", "gists_url": "https://api.github.com/users/sahiliitm/gists{/gist_id}", "starred_url": "https://api.github.com/users/sahiliitm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sahiliitm/subscriptions", "organizations_url": "https://api.github.com/users/sahiliitm/orgs", "repos_url": "https://api.github.com/users/sahiliitm/repos", "events_url": "https://api.github.com/users/sahiliitm/events{/privacy}", "received_events_url": "https://api.github.com/users/sahiliitm/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-20T04:27:42Z", "updated_at": "2016-08-20T04:27:42Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Are you sure that is the case?<br>\nThe reason I doubt that is because of this <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py#L161\">line of code</a>. We do pass the parameters <code>output_projection</code> to the function <code>tf.nn.seq2seq.model_with_buckets</code> (indirectly through the function defined at <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py#L125\">this line</a>) in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py\">seq2seq_model.py</a> but the output returned by it is still a <code>cell.output_size</code> sized tensor. Which is why the line of code I pointed out is needed to project it back to the size <code>num_decoder_symbols</code>.</p>\n<p>I guess what I'm asking is that if <code>output_projection</code> parameters are not <code>None</code>, is that the case when the functions under question would return <code>num_decoder_symbol</code> sized tensors? That does not seem to be the case.</p>", "body_text": "Are you sure that is the case?\nThe reason I doubt that is because of this line of code. We do pass the parameters output_projection to the function tf.nn.seq2seq.model_with_buckets (indirectly through the function defined at this line) in seq2seq_model.py but the output returned by it is still a cell.output_size sized tensor. Which is why the line of code I pointed out is needed to project it back to the size num_decoder_symbols.\nI guess what I'm asking is that if output_projection parameters are not None, is that the case when the functions under question would return num_decoder_symbol sized tensors? That does not seem to be the case.", "body": "Are you sure that is the case? \nThe reason I doubt that is because of this [line of code](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py#L161). We do pass the parameters `output_projection` to the function `tf.nn.seq2seq.model_with_buckets` (indirectly through the function defined at [this line](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py#L125)) in [seq2seq_model.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/models/rnn/translate/seq2seq_model.py) but the output returned by it is still a `cell.output_size` sized tensor. Which is why the line of code I pointed out is needed to project it back to the size `num_decoder_symbols`.\n\nI guess what I'm asking is that if `output_projection` parameters are not `None`, is that the case when the functions under question would return `num_decoder_symbol` sized tensors? That does not seem to be the case.\n"}