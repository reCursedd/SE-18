{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/438057103", "html_url": "https://github.com/tensorflow/tensorflow/issues/23050#issuecomment-438057103", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23050", "id": 438057103, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODA1NzEwMw==", "user": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-12T22:51:34Z", "updated_at": "2018-11-12T22:51:34Z", "author_association": "NONE", "body_html": "<p>Good that you dig deeper in the paper, guess I was not explicit in my explanation.</p>\n<blockquote>\n<p>The test phase requires the learned gamma and beta, whose shape are independent of the batch size.</p>\n</blockquote>\n<p>For normalization at inference time population mean and variance are used which were calculated during training using moving mean and variance. They are averaged over virtual batches. From Algorithm 1 in Hoffer's paper, I wouldn't say that alpha and gamma are independent of batch size. You can also find more details on Algorithm 1&amp;2 in the paper from <a href=\"https://arxiv.org/pdf/1502.03167.pdf\" rel=\"nofollow\">Ioffe &amp; Szegedy</a>.</p>\n<blockquote>\n<p>Test/inference should be able to happen for any batch_size.</p>\n</blockquote>\n<p>This is true in general, but your last sess.run call is made on x_norm, i.e. tf.layers.batch_normalization of x during training where virtual_batch_size is used.</p>", "body_text": "Good that you dig deeper in the paper, guess I was not explicit in my explanation.\n\nThe test phase requires the learned gamma and beta, whose shape are independent of the batch size.\n\nFor normalization at inference time population mean and variance are used which were calculated during training using moving mean and variance. They are averaged over virtual batches. From Algorithm 1 in Hoffer's paper, I wouldn't say that alpha and gamma are independent of batch size. You can also find more details on Algorithm 1&2 in the paper from Ioffe & Szegedy.\n\nTest/inference should be able to happen for any batch_size.\n\nThis is true in general, but your last sess.run call is made on x_norm, i.e. tf.layers.batch_normalization of x during training where virtual_batch_size is used.", "body": "Good that you dig deeper in the paper, guess I was not explicit in my explanation. \r\n\r\n> The test phase requires the learned gamma and beta, whose shape are independent of the batch size.\r\n\r\nFor normalization at inference time population mean and variance are used which were calculated during training using moving mean and variance. They are averaged over virtual batches. From Algorithm 1 in Hoffer's paper, I wouldn't say that alpha and gamma are independent of batch size. You can also find more details on Algorithm 1&2 in the paper from [Ioffe & Szegedy](https://arxiv.org/pdf/1502.03167.pdf).\r\n\r\n> Test/inference should be able to happen for any batch_size.\r\n\r\nThis is true in general, but your last sess.run call is made on x_norm, i.e. tf.layers.batch_normalization of x during training where virtual_batch_size is used. "}