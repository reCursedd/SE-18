{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/438649315", "html_url": "https://github.com/tensorflow/tensorflow/issues/23050#issuecomment-438649315", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23050", "id": 438649315, "node_id": "MDEyOklzc3VlQ29tbWVudDQzODY0OTMxNQ==", "user": {"login": "ccurro", "id": 44445, "node_id": "MDQ6VXNlcjQ0NDQ1", "avatar_url": "https://avatars1.githubusercontent.com/u/44445?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ccurro", "html_url": "https://github.com/ccurro", "followers_url": "https://api.github.com/users/ccurro/followers", "following_url": "https://api.github.com/users/ccurro/following{/other_user}", "gists_url": "https://api.github.com/users/ccurro/gists{/gist_id}", "starred_url": "https://api.github.com/users/ccurro/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ccurro/subscriptions", "organizations_url": "https://api.github.com/users/ccurro/orgs", "repos_url": "https://api.github.com/users/ccurro/repos", "events_url": "https://api.github.com/users/ccurro/events{/privacy}", "received_events_url": "https://api.github.com/users/ccurro/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-14T12:44:19Z", "updated_at": "2018-11-14T12:44:19Z", "author_association": "NONE", "body_html": "<p>I am aware that gamma and beta are parameters - nothing I have said indicates otherwise.</p>\n<p>Your statement is trivial and reductive. Obviously if I make <code>virtual_batch_size=1</code> then yes, at inference time, as the algorithm is currently implemented, I will not encounter a runtime error - that does not however make any statements about the correctness of the implementation. Further, <code>virtual_batch_size=1</code> is more than likely completely useless, since the variance for any virtual batch would be zero.</p>\n<p>Your statement does nothing to refute the utility, fundamental soundness, and clear expositional foundation (from the original paper) of training with a given batch size and evaluating with another, possibly singleton, batch size.</p>\n<p>Forward batch statistics are not computed during inference. There should be no requirement for an inference batch. All batch statistics are computed during training which are then used at inference time.</p>\n<p>If you will not change your position, please provide a sound argument or reassign this issue to another Googler/contributor.</p>", "body_text": "I am aware that gamma and beta are parameters - nothing I have said indicates otherwise.\nYour statement is trivial and reductive. Obviously if I make virtual_batch_size=1 then yes, at inference time, as the algorithm is currently implemented, I will not encounter a runtime error - that does not however make any statements about the correctness of the implementation. Further, virtual_batch_size=1 is more than likely completely useless, since the variance for any virtual batch would be zero.\nYour statement does nothing to refute the utility, fundamental soundness, and clear expositional foundation (from the original paper) of training with a given batch size and evaluating with another, possibly singleton, batch size.\nForward batch statistics are not computed during inference. There should be no requirement for an inference batch. All batch statistics are computed during training which are then used at inference time.\nIf you will not change your position, please provide a sound argument or reassign this issue to another Googler/contributor.", "body": "I am aware that gamma and beta are parameters - nothing I have said indicates otherwise. \r\n\r\nYour statement is trivial and reductive. Obviously if I make `virtual_batch_size=1` then yes, at inference time, as the algorithm is currently implemented, I will not encounter a runtime error - that does not however make any statements about the correctness of the implementation. Further, `virtual_batch_size=1` is more than likely completely useless, since the variance for any virtual batch would be zero. \r\n\r\nYour statement does nothing to refute the utility, fundamental soundness, and clear expositional foundation (from the original paper) of training with a given batch size and evaluating with another, possibly singleton, batch size. \r\n\r\nForward batch statistics are not computed during inference. There should be no requirement for an inference batch. All batch statistics are computed during training which are then used at inference time. \r\n\r\nIf you will not change your position, please provide a sound argument or reassign this issue to another Googler/contributor. "}