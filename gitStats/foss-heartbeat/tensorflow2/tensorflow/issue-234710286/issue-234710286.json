{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10581", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10581/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10581/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10581/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10581", "id": 234710286, "node_id": "MDU6SXNzdWUyMzQ3MTAyODY=", "number": 10581, "title": "Run quantize_graph error", "user": {"login": "JcmeLs", "id": 12932339, "node_id": "MDQ6VXNlcjEyOTMyMzM5", "avatar_url": "https://avatars2.githubusercontent.com/u/12932339?v=4", "gravatar_id": "", "url": "https://api.github.com/users/JcmeLs", "html_url": "https://github.com/JcmeLs", "followers_url": "https://api.github.com/users/JcmeLs/followers", "following_url": "https://api.github.com/users/JcmeLs/following{/other_user}", "gists_url": "https://api.github.com/users/JcmeLs/gists{/gist_id}", "starred_url": "https://api.github.com/users/JcmeLs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/JcmeLs/subscriptions", "organizations_url": "https://api.github.com/users/JcmeLs/orgs", "repos_url": "https://api.github.com/users/JcmeLs/repos", "events_url": "https://api.github.com/users/JcmeLs/events{/privacy}", "received_events_url": "https://api.github.com/users/JcmeLs/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-06-09T03:10:44Z", "updated_at": "2017-10-17T13:05:08Z", "closed_at": "2017-06-12T06:08:36Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nNo</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nOS X EI Caption 10.11.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary (pip install)</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\nTensorFlow 1.2.0-rc1 CPU Only</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nBuild label: 0.4.5-homebrew<br>\nBuild target: bazel-out/local-<br>\nopt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Thu Mar 16 13:37:54 2017 (1489671474)<br>\nBuild timestamp: 1489671474<br>\nBuild timestamp as int: 1489671474</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nCPU Only</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Because of the buffers holding the model weight values are 77MB in size, the memory needed to load these into the app can put a lot of pressure on RAM in Android even before the model is run.<br>\nSo, I need to further compress the model and round the weights,Even at the expense of accuracy.<br>\nSo I build <code>/tensorflow/tools/quantization/quantize_graph</code>.There are something wrongs when I run <code>quantize_graph</code> .</p>\n<h3>Source code / logs</h3>\n<p>Source code:</p>\n<pre><code>bazel-bin/tensorflow/tools/quantization/quantize_graph \\\n\u2013input=/Users/liba/Desktop/OptimizeCTNModel.pb \\\n\u2013output=/Users/liba/Desktop/RoundedCTNModel.pb \\ \n\u2013output_node_names=output/outputs \\\n\u2013mode=weights_rounded\n</code></pre>\n<p>log:</p>\n<pre><code>Traceback (most recent call last):\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1301, in &lt;module&gt;\n    app.run()\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1267, in main\n    data = f.read()\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 125, in read\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))\n  File \"/Users/liba/anaconda/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.FailedPreconditionError: .\n\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nNo\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nOS X EI Caption 10.11.6\nTensorFlow installed from (source or binary):\nbinary (pip install)\nTensorFlow version (use command below):\nTensorFlow 1.2.0-rc1 CPU Only\nBazel version (if compiling from source):\nBuild label: 0.4.5-homebrew\nBuild target: bazel-out/local-\nopt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Mar 16 13:37:54 2017 (1489671474)\nBuild timestamp: 1489671474\nBuild timestamp as int: 1489671474\nCUDA/cuDNN version:\nCPU Only\n\nDescribe the problem\nBecause of the buffers holding the model weight values are 77MB in size, the memory needed to load these into the app can put a lot of pressure on RAM in Android even before the model is run.\nSo, I need to further compress the model and round the weights,Even at the expense of accuracy.\nSo I build /tensorflow/tools/quantization/quantize_graph.There are something wrongs when I run quantize_graph .\nSource code / logs\nSource code:\nbazel-bin/tensorflow/tools/quantization/quantize_graph \\\n\u2013input=/Users/liba/Desktop/OptimizeCTNModel.pb \\\n\u2013output=/Users/liba/Desktop/RoundedCTNModel.pb \\ \n\u2013output_node_names=output/outputs \\\n\u2013mode=weights_rounded\n\nlog:\nTraceback (most recent call last):\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1301, in <module>\n    app.run()\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1267, in main\n    data = f.read()\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 125, in read\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))\n  File \"/Users/liba/anaconda/lib/python2.7/contextlib.py\", line 24, in __exit__\n    self.gen.next()\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\n    pywrap_tensorflow.TF_GetCode(status))\ntensorflow.python.framework.errors_impl.FailedPreconditionError: .", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n    No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n    OS X EI Caption 10.11.6\r\n- **TensorFlow installed from (source or binary)**:\r\n    binary (pip install)\r\n- **TensorFlow version (use command below)**:\r\n    TensorFlow 1.2.0-rc1 CPU Only\r\n- **Bazel version (if compiling from source)**:\r\n    Build label: 0.4.5-homebrew\r\n    Build target: bazel-out/local-\r\n    opt/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\n    Build time: Thu Mar 16 13:37:54 2017 (1489671474)\r\n    Build timestamp: 1489671474\r\n    Build timestamp as int: 1489671474\r\n- **CUDA/cuDNN version**:\r\n    CPU Only\r\n\r\n### Describe the problem\r\nBecause of the buffers holding the model weight values are 77MB in size, the memory needed to load these into the app can put a lot of pressure on RAM in Android even before the model is run.  \r\nSo, I need to further compress the model and round the weights,Even at the expense of accuracy.  \r\nSo I build `/tensorflow/tools/quantization/quantize_graph`.There are something wrongs when I run `quantize_graph` .\r\n### Source code / logs\r\nSource code:\r\n``` \r\nbazel-bin/tensorflow/tools/quantization/quantize_graph \\\r\n\u2013input=/Users/liba/Desktop/OptimizeCTNModel.pb \\\r\n\u2013output=/Users/liba/Desktop/RoundedCTNModel.pb \\ \r\n\u2013output_node_names=output/outputs \\\r\n\u2013mode=weights_rounded\r\n```\r\n\r\nlog:\r\n``` \r\nTraceback (most recent call last):\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1301, in <module>\r\n    app.run()\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/platform/app.py\", line 48, in run\r\n    _sys.exit(main(_sys.argv[:1] + flags_passthrough))\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/tools/quantization/quantize_graph.py\", line 1267, in main\r\n    data = f.read()\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/lib/io/file_io.py\", line 125, in read\r\n    pywrap_tensorflow.ReadFromStream(self._read_buf, length, status))\r\n  File \"/Users/liba/anaconda/lib/python2.7/contextlib.py\", line 24, in __exit__\r\n    self.gen.next()\r\n  File \"/Users/liba/Desktop/tensorflow/bazel-bin/tensorflow/tools/quantization/quantize_graph.runfiles/org_tensorflow/tensorflow/python/framework/errors_impl.py\", line 466, in raise_exception_on_not_ok_status\r\n    pywrap_tensorflow.TF_GetCode(status))\r\ntensorflow.python.framework.errors_impl.FailedPreconditionError: .\r\n\r\n```\r\n\r\n"}