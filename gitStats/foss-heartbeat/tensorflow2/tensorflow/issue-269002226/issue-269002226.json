{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14018", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14018/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14018/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14018/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14018", "id": 269002226, "node_id": "MDU6SXNzdWUyNjkwMDIyMjY=", "number": 14018, "title": "Tutorial request for hybrid model (word+character) ", "user": {"login": "Raghava14", "id": 22686557, "node_id": "MDQ6VXNlcjIyNjg2NTU3", "avatar_url": "https://avatars2.githubusercontent.com/u/22686557?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Raghava14", "html_url": "https://github.com/Raghava14", "followers_url": "https://api.github.com/users/Raghava14/followers", "following_url": "https://api.github.com/users/Raghava14/following{/other_user}", "gists_url": "https://api.github.com/users/Raghava14/gists{/gist_id}", "starred_url": "https://api.github.com/users/Raghava14/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Raghava14/subscriptions", "organizations_url": "https://api.github.com/users/Raghava14/orgs", "repos_url": "https://api.github.com/users/Raghava14/repos", "events_url": "https://api.github.com/users/Raghava14/events{/privacy}", "received_events_url": "https://api.github.com/users/Raghava14/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 284443156, "node_id": "MDU6TGFiZWwyODQ0NDMxNTY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:docs", "name": "type:docs", "color": "159b2e", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-10-27T06:35:38Z", "updated_at": "2018-08-25T00:04:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>The implementation done in the paper:<br>\n<a href=\"http://aclweb.org/anthology/P/P16/P16-1100.pdf\" rel=\"nofollow\">http://aclweb.org/anthology/P/P16/P16-1100.pdf</a></p>\n<p>is a hybrid seq2seq model with advancements where the encoder is fed with inputs based on following two cases:</p>\n<p>1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary</p>\n<p>2.Output of another LSTM network - when the word is <strong>out of vocabulary</strong> and a separate character based LSTM is used to <strong>generate an embedding on the fly</strong></p>\n<p>Consider the following example sentence:<br>\n\"The brown fox jumped over the lazy dog\"</p>\n<p>Assume these are the words present in the vocabulary: <em>The, brown, jumped, over, dog</em> - These words are fed to the seq2seq encoder as such</p>\n<p>out of vocabulary(OOV) words are: <em>fox, lazy</em> - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words</p>\n<p>These both word level and character level encoder needs to be trained end to end simultaneously.</p>\n<p>Since the implementation is a bit different from the normal seq2seq can a tutorial or example of such case be added to the examples section?</p>", "body_text": "The implementation done in the paper:\nhttp://aclweb.org/anthology/P/P16/P16-1100.pdf\nis a hybrid seq2seq model with advancements where the encoder is fed with inputs based on following two cases:\n1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary\n2.Output of another LSTM network - when the word is out of vocabulary and a separate character based LSTM is used to generate an embedding on the fly\nConsider the following example sentence:\n\"The brown fox jumped over the lazy dog\"\nAssume these are the words present in the vocabulary: The, brown, jumped, over, dog - These words are fed to the seq2seq encoder as such\nout of vocabulary(OOV) words are: fox, lazy - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words\nThese both word level and character level encoder needs to be trained end to end simultaneously.\nSince the implementation is a bit different from the normal seq2seq can a tutorial or example of such case be added to the examples section?", "body": "The implementation done in the paper:\r\nhttp://aclweb.org/anthology/P/P16/P16-1100.pdf\r\n\r\nis a hybrid seq2seq model with advancements where the encoder is fed with inputs based on following two cases:\r\n\r\n1.Normal vector representation of a word (Embedding vector) - when the word input is present in the vocabulary\r\n\r\n2.Output of another LSTM network - when the word is **out of vocabulary** and a separate character based LSTM is used to **generate an embedding on the fly**\r\n\r\nConsider the following example sentence:\r\n\"The brown fox jumped over the lazy dog\"\r\n\r\nAssume these are the words present in the vocabulary: _The, brown, jumped, over, dog_ - These words are fed to the seq2seq encoder as such\r\n\r\nout of vocabulary(OOV) words are: _fox, lazy_ - These words are passed to a character LSTM and the output of the same is passed to the seq2seq model along with the above words\r\n\r\nThese both word level and character level encoder needs to be trained end to end simultaneously. \r\n\r\nSince the implementation is a bit different from the normal seq2seq can a tutorial or example of such case be added to the examples section?"}