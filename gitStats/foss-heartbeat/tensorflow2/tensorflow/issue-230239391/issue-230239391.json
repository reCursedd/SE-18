{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10083", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10083/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10083/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10083/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10083", "id": 230239391, "node_id": "MDU6SXNzdWUyMzAyMzkzOTE=", "number": 10083, "title": "Nullptr check failed, when using TensorArray in combination with while_loop and swap_memory", "user": {"login": "davzha", "id": 27867765, "node_id": "MDQ6VXNlcjI3ODY3NzY1", "avatar_url": "https://avatars0.githubusercontent.com/u/27867765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davzha", "html_url": "https://github.com/davzha", "followers_url": "https://api.github.com/users/davzha/followers", "following_url": "https://api.github.com/users/davzha/following{/other_user}", "gists_url": "https://api.github.com/users/davzha/gists{/gist_id}", "starred_url": "https://api.github.com/users/davzha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davzha/subscriptions", "organizations_url": "https://api.github.com/users/davzha/orgs", "repos_url": "https://api.github.com/users/davzha/repos", "events_url": "https://api.github.com/users/davzha/events{/privacy}", "received_events_url": "https://api.github.com/users/davzha/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2017-05-21T18:41:27Z", "updated_at": "2018-09-09T16:32:50Z", "closed_at": "2018-09-09T16:32:50Z", "author_association": "NONE", "body_html": "<p>Environment: TensorFlow-gpu r1.1 build from source on Windows 10</p>\n<p>I am using the python API of TensorFlow to train a variant of an LSTM.<br>\nFor that purpose I use the <code>tf.while_loop</code> function to iterate over the time steps<br>\nWith <code>swap_memory=True</code> and not limiting devices to cpu I get the following failure:</p>\n<p><code>...tensorflow/tensorflow/core/framework/tensor.cc:885] Check failed: nullptr != b.buf_ (nullptr vs. 00...)</code></p>\n<p>With <code>swap_memory=False</code> or limiting devices to cpu this does not happen.</p>\n<p>The part of my code, that causes this failure (when commenting it out, it works) is in the body of the while loop:</p>\n<pre><code>...\nh_gathered = h_ta.gather(tf.range(time))\nh_gathered = tf.transpose(h_gathered, [1, 0, 2])\nsyn_t = self.syntactic_weights_ta.read(time)[:, :time]\nsyn_t = tf.expand_dims(syn_t, 1)\nsyn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\n...\n</code></pre>\n<p>where <code>time</code> is zero based and incremented after each step, <code>h_ta</code> is a TensorArray</p>\n<pre><code>h_ta = tf.TensorArray(\n        dtype=dtype,\n        size=max_seq_len,\n        clear_after_read=False,\n        element_shape=[batch_size, num_hidden],\n        tensor_array_name=\"fw_output\")\n</code></pre>\n<p>and <code>self.syntactic_weights_ta</code> is also a TensorArray</p>\n<pre><code>self.syntactic_weights_ta = tf.TensorArray(\n        dtype=dtype,\n        size=max_seq_len,\n        tensor_array_name=\"fw_syntactic_weights\")\nself.syntactic_weights_ta = self.syntactic_weights_ta.unstack(syntactic_weights)\n</code></pre>\n<p>What I am trying to achieve in the code snippet is basically a weighted sum over the past outputs, stored in <code>h_ta</code>.<br>\nIn the end I train the network with <code>tf.train.AdamOptimizer</code>.</p>\n<p>The forward propagation seems to work, as inserting <code>tf.Print</code> commands in the sensitive part of the code works.<br>\nBut I guess the backward propagation</p>\n<p>Unfortunately I could not find out which tensor's buffer points to nullptr.</p>", "body_text": "Environment: TensorFlow-gpu r1.1 build from source on Windows 10\nI am using the python API of TensorFlow to train a variant of an LSTM.\nFor that purpose I use the tf.while_loop function to iterate over the time steps\nWith swap_memory=True and not limiting devices to cpu I get the following failure:\n...tensorflow/tensorflow/core/framework/tensor.cc:885] Check failed: nullptr != b.buf_ (nullptr vs. 00...)\nWith swap_memory=False or limiting devices to cpu this does not happen.\nThe part of my code, that causes this failure (when commenting it out, it works) is in the body of the while loop:\n...\nh_gathered = h_ta.gather(tf.range(time))\nh_gathered = tf.transpose(h_gathered, [1, 0, 2])\nsyn_t = self.syntactic_weights_ta.read(time)[:, :time]\nsyn_t = tf.expand_dims(syn_t, 1)\nsyn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\n...\n\nwhere time is zero based and incremented after each step, h_ta is a TensorArray\nh_ta = tf.TensorArray(\n        dtype=dtype,\n        size=max_seq_len,\n        clear_after_read=False,\n        element_shape=[batch_size, num_hidden],\n        tensor_array_name=\"fw_output\")\n\nand self.syntactic_weights_ta is also a TensorArray\nself.syntactic_weights_ta = tf.TensorArray(\n        dtype=dtype,\n        size=max_seq_len,\n        tensor_array_name=\"fw_syntactic_weights\")\nself.syntactic_weights_ta = self.syntactic_weights_ta.unstack(syntactic_weights)\n\nWhat I am trying to achieve in the code snippet is basically a weighted sum over the past outputs, stored in h_ta.\nIn the end I train the network with tf.train.AdamOptimizer.\nThe forward propagation seems to work, as inserting tf.Print commands in the sensitive part of the code works.\nBut I guess the backward propagation\nUnfortunately I could not find out which tensor's buffer points to nullptr.", "body": "Environment: TensorFlow-gpu r1.1 build from source on Windows 10\r\n\r\nI am using the python API of TensorFlow to train a variant of an LSTM.\r\nFor that purpose I use the `tf.while_loop` function to iterate over the time steps\r\nWith `swap_memory=True` and not limiting devices to cpu I get the following failure:\r\n\r\n`...tensorflow/tensorflow/core/framework/tensor.cc:885] Check failed: nullptr != b.buf_ (nullptr vs. 00...)`\r\n\r\nWith `swap_memory=False` or limiting devices to cpu this does not happen.\r\n\r\nThe part of my code, that causes this failure (when commenting it out, it works) is in the body of the while loop:\r\n\r\n    ...\r\n    h_gathered = h_ta.gather(tf.range(time))\r\n    h_gathered = tf.transpose(h_gathered, [1, 0, 2])\r\n    syn_t = self.syntactic_weights_ta.read(time)[:, :time]\r\n    syn_t = tf.expand_dims(syn_t, 1)\r\n    syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\r\n    ...\r\n\r\nwhere `time` is zero based and incremented after each step, `h_ta` is a TensorArray\r\n\r\n    h_ta = tf.TensorArray(\r\n            dtype=dtype,\r\n            size=max_seq_len,\r\n            clear_after_read=False,\r\n            element_shape=[batch_size, num_hidden],\r\n            tensor_array_name=\"fw_output\")\r\nand `self.syntactic_weights_ta` is also a TensorArray\r\n\r\n    self.syntactic_weights_ta = tf.TensorArray(\r\n            dtype=dtype,\r\n            size=max_seq_len,\r\n            tensor_array_name=\"fw_syntactic_weights\")\r\n    self.syntactic_weights_ta = self.syntactic_weights_ta.unstack(syntactic_weights)\r\n\r\nWhat I am trying to achieve in the code snippet is basically a weighted sum over the past outputs, stored in `h_ta`.\r\nIn the end I train the network with `tf.train.AdamOptimizer`.\r\n\r\nThe forward propagation seems to work, as inserting `tf.Print` commands in the sensitive part of the code works.\r\nBut I guess the backward propagation \r\n\r\nUnfortunately I could not find out which tensor's buffer points to nullptr.\r\n"}