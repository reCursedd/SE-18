{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/303508105", "html_url": "https://github.com/tensorflow/tensorflow/issues/10083#issuecomment-303508105", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10083", "id": 303508105, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMzUwODEwNQ==", "user": {"login": "davzha", "id": 27867765, "node_id": "MDQ6VXNlcjI3ODY3NzY1", "avatar_url": "https://avatars0.githubusercontent.com/u/27867765?v=4", "gravatar_id": "", "url": "https://api.github.com/users/davzha", "html_url": "https://github.com/davzha", "followers_url": "https://api.github.com/users/davzha/followers", "following_url": "https://api.github.com/users/davzha/following{/other_user}", "gists_url": "https://api.github.com/users/davzha/gists{/gist_id}", "starred_url": "https://api.github.com/users/davzha/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/davzha/subscriptions", "organizations_url": "https://api.github.com/users/davzha/orgs", "repos_url": "https://api.github.com/users/davzha/repos", "events_url": "https://api.github.com/users/davzha/events{/privacy}", "received_events_url": "https://api.github.com/users/davzha/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-23T19:34:15Z", "updated_at": "2017-05-23T19:34:32Z", "author_association": "NONE", "body_html": "<p>I was able to reproduce the error running the following script.<br>\nSince I followed this guide <a href=\"https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\">https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake</a><br>\nto build tensorflow-gpu r1.1, it does not support debugging, though if you still require a debug log I'll try to rebuild it.</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.rnn.python.ops.lstm_ops import _lstm_block_cell\nimport numpy as np\n\n\nclass SALSTMloop(object):\n    \"\"\"Syntax aware LSTM loop, using `_lstm_block_cell`.\n    \"\"\"\n\n    def __init__(self,\n                 num_units,\n                 input_ta,\n                 syntactic_weights_ta,\n                 max_sen_len,\n                 batch_size,\n                 forget_bias=1.0):\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._names = {\n            \"W\": \"weights\",\n            \"b\": \"biases\",\n            \"W_syn\": \"weight_syntactic\",\n            \"b_syn\": \"bias_syntactic\",\n            \"wci\": \"w_i_diag\",\n            \"wco\": \"w_o_diag\",\n            \"wcf\": \"w_f_diag\",\n            \"scope\": \"lstm_cell\"\n        }\n        self.max_sen_len = max_sen_len\n        self.zero_output = tf.zeros([batch_size, num_units])\n        self.input_ta = input_ta\n        self.syntactic_weights_ta = syntactic_weights_ta\n\n    def __call__(self, time, prev_state, h_ta, scope=None):\n        with tf.variable_scope(scope or self._names[\"scope\"]):\n            x = self.input_ta.read(time)\n            input_size = x.get_shape()[1].value\n\n            w = tf.get_variable(self._names[\"W\"], [\n                input_size + self._num_units, self._num_units * 4\n            ])\n            b = tf.get_variable(\n                self._names[\"b\"], [w.get_shape().with_rank(2)[1].value],\n                initializer=tf.constant_initializer(0.0))\n            w_syn = tf.get_variable(self._names[\"W_syn\"], [\n                input_size + self._num_units, self._num_units\n            ])\n            b_syn = tf.get_variable(\n                self._names[\"b_syn\"],\n                [w_syn.get_shape().with_rank(2)[1].value],\n                initializer=tf.constant_initializer(0.0))\n            wci = wco = wcf = tf.zeros([self._num_units])\n\n            (cs_prev, h_prev) = prev_state\n            (_, cs, _, _, _, _, h) = _lstm_block_cell(\n                x,\n                cs_prev,\n                h_prev,\n                w,\n                b,\n                wci=wci,\n                wco=wco,\n                wcf=wcf,\n                forget_bias=self._forget_bias,\n                use_peephole=False)\n\n            h_gathered = h_ta.gather(tf.range(time))\n            h_gathered = tf.transpose(h_gathered, [1, 0, 2])\n            syn_t = self.syntactic_weights_ta.read(time)[:, :time]\n\n            syn_t = tf.expand_dims(syn_t, 1)\n\n            syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\n            gate_syn = tf.matmul(tf.concat([x, h_prev], 1), w_syn) + b_syn\n            state_h = tf.multiply(gate_syn, syn_state_t)\n\n            h = h + state_h\n\n            new_state = (cs, h)\n            return (time + 1), new_state, h_ta.write(time, h)\n\n\nmax_sen_len = 60\ndtype = tf.float32\nnum_hidden = 512\nbatch_size = 100\ninit_state = (\n    tf.constant(0.0, dtype=tf.float32, shape=[batch_size, num_hidden]), ) * 2\nfw_syn_ta = tf.TensorArray(\n    dtype=dtype,\n    size=max_sen_len,\n    tensor_array_name=\"syntactic_weights\",\n    clear_after_read=False)\nsyntactic_weights = tf.random_uniform([max_sen_len, batch_size, max_sen_len])\nfw_syn_ta = fw_syn_ta.unstack(syntactic_weights)\ninputs = tf.constant(\n    np.arange(batch_size * max_sen_len * max_sen_len),\n    shape=[max_sen_len, batch_size, max_sen_len],\n    dtype=tf.float32)\n\nfw_input_ta = tf.TensorArray(\n    dtype=dtype, size=max_sen_len,\n    tensor_array_name=\"fw_input\").unstack(inputs)\nfw_output_ta = tf.TensorArray(\n    dtype=dtype,\n    size=max_sen_len,\n    clear_after_read=False,\n    element_shape=[batch_size, num_hidden],\n    tensor_array_name=\"fw_output\")\nfw_loop = SALSTMloop(\n    num_units=num_hidden,\n    input_ta=fw_input_ta,\n    syntactic_weights_ta=fw_syn_ta,\n    max_sen_len=max_sen_len,\n    batch_size=batch_size)\ntime = tf.constant(0, dtype=tf.int32, name=\"time\")\n\n_, fw_final_state, fw_final_output_ta = tf.while_loop(\n    lambda time, *_: time &lt; max_sen_len,\n    fw_loop,\n    loop_vars=[time, init_state, fw_output_ta],\n    swap_memory=True)\nfw_output = fw_final_output_ta.stack()\nfw_final_output_ta.close()\n\nfinal_output = fw_final_output_ta.stack()\n\nloss = tf.reduce_sum(final_output)\n\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n\ninit_op = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init_op)\n    output, _ = sess.run([final_output, train_op])\n</code></pre>", "body_text": "I was able to reproduce the error running the following script.\nSince I followed this guide https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\nto build tensorflow-gpu r1.1, it does not support debugging, though if you still require a debug log I'll try to rebuild it.\nimport tensorflow as tf\nfrom tensorflow.contrib.rnn.python.ops.lstm_ops import _lstm_block_cell\nimport numpy as np\n\n\nclass SALSTMloop(object):\n    \"\"\"Syntax aware LSTM loop, using `_lstm_block_cell`.\n    \"\"\"\n\n    def __init__(self,\n                 num_units,\n                 input_ta,\n                 syntactic_weights_ta,\n                 max_sen_len,\n                 batch_size,\n                 forget_bias=1.0):\n        self._num_units = num_units\n        self._forget_bias = forget_bias\n        self._names = {\n            \"W\": \"weights\",\n            \"b\": \"biases\",\n            \"W_syn\": \"weight_syntactic\",\n            \"b_syn\": \"bias_syntactic\",\n            \"wci\": \"w_i_diag\",\n            \"wco\": \"w_o_diag\",\n            \"wcf\": \"w_f_diag\",\n            \"scope\": \"lstm_cell\"\n        }\n        self.max_sen_len = max_sen_len\n        self.zero_output = tf.zeros([batch_size, num_units])\n        self.input_ta = input_ta\n        self.syntactic_weights_ta = syntactic_weights_ta\n\n    def __call__(self, time, prev_state, h_ta, scope=None):\n        with tf.variable_scope(scope or self._names[\"scope\"]):\n            x = self.input_ta.read(time)\n            input_size = x.get_shape()[1].value\n\n            w = tf.get_variable(self._names[\"W\"], [\n                input_size + self._num_units, self._num_units * 4\n            ])\n            b = tf.get_variable(\n                self._names[\"b\"], [w.get_shape().with_rank(2)[1].value],\n                initializer=tf.constant_initializer(0.0))\n            w_syn = tf.get_variable(self._names[\"W_syn\"], [\n                input_size + self._num_units, self._num_units\n            ])\n            b_syn = tf.get_variable(\n                self._names[\"b_syn\"],\n                [w_syn.get_shape().with_rank(2)[1].value],\n                initializer=tf.constant_initializer(0.0))\n            wci = wco = wcf = tf.zeros([self._num_units])\n\n            (cs_prev, h_prev) = prev_state\n            (_, cs, _, _, _, _, h) = _lstm_block_cell(\n                x,\n                cs_prev,\n                h_prev,\n                w,\n                b,\n                wci=wci,\n                wco=wco,\n                wcf=wcf,\n                forget_bias=self._forget_bias,\n                use_peephole=False)\n\n            h_gathered = h_ta.gather(tf.range(time))\n            h_gathered = tf.transpose(h_gathered, [1, 0, 2])\n            syn_t = self.syntactic_weights_ta.read(time)[:, :time]\n\n            syn_t = tf.expand_dims(syn_t, 1)\n\n            syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\n            gate_syn = tf.matmul(tf.concat([x, h_prev], 1), w_syn) + b_syn\n            state_h = tf.multiply(gate_syn, syn_state_t)\n\n            h = h + state_h\n\n            new_state = (cs, h)\n            return (time + 1), new_state, h_ta.write(time, h)\n\n\nmax_sen_len = 60\ndtype = tf.float32\nnum_hidden = 512\nbatch_size = 100\ninit_state = (\n    tf.constant(0.0, dtype=tf.float32, shape=[batch_size, num_hidden]), ) * 2\nfw_syn_ta = tf.TensorArray(\n    dtype=dtype,\n    size=max_sen_len,\n    tensor_array_name=\"syntactic_weights\",\n    clear_after_read=False)\nsyntactic_weights = tf.random_uniform([max_sen_len, batch_size, max_sen_len])\nfw_syn_ta = fw_syn_ta.unstack(syntactic_weights)\ninputs = tf.constant(\n    np.arange(batch_size * max_sen_len * max_sen_len),\n    shape=[max_sen_len, batch_size, max_sen_len],\n    dtype=tf.float32)\n\nfw_input_ta = tf.TensorArray(\n    dtype=dtype, size=max_sen_len,\n    tensor_array_name=\"fw_input\").unstack(inputs)\nfw_output_ta = tf.TensorArray(\n    dtype=dtype,\n    size=max_sen_len,\n    clear_after_read=False,\n    element_shape=[batch_size, num_hidden],\n    tensor_array_name=\"fw_output\")\nfw_loop = SALSTMloop(\n    num_units=num_hidden,\n    input_ta=fw_input_ta,\n    syntactic_weights_ta=fw_syn_ta,\n    max_sen_len=max_sen_len,\n    batch_size=batch_size)\ntime = tf.constant(0, dtype=tf.int32, name=\"time\")\n\n_, fw_final_state, fw_final_output_ta = tf.while_loop(\n    lambda time, *_: time < max_sen_len,\n    fw_loop,\n    loop_vars=[time, init_state, fw_output_ta],\n    swap_memory=True)\nfw_output = fw_final_output_ta.stack()\nfw_final_output_ta.close()\n\nfinal_output = fw_final_output_ta.stack()\n\nloss = tf.reduce_sum(final_output)\n\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n\ninit_op = tf.global_variables_initializer()\nwith tf.Session() as sess:\n    sess.run(init_op)\n    output, _ = sess.run([final_output, train_op])", "body": "I was able to reproduce the error running the following script.\r\nSince I followed this guide https://github.com/tensorflow/tensorflow/tree/r0.12/tensorflow/contrib/cmake\r\nto build tensorflow-gpu r1.1, it does not support debugging, though if you still require a debug log I'll try to rebuild it.\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib.rnn.python.ops.lstm_ops import _lstm_block_cell\r\nimport numpy as np\r\n\r\n\r\nclass SALSTMloop(object):\r\n    \"\"\"Syntax aware LSTM loop, using `_lstm_block_cell`.\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 num_units,\r\n                 input_ta,\r\n                 syntactic_weights_ta,\r\n                 max_sen_len,\r\n                 batch_size,\r\n                 forget_bias=1.0):\r\n        self._num_units = num_units\r\n        self._forget_bias = forget_bias\r\n        self._names = {\r\n            \"W\": \"weights\",\r\n            \"b\": \"biases\",\r\n            \"W_syn\": \"weight_syntactic\",\r\n            \"b_syn\": \"bias_syntactic\",\r\n            \"wci\": \"w_i_diag\",\r\n            \"wco\": \"w_o_diag\",\r\n            \"wcf\": \"w_f_diag\",\r\n            \"scope\": \"lstm_cell\"\r\n        }\r\n        self.max_sen_len = max_sen_len\r\n        self.zero_output = tf.zeros([batch_size, num_units])\r\n        self.input_ta = input_ta\r\n        self.syntactic_weights_ta = syntactic_weights_ta\r\n\r\n    def __call__(self, time, prev_state, h_ta, scope=None):\r\n        with tf.variable_scope(scope or self._names[\"scope\"]):\r\n            x = self.input_ta.read(time)\r\n            input_size = x.get_shape()[1].value\r\n\r\n            w = tf.get_variable(self._names[\"W\"], [\r\n                input_size + self._num_units, self._num_units * 4\r\n            ])\r\n            b = tf.get_variable(\r\n                self._names[\"b\"], [w.get_shape().with_rank(2)[1].value],\r\n                initializer=tf.constant_initializer(0.0))\r\n            w_syn = tf.get_variable(self._names[\"W_syn\"], [\r\n                input_size + self._num_units, self._num_units\r\n            ])\r\n            b_syn = tf.get_variable(\r\n                self._names[\"b_syn\"],\r\n                [w_syn.get_shape().with_rank(2)[1].value],\r\n                initializer=tf.constant_initializer(0.0))\r\n            wci = wco = wcf = tf.zeros([self._num_units])\r\n\r\n            (cs_prev, h_prev) = prev_state\r\n            (_, cs, _, _, _, _, h) = _lstm_block_cell(\r\n                x,\r\n                cs_prev,\r\n                h_prev,\r\n                w,\r\n                b,\r\n                wci=wci,\r\n                wco=wco,\r\n                wcf=wcf,\r\n                forget_bias=self._forget_bias,\r\n                use_peephole=False)\r\n\r\n            h_gathered = h_ta.gather(tf.range(time))\r\n            h_gathered = tf.transpose(h_gathered, [1, 0, 2])\r\n            syn_t = self.syntactic_weights_ta.read(time)[:, :time]\r\n\r\n            syn_t = tf.expand_dims(syn_t, 1)\r\n\r\n            syn_state_t = tf.squeeze(tf.tanh(tf.matmul(syn_t, h_gathered)), 1)\r\n            gate_syn = tf.matmul(tf.concat([x, h_prev], 1), w_syn) + b_syn\r\n            state_h = tf.multiply(gate_syn, syn_state_t)\r\n\r\n            h = h + state_h\r\n\r\n            new_state = (cs, h)\r\n            return (time + 1), new_state, h_ta.write(time, h)\r\n\r\n\r\nmax_sen_len = 60\r\ndtype = tf.float32\r\nnum_hidden = 512\r\nbatch_size = 100\r\ninit_state = (\r\n    tf.constant(0.0, dtype=tf.float32, shape=[batch_size, num_hidden]), ) * 2\r\nfw_syn_ta = tf.TensorArray(\r\n    dtype=dtype,\r\n    size=max_sen_len,\r\n    tensor_array_name=\"syntactic_weights\",\r\n    clear_after_read=False)\r\nsyntactic_weights = tf.random_uniform([max_sen_len, batch_size, max_sen_len])\r\nfw_syn_ta = fw_syn_ta.unstack(syntactic_weights)\r\ninputs = tf.constant(\r\n    np.arange(batch_size * max_sen_len * max_sen_len),\r\n    shape=[max_sen_len, batch_size, max_sen_len],\r\n    dtype=tf.float32)\r\n\r\nfw_input_ta = tf.TensorArray(\r\n    dtype=dtype, size=max_sen_len,\r\n    tensor_array_name=\"fw_input\").unstack(inputs)\r\nfw_output_ta = tf.TensorArray(\r\n    dtype=dtype,\r\n    size=max_sen_len,\r\n    clear_after_read=False,\r\n    element_shape=[batch_size, num_hidden],\r\n    tensor_array_name=\"fw_output\")\r\nfw_loop = SALSTMloop(\r\n    num_units=num_hidden,\r\n    input_ta=fw_input_ta,\r\n    syntactic_weights_ta=fw_syn_ta,\r\n    max_sen_len=max_sen_len,\r\n    batch_size=batch_size)\r\ntime = tf.constant(0, dtype=tf.int32, name=\"time\")\r\n\r\n_, fw_final_state, fw_final_output_ta = tf.while_loop(\r\n    lambda time, *_: time < max_sen_len,\r\n    fw_loop,\r\n    loop_vars=[time, init_state, fw_output_ta],\r\n    swap_memory=True)\r\nfw_output = fw_final_output_ta.stack()\r\nfw_final_output_ta.close()\r\n\r\nfinal_output = fw_final_output_ta.stack()\r\n\r\nloss = tf.reduce_sum(final_output)\r\n\r\ntrain_op = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\r\n\r\ninit_op = tf.global_variables_initializer()\r\nwith tf.Session() as sess:\r\n    sess.run(init_op)\r\n    output, _ = sess.run([final_output, train_op])\r\n```"}