{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253106402", "html_url": "https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253106402", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4898", "id": 253106402, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzEwNjQwMg==", "user": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-12T03:02:01Z", "updated_at": "2016-10-12T03:02:01Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The batch_size is the batch size for the output; and it can be a scalar<br>\nTensor; i.e., you can use a Placeholder for it.  tf.train.batch really just<br>\nuses a Queue underneath, it's agnostic to how you feed data.</p>\n<p>Either way, if you're taking in-memory input, I still don't understand how<br>\nyou're planning to use a queue.  Do you plan to feed input directly in one<br>\npython thread, and have the eval running in a separate thread?</p>\n<p>On Tue, Oct 11, 2016 at 4:27 PM, Yandi Xia <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a> <a href=\"https://github.com/ebrevdo\">https://github.com/ebrevdo</a><br>\nI don't think so.</p>\n<p>even if using tf.train.batch with enqueue_many=True, I think a fixed<br>\nbatch_size need to be set, right? I really need a variable batch_size here.</p>\n<p>Further more, I use this input method for evaluation graph. And the graph<br>\nwill take in-memory input, rather than TFRecord or CSV files. Therefore, I<br>\ndon't think tf.train.batch can solve this problem.</p>\n<p>Thank you very much!</p>\n<p>\u2014<br>\nYou are receiving this because you were mentioned.<br>\nReply to this email directly, view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"182348872\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4898\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4898/hovercard?comment_id=253076539&amp;comment_type=issue_comment\" href=\"https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253076539\">#4898 (comment)</a>,<br>\nor mute the thread<br>\n<a href=\"https://github.com/notifications/unsubscribe-auth/ABtim7f5RBqbwxzN7u-G-6QmMzladFZJks5qzBtegaJpZM4KT_tb\">https://github.com/notifications/unsubscribe-auth/ABtim7f5RBqbwxzN7u-G-6QmMzladFZJks5qzBtegaJpZM4KT_tb</a><br>\n.</p>\n</blockquote>", "body_text": "The batch_size is the batch size for the output; and it can be a scalar\nTensor; i.e., you can use a Placeholder for it.  tf.train.batch really just\nuses a Queue underneath, it's agnostic to how you feed data.\nEither way, if you're taking in-memory input, I still don't understand how\nyou're planning to use a queue.  Do you plan to feed input directly in one\npython thread, and have the eval running in a separate thread?\nOn Tue, Oct 11, 2016 at 4:27 PM, Yandi Xia notifications@github.com wrote:\n\n@ebrevdo https://github.com/ebrevdo\nI don't think so.\neven if using tf.train.batch with enqueue_many=True, I think a fixed\nbatch_size need to be set, right? I really need a variable batch_size here.\nFurther more, I use this input method for evaluation graph. And the graph\nwill take in-memory input, rather than TFRecord or CSV files. Therefore, I\ndon't think tf.train.batch can solve this problem.\nThank you very much!\n\u2014\nYou are receiving this because you were mentioned.\nReply to this email directly, view it on GitHub\n#4898 (comment),\nor mute the thread\nhttps://github.com/notifications/unsubscribe-auth/ABtim7f5RBqbwxzN7u-G-6QmMzladFZJks5qzBtegaJpZM4KT_tb\n.", "body": "The batch_size is the batch size for the output; and it can be a scalar\nTensor; i.e., you can use a Placeholder for it.  tf.train.batch really just\nuses a Queue underneath, it's agnostic to how you feed data.\n\nEither way, if you're taking in-memory input, I still don't understand how\nyou're planning to use a queue.  Do you plan to feed input directly in one\npython thread, and have the eval running in a separate thread?\n\nOn Tue, Oct 11, 2016 at 4:27 PM, Yandi Xia notifications@github.com wrote:\n\n> @ebrevdo https://github.com/ebrevdo\n> I don't think so.\n> \n> even if using tf.train.batch with enqueue_many=True, I think a fixed\n> batch_size need to be set, right? I really need a variable batch_size here.\n> \n> Further more, I use this input method for evaluation graph. And the graph\n> will take in-memory input, rather than TFRecord or CSV files. Therefore, I\n> don't think tf.train.batch can solve this problem.\n> \n> Thank you very much!\n> \n> \u2014\n> You are receiving this because you were mentioned.\n> Reply to this email directly, view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/4898#issuecomment-253076539,\n> or mute the thread\n> https://github.com/notifications/unsubscribe-auth/ABtim7f5RBqbwxzN7u-G-6QmMzladFZJks5qzBtegaJpZM4KT_tb\n> .\n"}