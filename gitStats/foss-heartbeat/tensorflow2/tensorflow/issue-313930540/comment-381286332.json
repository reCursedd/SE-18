{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/381286332", "html_url": "https://github.com/tensorflow/tensorflow/issues/18477#issuecomment-381286332", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18477", "id": 381286332, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MTI4NjMzMg==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-13T23:54:58Z", "updated_at": "2018-04-13T23:54:58Z", "author_association": "MEMBER", "body_html": "<p>The root cause of this is that as of TensorFlow 1.7, by default we do not silently copy tensors between devices and since most operations on int32 tensors require the tensor to be placed on CPU, you get the error.</p>\n<p>As the error message suggests, in TensorFlow 1.7 you can work around this by using:</p>\n<div class=\"highlight highlight-source-python\"><pre>tf.enable_eager_execution(<span class=\"pl-v\">device_policy</span><span class=\"pl-k\">=</span>tf.contrib.eager.<span class=\"pl-c1\">DEVICE_PLACEMENT_SILENT</span>)</pre></div>\n<p>With TensorFlow 1.8 we've done a bit more to make behavior in eager and graph more aligned - so you won't need the <code>with tf.device('/gpu:0')</code> block since GPUs will be used where available and tensors silently copied if needed by default.</p>\n<p>Thus, for 1.7, try a different <code>device_policy</code> when enabling eager execution and with 1.8 the issue you described will go away.</p>\n<p>Hope that helps.</p>\n<p>Some additional details for the record:</p>\n<ul>\n<li>This silent copying could become a bottleneck in model performance, so you can revert to <code>tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT</code> to help identify the points at which copying happens.</li>\n<li>The behavior between graph and eager when it comes to placement of operations won't always be <em>exactly</em> the same as with graph execution the runtime has the benefit of seeing the complete computation to make more informed placement decisions, while with eager execution placement decisions have to be made greedily.</li>\n</ul>\n<p>Closing this out since the behavior is intentional, there is a workaround for 1.7, and this problem won't occur in 1.8. Please re-open if I've misunderstood. Thanks.</p>", "body_text": "The root cause of this is that as of TensorFlow 1.7, by default we do not silently copy tensors between devices and since most operations on int32 tensors require the tensor to be placed on CPU, you get the error.\nAs the error message suggests, in TensorFlow 1.7 you can work around this by using:\ntf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_SILENT)\nWith TensorFlow 1.8 we've done a bit more to make behavior in eager and graph more aligned - so you won't need the with tf.device('/gpu:0') block since GPUs will be used where available and tensors silently copied if needed by default.\nThus, for 1.7, try a different device_policy when enabling eager execution and with 1.8 the issue you described will go away.\nHope that helps.\nSome additional details for the record:\n\nThis silent copying could become a bottleneck in model performance, so you can revert to tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT to help identify the points at which copying happens.\nThe behavior between graph and eager when it comes to placement of operations won't always be exactly the same as with graph execution the runtime has the benefit of seeing the complete computation to make more informed placement decisions, while with eager execution placement decisions have to be made greedily.\n\nClosing this out since the behavior is intentional, there is a workaround for 1.7, and this problem won't occur in 1.8. Please re-open if I've misunderstood. Thanks.", "body": "The root cause of this is that as of TensorFlow 1.7, by default we do not silently copy tensors between devices and since most operations on int32 tensors require the tensor to be placed on CPU, you get the error.\r\n\r\nAs the error message suggests, in TensorFlow 1.7 you can work around this by using:\r\n\r\n```python\r\ntf.enable_eager_execution(device_policy=tf.contrib.eager.DEVICE_PLACEMENT_SILENT)\r\n```\r\n\r\nWith TensorFlow 1.8 we've done a bit more to make behavior in eager and graph more aligned - so you won't need the `with tf.device('/gpu:0')` block since GPUs will be used where available and tensors silently copied if needed by default. \r\n\r\nThus, for 1.7, try a different `device_policy` when enabling eager execution and with 1.8 the issue you described will go away.\r\n\r\nHope that helps.\r\n\r\nSome additional details for the record:\r\n- This silent copying could become a bottleneck in model performance, so you can revert to `tf.contrib.eager.DEVICE_PLACEMENT_EXPLICIT` to help identify the points at which copying happens.\r\n- The behavior between graph and eager when it comes to placement of operations won't always be _exactly_ the same as with graph execution the runtime has the benefit of seeing the complete computation to make more informed placement decisions, while with eager execution placement decisions have to be made greedily.\r\n\r\nClosing this out since the behavior is intentional, there is a workaround for 1.7, and this problem won't occur in 1.8. Please re-open if I've misunderstood. Thanks.\r\n"}