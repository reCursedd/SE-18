{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19812", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19812/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19812/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19812/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19812", "id": 329921993, "node_id": "MDU6SXNzdWUzMjk5MjE5OTM=", "number": 19812, "title": "[feature request] Improve multinomial sampling efficiency", "user": {"login": "Twice22", "id": 7197622, "node_id": "MDQ6VXNlcjcxOTc2MjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/7197622?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Twice22", "html_url": "https://github.com/Twice22", "followers_url": "https://api.github.com/users/Twice22/followers", "following_url": "https://api.github.com/users/Twice22/following{/other_user}", "gists_url": "https://api.github.com/users/Twice22/gists{/gist_id}", "starred_url": "https://api.github.com/users/Twice22/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Twice22/subscriptions", "organizations_url": "https://api.github.com/users/Twice22/orgs", "repos_url": "https://api.github.com/users/Twice22/repos", "events_url": "https://api.github.com/users/Twice22/events{/privacy}", "received_events_url": "https://api.github.com/users/Twice22/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "srvasude", "id": 1048839, "node_id": "MDQ6VXNlcjEwNDg4Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1048839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srvasude", "html_url": "https://github.com/srvasude", "followers_url": "https://api.github.com/users/srvasude/followers", "following_url": "https://api.github.com/users/srvasude/following{/other_user}", "gists_url": "https://api.github.com/users/srvasude/gists{/gist_id}", "starred_url": "https://api.github.com/users/srvasude/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srvasude/subscriptions", "organizations_url": "https://api.github.com/users/srvasude/orgs", "repos_url": "https://api.github.com/users/srvasude/repos", "events_url": "https://api.github.com/users/srvasude/events{/privacy}", "received_events_url": "https://api.github.com/users/srvasude/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "srvasude", "id": 1048839, "node_id": "MDQ6VXNlcjEwNDg4Mzk=", "avatar_url": "https://avatars2.githubusercontent.com/u/1048839?v=4", "gravatar_id": "", "url": "https://api.github.com/users/srvasude", "html_url": "https://github.com/srvasude", "followers_url": "https://api.github.com/users/srvasude/followers", "following_url": "https://api.github.com/users/srvasude/following{/other_user}", "gists_url": "https://api.github.com/users/srvasude/gists{/gist_id}", "starred_url": "https://api.github.com/users/srvasude/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/srvasude/subscriptions", "organizations_url": "https://api.github.com/users/srvasude/orgs", "repos_url": "https://api.github.com/users/srvasude/repos", "events_url": "https://api.github.com/users/srvasude/events{/privacy}", "received_events_url": "https://api.github.com/users/srvasude/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-06-06T15:34:37Z", "updated_at": "2018-11-20T13:28:06Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows 10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: b'v1.8.0-0-g93bc2e2072'</li>\n<li><strong>Python version</strong>:  3.5.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0</li>\n<li><strong>GPU model and memory</strong>:  Quadro M1200</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I need to perform several samplings from a multinomial distribution. The problem is that it is very slow. After debugging, using a <strong>Profiler</strong>, I've realized that it comes from this piece of code:</p>\n<p><code>math_ops.reduce_sum(array_ops.one_hot(x, depth=k), axis=-2)</code> (line 257 at this moment)</p>\n<p>from <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/distributions/multinomial.py\">this python file</a></p>\n<p>Indeed, the code actually allocate a matrix for each row we want to sample. So it allocate too much memory for no actual reason! Here is my <a href=\"https://stackoverflow.com/questions/50704004/tensorflow-efficient-multinomial-sampling-theano-x50-faster/50723793#50723793\" rel=\"nofollow\">stackoverflow post</a> for more detail about the problem</p>\n<h3>Source code / logs</h3>\n<p>Actually Theano's implementation run x25 faster as described in my stackoverflow post. Also my (not so generic and not so fast solution) runs x3 times faster than the native TensorFlow sample function. Here is a snippet for comparison:</p>\n<p>Using native implementation:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow.contrib.distributions <span class=\"pl-k\">as</span> ds\n<span class=\"pl-k\">import</span> time\n\ntf.reset_default_graph()\n\nnb_distribution <span class=\"pl-k\">=</span> <span class=\"pl-c1\">100</span> <span class=\"pl-c\"><span class=\"pl-c\">#</span> number of probabilities distribution</span>\n\nu <span class=\"pl-k\">=</span> np.random.randint(<span class=\"pl-c1\">2000</span>, <span class=\"pl-c1\">3500</span>, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>nb_distribution) <span class=\"pl-c\"><span class=\"pl-c\">#</span> define number of counts (vector of size 100 with int in 2000, 3500)</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> probsn is a matrix of probability:</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> each row of probsn contains a vector of size 30 that sums to 1</span>\nprobsn <span class=\"pl-k\">=</span> np.random.uniform(<span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>(nb_distribution, <span class=\"pl-c1\">30</span>))\nprobsn <span class=\"pl-k\">/=</span> np.sum(probsn, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)[:, <span class=\"pl-c1\">None</span>]\n\ncounts <span class=\"pl-k\">=</span> tf.Variable(u, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\nprobs <span class=\"pl-k\">=</span> tf.Variable(tf.convert_to_tensor(probsn.astype(np.float32)))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> sample from the multinomial</span>\ndist <span class=\"pl-k\">=</span> ds.Multinomial(<span class=\"pl-v\">total_count</span><span class=\"pl-k\">=</span>counts, <span class=\"pl-v\">probs</span><span class=\"pl-k\">=</span>probs)\nout <span class=\"pl-k\">=</span> dist.sample()\n\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    res <span class=\"pl-k\">=</span> sess.run(out) <span class=\"pl-c\"><span class=\"pl-c\">#</span> if remove this line the code is slower...</span>\n    start <span class=\"pl-k\">=</span> time.time()\n    res <span class=\"pl-k\">=</span> sess.run(out)\n    <span class=\"pl-c1\">print</span>(time.time() <span class=\"pl-k\">-</span> start)\n    <span class=\"pl-c1\">print</span>(np.all(u <span class=\"pl-k\">==</span> np.sum(res, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)))</pre></div>\n<p>On my computer it runs in <strong>0.05</strong> seconds</p>\n<p>And here is my own (not generic) implementation of multinomial sampling that uses<br>\n<code>tf.scatter_nd()</code> function:</p>\n<p>Using my own multinomial sampling:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">vmultinomial_sampling</span>(<span class=\"pl-smi\">counts</span>, <span class=\"pl-smi\">pvals</span>, <span class=\"pl-smi\">seed</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>):\n    k <span class=\"pl-k\">=</span> tf.shape(pvals)[<span class=\"pl-c1\">1</span>]\n    logits <span class=\"pl-k\">=</span> tf.expand_dims(tf.log(pvals), <span class=\"pl-c1\">1</span>)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">sample_single</span>(<span class=\"pl-smi\">args</span>):\n        logits_, n_draw_ <span class=\"pl-k\">=</span> args[<span class=\"pl-c1\">0</span>], args[<span class=\"pl-c1\">1</span>]\n        x <span class=\"pl-k\">=</span> tf.multinomial(logits_, n_draw_, seed)\n        indices <span class=\"pl-k\">=</span> tf.cast(tf.reshape(x, [<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>]), tf.int32)\n        updates <span class=\"pl-k\">=</span> tf.ones(n_draw_) <span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.shape(indices)[0]</span>\n        <span class=\"pl-k\">return</span> tf.scatter_nd(indices, updates, [k])\n\n    x <span class=\"pl-k\">=</span> tf.map_fn(sample_single, [logits, counts], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n\n    <span class=\"pl-k\">return</span> x\n\nxx <span class=\"pl-k\">=</span> vmultinomial_sampling(u, probsn)\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> check = tf.expand_dims(counts, 1) * probs</span>\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    sess.run(tf.global_variables_initializer())\n    res <span class=\"pl-k\">=</span> sess.run(xx) <span class=\"pl-c\"><span class=\"pl-c\">#</span> if remove this line the code is slower...</span>\n    start_t <span class=\"pl-k\">=</span> time.time()\n    res <span class=\"pl-k\">=</span> sess.run(xx)\n    <span class=\"pl-c1\">print</span>(time.time() <span class=\"pl-k\">-</span>start_t)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>print(np.sum(res, axis=1))</span>\n    <span class=\"pl-c1\">print</span>(np.all(u <span class=\"pl-k\">==</span> np.sum(res, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)))</pre></div>\n<p>On my computer the code took <strong>0.016</strong> seconds to execute.</p>\n<p>For comparison Theano's implementation takes <strong>0.0025</strong> seconds...</p>\n<p>Indeed, my implementation doesn't take advantage of parallelization (changing the <code>parallel_iterations</code> in <code>map_fn</code> doesn't change anything. I'm using my GPU) while it totally makes sense that using parallel computations will improve the code because sampling from a specific row is independent from sampling from any other rows...</p>\n<p>Do you think you can improve the code in order to avoid useless memory usage and allow parallel samplings?</p>\n<p>Thank you.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows 10\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): b'v1.8.0-0-g93bc2e2072'\nPython version:  3.5.3\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0\nGPU model and memory:  Quadro M1200\nExact command to reproduce:\n\nDescribe the problem\nI need to perform several samplings from a multinomial distribution. The problem is that it is very slow. After debugging, using a Profiler, I've realized that it comes from this piece of code:\nmath_ops.reduce_sum(array_ops.one_hot(x, depth=k), axis=-2) (line 257 at this moment)\nfrom this python file\nIndeed, the code actually allocate a matrix for each row we want to sample. So it allocate too much memory for no actual reason! Here is my stackoverflow post for more detail about the problem\nSource code / logs\nActually Theano's implementation run x25 faster as described in my stackoverflow post. Also my (not so generic and not so fast solution) runs x3 times faster than the native TensorFlow sample function. Here is a snippet for comparison:\nUsing native implementation:\nimport tensorflow as tf\nimport numpy as np\nimport tensorflow.contrib.distributions as ds\nimport time\n\ntf.reset_default_graph()\n\nnb_distribution = 100 # number of probabilities distribution\n\nu = np.random.randint(2000, 3500, size=nb_distribution) # define number of counts (vector of size 100 with int in 2000, 3500)\n\n# probsn is a matrix of probability:\n# each row of probsn contains a vector of size 30 that sums to 1\nprobsn = np.random.uniform(size=(nb_distribution, 30))\nprobsn /= np.sum(probsn, axis=1)[:, None]\n\ncounts = tf.Variable(u, dtype=tf.float32)\nprobs = tf.Variable(tf.convert_to_tensor(probsn.astype(np.float32)))\n\n# sample from the multinomial\ndist = ds.Multinomial(total_count=counts, probs=probs)\nout = dist.sample()\n\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    res = sess.run(out) # if remove this line the code is slower...\n    start = time.time()\n    res = sess.run(out)\n    print(time.time() - start)\n    print(np.all(u == np.sum(res, axis=1)))\nOn my computer it runs in 0.05 seconds\nAnd here is my own (not generic) implementation of multinomial sampling that uses\ntf.scatter_nd() function:\nUsing my own multinomial sampling:\ndef vmultinomial_sampling(counts, pvals, seed=None):\n    k = tf.shape(pvals)[1]\n    logits = tf.expand_dims(tf.log(pvals), 1)\n\n    def sample_single(args):\n        logits_, n_draw_ = args[0], args[1]\n        x = tf.multinomial(logits_, n_draw_, seed)\n        indices = tf.cast(tf.reshape(x, [-1,1]), tf.int32)\n        updates = tf.ones(n_draw_) # tf.shape(indices)[0]\n        return tf.scatter_nd(indices, updates, [k])\n\n    x = tf.map_fn(sample_single, [logits, counts], dtype=tf.float32)\n\n    return x\n\nxx = vmultinomial_sampling(u, probsn)\n# check = tf.expand_dims(counts, 1) * probs\n\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n    res = sess.run(xx) # if remove this line the code is slower...\n    start_t = time.time()\n    res = sess.run(xx)\n    print(time.time() -start_t)\n    #print(np.sum(res, axis=1))\n    print(np.all(u == np.sum(res, axis=1)))\nOn my computer the code took 0.016 seconds to execute.\nFor comparison Theano's implementation takes 0.0025 seconds...\nIndeed, my implementation doesn't take advantage of parallelization (changing the parallel_iterations in map_fn doesn't change anything. I'm using my GPU) while it totally makes sense that using parallel computations will improve the code because sampling from a specific row is independent from sampling from any other rows...\nDo you think you can improve the code in order to avoid useless memory usage and allow parallel samplings?\nThank you.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows 10\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: b'v1.8.0-0-g93bc2e2072'\r\n- **Python version**:  3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0\r\n- **GPU model and memory**:  Quadro M1200\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nI need to perform several samplings from a multinomial distribution. The problem is that it is very slow. After debugging, using a **Profiler**, I've realized that it comes from this piece of code:\r\n\r\n`math_ops.reduce_sum(array_ops.one_hot(x, depth=k), axis=-2)` (line 257 at this moment)\r\n\r\nfrom [this python file](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/distributions/multinomial.py)\r\n\r\nIndeed, the code actually allocate a matrix for each row we want to sample. So it allocate too much memory for no actual reason! Here is my [stackoverflow post](https://stackoverflow.com/questions/50704004/tensorflow-efficient-multinomial-sampling-theano-x50-faster/50723793#50723793) for more detail about the problem\r\n\r\n### Source code / logs\r\nActually Theano's implementation run x25 faster as described in my stackoverflow post. Also my (not so generic and not so fast solution) runs x3 times faster than the native TensorFlow sample function. Here is a snippet for comparison:\r\n\r\nUsing native implementation:\r\n```python\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport tensorflow.contrib.distributions as ds\r\nimport time\r\n\r\ntf.reset_default_graph()\r\n\r\nnb_distribution = 100 # number of probabilities distribution\r\n\r\nu = np.random.randint(2000, 3500, size=nb_distribution) # define number of counts (vector of size 100 with int in 2000, 3500)\r\n\r\n# probsn is a matrix of probability:\r\n# each row of probsn contains a vector of size 30 that sums to 1\r\nprobsn = np.random.uniform(size=(nb_distribution, 30))\r\nprobsn /= np.sum(probsn, axis=1)[:, None]\r\n\r\ncounts = tf.Variable(u, dtype=tf.float32)\r\nprobs = tf.Variable(tf.convert_to_tensor(probsn.astype(np.float32)))\r\n\r\n# sample from the multinomial\r\ndist = ds.Multinomial(total_count=counts, probs=probs)\r\nout = dist.sample()\r\n\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    res = sess.run(out) # if remove this line the code is slower...\r\n    start = time.time()\r\n    res = sess.run(out)\r\n    print(time.time() - start)\r\n    print(np.all(u == np.sum(res, axis=1)))\r\n```\r\n\r\nOn my computer it runs in **0.05** seconds\r\n\r\nAnd here is my own (not generic) implementation of multinomial sampling that uses\r\n`tf.scatter_nd()` function:\r\n\r\n\r\nUsing my own multinomial sampling:\r\n```python\r\ndef vmultinomial_sampling(counts, pvals, seed=None):\r\n    k = tf.shape(pvals)[1]\r\n    logits = tf.expand_dims(tf.log(pvals), 1)\r\n\r\n    def sample_single(args):\r\n        logits_, n_draw_ = args[0], args[1]\r\n        x = tf.multinomial(logits_, n_draw_, seed)\r\n        indices = tf.cast(tf.reshape(x, [-1,1]), tf.int32)\r\n        updates = tf.ones(n_draw_) # tf.shape(indices)[0]\r\n        return tf.scatter_nd(indices, updates, [k])\r\n\r\n    x = tf.map_fn(sample_single, [logits, counts], dtype=tf.float32)\r\n\r\n    return x\r\n\r\nxx = vmultinomial_sampling(u, probsn)\r\n# check = tf.expand_dims(counts, 1) * probs\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n    res = sess.run(xx) # if remove this line the code is slower...\r\n    start_t = time.time()\r\n    res = sess.run(xx)\r\n    print(time.time() -start_t)\r\n    #print(np.sum(res, axis=1))\r\n    print(np.all(u == np.sum(res, axis=1)))\r\n```\r\n\r\nOn my computer the code took **0.016** seconds to execute.\r\n\r\nFor comparison Theano's implementation takes **0.0025** seconds...\r\n\r\n\r\nIndeed, my implementation doesn't take advantage of parallelization (changing the `parallel_iterations` in `map_fn` doesn't change anything. I'm using my GPU) while it totally makes sense that using parallel computations will improve the code because sampling from a specific row is independent from sampling from any other rows...\r\n\r\n\r\nDo you think you can improve the code in order to avoid useless memory usage and allow parallel samplings?\r\n\r\nThank you."}