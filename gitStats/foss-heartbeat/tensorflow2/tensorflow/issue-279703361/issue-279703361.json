{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15150", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15150/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15150/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15150/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15150", "id": 279703361, "node_id": "MDU6SXNzdWUyNzk3MDMzNjE=", "number": 15150, "title": "`variational_recurrent` in contrib DropoutWrapper causes extreme perplexity jumps", "user": {"login": "zotroneneis", "id": 15320635, "node_id": "MDQ6VXNlcjE1MzIwNjM1", "avatar_url": "https://avatars0.githubusercontent.com/u/15320635?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zotroneneis", "html_url": "https://github.com/zotroneneis", "followers_url": "https://api.github.com/users/zotroneneis/followers", "following_url": "https://api.github.com/users/zotroneneis/following{/other_user}", "gists_url": "https://api.github.com/users/zotroneneis/gists{/gist_id}", "starred_url": "https://api.github.com/users/zotroneneis/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zotroneneis/subscriptions", "organizations_url": "https://api.github.com/users/zotroneneis/orgs", "repos_url": "https://api.github.com/users/zotroneneis/repos", "events_url": "https://api.github.com/users/zotroneneis/events{/privacy}", "received_events_url": "https://api.github.com/users/zotroneneis/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 24, "created_at": "2017-12-06T10:36:14Z", "updated_at": "2018-07-17T19:30:03Z", "closed_at": "2018-07-17T19:30:03Z", "author_association": "NONE", "body_html": "<p>I have done extensive testing of the <code>variational_recurrent</code> option in the tf.contrib dropout wrapper and neither me nor my colleagues can explain the extreme perplexity jumps that are caused by it.</p>\n<p>I am training an RNN language model on the Penn Treebank Dataset. The model code is very similar to the one provided in the <a href=\"https://www.tensorflow.org/tutorials/recurrent\" rel=\"nofollow\">TensorFlow Tutorial</a>, using the same learning parameters, hidden sizes, etc. like the MEDIUM config. I am using the newest TensorFlow version (1.4.0).</p>\n<p>Consider the following models together with the dropout values used in the tf.contrib Dropout Wrapper. If not mentioned, no further regularization was used.</p>\n<p><strong>model1:</strong></p>\n<ul>\n<li>input dropout 0.3, state dropout: 0.3, output dropout: 0.3</li>\n<li>variational_recurrent=True</li>\n<li>Perplexity test set: 83.81</li>\n<li>Perplexity validation set: 86.97</li>\n</ul>\n<p><strong>model2:</strong></p>\n<ul>\n<li>input dropout 0.5, state dropout: 0.3, output dropout: 0.5</li>\n<li>variational_recurrent=True</li>\n<li>Perplexity test set: 639.65</li>\n<li>Perplexity validation set: 686.95</li>\n</ul>\n<p><strong>model3 (as a comparison):</strong></p>\n<ul>\n<li>input dropout 0.5, output dropout: 0.5</li>\n<li>variational_recurrent=False</li>\n<li>Perplexity test set: 82.88</li>\n<li>Perplexity validation set: 86.11</li>\n</ul>\n<p>I have tested various architectures, with and without variational dropout. I could not find an explanation for the fact that the perplexity sometimes jumps up to &gt;600 when using variational dropout. Also, the effects vanish when tying the embedding and softmax weights.<br>\nIn general, variational dropout does not improve but worsen the results (which is different to the results reported in recent papers using variational dropout on the PTB dataset).</p>\n<p>To test this problem further, I have adapted the official tensorflow tutorial to use variational dropout instead of standard dropout, by removing lines 131+132 and replacing lines 218-220 with:</p>\n<pre><code>if is_training and config.keep_prob &lt; 1:\n  cell = tf.contrib.rnn.DropoutWrapper(cell,\n            input_keep_prob=config.keep_prob,\n            output_keep_prob=config.keep_prob,\n            state_keep_prob=config.keep_prob,\n            variational_recurrent=True, dtype=tf.float32,\n            input_size=config.hidden_size)\n</code></pre>\n<p>Training the medium model with this configuration causes the same issues, i.e. perplexity &gt; 600</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: custom code</li>\n<li><strong>OS Platform and Distribution</strong>:  Debian GNU/Linux 8.9 (jessie)</li>\n<li>**TensorFlow version **: v1.4.0-rc1-11-g130a514 1.4.0</li>\n<li><strong>Python version</strong>: Python 3.5.4</li>\n<li><strong>GCC/Compiler version</strong>:<br>\ntf.VERSION = 1.4.0<br>\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514<br>\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514</li>\n</ul>", "body_text": "I have done extensive testing of the variational_recurrent option in the tf.contrib dropout wrapper and neither me nor my colleagues can explain the extreme perplexity jumps that are caused by it.\nI am training an RNN language model on the Penn Treebank Dataset. The model code is very similar to the one provided in the TensorFlow Tutorial, using the same learning parameters, hidden sizes, etc. like the MEDIUM config. I am using the newest TensorFlow version (1.4.0).\nConsider the following models together with the dropout values used in the tf.contrib Dropout Wrapper. If not mentioned, no further regularization was used.\nmodel1:\n\ninput dropout 0.3, state dropout: 0.3, output dropout: 0.3\nvariational_recurrent=True\nPerplexity test set: 83.81\nPerplexity validation set: 86.97\n\nmodel2:\n\ninput dropout 0.5, state dropout: 0.3, output dropout: 0.5\nvariational_recurrent=True\nPerplexity test set: 639.65\nPerplexity validation set: 686.95\n\nmodel3 (as a comparison):\n\ninput dropout 0.5, output dropout: 0.5\nvariational_recurrent=False\nPerplexity test set: 82.88\nPerplexity validation set: 86.11\n\nI have tested various architectures, with and without variational dropout. I could not find an explanation for the fact that the perplexity sometimes jumps up to >600 when using variational dropout. Also, the effects vanish when tying the embedding and softmax weights.\nIn general, variational dropout does not improve but worsen the results (which is different to the results reported in recent papers using variational dropout on the PTB dataset).\nTo test this problem further, I have adapted the official tensorflow tutorial to use variational dropout instead of standard dropout, by removing lines 131+132 and replacing lines 218-220 with:\nif is_training and config.keep_prob < 1:\n  cell = tf.contrib.rnn.DropoutWrapper(cell,\n            input_keep_prob=config.keep_prob,\n            output_keep_prob=config.keep_prob,\n            state_keep_prob=config.keep_prob,\n            variational_recurrent=True, dtype=tf.float32,\n            input_size=config.hidden_size)\n\nTraining the medium model with this configuration causes the same issues, i.e. perplexity > 600\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): custom code\nOS Platform and Distribution:  Debian GNU/Linux 8.9 (jessie)\n**TensorFlow version **: v1.4.0-rc1-11-g130a514 1.4.0\nPython version: Python 3.5.4\nGCC/Compiler version:\ntf.VERSION = 1.4.0\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514", "body": "I have done extensive testing of the `variational_recurrent` option in the tf.contrib dropout wrapper and neither me nor my colleagues can explain the extreme perplexity jumps that are caused by it.\r\n\r\nI am training an RNN language model on the Penn Treebank Dataset. The model code is very similar to the one provided in the [TensorFlow Tutorial](https://www.tensorflow.org/tutorials/recurrent), using the same learning parameters, hidden sizes, etc. like the MEDIUM config. I am using the newest TensorFlow version (1.4.0).\r\n\r\nConsider the following models together with the dropout values used in the tf.contrib Dropout Wrapper. If not mentioned, no further regularization was used.\r\n\r\n**model1:** \r\n\r\n - input dropout 0.3, state dropout: 0.3, output dropout: 0.3\r\n - variational_recurrent=True\r\n - Perplexity test set: 83.81\r\n - Perplexity validation set: 86.97\r\n\r\n**model2:**\r\n\r\n - input dropout 0.5, state dropout: 0.3, output dropout: 0.5\r\n - variational_recurrent=True   \r\n - Perplexity test set: 639.65\r\n - Perplexity validation set: 686.95\r\n\r\n**model3 (as a comparison):**\r\n\r\n - input dropout 0.5, output dropout: 0.5\r\n - variational_recurrent=False\r\n - Perplexity test set: 82.88\r\n - Perplexity validation set: 86.11\r\n\r\n\r\nI have tested various architectures, with and without variational dropout. I could not find an explanation for the fact that the perplexity sometimes jumps up to >600 when using variational dropout. Also, the effects vanish when tying the embedding and softmax weights.\r\nIn general, variational dropout does not improve but worsen the results (which is different to the results reported in recent papers using variational dropout on the PTB dataset).\r\n\r\nTo test this problem further, I have adapted the official tensorflow tutorial to use variational dropout instead of standard dropout, by removing lines 131+132 and replacing lines 218-220 with:\r\n\r\n    if is_training and config.keep_prob < 1:\r\n      cell = tf.contrib.rnn.DropoutWrapper(cell,\r\n                input_keep_prob=config.keep_prob,\r\n                output_keep_prob=config.keep_prob,\r\n                state_keep_prob=config.keep_prob,\r\n                variational_recurrent=True, dtype=tf.float32,\r\n                input_size=config.hidden_size)\r\n\r\nTraining the medium model with this configuration causes the same issues, i.e. perplexity > 600\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: custom code\r\n- **OS Platform and Distribution**:  Debian GNU/Linux 8.9 (jessie)\r\n- **TensorFlow version **: v1.4.0-rc1-11-g130a514 1.4.0\r\n- **Python version**: Python 3.5.4 \r\n- **GCC/Compiler version**:\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-rc1-11-g130a514\r\ntf.COMPILER_VERSION = v1.4.0-rc1-11-g130a514"}