{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16576", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16576/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16576/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16576/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16576", "id": 292710497, "node_id": "MDU6SXNzdWUyOTI3MTA0OTc=", "number": 16576, "title": "Feature request: tf.estimator hyperparameter tuning", "user": {"login": "carlthome", "id": 1595907, "node_id": "MDQ6VXNlcjE1OTU5MDc=", "avatar_url": "https://avatars3.githubusercontent.com/u/1595907?v=4", "gravatar_id": "", "url": "https://api.github.com/users/carlthome", "html_url": "https://github.com/carlthome", "followers_url": "https://api.github.com/users/carlthome/followers", "following_url": "https://api.github.com/users/carlthome/following{/other_user}", "gists_url": "https://api.github.com/users/carlthome/gists{/gist_id}", "starred_url": "https://api.github.com/users/carlthome/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/carlthome/subscriptions", "organizations_url": "https://api.github.com/users/carlthome/orgs", "repos_url": "https://api.github.com/users/carlthome/repos", "events_url": "https://api.github.com/users/carlthome/events{/privacy}", "received_events_url": "https://api.github.com/users/carlthome/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2018-01-30T09:31:55Z", "updated_at": "2018-08-27T03:19:04Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<p>I'm making an issue here because I'm sure this is being worked on somewhere in this huge repo and I've failed to find it by search.</p>\n<p>Almost all models have hyperparameters that cannot be set by gradient descent (number of layers, for example). These needs to be tuned, preferably programatically with a smart strategy.</p>\n<p><strong>What's the canonical way of doing hyperparameter tuning with the tf.estimator API?</strong></p>\n<p>(also, how can we do early stopping with tf.estimator?)</p>\n<p>I'm currently wrapping around scikit-optimize which is ok, but then I'll never be able to run parallel experiments across workers, and it's a bit tricky to know if the hyperparameters will lead to OOM aside from using tf.profile.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> os\n\n<span class=\"pl-k\">from</span> skopt <span class=\"pl-k\">import</span> gp_minimize\n<span class=\"pl-k\">from</span> skopt.space <span class=\"pl-k\">import</span> Real, Categorical, Integer\n<span class=\"pl-k\">from</span> skopt.utils <span class=\"pl-k\">import</span> use_named_args\n\nlogdir <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tensorboard/<span class=\"pl-pds\">'</span></span>\nspace <span class=\"pl-k\">=</span> [\n    Real(<span class=\"pl-c1\">0.0</span>, <span class=\"pl-c1\">0.1</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>learning_rate<span class=\"pl-pds\">'</span></span>),\n    Categorical([<span class=\"pl-c1\">True</span>, <span class=\"pl-c1\">False</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>skip_connections<span class=\"pl-pds\">'</span></span>),\n    Integer(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">9</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>layers<span class=\"pl-pds\">'</span></span>)]\n\n\n<span class=\"pl-en\">@use_named_args</span>(space)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">score</span>(<span class=\"pl-k\">**</span><span class=\"pl-smi\">params</span>):\n    model_dir <span class=\"pl-k\">=</span> os.path.join(logdir, <span class=\"pl-c1\">str</span>(params))\n    estimator <span class=\"pl-k\">=</span> tf.estimator.Estimator(model_fn, model_dir, <span class=\"pl-v\">params</span><span class=\"pl-k\">=</span>params)\n    trainspec <span class=\"pl-k\">=</span> tf.estimator.TrainSpec(train_input_fn)\n    evalspec <span class=\"pl-k\">=</span> tf.estimator.EvalSpec(eval_input_fn)\n    <span class=\"pl-k\">try</span>:\n        tf.estimator.train_and_evaluate(estimator, trainspec, evalspec)\n        metrics <span class=\"pl-k\">=</span> estimator.evaluate(test_input_fn)\n        <span class=\"pl-k\">return</span> metrics[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>]\n    <span class=\"pl-k\">except</span> (tf.errors.ResourceExhaustedError, tf.train.NanLossDuringTrainingError):\n        <span class=\"pl-k\">return</span> <span class=\"pl-c1\">1e9</span>\n\n\ngp_minimize(score, space)</pre></div>", "body_text": "I'm making an issue here because I'm sure this is being worked on somewhere in this huge repo and I've failed to find it by search.\nAlmost all models have hyperparameters that cannot be set by gradient descent (number of layers, for example). These needs to be tuned, preferably programatically with a smart strategy.\nWhat's the canonical way of doing hyperparameter tuning with the tf.estimator API?\n(also, how can we do early stopping with tf.estimator?)\nI'm currently wrapping around scikit-optimize which is ok, but then I'll never be able to run parallel experiments across workers, and it's a bit tricky to know if the hyperparameters will lead to OOM aside from using tf.profile.\nimport os\n\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args\n\nlogdir = 'tensorboard/'\nspace = [\n    Real(0.0, 0.1, name='learning_rate'),\n    Categorical([True, False], name='skip_connections'),\n    Integer(1, 9, name='layers')]\n\n\n@use_named_args(space)\ndef score(**params):\n    model_dir = os.path.join(logdir, str(params))\n    estimator = tf.estimator.Estimator(model_fn, model_dir, params=params)\n    trainspec = tf.estimator.TrainSpec(train_input_fn)\n    evalspec = tf.estimator.EvalSpec(eval_input_fn)\n    try:\n        tf.estimator.train_and_evaluate(estimator, trainspec, evalspec)\n        metrics = estimator.evaluate(test_input_fn)\n        return metrics['loss']\n    except (tf.errors.ResourceExhaustedError, tf.train.NanLossDuringTrainingError):\n        return 1e9\n\n\ngp_minimize(score, space)", "body": "I'm making an issue here because I'm sure this is being worked on somewhere in this huge repo and I've failed to find it by search.\r\n\r\nAlmost all models have hyperparameters that cannot be set by gradient descent (number of layers, for example). These needs to be tuned, preferably programatically with a smart strategy.\r\n\r\n**What's the canonical way of doing hyperparameter tuning with the tf.estimator API?**\r\n\r\n(also, how can we do early stopping with tf.estimator?)\r\n\r\nI'm currently wrapping around scikit-optimize which is ok, but then I'll never be able to run parallel experiments across workers, and it's a bit tricky to know if the hyperparameters will lead to OOM aside from using tf.profile.\r\n\r\n```python\r\nimport os\r\n\r\nfrom skopt import gp_minimize\r\nfrom skopt.space import Real, Categorical, Integer\r\nfrom skopt.utils import use_named_args\r\n\r\nlogdir = 'tensorboard/'\r\nspace = [\r\n    Real(0.0, 0.1, name='learning_rate'),\r\n    Categorical([True, False], name='skip_connections'),\r\n    Integer(1, 9, name='layers')]\r\n\r\n\r\n@use_named_args(space)\r\ndef score(**params):\r\n    model_dir = os.path.join(logdir, str(params))\r\n    estimator = tf.estimator.Estimator(model_fn, model_dir, params=params)\r\n    trainspec = tf.estimator.TrainSpec(train_input_fn)\r\n    evalspec = tf.estimator.EvalSpec(eval_input_fn)\r\n    try:\r\n        tf.estimator.train_and_evaluate(estimator, trainspec, evalspec)\r\n        metrics = estimator.evaluate(test_input_fn)\r\n        return metrics['loss']\r\n    except (tf.errors.ResourceExhaustedError, tf.train.NanLossDuringTrainingError):\r\n        return 1e9\r\n\r\n\r\ngp_minimize(score, space)\r\n```"}