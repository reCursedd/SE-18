{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362401867", "html_url": "https://github.com/tensorflow/tensorflow/issues/16627#issuecomment-362401867", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16627", "id": 362401867, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjQwMTg2Nw==", "user": {"login": "jingraham", "id": 2096611, "node_id": "MDQ6VXNlcjIwOTY2MTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/2096611?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jingraham", "html_url": "https://github.com/jingraham", "followers_url": "https://api.github.com/users/jingraham/followers", "following_url": "https://api.github.com/users/jingraham/following{/other_user}", "gists_url": "https://api.github.com/users/jingraham/gists{/gist_id}", "starred_url": "https://api.github.com/users/jingraham/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jingraham/subscriptions", "organizations_url": "https://api.github.com/users/jingraham/orgs", "repos_url": "https://api.github.com/users/jingraham/repos", "events_url": "https://api.github.com/users/jingraham/events{/privacy}", "received_events_url": "https://api.github.com/users/jingraham/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-01T21:07:07Z", "updated_at": "2018-02-01T21:07:07Z", "author_association": "NONE", "body_html": "<p>I just encountered a similar bug and a weird workaround when upgrading to 1.5 on a different codebase. While trying to diagnose that problem by injecting various Print ops around the graph I found a bizarre fix: if you wrap the placeholder with a tf.identity() (or Print()) and then feed to that tensor rather than the placeholder, it stops hanging.</p>\n<p>I just tried it and, sure enough, replacing the line in the above code</p>\n<p><code>x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))</code></p>\n<p>with</p>\n<p><code>x_var = tf.identity(tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512)))</code></p>\n<p>fixes the memory hang problem for me on a K80 instance with Tensorflow 1.5. And, since that tensor still gets fed, it does not seem to affect downstream computation.</p>\n<p>Not sure what is common between the codebases, but for what it's worth the one I am working on also involves a combination of conv layers, a tf.while_loop, and placeholders with multiple unknown dimensions.</p>", "body_text": "I just encountered a similar bug and a weird workaround when upgrading to 1.5 on a different codebase. While trying to diagnose that problem by injecting various Print ops around the graph I found a bizarre fix: if you wrap the placeholder with a tf.identity() (or Print()) and then feed to that tensor rather than the placeholder, it stops hanging.\nI just tried it and, sure enough, replacing the line in the above code\nx_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))\nwith\nx_var = tf.identity(tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512)))\nfixes the memory hang problem for me on a K80 instance with Tensorflow 1.5. And, since that tensor still gets fed, it does not seem to affect downstream computation.\nNot sure what is common between the codebases, but for what it's worth the one I am working on also involves a combination of conv layers, a tf.while_loop, and placeholders with multiple unknown dimensions.", "body": "I just encountered a similar bug and a weird workaround when upgrading to 1.5 on a different codebase. While trying to diagnose that problem by injecting various Print ops around the graph I found a bizarre fix: if you wrap the placeholder with a tf.identity() (or Print()) and then feed to that tensor rather than the placeholder, it stops hanging.\r\n\r\nI just tried it and, sure enough, replacing the line in the above code\r\n\r\n`x_var = tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512))`\r\n\r\nwith\r\n\r\n`x_var = tf.identity(tf.placeholder(dtype=tf.float32, shape=(None, None, None, 512)))`\r\n\r\nfixes the memory hang problem for me on a K80 instance with Tensorflow 1.5. And, since that tensor still gets fed, it does not seem to affect downstream computation.\r\n\r\nNot sure what is common between the codebases, but for what it's worth the one I am working on also involves a combination of conv layers, a tf.while_loop, and placeholders with multiple unknown dimensions."}