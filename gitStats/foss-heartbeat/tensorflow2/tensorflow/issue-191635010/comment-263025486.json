{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263025486", "html_url": "https://github.com/tensorflow/tensorflow/issues/5845#issuecomment-263025486", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5845", "id": 263025486, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzAyNTQ4Ng==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-25T21:45:32Z", "updated_at": "2016-11-25T21:51:10Z", "author_association": "MEMBER", "body_html": "<p>Please can you at least provide the information requested in the issue reporting template.<br>\nThis sort of vague question is impossible to answer without detailed repro instructions.</p>\n<blockquote>\n<p>I follow the cifar10 example to suit my experiment<br>\nAre you comparing two different cifar10 implementations on different frameworks, or is this a program you wrote yourself?</p>\n</blockquote>\n<p>Also, please note that many equivalently named benchmarks tend to do very different amounts of work - e.g. the amount of data preprocessing (image distortions, cropping, scaling) seems to vary massively between various \"inception\" benchmarks I've looked at.  For example, MXnet samples usually work from a pre-processed image dataset whereas TF examples work from the un-preprocessed imagenet data.</p>\n<p>If you believe there is a specific bug/issue then it would be useful to provide some supporting data - e.g. a Timeline.</p>\n<p>Failing that, I would suggest that this sort of discussion is best suited to StackOverflow:</p>\n<blockquote>\n<p>For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: <a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a>. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.</p>\n</blockquote>", "body_text": "Please can you at least provide the information requested in the issue reporting template.\nThis sort of vague question is impossible to answer without detailed repro instructions.\n\nI follow the cifar10 example to suit my experiment\nAre you comparing two different cifar10 implementations on different frameworks, or is this a program you wrote yourself?\n\nAlso, please note that many equivalently named benchmarks tend to do very different amounts of work - e.g. the amount of data preprocessing (image distortions, cropping, scaling) seems to vary massively between various \"inception\" benchmarks I've looked at.  For example, MXnet samples usually work from a pre-processed image dataset whereas TF examples work from the un-preprocessed imagenet data.\nIf you believe there is a specific bug/issue then it would be useful to provide some supporting data - e.g. a Timeline.\nFailing that, I would suggest that this sort of discussion is best suited to StackOverflow:\n\nFor help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.", "body": "Please can you at least provide the information requested in the issue reporting template.\r\nThis sort of vague question is impossible to answer without detailed repro instructions.\r\n\r\n> I follow the cifar10 example to suit my experiment\r\nAre you comparing two different cifar10 implementations on different frameworks, or is this a program you wrote yourself? \r\n\r\nAlso, please note that many equivalently named benchmarks tend to do very different amounts of work - e.g. the amount of data preprocessing (image distortions, cropping, scaling) seems to vary massively between various \"inception\" benchmarks I've looked at.  For example, MXnet samples usually work from a pre-processed image dataset whereas TF examples work from the un-preprocessed imagenet data.\r\n  \r\nIf you believe there is a specific bug/issue then it would be useful to provide some supporting data - e.g. a Timeline.\r\n\r\nFailing that, I would suggest that this sort of discussion is best suited to StackOverflow:\r\n>For help and support, technical or algorithmic questions, please submit your questions to Stack Overflow: https://stackoverflow.com/questions/tagged/tensorflow. You may also find answers in our FAQ, our glossary, or in the shapes, sizes and types guide. Please do not use the mailing list or issue tracker for support.\r\n\r\n \r\n\r\n  "}