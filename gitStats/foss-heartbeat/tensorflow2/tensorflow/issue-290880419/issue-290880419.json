{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16328", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16328/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16328/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16328/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16328", "id": 290880419, "node_id": "MDU6SXNzdWUyOTA4ODA0MTk=", "number": 16328, "title": "Gradient computation across multi-GPU", "user": {"login": "holyseven", "id": 13829174, "node_id": "MDQ6VXNlcjEzODI5MTc0", "avatar_url": "https://avatars3.githubusercontent.com/u/13829174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/holyseven", "html_url": "https://github.com/holyseven", "followers_url": "https://api.github.com/users/holyseven/followers", "following_url": "https://api.github.com/users/holyseven/following{/other_user}", "gists_url": "https://api.github.com/users/holyseven/gists{/gist_id}", "starred_url": "https://api.github.com/users/holyseven/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/holyseven/subscriptions", "organizations_url": "https://api.github.com/users/holyseven/orgs", "repos_url": "https://api.github.com/users/holyseven/repos", "events_url": "https://api.github.com/users/holyseven/events{/privacy}", "received_events_url": "https://api.github.com/users/holyseven/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2018-01-23T15:30:55Z", "updated_at": "2018-01-24T14:15:40Z", "closed_at": "2018-01-24T14:15:40Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4</li>\n<li><strong>Python version</strong>: 2.7.6</li>\n<li><strong>CUDA/cuDNN version</strong>: 8.0/6.0</li>\n</ul>\n<p>I am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With <code>\\sigma^2 = mean(x^2) - mean(x)^2</code>, the gradient w.r.t. each <code>x</code> can be computed independently in the GPU that <code>x</code> is attached to.</p>\n<p>However, when computing the gradients, I met a problem: without specifying GPU device, <code>tf.gradient</code> will use the <code>\\gpu:0</code>. I cannot specify each operation of gradient computation because the gradients are computed automatically by the <code>optimizer</code> and only gradients of parameters are computed.</p>\n<p>My question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?</p>\n<p>I tried this code and get two timeline files <a href=\"https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip\">timelines.zip</a> and two snapshots bellow.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.client import timeline\n\nN_SAMPLES = 100000000\n\n\ndef all_reduce(gpu_num):\n    means = []\n    x2s = []\n    axs = []\n    for i in range(gpu_num):\n        with tf.device('/cpu:0'):\n            x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)\n        with tf.device('/gpu:%d'%i):\n            ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)\n            mean = tf.reduce_mean(ax, name='local_mean_%d'%i)\n            x2 = tf.square(ax, name='local_square_%d'%i)\n            axs.append(ax)\n            means.append(mean)\n            x2s.append(x2)\n\n    with tf.device('/gpu:0'):\n        global_mean = tf.reduce_mean(means, name='global_mean')\n        global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),\n                                 tf.square(global_mean, name='global_mean_square'),\n                                 name='global_sub')\n        print global_var.get_shape()\n\n    gs = []\n    # manually\n    # for i in range(gpu_num):\n    #     with tf.device('/gpu:%d'%i):\n    #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])\n    #         gradient_wrt_var = tf.gradients(global_var, axs[i])\n    #         gs.append(gradient_wrt_mean)\n    #         gs.append(gradient_wrt_var)\n\n    # auto by tf\n    gradient_wrt_mean = tf.gradients(global_mean, axs)\n    gradient_wrt_var = tf.gradients(global_var, axs)\n    gs.append(gradient_wrt_var)\n    gs.append(gradient_wrt_mean)\n\n    for n in tf.get_default_graph().as_graph_def().node:\n        print [n.name, n.device]\n\n    return global_mean, global_var, axs, gs\n\n\ndef main(_):\n    gpu_num = 2\n    mean_op, var_op, xs, gs = all_reduce(gpu_num)\n    x = np.random.randn(N_SAMPLES*gpu_num)\n    print np.mean(x), np.var(x)\n    feed_dict = dict()\n    for i in range(gpu_num):\n        feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]\n\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    gpu_options = tf.GPUOptions(allow_growth=False)\n    config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\n    sess = tf.Session(config=config)\n\n    # mean, var, g = sess.run([\n    #     mean_op, var_op, gs\n    # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n    # print mean, var\n\n    g = sess.run([\n        gs\n    ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n\n    # Create the Timeline object, and write it to a json\n    tl = timeline.Timeline(run_metadata.step_stats)\n    ctf = tl.generate_chrome_trace_format()\n    with open('timeline.json', 'w') as f:\n        f.write(ctf)\n\n\nif __name__ == '__main__':\n    tf.app.run()\n</code></pre>\n<p>Two figures:<br>\nauto, without specifying GPU device.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png\"><img src=\"https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>manually specifying GPU device.<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png\"><img src=\"https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png\" alt=\"image\" style=\"max-width:100%;\"></a></p>\n<p>If using <code>tf.gradient</code> without specifying GPU devices, only a <code>tf.reduce_mean</code> operation is done in <code>/gpu:1</code>. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?</p>", "body_text": "System information\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4\nPython version: 2.7.6\nCUDA/cuDNN version: 8.0/6.0\n\nI am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With \\sigma^2 = mean(x^2) - mean(x)^2, the gradient w.r.t. each x can be computed independently in the GPU that x is attached to.\nHowever, when computing the gradients, I met a problem: without specifying GPU device, tf.gradient will use the \\gpu:0. I cannot specify each operation of gradient computation because the gradients are computed automatically by the optimizer and only gradients of parameters are computed.\nMy question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?\nI tried this code and get two timeline files timelines.zip and two snapshots bellow.\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.python.client import timeline\n\nN_SAMPLES = 100000000\n\n\ndef all_reduce(gpu_num):\n    means = []\n    x2s = []\n    axs = []\n    for i in range(gpu_num):\n        with tf.device('/cpu:0'):\n            x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)\n        with tf.device('/gpu:%d'%i):\n            ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)\n            mean = tf.reduce_mean(ax, name='local_mean_%d'%i)\n            x2 = tf.square(ax, name='local_square_%d'%i)\n            axs.append(ax)\n            means.append(mean)\n            x2s.append(x2)\n\n    with tf.device('/gpu:0'):\n        global_mean = tf.reduce_mean(means, name='global_mean')\n        global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),\n                                 tf.square(global_mean, name='global_mean_square'),\n                                 name='global_sub')\n        print global_var.get_shape()\n\n    gs = []\n    # manually\n    # for i in range(gpu_num):\n    #     with tf.device('/gpu:%d'%i):\n    #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])\n    #         gradient_wrt_var = tf.gradients(global_var, axs[i])\n    #         gs.append(gradient_wrt_mean)\n    #         gs.append(gradient_wrt_var)\n\n    # auto by tf\n    gradient_wrt_mean = tf.gradients(global_mean, axs)\n    gradient_wrt_var = tf.gradients(global_var, axs)\n    gs.append(gradient_wrt_var)\n    gs.append(gradient_wrt_mean)\n\n    for n in tf.get_default_graph().as_graph_def().node:\n        print [n.name, n.device]\n\n    return global_mean, global_var, axs, gs\n\n\ndef main(_):\n    gpu_num = 2\n    mean_op, var_op, xs, gs = all_reduce(gpu_num)\n    x = np.random.randn(N_SAMPLES*gpu_num)\n    print np.mean(x), np.var(x)\n    feed_dict = dict()\n    for i in range(gpu_num):\n        feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]\n\n    run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n    run_metadata = tf.RunMetadata()\n    gpu_options = tf.GPUOptions(allow_growth=False)\n    config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\n    sess = tf.Session(config=config)\n\n    # mean, var, g = sess.run([\n    #     mean_op, var_op, gs\n    # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n    # print mean, var\n\n    g = sess.run([\n        gs\n    ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\n\n    # Create the Timeline object, and write it to a json\n    tl = timeline.Timeline(run_metadata.step_stats)\n    ctf = tl.generate_chrome_trace_format()\n    with open('timeline.json', 'w') as f:\n        f.write(ctf)\n\n\nif __name__ == '__main__':\n    tf.app.run()\n\nTwo figures:\nauto, without specifying GPU device.\n\nmanually specifying GPU device.\n\nIf using tf.gradient without specifying GPU devices, only a tf.reduce_mean operation is done in /gpu:1. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?", "body": "\r\n------------------------\r\n\r\n### System information\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.4\r\n- **Python version**: 2.7.6\r\n- **CUDA/cuDNN version**: 8.0/6.0\r\n\r\nI am trying to compute global mean and global variance for batch normalization layer across GPUs, both forward and backward should be considered. With `\\sigma^2 = mean(x^2) - mean(x)^2`, the gradient w.r.t. each `x` can be computed independently in the GPU that `x` is attached to. \r\n\r\nHowever, when computing the gradients, I met a problem: without specifying GPU device, `tf.gradient` will use the `\\gpu:0`. I cannot specify each operation of gradient computation because the gradients are computed automatically by the `optimizer` and only gradients of parameters are computed.\r\n\r\nMy question is that if a node is explicitly attached to a GPU device, why the gradient can not be attached to the same GPU device?\r\n\r\nI tried this code and get two timeline files [timelines.zip](https://github.com/tensorflow/tensorflow/files/1649923/timelines.zip) and two snapshots bellow.\r\n\r\n    import tensorflow as tf\r\n    import numpy as np\r\n    from tensorflow.python.client import timeline\r\n    \r\n    N_SAMPLES = 100000000\r\n    \r\n    \r\n    def all_reduce(gpu_num):\r\n        means = []\r\n        x2s = []\r\n        axs = []\r\n        for i in range(gpu_num):\r\n            with tf.device('/cpu:0'):\r\n                x = tf.placeholder(dtype=tf.float32, shape=[N_SAMPLES], name='local_input_%d' % i)\r\n            with tf.device('/gpu:%d'%i):\r\n                ax = tf.multiply(10.0, x, name='local_multiply_%d'%i)\r\n                mean = tf.reduce_mean(ax, name='local_mean_%d'%i)\r\n                x2 = tf.square(ax, name='local_square_%d'%i)\r\n                axs.append(ax)\r\n                means.append(mean)\r\n                x2s.append(x2)\r\n    \r\n        with tf.device('/gpu:0'):\r\n            global_mean = tf.reduce_mean(means, name='global_mean')\r\n            global_var = tf.subtract(tf.reduce_mean(x2s, name='global_x2'),\r\n                                     tf.square(global_mean, name='global_mean_square'),\r\n                                     name='global_sub')\r\n            print global_var.get_shape()\r\n    \r\n        gs = []\r\n        # manually\r\n        # for i in range(gpu_num):\r\n        #     with tf.device('/gpu:%d'%i):\r\n        #         gradient_wrt_mean = tf.gradients(global_mean, axs[i])\r\n        #         gradient_wrt_var = tf.gradients(global_var, axs[i])\r\n        #         gs.append(gradient_wrt_mean)\r\n        #         gs.append(gradient_wrt_var)\r\n    \r\n        # auto by tf\r\n        gradient_wrt_mean = tf.gradients(global_mean, axs)\r\n        gradient_wrt_var = tf.gradients(global_var, axs)\r\n        gs.append(gradient_wrt_var)\r\n        gs.append(gradient_wrt_mean)\r\n    \r\n        for n in tf.get_default_graph().as_graph_def().node:\r\n            print [n.name, n.device]\r\n    \r\n        return global_mean, global_var, axs, gs\r\n    \r\n    \r\n    def main(_):\r\n        gpu_num = 2\r\n        mean_op, var_op, xs, gs = all_reduce(gpu_num)\r\n        x = np.random.randn(N_SAMPLES*gpu_num)\r\n        print np.mean(x), np.var(x)\r\n        feed_dict = dict()\r\n        for i in range(gpu_num):\r\n            feed_dict[xs[i]] = x[i*N_SAMPLES:(i+1)*N_SAMPLES]\r\n    \r\n        run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\r\n        run_metadata = tf.RunMetadata()\r\n        gpu_options = tf.GPUOptions(allow_growth=False)\r\n        config = tf.ConfigProto(log_device_placement=False, gpu_options=gpu_options)\r\n        sess = tf.Session(config=config)\r\n    \r\n        # mean, var, g = sess.run([\r\n        #     mean_op, var_op, gs\r\n        # ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\r\n        # print mean, var\r\n    \r\n        g = sess.run([\r\n            gs\r\n        ], feed_dict=feed_dict, options=run_options, run_metadata=run_metadata)\r\n    \r\n        # Create the Timeline object, and write it to a json\r\n        tl = timeline.Timeline(run_metadata.step_stats)\r\n        ctf = tl.generate_chrome_trace_format()\r\n        with open('timeline.json', 'w') as f:\r\n            f.write(ctf)\r\n    \r\n    \r\n    if __name__ == '__main__':\r\n        tf.app.run()\r\n\r\nTwo figures:\r\nauto, without specifying GPU device.\r\n![image](https://user-images.githubusercontent.com/13829174/35196526-76c1fa20-fed3-11e7-995f-6c63813acc83.png)\r\n\r\nmanually specifying GPU device.\r\n![image](https://user-images.githubusercontent.com/13829174/35196537-93757eee-fed3-11e7-8df5-986a90a8c0f8.png)\r\n\r\nIf using `tf.gradient` without specifying GPU devices, only a `tf.reduce_mean` operation is done in `/gpu:1`. So is there some easy way that the operations of gradient computation can be assigned automatically to the corresponded GPU device?\r\n\r\n\r\n"}