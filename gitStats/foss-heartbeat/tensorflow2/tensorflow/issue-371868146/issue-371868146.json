{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23101", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23101/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23101/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23101/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23101", "id": 371868146, "node_id": "MDU6SXNzdWUzNzE4NjgxNDY=", "number": 23101, "title": "Possible bug in the design of `tf.keras.layers.Conv`", "user": {"login": "fabio12345", "id": 33119878, "node_id": "MDQ6VXNlcjMzMTE5ODc4", "avatar_url": "https://avatars1.githubusercontent.com/u/33119878?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fabio12345", "html_url": "https://github.com/fabio12345", "followers_url": "https://api.github.com/users/fabio12345/followers", "following_url": "https://api.github.com/users/fabio12345/following{/other_user}", "gists_url": "https://api.github.com/users/fabio12345/gists{/gist_id}", "starred_url": "https://api.github.com/users/fabio12345/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fabio12345/subscriptions", "organizations_url": "https://api.github.com/users/fabio12345/orgs", "repos_url": "https://api.github.com/users/fabio12345/repos", "events_url": "https://api.github.com/users/fabio12345/events{/privacy}", "received_events_url": "https://api.github.com/users/fabio12345/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097546578, "node_id": "MDU6TGFiZWwxMDk3NTQ2NTc4", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:keras", "name": "comp:keras", "color": "0052cc", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-10-19T09:03:22Z", "updated_at": "2018-11-23T18:36:56Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>TensorFlow version (you are using): 1.10</li>\n<li>Are you willing to contribute it (Yes/No): No (Don't know tf's code base well enough.)</li>\n</ul>\n<p><strong>Describe the feature and the current behavior/state.</strong><br>\nCurrent behaviour of <code>tf.layers.Conv2D</code> is that</p>\n<pre><code>import tensorflow as tf\n\nexample = tf.zeros([1, 32, 32, 1])\nexample2 = tf.zeros([1, 41, 41, 1])\n\n# Using a dilated convolution layer\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\n\nprint(convolution_op( example ))\nprint(convolution_op( example2 ))\n</code></pre>\n<p>causes the following exception</p>\n<pre><code>ValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d_2/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = &lt;4 4&gt;, input[2] = &lt;[4 4][4 4]&gt;.\n</code></pre>\n<p>The reason for this is that in the <code>build</code> function for <code>tf.keras.layers.Conv</code> (<a href=\"https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py\">https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py</a>) the convolution operation is created - however, the <em>dilated</em> convolution operation also contains padding dependent on the size of the first input to <code>call</code>.<br>\nThere's no reason for this, as the padding operation (i.e. the call to <code>tf.nn.conv2d</code>) could simply be different for each of the two <code>call</code>s.<br>\nNote that with the functional layer API it is still possible to do the reuse of the layers:</p>\n<pre><code>conv1 = tf.layers.conv2d(example, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\")\nconv2 = tf.layers.conv2d(example2, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\", reuse=True)\n</code></pre>\n<p>because this causes two separate layer creations. I am proposing that the behaviour should be similar to this.</p>\n<p>The consequence of this is that at the moment, to my knowledge, there is no good way of doing weight reuse of dilated convolution weights using <code>tf.keras.layers</code>. I think that the above is possibly a bug in the implementation - the convolution operation should maybe be create in the <code>call</code> function of <code>tf.keras.layers.Conv</code>?</p>\n<p><strong>Will this change the current api? How?</strong><br>\nThis should not raise an error:</p>\n<pre><code>import tensorflow as tf\n\nexample = tf.zeros([1, 32, 32, 1])\nexample2 = tf.zeros([1, 41, 41, 1])\n\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\n\nprint(convolution_op( example ))\nprint(convolution_op( example2 ))\n\n</code></pre>\n<p><strong>Who will benefit with this feature?</strong><br>\nEveryone, as this would be a more consistent behavior of the API, and would allow things that right now are not possible.</p>\n<p><strong>Any Other info.</strong><br>\nMore information is also in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"370534507\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/23019\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/23019/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/23019\">#23019</a> - I had raised it but it was closed without a relevant reply.</p>\n<p>Have I written custom code<br>\nas above.<br>\nOS Platform and Distribution<br>\nN/A<br>\nTensorFlow installed from<br>\npip3<br>\nBazel version<br>\nN/A<br>\nCUDA/cuDNN version<br>\nN/A<br>\nGPU model and memory<br>\nN/A<br>\nExact command to reproduce<br>\nas above<br>\nMobile device<br>\nN/A</p>", "body_text": "System information\n\nTensorFlow version (you are using): 1.10\nAre you willing to contribute it (Yes/No): No (Don't know tf's code base well enough.)\n\nDescribe the feature and the current behavior/state.\nCurrent behaviour of tf.layers.Conv2D is that\nimport tensorflow as tf\n\nexample = tf.zeros([1, 32, 32, 1])\nexample2 = tf.zeros([1, 41, 41, 1])\n\n# Using a dilated convolution layer\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\n\nprint(convolution_op( example ))\nprint(convolution_op( example2 ))\n\ncauses the following exception\nValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d_2/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.\n\nThe reason for this is that in the build function for tf.keras.layers.Conv (https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py) the convolution operation is created - however, the dilated convolution operation also contains padding dependent on the size of the first input to call.\nThere's no reason for this, as the padding operation (i.e. the call to tf.nn.conv2d) could simply be different for each of the two calls.\nNote that with the functional layer API it is still possible to do the reuse of the layers:\nconv1 = tf.layers.conv2d(example, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\")\nconv2 = tf.layers.conv2d(example2, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\", reuse=True)\n\nbecause this causes two separate layer creations. I am proposing that the behaviour should be similar to this.\nThe consequence of this is that at the moment, to my knowledge, there is no good way of doing weight reuse of dilated convolution weights using tf.keras.layers. I think that the above is possibly a bug in the implementation - the convolution operation should maybe be create in the call function of tf.keras.layers.Conv?\nWill this change the current api? How?\nThis should not raise an error:\nimport tensorflow as tf\n\nexample = tf.zeros([1, 32, 32, 1])\nexample2 = tf.zeros([1, 41, 41, 1])\n\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\n\nprint(convolution_op( example ))\nprint(convolution_op( example2 ))\n\n\nWho will benefit with this feature?\nEveryone, as this would be a more consistent behavior of the API, and would allow things that right now are not possible.\nAny Other info.\nMore information is also in #23019 - I had raised it but it was closed without a relevant reply.\nHave I written custom code\nas above.\nOS Platform and Distribution\nN/A\nTensorFlow installed from\npip3\nBazel version\nN/A\nCUDA/cuDNN version\nN/A\nGPU model and memory\nN/A\nExact command to reproduce\nas above\nMobile device\nN/A", "body": "**System information**\r\n- TensorFlow version (you are using): 1.10\r\n- Are you willing to contribute it (Yes/No): No (Don't know tf's code base well enough.)\r\n\r\n**Describe the feature and the current behavior/state.**\r\nCurrent behaviour of `tf.layers.Conv2D` is that\r\n```\r\nimport tensorflow as tf\r\n\r\nexample = tf.zeros([1, 32, 32, 1])\r\nexample2 = tf.zeros([1, 41, 41, 1])\r\n\r\n# Using a dilated convolution layer\r\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\r\n\r\nprint(convolution_op( example ))\r\nprint(convolution_op( example2 ))\r\n```\r\ncauses the following exception\r\n```\r\nValueError: Dimension size must be evenly divisible by 4 but is 49 for 'conv2d_2/SpaceToBatchND_1' (op: 'SpaceToBatchND') with input shapes: [1,41,41,1], [2], [2,2] and with computed input tensors: input[1] = <4 4>, input[2] = <[4 4][4 4]>.\r\n```\r\nThe reason for this is that in the `build` function for `tf.keras.layers.Conv` (https://github.com/tensorflow/tensorflow/blob/r1.11/tensorflow/python/keras/layers/convolutional.py) the convolution operation is created - however, the *dilated* convolution operation also contains padding dependent on the size of the first input to `call`.\r\nThere's no reason for this, as the padding operation (i.e. the call to `tf.nn.conv2d`) could simply be different for each of the two `call`s.\r\nNote that with the functional layer API it is still possible to do the reuse of the layers:\r\n```\r\nconv1 = tf.layers.conv2d(example, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\")\r\nconv2 = tf.layers.conv2d(example2, 4, 3, padding=\"SAME\", dilation_rate=4, name=\"test\", reuse=True)\r\n```\r\nbecause this causes two separate layer creations. I am proposing that the behaviour should be similar to this.\r\n\r\nThe consequence of this is that at the moment, to my knowledge, there is no good way of doing weight reuse of dilated convolution weights using `tf.keras.layers`. I think that the above is possibly a bug in the implementation - the convolution operation should maybe be create in the `call` function of `tf.keras.layers.Conv`?\r\n\r\n\r\n\r\n\r\n**Will this change the current api? How?**\r\nThis should not raise an error:\r\n```\r\nimport tensorflow as tf\r\n\r\nexample = tf.zeros([1, 32, 32, 1])\r\nexample2 = tf.zeros([1, 41, 41, 1])\r\n\r\nconvolution_op = tf.layers.Conv2D(4, 3, padding=\"SAME\", dilation_rate=4)\r\n\r\nprint(convolution_op( example ))\r\nprint(convolution_op( example2 ))\r\n\r\n```\r\n**Who will benefit with this feature?**\r\nEveryone, as this would be a more consistent behavior of the API, and would allow things that right now are not possible.\r\n\r\n**Any Other info.**\r\nMore information is also in https://github.com/tensorflow/tensorflow/issues/23019 - I had raised it but it was closed without a relevant reply.\r\n\r\nHave I written custom code \r\nas above.\r\nOS Platform and Distribution\r\nN/A\r\nTensorFlow installed from\r\npip3\r\nBazel version\r\nN/A\r\nCUDA/cuDNN version\r\nN/A\r\nGPU model and memory\r\nN/A\r\nExact command to reproduce\r\nas above\r\nMobile device\r\nN/A\r\n"}