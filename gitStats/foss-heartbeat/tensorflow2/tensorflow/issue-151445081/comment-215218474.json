{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/215218474", "html_url": "https://github.com/tensorflow/tensorflow/issues/2135#issuecomment-215218474", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2135", "id": 215218474, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNTIxODQ3NA==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-04-27T20:30:39Z", "updated_at": "2016-04-27T20:30:39Z", "author_association": "MEMBER", "body_html": "<p>Yes, it would be very useful to have better tools to analyze and predict TensorFlow program costs, in terms of time, memory and other resource needs.</p>\n<p>TensorFlow programs can be analyzed both statically and dynamically.  Your questions seem to me to suggest that maybe the properties in which you're interested can be discovered through static analysis.</p>\n<p>By and large, through static analysis one can determine the Tensor shapes and hence sizes of Variables.  Whether or not a given Variable functions as a model parameter is another question.</p>\n<p>The maximum memory required to execute a program is harder to get through static analysis because it depends on the extent to which memory intensive operations not otherwise constrained may overlap in time.  This may be non-deterministic, or determined by the runtime by rules that go beyond the denotational semantics of TensorFlow.  For example, there is the possibility of leaving values in memory for a long time, or recomputing them on demand, when the cost of doing so is less than the cost of leaving the memory occupied.  A version of this strategy is swapping values between CPU and GPU memory to free GPU memory for a period.</p>\n<p>Determining the number of operations required to execute a node is also difficult for static analysis, so long as the Op implementing the node is opaque to the analysis, i.e. if you're not able to examine the actual machine instructions generated by the compiler.  In this case it seems more practical to dynamically measure the time or cycles consumed by an Op and use that to estimate its cost.</p>\n<p>Better runtime profiling tools would be a welcome addition.</p>", "body_text": "Yes, it would be very useful to have better tools to analyze and predict TensorFlow program costs, in terms of time, memory and other resource needs.\nTensorFlow programs can be analyzed both statically and dynamically.  Your questions seem to me to suggest that maybe the properties in which you're interested can be discovered through static analysis.\nBy and large, through static analysis one can determine the Tensor shapes and hence sizes of Variables.  Whether or not a given Variable functions as a model parameter is another question.\nThe maximum memory required to execute a program is harder to get through static analysis because it depends on the extent to which memory intensive operations not otherwise constrained may overlap in time.  This may be non-deterministic, or determined by the runtime by rules that go beyond the denotational semantics of TensorFlow.  For example, there is the possibility of leaving values in memory for a long time, or recomputing them on demand, when the cost of doing so is less than the cost of leaving the memory occupied.  A version of this strategy is swapping values between CPU and GPU memory to free GPU memory for a period.\nDetermining the number of operations required to execute a node is also difficult for static analysis, so long as the Op implementing the node is opaque to the analysis, i.e. if you're not able to examine the actual machine instructions generated by the compiler.  In this case it seems more practical to dynamically measure the time or cycles consumed by an Op and use that to estimate its cost.\nBetter runtime profiling tools would be a welcome addition.", "body": "Yes, it would be very useful to have better tools to analyze and predict TensorFlow program costs, in terms of time, memory and other resource needs.\n\nTensorFlow programs can be analyzed both statically and dynamically.  Your questions seem to me to suggest that maybe the properties in which you're interested can be discovered through static analysis.  \n\nBy and large, through static analysis one can determine the Tensor shapes and hence sizes of Variables.  Whether or not a given Variable functions as a model parameter is another question.    \n\nThe maximum memory required to execute a program is harder to get through static analysis because it depends on the extent to which memory intensive operations not otherwise constrained may overlap in time.  This may be non-deterministic, or determined by the runtime by rules that go beyond the denotational semantics of TensorFlow.  For example, there is the possibility of leaving values in memory for a long time, or recomputing them on demand, when the cost of doing so is less than the cost of leaving the memory occupied.  A version of this strategy is swapping values between CPU and GPU memory to free GPU memory for a period.\n\nDetermining the number of operations required to execute a node is also difficult for static analysis, so long as the Op implementing the node is opaque to the analysis, i.e. if you're not able to examine the actual machine instructions generated by the compiler.  In this case it seems more practical to dynamically measure the time or cycles consumed by an Op and use that to estimate its cost.\n\nBetter runtime profiling tools would be a welcome addition.\n"}