{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/232504907", "pull_request_review_id": 173625040, "id": 232504907, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIzMjUwNDkwNw==", "diff_hunk": "@@ -195,6 +222,224 @@ void DoNonMaxSuppressionOp(\n   std::copy_n(selected.begin(), selected.size(), output_indices_data.data());\n }\n \n+void BatchedNonMaxSuppressionOp(OpKernelContext* context,\n+                                const Tensor& inp_boxes,\n+                                const Tensor& inp_scores,\n+                                int num_boxes,\n+                                const int max_size_per_class,\n+                                const int total_size_per_batch,\n+                                const float score_threshold,\n+                                const float iou_threshold,\n+                                bool use_static_shapes = false) {\n+\n+  int q = inp_boxes.dim_size(2);\n+  int num_classes = inp_scores.dim_size(2);\n+  //unpack along batch dimension\n+  const int num_batches = inp_boxes.dim_size(0);\n+\n+  // Default clip window of [0, 0, 1, 1] if none specified\n+  std::vector <float> clip_window{0, 0, 1, 1};\n+  \n+  // [num_batches, per_batch_size, 4]\n+  std::vector <std::vector<float>> nmsed_boxes(num_batches);\n+  // [num_batches, per_batch_size]\n+  std::vector <std::vector<float>> nmsed_scores(num_batches);\n+  // [num_batches, per_batch_size]\n+  std::vector <std::vector<float>> nmsed_classes(num_batches);\n+  // [num_batches]\n+  std::vector <int> final_valid_detections;\n+  // [num_batches, per_batch_size]\n+  std::vector <std::vector<int>> selected_indices(num_batches);\n+\n+  int per_batch_size = total_size_per_batch;\n+\n+  //perform non_max_suppression operation for each batch independently\n+  for (int batch = 0; batch < num_batches; ++batch) {\n+    // dims of per_batch_boxes [num_boxes, q, 4]\n+    Tensor per_batch_boxes = inp_boxes.Slice(batch, batch+1);\n+    // dims of per_batch_scores [num_boxes, num_classes]\n+    Tensor per_batch_scores = inp_scores.Slice(batch, batch+1);\n+\n+    struct ResultCandidate {\n+      int box_index;\n+      float score;\n+      int class_idx;\n+      float box_coord[4];\n+    };\n+\n+    auto rc_cmp = [](const ResultCandidate rc_i, const ResultCandidate rc_j) {\n+        return rc_i.score < rc_j.score;\n+    };\n+    std::priority_queue<ResultCandidate, std::vector<ResultCandidate>, \n+      decltype(rc_cmp)> result_candidate_pq(rc_cmp);\n+\n+    float * scoresData = per_batch_scores.unaligned_flat<float>().data();\n+    float * boxesData = per_batch_boxes.unaligned_flat<float>().data();\n+\n+    //Iterate through all classes\n+    for (int class_idx = 0; class_idx < num_classes; ++class_idx) {\n+      std::vector<float> scores_data;\n+      std::vector<float> boxes_data_vec;\n+\n+      for (int box = 0; box < num_boxes; ++box) {\n+        //Get the scores per class \n+        //scores_data dim is [num_boxes].\n+        scores_data.push_back(scoresData[box * num_classes + class_idx]);\n+        for(int cid = 0; cid < 4; ++cid){\n+          if(q > 1){\n+            //Get the boxes per class. boxes_data_vec dims is [num_boxes, 4]\n+            boxes_data_vec.push_back(boxesData[box * q * 4 + \n+                  class_idx * 4 + cid]);\n+          }\n+          else \n+              boxes_data_vec.push_back(boxesData[box * 4 + cid]);\n+        }\n+      }\n+        \n+      //Copy boxes_data_vec to a tensor\n+      TensorShape boxesShape({num_boxes, 4});\n+      Tensor boxes(per_batch_boxes.dtype(), boxesShape);\n+      std::copy_n(boxes_data_vec.begin(), boxes_data_vec.size(), \n+          boxes.unaligned_flat<float>().data());\n+\n+      const int output_size = std::min(max_size_per_class, num_boxes);\n+      // Do NMS, get the candidate indices of form vector<int>\n+      // Data structure for selection candidate in NMS.\n+      struct Candidate {\n+        int box_index;\n+        float score;\n+      };\n+      auto cmp = [](const Candidate bs_i, const Candidate bs_j) {\n+        return bs_i.score < bs_j.score;\n+      };\n+      std::priority_queue<Candidate, std::deque<Candidate>, decltype(cmp)>\n+        candidate_priority_queue(cmp);\n+      for (int i = 0; i < scores_data.size(); ++i) {\n+        if (scores_data[i] > score_threshold) {\n+          candidate_priority_queue.emplace(Candidate({i, scores_data[i]}));\n+        }\n+      }\n+\n+      std::vector<int> selected;\n+      std::vector<float> selected_boxes;\n+      Candidate next_candidate;\n+\n+      const Tensor const_boxes = boxes;\n+      typename TTypes<float, 2>::ConstTensor boxes_data = \n+        const_boxes.tensor<float, 2>();\n+      while (selected.size() < output_size && !candidate_priority_queue.empty())\n+      {\n+        next_candidate = candidate_priority_queue.top();\n+        candidate_priority_queue.pop();\n+\n+        // Overlapping boxes are likely to have similar scores,\n+        // therefore we iterate through the previously selected boxes backwards\n+        // in order to see if `next_candidate` should be suppressed.\n+        bool should_select = true;\n+        for (int j = selected.size() - 1; j >= 0; --j) {\n+          if (IOUGreaterThanThreshold(boxes_data, next_candidate.box_index, \n+              selected[j], iou_threshold)) {\n+            should_select = false;\n+            break;\n+          }\n+        }\n+\n+        if (should_select) {\n+          selected.push_back(next_candidate.box_index);\n+          //Add the selected box to the result candidate. Sorted by score\n+          int id = next_candidate.box_index;\n+          ResultCandidate rc = {next_candidate.box_index, next_candidate.score,\n+            class_idx, {boxes_data(id, 0), boxes_data(id, 1), boxes_data(id, 2),\n+            boxes_data(id, 3)}}; \n+          result_candidate_pq.push(rc);\n+        }\n+      }\n+\n+    }\n+    int max_detections = 0;\n+    // If use_static_shapes is false, we always pad to max_total_size\n+    if (!use_static_shapes) {\n+      max_detections = std::min((int) result_candidate_pq.size(), \n+        total_size_per_batch);\n+      per_batch_size = total_size_per_batch;\n+    }\n+    else {\n+      per_batch_size = std::min(total_size_per_batch, \n+                            max_size_per_class * num_classes);\n+      max_detections = std::min(per_batch_size, \n+                            (int) result_candidate_pq.size());\n+    }\n+\n+    final_valid_detections.push_back(max_detections);\n+\n+    int curr_total_size = max_detections;\n+    // Pick the top max_detections values \n+    while(curr_total_size > 0 && !result_candidate_pq.empty())\n+    {\n+      ResultCandidate next_candidate = result_candidate_pq.top();\n+      result_candidate_pq.pop();\n+      // Add to final output vectors\n+      nmsed_boxes[batch].push_back(\n+              std::max(next_candidate.box_coord[0], clip_window[0]));", "path": "tensorflow/core/kernels/non_max_suppression_op.cc", "position": null, "original_position": 196, "commit_id": "e433fd8adab6a6646f9c6c738ce70682a15193bb", "original_commit_id": "ff885ef229da8e0e19792b9ce6a9c506d3738157", "user": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "body": "should do min with clip_window[2].", "created_at": "2018-11-11T21:21:28Z", "updated_at": "2018-11-15T21:42:03Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/23567#discussion_r232504907", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23567", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/232504907"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/23567#discussion_r232504907"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/23567"}}, "body_html": "<p>should do min with clip_window[2].</p>", "body_text": "should do min with clip_window[2]."}