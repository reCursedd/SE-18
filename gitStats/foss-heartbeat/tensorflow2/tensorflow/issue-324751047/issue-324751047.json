{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19425", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19425/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19425/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19425/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/19425", "id": 324751047, "node_id": "MDExOlB1bGxSZXF1ZXN0MTg5MjU4MTEz", "number": 19425, "title": "[INTEL MKL] Fused quantized Ops for Intel CPU [PR for comment only].", "user": {"login": "mdfaijul", "id": 27521767, "node_id": "MDQ6VXNlcjI3NTIxNzY3", "avatar_url": "https://avatars3.githubusercontent.com/u/27521767?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mdfaijul", "html_url": "https://github.com/mdfaijul", "followers_url": "https://api.github.com/users/mdfaijul/followers", "following_url": "https://api.github.com/users/mdfaijul/following{/other_user}", "gists_url": "https://api.github.com/users/mdfaijul/gists{/gist_id}", "starred_url": "https://api.github.com/users/mdfaijul/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mdfaijul/subscriptions", "organizations_url": "https://api.github.com/users/mdfaijul/orgs", "repos_url": "https://api.github.com/users/mdfaijul/repos", "events_url": "https://api.github.com/users/mdfaijul/events{/privacy}", "received_events_url": "https://api.github.com/users/mdfaijul/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-05-20T23:15:30Z", "updated_at": "2018-08-08T00:39:06Z", "closed_at": "2018-08-08T00:39:05Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/19425", "html_url": "https://github.com/tensorflow/tensorflow/pull/19425", "diff_url": "https://github.com/tensorflow/tensorflow/pull/19425.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/19425.patch"}, "body_html": "<p>This PR is intended for comments on eight bit precision inference on Intel CPU. Please do not merge this PR. Here is summary:</p>\n<ol>\n<li>Introduce quantized Conv2D and its fused operators. (i) Conv2D (ii) Conv2D + BiasAdd (iii) Conv2D + BiasAdd + ReLU (iv) Conv2D + Relu.</li>\n<li>Adds algorithms for graph quantization [tensorflow/tools/quantization/quantize_graph.py]. The resulting graph is partially quantized, more quantized operators will be added as more implementations are done. We set round_mode to \"HALF_TO_EVEN\" in QuantizeOp since Intel CPU supports only that rounding mode. Future PR will include an implementation of MklQuantizeOp.</li>\n<li>QuantizedConv2D and its fusion works with unsigned input and signed filter on Intel CPU.</li>\n</ol>\n<p><strong>Example of fp32 graph:</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/27521767/40284734-69f76140-5c48-11e8-9a8b-46c9d9ecfcaf.png\"><img src=\"https://user-images.githubusercontent.com/27521767/40284734-69f76140-5c48-11e8-9a8b-46c9d9ecfcaf.png\" alt=\"fp_graph\" style=\"max-width:100%;\"></a></p>\n<p><strong>Example of partially quantized graph:</strong><br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/27521767/40284729-5d4af7fe-5c48-11e8-8ab9-b17bd8778982.png\"><img src=\"https://user-images.githubusercontent.com/27521767/40284729-5d4af7fe-5c48-11e8-8ab9-b17bd8778982.png\" alt=\"quant_graph\" style=\"max-width:100%;\"></a></p>", "body_text": "This PR is intended for comments on eight bit precision inference on Intel CPU. Please do not merge this PR. Here is summary:\n\nIntroduce quantized Conv2D and its fused operators. (i) Conv2D (ii) Conv2D + BiasAdd (iii) Conv2D + BiasAdd + ReLU (iv) Conv2D + Relu.\nAdds algorithms for graph quantization [tensorflow/tools/quantization/quantize_graph.py]. The resulting graph is partially quantized, more quantized operators will be added as more implementations are done. We set round_mode to \"HALF_TO_EVEN\" in QuantizeOp since Intel CPU supports only that rounding mode. Future PR will include an implementation of MklQuantizeOp.\nQuantizedConv2D and its fusion works with unsigned input and signed filter on Intel CPU.\n\nExample of fp32 graph:\n\nExample of partially quantized graph:", "body": "This PR is intended for comments on eight bit precision inference on Intel CPU. Please do not merge this PR. Here is summary:\r\n\r\n1. Introduce quantized Conv2D and its fused operators. (i) Conv2D (ii) Conv2D + BiasAdd (iii) Conv2D + BiasAdd + ReLU (iv) Conv2D + Relu.\r\n2. Adds algorithms for graph quantization [tensorflow/tools/quantization/quantize_graph.py]. The resulting graph is partially quantized, more quantized operators will be added as more implementations are done. We set round_mode to \"HALF_TO_EVEN\" in QuantizeOp since Intel CPU supports only that rounding mode. Future PR will include an implementation of MklQuantizeOp.\r\n3. QuantizedConv2D and its fusion works with unsigned input and signed filter on Intel CPU.\r\n\r\n**Example of fp32 graph:**\r\n![fp_graph](https://user-images.githubusercontent.com/27521767/40284734-69f76140-5c48-11e8-9a8b-46c9d9ecfcaf.png)\r\n\r\n**Example of partially quantized graph:** \r\n![quant_graph](https://user-images.githubusercontent.com/27521767/40284729-5d4af7fe-5c48-11e8-8ab9-b17bd8778982.png)\r\n"}