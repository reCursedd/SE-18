{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1268", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1268/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1268/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1268/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1268", "id": 135949520, "node_id": "MDU6SXNzdWUxMzU5NDk1MjA=", "number": 1268, "title": "Running language model example on multiple GPUs?", "user": {"login": "Palang2014", "id": 13631873, "node_id": "MDQ6VXNlcjEzNjMxODcz", "avatar_url": "https://avatars1.githubusercontent.com/u/13631873?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Palang2014", "html_url": "https://github.com/Palang2014", "followers_url": "https://api.github.com/users/Palang2014/followers", "following_url": "https://api.github.com/users/Palang2014/following{/other_user}", "gists_url": "https://api.github.com/users/Palang2014/gists{/gist_id}", "starred_url": "https://api.github.com/users/Palang2014/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Palang2014/subscriptions", "organizations_url": "https://api.github.com/users/Palang2014/orgs", "repos_url": "https://api.github.com/users/Palang2014/repos", "events_url": "https://api.github.com/users/Palang2014/events{/privacy}", "received_events_url": "https://api.github.com/users/Palang2014/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2016-02-24T04:15:21Z", "updated_at": "2016-08-15T23:23:44Z", "closed_at": "2016-08-15T23:23:40Z", "author_association": "NONE", "body_html": "<p>Hi all,</p>\n<p>First of all thanks for this great library and state of the art examples provided.</p>\n<p>I am trying to train a language model on multiple GPUs following your language model example(<a href=\"https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html\" rel=\"nofollow\">https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html</a>). This is because (i) Dataset is huge (ii) Vocabulary is large.</p>\n<p>I tried to change <code>with tf.device(\"/cpu:0\"):</code> in the code to something like:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">for</span> d <span class=\"pl-k\">in</span> [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:1<span class=\"pl-pds\">'</span></span>]:\n    <span class=\"pl-k\">with</span> tf.device(d):\n        embedding <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>embedding<span class=\"pl-pds\">\"</span></span>, [vocab_size, size])\n        inputs <span class=\"pl-k\">=</span> tf.nn.embedding_lookup(embedding, <span class=\"pl-c1\">self</span>._input_data) </pre></div>\n<p>But got an error from <code>MatMul</code>. It happened even when I set it to one GPU. To resolve the problem with one GPU I tried to use:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">device_for_node</span>(<span class=\"pl-smi\">n</span>):\n  <span class=\"pl-k\">if</span> n.type <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MatMul<span class=\"pl-pds\">\"</span></span>:\n      <span class=\"pl-k\">return</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/gpu:1<span class=\"pl-pds\">\"</span></span>\n  <span class=\"pl-k\">else</span>:\n      <span class=\"pl-k\">return</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>/cpu:0<span class=\"pl-pds\">\"</span></span>\n<span class=\"pl-k\">&lt;</span>some code<span class=\"pl-k\">&gt;</span>\n<span class=\"pl-k\">with</span> tf.Graph().as_default(), tf.Session() <span class=\"pl-k\">as</span> session:\n    <span class=\"pl-k\">with</span> session.graph.device( device_for_node ):</pre></div>\n<p>which worked but speed dropped significantly, e.g., from 32,000 wps to 1200 wps which apparently means that it is not a good idea probably because TF optimizes both CPU and GPU usage.<br>\nI still can not figure out how to use multiple GPUs without losing the speed for this example. Anyone has had the same problem?</p>\n<p>Thanks!<br>\nHamid</p>", "body_text": "Hi all,\nFirst of all thanks for this great library and state of the art examples provided.\nI am trying to train a language model on multiple GPUs following your language model example(https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html). This is because (i) Dataset is huge (ii) Vocabulary is large.\nI tried to change with tf.device(\"/cpu:0\"): in the code to something like:\nfor d in ['/gpu:0', '/gpu:1']:\n    with tf.device(d):\n        embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n        inputs = tf.nn.embedding_lookup(embedding, self._input_data) \nBut got an error from MatMul. It happened even when I set it to one GPU. To resolve the problem with one GPU I tried to use:\ndef device_for_node(n):\n  if n.type == \"MatMul\":\n      return \"/gpu:1\"\n  else:\n      return \"/cpu:0\"\n<some code>\nwith tf.Graph().as_default(), tf.Session() as session:\n    with session.graph.device( device_for_node ):\nwhich worked but speed dropped significantly, e.g., from 32,000 wps to 1200 wps which apparently means that it is not a good idea probably because TF optimizes both CPU and GPU usage.\nI still can not figure out how to use multiple GPUs without losing the speed for this example. Anyone has had the same problem?\nThanks!\nHamid", "body": "Hi all,\n\nFirst of all thanks for this great library and state of the art examples provided.\n\nI am trying to train a language model on multiple GPUs following your language model example(https://www.tensorflow.org/versions/r0.7/tutorials/recurrent/index.html). This is because (i) Dataset is huge (ii) Vocabulary is large. \n\nI tried to change `with tf.device(\"/cpu:0\"):` in the code to something like: \n\n``` python\nfor d in ['/gpu:0', '/gpu:1']:\n    with tf.device(d):\n        embedding = tf.get_variable(\"embedding\", [vocab_size, size])\n        inputs = tf.nn.embedding_lookup(embedding, self._input_data) \n```\n\nBut got an error from `MatMul`. It happened even when I set it to one GPU. To resolve the problem with one GPU I tried to use:\n\n``` python\ndef device_for_node(n):\n  if n.type == \"MatMul\":\n      return \"/gpu:1\"\n  else:\n      return \"/cpu:0\"\n<some code>\nwith tf.Graph().as_default(), tf.Session() as session:\n    with session.graph.device( device_for_node ):\n```\n\nwhich worked but speed dropped significantly, e.g., from 32,000 wps to 1200 wps which apparently means that it is not a good idea probably because TF optimizes both CPU and GPU usage. \nI still can not figure out how to use multiple GPUs without losing the speed for this example. Anyone has had the same problem?\n\nThanks!\nHamid\n"}