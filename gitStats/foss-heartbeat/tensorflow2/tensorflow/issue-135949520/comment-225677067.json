{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/225677067", "html_url": "https://github.com/tensorflow/tensorflow/issues/1268#issuecomment-225677067", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1268", "id": 225677067, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNTY3NzA2Nw==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-13T19:05:00Z", "updated_at": "2016-06-13T19:05:00Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=13631873\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/Palang2014\">@Palang2014</a>, a typical set up is to have all the variables on CPU, and all the computation on GPU. If you have multiple GPUs, it is a good idea to merge the gradients on them before sending the delta back to CPU.</p>\n<p>It is not a good idea to place some ops on one device, and others on another, with fine granularity. The problem with that is the data transfer. Each time your data flow across device boundary, the device needs to synchronize and copy the data over. This can easily defeats whatever computation gain you are getting.</p>", "body_text": "@Palang2014, a typical set up is to have all the variables on CPU, and all the computation on GPU. If you have multiple GPUs, it is a good idea to merge the gradients on them before sending the delta back to CPU.\nIt is not a good idea to place some ops on one device, and others on another, with fine granularity. The problem with that is the data transfer. Each time your data flow across device boundary, the device needs to synchronize and copy the data over. This can easily defeats whatever computation gain you are getting.", "body": "@Palang2014, a typical set up is to have all the variables on CPU, and all the computation on GPU. If you have multiple GPUs, it is a good idea to merge the gradients on them before sending the delta back to CPU. \n\nIt is not a good idea to place some ops on one device, and others on another, with fine granularity. The problem with that is the data transfer. Each time your data flow across device boundary, the device needs to synchronize and copy the data over. This can easily defeats whatever computation gain you are getting. \n"}