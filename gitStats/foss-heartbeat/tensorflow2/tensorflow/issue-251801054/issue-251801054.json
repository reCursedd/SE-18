{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12465", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12465/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12465/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12465/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12465", "id": 251801054, "node_id": "MDU6SXNzdWUyNTE4MDEwNTQ=", "number": 12465, "title": "Tensorflow Debugger eats disk space with RNNs", "user": {"login": "alanhdu", "id": 1914111, "node_id": "MDQ6VXNlcjE5MTQxMTE=", "avatar_url": "https://avatars1.githubusercontent.com/u/1914111?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alanhdu", "html_url": "https://github.com/alanhdu", "followers_url": "https://api.github.com/users/alanhdu/followers", "following_url": "https://api.github.com/users/alanhdu/following{/other_user}", "gists_url": "https://api.github.com/users/alanhdu/gists{/gist_id}", "starred_url": "https://api.github.com/users/alanhdu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alanhdu/subscriptions", "organizations_url": "https://api.github.com/users/alanhdu/orgs", "repos_url": "https://api.github.com/users/alanhdu/repos", "events_url": "https://api.github.com/users/alanhdu/events{/privacy}", "received_events_url": "https://api.github.com/users/alanhdu/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "caisq", "id": 16824702, "node_id": "MDQ6VXNlcjE2ODI0NzAy", "avatar_url": "https://avatars2.githubusercontent.com/u/16824702?v=4", "gravatar_id": "", "url": "https://api.github.com/users/caisq", "html_url": "https://github.com/caisq", "followers_url": "https://api.github.com/users/caisq/followers", "following_url": "https://api.github.com/users/caisq/following{/other_user}", "gists_url": "https://api.github.com/users/caisq/gists{/gist_id}", "starred_url": "https://api.github.com/users/caisq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/caisq/subscriptions", "organizations_url": "https://api.github.com/users/caisq/orgs", "repos_url": "https://api.github.com/users/caisq/repos", "events_url": "https://api.github.com/users/caisq/events{/privacy}", "received_events_url": "https://api.github.com/users/caisq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2017-08-21T23:56:09Z", "updated_at": "2018-09-30T02:23:19Z", "closed_at": "2018-09-28T21:47:50Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.3</li>\n<li><strong>Python version</strong>:  3.5</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have an RNN that I'm trying to debug using <code>LocalCLIDebugWrapperSession</code> which runs on very long sequences (4800 steps). Even for a basic LSTM cell that I apply via <code>tf.nn.dynamic_rnn</code>, <code>tfdbg</code> uses a lot of disk space (&gt;5 GB sometimes). When I accidentally collected gradient infromation via <code>tf.contrib.layers.optimize_loss(..., summaries=[\"gradients\", ...])</code>, this ballooned even more to &gt;70 GB (which promptly crashed my laptop).</p>\n<p>From inspecting the <code>tfdbg</code> dump in <code>/tmp/</code>, it looks like this is because tfdbg<code>dumps out information for each time step. Especially with</code>tf.contrib.layers.optimize_loss` capturing gradients, this means that there are hundreds (thousands?) of small files being created on each time step.</p>\n<p>In some sense, this is expected behavior (each time step represents a group of TF operations), but using 70 GB seems like a pretty sharp-edged API that's easy to mis-use. I'm not sure what to really do here -- maybe there's a way to somehow compress these files or to combine all these small files across time steps into one large file? It could also just be a documentation problem.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.3\nPython version:  3.5\n\nDescribe the problem\nI have an RNN that I'm trying to debug using LocalCLIDebugWrapperSession which runs on very long sequences (4800 steps). Even for a basic LSTM cell that I apply via tf.nn.dynamic_rnn, tfdbg uses a lot of disk space (>5 GB sometimes). When I accidentally collected gradient infromation via tf.contrib.layers.optimize_loss(..., summaries=[\"gradients\", ...]), this ballooned even more to >70 GB (which promptly crashed my laptop).\nFrom inspecting the tfdbg dump in /tmp/, it looks like this is because tfdbgdumps out information for each time step. Especially withtf.contrib.layers.optimize_loss` capturing gradients, this means that there are hundreds (thousands?) of small files being created on each time step.\nIn some sense, this is expected behavior (each time step represents a group of TF operations), but using 70 GB seems like a pretty sharp-edged API that's easy to mis-use. I'm not sure what to really do here -- maybe there's a way to somehow compress these files or to combine all these small files across time steps into one large file? It could also just be a documentation problem.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.3\r\n- **Python version**:  3.5\r\n\r\n### Describe the problem\r\n\r\nI have an RNN that I'm trying to debug using `LocalCLIDebugWrapperSession` which runs on very long sequences (4800 steps). Even for a basic LSTM cell that I apply via `tf.nn.dynamic_rnn`, `tfdbg` uses a lot of disk space (>5 GB sometimes). When I accidentally collected gradient infromation via `tf.contrib.layers.optimize_loss(..., summaries=[\"gradients\", ...])`, this ballooned even more to >70 GB (which promptly crashed my laptop).\r\n\r\nFrom inspecting the `tfdbg` dump in `/tmp/`, it looks like this is because tfdbg` dumps out information for each time step. Especially with `tf.contrib.layers.optimize_loss` capturing gradients, this means that there are hundreds (thousands?) of small files being created on each time step.\r\n\r\nIn some sense, this is expected behavior (each time step represents a group of TF operations), but using 70 GB seems like a pretty sharp-edged API that's easy to mis-use. I'm not sure what to really do here -- maybe there's a way to somehow compress these files or to combine all these small files across time steps into one large file? It could also just be a documentation problem."}