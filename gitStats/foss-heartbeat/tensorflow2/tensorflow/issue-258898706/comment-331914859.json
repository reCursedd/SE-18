{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331914859", "html_url": "https://github.com/tensorflow/tensorflow/issues/13160#issuecomment-331914859", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13160", "id": 331914859, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTkxNDg1OQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-25T15:18:39Z", "updated_at": "2017-09-25T15:19:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>@biolee btw, here's an <a href=\"https://github.com/yaroslavvb/stuff/blob/master/saving%20memory%20by%20using%20functions.ipynb\">example</a> of saving memory using functions. A function seems to be treated by tf.gradients as a single computation block and values inside are automatically recomputable.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a><br>\nI'm a bit confused on whether we can rely on this behavior in our models using default <code>tf.Defun</code> or whether the framework may start looking reusing computation inside functions at any version.</p>\n<p>Some places use <code>noinline=True</code> attribute but the tests say its broken</p>\n<p>(<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/01daba61e3a5099c6ad6439fa47e30c71560f06b/tensorflow/compiler/tests/function_test.py#L106\">tensorflow/tensorflow/compiler/tests/function_test.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 106\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/01daba61e3a5099c6ad6439fa47e30c71560f06b\">01daba6</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L106\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"106\"></td>\n          <td id=\"LC106\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">TODO</span>(b/36139787): Re-enable this test when noinline works again.</span> </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n)</p>", "body_text": "@biolee btw, here's an example of saving memory using functions. A function seems to be treated by tf.gradients as a single computation block and values inside are automatically recomputable.\n@zheng-xq\nI'm a bit confused on whether we can rely on this behavior in our models using default tf.Defun or whether the framework may start looking reusing computation inside functions at any version.\nSome places use noinline=True attribute but the tests say its broken\n(\n  \n    \n      tensorflow/tensorflow/compiler/tests/function_test.py\n    \n    \n         Line 106\n      in\n      01daba6\n    \n    \n    \n    \n\n        \n          \n           # TODO(b/36139787): Re-enable this test when noinline works again. \n        \n    \n  \n\n)", "body": "@biolee btw, here's an [example](https://github.com/yaroslavvb/stuff/blob/master/saving%20memory%20by%20using%20functions.ipynb) of saving memory using functions. A function seems to be treated by tf.gradients as a single computation block and values inside are automatically recomputable.\r\n\r\n@zheng-xq \r\nI'm a bit confused on whether we can rely on this behavior in our models using default `tf.Defun` or whether the framework may start looking reusing computation inside functions at any version.\r\n\r\nSome places use `noinline=True` attribute but the tests say its broken\r\n\r\n(https://github.com/tensorflow/tensorflow/blob/01daba61e3a5099c6ad6439fa47e30c71560f06b/tensorflow/compiler/tests/function_test.py#L106)\r\n"}