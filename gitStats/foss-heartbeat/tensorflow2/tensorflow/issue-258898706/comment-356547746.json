{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356547746", "html_url": "https://github.com/tensorflow/tensorflow/issues/13160#issuecomment-356547746", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13160", "id": 356547746, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjU0Nzc0Ng==", "user": {"login": "zffchen78", "id": 7943790, "node_id": "MDQ6VXNlcjc5NDM3OTA=", "avatar_url": "https://avatars3.githubusercontent.com/u/7943790?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zffchen78", "html_url": "https://github.com/zffchen78", "followers_url": "https://api.github.com/users/zffchen78/followers", "following_url": "https://api.github.com/users/zffchen78/following{/other_user}", "gists_url": "https://api.github.com/users/zffchen78/gists{/gist_id}", "starred_url": "https://api.github.com/users/zffchen78/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zffchen78/subscriptions", "organizations_url": "https://api.github.com/users/zffchen78/orgs", "repos_url": "https://api.github.com/users/zffchen78/repos", "events_url": "https://api.github.com/users/zffchen78/events{/privacy}", "received_events_url": "https://api.github.com/users/zffchen78/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-10T09:27:03Z", "updated_at": "2018-01-10T09:27:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The forementioned mechanism should be working. The test here illustrates w/ vs. w/o marking the forward function as noinline=True.</p>\n<p><div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/25d275280dfb163674f81c7681c2c1d34545a155/tensorflow/python/framework/function_test.py#L1435\">tensorflow/tensorflow/python/framework/function_test.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 1435\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/25d275280dfb163674f81c7681c2c1d34545a155\">25d2752</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L1435\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1435\"></td>\n          <td id=\"LC1435\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">for</span> noinline <span class=\"pl-k\">in</span> [<span class=\"pl-c1\">False</span>, <span class=\"pl-c1\">True</span>]: </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>To measure the runtime's peak memory usage, one can use ops demonstrated by this test:<br>\n<a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/memory_stats/python/kernel_tests/memory_stats_ops_test.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/memory_stats/python/kernel_tests/memory_stats_ops_test.py</a></p>\n<p>To see the effect more pronounced, one can increase the vector size (16 to something much larger):<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/25d275280dfb163674f81c7681c2c1d34545a155/tensorflow/python/framework/function_test.py#L1459\">tensorflow/tensorflow/python/framework/function_test.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 1459\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/25d275280dfb163674f81c7681c2c1d34545a155\">25d2752</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L1459\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"1459\"></td>\n          <td id=\"LC1459\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> inp <span class=\"pl-k\">=</span> np.random.uniform(<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>, [<span class=\"pl-c1\">16</span>, <span class=\"pl-c1\">1</span>]).astype(np.float32) </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a> I can not predict future. But several production scale models I'm working on depends on noinline=True to be respected and if it were broken somehow, some production teams would complain badly.</p>", "body_text": "The forementioned mechanism should be working. The test here illustrates w/ vs. w/o marking the forward function as noinline=True.\n\n  \n    \n      tensorflow/tensorflow/python/framework/function_test.py\n    \n    \n         Line 1435\n      in\n      25d2752\n    \n    \n    \n    \n\n        \n          \n           for noinline in [False, True]: \n        \n    \n  \n\n\nTo measure the runtime's peak memory usage, one can use ops demonstrated by this test:\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/memory_stats/python/kernel_tests/memory_stats_ops_test.py\nTo see the effect more pronounced, one can increase the vector size (16 to something much larger):\n\n  \n    \n      tensorflow/tensorflow/python/framework/function_test.py\n    \n    \n         Line 1459\n      in\n      25d2752\n    \n    \n    \n    \n\n        \n          \n           inp = np.random.uniform(-1, 1, [16, 1]).astype(np.float32) \n        \n    \n  \n\n\n@yaroslavvb I can not predict future. But several production scale models I'm working on depends on noinline=True to be respected and if it were broken somehow, some production teams would complain badly.", "body": "The forementioned mechanism should be working. The test here illustrates w/ vs. w/o marking the forward function as noinline=True. \r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/25d275280dfb163674f81c7681c2c1d34545a155/tensorflow/python/framework/function_test.py#L1435\r\n\r\nTo measure the runtime's peak memory usage, one can use ops demonstrated by this test:\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/memory_stats/python/kernel_tests/memory_stats_ops_test.py\r\n\r\nTo see the effect more pronounced, one can increase the vector size (16 to something much larger):\r\nhttps://github.com/tensorflow/tensorflow/blob/25d275280dfb163674f81c7681c2c1d34545a155/tensorflow/python/framework/function_test.py#L1459\r\n\r\n@yaroslavvb I can not predict future. But several production scale models I'm working on depends on noinline=True to be respected and if it were broken somehow, some production teams would complain badly."}