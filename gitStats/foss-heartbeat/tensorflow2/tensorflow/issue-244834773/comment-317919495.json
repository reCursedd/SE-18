{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/317919495", "html_url": "https://github.com/tensorflow/tensorflow/issues/11681#issuecomment-317919495", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11681", "id": 317919495, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNzkxOTQ5NQ==", "user": {"login": "ZhongBaby", "id": 8222739, "node_id": "MDQ6VXNlcjgyMjI3Mzk=", "avatar_url": "https://avatars1.githubusercontent.com/u/8222739?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ZhongBaby", "html_url": "https://github.com/ZhongBaby", "followers_url": "https://api.github.com/users/ZhongBaby/followers", "following_url": "https://api.github.com/users/ZhongBaby/following{/other_user}", "gists_url": "https://api.github.com/users/ZhongBaby/gists{/gist_id}", "starred_url": "https://api.github.com/users/ZhongBaby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ZhongBaby/subscriptions", "organizations_url": "https://api.github.com/users/ZhongBaby/orgs", "repos_url": "https://api.github.com/users/ZhongBaby/repos", "events_url": "https://api.github.com/users/ZhongBaby/events{/privacy}", "received_events_url": "https://api.github.com/users/ZhongBaby/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-26T01:24:39Z", "updated_at": "2017-07-26T01:24:39Z", "author_association": "NONE", "body_html": "<p>Ok, I think the neural network layers are ops, so they won't be automatically placed on different GPU. So the best practice is to use same type of GPUs, and get them synchronized parameters, asynchronously trained on different batches.</p>", "body_text": "Ok, I think the neural network layers are ops, so they won't be automatically placed on different GPU. So the best practice is to use same type of GPUs, and get them synchronized parameters, asynchronously trained on different batches.", "body": "Ok, I think the neural network layers are ops, so they won't be automatically placed on different GPU. So the best practice is to use same type of GPUs, and get them synchronized parameters, asynchronously trained on different batches."}