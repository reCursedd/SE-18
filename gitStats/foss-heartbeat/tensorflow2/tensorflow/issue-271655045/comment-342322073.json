{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/342322073", "html_url": "https://github.com/tensorflow/tensorflow/issues/14303#issuecomment-342322073", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14303", "id": 342322073, "node_id": "MDEyOklzc3VlQ29tbWVudDM0MjMyMjA3Mw==", "user": {"login": "rasmi", "id": 2267370, "node_id": "MDQ6VXNlcjIyNjczNzA=", "avatar_url": "https://avatars1.githubusercontent.com/u/2267370?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rasmi", "html_url": "https://github.com/rasmi", "followers_url": "https://api.github.com/users/rasmi/followers", "following_url": "https://api.github.com/users/rasmi/following{/other_user}", "gists_url": "https://api.github.com/users/rasmi/gists{/gist_id}", "starred_url": "https://api.github.com/users/rasmi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rasmi/subscriptions", "organizations_url": "https://api.github.com/users/rasmi/orgs", "repos_url": "https://api.github.com/users/rasmi/repos", "events_url": "https://api.github.com/users/rasmi/events{/privacy}", "received_events_url": "https://api.github.com/users/rasmi/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-06T23:23:16Z", "updated_at": "2017-11-06T23:23:55Z", "author_association": "MEMBER", "body_html": "<p>Here is a quick workaround. Essentially, define a wrapper function which gets <code>num_classes</code> from elsewhere in your model.py and passes it to <code>eval_metrics</code>. But ideally this can be inferred from elsewhere in the graph.</p>\n<pre><code>def mean_per_class_accuracy(labels, predictions):\n      return tf.metrics.mean_per_class_accuracy(labels, predictions, num_classes=len(model.LABELS))\n\nreturn tf.contrib.learn.Experiment(\n        train_input_fn = train_input,\n        eval_input_fn = eval_input,\n        train_steps = 10000,\n        eval_steps = 100,\n        eval_metrics = {\n          \"mean_per_class_accuracy\": tf.contrib.learn.MetricSpec(\n            metric_fn=mean_per_class_accuracy,\n            prediction_key=tf.contrib.learn.PredictionKey.CLASSES\n          )\n      }\n\n</code></pre>", "body_text": "Here is a quick workaround. Essentially, define a wrapper function which gets num_classes from elsewhere in your model.py and passes it to eval_metrics. But ideally this can be inferred from elsewhere in the graph.\ndef mean_per_class_accuracy(labels, predictions):\n      return tf.metrics.mean_per_class_accuracy(labels, predictions, num_classes=len(model.LABELS))\n\nreturn tf.contrib.learn.Experiment(\n        train_input_fn = train_input,\n        eval_input_fn = eval_input,\n        train_steps = 10000,\n        eval_steps = 100,\n        eval_metrics = {\n          \"mean_per_class_accuracy\": tf.contrib.learn.MetricSpec(\n            metric_fn=mean_per_class_accuracy,\n            prediction_key=tf.contrib.learn.PredictionKey.CLASSES\n          )\n      }", "body": "Here is a quick workaround. Essentially, define a wrapper function which gets `num_classes` from elsewhere in your model.py and passes it to `eval_metrics`. But ideally this can be inferred from elsewhere in the graph.\r\n```\r\ndef mean_per_class_accuracy(labels, predictions):\r\n      return tf.metrics.mean_per_class_accuracy(labels, predictions, num_classes=len(model.LABELS))\r\n\r\nreturn tf.contrib.learn.Experiment(\r\n        train_input_fn = train_input,\r\n        eval_input_fn = eval_input,\r\n        train_steps = 10000,\r\n        eval_steps = 100,\r\n        eval_metrics = {\r\n          \"mean_per_class_accuracy\": tf.contrib.learn.MetricSpec(\r\n            metric_fn=mean_per_class_accuracy,\r\n            prediction_key=tf.contrib.learn.PredictionKey.CLASSES\r\n          )\r\n      }\r\n\r\n```\r\n"}