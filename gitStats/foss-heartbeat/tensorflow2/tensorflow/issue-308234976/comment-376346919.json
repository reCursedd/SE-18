{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/376346919", "html_url": "https://github.com/tensorflow/tensorflow/issues/17968#issuecomment-376346919", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17968", "id": 376346919, "node_id": "MDEyOklzc3VlQ29tbWVudDM3NjM0NjkxOQ==", "user": {"login": "harryxu-yscz", "id": 25258500, "node_id": "MDQ6VXNlcjI1MjU4NTAw", "avatar_url": "https://avatars1.githubusercontent.com/u/25258500?v=4", "gravatar_id": "", "url": "https://api.github.com/users/harryxu-yscz", "html_url": "https://github.com/harryxu-yscz", "followers_url": "https://api.github.com/users/harryxu-yscz/followers", "following_url": "https://api.github.com/users/harryxu-yscz/following{/other_user}", "gists_url": "https://api.github.com/users/harryxu-yscz/gists{/gist_id}", "starred_url": "https://api.github.com/users/harryxu-yscz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/harryxu-yscz/subscriptions", "organizations_url": "https://api.github.com/users/harryxu-yscz/orgs", "repos_url": "https://api.github.com/users/harryxu-yscz/repos", "events_url": "https://api.github.com/users/harryxu-yscz/events{/privacy}", "received_events_url": "https://api.github.com/users/harryxu-yscz/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-26T23:38:57Z", "updated_at": "2018-03-26T23:38:57Z", "author_association": "NONE", "body_html": "<p>Unfortunate I am not able to provide a snippet that reproduces the problem. I hope the call stacks are helpful.</p>\n<p>Strangely, my friend ran exactly the same code on his machine and had no problem with it. Am I running into the hardware issue here <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148393708\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1947\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1947/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1947\">#1947</a>?</p>\n<p>The LSTM is written as:</p>\n<pre><code>        import tensorflow.contrib as tc\n        # bi-lstm\n        lstm_fw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\n                                         num_units=hidden_size,\n                                         input_size=input_size,\n                                         # dropout = 1 - keep_prob\n                                         dropout=0.)\n        lstm_bw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\n                                         num_units=hidden_size,\n                                         input_size=input_size,\n                                         # dropout = 1 - keep_prob\n                                         dropout=0.)\n        param_fw = tf.get_variable(\"lstm_fw_params\",\n                                   initializer=tf.random_uniform([lstm_fw.params_size()],\n                                                                 -0.1, 0.1),\n                                   validate_shape=False)\n        param_bw = tf.get_variable(\"lstm_bw_params\",\n                                   initializer=tf.random_uniform([lstm_bw.params_size()],\n                                                                 -0.1, 0.1),\n                                   validate_shape=False)\n        c_fw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        c_bw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        h_fw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        h_bw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        with tf.variable_scope(\"fw\"):\n            out_fw, h_fw, c_fw = lstm_fw(inputs, h_fw, c_fw, param_fw)\n        with tf.variable_scope(\"bw\"):\n            inputs_bw = tf.reverse_sequence(\n                inputs, seq_lengths=length, seq_dim=0, batch_dim=1)\n            out_bw, h_bw, c_bw = lstm_bw(inputs_bw, h_bw, c_bw, param_bw)\n            out_bw = tf.reverse_sequence(\n                out_bw, seq_lengths=length, seq_dim=0, batch_dim=1)\n        output = tf.concat([out_fw, out_bw], axis=2)\n        output = tf.transpose(output, [1, 0, 2])\n        output_h = tf.concat([h_fw, h_bw], axis=0)\n        output_c = tf.concat([c_fw, c_bw], axis=0)\n        return output, (tc.rnn.LSTMStateTuple(h=output_h, c=output_c),)\n</code></pre>", "body_text": "Unfortunate I am not able to provide a snippet that reproduces the problem. I hope the call stacks are helpful.\nStrangely, my friend ran exactly the same code on his machine and had no problem with it. Am I running into the hardware issue here #1947?\nThe LSTM is written as:\n        import tensorflow.contrib as tc\n        # bi-lstm\n        lstm_fw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\n                                         num_units=hidden_size,\n                                         input_size=input_size,\n                                         # dropout = 1 - keep_prob\n                                         dropout=0.)\n        lstm_bw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\n                                         num_units=hidden_size,\n                                         input_size=input_size,\n                                         # dropout = 1 - keep_prob\n                                         dropout=0.)\n        param_fw = tf.get_variable(\"lstm_fw_params\",\n                                   initializer=tf.random_uniform([lstm_fw.params_size()],\n                                                                 -0.1, 0.1),\n                                   validate_shape=False)\n        param_bw = tf.get_variable(\"lstm_bw_params\",\n                                   initializer=tf.random_uniform([lstm_bw.params_size()],\n                                                                 -0.1, 0.1),\n                                   validate_shape=False)\n        c_fw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        c_bw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        h_fw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        h_bw = tf.zeros([layer_num, batch_size, hidden_size],\n                        tf.float32)\n        with tf.variable_scope(\"fw\"):\n            out_fw, h_fw, c_fw = lstm_fw(inputs, h_fw, c_fw, param_fw)\n        with tf.variable_scope(\"bw\"):\n            inputs_bw = tf.reverse_sequence(\n                inputs, seq_lengths=length, seq_dim=0, batch_dim=1)\n            out_bw, h_bw, c_bw = lstm_bw(inputs_bw, h_bw, c_bw, param_bw)\n            out_bw = tf.reverse_sequence(\n                out_bw, seq_lengths=length, seq_dim=0, batch_dim=1)\n        output = tf.concat([out_fw, out_bw], axis=2)\n        output = tf.transpose(output, [1, 0, 2])\n        output_h = tf.concat([h_fw, h_bw], axis=0)\n        output_c = tf.concat([c_fw, c_bw], axis=0)\n        return output, (tc.rnn.LSTMStateTuple(h=output_h, c=output_c),)", "body": "Unfortunate I am not able to provide a snippet that reproduces the problem. I hope the call stacks are helpful. \r\n\r\nStrangely, my friend ran exactly the same code on his machine and had no problem with it. Am I running into the hardware issue here https://github.com/tensorflow/tensorflow/issues/1947?\r\n\r\nThe LSTM is written as:\r\n```\r\n        import tensorflow.contrib as tc\r\n        # bi-lstm\r\n        lstm_fw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\r\n                                         num_units=hidden_size,\r\n                                         input_size=input_size,\r\n                                         # dropout = 1 - keep_prob\r\n                                         dropout=0.)\r\n        lstm_bw = tc.cudnn_rnn.CudnnLSTM(num_layers=layer_num,\r\n                                         num_units=hidden_size,\r\n                                         input_size=input_size,\r\n                                         # dropout = 1 - keep_prob\r\n                                         dropout=0.)\r\n        param_fw = tf.get_variable(\"lstm_fw_params\",\r\n                                   initializer=tf.random_uniform([lstm_fw.params_size()],\r\n                                                                 -0.1, 0.1),\r\n                                   validate_shape=False)\r\n        param_bw = tf.get_variable(\"lstm_bw_params\",\r\n                                   initializer=tf.random_uniform([lstm_bw.params_size()],\r\n                                                                 -0.1, 0.1),\r\n                                   validate_shape=False)\r\n        c_fw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        c_bw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        h_fw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        h_bw = tf.zeros([layer_num, batch_size, hidden_size],\r\n                        tf.float32)\r\n        with tf.variable_scope(\"fw\"):\r\n            out_fw, h_fw, c_fw = lstm_fw(inputs, h_fw, c_fw, param_fw)\r\n        with tf.variable_scope(\"bw\"):\r\n            inputs_bw = tf.reverse_sequence(\r\n                inputs, seq_lengths=length, seq_dim=0, batch_dim=1)\r\n            out_bw, h_bw, c_bw = lstm_bw(inputs_bw, h_bw, c_bw, param_bw)\r\n            out_bw = tf.reverse_sequence(\r\n                out_bw, seq_lengths=length, seq_dim=0, batch_dim=1)\r\n        output = tf.concat([out_fw, out_bw], axis=2)\r\n        output = tf.transpose(output, [1, 0, 2])\r\n        output_h = tf.concat([h_fw, h_bw], axis=0)\r\n        output_c = tf.concat([c_fw, c_bw], axis=0)\r\n        return output, (tc.rnn.LSTMStateTuple(h=output_h, c=output_c),)\r\n```"}