{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14448", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14448/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14448/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14448/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14448", "id": 272937718, "node_id": "MDU6SXNzdWUyNzI5Mzc3MTg=", "number": 14448, "title": "[Feature request] Improve syntax for accessing Python objects in dataset pipelines", "user": {"login": "boeddeker", "id": 13744128, "node_id": "MDQ6VXNlcjEzNzQ0MTI4", "avatar_url": "https://avatars3.githubusercontent.com/u/13744128?v=4", "gravatar_id": "", "url": "https://api.github.com/users/boeddeker", "html_url": "https://github.com/boeddeker", "followers_url": "https://api.github.com/users/boeddeker/followers", "following_url": "https://api.github.com/users/boeddeker/following{/other_user}", "gists_url": "https://api.github.com/users/boeddeker/gists{/gist_id}", "starred_url": "https://api.github.com/users/boeddeker/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/boeddeker/subscriptions", "organizations_url": "https://api.github.com/users/boeddeker/orgs", "repos_url": "https://api.github.com/users/boeddeker/repos", "events_url": "https://api.github.com/users/boeddeker/events{/privacy}", "received_events_url": "https://api.github.com/users/boeddeker/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-11-10T13:48:06Z", "updated_at": "2017-12-05T09:05:41Z", "closed_at": null, "author_association": "CONTRIBUTOR", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>:</li>\n<li><strong>GPU model and memory</strong>:</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Hey, I have a python database class</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">MyDatabase</span>:\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__len__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>): <span class=\"pl-k\">return</span> <span class=\"pl-c1\">...</span>\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__getitem__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">item</span>): <span class=\"pl-k\">return</span> <span class=\"pl-c1\">...</span>  <span class=\"pl-c\"><span class=\"pl-c\">#</span> slow code without gitlock, i.e. io and numpy</span></pre></div>\n<p>that supports indexing and returns a dict. The loading of the data is longer than a NN iteration, therefore it would be nice when tensorflow loads the data in parallel. When I use tf.data.Dataset.from_generator I have to convert my Database to a generator</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">generator</span>():\n    db <span class=\"pl-k\">=</span> MyDatabase()\n    <span class=\"pl-k\">yield from</span> [db[i] <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(db))]</pre></div>\n<p><code>from_generator</code> has no argument <code>num_parallel_calls</code>, therefore the loading is serial.<br>\nI am not sure how difficult it is to add such an argument.<br>\nBut in the case that <code>MyDatabase</code> has above structure it is possible to combine <code>tf.data.Dataset.range</code> with <code>tf.data.Dataset.map</code> and <code>tf.py_func</code> to get a parallel load.<br>\nIn my mind, it could be easier to implement a parallel load for an indexable object.</p>\n<p>Further point the interface of <code>tf.py_func</code> has more constraints than <code>tf.data.Dataset.from_generator</code> (dict's are not allowed and it hat no output_shape argument).</p>\n<p>Since I am new to tensorflow and have problems to understand the <code>tf.data.Dataset</code> code, here my feature request for a <code>num_parallel_calls</code> argument in <code>tf.data.Dataset.from_generator</code> or a new function <code>tf.data.Dataset.from_generator</code> with a <code>num_parallel_calls</code> argument.</p>\n<h3>Source code / logs</h3>\n<p>Here a toy example, that demonstrate the non-parallel from_generator</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n\nstart <span class=\"pl-k\">=</span> time.perf_counter()\n    \n<span class=\"pl-k\">def</span> <span class=\"pl-en\">body</span>(<span class=\"pl-smi\">i</span>):\n    <span class=\"pl-k\">global</span> start\n    <span class=\"pl-k\">if</span> i <span class=\"pl-k\">==</span> <span class=\"pl-c1\">0</span>:\n        start <span class=\"pl-k\">=</span> time.perf_counter()\n    time.sleep(<span class=\"pl-c1\">0.2</span>)\n    <span class=\"pl-k\">return</span> np.array([<span class=\"pl-c1\">float</span>(i), time.perf_counter() <span class=\"pl-k\">-</span> start])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">gen</span>():\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">5</span>):\n        <span class=\"pl-k\">yield</span> body(i)\n        \nds <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(gen, tf.float64)\nds <span class=\"pl-k\">=</span> ds.prefetch(<span class=\"pl-c1\">5</span>)\niterator <span class=\"pl-k\">=</span> ds.make_one_shot_iterator()\n\nentry <span class=\"pl-k\">=</span> iterator.get_next()\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    \n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            <span class=\"pl-c1\">print</span>(sess.run(entry))\n    <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n        <span class=\"pl-k\">pass</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Serial execution:  [index, time from start of first load to return of current load]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 0.          0.20034038]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 1.          0.40189139]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 2.          0.60322792]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 3.          0.80472201]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 4.          1.00612245]</span></pre></div>\n<p>and here the parallel version (Less nice code and does not generalise so good as from_generator, e.g. no return dict support)</p>\n<div class=\"highlight highlight-source-python\"><pre>ds <span class=\"pl-k\">=</span> tf.data.Dataset.range(<span class=\"pl-c1\">5</span>)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">map_func</span>(<span class=\"pl-smi\">i</span>):\n    <span class=\"pl-k\">return</span> tf.py_func(body, [i], tf.float64, <span class=\"pl-v\">stateful</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\nds <span class=\"pl-k\">=</span> ds.map(map_func, <span class=\"pl-v\">num_parallel_calls</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)\nds <span class=\"pl-k\">=</span> ds.prefetch(<span class=\"pl-c1\">1</span>)\niterator <span class=\"pl-k\">=</span> ds.make_one_shot_iterator()\n\nentry <span class=\"pl-k\">=</span> iterator.get_next()\n\n<span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n    \n    <span class=\"pl-k\">try</span>:\n        <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n            <span class=\"pl-c1\">print</span>(sess.run(entry))\n    <span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n        <span class=\"pl-k\">pass</span>\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Parallel execution:  [index, time from start of first load to return of current load]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 0.          0.20026697]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 1.          0.20084557]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 2.          0.20095535]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 3.          0.20048737]</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [ 4.          0.40154806]</span></pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.4.0\nPython version: 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nGPU model and memory:\nExact command to reproduce:\n\nDescribe the problem\nHey, I have a python database class\nclass MyDatabase:\n    def __len__(self): return ...\n    def __getitem__(self, item): return ...  # slow code without gitlock, i.e. io and numpy\nthat supports indexing and returns a dict. The loading of the data is longer than a NN iteration, therefore it would be nice when tensorflow loads the data in parallel. When I use tf.data.Dataset.from_generator I have to convert my Database to a generator\ndef generator():\n    db = MyDatabase()\n    yield from [db[i] for i in range(len(db))]\nfrom_generator has no argument num_parallel_calls, therefore the loading is serial.\nI am not sure how difficult it is to add such an argument.\nBut in the case that MyDatabase has above structure it is possible to combine tf.data.Dataset.range with tf.data.Dataset.map and tf.py_func to get a parallel load.\nIn my mind, it could be easier to implement a parallel load for an indexable object.\nFurther point the interface of tf.py_func has more constraints than tf.data.Dataset.from_generator (dict's are not allowed and it hat no output_shape argument).\nSince I am new to tensorflow and have problems to understand the tf.data.Dataset code, here my feature request for a num_parallel_calls argument in tf.data.Dataset.from_generator or a new function tf.data.Dataset.from_generator with a num_parallel_calls argument.\nSource code / logs\nHere a toy example, that demonstrate the non-parallel from_generator\nimport time\n\nstart = time.perf_counter()\n    \ndef body(i):\n    global start\n    if i == 0:\n        start = time.perf_counter()\n    time.sleep(0.2)\n    return np.array([float(i), time.perf_counter() - start])\n\ndef gen():\n    for i in range(5):\n        yield body(i)\n        \nds = tf.data.Dataset.from_generator(gen, tf.float64)\nds = ds.prefetch(5)\niterator = ds.make_one_shot_iterator()\n\nentry = iterator.get_next()\n\nwith tf.Session() as sess:\n    \n    try:\n        while True:\n            print(sess.run(entry))\n    except tf.errors.OutOfRangeError:\n        pass\n# Serial execution:  [index, time from start of first load to return of current load]\n# [ 0.          0.20034038]\n# [ 1.          0.40189139]\n# [ 2.          0.60322792]\n# [ 3.          0.80472201]\n# [ 4.          1.00612245]\nand here the parallel version (Less nice code and does not generalise so good as from_generator, e.g. no return dict support)\nds = tf.data.Dataset.range(5)\n\ndef map_func(i):\n    return tf.py_func(body, [i], tf.float64, stateful=False)\n\nds = ds.map(map_func, num_parallel_calls=4)\nds = ds.prefetch(1)\niterator = ds.make_one_shot_iterator()\n\nentry = iterator.get_next()\n\nwith tf.Session() as sess:\n    \n    try:\n        while True:\n            print(sess.run(entry))\n    except tf.errors.OutOfRangeError:\n        pass\n\n# Parallel execution:  [index, time from start of first load to return of current load]\n# [ 0.          0.20026697]\n# [ 1.          0.20084557]\n# [ 2.          0.20095535]\n# [ 3.          0.20048737]\n# [ 4.          0.40154806]", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary \r\n- **TensorFlow version (use command below)**: 1.4.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**:\r\n- **GPU model and memory**:\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\n\r\nHey, I have a python database class\r\n```python\r\nclass MyDatabase:\r\n    def __len__(self): return ...\r\n    def __getitem__(self, item): return ...  # slow code without gitlock, i.e. io and numpy\r\n```\r\nthat supports indexing and returns a dict. The loading of the data is longer than a NN iteration, therefore it would be nice when tensorflow loads the data in parallel. When I use tf.data.Dataset.from_generator I have to convert my Database to a generator\r\n```python\r\ndef generator():\r\n    db = MyDatabase()\r\n    yield from [db[i] for i in range(len(db))]\r\n```\r\n`from_generator` has no argument `num_parallel_calls`, therefore the loading is serial.\r\nI am not sure how difficult it is to add such an argument. \r\nBut in the case that `MyDatabase` has above structure it is possible to combine `tf.data.Dataset.range` with `tf.data.Dataset.map` and `tf.py_func` to get a parallel load. \r\nIn my mind, it could be easier to implement a parallel load for an indexable object.\r\n\r\nFurther point the interface of `tf.py_func` has more constraints than `tf.data.Dataset.from_generator` (dict's are not allowed and it hat no output_shape argument).\r\n\r\nSince I am new to tensorflow and have problems to understand the `tf.data.Dataset` code, here my feature request for a `num_parallel_calls` argument in `tf.data.Dataset.from_generator` or a new function `tf.data.Dataset.from_generator` with a `num_parallel_calls` argument.\r\n\r\n### Source code / logs\r\n\r\nHere a toy example, that demonstrate the non-parallel from_generator\r\n\r\n```python\r\nimport time\r\n\r\nstart = time.perf_counter()\r\n    \r\ndef body(i):\r\n    global start\r\n    if i == 0:\r\n        start = time.perf_counter()\r\n    time.sleep(0.2)\r\n    return np.array([float(i), time.perf_counter() - start])\r\n\r\ndef gen():\r\n    for i in range(5):\r\n        yield body(i)\r\n        \r\nds = tf.data.Dataset.from_generator(gen, tf.float64)\r\nds = ds.prefetch(5)\r\niterator = ds.make_one_shot_iterator()\r\n\r\nentry = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    \r\n    try:\r\n        while True:\r\n            print(sess.run(entry))\r\n    except tf.errors.OutOfRangeError:\r\n        pass\r\n# Serial execution:  [index, time from start of first load to return of current load]\r\n# [ 0.          0.20034038]\r\n# [ 1.          0.40189139]\r\n# [ 2.          0.60322792]\r\n# [ 3.          0.80472201]\r\n# [ 4.          1.00612245]\r\n```\r\nand here the parallel version (Less nice code and does not generalise so good as from_generator, e.g. no return dict support)\r\n```python\r\nds = tf.data.Dataset.range(5)\r\n\r\ndef map_func(i):\r\n    return tf.py_func(body, [i], tf.float64, stateful=False)\r\n\r\nds = ds.map(map_func, num_parallel_calls=4)\r\nds = ds.prefetch(1)\r\niterator = ds.make_one_shot_iterator()\r\n\r\nentry = iterator.get_next()\r\n\r\nwith tf.Session() as sess:\r\n    \r\n    try:\r\n        while True:\r\n            print(sess.run(entry))\r\n    except tf.errors.OutOfRangeError:\r\n        pass\r\n\r\n# Parallel execution:  [index, time from start of first load to return of current load]\r\n# [ 0.          0.20026697]\r\n# [ 1.          0.20084557]\r\n# [ 2.          0.20095535]\r\n# [ 3.          0.20048737]\r\n# [ 4.          0.40154806]\r\n```"}