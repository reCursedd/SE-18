{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/246698021", "html_url": "https://github.com/tensorflow/tensorflow/issues/4074#issuecomment-246698021", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4074", "id": 246698021, "node_id": "MDEyOklzc3VlQ29tbWVudDI0NjY5ODAyMQ==", "user": {"login": "brianwa84", "id": 22173987, "node_id": "MDQ6VXNlcjIyMTczOTg3", "avatar_url": "https://avatars0.githubusercontent.com/u/22173987?v=4", "gravatar_id": "", "url": "https://api.github.com/users/brianwa84", "html_url": "https://github.com/brianwa84", "followers_url": "https://api.github.com/users/brianwa84/followers", "following_url": "https://api.github.com/users/brianwa84/following{/other_user}", "gists_url": "https://api.github.com/users/brianwa84/gists{/gist_id}", "starred_url": "https://api.github.com/users/brianwa84/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/brianwa84/subscriptions", "organizations_url": "https://api.github.com/users/brianwa84/orgs", "repos_url": "https://api.github.com/users/brianwa84/repos", "events_url": "https://api.github.com/users/brianwa84/events{/privacy}", "received_events_url": "https://api.github.com/users/brianwa84/received_events", "type": "User", "site_admin": false}, "created_at": "2016-09-13T14:23:56Z", "updated_at": "2016-09-13T14:25:20Z", "author_association": "CONTRIBUTOR", "body_html": "<p>A few explanations here, and I think everything is working as intended:</p>\n<ol>\n<li>Multiple uses of session.run - this means the Sample: [0] that you print is not necessarily the one that you are will use for training. That in itself doesn't explain why multiple columns would change for a given iteration.</li>\n<li>You are using the adam optimizer, which uses momentum (i.e. the update of this iteration is some fusion of previous updates plus the current gradient step). This explains why a single column changes on the first iteration, two change on the next, etc.</li>\n</ol>\n<p>tf.Print is your friend, as is tf.gradients. Modifying your code to work with 0.10, I can see that the gradients are single-column only (good). This is what gave me the hint about momentum.</p>\n<pre><code>import tensorflow as tf\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n    sample_op = tf.stop_gradient(tf.contrib.distributions.Categorical(logits).sample_n(1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n          loss, ix, Wg, grads, _ = sess.run([loss_op, index, W_gen, tf.gradients(loss_op, [W_gen]), train_op])\n          print('Loss at iteration %d: %f' % (i, loss))\n          print('Sample: [%d]' % ix)\n          print(\"grads: %s\" % grads)\n          print Wg\n        else:\n          sess.run(train_op)\n    print sess.run(output)\n</code></pre>\n<p>Switch to GradientDescentOptimizer if you want to see only a single column updated.</p>", "body_text": "A few explanations here, and I think everything is working as intended:\n\nMultiple uses of session.run - this means the Sample: [0] that you print is not necessarily the one that you are will use for training. That in itself doesn't explain why multiple columns would change for a given iteration.\nYou are using the adam optimizer, which uses momentum (i.e. the update of this iteration is some fusion of previous updates plus the current gradient step). This explains why a single column changes on the first iteration, two change on the next, etc.\n\ntf.Print is your friend, as is tf.gradients. Modifying your code to work with 0.10, I can see that the gradients are single-column only (good). This is what gave me the hint about momentum.\nimport tensorflow as tf\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n    sample_op = tf.stop_gradient(tf.contrib.distributions.Categorical(logits).sample_n(1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n          loss, ix, Wg, grads, _ = sess.run([loss_op, index, W_gen, tf.gradients(loss_op, [W_gen]), train_op])\n          print('Loss at iteration %d: %f' % (i, loss))\n          print('Sample: [%d]' % ix)\n          print(\"grads: %s\" % grads)\n          print Wg\n        else:\n          sess.run(train_op)\n    print sess.run(output)\n\nSwitch to GradientDescentOptimizer if you want to see only a single column updated.", "body": "A few explanations here, and I think everything is working as intended:\n1. Multiple uses of session.run - this means the Sample: [0] that you print is not necessarily the one that you are will use for training. That in itself doesn't explain why multiple columns would change for a given iteration.\n2. You are using the adam optimizer, which uses momentum (i.e. the update of this iteration is some fusion of previous updates plus the current gradient step). This explains why a single column changes on the first iteration, two change on the next, etc.\n\ntf.Print is your friend, as is tf.gradients. Modifying your code to work with 0.10, I can see that the gradients are single-column only (good). This is what gave me the hint about momentum.\n\n```\nimport tensorflow as tf\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n    sample_op = tf.stop_gradient(tf.contrib.distributions.Categorical(logits).sample_n(1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.GradientDescentOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n          loss, ix, Wg, grads, _ = sess.run([loss_op, index, W_gen, tf.gradients(loss_op, [W_gen]), train_op])\n          print('Loss at iteration %d: %f' % (i, loss))\n          print('Sample: [%d]' % ix)\n          print(\"grads: %s\" % grads)\n          print Wg\n        else:\n          sess.run(train_op)\n    print sess.run(output)\n```\n\nSwitch to GradientDescentOptimizer if you want to see only a single column updated.\n"}