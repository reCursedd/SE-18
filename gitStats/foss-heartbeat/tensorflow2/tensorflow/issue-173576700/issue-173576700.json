{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4074", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4074/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4074/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4074/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4074", "id": 173576700, "node_id": "MDU6SXNzdWUxNzM1NzY3MDA=", "number": 4074, "title": "Gradients Not Being Computed Correctly While Using tf.contrib.distributions.Categorical", "user": {"login": "liamb315", "id": 1130820, "node_id": "MDQ6VXNlcjExMzA4MjA=", "avatar_url": "https://avatars0.githubusercontent.com/u/1130820?v=4", "gravatar_id": "", "url": "https://api.github.com/users/liamb315", "html_url": "https://github.com/liamb315", "followers_url": "https://api.github.com/users/liamb315/followers", "following_url": "https://api.github.com/users/liamb315/following{/other_user}", "gists_url": "https://api.github.com/users/liamb315/gists{/gist_id}", "starred_url": "https://api.github.com/users/liamb315/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/liamb315/subscriptions", "organizations_url": "https://api.github.com/users/liamb315/orgs", "repos_url": "https://api.github.com/users/liamb315/repos", "events_url": "https://api.github.com/users/liamb315/events{/privacy}", "received_events_url": "https://api.github.com/users/liamb315/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-08-27T04:13:47Z", "updated_at": "2018-01-23T03:25:20Z", "closed_at": "2017-06-16T18:16:31Z", "author_association": "NONE", "body_html": "<p>Hello, there appears to be an issue with how TensorFlow gradients are being computed while using Categorical in the graph.  In particular, let's say we were computing the gradient through a single logit of a vector, we would expect only the corresponding column of the weight matrix affecting that logit value to update with gradients. This is indeed the case when I keep this index fixed, say <code>tf.constant(0, dtype=tf.int32)</code>, however, this is not the case while using Categorical.  The code below should clarify further.</p>\n<p>Thanks!<br>\nLiam</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I have not found this particular bug in a quick search.</p>\n<h3>Environment info</h3>\n<p>Operating System:  Ubuntu 14.04.4 LTS</p>\n<p>Installed version of CUDA and cuDNN:<br>\n-rw-r--r-- 1 root root 189170 Mar 17 17:29 /usr/local/cuda/lib/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root     16 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.5<br>\nlrwxrwxrwx 1 root root     19 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n-rwxr-xr-x 1 root root 311596 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5.18<br>\n-rw-r--r-- 1 root root 558020 Mar 17 17:29 /usr/local/cuda/lib/libcudart_static.a</p>\n<p>TensorFlow from source.</p>\n<ol>\n<li>Commit hash:  1b50845ff01200b3f6fc78a2780df49baea674ff</li>\n<li>bazel version:</li>\n</ol>\n<p>Build label: 0.2.2b<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Mon Apr 25 08:08:53 2016 (1461571733)<br>\nBuild timestamp: 1461571733<br>\nBuild timestamp as int: 1461571733</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>import tensorflow as tf\nfrom tensorflow.contrib.distributions import Categorical\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n\n    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n            print('Sample: [%d]' % sess.run(index))\n            print sess.run(W_gen)\n        sess.run(train_op)\n    print sess.run(output)\n</code></pre>\n<h3>Logs or other output that would be helpful</h3>\n<p>Here I show you the Sample generated by Categorical and then the corresponding change to the weight matrix.  Notice, that the first training step operates as expected, only the '1th' column is affected.</p>\n<p>However, the second training step both column 1 and 2 change.</p>\n<pre><code>Sample: [1]\n[[ 0.81418228  0.8655349   0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.69035411  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.52124786  0.23499095  0.59907818  0.44014001]]\n\nSample: [1]\n[[ 0.81418228  0.76553494  0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.59035414  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.4212479   0.23499095  0.59907818  0.44014001]]\n\nSample: [0]\n[[ 0.81418228  0.69852936  0.23505335  0.55864608  0.35103011]\n [ 0.26089203  0.52334857  0.71461827  0.02805829  0.99758911]\n [ 0.56620026  0.35424232  0.30940431  0.59907818  0.44014001]]\n</code></pre>", "body_text": "Hello, there appears to be an issue with how TensorFlow gradients are being computed while using Categorical in the graph.  In particular, let's say we were computing the gradient through a single logit of a vector, we would expect only the corresponding column of the weight matrix affecting that logit value to update with gradients. This is indeed the case when I keep this index fixed, say tf.constant(0, dtype=tf.int32), however, this is not the case while using Categorical.  The code below should clarify further.\nThanks!\nLiam\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI have not found this particular bug in a quick search.\nEnvironment info\nOperating System:  Ubuntu 14.04.4 LTS\nInstalled version of CUDA and cuDNN:\n-rw-r--r-- 1 root root 189170 Mar 17 17:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Mar 17 17:29 /usr/local/cuda/lib/libcudart_static.a\nTensorFlow from source.\n\nCommit hash:  1b50845ff01200b3f6fc78a2780df49baea674ff\nbazel version:\n\nBuild label: 0.2.2b\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Apr 25 08:08:53 2016 (1461571733)\nBuild timestamp: 1461571733\nBuild timestamp as int: 1461571733\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions import Categorical\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n\n    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n            print('Sample: [%d]' % sess.run(index))\n            print sess.run(W_gen)\n        sess.run(train_op)\n    print sess.run(output)\n\nLogs or other output that would be helpful\nHere I show you the Sample generated by Categorical and then the corresponding change to the weight matrix.  Notice, that the first training step operates as expected, only the '1th' column is affected.\nHowever, the second training step both column 1 and 2 change.\nSample: [1]\n[[ 0.81418228  0.8655349   0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.69035411  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.52124786  0.23499095  0.59907818  0.44014001]]\n\nSample: [1]\n[[ 0.81418228  0.76553494  0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.59035414  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.4212479   0.23499095  0.59907818  0.44014001]]\n\nSample: [0]\n[[ 0.81418228  0.69852936  0.23505335  0.55864608  0.35103011]\n [ 0.26089203  0.52334857  0.71461827  0.02805829  0.99758911]\n [ 0.56620026  0.35424232  0.30940431  0.59907818  0.44014001]]", "body": "Hello, there appears to be an issue with how TensorFlow gradients are being computed while using Categorical in the graph.  In particular, let's say we were computing the gradient through a single logit of a vector, we would expect only the corresponding column of the weight matrix affecting that logit value to update with gradients. This is indeed the case when I keep this index fixed, say `tf.constant(0, dtype=tf.int32)`, however, this is not the case while using Categorical.  The code below should clarify further.\n\nThanks!\nLiam\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI have not found this particular bug in a quick search.\n### Environment info\n\nOperating System:  Ubuntu 14.04.4 LTS\n\nInstalled version of CUDA and cuDNN:  \n-rw-r--r-- 1 root root 189170 Mar 17 17:29 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Mar 17 17:29 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Mar 17 17:29 /usr/local/cuda/lib/libcudart_static.a\n\nTensorFlow from source.\n1.  Commit hash:  1b50845ff01200b3f6fc78a2780df49baea674ff\n2.  bazel version:\n\nBuild label: 0.2.2b\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Mon Apr 25 08:08:53 2016 (1461571733)\nBuild timestamp: 1461571733\nBuild timestamp as int: 1461571733\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\n\n```\nimport tensorflow as tf\nfrom tensorflow.contrib.distributions import Categorical\ntf.reset_default_graph()\n\ninput_dim = 3\nhidden_dim = 5\noutput_dim = 3\nlr = 1e-1\nnum_iterations = 50\nprint_every = 1\n\nx = tf.fill([1, input_dim], 1.)\ny = tf.fill([1, output_dim], 1.)\n\nwith tf.name_scope('Model'):\n    W_gen = tf.Variable(tf.random_uniform([input_dim, hidden_dim]), name = 'W_gen')\n    logits = tf.matmul(x, W_gen)\n\n    sample_op = tf.stop_gradient(Categorical(logits).sample(n=1))\n    index = tf.squeeze(sample_op)\n    one_hot = tf.one_hot(index, hidden_dim, dtype = tf.float32)\n    logits = logits * one_hot\n\n    W_dis = tf.Variable(tf.random_uniform([hidden_dim, output_dim]), name = 'W_dis')\n    output = tf.matmul(logits, W_dis)\n\n\nwith tf.name_scope('Loss'):\n    loss_op = tf.reduce_mean(tf.squared_difference(output, y))\n\nwith tf.name_scope('Train'):\n    train_vars = [W_gen]\n    train_op = tf.train.AdamOptimizer(lr).minimize(loss_op, var_list = train_vars)\n\n\nwith tf.Session() as sess:\n    init_op = tf.initialize_all_variables()\n    sess.run(init_op)\n\n    for i in xrange(num_iterations):\n        if i % print_every == 0:\n            print('Loss at iteration %d: %f' % (i, sess.run(loss_op)))\n            print('Sample: [%d]' % sess.run(index))\n            print sess.run(W_gen)\n        sess.run(train_op)\n    print sess.run(output)\n```\n### Logs or other output that would be helpful\n\nHere I show you the Sample generated by Categorical and then the corresponding change to the weight matrix.  Notice, that the first training step operates as expected, only the '1th' column is affected. \n\nHowever, the second training step both column 1 and 2 change.  \n\n```\nSample: [1]\n[[ 0.81418228  0.8655349   0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.69035411  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.52124786  0.23499095  0.59907818  0.44014001]]\n\nSample: [1]\n[[ 0.81418228  0.76553494  0.16064     0.55864608  0.35103011]\n [ 0.26089203  0.59035414  0.64020491  0.02805829  0.99758911]\n [ 0.56620026  0.4212479   0.23499095  0.59907818  0.44014001]]\n\nSample: [0]\n[[ 0.81418228  0.69852936  0.23505335  0.55864608  0.35103011]\n [ 0.26089203  0.52334857  0.71461827  0.02805829  0.99758911]\n [ 0.56620026  0.35424232  0.30940431  0.59907818  0.44014001]]\n```\n"}