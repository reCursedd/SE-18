{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/226033903", "html_url": "https://github.com/tensorflow/tensorflow/issues/2384#issuecomment-226033903", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2384", "id": 226033903, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNjAzMzkwMw==", "user": {"login": "mmuneebs", "id": 16750872, "node_id": "MDQ6VXNlcjE2NzUwODcy", "avatar_url": "https://avatars1.githubusercontent.com/u/16750872?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mmuneebs", "html_url": "https://github.com/mmuneebs", "followers_url": "https://api.github.com/users/mmuneebs/followers", "following_url": "https://api.github.com/users/mmuneebs/following{/other_user}", "gists_url": "https://api.github.com/users/mmuneebs/gists{/gist_id}", "starred_url": "https://api.github.com/users/mmuneebs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mmuneebs/subscriptions", "organizations_url": "https://api.github.com/users/mmuneebs/orgs", "repos_url": "https://api.github.com/users/mmuneebs/repos", "events_url": "https://api.github.com/users/mmuneebs/events{/privacy}", "received_events_url": "https://api.github.com/users/mmuneebs/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-14T22:21:15Z", "updated_at": "2016-06-14T22:27:34Z", "author_association": "NONE", "body_html": "<p>I fixed this issue with the workers by commenting the <code>with tf.device('/cpu:0'):</code> spec while calling <code>batch_inputs</code> in <a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/image_processing.py#L132\"><code>image_processing.py</code></a>. One reason this may have happened with my setup though not completely clear is that I use</p>\n<pre><code>with tf.device(tf.train.replica_device_setter(ps_tasks=1,\n                                              worker_device=\"/job:worker/task:%d\" % FLAGS.task_id,\n                                              cluster=cluster_spec)):\n</code></pre>\n<p>instead of</p>\n<pre><code># Ops are assigned to worker by default.\nwith tf.device('/job:worker/task:%d' % FLAGS.task_id):\n    # Variables and its related init/assign ops are assigned to ps.\n    with slim.scopes.arg_scope(\n            [slim.variables.variable, slim.variables.global_step],\n            device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n</code></pre>\n<p>as the outermost training scope inside which the batch processing gets called (<a href=\"https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py#L113\"><code>inception_distributed_train.py</code></a>).</p>\n<p>Not sure why exactly this became a problem for my modified setup, but now the memory increase trend has reduced at least ten-fold, and tested to run for a 100 epochs.<br>\nMaybe the original code will work fine without this CPU device specification as well.</p>", "body_text": "I fixed this issue with the workers by commenting the with tf.device('/cpu:0'): spec while calling batch_inputs in image_processing.py. One reason this may have happened with my setup though not completely clear is that I use\nwith tf.device(tf.train.replica_device_setter(ps_tasks=1,\n                                              worker_device=\"/job:worker/task:%d\" % FLAGS.task_id,\n                                              cluster=cluster_spec)):\n\ninstead of\n# Ops are assigned to worker by default.\nwith tf.device('/job:worker/task:%d' % FLAGS.task_id):\n    # Variables and its related init/assign ops are assigned to ps.\n    with slim.scopes.arg_scope(\n            [slim.variables.variable, slim.variables.global_step],\n            device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n\nas the outermost training scope inside which the batch processing gets called (inception_distributed_train.py).\nNot sure why exactly this became a problem for my modified setup, but now the memory increase trend has reduced at least ten-fold, and tested to run for a 100 epochs.\nMaybe the original code will work fine without this CPU device specification as well.", "body": "I fixed this issue with the workers by commenting the `with tf.device('/cpu:0'):` spec while calling `batch_inputs` in [`image_processing.py`](https://github.com/tensorflow/models/blob/master/inception/inception/image_processing.py#L132). One reason this may have happened with my setup though not completely clear is that I use\n\n```\nwith tf.device(tf.train.replica_device_setter(ps_tasks=1,\n                                              worker_device=\"/job:worker/task:%d\" % FLAGS.task_id,\n                                              cluster=cluster_spec)):\n```\n\ninstead of\n\n```\n# Ops are assigned to worker by default.\nwith tf.device('/job:worker/task:%d' % FLAGS.task_id):\n    # Variables and its related init/assign ops are assigned to ps.\n    with slim.scopes.arg_scope(\n            [slim.variables.variable, slim.variables.global_step],\n            device=slim.variables.VariableDeviceChooser(num_parameter_servers)):\n```\n\nas the outermost training scope inside which the batch processing gets called ([`inception_distributed_train.py`](https://github.com/tensorflow/models/blob/master/inception/inception/inception_distributed_train.py#L113)).\n\nNot sure why exactly this became a problem for my modified setup, but now the memory increase trend has reduced at least ten-fold, and tested to run for a 100 epochs.\nMaybe the original code will work fine without this CPU device specification as well.\n"}