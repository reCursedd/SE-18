{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9080", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9080/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9080/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9080/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9080", "id": 220462551, "node_id": "MDU6SXNzdWUyMjA0NjI1NTE=", "number": 9080, "title": "Support for nvidia-cuda-mps-server ", "user": {"login": "elefthei", "id": 766892, "node_id": "MDQ6VXNlcjc2Njg5Mg==", "avatar_url": "https://avatars0.githubusercontent.com/u/766892?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elefthei", "html_url": "https://github.com/elefthei", "followers_url": "https://api.github.com/users/elefthei/followers", "following_url": "https://api.github.com/users/elefthei/following{/other_user}", "gists_url": "https://api.github.com/users/elefthei/gists{/gist_id}", "starred_url": "https://api.github.com/users/elefthei/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elefthei/subscriptions", "organizations_url": "https://api.github.com/users/elefthei/orgs", "repos_url": "https://api.github.com/users/elefthei/repos", "events_url": "https://api.github.com/users/elefthei/events{/privacy}", "received_events_url": "https://api.github.com/users/elefthei/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 17, "created_at": "2017-04-09T10:32:48Z", "updated_at": "2018-11-19T18:05:03Z", "closed_at": "2018-03-08T06:35:41Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm experimenting with multiple Tensorflow GPU processes and the NVIDIA Multi-Process Server.<br>\n<a href=\"https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf\" rel=\"nofollow\">https://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf</a></p>\n<p>I have the following MNIST example as a benchmark (neural.py)</p>\n<pre><code>import tensorflow as tf\nimport os\n\nfrom tensorflow.examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets('MNIST_data_%d' % os.getpid(), one_hot=True)\n\n# construction phase\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny = tf.placeholder(tf.float32, shape=[None, 10])\n\nwith tf.name_scope('fc_1'):\n  W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\n  b1 = tf.Variable(tf.truncated_normal([200], stddev=0.1))\n  h = tf.sigmoid(tf.matmul(x, W1) + b1)\n\nwith tf.name_scope('fc_2'):\n  W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\n  b2 = tf.Variable(tf.truncated_normal([10], stddev=0.1))\n  y_predict = tf.nn.softmax(tf.matmul(h, W2) + b2)\n\nwith tf.name_scope('eval'):\n  with tf.name_scope('loss'):\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_predict), reduction_indices=[1]))\n\nlearning_rate = 0.5\n\nbackprop = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n\ncorrect = tf.equal(tf.argmax(y, 1), tf.argmax(y_predict, 1))\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n#execution\n\nsess = tf.Session()\n\nsess.run(tf.initialize_all_variables())\n\ntrain_steps = 2000\nbatch_size = 50\n\nfor i in range(train_steps):\n  batch_x, batch_y = data.train.next_batch(batch_size)\n  sess.run(backprop, feed_dict={x: batch_x, y: batch_y})\n\nprint(sess.run(accuracy, feed_dict={x: data.test.images, y: data.test.labels}))\n</code></pre>\n<p>And I'm running two processes like this:<br>\n<code>$ time python neural.py &amp; time python neural.py </code></p>\n<p>Without <code>nvidia-cuda-mps-control</code> running as a daemon, this is the output:</p>\n<pre><code>0.9483\n0.947\n\nreal    0m15.602s\nuser    0m6.172s\nsys     0m5.092s\n\nreal    0m15.861s\nuser    0m6.288s\nsys     0m1.964s\n</code></pre>\n<p>With <code>nvidia-cuda-mps-control</code> running as a daemon, I'm getting an internal error:</p>\n<pre><code>F tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\n-bash: line 76: 47018 Aborted                 (core dumped) python neural.py\n</code></pre>\n<p>I can verify from the nvidia-mps logs in /var/log/nvidia-mps that the tensorflow Cuda context successfully started an nvidia-cuda-mps-server and connected to it.</p>\n<p><strong>/var/log/nvidia-mps/control.log</strong></p>\n<blockquote>\n<p>[2017-04-09 10:05:09.539 Control 46322] Start<br>\n[2017-04-09 10:05:21.023 Control 46322] Accepting connection...<br>\n[2017-04-09 10:05:21.024 Control 46322] NEW CLIENT 46325 from user 1000: Server is not ready, push client to pending list<br>\n[2017-04-09 10:05:21.024 Control 46322] Starting new server 46348 for user 1000</p>\n</blockquote>\n<p>The MPS server should be compatible with the Cuda API which Tensorflow uses, so I'm uncertain about why I'm getting this error.</p>\n<p><strong>Tensorflow version: 1.01</strong><br>\n<strong>Ubuntu 16.04</strong><br>\n<strong>Cuda 8.0, CuDNN</strong></p>\n<pre><code>+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 8915:00:00.0     Off |                  Off |\n| N/A   51C    P8    28W / 149W |     82MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n</code></pre>", "body_text": "I'm experimenting with multiple Tensorflow GPU processes and the NVIDIA Multi-Process Server.\nhttps://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf\nI have the following MNIST example as a benchmark (neural.py)\nimport tensorflow as tf\nimport os\n\nfrom tensorflow.examples.tutorials.mnist import input_data\ndata = input_data.read_data_sets('MNIST_data_%d' % os.getpid(), one_hot=True)\n\n# construction phase\nx = tf.placeholder(tf.float32, shape=[None, 784])\ny = tf.placeholder(tf.float32, shape=[None, 10])\n\nwith tf.name_scope('fc_1'):\n  W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\n  b1 = tf.Variable(tf.truncated_normal([200], stddev=0.1))\n  h = tf.sigmoid(tf.matmul(x, W1) + b1)\n\nwith tf.name_scope('fc_2'):\n  W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\n  b2 = tf.Variable(tf.truncated_normal([10], stddev=0.1))\n  y_predict = tf.nn.softmax(tf.matmul(h, W2) + b2)\n\nwith tf.name_scope('eval'):\n  with tf.name_scope('loss'):\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_predict), reduction_indices=[1]))\n\nlearning_rate = 0.5\n\nbackprop = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n\ncorrect = tf.equal(tf.argmax(y, 1), tf.argmax(y_predict, 1))\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n\n#execution\n\nsess = tf.Session()\n\nsess.run(tf.initialize_all_variables())\n\ntrain_steps = 2000\nbatch_size = 50\n\nfor i in range(train_steps):\n  batch_x, batch_y = data.train.next_batch(batch_size)\n  sess.run(backprop, feed_dict={x: batch_x, y: batch_y})\n\nprint(sess.run(accuracy, feed_dict={x: data.test.images, y: data.test.labels}))\n\nAnd I'm running two processes like this:\n$ time python neural.py & time python neural.py \nWithout nvidia-cuda-mps-control running as a daemon, this is the output:\n0.9483\n0.947\n\nreal    0m15.602s\nuser    0m6.172s\nsys     0m5.092s\n\nreal    0m15.861s\nuser    0m6.288s\nsys     0m1.964s\n\nWith nvidia-cuda-mps-control running as a daemon, I'm getting an internal error:\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\n-bash: line 76: 47018 Aborted                 (core dumped) python neural.py\n\nI can verify from the nvidia-mps logs in /var/log/nvidia-mps that the tensorflow Cuda context successfully started an nvidia-cuda-mps-server and connected to it.\n/var/log/nvidia-mps/control.log\n\n[2017-04-09 10:05:09.539 Control 46322] Start\n[2017-04-09 10:05:21.023 Control 46322] Accepting connection...\n[2017-04-09 10:05:21.024 Control 46322] NEW CLIENT 46325 from user 1000: Server is not ready, push client to pending list\n[2017-04-09 10:05:21.024 Control 46322] Starting new server 46348 for user 1000\n\nThe MPS server should be compatible with the Cuda API which Tensorflow uses, so I'm uncertain about why I'm getting this error.\nTensorflow version: 1.01\nUbuntu 16.04\nCuda 8.0, CuDNN\n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla K80           Off  | 8915:00:00.0     Off |                  Off |\n| N/A   51C    P8    28W / 149W |     82MiB / 12205MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+", "body": "I'm experimenting with multiple Tensorflow GPU processes and the NVIDIA Multi-Process Server. \r\nhttps://docs.nvidia.com/deploy/pdf/CUDA_Multi_Process_Service_Overview.pdf\r\n\r\nI have the following MNIST example as a benchmark (neural.py)\r\n\r\n```\r\nimport tensorflow as tf\r\nimport os\r\n\r\nfrom tensorflow.examples.tutorials.mnist import input_data\r\ndata = input_data.read_data_sets('MNIST_data_%d' % os.getpid(), one_hot=True)\r\n\r\n# construction phase\r\nx = tf.placeholder(tf.float32, shape=[None, 784])\r\ny = tf.placeholder(tf.float32, shape=[None, 10])\r\n\r\nwith tf.name_scope('fc_1'):\r\n  W1 = tf.Variable(tf.truncated_normal([784, 200], stddev=0.1))\r\n  b1 = tf.Variable(tf.truncated_normal([200], stddev=0.1))\r\n  h = tf.sigmoid(tf.matmul(x, W1) + b1)\r\n\r\nwith tf.name_scope('fc_2'):\r\n  W2 = tf.Variable(tf.truncated_normal([200, 10], stddev=0.1))\r\n  b2 = tf.Variable(tf.truncated_normal([10], stddev=0.1))\r\n  y_predict = tf.nn.softmax(tf.matmul(h, W2) + b2)\r\n\r\nwith tf.name_scope('eval'):\r\n  with tf.name_scope('loss'):\r\n    cross_entropy = tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_predict), reduction_indices=[1]))\r\n\r\nlearning_rate = 0.5\r\n\r\nbackprop = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\r\n\r\ncorrect = tf.equal(tf.argmax(y, 1), tf.argmax(y_predict, 1))\r\naccuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\r\n\r\n#execution\r\n\r\nsess = tf.Session()\r\n\r\nsess.run(tf.initialize_all_variables())\r\n\r\ntrain_steps = 2000\r\nbatch_size = 50\r\n\r\nfor i in range(train_steps):\r\n  batch_x, batch_y = data.train.next_batch(batch_size)\r\n  sess.run(backprop, feed_dict={x: batch_x, y: batch_y})\r\n\r\nprint(sess.run(accuracy, feed_dict={x: data.test.images, y: data.test.labels}))\r\n```\r\n\r\nAnd I'm running two processes like this:\r\n`$ time python neural.py &\r\n     time python neural.py\r\n`\r\n\r\nWithout `nvidia-cuda-mps-control` running as a daemon, this is the output:\r\n```\r\n0.9483\r\n0.947\r\n\r\nreal    0m15.602s\r\nuser    0m6.172s\r\nsys     0m5.092s\r\n\r\nreal    0m15.861s\r\nuser    0m6.288s\r\nsys     0m1.964s\r\n```\r\n\r\nWith `nvidia-cuda-mps-control` running as a daemon, I'm getting an internal error:\r\n\r\n```\r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\r\nF tensorflow/core/common_runtime/gpu/gpu_device.cc:121] Check failed: err == cudaSuccess (71 vs. 0)\r\n-bash: line 76: 47018 Aborted                 (core dumped) python neural.py\r\n```\r\n\r\nI can verify from the nvidia-mps logs in /var/log/nvidia-mps that the tensorflow Cuda context successfully started an nvidia-cuda-mps-server and connected to it.\r\n\r\n**/var/log/nvidia-mps/control.log**\r\n> [2017-04-09 10:05:09.539 Control 46322] Start\r\n> [2017-04-09 10:05:21.023 Control 46322] Accepting connection...\r\n> [2017-04-09 10:05:21.024 Control 46322] NEW CLIENT 46325 from user 1000: Server is not ready, push client to pending list\r\n> [2017-04-09 10:05:21.024 Control 46322] Starting new server 46348 for user 1000\r\n\r\nThe MPS server should be compatible with the Cuda API which Tensorflow uses, so I'm uncertain about why I'm getting this error.\r\n\r\n**Tensorflow version: 1.01**\r\n**Ubuntu 16.04**\r\n**Cuda 8.0, CuDNN**\r\n\r\n```\r\n+-----------------------------------------------------------------------------+\r\n| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |\r\n|-------------------------------+----------------------+----------------------+\r\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n|===============================+======================+======================|\r\n|   0  Tesla K80           Off  | 8915:00:00.0     Off |                  Off |\r\n| N/A   51C    P8    28W / 149W |     82MiB / 12205MiB |      0%      Default |\r\n+-------------------------------+----------------------+----------------------+\r\n```"}