{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/319785079", "html_url": "https://github.com/tensorflow/tensorflow/issues/9080#issuecomment-319785079", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9080", "id": 319785079, "node_id": "MDEyOklzc3VlQ29tbWVudDMxOTc4NTA3OQ==", "user": {"login": "nsmetanin", "id": 764902, "node_id": "MDQ6VXNlcjc2NDkwMg==", "avatar_url": "https://avatars1.githubusercontent.com/u/764902?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nsmetanin", "html_url": "https://github.com/nsmetanin", "followers_url": "https://api.github.com/users/nsmetanin/followers", "following_url": "https://api.github.com/users/nsmetanin/following{/other_user}", "gists_url": "https://api.github.com/users/nsmetanin/gists{/gist_id}", "starred_url": "https://api.github.com/users/nsmetanin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nsmetanin/subscriptions", "organizations_url": "https://api.github.com/users/nsmetanin/orgs", "repos_url": "https://api.github.com/users/nsmetanin/repos", "events_url": "https://api.github.com/users/nsmetanin/events{/privacy}", "received_events_url": "https://api.github.com/users/nsmetanin/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-02T20:14:44Z", "updated_at": "2017-08-02T20:15:00Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=766892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/elefthei\">@elefthei</a>  I've tested it in multiprocess environment \u2014 3-4 instances of the same process per GPU, each with sufficient amount of GPU memory, all under simultaneous high load (each process handle requests by several GPU inference calls alternating with calculations on CPU), it's actually a LSTM-based dialog model in a nutshell. All the difference was in running it thought MPS or not and I've got ~10% performance decrease on a MPS-enabled configuration.</p>\n<p>But I didn't benchmark any training tasks (I don't actually think you could benefit from MPS at training time, but who knows) and any other models (like CNN-based, inception v3 i.e).</p>\n<p>So my advice is to try MPS by your own (building TensorFlow from sources isn't THAT scary as it looks like :) at your own tasks, because I think results may be very different.</p>", "body_text": "@elefthei  I've tested it in multiprocess environment \u2014 3-4 instances of the same process per GPU, each with sufficient amount of GPU memory, all under simultaneous high load (each process handle requests by several GPU inference calls alternating with calculations on CPU), it's actually a LSTM-based dialog model in a nutshell. All the difference was in running it thought MPS or not and I've got ~10% performance decrease on a MPS-enabled configuration.\nBut I didn't benchmark any training tasks (I don't actually think you could benefit from MPS at training time, but who knows) and any other models (like CNN-based, inception v3 i.e).\nSo my advice is to try MPS by your own (building TensorFlow from sources isn't THAT scary as it looks like :) at your own tasks, because I think results may be very different.", "body": "@elefthei  I've tested it in multiprocess environment \u2014 3-4 instances of the same process per GPU, each with sufficient amount of GPU memory, all under simultaneous high load (each process handle requests by several GPU inference calls alternating with calculations on CPU), it's actually a LSTM-based dialog model in a nutshell. All the difference was in running it thought MPS or not and I've got ~10% performance decrease on a MPS-enabled configuration.\r\n\r\nBut I didn't benchmark any training tasks (I don't actually think you could benefit from MPS at training time, but who knows) and any other models (like CNN-based, inception v3 i.e). \r\n\r\nSo my advice is to try MPS by your own (building TensorFlow from sources isn't THAT scary as it looks like :) at your own tasks, because I think results may be very different."}