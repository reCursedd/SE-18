{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/348018901", "html_url": "https://github.com/tensorflow/tensorflow/issues/13982#issuecomment-348018901", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13982", "id": 348018901, "node_id": "MDEyOklzc3VlQ29tbWVudDM0ODAxODkwMQ==", "user": {"login": "theflofly", "id": 3902382, "node_id": "MDQ6VXNlcjM5MDIzODI=", "avatar_url": "https://avatars1.githubusercontent.com/u/3902382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/theflofly", "html_url": "https://github.com/theflofly", "followers_url": "https://api.github.com/users/theflofly/followers", "following_url": "https://api.github.com/users/theflofly/following{/other_user}", "gists_url": "https://api.github.com/users/theflofly/gists{/gist_id}", "starred_url": "https://api.github.com/users/theflofly/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/theflofly/subscriptions", "organizations_url": "https://api.github.com/users/theflofly/orgs", "repos_url": "https://api.github.com/users/theflofly/repos", "events_url": "https://api.github.com/users/theflofly/events{/privacy}", "received_events_url": "https://api.github.com/users/theflofly/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-29T22:31:30Z", "updated_at": "2017-11-29T22:32:48Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1340575\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/gdeer81\">@gdeer81</a> Sorry for the delay.</p>\n<p>As a NoGradient cut the flow of the gradient I don't see what we can test. I mean if you do something like that:</p>\n<pre><code>auto y = Square(Floor(Matmul(x1, X2)))\n</code></pre>\n<p>And you try to differentiate y w.r.t x1 or x2, it will just do nothing (no error, no result) whereas in Python there is:  <code>No gradients provided for any variable, check your graph for ops that do not support gradients, between variables...</code>. I think that the warning should be also added, else it will be very difficult to debug.</p>\n<p>Also I think that:</p>\n<pre><code>REGISTER_NO_GRADIENT_OP(\"Floor\");\n</code></pre>\n<p>is enough, no need for the code that I previously pasted, NoGradientOp are already defined like that for LogicalNot, LogicalOr, ... and they aren't tested either.</p>\n<p>In summary, I'd say add only <code>REGISTER_NO_GRADIENT_OP(\"Floor\");</code> and we should test if the variables are reachable taking into account NoGradient links. As I worked on reachable nodes I can eventually do it, what do you think <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=88808\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/skye\">@skye</a> ?</p>", "body_text": "@gdeer81 Sorry for the delay.\nAs a NoGradient cut the flow of the gradient I don't see what we can test. I mean if you do something like that:\nauto y = Square(Floor(Matmul(x1, X2)))\n\nAnd you try to differentiate y w.r.t x1 or x2, it will just do nothing (no error, no result) whereas in Python there is:  No gradients provided for any variable, check your graph for ops that do not support gradients, between variables.... I think that the warning should be also added, else it will be very difficult to debug.\nAlso I think that:\nREGISTER_NO_GRADIENT_OP(\"Floor\");\n\nis enough, no need for the code that I previously pasted, NoGradientOp are already defined like that for LogicalNot, LogicalOr, ... and they aren't tested either.\nIn summary, I'd say add only REGISTER_NO_GRADIENT_OP(\"Floor\"); and we should test if the variables are reachable taking into account NoGradient links. As I worked on reachable nodes I can eventually do it, what do you think @skye ?", "body": "@gdeer81 Sorry for the delay. \r\n\r\nAs a NoGradient cut the flow of the gradient I don't see what we can test. I mean if you do something like that:\r\n```\r\nauto y = Square(Floor(Matmul(x1, X2)))\r\n```\r\nAnd you try to differentiate y w.r.t x1 or x2, it will just do nothing (no error, no result) whereas in Python there is:  `No gradients provided for any variable, check your graph for ops that do not support gradients, between variables...`. I think that the warning should be also added, else it will be very difficult to debug.\r\n\r\nAlso I think that:\r\n```\r\nREGISTER_NO_GRADIENT_OP(\"Floor\");\r\n```\r\nis enough, no need for the code that I previously pasted, NoGradientOp are already defined like that for LogicalNot, LogicalOr, ... and they aren't tested either.\r\n\r\nIn summary, I'd say add only `REGISTER_NO_GRADIENT_OP(\"Floor\");` and we should test if the variables are reachable taking into account NoGradient links. As I worked on reachable nodes I can eventually do it, what do you think @skye ?"}