{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20298", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20298/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20298/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20298/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20298", "id": 335686824, "node_id": "MDU6SXNzdWUzMzU2ODY4MjQ=", "number": 20298, "title": "Memory leak when run multi-times in C++", "user": {"login": "7oud", "id": 7981826, "node_id": "MDQ6VXNlcjc5ODE4MjY=", "avatar_url": "https://avatars0.githubusercontent.com/u/7981826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/7oud", "html_url": "https://github.com/7oud", "followers_url": "https://api.github.com/users/7oud/followers", "following_url": "https://api.github.com/users/7oud/following{/other_user}", "gists_url": "https://api.github.com/users/7oud/gists{/gist_id}", "starred_url": "https://api.github.com/users/7oud/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/7oud/subscriptions", "organizations_url": "https://api.github.com/users/7oud/orgs", "repos_url": "https://api.github.com/users/7oud/repos", "events_url": "https://api.github.com/users/7oud/events{/privacy}", "received_events_url": "https://api.github.com/users/7oud/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 0, "created_at": "2018-06-26T07:11:31Z", "updated_at": "2018-06-26T15:37:42Z", "closed_at": "2018-06-26T15:37:42Z", "author_association": "NONE", "body_html": "<p>I build tensorflow dll in Win10 and call it in my project. I get correct results, but when running serveral times it will leak CPU memory, is there any resource I do not release ? Here is some C++ code</p>\n<pre><code>for (int i = 0; i &lt; 1000; i++)\n{\n  NewSession(SessionOptions(), &amp;_session);\n  ReadBinaryProto(Env::Default(), model_path, &amp;_graphdef);\n  _session-&gt;Create(_graphdef);\n  _session-&gt;Run(inputs, _output_tensor_names, {}, &amp;outputs);\n  _session-&gt;Close();\n}\n</code></pre>\n<p>if I change to the following code, there is no CPU memery leak, but I hope to release GPU memory when not using GPU</p>\n<pre><code>NewSession(SessionOptions(), &amp;_session);\nReadBinaryProto(Env::Default(), model_path, &amp;_graphdef);\n_session-&gt;Create(_graphdef);\nfor (int i = 0; i &lt; 1000; i++)\n{\n  _session-&gt;Run(inputs, _output_tensor_names, {}, &amp;outputs);\n  _session-&gt;Close();\n}\n</code></pre>\n<p>make code simple , only newsession and close multi-times, it will leak CPU memory</p>\n<pre><code>for (int i = 0; i &lt; 1000; i++)\n{\n  NewSession(SessionOptions(), &amp;_session);\n  _session-&gt;Close();\n}\n</code></pre>", "body_text": "I build tensorflow dll in Win10 and call it in my project. I get correct results, but when running serveral times it will leak CPU memory, is there any resource I do not release ? Here is some C++ code\nfor (int i = 0; i < 1000; i++)\n{\n  NewSession(SessionOptions(), &_session);\n  ReadBinaryProto(Env::Default(), model_path, &_graphdef);\n  _session->Create(_graphdef);\n  _session->Run(inputs, _output_tensor_names, {}, &outputs);\n  _session->Close();\n}\n\nif I change to the following code, there is no CPU memery leak, but I hope to release GPU memory when not using GPU\nNewSession(SessionOptions(), &_session);\nReadBinaryProto(Env::Default(), model_path, &_graphdef);\n_session->Create(_graphdef);\nfor (int i = 0; i < 1000; i++)\n{\n  _session->Run(inputs, _output_tensor_names, {}, &outputs);\n  _session->Close();\n}\n\nmake code simple , only newsession and close multi-times, it will leak CPU memory\nfor (int i = 0; i < 1000; i++)\n{\n  NewSession(SessionOptions(), &_session);\n  _session->Close();\n}", "body": "I build tensorflow dll in Win10 and call it in my project. I get correct results, but when running serveral times it will leak CPU memory, is there any resource I do not release ? Here is some C++ code\r\n```\r\nfor (int i = 0; i < 1000; i++)\r\n{\r\n  NewSession(SessionOptions(), &_session);\r\n  ReadBinaryProto(Env::Default(), model_path, &_graphdef);\r\n  _session->Create(_graphdef);\r\n  _session->Run(inputs, _output_tensor_names, {}, &outputs);\r\n  _session->Close();\r\n}\r\n```\r\nif I change to the following code, there is no CPU memery leak, but I hope to release GPU memory when not using GPU\r\n```\r\nNewSession(SessionOptions(), &_session);\r\nReadBinaryProto(Env::Default(), model_path, &_graphdef);\r\n_session->Create(_graphdef);\r\nfor (int i = 0; i < 1000; i++)\r\n{\r\n  _session->Run(inputs, _output_tensor_names, {}, &outputs);\r\n  _session->Close();\r\n}\r\n```\r\nmake code simple , only newsession and close multi-times, it will leak CPU memory\r\n```\r\nfor (int i = 0; i < 1000; i++)\r\n{\r\n  NewSession(SessionOptions(), &_session);\r\n  _session->Close();\r\n}\r\n```\r\n"}