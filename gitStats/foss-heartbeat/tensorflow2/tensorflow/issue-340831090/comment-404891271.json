{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404891271", "html_url": "https://github.com/tensorflow/tensorflow/pull/20757#issuecomment-404891271", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20757", "id": 404891271, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDg5MTI3MQ==", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-13T16:55:05Z", "updated_at": "2018-07-13T16:55:05Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> It turns out the rabbit hole is quite deep. In order to ditch <code>std::pair&lt;int, int&gt;</code> in XLA, and adopt a common <code>DeviceVersion</code> class for NVPTX and AMDGPU, the following components have to be changed:</p>\n<ul>\n<li>GPU common runtime : move <code>CudaVersion</code> to StreamExecutor and rename it to <code>DeviceVersion</code></li>\n<li>StreamExecutor : instead of using <code>cc_major</code> and <code>cc_minor</code>, use <code>DeviceVersion</code></li>\n<li>XLA : forefeit interfaces which take <code>std::pair&lt;int, int&gt;</code> and switch to <code>DeviceVersion</code></li>\n</ul>\n<p>I'll submit one PR for this soon and will then revise this PR.</p>", "body_text": "@jlebar It turns out the rabbit hole is quite deep. In order to ditch std::pair<int, int> in XLA, and adopt a common DeviceVersion class for NVPTX and AMDGPU, the following components have to be changed:\n\nGPU common runtime : move CudaVersion to StreamExecutor and rename it to DeviceVersion\nStreamExecutor : instead of using cc_major and cc_minor, use DeviceVersion\nXLA : forefeit interfaces which take std::pair<int, int> and switch to DeviceVersion\n\nI'll submit one PR for this soon and will then revise this PR.", "body": "@jlebar It turns out the rabbit hole is quite deep. In order to ditch `std::pair<int, int>` in XLA, and adopt a common `DeviceVersion` class for NVPTX and AMDGPU, the following components have to be changed:\r\n\r\n- GPU common runtime : move `CudaVersion` to StreamExecutor and rename it to `DeviceVersion`\r\n- StreamExecutor : instead of using `cc_major` and `cc_minor`, use `DeviceVersion`\r\n- XLA : forefeit interfaces which take `std::pair<int, int>` and switch to `DeviceVersion`\r\n\r\nI'll submit one PR for this soon and will then revise this PR."}