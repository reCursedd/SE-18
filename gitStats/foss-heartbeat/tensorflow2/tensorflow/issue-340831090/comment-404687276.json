{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/404687276", "html_url": "https://github.com/tensorflow/tensorflow/pull/20757#issuecomment-404687276", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20757", "id": 404687276, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNDY4NzI3Ng==", "user": {"login": "whchung", "id": 1673574, "node_id": "MDQ6VXNlcjE2NzM1NzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1673574?v=4", "gravatar_id": "", "url": "https://api.github.com/users/whchung", "html_url": "https://github.com/whchung", "followers_url": "https://api.github.com/users/whchung/followers", "following_url": "https://api.github.com/users/whchung/following{/other_user}", "gists_url": "https://api.github.com/users/whchung/gists{/gist_id}", "starred_url": "https://api.github.com/users/whchung/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/whchung/subscriptions", "organizations_url": "https://api.github.com/users/whchung/orgs", "repos_url": "https://api.github.com/users/whchung/repos", "events_url": "https://api.github.com/users/whchung/events{/privacy}", "received_events_url": "https://api.github.com/users/whchung/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-13T00:07:46Z", "updated_at": "2018-07-13T00:08:13Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> I think it's fine to use <code>std::pair&lt;int, int&gt;</code> for AMDGPU LLVM backend and leave the second value be 0.</p>\n<p>However there is another complexity imposed in the interface of <code>DeviceDescription::cuda_compute_capability()</code>. Certainly it's not ideal to call such interface in a generic <code>GpuExecutable</code>.</p>\n<p>In PR <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"340369230\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/20709\" data-hovercard-type=\"pull_request\" data-hovercard-url=\"/tensorflow/tensorflow/pull/20709/hovercard\" href=\"https://github.com/tensorflow/tensorflow/pull/20709\">#20709</a> , I amended an member function <code>DeviceDescription::rocm_amdgpu_isa_version()</code> for ROCm path, and let AMDGPU subclass of <code>GpuExecutable</code>, <code>AMDGPUExecutable</code> , use this new interface.</p>\n<p>Do you see it feasible to merge these 2 interfaces into one in StreamExecutor and give it a more generic name?</p>", "body_text": "@jlebar I think it's fine to use std::pair<int, int> for AMDGPU LLVM backend and leave the second value be 0.\nHowever there is another complexity imposed in the interface of DeviceDescription::cuda_compute_capability(). Certainly it's not ideal to call such interface in a generic GpuExecutable.\nIn PR #20709 , I amended an member function DeviceDescription::rocm_amdgpu_isa_version() for ROCm path, and let AMDGPU subclass of GpuExecutable, AMDGPUExecutable , use this new interface.\nDo you see it feasible to merge these 2 interfaces into one in StreamExecutor and give it a more generic name?", "body": "@jlebar I think it's fine to use `std::pair<int, int>` for AMDGPU LLVM backend and leave the second value be 0.\r\n\r\nHowever there is another complexity imposed in the interface of `DeviceDescription::cuda_compute_capability()`. Certainly it's not ideal to call such interface in a generic `GpuExecutable`.\r\n\r\nIn PR #20709 , I amended an member function `DeviceDescription::rocm_amdgpu_isa_version()` for ROCm path, and let AMDGPU subclass of `GpuExecutable`, `AMDGPUExecutable` , use this new interface.\r\n\r\nDo you see it feasible to merge these 2 interfaces into one in StreamExecutor and give it a more generic name?"}