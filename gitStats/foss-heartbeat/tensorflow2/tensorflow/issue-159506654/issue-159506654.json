{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2765", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2765/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2765/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2765/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2765", "id": 159506654, "node_id": "MDU6SXNzdWUxNTk1MDY2NTQ=", "number": 2765, "title": "Initializing all weight matrices for RNN", "user": {"login": "PhABC", "id": 9306422, "node_id": "MDQ6VXNlcjkzMDY0MjI=", "avatar_url": "https://avatars2.githubusercontent.com/u/9306422?v=4", "gravatar_id": "", "url": "https://api.github.com/users/PhABC", "html_url": "https://github.com/PhABC", "followers_url": "https://api.github.com/users/PhABC/followers", "following_url": "https://api.github.com/users/PhABC/following{/other_user}", "gists_url": "https://api.github.com/users/PhABC/gists{/gist_id}", "starred_url": "https://api.github.com/users/PhABC/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/PhABC/subscriptions", "organizations_url": "https://api.github.com/users/PhABC/orgs", "repos_url": "https://api.github.com/users/PhABC/repos", "events_url": "https://api.github.com/users/PhABC/events{/privacy}", "received_events_url": "https://api.github.com/users/PhABC/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2016-06-09T20:59:11Z", "updated_at": "2016-06-11T16:10:14Z", "closed_at": "2016-06-11T16:10:14Z", "author_association": "NONE", "body_html": "<p>It is currently impossible to initialize the input -&gt; hidden connectivity matrix, and the hidden-&gt;hidden connectivity matrix in RNNCells. Indeed, these are generated automatically in tf.python.ops.rnn_cell._linear, which constricts the connectivity architecture. As an example, I would like to have an identity connectivity matrix for input-&gt;hidden (H), an all-to-all for H-&gt;H and another identity for H-&gt;output. This is currently not feasible if you don't want to correct the weights at every iteration.</p>\n<p>I am not sure if this is the right place to post it, since I have never contributed to big projects like tensorflow. Nevertheless, I modified the BasicRNNCell and _linear function in order to allow such initialization.</p>\n<p><em>High level description</em>:<br>\nBasicRNNCell can now take 'initializer' and 'trainable' as inputs, which can be both list of two elements.<br>\n<strong>Initializer</strong>: Weight matrix combining Input -&gt; H and H -&gt; H connections. Initializer can actually be a list of two matrices, or a matrice and a vector. The first matrix can act as an update index (if trainable of the first matrix is set to False) and the other one (trainable) will either do a dot product (if w2 is a vector) or element wise model (if w2 is a matrix).<br>\n<strong>Trainable</strong>: Whether the specified weight matrice can be trained. If 'initializer' is composed of two matrices, 'trainable' has to be a boolean list of two elements.</p>\n<p>This allows a full control of the weight matrices for the cell, while still leaving all the defaults behaviours.</p>\n<p>*<em>Code changes are wrap between double stars ( <em>* ). Do a CTRL+F to highlight them.</em></em></p>\n<h2>BasicRNNCell function</h2>\n<pre><code>`class BasicRNNCell(RNNCell):\n  \"\"\"The most basic RNN cell.\"\"\"\n\n  def __init__(self, num_units, input_size=None, activation=tanh,\n                     **initializer = None, trainable = True**):\n    if input_size is not None:\n      logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n   self._num_units   = num_units\n    self._activation  = activation\n   **self._initializer = initializer**\n    **self._trainable   = trainable**\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n      output = self._activation(_linear([inputs, state], \n                                        self._num_units, \n                                        True,\n                                        **initializer = self._initializer**,\n                                        **trainable   = self._trainable**))\n    return output, output`\n</code></pre>\n<h2>_linear function</h2>\n<pre><code>def _linear(args, output_size, bias, bias_start=0.0, scope=None,\n            **initializer=None, trainable=True**):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n    **initializer: Initial weight tensor. Can be a list with up to 2 tensors. \n                 If both tensors are 2D, an element wise multiplication \n                 will be applied. If the first tensor is 2D and the second 1D, \n                 innder product will be applied. Ws have to have inner dimension\n                 equal to total_arg_size.\n    trainable: True if the variable is trainable. Can be a list with \n               same dimension as initializer.**\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"\n  if args is None or (_is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not _is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n    else:\n      total_arg_size += shape[1]   \n\n  **if not np.shape(initializer) and not np.shape(trainable):\n    trainLen = initLen = 0\n  elif not np.shape(trainable):\n    initLen  = np.shape(initializer)[0]\n    trainLen = 1\n  else:\n    initLen = len(initializer)\n    trainLen = len(trainable)\n  if initLen != trainLen:\n    raise ValueError(\"`initializer`(len: %s) and `trainable`(len: %s) must be the same len.\"\n                      % (str(initLen), str(trainLen)))**\n\n  # Now the computation.\n  with vs.variable_scope(scope or \"Linear\"):\n    **if not initializer:**\n      matrix = vs.get_variable(\"Matrix\", [total_arg_size, output_size], \n                                 **trainable = trainable)\n    else:\n      if initLen == 1: \n        matrix = vs.get_variable(\"Matrix\",initializer = initializer,\n                                          trainable   = trainable)\n      elif initLen == 2:\n        init0Shape = initializer[0].get_shape().as_list()\n        init1Shape = initializer[1].get_shape().as_list() \n\n        if init0Shape[0] !=total_arg_size:\n          raise ValueError(\n    \"`initializer` first dimension (%s,) should be equal to the `input` inner dimension (%s,).\"\n                            % (str(init0Shape[0]),str(total_arg_size)))\n\n        matrix0 = vs.get_variable(\"Matrix0\",initializer = initializer[0],\n                                            trainable   = trainable[0]  )\n        matrix1 = vs.get_variable(\"Matrix1\",initializer = initializer[1],\n                                            trainable   = trainable[1]  )            \n        if init0Shape[1] &gt; init1Shape[1]:\n          matrix = math_ops.matmul(matrix0,matrix1)\n        elif init0Shape[1] == init1Shape[1]:\n          matrix = math_ops.mul(matrix0,matrix1)\n        else:\n          raise ValueError(\"First matrix in `initializer` should be equal or bigger \\\n                            than the second matrix\")\n      else:\n        raise ValueError(\"`initializer`and `trainable` must be lists of 2 elements \\\n                               maximum\") \n    if len(args) == 1:\n      res = math_ops.matmul(args[0], matrix)\n    else:\n      res = math_ops.matmul(array_ops.concat(1, args), matrix)**\n    if not bias:\n      return res\n    bias_term = vs.get_variable(\n        \"Bias\", [output_size],\n        initializer = init_ops.constant_initializer(bias_start))\n  return res + bias_term`\n</code></pre>\n<p>Would this be an interesting feature for a future release? I added error raises that cover pretty much all errors I could think of. This is a simple change and could be implemented in the other RNNCells type.</p>\n<p>Thank you for your feedback.</p>", "body_text": "It is currently impossible to initialize the input -> hidden connectivity matrix, and the hidden->hidden connectivity matrix in RNNCells. Indeed, these are generated automatically in tf.python.ops.rnn_cell._linear, which constricts the connectivity architecture. As an example, I would like to have an identity connectivity matrix for input->hidden (H), an all-to-all for H->H and another identity for H->output. This is currently not feasible if you don't want to correct the weights at every iteration.\nI am not sure if this is the right place to post it, since I have never contributed to big projects like tensorflow. Nevertheless, I modified the BasicRNNCell and _linear function in order to allow such initialization.\nHigh level description:\nBasicRNNCell can now take 'initializer' and 'trainable' as inputs, which can be both list of two elements.\nInitializer: Weight matrix combining Input -> H and H -> H connections. Initializer can actually be a list of two matrices, or a matrice and a vector. The first matrix can act as an update index (if trainable of the first matrix is set to False) and the other one (trainable) will either do a dot product (if w2 is a vector) or element wise model (if w2 is a matrix).\nTrainable: Whether the specified weight matrice can be trained. If 'initializer' is composed of two matrices, 'trainable' has to be a boolean list of two elements.\nThis allows a full control of the weight matrices for the cell, while still leaving all the defaults behaviours.\n*Code changes are wrap between double stars ( * ). Do a CTRL+F to highlight them.\nBasicRNNCell function\n`class BasicRNNCell(RNNCell):\n  \"\"\"The most basic RNN cell.\"\"\"\n\n  def __init__(self, num_units, input_size=None, activation=tanh,\n                     **initializer = None, trainable = True**):\n    if input_size is not None:\n      logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n   self._num_units   = num_units\n    self._activation  = activation\n   **self._initializer = initializer**\n    **self._trainable   = trainable**\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n      output = self._activation(_linear([inputs, state], \n                                        self._num_units, \n                                        True,\n                                        **initializer = self._initializer**,\n                                        **trainable   = self._trainable**))\n    return output, output`\n\n_linear function\ndef _linear(args, output_size, bias, bias_start=0.0, scope=None,\n            **initializer=None, trainable=True**):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n    **initializer: Initial weight tensor. Can be a list with up to 2 tensors. \n                 If both tensors are 2D, an element wise multiplication \n                 will be applied. If the first tensor is 2D and the second 1D, \n                 innder product will be applied. Ws have to have inner dimension\n                 equal to total_arg_size.\n    trainable: True if the variable is trainable. Can be a list with \n               same dimension as initializer.**\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"\n  if args is None or (_is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not _is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n    else:\n      total_arg_size += shape[1]   \n\n  **if not np.shape(initializer) and not np.shape(trainable):\n    trainLen = initLen = 0\n  elif not np.shape(trainable):\n    initLen  = np.shape(initializer)[0]\n    trainLen = 1\n  else:\n    initLen = len(initializer)\n    trainLen = len(trainable)\n  if initLen != trainLen:\n    raise ValueError(\"`initializer`(len: %s) and `trainable`(len: %s) must be the same len.\"\n                      % (str(initLen), str(trainLen)))**\n\n  # Now the computation.\n  with vs.variable_scope(scope or \"Linear\"):\n    **if not initializer:**\n      matrix = vs.get_variable(\"Matrix\", [total_arg_size, output_size], \n                                 **trainable = trainable)\n    else:\n      if initLen == 1: \n        matrix = vs.get_variable(\"Matrix\",initializer = initializer,\n                                          trainable   = trainable)\n      elif initLen == 2:\n        init0Shape = initializer[0].get_shape().as_list()\n        init1Shape = initializer[1].get_shape().as_list() \n\n        if init0Shape[0] !=total_arg_size:\n          raise ValueError(\n    \"`initializer` first dimension (%s,) should be equal to the `input` inner dimension (%s,).\"\n                            % (str(init0Shape[0]),str(total_arg_size)))\n\n        matrix0 = vs.get_variable(\"Matrix0\",initializer = initializer[0],\n                                            trainable   = trainable[0]  )\n        matrix1 = vs.get_variable(\"Matrix1\",initializer = initializer[1],\n                                            trainable   = trainable[1]  )            \n        if init0Shape[1] > init1Shape[1]:\n          matrix = math_ops.matmul(matrix0,matrix1)\n        elif init0Shape[1] == init1Shape[1]:\n          matrix = math_ops.mul(matrix0,matrix1)\n        else:\n          raise ValueError(\"First matrix in `initializer` should be equal or bigger \\\n                            than the second matrix\")\n      else:\n        raise ValueError(\"`initializer`and `trainable` must be lists of 2 elements \\\n                               maximum\") \n    if len(args) == 1:\n      res = math_ops.matmul(args[0], matrix)\n    else:\n      res = math_ops.matmul(array_ops.concat(1, args), matrix)**\n    if not bias:\n      return res\n    bias_term = vs.get_variable(\n        \"Bias\", [output_size],\n        initializer = init_ops.constant_initializer(bias_start))\n  return res + bias_term`\n\nWould this be an interesting feature for a future release? I added error raises that cover pretty much all errors I could think of. This is a simple change and could be implemented in the other RNNCells type.\nThank you for your feedback.", "body": "It is currently impossible to initialize the input -> hidden connectivity matrix, and the hidden->hidden connectivity matrix in RNNCells. Indeed, these are generated automatically in tf.python.ops.rnn_cell._linear, which constricts the connectivity architecture. As an example, I would like to have an identity connectivity matrix for input->hidden (H), an all-to-all for H->H and another identity for H->output. This is currently not feasible if you don't want to correct the weights at every iteration. \n\nI am not sure if this is the right place to post it, since I have never contributed to big projects like tensorflow. Nevertheless, I modified the BasicRNNCell and _linear function in order to allow such initialization.\n\n_High level description_: \nBasicRNNCell can now take 'initializer' and 'trainable' as inputs, which can be both list of two elements.\n**Initializer**: Weight matrix combining Input -> H and H -> H connections. Initializer can actually be a list of two matrices, or a matrice and a vector. The first matrix can act as an update index (if trainable of the first matrix is set to False) and the other one (trainable) will either do a dot product (if w2 is a vector) or element wise model (if w2 is a matrix). \n**Trainable**: Whether the specified weight matrice can be trained. If 'initializer' is composed of two matrices, 'trainable' has to be a boolean list of two elements. \n\nThis allows a full control of the weight matrices for the cell, while still leaving all the defaults behaviours. \n\n**Code changes are wrap between double stars ( *\\* ). Do a CTRL+F to highlight them.**\n## BasicRNNCell function\n\n```\n`class BasicRNNCell(RNNCell):\n  \"\"\"The most basic RNN cell.\"\"\"\n\n  def __init__(self, num_units, input_size=None, activation=tanh,\n                     **initializer = None, trainable = True**):\n    if input_size is not None:\n      logging.warn(\"%s: The input_size parameter is deprecated.\" % self)\n   self._num_units   = num_units\n    self._activation  = activation\n   **self._initializer = initializer**\n    **self._trainable   = trainable**\n  @property\n  def state_size(self):\n    return self._num_units\n\n  @property\n  def output_size(self):\n    return self._num_units\n\n  def __call__(self, inputs, state, scope=None):\n    \"\"\"Most basic RNN: output = new_state = activation(W * input + U * state + B).\"\"\"\n    with vs.variable_scope(scope or type(self).__name__):  # \"BasicRNNCell\"\n      output = self._activation(_linear([inputs, state], \n                                        self._num_units, \n                                        True,\n                                        **initializer = self._initializer**,\n                                        **trainable   = self._trainable**))\n    return output, output`\n```\n## _linear function\n\n```\ndef _linear(args, output_size, bias, bias_start=0.0, scope=None,\n            **initializer=None, trainable=True**):\n  \"\"\"Linear map: sum_i(args[i] * W[i]), where W[i] is a variable.\n\n  Args:\n    args: a 2D Tensor or a list of 2D, batch x n, Tensors.\n    output_size: int, second dimension of W[i].\n    bias: boolean, whether to add a bias term or not.\n    bias_start: starting value to initialize the bias; 0 by default.\n    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n    **initializer: Initial weight tensor. Can be a list with up to 2 tensors. \n                 If both tensors are 2D, an element wise multiplication \n                 will be applied. If the first tensor is 2D and the second 1D, \n                 innder product will be applied. Ws have to have inner dimension\n                 equal to total_arg_size.\n    trainable: True if the variable is trainable. Can be a list with \n               same dimension as initializer.**\n\n  Returns:\n    A 2D Tensor with shape [batch x output_size] equal to\n    sum_i(args[i] * W[i]), where W[i]s are newly created matrices.\n\n  Raises:\n    ValueError: if some of the arguments has unspecified or wrong shape.\n  \"\"\"\n  if args is None or (_is_sequence(args) and not args):\n    raise ValueError(\"`args` must be specified\")\n  if not _is_sequence(args):\n    args = [args]\n\n  # Calculate the total size of arguments on dimension 1.\n  total_arg_size = 0\n  shapes = [a.get_shape().as_list() for a in args]\n  for shape in shapes:\n    if len(shape) != 2:\n      raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shapes))\n    if not shape[1]:\n      raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shapes))\n    else:\n      total_arg_size += shape[1]   \n\n  **if not np.shape(initializer) and not np.shape(trainable):\n    trainLen = initLen = 0\n  elif not np.shape(trainable):\n    initLen  = np.shape(initializer)[0]\n    trainLen = 1\n  else:\n    initLen = len(initializer)\n    trainLen = len(trainable)\n  if initLen != trainLen:\n    raise ValueError(\"`initializer`(len: %s) and `trainable`(len: %s) must be the same len.\"\n                      % (str(initLen), str(trainLen)))**\n\n  # Now the computation.\n  with vs.variable_scope(scope or \"Linear\"):\n    **if not initializer:**\n      matrix = vs.get_variable(\"Matrix\", [total_arg_size, output_size], \n                                 **trainable = trainable)\n    else:\n      if initLen == 1: \n        matrix = vs.get_variable(\"Matrix\",initializer = initializer,\n                                          trainable   = trainable)\n      elif initLen == 2:\n        init0Shape = initializer[0].get_shape().as_list()\n        init1Shape = initializer[1].get_shape().as_list() \n\n        if init0Shape[0] !=total_arg_size:\n          raise ValueError(\n    \"`initializer` first dimension (%s,) should be equal to the `input` inner dimension (%s,).\"\n                            % (str(init0Shape[0]),str(total_arg_size)))\n\n        matrix0 = vs.get_variable(\"Matrix0\",initializer = initializer[0],\n                                            trainable   = trainable[0]  )\n        matrix1 = vs.get_variable(\"Matrix1\",initializer = initializer[1],\n                                            trainable   = trainable[1]  )            \n        if init0Shape[1] > init1Shape[1]:\n          matrix = math_ops.matmul(matrix0,matrix1)\n        elif init0Shape[1] == init1Shape[1]:\n          matrix = math_ops.mul(matrix0,matrix1)\n        else:\n          raise ValueError(\"First matrix in `initializer` should be equal or bigger \\\n                            than the second matrix\")\n      else:\n        raise ValueError(\"`initializer`and `trainable` must be lists of 2 elements \\\n                               maximum\") \n    if len(args) == 1:\n      res = math_ops.matmul(args[0], matrix)\n    else:\n      res = math_ops.matmul(array_ops.concat(1, args), matrix)**\n    if not bias:\n      return res\n    bias_term = vs.get_variable(\n        \"Bias\", [output_size],\n        initializer = init_ops.constant_initializer(bias_start))\n  return res + bias_term`\n```\n\nWould this be an interesting feature for a future release? I added error raises that cover pretty much all errors I could think of. This is a simple change and could be implemented in the other RNNCells type.\n\nThank you for your feedback.\n"}