{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1354", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1354/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1354/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1354/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/1354", "id": 137795764, "node_id": "MDU6SXNzdWUxMzc3OTU3NjQ=", "number": 1354, "title": "How to decrease memory usage on GPU card", "user": {"login": "gaoteng-hi", "id": 17082524, "node_id": "MDQ6VXNlcjE3MDgyNTI0", "avatar_url": "https://avatars1.githubusercontent.com/u/17082524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaoteng-hi", "html_url": "https://github.com/gaoteng-hi", "followers_url": "https://api.github.com/users/gaoteng-hi/followers", "following_url": "https://api.github.com/users/gaoteng-hi/following{/other_user}", "gists_url": "https://api.github.com/users/gaoteng-hi/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaoteng-hi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaoteng-hi/subscriptions", "organizations_url": "https://api.github.com/users/gaoteng-hi/orgs", "repos_url": "https://api.github.com/users/gaoteng-hi/repos", "events_url": "https://api.github.com/users/gaoteng-hi/events{/privacy}", "received_events_url": "https://api.github.com/users/gaoteng-hi/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-03-02T07:23:48Z", "updated_at": "2016-03-02T17:03:16Z", "closed_at": "2016-03-02T17:03:16Z", "author_association": "NONE", "body_html": "<p>When I run example like mnist on GPU card, I see that it consumes almost all device memory of that card. It seems like the tensorflow reserves as much memory as possible for memory pool at the start of process.<br>\nHowever, we need to run the tensor flow on a shared GPU card, on which runs other users' processes. So we can't occupy all device memory of it. Is there any switch to turn this memory pool off, or decrease the memory pool to a user defined size?<br>\nThanks a lot in advance!</p>", "body_text": "When I run example like mnist on GPU card, I see that it consumes almost all device memory of that card. It seems like the tensorflow reserves as much memory as possible for memory pool at the start of process.\nHowever, we need to run the tensor flow on a shared GPU card, on which runs other users' processes. So we can't occupy all device memory of it. Is there any switch to turn this memory pool off, or decrease the memory pool to a user defined size?\nThanks a lot in advance!", "body": "When I run example like mnist on GPU card, I see that it consumes almost all device memory of that card. It seems like the tensorflow reserves as much memory as possible for memory pool at the start of process.\nHowever, we need to run the tensor flow on a shared GPU card, on which runs other users' processes. So we can't occupy all device memory of it. Is there any switch to turn this memory pool off, or decrease the memory pool to a user defined size?\nThanks a lot in advance!\n"}