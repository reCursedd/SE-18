{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/339697205", "html_url": "https://github.com/tensorflow/tensorflow/issues/1355#issuecomment-339697205", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/1355", "id": 339697205, "node_id": "MDEyOklzc3VlQ29tbWVudDMzOTY5NzIwNQ==", "user": {"login": "lishen", "id": 874315, "node_id": "MDQ6VXNlcjg3NDMxNQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/874315?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lishen", "html_url": "https://github.com/lishen", "followers_url": "https://api.github.com/users/lishen/followers", "following_url": "https://api.github.com/users/lishen/following{/other_user}", "gists_url": "https://api.github.com/users/lishen/gists{/gist_id}", "starred_url": "https://api.github.com/users/lishen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lishen/subscriptions", "organizations_url": "https://api.github.com/users/lishen/orgs", "repos_url": "https://api.github.com/users/lishen/repos", "events_url": "https://api.github.com/users/lishen/events{/privacy}", "received_events_url": "https://api.github.com/users/lishen/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-26T15:08:04Z", "updated_at": "2017-10-26T15:08:04Z", "author_association": "NONE", "body_html": "<p>Reducing batch size of course reduces the memory usage but that is not the key issue here. The key is why running multiple models consecutively caused GPU blow-up.</p>\n<p>I had a similar problem. When run model training using single GPU worked fine. But when I duplicated the model onto multiple GPUs (with same batch size per GPU) blew up. I was using Keras' new multi_gpu_model API call.</p>", "body_text": "Reducing batch size of course reduces the memory usage but that is not the key issue here. The key is why running multiple models consecutively caused GPU blow-up.\nI had a similar problem. When run model training using single GPU worked fine. But when I duplicated the model onto multiple GPUs (with same batch size per GPU) blew up. I was using Keras' new multi_gpu_model API call.", "body": "Reducing batch size of course reduces the memory usage but that is not the key issue here. The key is why running multiple models consecutively caused GPU blow-up. \r\n\r\nI had a similar problem. When run model training using single GPU worked fine. But when I duplicated the model onto multiple GPUs (with same batch size per GPU) blew up. I was using Keras' new multi_gpu_model API call."}