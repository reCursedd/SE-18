{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/345543026", "html_url": "https://github.com/tensorflow/tensorflow/issues/446#issuecomment-345543026", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/446", "id": 345543026, "node_id": "MDEyOklzc3VlQ29tbWVudDM0NTU0MzAyNg==", "user": {"login": "debasish83", "id": 510896, "node_id": "MDQ6VXNlcjUxMDg5Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/510896?v=4", "gravatar_id": "", "url": "https://api.github.com/users/debasish83", "html_url": "https://github.com/debasish83", "followers_url": "https://api.github.com/users/debasish83/followers", "following_url": "https://api.github.com/users/debasish83/following{/other_user}", "gists_url": "https://api.github.com/users/debasish83/gists{/gist_id}", "starred_url": "https://api.github.com/users/debasish83/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/debasish83/subscriptions", "organizations_url": "https://api.github.com/users/debasish83/orgs", "repos_url": "https://api.github.com/users/debasish83/repos", "events_url": "https://api.github.com/users/debasish83/events{/privacy}", "received_events_url": "https://api.github.com/users/debasish83/received_events", "type": "User", "site_admin": false}, "created_at": "2017-11-19T19:34:45Z", "updated_at": "2017-11-19T19:34:45Z", "author_association": "NONE", "body_html": "<p>We have added LBFGS based optimizers in Breeze which is being used in Spark MLlib for example and the C++ port from fortran is available through multiple projects (I benchmarked this for example <a href=\"https://github.com/chokkan/liblbfgs\">https://github.com/chokkan/liblbfgs</a>) but given that neural nets are based on non-convex loss (original distbelief paper also found that sgd + adagrad works better than LBFGS), is it useful to add LBFGS in tensorflow ? Most likely users are now running convex models in tensorflow and for such models LBFGS is a good fit compared to SGD + Adagrad ?</p>", "body_text": "We have added LBFGS based optimizers in Breeze which is being used in Spark MLlib for example and the C++ port from fortran is available through multiple projects (I benchmarked this for example https://github.com/chokkan/liblbfgs) but given that neural nets are based on non-convex loss (original distbelief paper also found that sgd + adagrad works better than LBFGS), is it useful to add LBFGS in tensorflow ? Most likely users are now running convex models in tensorflow and for such models LBFGS is a good fit compared to SGD + Adagrad ?", "body": "We have added LBFGS based optimizers in Breeze which is being used in Spark MLlib for example and the C++ port from fortran is available through multiple projects (I benchmarked this for example https://github.com/chokkan/liblbfgs) but given that neural nets are based on non-convex loss (original distbelief paper also found that sgd + adagrad works better than LBFGS), is it useful to add LBFGS in tensorflow ? Most likely users are now running convex models in tensorflow and for such models LBFGS is a good fit compared to SGD + Adagrad ? "}