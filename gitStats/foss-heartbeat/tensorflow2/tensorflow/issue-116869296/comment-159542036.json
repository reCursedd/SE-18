{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/159542036", "html_url": "https://github.com/tensorflow/tensorflow/issues/212#issuecomment-159542036", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/212", "id": 159542036, "node_id": "MDEyOklzc3VlQ29tbWVudDE1OTU0MjAzNg==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-25T09:06:38Z", "updated_at": "2015-11-25T09:06:38Z", "author_association": "NONE", "body_html": "<p>Thanks Lukasz! I had a chance to try it tonight and I still can't get this right.</p>\n<p>Are you suggesting that I have one file containing all of the models? What I'm trying utilizes two models, M_1 and M_2, and then a third, J_12, to join them. But there could be a lot more M_k and, for each two of them, there would be a J. Ideally, I would put each of the M_k in their own model directory so that I can build J_pq by loading only M_p and M_q.</p>\n<p>If I had just one checkpoint file, then I'd be breaking a lot of the modularity involved. It would also be difficult to keep track of experiments using different hyper-parameters.</p>\n<p>Taking your advice, I removed the saver from the Seq2Seq model. The training for the M_k now looks like this:</p>\n<pre><code>def create_model(..., do_restore=True):\n  ...\n  with tf.variable_scope(model_name):\n    model = Seq2SeqModel(\n      vocab_size, vocab_size, bucket_sizes, layer_size, num_layers,\n      max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor,\n      forward_only=forward_only)\n\n  ...\n  saver = tf.train.Saver(tf.all_variables())\n  if model_checkpoint_path:\n      if do_restore:\n          saver.restore(session, model_checkpoint_path)\n      else:\n          return model_checkpoint_path, model, saver\n  else:\n      session.run(tf.variables.initialize_all_variables())\n  return None, model, saver\n\nckpt_path, model, saver = create_model(..., model_name=model_name, do_restore=True)\n&lt;Train Train Train&gt;\nif current_step % steps_per_checkpoint == 0:\n    saver.save(session, ckpt_path, global_step=model.global_step)\n</code></pre>\n<p>And for the J_pq it looks like this:</p>\n<pre><code>ckpt_path_one, model_one, _ = get_rnn(\n            ..., model_dir=model_dir_one, model_name=model_one_scope, do_restore=False)\nckpt_path_two, model_two, _ = get_rnn(\n            ..., model_dir=model_dir_two, model_name=model_two_scope, do_restore=False)\n\nwith tf.variable_scope('j_pq'):\n    j_pq = ...\n\nsaver = tf.train.Saver(tf.all_variables())\nsaver.restore(session, ckpt_path_one)\nsaver.restore(session, ckpt_path_two)\n</code></pre>\n<p>This didn't work and failed with similar errors (lots of them):</p>\n<pre><code>Tensor name \"j_pq/weights\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\nTensor name \"model-en-10000-256-2-10000/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/MultiRNNCell/Cell1/GRUCell/Candidate/Linear/Matrix\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\n</code></pre>", "body_text": "Thanks Lukasz! I had a chance to try it tonight and I still can't get this right.\nAre you suggesting that I have one file containing all of the models? What I'm trying utilizes two models, M_1 and M_2, and then a third, J_12, to join them. But there could be a lot more M_k and, for each two of them, there would be a J. Ideally, I would put each of the M_k in their own model directory so that I can build J_pq by loading only M_p and M_q.\nIf I had just one checkpoint file, then I'd be breaking a lot of the modularity involved. It would also be difficult to keep track of experiments using different hyper-parameters.\nTaking your advice, I removed the saver from the Seq2Seq model. The training for the M_k now looks like this:\ndef create_model(..., do_restore=True):\n  ...\n  with tf.variable_scope(model_name):\n    model = Seq2SeqModel(\n      vocab_size, vocab_size, bucket_sizes, layer_size, num_layers,\n      max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor,\n      forward_only=forward_only)\n\n  ...\n  saver = tf.train.Saver(tf.all_variables())\n  if model_checkpoint_path:\n      if do_restore:\n          saver.restore(session, model_checkpoint_path)\n      else:\n          return model_checkpoint_path, model, saver\n  else:\n      session.run(tf.variables.initialize_all_variables())\n  return None, model, saver\n\nckpt_path, model, saver = create_model(..., model_name=model_name, do_restore=True)\n<Train Train Train>\nif current_step % steps_per_checkpoint == 0:\n    saver.save(session, ckpt_path, global_step=model.global_step)\n\nAnd for the J_pq it looks like this:\nckpt_path_one, model_one, _ = get_rnn(\n            ..., model_dir=model_dir_one, model_name=model_one_scope, do_restore=False)\nckpt_path_two, model_two, _ = get_rnn(\n            ..., model_dir=model_dir_two, model_name=model_two_scope, do_restore=False)\n\nwith tf.variable_scope('j_pq'):\n    j_pq = ...\n\nsaver = tf.train.Saver(tf.all_variables())\nsaver.restore(session, ckpt_path_one)\nsaver.restore(session, ckpt_path_two)\n\nThis didn't work and failed with similar errors (lots of them):\nTensor name \"j_pq/weights\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\nTensor name \"model-en-10000-256-2-10000/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/MultiRNNCell/Cell1/GRUCell/Candidate/Linear/Matrix\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300", "body": "Thanks Lukasz! I had a chance to try it tonight and I still can't get this right.\n\nAre you suggesting that I have one file containing all of the models? What I'm trying utilizes two models, M_1 and M_2, and then a third, J_12, to join them. But there could be a lot more M_k and, for each two of them, there would be a J. Ideally, I would put each of the M_k in their own model directory so that I can build J_pq by loading only M_p and M_q.\n\nIf I had just one checkpoint file, then I'd be breaking a lot of the modularity involved. It would also be difficult to keep track of experiments using different hyper-parameters.\n\nTaking your advice, I removed the saver from the Seq2Seq model. The training for the M_k now looks like this:\n\n```\ndef create_model(..., do_restore=True):\n  ...\n  with tf.variable_scope(model_name):\n    model = Seq2SeqModel(\n      vocab_size, vocab_size, bucket_sizes, layer_size, num_layers,\n      max_gradient_norm, batch_size, learning_rate, learning_rate_decay_factor,\n      forward_only=forward_only)\n\n  ...\n  saver = tf.train.Saver(tf.all_variables())\n  if model_checkpoint_path:\n      if do_restore:\n          saver.restore(session, model_checkpoint_path)\n      else:\n          return model_checkpoint_path, model, saver\n  else:\n      session.run(tf.variables.initialize_all_variables())\n  return None, model, saver\n\nckpt_path, model, saver = create_model(..., model_name=model_name, do_restore=True)\n<Train Train Train>\nif current_step % steps_per_checkpoint == 0:\n    saver.save(session, ckpt_path, global_step=model.global_step)\n```\n\nAnd for the J_pq it looks like this:\n\n```\nckpt_path_one, model_one, _ = get_rnn(\n            ..., model_dir=model_dir_one, model_name=model_one_scope, do_restore=False)\nckpt_path_two, model_two, _ = get_rnn(\n            ..., model_dir=model_dir_two, model_name=model_two_scope, do_restore=False)\n\nwith tf.variable_scope('j_pq'):\n    j_pq = ...\n\nsaver = tf.train.Saver(tf.all_variables())\nsaver.restore(session, ckpt_path_one)\nsaver.restore(session, ckpt_path_two)\n```\n\nThis didn't work and failed with similar errors (lots of them):\n\n```\nTensor name \"j_pq/weights\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\nTensor name \"model-en-10000-256-2-10000/embedding_rnn_seq2seq/embedding_rnn_decoder/rnn_decoder/MultiRNNCell/Cell1/GRUCell/Candidate/Linear/Matrix\" not found in checkpoint files model-de-10000-256-2-10000/translate.ckpt-300\n```\n"}