{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/158246866", "html_url": "https://github.com/tensorflow/tensorflow/issues/212#issuecomment-158246866", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/212", "id": 158246866, "node_id": "MDEyOklzc3VlQ29tbWVudDE1ODI0Njg2Ng==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-11-20T00:57:58Z", "updated_at": "2015-11-20T00:58:14Z", "author_association": "NONE", "body_html": "<p>Thanks for the fast replies guys.</p>\n<p>I changed it so that the model is created under a separate scope when I build it.</p>\n<pre><code>    with tf.variable_scope(model_name):\n      model = Seq2SeqModel(...)\n\n    ...\n\n    Seq2SeqModel():\n        def __init__(...):\n            ...\n            self.learning_rate = tf.get_variable('learning_rate', shape=[], initializer=tf.constant_initializer(float(learning_rate)), trainable=False)\n            self.global_step = tf.get_variable('global_step', shape=[], initializer=tf.constant_initializer(0), trainable=False)\n            ...\n</code></pre>\n<p>Then I made a couple of test models to a few hundred steps. Afterwards, I tried loading them again. They still load fine individually and decode, etc.</p>\n<p>So I tried loading them both at the same time:</p>\n<pre><code>print 'Creating model one from directory %s with scope %s.' % (                              \n    FLAGS.model_one_dir, FLAGS.model_one_scope)                                                    \nmodel_one = get_rnn_model(session, model_dir=FLAGS.model_one_dir,                                  \n                          model_name=FLAGS.model_one_scope)                                  \nmodel_one.batch_size = bridge_batch_size                                                     \n\nprint 'Creating model two from directory %s with scope %s.' % (                              \n    FLAGS.model_two_dir, FLAGS.model_two_scope)                                                    \nmodel_two = get_rnn_model(session, model_dir=FLAGS.model_two_dir,                                  \n                          model_name=FLAGS.model_two_scope)                                  \nmodel_two.batch_size = bridge_batch_size   \n</code></pre>\n<p>The call to <code>get_rnn_model</code> just encapsulates the model restoration in a <code>with tf.variable_scope(model_name)</code>. However, when loaded together, I'm running into the following error where the scope for the first model is used in place of the scope for the second model (...Tensor name \"<strong>de-10000-256-2-10000</strong>/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint files <strong>model-en-10000-256-2-10000</strong>/translate.ckpt-300)</p>\n<p>I thought this had to be just a coding error, but debugging it shows that the directory model it's restoring from and the model_name it's using for scope are aligned.</p>\n<p>What am I missing? I don't think it's to do with the saver because it is saving and restoring fine when using just one model.</p>\n<pre><code>Creating model one from directory &lt;...&gt;/model-de-10000-256-2-10000 with scope de-10000-256-2-10000.\nReading model parameters from model-de-10000-256-2-10000/translate.ckpt-400\n\nCreating model two from directory &lt;...&gt;/model-en-10000-256-2-10000 with scope en-10000-256-2-10000.\nReading model parameters from model-en-10000-256-2-10000/translate.ckpt-300\n\nW tensorflow/core/common_runtime/executor.cc:1052] 0xbec32b0 Compute status: Not found: Tensor name \"de-10000-256-2-10000/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint\n files model-en-10000-256-2-10000/translate.ckpt-300\n</code></pre>", "body_text": "Thanks for the fast replies guys.\nI changed it so that the model is created under a separate scope when I build it.\n    with tf.variable_scope(model_name):\n      model = Seq2SeqModel(...)\n\n    ...\n\n    Seq2SeqModel():\n        def __init__(...):\n            ...\n            self.learning_rate = tf.get_variable('learning_rate', shape=[], initializer=tf.constant_initializer(float(learning_rate)), trainable=False)\n            self.global_step = tf.get_variable('global_step', shape=[], initializer=tf.constant_initializer(0), trainable=False)\n            ...\n\nThen I made a couple of test models to a few hundred steps. Afterwards, I tried loading them again. They still load fine individually and decode, etc.\nSo I tried loading them both at the same time:\nprint 'Creating model one from directory %s with scope %s.' % (                              \n    FLAGS.model_one_dir, FLAGS.model_one_scope)                                                    \nmodel_one = get_rnn_model(session, model_dir=FLAGS.model_one_dir,                                  \n                          model_name=FLAGS.model_one_scope)                                  \nmodel_one.batch_size = bridge_batch_size                                                     \n\nprint 'Creating model two from directory %s with scope %s.' % (                              \n    FLAGS.model_two_dir, FLAGS.model_two_scope)                                                    \nmodel_two = get_rnn_model(session, model_dir=FLAGS.model_two_dir,                                  \n                          model_name=FLAGS.model_two_scope)                                  \nmodel_two.batch_size = bridge_batch_size   \n\nThe call to get_rnn_model just encapsulates the model restoration in a with tf.variable_scope(model_name). However, when loaded together, I'm running into the following error where the scope for the first model is used in place of the scope for the second model (...Tensor name \"de-10000-256-2-10000/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint files model-en-10000-256-2-10000/translate.ckpt-300)\nI thought this had to be just a coding error, but debugging it shows that the directory model it's restoring from and the model_name it's using for scope are aligned.\nWhat am I missing? I don't think it's to do with the saver because it is saving and restoring fine when using just one model.\nCreating model one from directory <...>/model-de-10000-256-2-10000 with scope de-10000-256-2-10000.\nReading model parameters from model-de-10000-256-2-10000/translate.ckpt-400\n\nCreating model two from directory <...>/model-en-10000-256-2-10000 with scope en-10000-256-2-10000.\nReading model parameters from model-en-10000-256-2-10000/translate.ckpt-300\n\nW tensorflow/core/common_runtime/executor.cc:1052] 0xbec32b0 Compute status: Not found: Tensor name \"de-10000-256-2-10000/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint\n files model-en-10000-256-2-10000/translate.ckpt-300", "body": "Thanks for the fast replies guys.\n\nI changed it so that the model is created under a separate scope when I build it.\n\n```\n    with tf.variable_scope(model_name):\n      model = Seq2SeqModel(...)\n\n    ...\n\n    Seq2SeqModel():\n        def __init__(...):\n            ...\n            self.learning_rate = tf.get_variable('learning_rate', shape=[], initializer=tf.constant_initializer(float(learning_rate)), trainable=False)\n            self.global_step = tf.get_variable('global_step', shape=[], initializer=tf.constant_initializer(0), trainable=False)\n            ...\n```\n\nThen I made a couple of test models to a few hundred steps. Afterwards, I tried loading them again. They still load fine individually and decode, etc. \n\nSo I tried loading them both at the same time:\n\n```\nprint 'Creating model one from directory %s with scope %s.' % (                              \n    FLAGS.model_one_dir, FLAGS.model_one_scope)                                                    \nmodel_one = get_rnn_model(session, model_dir=FLAGS.model_one_dir,                                  \n                          model_name=FLAGS.model_one_scope)                                  \nmodel_one.batch_size = bridge_batch_size                                                     \n\nprint 'Creating model two from directory %s with scope %s.' % (                              \n    FLAGS.model_two_dir, FLAGS.model_two_scope)                                                    \nmodel_two = get_rnn_model(session, model_dir=FLAGS.model_two_dir,                                  \n                          model_name=FLAGS.model_two_scope)                                  \nmodel_two.batch_size = bridge_batch_size   \n```\n\nThe call to `get_rnn_model` just encapsulates the model restoration in a `with tf.variable_scope(model_name)`. However, when loaded together, I'm running into the following error where the scope for the first model is used in place of the scope for the second model (...Tensor name \"**de-10000-256-2-10000**/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint files **model-en-10000-256-2-10000**/translate.ckpt-300)\n\nI thought this had to be just a coding error, but debugging it shows that the directory model it's restoring from and the model_name it's using for scope are aligned. \n\nWhat am I missing? I don't think it's to do with the saver because it is saving and restoring fine when using just one model.\n\n```\nCreating model one from directory <...>/model-de-10000-256-2-10000 with scope de-10000-256-2-10000.\nReading model parameters from model-de-10000-256-2-10000/translate.ckpt-400\n\nCreating model two from directory <...>/model-en-10000-256-2-10000 with scope en-10000-256-2-10000.\nReading model parameters from model-en-10000-256-2-10000/translate.ckpt-300\n\nW tensorflow/core/common_runtime/executor.cc:1052] 0xbec32b0 Compute status: Not found: Tensor name \"de-10000-256-2-10000/embedding_rnn_seq2seq/RNN/EmbeddingWrapper/embedding\" not found in checkpoint\n files model-en-10000-256-2-10000/translate.ckpt-300\n```\n"}