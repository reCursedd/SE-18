{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3509", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3509/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3509/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3509/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3509", "id": 167648028, "node_id": "MDU6SXNzdWUxNjc2NDgwMjg=", "number": 3509, "title": "Check failed,  cuCtxSetCurrent(context). training cifar10 multi-gpu example on 2 gpus.", "user": {"login": "gideonite", "id": 2163686, "node_id": "MDQ6VXNlcjIxNjM2ODY=", "avatar_url": "https://avatars2.githubusercontent.com/u/2163686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gideonite", "html_url": "https://github.com/gideonite", "followers_url": "https://api.github.com/users/gideonite/followers", "following_url": "https://api.github.com/users/gideonite/following{/other_user}", "gists_url": "https://api.github.com/users/gideonite/gists{/gist_id}", "starred_url": "https://api.github.com/users/gideonite/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gideonite/subscriptions", "organizations_url": "https://api.github.com/users/gideonite/orgs", "repos_url": "https://api.github.com/users/gideonite/repos", "events_url": "https://api.github.com/users/gideonite/events{/privacy}", "received_events_url": "https://api.github.com/users/gideonite/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2016-07-26T16:09:15Z", "updated_at": "2016-07-26T18:18:04Z", "closed_at": "2016-07-26T18:18:04Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I'm having trouble getting the multiple GPU example to work. Here is the full output:</p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                            \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                             \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                                                                \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                                                               \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                                                               \n&gt;&gt; Downloading cifar-10-binary.tar.gz 100.0%                                                                                                                                            \nSuccessfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\n</code></pre>\n<h3>Environment info</h3>\n<p>Operating System: CentOS release 6.7 (Final). Using a docker image available here: <a href=\"https://hub.docker.com/r/gideonitemd/hal-tf/\" rel=\"nofollow\">gideonitemd/hal-tf</a>.</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p>cudnn/7.0/lib64/libcudnn.so<br>\ncudnn/7.0/lib64/libcudnn.so.7.0.64<br>\ncudnn/7.0/lib64/libcudnn.so.7.0<br>\ncudnn/7.0/lib64/libcudnn_static.a</p>\n<p>/usr/lib/libcuda.so<br>\n/usr/lib/libcuda.so.1<br>\n/usr/lib/libcuda.so.352.39</p>\n<p>(should be libcuda v7.5)</p>\n<p>Running <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code> yields 0.8.0</p>\n<h3>Steps to reproduce</h3>\n<p>Run this (the output of the <code>docker_run_gpu.sh</code> script):</p>\n<pre><code>docker run -it -v /usr/lib64/libcuda.so:/usr/lib64/libcuda.so -v /usr/lib64/libcuda.so.1:/usr/lib64/libcuda.so.1 -v /usr/lib64/libcuda.so.352.39:/usr/lib64/libcuda.so.352.39 -v /lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko:/lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko -v /lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin,:/lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin, --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm --env LD_LIBRARY_PATH=/cbio/shared/software/cudnn/7.0/lib64:/usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib:/usr/local/cuda-7.5/targets/x86_64-linux/lib/:/opt/mpich2/gcc/eth/lib:/opt/gnu/gcc/4.8.1/lib64:/opt/gnu/gcc/4.8.1/lib:/opt/gnu/gmp/lib:/opt/gnu/mpc/lib:/opt/gnu/mpfr/lib:/usr/lib64/ --env CUDA_VISIBLE_DEVICES=0,1 -v /cbio/ski/fuchs/projects/tissue-microarray-resnet/:/mnt/data -v /cbio/ski/fuchs/home/dresdnerg/software/tensorflow/tensorflow/models/image/cifar10:/mnt/code -it gideonitemd/tmrn python cifar10_multi_gpu_train.py --num_gpus 2\n</code></pre>\n<h3>What have you tried?</h3>\n<ol>\n<li>Messing with <code>CUDA_VISIBLE_DEVICES</code>: empty, <code>=0</code>, <code>=0,1</code>. All failed.</li>\n<li>Looking at other issues with the same error, none address this specific problem since it works fine on a single GPU.</li>\n<li><code>nvidia-smi</code> shows no other jobs accessing the GPU</li>\n<li><code>docker ps</code> shows no zombie images</li>\n</ol>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment).</p>", "body_text": "I'm having trouble getting the multiple GPU example to work. Here is the full output:\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                            \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                             \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                                                                \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                                                               \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                                                               \n>> Downloading cifar-10-binary.tar.gz 100.0%                                                                                                                                            \nSuccessfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\n\nEnvironment info\nOperating System: CentOS release 6.7 (Final). Using a docker image available here: gideonitemd/hal-tf.\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\ncudnn/7.0/lib64/libcudnn.so\ncudnn/7.0/lib64/libcudnn.so.7.0.64\ncudnn/7.0/lib64/libcudnn.so.7.0\ncudnn/7.0/lib64/libcudnn_static.a\n/usr/lib/libcuda.so\n/usr/lib/libcuda.so.1\n/usr/lib/libcuda.so.352.39\n(should be libcuda v7.5)\nRunning python -c \"import tensorflow; print(tensorflow.__version__)\" yields 0.8.0\nSteps to reproduce\nRun this (the output of the docker_run_gpu.sh script):\ndocker run -it -v /usr/lib64/libcuda.so:/usr/lib64/libcuda.so -v /usr/lib64/libcuda.so.1:/usr/lib64/libcuda.so.1 -v /usr/lib64/libcuda.so.352.39:/usr/lib64/libcuda.so.352.39 -v /lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko:/lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko -v /lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin,:/lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin, --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm --env LD_LIBRARY_PATH=/cbio/shared/software/cudnn/7.0/lib64:/usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib:/usr/local/cuda-7.5/targets/x86_64-linux/lib/:/opt/mpich2/gcc/eth/lib:/opt/gnu/gcc/4.8.1/lib64:/opt/gnu/gcc/4.8.1/lib:/opt/gnu/gmp/lib:/opt/gnu/mpc/lib:/opt/gnu/mpfr/lib:/usr/lib64/ --env CUDA_VISIBLE_DEVICES=0,1 -v /cbio/ski/fuchs/projects/tissue-microarray-resnet/:/mnt/data -v /cbio/ski/fuchs/home/dresdnerg/software/tensorflow/tensorflow/models/image/cifar10:/mnt/code -it gideonitemd/tmrn python cifar10_multi_gpu_train.py --num_gpus 2\n\nWhat have you tried?\n\nMessing with CUDA_VISIBLE_DEVICES: empty, =0, =0,1. All failed.\nLooking at other issues with the same error, none address this specific problem since it works fine on a single GPU.\nnvidia-smi shows no other jobs accessing the GPU\ndocker ps shows no zombie images\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment).", "body": "I'm having trouble getting the multiple GPU example to work. Here is the full output:\n\n```\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcublas.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                            \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcudnn.so locallyeonitemd/tmrn python cifar10_multi_gpu_train.py --n                                                             \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcufft.so locally                                                                                                                \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcuda.so.1 locally                                                                                                               \nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA library libcurand.so locally                                                                                                               \n>> Downloading cifar-10-binary.tar.gz 100.0%                                                                                                                                            \nSuccessfully downloaded cifar-10-binary.tar.gz 170052171 bytes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nFilling queue with 20000 CIFAR images before starting to train. This will take a few minutes.\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:02:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 1 with properties: \nname: GeForce GTX TITAN X\nmajor: 5 minor: 2 memoryClockRate (GHz) 1.076\npciBusID 0000:03:00.0\nTotal memory: 12.00GiB\nFree memory: 11.87GiB\nI tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 1:   Y Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:755] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX TITAN X, pci bus id: 0000:03:00.0)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\nF tensorflow/stream_executor/cuda/cuda_driver.cc:383] Check failed: CUDA_SUCCESS == dynload::cuCtxSetCurrent(context) (0 vs. 216)\n```\n### Environment info\n\nOperating System: CentOS release 6.7 (Final). Using a docker image available here: [gideonitemd/hal-tf](https://hub.docker.com/r/gideonitemd/hal-tf/).\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n\ncudnn/7.0/lib64/libcudnn.so\ncudnn/7.0/lib64/libcudnn.so.7.0.64  \ncudnn/7.0/lib64/libcudnn.so.7.0\ncudnn/7.0/lib64/libcudnn_static.a  \n\n/usr/lib/libcuda.so \n/usr/lib/libcuda.so.1 \n/usr/lib/libcuda.so.352.39\n\n(should be libcuda v7.5)\n\nRunning `python -c \"import tensorflow; print(tensorflow.__version__)\"` yields 0.8.0\n### Steps to reproduce\n\nRun this (the output of the `docker_run_gpu.sh` script):\n\n```\ndocker run -it -v /usr/lib64/libcuda.so:/usr/lib64/libcuda.so -v /usr/lib64/libcuda.so.1:/usr/lib64/libcuda.so.1 -v /usr/lib64/libcuda.so.352.39:/usr/lib64/libcuda.so.352.39 -v /lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko:/lib/modules/2.6.32-573.7.1.el6.x86_64/kernel/drivers/video/nvidia.ko -v /lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin,:/lib/modules/2.6.32-573.7.1.el6.x86_64/modules.dep.bin, --device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 --device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm --env LD_LIBRARY_PATH=/cbio/shared/software/cudnn/7.0/lib64:/usr/local/cuda-7.5/lib64:/usr/local/cuda-7.5/lib:/usr/local/cuda-7.5/targets/x86_64-linux/lib/:/opt/mpich2/gcc/eth/lib:/opt/gnu/gcc/4.8.1/lib64:/opt/gnu/gcc/4.8.1/lib:/opt/gnu/gmp/lib:/opt/gnu/mpc/lib:/opt/gnu/mpfr/lib:/usr/lib64/ --env CUDA_VISIBLE_DEVICES=0,1 -v /cbio/ski/fuchs/projects/tissue-microarray-resnet/:/mnt/data -v /cbio/ski/fuchs/home/dresdnerg/software/tensorflow/tensorflow/models/image/cifar10:/mnt/code -it gideonitemd/tmrn python cifar10_multi_gpu_train.py --num_gpus 2\n```\n### What have you tried?\n1. Messing with `CUDA_VISIBLE_DEVICES`: empty, `=0`, `=0,1`. All failed.\n2. Looking at other issues with the same error, none address this specific problem since it works fine on a single GPU.\n3. `nvidia-smi` shows no other jobs accessing the GPU\n4. `docker ps` shows no zombie images\n### Logs or other output that would be helpful\n\n(If logs are large, please upload as attachment).\n"}