{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335062783", "html_url": "https://github.com/tensorflow/tensorflow/issues/10213#issuecomment-335062783", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10213", "id": 335062783, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTA2Mjc4Mw==", "user": {"login": "adil25", "id": 22212483, "node_id": "MDQ6VXNlcjIyMjEyNDgz", "avatar_url": "https://avatars3.githubusercontent.com/u/22212483?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adil25", "html_url": "https://github.com/adil25", "followers_url": "https://api.github.com/users/adil25/followers", "following_url": "https://api.github.com/users/adil25/following{/other_user}", "gists_url": "https://api.github.com/users/adil25/gists{/gist_id}", "starred_url": "https://api.github.com/users/adil25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adil25/subscriptions", "organizations_url": "https://api.github.com/users/adil25/orgs", "repos_url": "https://api.github.com/users/adil25/repos", "events_url": "https://api.github.com/users/adil25/events{/privacy}", "received_events_url": "https://api.github.com/users/adil25/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-09T03:55:35Z", "updated_at": "2017-10-09T03:55:35Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16261331\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zhedongzheng\">@zhedongzheng</a> ...Thank you for your reply, please after making the changes i got another error which i mentioning here below. but before that please forgive if you feel something nonsense about me skills as i am very new to python and tensorflow...here is the error and code,<br>\nimport numpy as np<br>\nimport tensorflow as tf<br>\nimport tensorflow.contrib.slim as slim</p>\n<p>class DRQN():<br>\ndef <strong>init</strong>(self, im_h, im_w, k, n_actions, scope, learning_rate,<br>\ntest=False, use_game_features=False, learn_q=True, recurrent=True):<br>\nself.learning_rate = learning_rate<br>\nself.im_h, self.im_w, self.k = im_h, im_w, k<br>\nself.scope, self.n_actions = scope, n_actions<br>\n# self.batch_size = tf.placeholder(tf.int32, name='batch_size')<br>\nself.batch_size=tf.shape(x)[0]<br>\nself.sequence_length = tf.placeholder(tf.int32, name='sequence_length')<br>\nself.use_game_features = use_game_features<br>\nself.learn_q = learn_q<br>\nself.recurrent = recurrent</p>\n<pre><code>    # Dropout probability\n    self.dropout_p = tf.placeholder(tf.float32, name='dropout_p')\n\n    self.images = tf.placeholder(tf.float32, name='images',\n                                 shape=[None, None, im_h, im_w, 3])\n    # we'll merge all sequences in one single batch for treatment\n    # but all outputs will be reshaped to [batch_size, length, ...]\n    self.all_images = tf.reshape(self.images,\n                                 [self.batch_size*self.sequence_length,\n                                  im_h, im_w, 3])\n\n    self._init_conv_layers()\n    self._init_game_features_output()\n    if recurrent:\n        self._init_recurrent_part()\n    else:\n        self._init_dqn_output()\n    if not test:\n        self._define_loss()\n\ndef _init_conv_layers(self):\n    # First convolution from screen buffer\n    self.conv1 = slim.conv2d(\n        self.all_images, num_outputs=32,\n        kernel_size=[8, 8], stride=[4, 4], padding='VALID',\n        scope=self.scope+'_conv1'\n    )\n\n    # Second convolution layer\n    self.conv2 = slim.conv2d(\n        self.conv1, num_outputs=64,\n        kernel_size=[4, 4], stride=[2, 2], padding='VALID',\n        scope=self.scope+'_conv2'\n    )\n\ndef _init_game_features_output(self):\n    self.layer4 = tf.nn.dropout(\n        slim.fully_connected(slim.flatten(self.conv2), 512,\n                             scope=self.scope+'_l4'),\n        self.dropout_p,\n    )\n    self.flat_game_features = slim.fully_connected(self.layer4, self.k,\n                                                   scope=self.scope+'_l4.5',\n                                                   activation_fn=None)\n\n    # Output layer\n    self.game_features = tf.reshape(self.flat_game_features,\n                                    shape=[self.batch_size, self.sequence_length, self.k])\n\n    # Observed game features\n    self.game_features_in = tf.placeholder(tf.float32,\n                                           name='game_features_in',\n                                           shape=[None, None, self.k])\n\n    # Difference between observed and predicted game features\n    delta = self.game_features - self.game_features_in\n    # delta = tf.Print(delta, [delta], summarize=10, name=\"dFeatures\")\n\n    # Optimize on RMS of this difference\n    self.features_loss = tf.reduce_mean(tf.square(delta))\n\ndef _init_dqn_output(self):\n    self.layer3 = tf.nn.dropout(\n        tf.reshape(slim.flatten(self.conv2),\n                   [self.batch_size, self.sequence_length, 4608]),\n        self.dropout_p,\n    )\n    self.layer3_5 = slim.fully_connected(self.layer3, 512,\n            scope=self.scope+\"_layer3_5\")\n    Q = slim.fully_connected(\n        self.layer3_5, self.n_actions, scope=self.scope+'_actions',\n        activation_fn=None)\n    self.Q = tf.reshape(Q, [self.batch_size, self.sequence_length,\n                            self.n_actions])\n    self.choice = tf.argmax(self.Q, 2)\n    self.max_Q = tf.reduce_max(self.Q, 2)\n\ndef _init_recurrent_part(self):\n    # Flat fully connected layer (Layer3' in the paper)\n    self.h_size = 300\n    self.layer3 = tf.nn.dropout(\n        tf.reshape(slim.flatten(self.conv2),\n                   [self.batch_size, self.sequence_length, 4608]),\n        self.dropout_p,\n    )\n\n    # LSTM cell\n    self.cell = tf.nn.rnn_cell.LSTMCell(self.h_size)\n    self.state_in = self.cell.zero_state(self.batch_size, tf.float32)\n\n    # Recurrence\n    rnn_output, self.state_out = tf.nn.dynamic_rnn(\n            self.cell,\n            self.layer3,\n            initial_state=self.state_in,\n            dtype=tf.float32,\n            scope=self.scope+'_RNN/')\n\n    self.rnn_output = tf.reshape(rnn_output, [-1, self.h_size])\n\n    # Q-estimator for actions\n    Q = slim.fully_connected(\n        self.rnn_output, self.n_actions, scope=self.scope+'_actions',\n        activation_fn=None)\n    self.Q = tf.reshape(Q, [self.batch_size, self.sequence_length,\n                            self.n_actions])\n    self.choice = tf.argmax(self.Q, 2)\n    self.max_Q = tf.reduce_max(self.Q, 2)\n\ndef _define_loss(self):\n    self.gamma = tf.placeholder(tf.float32, name='gamma')\n    self.target_q = tf.placeholder(tf.float32, name='target_q',\n                                   shape=[None, None])\n    self.rewards = tf.placeholder(tf.float32, name='rewards',\n                                  shape=[None, None])\n    self.actions = tf.placeholder(tf.float32, name='actions',\n                                  shape=[None, None, self.n_actions])\n    y = self.rewards + self.gamma * self.target_q\n    Qas = tf.reduce_sum(tf.one_hot(tf.argmax(self.actions, 2), \n                                   self.n_actions) * self.Q, 2)\n\n    self.ignore_up_to = tf.placeholder(tf.int32, name='ignore_up_to')\n    y = tf.slice(y, [0, self.ignore_up_to], [-1, -1])\n    Qas = tf.slice(Qas, [0, self.ignore_up_to], [-1, -1])\n    self.q_loss = tf.reduce_mean(tf.square(y-Qas))\n\n    if self.use_game_features:\n        if self.learn_q:\n            print(\"Learn Q and Game Features\")\n            self.loss = self.q_loss + self.features_loss\n        else:\n            print(\"Learn Game Features only\")\n            self.loss = self.features_loss\n    elif self.learn_q:\n        print(\"Learn Q only\")\n        self.loss = self.q_loss\n    self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n\ndef feed_lstm(self, sess, screens, actions, rewards):\n    assert screens.shape[:2] == actions.shape[:2]\n    assert screens.shape[:2] == rewards.shape[:2]\n    batch_size, sequence_length = screens.shape[:2]\n\n    actions, state = sess.run([self.choice, self.state_out], feed_dict={\n        self.batch_size: batch_size,\n        self.sequence_length: sequence_length,\n        self.images: screens,\n        self.state_in: self.rnn_state,\n        self.dropout_p: 0.75,\n    })\n\n    self.last_state = sess.run(self.state_out, feed_dict={\n        self.batch_size: batch_size,\n        self.sequence_length: sequence_length,\n        self.images: screens,\n        self.state_in: self.rnn_state,\n        self.dropout_p: 0.75,\n    })\n\ndef choose(self, sess, epsilon, screenbuf, dropout_p, state_in):\n    \"\"\"Choose an action based on the current screen buffer\"\"\"\n    is_random = np.random.rand() &lt;= epsilon\n    to_get = [self.Q] if not self.recurrent else [self.state_out]\n    if not is_random:\n        to_get += [self.choice]\n    feed_dict={\n        self.batch_size: 1,\n        self.sequence_length: 1,\n        self.images: [[screenbuf]],\n        self.dropout_p: dropout_p,\n    }\n    if self.recurrent:\n        feed_dict[self.state_in] = state_in\n    r = sess.run(to_get, feed_dict)\n    res = (np.random.randint(self.n_actions), r[0])\n    if not is_random:\n        res = (r[1][0][0], r[0])\n    return res\n</code></pre>\n<p>Error:</p>\n<p>Connected to pydev debugger (build 163.15188.4)<br>\nBuilding main DRQN<br>\nTraceback (most recent call last):<br>\nFile \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/pydevd.py\", line 1596, in <br>\nglobals = debugger.run(setup['file'], None, None, is_module)<br>\nFile \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/pydevd.py\", line 974, in run<br>\npydev_imports.execfile(file, globals, locals)  # execute the script<br>\nFile \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile<br>\nexec(compile(contents+\"\\n\", file, 'exec'), glob, loc)<br>\nFile \"/home/adil/Documents/deepdoom-master/src/agent.py\", line 28, in <br>\nrecurrent=USE_RECURRENCE)<br>\nFile \"/home/adil/Documents/deepdoom-master/src/network.py\", line 13, in <strong>init</strong><br>\nself.batch_size=tf.shape(x)[0]<br>\nNameError: name 'x' is not defined</p>", "body_text": "@zhedongzheng ...Thank you for your reply, please after making the changes i got another error which i mentioning here below. but before that please forgive if you feel something nonsense about me skills as i am very new to python and tensorflow...here is the error and code,\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nclass DRQN():\ndef init(self, im_h, im_w, k, n_actions, scope, learning_rate,\ntest=False, use_game_features=False, learn_q=True, recurrent=True):\nself.learning_rate = learning_rate\nself.im_h, self.im_w, self.k = im_h, im_w, k\nself.scope, self.n_actions = scope, n_actions\n# self.batch_size = tf.placeholder(tf.int32, name='batch_size')\nself.batch_size=tf.shape(x)[0]\nself.sequence_length = tf.placeholder(tf.int32, name='sequence_length')\nself.use_game_features = use_game_features\nself.learn_q = learn_q\nself.recurrent = recurrent\n    # Dropout probability\n    self.dropout_p = tf.placeholder(tf.float32, name='dropout_p')\n\n    self.images = tf.placeholder(tf.float32, name='images',\n                                 shape=[None, None, im_h, im_w, 3])\n    # we'll merge all sequences in one single batch for treatment\n    # but all outputs will be reshaped to [batch_size, length, ...]\n    self.all_images = tf.reshape(self.images,\n                                 [self.batch_size*self.sequence_length,\n                                  im_h, im_w, 3])\n\n    self._init_conv_layers()\n    self._init_game_features_output()\n    if recurrent:\n        self._init_recurrent_part()\n    else:\n        self._init_dqn_output()\n    if not test:\n        self._define_loss()\n\ndef _init_conv_layers(self):\n    # First convolution from screen buffer\n    self.conv1 = slim.conv2d(\n        self.all_images, num_outputs=32,\n        kernel_size=[8, 8], stride=[4, 4], padding='VALID',\n        scope=self.scope+'_conv1'\n    )\n\n    # Second convolution layer\n    self.conv2 = slim.conv2d(\n        self.conv1, num_outputs=64,\n        kernel_size=[4, 4], stride=[2, 2], padding='VALID',\n        scope=self.scope+'_conv2'\n    )\n\ndef _init_game_features_output(self):\n    self.layer4 = tf.nn.dropout(\n        slim.fully_connected(slim.flatten(self.conv2), 512,\n                             scope=self.scope+'_l4'),\n        self.dropout_p,\n    )\n    self.flat_game_features = slim.fully_connected(self.layer4, self.k,\n                                                   scope=self.scope+'_l4.5',\n                                                   activation_fn=None)\n\n    # Output layer\n    self.game_features = tf.reshape(self.flat_game_features,\n                                    shape=[self.batch_size, self.sequence_length, self.k])\n\n    # Observed game features\n    self.game_features_in = tf.placeholder(tf.float32,\n                                           name='game_features_in',\n                                           shape=[None, None, self.k])\n\n    # Difference between observed and predicted game features\n    delta = self.game_features - self.game_features_in\n    # delta = tf.Print(delta, [delta], summarize=10, name=\"dFeatures\")\n\n    # Optimize on RMS of this difference\n    self.features_loss = tf.reduce_mean(tf.square(delta))\n\ndef _init_dqn_output(self):\n    self.layer3 = tf.nn.dropout(\n        tf.reshape(slim.flatten(self.conv2),\n                   [self.batch_size, self.sequence_length, 4608]),\n        self.dropout_p,\n    )\n    self.layer3_5 = slim.fully_connected(self.layer3, 512,\n            scope=self.scope+\"_layer3_5\")\n    Q = slim.fully_connected(\n        self.layer3_5, self.n_actions, scope=self.scope+'_actions',\n        activation_fn=None)\n    self.Q = tf.reshape(Q, [self.batch_size, self.sequence_length,\n                            self.n_actions])\n    self.choice = tf.argmax(self.Q, 2)\n    self.max_Q = tf.reduce_max(self.Q, 2)\n\ndef _init_recurrent_part(self):\n    # Flat fully connected layer (Layer3' in the paper)\n    self.h_size = 300\n    self.layer3 = tf.nn.dropout(\n        tf.reshape(slim.flatten(self.conv2),\n                   [self.batch_size, self.sequence_length, 4608]),\n        self.dropout_p,\n    )\n\n    # LSTM cell\n    self.cell = tf.nn.rnn_cell.LSTMCell(self.h_size)\n    self.state_in = self.cell.zero_state(self.batch_size, tf.float32)\n\n    # Recurrence\n    rnn_output, self.state_out = tf.nn.dynamic_rnn(\n            self.cell,\n            self.layer3,\n            initial_state=self.state_in,\n            dtype=tf.float32,\n            scope=self.scope+'_RNN/')\n\n    self.rnn_output = tf.reshape(rnn_output, [-1, self.h_size])\n\n    # Q-estimator for actions\n    Q = slim.fully_connected(\n        self.rnn_output, self.n_actions, scope=self.scope+'_actions',\n        activation_fn=None)\n    self.Q = tf.reshape(Q, [self.batch_size, self.sequence_length,\n                            self.n_actions])\n    self.choice = tf.argmax(self.Q, 2)\n    self.max_Q = tf.reduce_max(self.Q, 2)\n\ndef _define_loss(self):\n    self.gamma = tf.placeholder(tf.float32, name='gamma')\n    self.target_q = tf.placeholder(tf.float32, name='target_q',\n                                   shape=[None, None])\n    self.rewards = tf.placeholder(tf.float32, name='rewards',\n                                  shape=[None, None])\n    self.actions = tf.placeholder(tf.float32, name='actions',\n                                  shape=[None, None, self.n_actions])\n    y = self.rewards + self.gamma * self.target_q\n    Qas = tf.reduce_sum(tf.one_hot(tf.argmax(self.actions, 2), \n                                   self.n_actions) * self.Q, 2)\n\n    self.ignore_up_to = tf.placeholder(tf.int32, name='ignore_up_to')\n    y = tf.slice(y, [0, self.ignore_up_to], [-1, -1])\n    Qas = tf.slice(Qas, [0, self.ignore_up_to], [-1, -1])\n    self.q_loss = tf.reduce_mean(tf.square(y-Qas))\n\n    if self.use_game_features:\n        if self.learn_q:\n            print(\"Learn Q and Game Features\")\n            self.loss = self.q_loss + self.features_loss\n        else:\n            print(\"Learn Game Features only\")\n            self.loss = self.features_loss\n    elif self.learn_q:\n        print(\"Learn Q only\")\n        self.loss = self.q_loss\n    self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\n\ndef feed_lstm(self, sess, screens, actions, rewards):\n    assert screens.shape[:2] == actions.shape[:2]\n    assert screens.shape[:2] == rewards.shape[:2]\n    batch_size, sequence_length = screens.shape[:2]\n\n    actions, state = sess.run([self.choice, self.state_out], feed_dict={\n        self.batch_size: batch_size,\n        self.sequence_length: sequence_length,\n        self.images: screens,\n        self.state_in: self.rnn_state,\n        self.dropout_p: 0.75,\n    })\n\n    self.last_state = sess.run(self.state_out, feed_dict={\n        self.batch_size: batch_size,\n        self.sequence_length: sequence_length,\n        self.images: screens,\n        self.state_in: self.rnn_state,\n        self.dropout_p: 0.75,\n    })\n\ndef choose(self, sess, epsilon, screenbuf, dropout_p, state_in):\n    \"\"\"Choose an action based on the current screen buffer\"\"\"\n    is_random = np.random.rand() <= epsilon\n    to_get = [self.Q] if not self.recurrent else [self.state_out]\n    if not is_random:\n        to_get += [self.choice]\n    feed_dict={\n        self.batch_size: 1,\n        self.sequence_length: 1,\n        self.images: [[screenbuf]],\n        self.dropout_p: dropout_p,\n    }\n    if self.recurrent:\n        feed_dict[self.state_in] = state_in\n    r = sess.run(to_get, feed_dict)\n    res = (np.random.randint(self.n_actions), r[0])\n    if not is_random:\n        res = (r[1][0][0], r[0])\n    return res\n\nError:\nConnected to pydev debugger (build 163.15188.4)\nBuilding main DRQN\nTraceback (most recent call last):\nFile \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/pydevd.py\", line 1596, in \nglobals = debugger.run(setup['file'], None, None, is_module)\nFile \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/pydevd.py\", line 974, in run\npydev_imports.execfile(file, globals, locals)  # execute the script\nFile \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\nexec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\nFile \"/home/adil/Documents/deepdoom-master/src/agent.py\", line 28, in \nrecurrent=USE_RECURRENCE)\nFile \"/home/adil/Documents/deepdoom-master/src/network.py\", line 13, in init\nself.batch_size=tf.shape(x)[0]\nNameError: name 'x' is not defined", "body": "@zhedongzheng ...Thank you for your reply, please after making the changes i got another error which i mentioning here below. but before that please forgive if you feel something nonsense about me skills as i am very new to python and tensorflow...here is the error and code,\r\nimport numpy as np\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\n\r\n\r\nclass DRQN():\r\n    def __init__(self, im_h, im_w, k, n_actions, scope, learning_rate, \r\n            test=False, use_game_features=False, learn_q=True, recurrent=True):\r\n        self.learning_rate = learning_rate\r\n        self.im_h, self.im_w, self.k = im_h, im_w, k\r\n        self.scope, self.n_actions = scope, n_actions\r\n      # self.batch_size = tf.placeholder(tf.int32, name='batch_size')\r\n        self.batch_size=tf.shape(x)[0]\r\n        self.sequence_length = tf.placeholder(tf.int32, name='sequence_length')\r\n        self.use_game_features = use_game_features\r\n        self.learn_q = learn_q\r\n        self.recurrent = recurrent\r\n\r\n        # Dropout probability\r\n        self.dropout_p = tf.placeholder(tf.float32, name='dropout_p')\r\n\r\n        self.images = tf.placeholder(tf.float32, name='images',\r\n                                     shape=[None, None, im_h, im_w, 3])\r\n        # we'll merge all sequences in one single batch for treatment\r\n        # but all outputs will be reshaped to [batch_size, length, ...]\r\n        self.all_images = tf.reshape(self.images,\r\n                                     [self.batch_size*self.sequence_length,\r\n                                      im_h, im_w, 3])\r\n\r\n        self._init_conv_layers()\r\n        self._init_game_features_output()\r\n        if recurrent:\r\n            self._init_recurrent_part()\r\n        else:\r\n            self._init_dqn_output()\r\n        if not test:\r\n            self._define_loss()\r\n\r\n    def _init_conv_layers(self):\r\n        # First convolution from screen buffer\r\n        self.conv1 = slim.conv2d(\r\n            self.all_images, num_outputs=32,\r\n            kernel_size=[8, 8], stride=[4, 4], padding='VALID',\r\n            scope=self.scope+'_conv1'\r\n        )\r\n\r\n        # Second convolution layer\r\n        self.conv2 = slim.conv2d(\r\n            self.conv1, num_outputs=64,\r\n            kernel_size=[4, 4], stride=[2, 2], padding='VALID',\r\n            scope=self.scope+'_conv2'\r\n        )\r\n\r\n    def _init_game_features_output(self):\r\n        self.layer4 = tf.nn.dropout(\r\n            slim.fully_connected(slim.flatten(self.conv2), 512,\r\n                                 scope=self.scope+'_l4'),\r\n            self.dropout_p,\r\n        )\r\n        self.flat_game_features = slim.fully_connected(self.layer4, self.k,\r\n                                                       scope=self.scope+'_l4.5',\r\n                                                       activation_fn=None)\r\n\r\n        # Output layer\r\n        self.game_features = tf.reshape(self.flat_game_features,\r\n                                        shape=[self.batch_size, self.sequence_length, self.k])\r\n\r\n        # Observed game features\r\n        self.game_features_in = tf.placeholder(tf.float32,\r\n                                               name='game_features_in',\r\n                                               shape=[None, None, self.k])\r\n\r\n        # Difference between observed and predicted game features\r\n        delta = self.game_features - self.game_features_in\r\n        # delta = tf.Print(delta, [delta], summarize=10, name=\"dFeatures\")\r\n\r\n        # Optimize on RMS of this difference\r\n        self.features_loss = tf.reduce_mean(tf.square(delta))\r\n\r\n    def _init_dqn_output(self):\r\n        self.layer3 = tf.nn.dropout(\r\n            tf.reshape(slim.flatten(self.conv2),\r\n                       [self.batch_size, self.sequence_length, 4608]),\r\n            self.dropout_p,\r\n        )\r\n        self.layer3_5 = slim.fully_connected(self.layer3, 512,\r\n                scope=self.scope+\"_layer3_5\")\r\n        Q = slim.fully_connected(\r\n            self.layer3_5, self.n_actions, scope=self.scope+'_actions',\r\n            activation_fn=None)\r\n        self.Q = tf.reshape(Q, [self.batch_size, self.sequence_length,\r\n                                self.n_actions])\r\n        self.choice = tf.argmax(self.Q, 2)\r\n        self.max_Q = tf.reduce_max(self.Q, 2)\r\n\r\n    def _init_recurrent_part(self):\r\n        # Flat fully connected layer (Layer3' in the paper)\r\n        self.h_size = 300\r\n        self.layer3 = tf.nn.dropout(\r\n            tf.reshape(slim.flatten(self.conv2),\r\n                       [self.batch_size, self.sequence_length, 4608]),\r\n            self.dropout_p,\r\n        )\r\n\r\n        # LSTM cell\r\n        self.cell = tf.nn.rnn_cell.LSTMCell(self.h_size)\r\n        self.state_in = self.cell.zero_state(self.batch_size, tf.float32)\r\n\r\n        # Recurrence\r\n        rnn_output, self.state_out = tf.nn.dynamic_rnn(\r\n                self.cell,\r\n                self.layer3,\r\n                initial_state=self.state_in,\r\n                dtype=tf.float32,\r\n                scope=self.scope+'_RNN/')\r\n\r\n        self.rnn_output = tf.reshape(rnn_output, [-1, self.h_size])\r\n\r\n        # Q-estimator for actions\r\n        Q = slim.fully_connected(\r\n            self.rnn_output, self.n_actions, scope=self.scope+'_actions',\r\n            activation_fn=None)\r\n        self.Q = tf.reshape(Q, [self.batch_size, self.sequence_length,\r\n                                self.n_actions])\r\n        self.choice = tf.argmax(self.Q, 2)\r\n        self.max_Q = tf.reduce_max(self.Q, 2)\r\n\r\n    def _define_loss(self):\r\n        self.gamma = tf.placeholder(tf.float32, name='gamma')\r\n        self.target_q = tf.placeholder(tf.float32, name='target_q',\r\n                                       shape=[None, None])\r\n        self.rewards = tf.placeholder(tf.float32, name='rewards',\r\n                                      shape=[None, None])\r\n        self.actions = tf.placeholder(tf.float32, name='actions',\r\n                                      shape=[None, None, self.n_actions])\r\n        y = self.rewards + self.gamma * self.target_q\r\n        Qas = tf.reduce_sum(tf.one_hot(tf.argmax(self.actions, 2), \r\n                                       self.n_actions) * self.Q, 2)\r\n\r\n        self.ignore_up_to = tf.placeholder(tf.int32, name='ignore_up_to')\r\n        y = tf.slice(y, [0, self.ignore_up_to], [-1, -1])\r\n        Qas = tf.slice(Qas, [0, self.ignore_up_to], [-1, -1])\r\n        self.q_loss = tf.reduce_mean(tf.square(y-Qas))\r\n\r\n        if self.use_game_features:\r\n            if self.learn_q:\r\n                print(\"Learn Q and Game Features\")\r\n                self.loss = self.q_loss + self.features_loss\r\n            else:\r\n                print(\"Learn Game Features only\")\r\n                self.loss = self.features_loss\r\n        elif self.learn_q:\r\n            print(\"Learn Q only\")\r\n            self.loss = self.q_loss\r\n        self.train_step = tf.train.RMSPropOptimizer(self.learning_rate).minimize(self.loss)\r\n\r\n    def feed_lstm(self, sess, screens, actions, rewards):\r\n        assert screens.shape[:2] == actions.shape[:2]\r\n        assert screens.shape[:2] == rewards.shape[:2]\r\n        batch_size, sequence_length = screens.shape[:2]\r\n\r\n        actions, state = sess.run([self.choice, self.state_out], feed_dict={\r\n            self.batch_size: batch_size,\r\n            self.sequence_length: sequence_length,\r\n            self.images: screens,\r\n            self.state_in: self.rnn_state,\r\n            self.dropout_p: 0.75,\r\n        })\r\n\r\n        self.last_state = sess.run(self.state_out, feed_dict={\r\n            self.batch_size: batch_size,\r\n            self.sequence_length: sequence_length,\r\n            self.images: screens,\r\n            self.state_in: self.rnn_state,\r\n            self.dropout_p: 0.75,\r\n        })\r\n\r\n    def choose(self, sess, epsilon, screenbuf, dropout_p, state_in):\r\n        \"\"\"Choose an action based on the current screen buffer\"\"\"\r\n        is_random = np.random.rand() <= epsilon\r\n        to_get = [self.Q] if not self.recurrent else [self.state_out]\r\n        if not is_random:\r\n            to_get += [self.choice]\r\n        feed_dict={\r\n            self.batch_size: 1,\r\n            self.sequence_length: 1,\r\n            self.images: [[screenbuf]],\r\n            self.dropout_p: dropout_p,\r\n        }\r\n        if self.recurrent:\r\n            feed_dict[self.state_in] = state_in\r\n        r = sess.run(to_get, feed_dict)\r\n        res = (np.random.randint(self.n_actions), r[0])\r\n        if not is_random:\r\n            res = (r[1][0][0], r[0])\r\n        return res\r\n\r\n\r\nError:\r\n\r\nConnected to pydev debugger (build 163.15188.4)\r\nBuilding main DRQN\r\nTraceback (most recent call last):\r\n  File \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/pydevd.py\", line 1596, in <module>\r\n    globals = debugger.run(setup['file'], None, None, is_module)\r\n  File \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/pydevd.py\", line 974, in run\r\n    pydev_imports.execfile(file, globals, locals)  # execute the script\r\n  File \"/home/adil/Desktop/pycharm-2016.3.3/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\r\n    exec(compile(contents+\"\\n\", file, 'exec'), glob, loc)\r\n  File \"/home/adil/Documents/deepdoom-master/src/agent.py\", line 28, in <module>\r\n    recurrent=USE_RECURRENCE)\r\n  File \"/home/adil/Documents/deepdoom-master/src/network.py\", line 13, in __init__\r\n    self.batch_size=tf.shape(x)[0]\r\nNameError: name 'x' is not defined\r\n"}