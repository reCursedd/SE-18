{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335008819", "html_url": "https://github.com/tensorflow/tensorflow/issues/10213#issuecomment-335008819", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10213", "id": 335008819, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTAwODgxOQ==", "user": {"login": "adil25", "id": 22212483, "node_id": "MDQ6VXNlcjIyMjEyNDgz", "avatar_url": "https://avatars3.githubusercontent.com/u/22212483?v=4", "gravatar_id": "", "url": "https://api.github.com/users/adil25", "html_url": "https://github.com/adil25", "followers_url": "https://api.github.com/users/adil25/followers", "following_url": "https://api.github.com/users/adil25/following{/other_user}", "gists_url": "https://api.github.com/users/adil25/gists{/gist_id}", "starred_url": "https://api.github.com/users/adil25/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/adil25/subscriptions", "organizations_url": "https://api.github.com/users/adil25/orgs", "repos_url": "https://api.github.com/users/adil25/repos", "events_url": "https://api.github.com/users/adil25/events{/privacy}", "received_events_url": "https://api.github.com/users/adil25/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-08T14:04:46Z", "updated_at": "2017-10-08T14:04:46Z", "author_association": "NONE", "body_html": "<div class=\"email-fragment\">Thank you for your reply, please i am new to tensorflow and i am not sure where to make your suggested change, please could you tell me where to make this change, i am sharing my code below.....\nimport random\nimport vizdoom as vd\nimport tensorflow as tf\nimport numpy as np\nimport scipy.ndimage as Simg\n\nfrom basic_ennemy_pos import basic_ennemy_x\nfrom network import tf, DRQN\nfrom video import VideoWriter\nfrom memory import ReplayMemory\nfrom config import (\n    N_ACTIONS, LEARNING_RATE, MIN_MEM_SIZE, MAX_MEM_SIZE,\n    MAX_CPUS, TRAINING_STEPS, BATCH_SIZE, SEQUENCE_LENGTH,\n    QLEARNING_STEPS, MAX_EPISODE_LENGTH, DEATH_PENALTY,\n    KILL_REWARD, PICKUP_REWARD, GREEDY_STEPS, IGNORE_UP_TO,\n    BACKPROP_STEPS, USE_GAME_FEATURES, LEARN_Q, USE_RECURRENCE,\n)\n\n# Config variables\nim_w, im_h = 108, 60\nN_FEATURES = 1\nACTION_SET = np.eye(N_ACTIONS, dtype=np.uint32).tolist()\nSECTION_SEPARATOR = \"------------\"\n\n# Neural nets and tools\nprint('Building main DRQN')\nmain = DRQN(im_h, im_w, N_FEATURES, N_ACTIONS, 'main', LEARNING_RATE,\n            use_game_features=USE_GAME_FEATURES, learn_q=LEARN_Q,\n            recurrent=USE_RECURRENCE)\nprint('Building target DRQN')\ntarget = DRQN(im_h, im_w, N_FEATURES, N_ACTIONS, 'target', LEARNING_RATE, True,\n        recurrent=USE_RECURRENCE)\nsaver = tf.train.Saver()\nmem = ReplayMemory(MIN_MEM_SIZE, MAX_MEM_SIZE)\n\n\ndef csv_output(*columns):\n    def wrapper(func):\n        def inner(*args, **kwargs):\n            print(\"---------\")\n            print(\"::\", func.__name__, \"::\")\n            print(\",\".join(columns))\n            return func(*args, **kwargs)\n        return inner\n    return wrapper\n\n\ndef create_game():\n    game = vd.DoomGame()\n    game.load_config(\"basic.cfg\")\n\n    # Ennemy detection\n    walls = None  # map_parser.parse(\"maps/deathmatch.txt\")\n    game.clear_available_game_variables()\n    game.add_available_game_variable(vd.GameVariable.POSITION_X)  # 0\n    game.add_available_game_variable(vd.GameVariable.POSITION_Y)  # 1\n    game.add_available_game_variable(vd.GameVariable.POSITION_Z)  # 2\n\n    game.add_available_game_variable(vd.GameVariable.KILLCOUNT)   # 3\n    game.add_available_game_variable(vd.GameVariable.DEATHCOUNT)  # 4\n    game.add_available_game_variable(vd.GameVariable.ITEMCOUNT)   # 5\n\n    game.set_labels_buffer_enabled(True)\n\n    game.init()\n    return game, walls\n\n\ndef reward_reshape(dump):\n    is_dead = len(dump) &lt; MAX_EPISODE_LENGTH\n    reward = [frame[2] for frame in dump]\n    kills = [frame[4] for frame in dump]\n    items = [frame[5] for frame in dump]\n    kill_diff = [0] + [(kills[i] - kills[i - 1]) * KILL_REWARD for i in range(1, len(kills))]\n    item_diff = [0] + [(items[i] - items[i - 1]) * PICKUP_REWARD for i in range(1, len(items))]\n\n    reshaped_reward = [r + k + i for r, k, i in zip(reward, kill_diff, item_diff)]\n\n    if is_dead:\n        reshaped_reward[-1] -= DEATH_PENALTY\n\n    return [\n        (buffer, action, r_reward, game_features)\n        for (buffer, action, _, game_features, _, _), r_reward\n        in zip(dump, reshaped_reward)\n    ]\n\n\ndef play_random_episode(game, walls, verbose=False, skip=1):\n    game.new_episode()\n    dump = []\n    zoomed = np.zeros((MAX_EPISODE_LENGTH, im_h, im_w, 3), dtype=np.uint8)\n    action = ACTION_SET[0]\n    while not game.is_episode_finished():\n        # Get screen buf\n        state = game.get_state()\n        S = state.screen_buffer  # NOQA\n\n        # Resample to our network size\n        h, w = S.shape[:2]\n        Simg.zoom(S, [1. * im_h / h, 1. * im_w / w, 1],\n                  output=zoomed[len(dump)], order=0)\n        S = zoomed[len(dump)]  # NOQA\n\n        # Get game features an action\n        game_features = [basic_ennemy_x(state)]\n        action = random.choice(ACTION_SET)\n        reward = game.make_action(action, skip)\n        dump.append((S, action, reward, game_features))\n    return dump\n\n\ndef wrap_play_random_episode(i=0):\n    try:\n        game, walls = create_game()\n        res = play_random_episode(game, walls, skip=4)\n        game.close()\n        return res\n    except vd.vizdoom.ViZDoomErrorException:\n        print(\"ViZDoom ERROR\")\n        return []\n\n# Need to be imported and created after wrap_play_random_episode\nfrom multiprocessing import Pool, cpu_count\nN_CORES = min(cpu_count(), MAX_CPUS)\nif N_CORES &gt; 1:\n    workers = Pool(N_CORES)\n\n\ndef multiplay():\n    if N_CORES &lt;= 1:\n        return [wrap_play_random_episode()]\n    else:\n        return workers.map(wrap_play_random_episode, range(N_CORES))\n\n\ndef update_target(sess):\n    \"\"\"Transfer learned parameters from main to target NN\"\"\"\n    v = tf.trainable_variables()\n    main_vars = filter(lambda x: x.name.startswith('main'), v)\n    target_vars = filter(lambda x: x.name.startswith('target'), v)\n    for t, m in zip(target_vars, main_vars):\n        sess.run(t.assign(m.value()))\n\n\ndef init_phase(sess):\n    \"\"\"\n    Attempt to restore a model, or initialize all variables.\n    Then fills replay memory with random-action games\n    \"\"\"\n    try:\n        saver = tf.train.import_meta_graph('model.ckpt.meta')\n        saver.restore(sess, tf.train.latest_checkpoint('./'))\n        print(\"Successfully loaded model\")\n    except:\n        import traceback\n        traceback.print_exc()\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        print(\"=== Recreate new model ! ===\")\n\n\n@csv_output(\"mem_size\", \"n_games\")\ndef bootstrap_phase(sess):\n    while not mem.initialized:\n        for episode in multiplay():\n            if len(episode) &gt; SEQUENCE_LENGTH:\n                mem.add(episode)\n        print(\"{},{}\".format(len(mem), len(mem.episodes)))\n\ndef make_video(sess, filename, n_games=3):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n    w, h = 200, 125\n    video = VideoWriter(w, h, 25, filename)\n    sep_frame = np.zeros((w, h, 3), dtype=np.uint8)\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(n_games):\n        screenbuf = np.zeros((im_h, im_w, 3), dtype=np.uint8)\n        epsilon = 0\n\n        try:\n            # Initialize new hidden state\n            total_reward = 0\n            game.new_episode()\n            h_size = main.h_size if USE_RECURRENCE else 0\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                for i in range(3):\n                    video.add_frame(state.screen_buffer)\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf, order=0)\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf,\n                        dropout_p=1, state_in=hidden_state)\n                action = ACTION_SET[action_no]\n                total_reward += game.make_action(action, 4)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"VizDoom ERROR !\")\n            game, walls = create_game()\n\n        for i in range(25):\n            video.add_frame(sep_frame)\n    video.close()\n    game.close()\n\n\n\ncols = (\"qlearning_step\", \"epsilon\", \"reward\", \"steps\", \"loss_Q\", \"loss_gf\")\ncols += tuple(\"Q%d\" % i for i in range(N_ACTIONS))\n@csv_output(*cols)\ndef learning_phase(sess):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(QLEARNING_STEPS):\n        screenbuf = np.zeros((MAX_EPISODE_LENGTH, im_h, im_w, 3), dtype=np.uint8)\n\n        # Linearly decreasing epsilon\n        epsilon = max(0.1, 1 - (0.9 * i / GREEDY_STEPS))\n        episode = []\n\n        try:\n            game.new_episode()\n            # Initialize new hidden state\n            s = 0\n            h_size = 0 if not USE_RECURRENCE else main.h_size\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf[s], order=0)\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf[s],\n                                        dropout_p=0.75, state_in=hidden_state)\n\n                action = ACTION_SET[action_no]\n                reward = game.make_action(action, 4)\n                game_features = [basic_ennemy_x(state)]\n                episode.append((screenbuf[s], action, reward, game_features))\n                s += 1\n            # episode = reward_reshape(episode)\n            if len(episode) &gt; SEQUENCE_LENGTH:\n                mem.add(episode)\n            # deaths = 1 if len(episode) != MAX_EPISODE_LENGTH else 0\n            tot_reward = sum(r for (s, a, r, f) in episode)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"ViZDoom ERROR !\")\n            game, walls = create_game()\n\n        if i % 200 == 0:\n            make_video(sess, \"videos/learning%05d.mp4\" % i, 3)\n\n        # Adapt target every 10 runs\n        if i % 10 == 0:\n            update_target(sess)\n\n        # Then replay a few sequences\n        for j in range(BACKPROP_STEPS):\n            # Sample a batch and ingest into the NN\n            samples = mem.sample(BATCH_SIZE, SEQUENCE_LENGTH+1)\n            # screens, actions, rewards, game_features\n            S, A, R, F = map(np.array, zip(*samples))\n\n            target_q = sess.run(target.max_Q, feed_dict={\n                target.batch_size: BATCH_SIZE,\n                target.sequence_length: SEQUENCE_LENGTH,\n                target.images: S[:, 1:],\n                target.dropout_p: 1,\n            })\n\n            _, loss_q, loss_gf, qs = sess.run([main.train_step, main.q_loss, main.features_loss,main.Q], feed_dict={\n                main.batch_size: BATCH_SIZE,\n                main.sequence_length: SEQUENCE_LENGTH,\n                main.ignore_up_to: IGNORE_UP_TO,\n                main.images: S[:, :-1],\n                main.target_q: target_q,\n                main.gamma: 0.99,\n                main.rewards: R[:, :-1],\n                main.actions: A[:, :-1],\n                main.dropout_p: 0.75,\n                main.game_features_in: F[:, :-1]\n            })\n        qs = np.mean(np.mean(qs,axis =1),axis=0)\n        print(\"{},{},{},{},{},{},{}\".format(i, epsilon, tot_reward, len(episode), loss_q, loss_gf, \",\".join(map(str,qs))))\n\n        # Save the model periodically\n        if i &gt; 0 and i % 500 == 0:\n            saver.save(sess, \"./model.ckpt\")\n\n    game.close()\n\nfeature_names = [\n    \"\\033[31;1mENNEMIES\\033[0m\",\n    \"\\033[32;1mPICKUPS\\033[0m\",\n    \"\\033[33;1mBLASTS\\033[0m\"\n]\n\n@csv_output(\"actual_ennemy_pos\", \"predicted_pos\")\ndef testing_phase(sess):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(QLEARNING_STEPS):\n        screenbuf = np.zeros((im_h, im_w, 3), dtype=np.uint8)\n        epsilon = 0\n\n        try:\n            # Initialize new hidden state\n            total_reward = 0\n            game.new_episode()\n            h_size = 0 if not USE_RECURRENCE else main.h_size\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf, order=0)\n\n                features = sess.run(main.game_features, feed_dict={\n                    main.sequence_length: 1,\n                    main.batch_size: 1,\n                    main.images: [[screenbuf]],\n                    main.dropout_p: 1,  # No dropout in testing\n                })\n\n                observed_game_features = basic_ennemy_x(state)\n                predicted_game_features = features[0][0][0]\n                print(\"{},{}\".format(observed_game_features, predicted_game_features))\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf,\n                        dropout_p=1, state_in=hidden_state)\n                action = ACTION_SET[action_no]\n                total_reward += game.make_action(action, 4)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"VizDoom ERROR !\")\n            game, walls = create_game()\n\n    game.close()\n\n\n\n    On Sunday, October 8, 2017, 9:53:06 PM GMT+8, zhedongzheng &lt;notifications@github.com&gt; wrote:\n\n\nhi, adil25\nyou can just use batch_size = tf.shape(X)[0]\ntf.shape is a powerful op that returns the dim of your tensor in the runtime\nwhere X is your input sequence\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.</div>", "body_text": "Thank you for your reply, please i am new to tensorflow and i am not sure where to make your suggested change, please could you tell me where to make this change, i am sharing my code below.....\nimport random\nimport vizdoom as vd\nimport tensorflow as tf\nimport numpy as np\nimport scipy.ndimage as Simg\n\nfrom basic_ennemy_pos import basic_ennemy_x\nfrom network import tf, DRQN\nfrom video import VideoWriter\nfrom memory import ReplayMemory\nfrom config import (\n    N_ACTIONS, LEARNING_RATE, MIN_MEM_SIZE, MAX_MEM_SIZE,\n    MAX_CPUS, TRAINING_STEPS, BATCH_SIZE, SEQUENCE_LENGTH,\n    QLEARNING_STEPS, MAX_EPISODE_LENGTH, DEATH_PENALTY,\n    KILL_REWARD, PICKUP_REWARD, GREEDY_STEPS, IGNORE_UP_TO,\n    BACKPROP_STEPS, USE_GAME_FEATURES, LEARN_Q, USE_RECURRENCE,\n)\n\n# Config variables\nim_w, im_h = 108, 60\nN_FEATURES = 1\nACTION_SET = np.eye(N_ACTIONS, dtype=np.uint32).tolist()\nSECTION_SEPARATOR = \"------------\"\n\n# Neural nets and tools\nprint('Building main DRQN')\nmain = DRQN(im_h, im_w, N_FEATURES, N_ACTIONS, 'main', LEARNING_RATE,\n            use_game_features=USE_GAME_FEATURES, learn_q=LEARN_Q,\n            recurrent=USE_RECURRENCE)\nprint('Building target DRQN')\ntarget = DRQN(im_h, im_w, N_FEATURES, N_ACTIONS, 'target', LEARNING_RATE, True,\n        recurrent=USE_RECURRENCE)\nsaver = tf.train.Saver()\nmem = ReplayMemory(MIN_MEM_SIZE, MAX_MEM_SIZE)\n\n\ndef csv_output(*columns):\n    def wrapper(func):\n        def inner(*args, **kwargs):\n            print(\"---------\")\n            print(\"::\", func.__name__, \"::\")\n            print(\",\".join(columns))\n            return func(*args, **kwargs)\n        return inner\n    return wrapper\n\n\ndef create_game():\n    game = vd.DoomGame()\n    game.load_config(\"basic.cfg\")\n\n    # Ennemy detection\n    walls = None  # map_parser.parse(\"maps/deathmatch.txt\")\n    game.clear_available_game_variables()\n    game.add_available_game_variable(vd.GameVariable.POSITION_X)  # 0\n    game.add_available_game_variable(vd.GameVariable.POSITION_Y)  # 1\n    game.add_available_game_variable(vd.GameVariable.POSITION_Z)  # 2\n\n    game.add_available_game_variable(vd.GameVariable.KILLCOUNT)   # 3\n    game.add_available_game_variable(vd.GameVariable.DEATHCOUNT)  # 4\n    game.add_available_game_variable(vd.GameVariable.ITEMCOUNT)   # 5\n\n    game.set_labels_buffer_enabled(True)\n\n    game.init()\n    return game, walls\n\n\ndef reward_reshape(dump):\n    is_dead = len(dump) < MAX_EPISODE_LENGTH\n    reward = [frame[2] for frame in dump]\n    kills = [frame[4] for frame in dump]\n    items = [frame[5] for frame in dump]\n    kill_diff = [0] + [(kills[i] - kills[i - 1]) * KILL_REWARD for i in range(1, len(kills))]\n    item_diff = [0] + [(items[i] - items[i - 1]) * PICKUP_REWARD for i in range(1, len(items))]\n\n    reshaped_reward = [r + k + i for r, k, i in zip(reward, kill_diff, item_diff)]\n\n    if is_dead:\n        reshaped_reward[-1] -= DEATH_PENALTY\n\n    return [\n        (buffer, action, r_reward, game_features)\n        for (buffer, action, _, game_features, _, _), r_reward\n        in zip(dump, reshaped_reward)\n    ]\n\n\ndef play_random_episode(game, walls, verbose=False, skip=1):\n    game.new_episode()\n    dump = []\n    zoomed = np.zeros((MAX_EPISODE_LENGTH, im_h, im_w, 3), dtype=np.uint8)\n    action = ACTION_SET[0]\n    while not game.is_episode_finished():\n        # Get screen buf\n        state = game.get_state()\n        S = state.screen_buffer  # NOQA\n\n        # Resample to our network size\n        h, w = S.shape[:2]\n        Simg.zoom(S, [1. * im_h / h, 1. * im_w / w, 1],\n                  output=zoomed[len(dump)], order=0)\n        S = zoomed[len(dump)]  # NOQA\n\n        # Get game features an action\n        game_features = [basic_ennemy_x(state)]\n        action = random.choice(ACTION_SET)\n        reward = game.make_action(action, skip)\n        dump.append((S, action, reward, game_features))\n    return dump\n\n\ndef wrap_play_random_episode(i=0):\n    try:\n        game, walls = create_game()\n        res = play_random_episode(game, walls, skip=4)\n        game.close()\n        return res\n    except vd.vizdoom.ViZDoomErrorException:\n        print(\"ViZDoom ERROR\")\n        return []\n\n# Need to be imported and created after wrap_play_random_episode\nfrom multiprocessing import Pool, cpu_count\nN_CORES = min(cpu_count(), MAX_CPUS)\nif N_CORES > 1:\n    workers = Pool(N_CORES)\n\n\ndef multiplay():\n    if N_CORES <= 1:\n        return [wrap_play_random_episode()]\n    else:\n        return workers.map(wrap_play_random_episode, range(N_CORES))\n\n\ndef update_target(sess):\n    \"\"\"Transfer learned parameters from main to target NN\"\"\"\n    v = tf.trainable_variables()\n    main_vars = filter(lambda x: x.name.startswith('main'), v)\n    target_vars = filter(lambda x: x.name.startswith('target'), v)\n    for t, m in zip(target_vars, main_vars):\n        sess.run(t.assign(m.value()))\n\n\ndef init_phase(sess):\n    \"\"\"\n    Attempt to restore a model, or initialize all variables.\n    Then fills replay memory with random-action games\n    \"\"\"\n    try:\n        saver = tf.train.import_meta_graph('model.ckpt.meta')\n        saver.restore(sess, tf.train.latest_checkpoint('./'))\n        print(\"Successfully loaded model\")\n    except:\n        import traceback\n        traceback.print_exc()\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        print(\"=== Recreate new model ! ===\")\n\n\n@csv_output(\"mem_size\", \"n_games\")\ndef bootstrap_phase(sess):\n    while not mem.initialized:\n        for episode in multiplay():\n            if len(episode) > SEQUENCE_LENGTH:\n                mem.add(episode)\n        print(\"{},{}\".format(len(mem), len(mem.episodes)))\n\ndef make_video(sess, filename, n_games=3):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n    w, h = 200, 125\n    video = VideoWriter(w, h, 25, filename)\n    sep_frame = np.zeros((w, h, 3), dtype=np.uint8)\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(n_games):\n        screenbuf = np.zeros((im_h, im_w, 3), dtype=np.uint8)\n        epsilon = 0\n\n        try:\n            # Initialize new hidden state\n            total_reward = 0\n            game.new_episode()\n            h_size = main.h_size if USE_RECURRENCE else 0\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                for i in range(3):\n                    video.add_frame(state.screen_buffer)\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf, order=0)\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf,\n                        dropout_p=1, state_in=hidden_state)\n                action = ACTION_SET[action_no]\n                total_reward += game.make_action(action, 4)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"VizDoom ERROR !\")\n            game, walls = create_game()\n\n        for i in range(25):\n            video.add_frame(sep_frame)\n    video.close()\n    game.close()\n\n\n\ncols = (\"qlearning_step\", \"epsilon\", \"reward\", \"steps\", \"loss_Q\", \"loss_gf\")\ncols += tuple(\"Q%d\" % i for i in range(N_ACTIONS))\n@csv_output(*cols)\ndef learning_phase(sess):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(QLEARNING_STEPS):\n        screenbuf = np.zeros((MAX_EPISODE_LENGTH, im_h, im_w, 3), dtype=np.uint8)\n\n        # Linearly decreasing epsilon\n        epsilon = max(0.1, 1 - (0.9 * i / GREEDY_STEPS))\n        episode = []\n\n        try:\n            game.new_episode()\n            # Initialize new hidden state\n            s = 0\n            h_size = 0 if not USE_RECURRENCE else main.h_size\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf[s], order=0)\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf[s],\n                                        dropout_p=0.75, state_in=hidden_state)\n\n                action = ACTION_SET[action_no]\n                reward = game.make_action(action, 4)\n                game_features = [basic_ennemy_x(state)]\n                episode.append((screenbuf[s], action, reward, game_features))\n                s += 1\n            # episode = reward_reshape(episode)\n            if len(episode) > SEQUENCE_LENGTH:\n                mem.add(episode)\n            # deaths = 1 if len(episode) != MAX_EPISODE_LENGTH else 0\n            tot_reward = sum(r for (s, a, r, f) in episode)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"ViZDoom ERROR !\")\n            game, walls = create_game()\n\n        if i % 200 == 0:\n            make_video(sess, \"videos/learning%05d.mp4\" % i, 3)\n\n        # Adapt target every 10 runs\n        if i % 10 == 0:\n            update_target(sess)\n\n        # Then replay a few sequences\n        for j in range(BACKPROP_STEPS):\n            # Sample a batch and ingest into the NN\n            samples = mem.sample(BATCH_SIZE, SEQUENCE_LENGTH+1)\n            # screens, actions, rewards, game_features\n            S, A, R, F = map(np.array, zip(*samples))\n\n            target_q = sess.run(target.max_Q, feed_dict={\n                target.batch_size: BATCH_SIZE,\n                target.sequence_length: SEQUENCE_LENGTH,\n                target.images: S[:, 1:],\n                target.dropout_p: 1,\n            })\n\n            _, loss_q, loss_gf, qs = sess.run([main.train_step, main.q_loss, main.features_loss,main.Q], feed_dict={\n                main.batch_size: BATCH_SIZE,\n                main.sequence_length: SEQUENCE_LENGTH,\n                main.ignore_up_to: IGNORE_UP_TO,\n                main.images: S[:, :-1],\n                main.target_q: target_q,\n                main.gamma: 0.99,\n                main.rewards: R[:, :-1],\n                main.actions: A[:, :-1],\n                main.dropout_p: 0.75,\n                main.game_features_in: F[:, :-1]\n            })\n        qs = np.mean(np.mean(qs,axis =1),axis=0)\n        print(\"{},{},{},{},{},{},{}\".format(i, epsilon, tot_reward, len(episode), loss_q, loss_gf, \",\".join(map(str,qs))))\n\n        # Save the model periodically\n        if i > 0 and i % 500 == 0:\n            saver.save(sess, \"./model.ckpt\")\n\n    game.close()\n\nfeature_names = [\n    \"\\033[31;1mENNEMIES\\033[0m\",\n    \"\\033[32;1mPICKUPS\\033[0m\",\n    \"\\033[33;1mBLASTS\\033[0m\"\n]\n\n@csv_output(\"actual_ennemy_pos\", \"predicted_pos\")\ndef testing_phase(sess):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(QLEARNING_STEPS):\n        screenbuf = np.zeros((im_h, im_w, 3), dtype=np.uint8)\n        epsilon = 0\n\n        try:\n            # Initialize new hidden state\n            total_reward = 0\n            game.new_episode()\n            h_size = 0 if not USE_RECURRENCE else main.h_size\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf, order=0)\n\n                features = sess.run(main.game_features, feed_dict={\n                    main.sequence_length: 1,\n                    main.batch_size: 1,\n                    main.images: [[screenbuf]],\n                    main.dropout_p: 1,  # No dropout in testing\n                })\n\n                observed_game_features = basic_ennemy_x(state)\n                predicted_game_features = features[0][0][0]\n                print(\"{},{}\".format(observed_game_features, predicted_game_features))\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf,\n                        dropout_p=1, state_in=hidden_state)\n                action = ACTION_SET[action_no]\n                total_reward += game.make_action(action, 4)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"VizDoom ERROR !\")\n            game, walls = create_game()\n\n    game.close()\n\n\n\n    On Sunday, October 8, 2017, 9:53:06 PM GMT+8, zhedongzheng <notifications@github.com> wrote:\n\n\nhi, adil25\nyou can just use batch_size = tf.shape(X)[0]\ntf.shape is a powerful op that returns the dim of your tensor in the runtime\nwhere X is your input sequence\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.", "body": "Thank you for your reply, please i am new to tensorflow and i am not sure where to make your suggested change, please could you tell me where to make this change, i am sharing my code below.....\nimport random\nimport vizdoom as vd\nimport tensorflow as tf\nimport numpy as np\nimport scipy.ndimage as Simg\n\nfrom basic_ennemy_pos import basic_ennemy_x\nfrom network import tf, DRQN\nfrom video import VideoWriter\nfrom memory import ReplayMemory\nfrom config import (\n    N_ACTIONS, LEARNING_RATE, MIN_MEM_SIZE, MAX_MEM_SIZE,\n    MAX_CPUS, TRAINING_STEPS, BATCH_SIZE, SEQUENCE_LENGTH,\n    QLEARNING_STEPS, MAX_EPISODE_LENGTH, DEATH_PENALTY,\n    KILL_REWARD, PICKUP_REWARD, GREEDY_STEPS, IGNORE_UP_TO,\n    BACKPROP_STEPS, USE_GAME_FEATURES, LEARN_Q, USE_RECURRENCE,\n)\n\n# Config variables\nim_w, im_h = 108, 60\nN_FEATURES = 1\nACTION_SET = np.eye(N_ACTIONS, dtype=np.uint32).tolist()\nSECTION_SEPARATOR = \"------------\"\n\n# Neural nets and tools\nprint('Building main DRQN')\nmain = DRQN(im_h, im_w, N_FEATURES, N_ACTIONS, 'main', LEARNING_RATE, \n            use_game_features=USE_GAME_FEATURES, learn_q=LEARN_Q, \n            recurrent=USE_RECURRENCE)\nprint('Building target DRQN')\ntarget = DRQN(im_h, im_w, N_FEATURES, N_ACTIONS, 'target', LEARNING_RATE, True, \n        recurrent=USE_RECURRENCE)\nsaver = tf.train.Saver()\nmem = ReplayMemory(MIN_MEM_SIZE, MAX_MEM_SIZE)\n\n\ndef csv_output(*columns):\n    def wrapper(func):\n        def inner(*args, **kwargs):\n            print(\"---------\")\n            print(\"::\", func.__name__, \"::\")\n            print(\",\".join(columns))\n            return func(*args, **kwargs)\n        return inner\n    return wrapper\n\n\ndef create_game():\n    game = vd.DoomGame()\n    game.load_config(\"basic.cfg\")\n\n    # Ennemy detection\n    walls = None  # map_parser.parse(\"maps/deathmatch.txt\")\n    game.clear_available_game_variables()\n    game.add_available_game_variable(vd.GameVariable.POSITION_X)  # 0\n    game.add_available_game_variable(vd.GameVariable.POSITION_Y)  # 1\n    game.add_available_game_variable(vd.GameVariable.POSITION_Z)  # 2\n\n    game.add_available_game_variable(vd.GameVariable.KILLCOUNT)   # 3\n    game.add_available_game_variable(vd.GameVariable.DEATHCOUNT)  # 4\n    game.add_available_game_variable(vd.GameVariable.ITEMCOUNT)   # 5\n\n    game.set_labels_buffer_enabled(True)\n\n    game.init()\n    return game, walls\n\n\ndef reward_reshape(dump):\n    is_dead = len(dump) < MAX_EPISODE_LENGTH\n    reward = [frame[2] for frame in dump]\n    kills = [frame[4] for frame in dump]\n    items = [frame[5] for frame in dump]\n    kill_diff = [0] + [(kills[i] - kills[i - 1]) * KILL_REWARD for i in range(1, len(kills))]\n    item_diff = [0] + [(items[i] - items[i - 1]) * PICKUP_REWARD for i in range(1, len(items))]\n\n    reshaped_reward = [r + k + i for r, k, i in zip(reward, kill_diff, item_diff)]\n\n    if is_dead:\n        reshaped_reward[-1] -= DEATH_PENALTY\n\n    return [\n        (buffer, action, r_reward, game_features)\n        for (buffer, action, _, game_features, _, _), r_reward\n        in zip(dump, reshaped_reward)\n    ]\n\n\ndef play_random_episode(game, walls, verbose=False, skip=1):\n    game.new_episode()\n    dump = []\n    zoomed = np.zeros((MAX_EPISODE_LENGTH, im_h, im_w, 3), dtype=np.uint8)\n    action = ACTION_SET[0]\n    while not game.is_episode_finished():\n        # Get screen buf\n        state = game.get_state()\n        S = state.screen_buffer  # NOQA\n\n        # Resample to our network size\n        h, w = S.shape[:2]\n        Simg.zoom(S, [1. * im_h / h, 1. * im_w / w, 1],\n                  output=zoomed[len(dump)], order=0)\n        S = zoomed[len(dump)]  # NOQA\n\n        # Get game features an action\n        game_features = [basic_ennemy_x(state)]\n        action = random.choice(ACTION_SET)\n        reward = game.make_action(action, skip)\n        dump.append((S, action, reward, game_features))\n    return dump\n\n\ndef wrap_play_random_episode(i=0):\n    try:\n        game, walls = create_game()\n        res = play_random_episode(game, walls, skip=4)\n        game.close()\n        return res\n    except vd.vizdoom.ViZDoomErrorException:\n        print(\"ViZDoom ERROR\")\n        return []\n\n# Need to be imported and created after wrap_play_random_episode\nfrom multiprocessing import Pool, cpu_count\nN_CORES = min(cpu_count(), MAX_CPUS)\nif N_CORES > 1:\n    workers = Pool(N_CORES)\n\n\ndef multiplay():\n    if N_CORES <= 1:\n        return [wrap_play_random_episode()]\n    else:\n        return workers.map(wrap_play_random_episode, range(N_CORES))\n\n\ndef update_target(sess):\n    \"\"\"Transfer learned parameters from main to target NN\"\"\"\n    v = tf.trainable_variables()\n    main_vars = filter(lambda x: x.name.startswith('main'), v)\n    target_vars = filter(lambda x: x.name.startswith('target'), v)\n    for t, m in zip(target_vars, main_vars):\n        sess.run(t.assign(m.value()))\n\n\ndef init_phase(sess):\n    \"\"\"\n    Attempt to restore a model, or initialize all variables.\n    Then fills replay memory with random-action games\n    \"\"\"\n    try:\n        saver = tf.train.import_meta_graph('model.ckpt.meta')\n        saver.restore(sess, tf.train.latest_checkpoint('./'))\n        print(\"Successfully loaded model\")\n    except:\n        import traceback\n        traceback.print_exc()\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        print(\"=== Recreate new model ! ===\")\n\n\n@csv_output(\"mem_size\", \"n_games\")\ndef bootstrap_phase(sess):\n    while not mem.initialized:\n        for episode in multiplay():\n            if len(episode) > SEQUENCE_LENGTH:\n                mem.add(episode)\n        print(\"{},{}\".format(len(mem), len(mem.episodes)))\n\ndef make_video(sess, filename, n_games=3):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n    w, h = 200, 125\n    video = VideoWriter(w, h, 25, filename)\n    sep_frame = np.zeros((w, h, 3), dtype=np.uint8)\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(n_games):\n        screenbuf = np.zeros((im_h, im_w, 3), dtype=np.uint8)\n        epsilon = 0\n\n        try:\n            # Initialize new hidden state\n            total_reward = 0\n            game.new_episode()\n            h_size = main.h_size if USE_RECURRENCE else 0\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                for i in range(3):\n                    video.add_frame(state.screen_buffer)\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf, order=0)\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf,\n                        dropout_p=1, state_in=hidden_state)\n                action = ACTION_SET[action_no]\n                total_reward += game.make_action(action, 4)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"VizDoom ERROR !\")\n            game, walls = create_game()\n\n        for i in range(25):\n            video.add_frame(sep_frame)\n    video.close()\n    game.close()\n\n\n\ncols = (\"qlearning_step\", \"epsilon\", \"reward\", \"steps\", \"loss_Q\", \"loss_gf\")\ncols += tuple(\"Q%d\" % i for i in range(N_ACTIONS))\n@csv_output(*cols)\ndef learning_phase(sess):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(QLEARNING_STEPS):\n        screenbuf = np.zeros((MAX_EPISODE_LENGTH, im_h, im_w, 3), dtype=np.uint8)\n\n        # Linearly decreasing epsilon\n        epsilon = max(0.1, 1 - (0.9 * i / GREEDY_STEPS))\n        episode = []\n\n        try:\n            game.new_episode()\n            # Initialize new hidden state\n            s = 0\n            h_size = 0 if not USE_RECURRENCE else main.h_size\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf[s], order=0)\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf[s],\n                                        dropout_p=0.75, state_in=hidden_state)\n\n                action = ACTION_SET[action_no]\n                reward = game.make_action(action, 4)\n                game_features = [basic_ennemy_x(state)]\n                episode.append((screenbuf[s], action, reward, game_features))\n                s += 1\n            # episode = reward_reshape(episode)\n            if len(episode) > SEQUENCE_LENGTH:\n                mem.add(episode)\n            # deaths = 1 if len(episode) != MAX_EPISODE_LENGTH else 0\n            tot_reward = sum(r for (s, a, r, f) in episode)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"ViZDoom ERROR !\")\n            game, walls = create_game()\n\n        if i % 200 == 0:\n            make_video(sess, \"videos/learning%05d.mp4\" % i, 3)\n\n        # Adapt target every 10 runs\n        if i % 10 == 0:\n            update_target(sess)\n\n        # Then replay a few sequences\n        for j in range(BACKPROP_STEPS):\n            # Sample a batch and ingest into the NN\n            samples = mem.sample(BATCH_SIZE, SEQUENCE_LENGTH+1)\n            # screens, actions, rewards, game_features\n            S, A, R, F = map(np.array, zip(*samples))\n\n            target_q = sess.run(target.max_Q, feed_dict={\n                target.batch_size: BATCH_SIZE,\n                target.sequence_length: SEQUENCE_LENGTH,\n                target.images: S[:, 1:],\n                target.dropout_p: 1,\n            })\n\n            _, loss_q, loss_gf, qs = sess.run([main.train_step, main.q_loss, main.features_loss,main.Q], feed_dict={\n                main.batch_size: BATCH_SIZE,\n                main.sequence_length: SEQUENCE_LENGTH,\n                main.ignore_up_to: IGNORE_UP_TO,\n                main.images: S[:, :-1],\n                main.target_q: target_q,\n                main.gamma: 0.99,\n                main.rewards: R[:, :-1],\n                main.actions: A[:, :-1],\n                main.dropout_p: 0.75,\n                main.game_features_in: F[:, :-1]\n            })\n        qs = np.mean(np.mean(qs,axis =1),axis=0)\n        print(\"{},{},{},{},{},{},{}\".format(i, epsilon, tot_reward, len(episode), loss_q, loss_gf, \",\".join(map(str,qs))))\n\n        # Save the model periodically\n        if i > 0 and i % 500 == 0:\n            saver.save(sess, \"./model.ckpt\")\n\n    game.close()\n\nfeature_names = [\n    \"\\033[31;1mENNEMIES\\033[0m\",\n    \"\\033[32;1mPICKUPS\\033[0m\",\n    \"\\033[33;1mBLASTS\\033[0m\"\n]\n\n@csv_output(\"actual_ennemy_pos\", \"predicted_pos\")\ndef testing_phase(sess):\n    \"\"\"Reinforcement learning for Qvalues\"\"\"\n    game, walls = create_game()\n\n    # From now on, we don't use game features, but we provide an empty\n    # numpy array so that the ReplayMemory is still zippable\n    for i in range(QLEARNING_STEPS):\n        screenbuf = np.zeros((im_h, im_w, 3), dtype=np.uint8)\n        epsilon = 0\n\n        try:\n            # Initialize new hidden state\n            total_reward = 0\n            game.new_episode()\n            h_size = 0 if not USE_RECURRENCE else main.h_size\n            hidden_state = (np.zeros((1, h_size)), np.zeros((1, h_size)))\n            while not game.is_episode_finished():\n                # Get and resize screen buffer\n                state = game.get_state()\n                h, w, d = state.screen_buffer.shape\n                Simg.zoom(state.screen_buffer,\n                          [1. * im_h / h, 1. * im_w / w, 1],\n                          output=screenbuf, order=0)\n\n                features = sess.run(main.game_features, feed_dict={\n                    main.sequence_length: 1,\n                    main.batch_size: 1,\n                    main.images: [[screenbuf]],\n                    main.dropout_p: 1,  # No dropout in testing\n                })\n\n                observed_game_features = basic_ennemy_x(state)\n                predicted_game_features = features[0][0][0]\n                print(\"{},{}\".format(observed_game_features, predicted_game_features))\n\n                # Choose action with e-greedy network\n                action_no, hidden_state = main.choose(sess, epsilon, screenbuf,\n                        dropout_p=1, state_in=hidden_state)\n                action = ACTION_SET[action_no]\n                total_reward += game.make_action(action, 4)\n        except vd.vizdoom.ViZDoomErrorException:\n            print(\"VizDoom ERROR !\")\n            game, walls = create_game()\n\n    game.close()\n\n \n\n    On Sunday, October 8, 2017, 9:53:06 PM GMT+8, zhedongzheng <notifications@github.com> wrote:  \n \n \nhi, adil25\nyou can just use batch_size = tf.shape(X)[0]\ntf.shape is a powerful op that returns the dim of your tensor in the runtime\nwhere X is your input sequence\n\n\u2014\nYou are receiving this because you commented.\nReply to this email directly, view it on GitHub, or mute the thread.\n    "}