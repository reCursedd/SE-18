{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/327348424", "html_url": "https://github.com/tensorflow/tensorflow/pull/12493#issuecomment-327348424", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12493", "id": 327348424, "node_id": "MDEyOklzc3VlQ29tbWVudDMyNzM0ODQyNA==", "user": {"login": "qianyizhang", "id": 12500132, "node_id": "MDQ6VXNlcjEyNTAwMTMy", "avatar_url": "https://avatars0.githubusercontent.com/u/12500132?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qianyizhang", "html_url": "https://github.com/qianyizhang", "followers_url": "https://api.github.com/users/qianyizhang/followers", "following_url": "https://api.github.com/users/qianyizhang/following{/other_user}", "gists_url": "https://api.github.com/users/qianyizhang/gists{/gist_id}", "starred_url": "https://api.github.com/users/qianyizhang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qianyizhang/subscriptions", "organizations_url": "https://api.github.com/users/qianyizhang/orgs", "repos_url": "https://api.github.com/users/qianyizhang/repos", "events_url": "https://api.github.com/users/qianyizhang/events{/privacy}", "received_events_url": "https://api.github.com/users/qianyizhang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-06T01:30:59Z", "updated_at": "2017-09-06T01:30:59Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2887803\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/panyx0718\">@panyx0718</a><br>\nThanks for you info, you are correct that I am using the pip install on windows, and it's not a easy task to build from source. Fortunately with the rapid development and recent change in namespace t seems this feature will be open to test in the next patch or so ;-)</p>\n<p>In the meanwhile, is there a good way to measure to GPU memory usage? (nvidia-smi.exe monitor report is highly inaccurate)<br>\nAnd is there any trick of memory optimization (eg. maxnet has force_mirroring to force in-place calculation, <a href=\"https://github.com/dmlc/mxnet-memonger\">https://github.com/dmlc/mxnet-memonger</a>)</p>", "body_text": "@panyx0718\nThanks for you info, you are correct that I am using the pip install on windows, and it's not a easy task to build from source. Fortunately with the rapid development and recent change in namespace t seems this feature will be open to test in the next patch or so ;-)\nIn the meanwhile, is there a good way to measure to GPU memory usage? (nvidia-smi.exe monitor report is highly inaccurate)\nAnd is there any trick of memory optimization (eg. maxnet has force_mirroring to force in-place calculation, https://github.com/dmlc/mxnet-memonger)", "body": "@panyx0718 \r\nThanks for you info, you are correct that I am using the pip install on windows, and it's not a easy task to build from source. Fortunately with the rapid development and recent change in namespace t seems this feature will be open to test in the next patch or so ;-)\r\n\r\nIn the meanwhile, is there a good way to measure to GPU memory usage? (nvidia-smi.exe monitor report is highly inaccurate)\r\nAnd is there any trick of memory optimization (eg. maxnet has force_mirroring to force in-place calculation, https://github.com/dmlc/mxnet-memonger)"}