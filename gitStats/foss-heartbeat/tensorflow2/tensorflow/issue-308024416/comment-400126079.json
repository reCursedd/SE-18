{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/400126079", "html_url": "https://github.com/tensorflow/tensorflow/issues/17950#issuecomment-400126079", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17950", "id": 400126079, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMDEyNjA3OQ==", "user": {"login": "martinwicke", "id": 577277, "node_id": "MDQ6VXNlcjU3NzI3Nw==", "avatar_url": "https://avatars2.githubusercontent.com/u/577277?v=4", "gravatar_id": "", "url": "https://api.github.com/users/martinwicke", "html_url": "https://github.com/martinwicke", "followers_url": "https://api.github.com/users/martinwicke/followers", "following_url": "https://api.github.com/users/martinwicke/following{/other_user}", "gists_url": "https://api.github.com/users/martinwicke/gists{/gist_id}", "starred_url": "https://api.github.com/users/martinwicke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/martinwicke/subscriptions", "organizations_url": "https://api.github.com/users/martinwicke/orgs", "repos_url": "https://api.github.com/users/martinwicke/repos", "events_url": "https://api.github.com/users/martinwicke/events{/privacy}", "received_events_url": "https://api.github.com/users/martinwicke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-25T23:21:48Z", "updated_at": "2018-06-25T23:21:48Z", "author_association": "MEMBER", "body_html": "<p>Yes. The workaround <em>should</em> be something like this:</p>\n<pre><code># Batch norm requires update_ops to be added as a train_op dependency.\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n  train_op = optimizer.minimize(loss, tf.train.get_global_step())\n</code></pre>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15220929\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tanzhenyu\">@tanzhenyu</a> FYI</p>", "body_text": "Yes. The workaround should be something like this:\n# Batch norm requires update_ops to be added as a train_op dependency.\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n  train_op = optimizer.minimize(loss, tf.train.get_global_step())\n\n@tanzhenyu FYI", "body": "Yes. The workaround *should* be something like this:\r\n\r\n```\r\n# Batch norm requires update_ops to be added as a train_op dependency.\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n  train_op = optimizer.minimize(loss, tf.train.get_global_step())\r\n```\r\n\r\n@tanzhenyu FYI"}