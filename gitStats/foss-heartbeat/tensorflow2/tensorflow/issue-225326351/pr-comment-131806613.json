{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131806613", "pull_request_review_id": 54819948, "id": 131806613, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMTgwNjYxMw==", "diff_hunk": "@@ -0,0 +1,256 @@\n+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"DelayCompensatedGradientDescentOptimizer for TensorFlow.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.training import optimizer\n+from tensorflow.python.training import training_ops\n+\n+\n+class _RefVariableAsynchronousProcessor(optimizer._RefVariableProcessor):\n+  \"\"\"Processor for Variable.\"\"\"\n+  def update_op_asynchronous(self, optimizer, g, index):\n+    if isinstance(g, ops.Tensor):\n+      return optimizer._apply_dense(g, self._v, index)\n+    else:\n+      assert isinstance(g, ops.IndexedSlices), (\"Gradient \", g, \" is neither a \"\n+                                                \"tensor nor IndexedSlices.\")\n+      # pylint: disable=protected-access\n+      return optimizer._apply_sparse_duplicate_indices(g, self._v, index)\n+\n+\n+class _DenseResourceVariableAsynchronousProcessor(optimizer._DenseResourceVariableProcessor):\n+  \"\"\"Processor for dense ResourceVariables.\"\"\"\n+  def update_op_asynchronous(self, optimizer, g, index):\n+    # pylint: disable=protected-access\n+    if isinstance(g, ops.IndexedSlices):\n+      return optimizer._resource_apply_sparse_duplicate_indices(\n+        g.values, self._v, g.indices, index)\n+    return optimizer._resource_apply_dense(g, self._v, index)\n+\n+\n+def _get_processor(v):\n+  \"\"\"The processor of v.\"\"\"\n+  if v.op.type == \"VarHandleOp\":\n+    return _DenseResourceVariableAsynchronousProcessor(v)\n+  if isinstance(v, variables.Variable):\n+    return _RefVariableAsynchronousProcessor(v)\n+  raise NotImplementedError(\"Trying to optimize unsupported type \", v)\n+\n+\n+class DelayCompensatedGradientDescentOptimizer(optimizer.Optimizer):\n+  \"\"\"Optimizer that implements gradient descent with delay compensation.\n+\n+  See [Zheng, Shuxin, et al., 2016](https://arxiv.org/abs/1609.08326)\n+  ([pdf](https://arxiv.org/pdf/1609.08326.pdf)).\n+  \"\"\"\n+\n+  def __init__(self, learning_rate, variance_parameter, num_workers=1,\n+               use_locking=False, name=\"DelayCompensatedGradientDescent\"):\n+    \"\"\"Construct a new gradient descent optimizer with delay compensation.\n+\n+    Args:\n+      learning_rate: A Tensor or a floating point value.  The learning\n+        rate to use.\n+      variance_parameter: A Tensor or a floating point value. The lambda\n+        value to use.\n+      num_workers: A value to indicate number of workers computing gradients\n+        asynchronously.\n+      use_locking: If True use locks for update operations.\n+      name: Optional name prefix for the operations created when applying\n+        gradients. Defaults to \"DelayCompensatedGradientDescent\".\n+      \"\"\"\n+    if num_workers <= 0:\n+      raise ValueError(\"num_workers must be positive: %s\" % num_workers)\n+    super(DelayCompensatedGradientDescentOptimizer, self).__init__(\n+          use_locking, name)\n+    self._learning_rate = learning_rate\n+    self._lambda = variance_parameter\n+    self._num_workers = num_workers\n+\n+  def minimize(self, loss, global_step=None, var_list=None,\n+               gate_gradients=optimizer.Optimizer.GATE_OP, aggregation_method=None,\n+               colocate_gradients_with_ops=False, name=None,\n+               grad_loss=None, worker_index=None):\n+    \"\"\"Add operations to minimize `loss` by updating `var_list`.\n+\n+    This method simply combines calls `compute_gradients()` and\n+    `apply_gradients()`. If you want to process the gradient before applying\n+    them call `compute_gradients()` and `apply_gradients()` explicitly instead\n+    of using this function.\n+\n+    Args:\n+      loss: A `Tensor` containing the value to minimize.\n+      global_step: Optional `Variable` to increment by one after the\n+        variables have been updated.\n+      var_list: Optional list or tuple of `Variable` objects to update to\n+        minimize `loss`.  Defaults to the list of variables collected in\n+        the graph under the key `GraphKeys.TRAINABLE_VARIABLES`.\n+      gate_gradients: How to gate the computation of gradients.  Can be\n+        `GATE_NONE`, `GATE_OP`, or  `GATE_GRAPH`.\n+      aggregation_method: Specifies the method used to combine gradient terms.\n+        Valid values are defined in the class `AggregationMethod`.\n+      colocate_gradients_with_ops: If True, try colocating gradients with\n+        the corresponding op.\n+      name: Optional name for the returned operation.\n+      grad_loss: Optional. A `Tensor` holding the gradient computed for `loss`.\n+      worker_index: Optional. A value to indicate the instance of worker\n+        minimizing if computing asynchronously.\n+\n+    Returns:\n+      An Operation that updates the variables in `var_list`.  If `global_step`\n+      was not `None`, that operation also increments `global_step`.\n+\n+    Raises:\n+      ValueError: If some of the variables are not `Variable` objects.\n+    \"\"\"\n+    if (worker_index < 0 and worker_index is not None) or worker_index >= self._num_workers:\n+      raise ValueError(\"worker index must be in the range [0, num_workers): %s\" %\n+                        worker_index)\n+    grads_and_vars = self.compute_gradients(\n+        loss, var_list=var_list, gate_gradients=gate_gradients,\n+        aggregation_method=aggregation_method,\n+        colocate_gradients_with_ops=colocate_gradients_with_ops,\n+        grad_loss=grad_loss)\n+\n+    vars_with_grad = [v for g, v in grads_and_vars if g is not None]\n+    if not vars_with_grad:\n+      raise ValueError(\n+          \"No gradients provided for any variable, check your graph for ops\"\n+          \" that do not support gradients, between variables %s and loss %s.\" %\n+          ([str(v) for _, v in grads_and_vars], loss))\n+\n+    return self.apply_gradients(grads_and_vars, global_step=global_step,\n+                                name=name, worker_index=worker_index)\n+\n+  def apply_gradients(self,\n+                      grads_and_vars,\n+                      global_step=None,\n+                      name=None,\n+                      worker_index=None):", "path": "tensorflow/contrib/opt/python/training/delay_compensated_gradient_descent.py", "position": 150, "original_position": 150, "commit_id": "bee23d372307ec23f95acc65b262b79cfa060d28", "original_commit_id": "bee23d372307ec23f95acc65b262b79cfa060d28", "user": {"login": "ry", "id": 80, "node_id": "MDQ6VXNlcjgw", "avatar_url": "https://avatars1.githubusercontent.com/u/80?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ry", "html_url": "https://github.com/ry", "followers_url": "https://api.github.com/users/ry/followers", "following_url": "https://api.github.com/users/ry/following{/other_user}", "gists_url": "https://api.github.com/users/ry/gists{/gist_id}", "starred_url": "https://api.github.com/users/ry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ry/subscriptions", "organizations_url": "https://api.github.com/users/ry/orgs", "repos_url": "https://api.github.com/users/ry/repos", "events_url": "https://api.github.com/users/ry/events{/privacy}", "received_events_url": "https://api.github.com/users/ry/received_events", "type": "User", "site_admin": false}, "body": "@alisidd question for you above regarding subclassing. \r\ncc @vrv", "created_at": "2017-08-08T02:08:37Z", "updated_at": "2017-08-08T02:08:38Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9551#discussion_r131806613", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9551", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131806613"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9551#discussion_r131806613"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9551"}}, "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=16481751\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alisidd\">@alisidd</a> question for you above regarding subclassing.<br>\ncc <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a></p>", "body_text": "@alisidd question for you above regarding subclassing.\ncc @vrv", "in_reply_to_id": 131805751}