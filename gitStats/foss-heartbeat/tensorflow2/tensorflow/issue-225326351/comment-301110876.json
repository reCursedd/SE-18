{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301110876", "html_url": "https://github.com/tensorflow/tensorflow/pull/9551#issuecomment-301110876", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9551", "id": 301110876, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTExMDg3Ng==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-12T15:37:58Z", "updated_at": "2017-05-12T15:37:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for making these changes -- though I think changing the public API of optimizers just for this one case suggests that the Optimizer interface is now doing too much.</p>\n<p>I'm not too familiar with the optimizer interface personally, but one thought: the idea of delay compensated updates could theoretically apply to any gradient descent algorithm; instead of having this version descend from Optimizer, does it make sense to:</p>\n<ol>\n<li>\n<p>Subclass Optimizer to be a DelayCompensatedOptimizer, where you add your specific modifications based on worker_index</p>\n</li>\n<li>\n<p>Make GradientDescentDC descend from it?</p>\n</li>\n</ol>\n<p>Let me know what you think!</p>\n<p>(Also, I'd probably be verbose on the names here: DelayCompensatedGradientDescent; the term \"DC\" isn't standard enough for it to be abbreviated).</p>", "body_text": "Thanks for making these changes -- though I think changing the public API of optimizers just for this one case suggests that the Optimizer interface is now doing too much.\nI'm not too familiar with the optimizer interface personally, but one thought: the idea of delay compensated updates could theoretically apply to any gradient descent algorithm; instead of having this version descend from Optimizer, does it make sense to:\n\n\nSubclass Optimizer to be a DelayCompensatedOptimizer, where you add your specific modifications based on worker_index\n\n\nMake GradientDescentDC descend from it?\n\n\nLet me know what you think!\n(Also, I'd probably be verbose on the names here: DelayCompensatedGradientDescent; the term \"DC\" isn't standard enough for it to be abbreviated).", "body": "Thanks for making these changes -- though I think changing the public API of optimizers just for this one case suggests that the Optimizer interface is now doing too much.\r\n\r\nI'm not too familiar with the optimizer interface personally, but one thought: the idea of delay compensated updates could theoretically apply to any gradient descent algorithm; instead of having this version descend from Optimizer, does it make sense to:\r\n\r\n1) Subclass Optimizer to be a DelayCompensatedOptimizer, where you add your specific modifications based on worker_index \r\n\r\n2) Make GradientDescentDC descend from it?\r\n\r\nLet me know what you think!\r\n\r\n(Also, I'd probably be verbose on the names here: DelayCompensatedGradientDescent; the term \"DC\" isn't standard enough for it to be abbreviated).\r\n"}