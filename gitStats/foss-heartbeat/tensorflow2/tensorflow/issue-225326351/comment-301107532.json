{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/301107532", "html_url": "https://github.com/tensorflow/tensorflow/pull/9551#issuecomment-301107532", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9551", "id": 301107532, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMTEwNzUzMg==", "user": {"login": "alisidd", "id": 16481751, "node_id": "MDQ6VXNlcjE2NDgxNzUx", "avatar_url": "https://avatars1.githubusercontent.com/u/16481751?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alisidd", "html_url": "https://github.com/alisidd", "followers_url": "https://api.github.com/users/alisidd/followers", "following_url": "https://api.github.com/users/alisidd/following{/other_user}", "gists_url": "https://api.github.com/users/alisidd/gists{/gist_id}", "starred_url": "https://api.github.com/users/alisidd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alisidd/subscriptions", "organizations_url": "https://api.github.com/users/alisidd/orgs", "repos_url": "https://api.github.com/users/alisidd/repos", "events_url": "https://api.github.com/users/alisidd/events{/privacy}", "received_events_url": "https://api.github.com/users/alisidd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-12T15:25:33Z", "updated_at": "2017-05-12T15:25:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a> I've made the requested changes, and I found quite a few problems with my implementations that required a few changes that I wasn't sure if I should be making, like an addition of an optional parameter to a method in the Optimizer class. It allows for multithreaded processing using one optimizer object. Regardless, I'm sure you can see what else I had to change.</p>\n<p>Furthermore, I tested and validated the implementation using a modified version of mnist_softmax.py with 12 asynchronous workers, and a variance parameter of 5. I found there to be an improvement of around 1.5% in test accuracy over asynchronous stochastic gradient descent, similar to improvements found in the research paper.</p>", "body_text": "@vrv I've made the requested changes, and I found quite a few problems with my implementations that required a few changes that I wasn't sure if I should be making, like an addition of an optional parameter to a method in the Optimizer class. It allows for multithreaded processing using one optimizer object. Regardless, I'm sure you can see what else I had to change.\nFurthermore, I tested and validated the implementation using a modified version of mnist_softmax.py with 12 asynchronous workers, and a variance parameter of 5. I found there to be an improvement of around 1.5% in test accuracy over asynchronous stochastic gradient descent, similar to improvements found in the research paper.", "body": "@vrv I've made the requested changes, and I found quite a few problems with my implementations that required a few changes that I wasn't sure if I should be making, like an addition of an optional parameter to a method in the Optimizer class. It allows for multithreaded processing using one optimizer object. Regardless, I'm sure you can see what else I had to change. \r\n\r\nFurthermore, I tested and validated the implementation using a modified version of mnist_softmax.py with 12 asynchronous workers, and a variance parameter of 5. I found there to be an improvement of around 1.5% in test accuracy over asynchronous stochastic gradient descent, similar to improvements found in the research paper."}