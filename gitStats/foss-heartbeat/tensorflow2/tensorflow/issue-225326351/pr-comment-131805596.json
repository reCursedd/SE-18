{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131805596", "pull_request_review_id": 54818875, "id": 131805596, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMTgwNTU5Ng==", "diff_hunk": "@@ -0,0 +1,256 @@\n+# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+# ==============================================================================\n+\n+\"\"\"DelayCompensatedGradientDescentOptimizer for TensorFlow.\"\"\"\n+from __future__ import absolute_import\n+from __future__ import division\n+from __future__ import print_function\n+\n+from tensorflow.python.framework import ops\n+from tensorflow.python.ops import array_ops\n+from tensorflow.python.ops import math_ops\n+from tensorflow.python.ops import state_ops\n+from tensorflow.python.ops import variables\n+from tensorflow.python.training import optimizer\n+from tensorflow.python.training import training_ops\n+\n+\n+class _RefVariableAsynchronousProcessor(optimizer._RefVariableProcessor):\n+  \"\"\"Processor for Variable.\"\"\"\n+  def update_op_asynchronous(self, optimizer, g, index):\n+    if isinstance(g, ops.Tensor):\n+      return optimizer._apply_dense(g, self._v, index)\n+    else:\n+      assert isinstance(g, ops.IndexedSlices), (\"Gradient \", g, \" is neither a \"\n+                                                \"tensor nor IndexedSlices.\")\n+      # pylint: disable=protected-access\n+      return optimizer._apply_sparse_duplicate_indices(g, self._v, index)\n+\n+\n+class _DenseResourceVariableAsynchronousProcessor(optimizer._DenseResourceVariableProcessor):\n+  \"\"\"Processor for dense ResourceVariables.\"\"\"\n+  def update_op_asynchronous(self, optimizer, g, index):\n+    # pylint: disable=protected-access\n+    if isinstance(g, ops.IndexedSlices):\n+      return optimizer._resource_apply_sparse_duplicate_indices(\n+        g.values, self._v, g.indices, index)\n+    return optimizer._resource_apply_dense(g, self._v, index)\n+\n+\n+def _get_processor(v):\n+  \"\"\"The processor of v.\"\"\"\n+  if v.op.type == \"VarHandleOp\":\n+    return _DenseResourceVariableAsynchronousProcessor(v)\n+  if isinstance(v, variables.Variable):\n+    return _RefVariableAsynchronousProcessor(v)\n+  raise NotImplementedError(\"Trying to optimize unsupported type \", v)\n+\n+\n+class DelayCompensatedGradientDescentOptimizer(optimizer.Optimizer):\n+  \"\"\"Optimizer that implements gradient descent with delay compensation.\n+\n+  See [Zheng, Shuxin, et al., 2016](https://arxiv.org/abs/1609.08326)\n+  ([pdf](https://arxiv.org/pdf/1609.08326.pdf)).\n+  \"\"\"\n+\n+  def __init__(self, learning_rate, variance_parameter, num_workers=1,\n+               use_locking=False, name=\"DelayCompensatedGradientDescent\"):\n+    \"\"\"Construct a new gradient descent optimizer with delay compensation.\n+\n+    Args:\n+      learning_rate: A Tensor or a floating point value.  The learning\n+        rate to use.\n+      variance_parameter: A Tensor or a floating point value. The lambda\n+        value to use.\n+      num_workers: A value to indicate number of workers computing gradients\n+        asynchronously.\n+      use_locking: If True use locks for update operations.\n+      name: Optional name prefix for the operations created when applying\n+        gradients. Defaults to \"DelayCompensatedGradientDescent\".\n+      \"\"\"\n+    if num_workers <= 0:\n+      raise ValueError(\"num_workers must be positive: %s\" % num_workers)\n+    super(DelayCompensatedGradientDescentOptimizer, self).__init__(\n+          use_locking, name)\n+    self._learning_rate = learning_rate\n+    self._lambda = variance_parameter\n+    self._num_workers = num_workers\n+\n+  def minimize(self, loss, global_step=None, var_list=None,", "path": "tensorflow/contrib/opt/python/training/delay_compensated_gradient_descent.py", "position": 91, "original_position": 91, "commit_id": "bee23d372307ec23f95acc65b262b79cfa060d28", "original_commit_id": "bee23d372307ec23f95acc65b262b79cfa060d28", "user": {"login": "ry", "id": 80, "node_id": "MDQ6VXNlcjgw", "avatar_url": "https://avatars1.githubusercontent.com/u/80?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ry", "html_url": "https://github.com/ry", "followers_url": "https://api.github.com/users/ry/followers", "following_url": "https://api.github.com/users/ry/following{/other_user}", "gists_url": "https://api.github.com/users/ry/gists{/gist_id}", "starred_url": "https://api.github.com/users/ry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ry/subscriptions", "organizations_url": "https://api.github.com/users/ry/orgs", "repos_url": "https://api.github.com/users/ry/repos", "events_url": "https://api.github.com/users/ry/events{/privacy}", "received_events_url": "https://api.github.com/users/ry/received_events", "type": "User", "site_admin": false}, "body": "Hello - ", "created_at": "2017-08-08T01:57:58Z", "updated_at": "2017-08-08T02:02:40Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/9551#discussion_r131805596", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9551", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/131805596"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/9551#discussion_r131805596"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/9551"}}, "body_html": "<p>Hello -</p>", "body_text": "Hello -"}