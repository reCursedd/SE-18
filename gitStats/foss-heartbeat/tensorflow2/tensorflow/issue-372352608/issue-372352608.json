{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23150", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23150/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23150/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23150/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23150", "id": 372352608, "node_id": "MDU6SXNzdWUzNzIzNTI2MDg=", "number": 23150, "title": "TPU model doesn't work with tensorflow.python.keras learning rate scheduler.", "user": {"login": "leftys", "id": 1970226, "node_id": "MDQ6VXNlcjE5NzAyMjY=", "avatar_url": "https://avatars2.githubusercontent.com/u/1970226?v=4", "gravatar_id": "", "url": "https://api.github.com/users/leftys", "html_url": "https://github.com/leftys", "followers_url": "https://api.github.com/users/leftys/followers", "following_url": "https://api.github.com/users/leftys/following{/other_user}", "gists_url": "https://api.github.com/users/leftys/gists{/gist_id}", "starred_url": "https://api.github.com/users/leftys/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/leftys/subscriptions", "organizations_url": "https://api.github.com/users/leftys/orgs", "repos_url": "https://api.github.com/users/leftys/repos", "events_url": "https://api.github.com/users/leftys/events{/privacy}", "received_events_url": "https://api.github.com/users/leftys/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097541661, "node_id": "MDU6TGFiZWwxMDk3NTQxNjYx", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:tpus", "name": "comp:tpus", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-10-21T20:17:42Z", "updated_at": "2018-11-13T23:22:13Z", "closed_at": "2018-11-13T23:22:13Z", "author_association": "NONE", "body_html": "<p>When I convert Keras model into TPU model using <code>tensorflow.contrib.tpu.keras_to_tpu_model</code> and try to fit it with learning rate scheduler callback, the training crashes with error <code>ValueError: Optimizer must have a \"lr\" attribute.</code>. I guess the problem is that the keras and tensorflow optimizers use different learning rate variable name, which makes the keras callback fail. The callback in <code>tensorflow.python.keras</code> should be therefore corrected to use the proper variable.</p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Google Colab TPU runtime</li>\n<li>TF 1.12.0rc1</li>\n</ul>\n<p><strong>Code fragment</strong></p>\n<pre><code>    strategy = tensorflow.contrib.tpu.TPUDistributionStrategy(\n        tensorflow.contrib.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\n    )\n    model = tensorflow.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n    model.compile(\n        loss=tensorflow.keras.losses.categorical_crossentropy,\n        optimizer=tensorflow.train.GradientDescentOptimizer(learning_rate=lr_schedule(0)),\n        metrics=['accuracy']\n    )\n    (...)\n   callbacks = [tensorflow.python.keras.callbacks.LearningRateScheduler(lr_schedule_function)]\n</code></pre>\n<p><strong>Describe the current behavior</strong></p>\n<pre><code>Traceback (most recent call last)\n&lt;ipython-input-12-b6a9470abb3b&gt; in &lt;module&gt;()\n    441                         validation_data=(x_test, y_test),\n    442                         epochs=epochs, verbose=1, workers=4,\n--&gt; 443                         callbacks=callbacks)\n    444 \n    445 # Score trained model.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\n   2175         use_multiprocessing=use_multiprocessing,\n   2176         shuffle=shuffle,\n-&gt; 2177         initial_epoch=initial_epoch)\n   2178 \n   2179   def evaluate_generator(self,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\n    141       for m in model.stateful_metric_functions:\n    142         m.reset_states()\n--&gt; 143       callbacks.on_epoch_begin(epoch)\n    144       steps_done = 0\n    145       batch_index = 0\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\n    198     logs = logs or {}\n    199     for callback in self.callbacks:\n--&gt; 200       callback.on_epoch_begin(epoch, logs)\n    201     self._delta_t_batch = 0.\n    202     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\n    768   def on_epoch_begin(self, epoch, logs=None):\n    769     if not hasattr(self.model.optimizer, 'lr'):\n--&gt; 770       raise ValueError('Optimizer must have a \"lr\" attribute.')\n    771     try:  # new API\n    772       lr = float(K.get_value(self.model.optimizer.lr))\n\nValueError: Optimizer must have a \"lr\" attribute.\n</code></pre>", "body_text": "When I convert Keras model into TPU model using tensorflow.contrib.tpu.keras_to_tpu_model and try to fit it with learning rate scheduler callback, the training crashes with error ValueError: Optimizer must have a \"lr\" attribute.. I guess the problem is that the keras and tensorflow optimizers use different learning rate variable name, which makes the keras callback fail. The callback in tensorflow.python.keras should be therefore corrected to use the proper variable.\nSystem information\n\nGoogle Colab TPU runtime\nTF 1.12.0rc1\n\nCode fragment\n    strategy = tensorflow.contrib.tpu.TPUDistributionStrategy(\n        tensorflow.contrib.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\n    )\n    model = tensorflow.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\n    model.compile(\n        loss=tensorflow.keras.losses.categorical_crossentropy,\n        optimizer=tensorflow.train.GradientDescentOptimizer(learning_rate=lr_schedule(0)),\n        metrics=['accuracy']\n    )\n    (...)\n   callbacks = [tensorflow.python.keras.callbacks.LearningRateScheduler(lr_schedule_function)]\n\nDescribe the current behavior\nTraceback (most recent call last)\n<ipython-input-12-b6a9470abb3b> in <module>()\n    441                         validation_data=(x_test, y_test),\n    442                         epochs=epochs, verbose=1, workers=4,\n--> 443                         callbacks=callbacks)\n    444 \n    445 # Score trained model.\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\n   2175         use_multiprocessing=use_multiprocessing,\n   2176         shuffle=shuffle,\n-> 2177         initial_epoch=initial_epoch)\n   2178 \n   2179   def evaluate_generator(self,\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\n    141       for m in model.stateful_metric_functions:\n    142         m.reset_states()\n--> 143       callbacks.on_epoch_begin(epoch)\n    144       steps_done = 0\n    145       batch_index = 0\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\n    198     logs = logs or {}\n    199     for callback in self.callbacks:\n--> 200       callback.on_epoch_begin(epoch, logs)\n    201     self._delta_t_batch = 0.\n    202     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\n\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\n    768   def on_epoch_begin(self, epoch, logs=None):\n    769     if not hasattr(self.model.optimizer, 'lr'):\n--> 770       raise ValueError('Optimizer must have a \"lr\" attribute.')\n    771     try:  # new API\n    772       lr = float(K.get_value(self.model.optimizer.lr))\n\nValueError: Optimizer must have a \"lr\" attribute.", "body": "When I convert Keras model into TPU model using `tensorflow.contrib.tpu.keras_to_tpu_model` and try to fit it with learning rate scheduler callback, the training crashes with error `ValueError: Optimizer must have a \"lr\" attribute.`. I guess the problem is that the keras and tensorflow optimizers use different learning rate variable name, which makes the keras callback fail. The callback in `tensorflow.python.keras` should be therefore corrected to use the proper variable.\r\n\r\n**System information**\r\n- Google Colab TPU runtime\r\n- TF 1.12.0rc1\r\n\r\n**Code fragment**\r\n```\r\n    strategy = tensorflow.contrib.tpu.TPUDistributionStrategy(\r\n        tensorflow.contrib.cluster_resolver.TPUClusterResolver(tpu = 'grpc://' + os.environ['COLAB_TPU_ADDR'])\r\n    )\r\n    model = tensorflow.contrib.tpu.keras_to_tpu_model(model, strategy=strategy)\r\n    model.compile(\r\n        loss=tensorflow.keras.losses.categorical_crossentropy,\r\n        optimizer=tensorflow.train.GradientDescentOptimizer(learning_rate=lr_schedule(0)),\r\n        metrics=['accuracy']\r\n    )\r\n    (...)\r\n   callbacks = [tensorflow.python.keras.callbacks.LearningRateScheduler(lr_schedule_function)]\r\n```\r\n\r\n**Describe the current behavior**\r\n```\r\nTraceback (most recent call last)\r\n<ipython-input-12-b6a9470abb3b> in <module>()\r\n    441                         validation_data=(x_test, y_test),\r\n    442                         epochs=epochs, verbose=1, workers=4,\r\n--> 443                         callbacks=callbacks)\r\n    444 \r\n    445 # Score trained model.\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py in fit_generator(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n   2175         use_multiprocessing=use_multiprocessing,\r\n   2176         shuffle=shuffle,\r\n-> 2177         initial_epoch=initial_epoch)\r\n   2178 \r\n   2179   def evaluate_generator(self,\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py in fit_generator(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\r\n    141       for m in model.stateful_metric_functions:\r\n    142         m.reset_states()\r\n--> 143       callbacks.on_epoch_begin(epoch)\r\n    144       steps_done = 0\r\n    145       batch_index = 0\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\r\n    198     logs = logs or {}\r\n    199     for callback in self.callbacks:\r\n--> 200       callback.on_epoch_begin(epoch, logs)\r\n    201     self._delta_t_batch = 0.\r\n    202     self._delta_ts_batch_begin = deque([], maxlen=self.queue_length)\r\n\r\n/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/callbacks.py in on_epoch_begin(self, epoch, logs)\r\n    768   def on_epoch_begin(self, epoch, logs=None):\r\n    769     if not hasattr(self.model.optimizer, 'lr'):\r\n--> 770       raise ValueError('Optimizer must have a \"lr\" attribute.')\r\n    771     try:  # new API\r\n    772       lr = float(K.get_value(self.model.optimizer.lr))\r\n\r\nValueError: Optimizer must have a \"lr\" attribute.\r\n```"}