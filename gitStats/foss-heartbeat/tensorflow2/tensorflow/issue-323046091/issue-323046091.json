{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19283", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19283/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19283/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19283/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19283", "id": 323046091, "node_id": "MDU6SXNzdWUzMjMwNDYwOTE=", "number": 19283, "title": "Tensor with GPUBFCAllocator generate Segfaults on tensorflow::ops", "user": {"login": "tkurmann", "id": 4687000, "node_id": "MDQ6VXNlcjQ2ODcwMDA=", "avatar_url": "https://avatars3.githubusercontent.com/u/4687000?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tkurmann", "html_url": "https://github.com/tkurmann", "followers_url": "https://api.github.com/users/tkurmann/followers", "following_url": "https://api.github.com/users/tkurmann/following{/other_user}", "gists_url": "https://api.github.com/users/tkurmann/gists{/gist_id}", "starred_url": "https://api.github.com/users/tkurmann/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tkurmann/subscriptions", "organizations_url": "https://api.github.com/users/tkurmann/orgs", "repos_url": "https://api.github.com/users/tkurmann/repos", "events_url": "https://api.github.com/users/tkurmann/events{/privacy}", "received_events_url": "https://api.github.com/users/tkurmann/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ekelsen", "id": 2533174, "node_id": "MDQ6VXNlcjI1MzMxNzQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/2533174?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ekelsen", "html_url": "https://github.com/ekelsen", "followers_url": "https://api.github.com/users/ekelsen/followers", "following_url": "https://api.github.com/users/ekelsen/following{/other_user}", "gists_url": "https://api.github.com/users/ekelsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/ekelsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ekelsen/subscriptions", "organizations_url": "https://api.github.com/users/ekelsen/orgs", "repos_url": "https://api.github.com/users/ekelsen/repos", "events_url": "https://api.github.com/users/ekelsen/events{/privacy}", "received_events_url": "https://api.github.com/users/ekelsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-05-15T02:37:04Z", "updated_at": "2018-11-21T02:33:30Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: r1.8</li>\n<li><strong>Python version</strong>:  2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.18</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0 / 7.1</li>\n<li><strong>GPU model and memory</strong>: nvidia p6000</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Using the C++ api, when a tensorflow::Tensor is allocated using GPUBFCAllocator, all further operations (such as tensorflow::op::Identity) generate a segfault. Memory is properly allocated (tested with cudaMemcpy). If the allocator is removed, or switched to tensorflow::cpu_allocator(), the error does not occur, but the memory is not allocated on the gpu (memcpy fails). My final goal is to obtain the cuda pointer of the tensor to fill the tensor with data manually.</p>\n<h3>Source code / logs</h3>\n<p>Reproducible code:</p>\n<pre><code>  auto root = tensorflow::Scope::NewRootScope().WithDevice(\"/gpu:0\");\n  tensorflow::ClientSession* session = new tensorflow::ClientSession(root);\n  \n  tensorflow::VisitableAllocator* gpu_allocator;\n  gpu_allocator = new tensorflow::GPUBFCAllocator(tensorflow::CudaGpuId(0), sizeof(float) * 512 * 640 * 3, \"bfc_gpu_0\");\n  gpu_allocator = new tensorflow::GPUcudaMallocAllocator(gpu_allocator, tensorflow::CudaGpuId(0));\n\n  void* myp = gpu_allocator-&gt;AllocateRaw(1,512*640*3*4);\n  tensorflow::Tensor *myTensor = new tensorflow::Tensor(gpu_allocator,tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 512, 640, 3}));\n\n  checkCudaErrors(cudaMemcpy(tensorflow::DMAHelper::base(myTensor), myp, sizeof(float) * 512 * 640 *3, cudaMemcpyDeviceToDevice));\n\n  auto identity = tensorflow::ops::Identity(root.WithOpName(\"init\"), *myTensor);\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): r1.8\nPython version:  2.7\nBazel version (if compiling from source): 0.18\nGCC/Compiler version (if compiling from source): 5.4\nCUDA/cuDNN version: 9.0 / 7.1\nGPU model and memory: nvidia p6000\nExact command to reproduce:\n\nDescribe the problem\nUsing the C++ api, when a tensorflow::Tensor is allocated using GPUBFCAllocator, all further operations (such as tensorflow::op::Identity) generate a segfault. Memory is properly allocated (tested with cudaMemcpy). If the allocator is removed, or switched to tensorflow::cpu_allocator(), the error does not occur, but the memory is not allocated on the gpu (memcpy fails). My final goal is to obtain the cuda pointer of the tensor to fill the tensor with data manually.\nSource code / logs\nReproducible code:\n  auto root = tensorflow::Scope::NewRootScope().WithDevice(\"/gpu:0\");\n  tensorflow::ClientSession* session = new tensorflow::ClientSession(root);\n  \n  tensorflow::VisitableAllocator* gpu_allocator;\n  gpu_allocator = new tensorflow::GPUBFCAllocator(tensorflow::CudaGpuId(0), sizeof(float) * 512 * 640 * 3, \"bfc_gpu_0\");\n  gpu_allocator = new tensorflow::GPUcudaMallocAllocator(gpu_allocator, tensorflow::CudaGpuId(0));\n\n  void* myp = gpu_allocator->AllocateRaw(1,512*640*3*4);\n  tensorflow::Tensor *myTensor = new tensorflow::Tensor(gpu_allocator,tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 512, 640, 3}));\n\n  checkCudaErrors(cudaMemcpy(tensorflow::DMAHelper::base(myTensor), myp, sizeof(float) * 512 * 640 *3, cudaMemcpyDeviceToDevice));\n\n  auto identity = tensorflow::ops::Identity(root.WithOpName(\"init\"), *myTensor);", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: r1.8\r\n- **Python version**:  2.7\r\n- **Bazel version (if compiling from source)**: 0.18\r\n- **GCC/Compiler version (if compiling from source)**: 5.4\r\n- **CUDA/cuDNN version**: 9.0 / 7.1\r\n- **GPU model and memory**: nvidia p6000\r\n- **Exact command to reproduce**: \r\n\r\n\r\n### Describe the problem\r\nUsing the C++ api, when a tensorflow::Tensor is allocated using GPUBFCAllocator, all further operations (such as tensorflow::op::Identity) generate a segfault. Memory is properly allocated (tested with cudaMemcpy). If the allocator is removed, or switched to tensorflow::cpu_allocator(), the error does not occur, but the memory is not allocated on the gpu (memcpy fails). My final goal is to obtain the cuda pointer of the tensor to fill the tensor with data manually. \r\n\r\n### Source code / logs\r\nReproducible code: \r\n```\r\n  auto root = tensorflow::Scope::NewRootScope().WithDevice(\"/gpu:0\");\r\n  tensorflow::ClientSession* session = new tensorflow::ClientSession(root);\r\n  \r\n  tensorflow::VisitableAllocator* gpu_allocator;\r\n  gpu_allocator = new tensorflow::GPUBFCAllocator(tensorflow::CudaGpuId(0), sizeof(float) * 512 * 640 * 3, \"bfc_gpu_0\");\r\n  gpu_allocator = new tensorflow::GPUcudaMallocAllocator(gpu_allocator, tensorflow::CudaGpuId(0));\r\n\r\n  void* myp = gpu_allocator->AllocateRaw(1,512*640*3*4);\r\n  tensorflow::Tensor *myTensor = new tensorflow::Tensor(gpu_allocator,tensorflow::DT_FLOAT, tensorflow::TensorShape({1, 512, 640, 3}));\r\n\r\n  checkCudaErrors(cudaMemcpy(tensorflow::DMAHelper::base(myTensor), myp, sizeof(float) * 512 * 640 *3, cudaMemcpyDeviceToDevice));\r\n\r\n  auto identity = tensorflow::ops::Identity(root.WithOpName(\"init\"), *myTensor);\r\n```\r\n\r\n"}