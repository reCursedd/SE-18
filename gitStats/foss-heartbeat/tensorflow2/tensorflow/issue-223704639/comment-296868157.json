{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/296868157", "html_url": "https://github.com/tensorflow/tensorflow/issues/9401#issuecomment-296868157", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9401", "id": 296868157, "node_id": "MDEyOklzc3VlQ29tbWVudDI5Njg2ODE1Nw==", "user": {"login": "Songweiping", "id": 11703156, "node_id": "MDQ6VXNlcjExNzAzMTU2", "avatar_url": "https://avatars2.githubusercontent.com/u/11703156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Songweiping", "html_url": "https://github.com/Songweiping", "followers_url": "https://api.github.com/users/Songweiping/followers", "following_url": "https://api.github.com/users/Songweiping/following{/other_user}", "gists_url": "https://api.github.com/users/Songweiping/gists{/gist_id}", "starred_url": "https://api.github.com/users/Songweiping/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Songweiping/subscriptions", "organizations_url": "https://api.github.com/users/Songweiping/orgs", "repos_url": "https://api.github.com/users/Songweiping/repos", "events_url": "https://api.github.com/users/Songweiping/events{/privacy}", "received_events_url": "https://api.github.com/users/Songweiping/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-25T01:45:49Z", "updated_at": "2017-04-25T01:45:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Update:<br>\nActually with TF 1.0.1(built from binary),  in code:</p>\n<pre><code>with tf.variable_scope('rnn1'):\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\nwith tf.variable_scope('rnn2'):\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n</code></pre>\n<p>it's <em>doesn't reuse</em> the RNNCell \"cell\", the weights and biases have different names as following(just first cell here).</p>\n<pre><code>rnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\nrnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n</code></pre>\n<p>In this simple case, we can put these two dynamic unrolling in the same variable scope if we wanna reuse RNNCell. Like:</p>\n<pre><code>with tf.variable_scope('rnn1'):\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n    tf.get_variable_scope().reuse_variables()\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n</code></pre>\n<p>There will be exactly one set of RNN parameters. But under some more complicated situations, where other wrappers may be added to original RNNCell, it's hard to reuse one BasicLSTMCell.<br>\nFor example:</p>\n<ul>\n<li>\n<p>\"rnn\" will be added to variable names if we use \"dynamic_rnn\".</p>\n</li>\n<li>\n<p>\"attention\" will be added to variable names if we use \"AttentionWrapper\".</p>\n</li>\n<li>\n<p>\"decoder\" will be added to variable names if we use some instances of \"Decoder\".</p>\n</li>\n<li>\n<p>...</p>\n</li>\n</ul>\n<p>Looking forward to more suggestions on this.</p>", "body_text": "Update:\nActually with TF 1.0.1(built from binary),  in code:\nwith tf.variable_scope('rnn1'):\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\nwith tf.variable_scope('rnn2'):\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n\nit's doesn't reuse the RNNCell \"cell\", the weights and biases have different names as following(just first cell here).\nrnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\nrnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\n\nIn this simple case, we can put these two dynamic unrolling in the same variable scope if we wanna reuse RNNCell. Like:\nwith tf.variable_scope('rnn1'):\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n    tf.get_variable_scope().reuse_variables()\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\n\nThere will be exactly one set of RNN parameters. But under some more complicated situations, where other wrappers may be added to original RNNCell, it's hard to reuse one BasicLSTMCell.\nFor example:\n\n\n\"rnn\" will be added to variable names if we use \"dynamic_rnn\".\n\n\n\"attention\" will be added to variable names if we use \"AttentionWrapper\".\n\n\n\"decoder\" will be added to variable names if we use some instances of \"Decoder\".\n\n\n...\n\n\nLooking forward to more suggestions on this.", "body": "Update:\r\nActually with TF 1.0.1(built from binary),  in code:\r\n```\r\nwith tf.variable_scope('rnn1'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\nwith tf.variable_scope('rnn2'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n```\r\nit's _doesn't reuse_ the RNNCell \"cell\", the weights and biases have different names as following(just first cell here).\r\n```\r\nrnn1/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\r\nrnn2/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/weights:0\r\n```\r\nIn this simple case, we can put these two dynamic unrolling in the same variable scope if we wanna reuse RNNCell. Like:\r\n```\r\nwith tf.variable_scope('rnn1'):\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n    tf.get_variable_scope().reuse_variables()\r\n    _, state = tf.nn.dynamic_rnn(cell, inputs=inputs, initial_state=zero_state)\r\n```\r\nThere will be exactly one set of RNN parameters. But under some more complicated situations, where other wrappers may be added to original RNNCell, it's hard to reuse one BasicLSTMCell.\r\nFor example:\r\n\r\n- \"rnn\" will be added to variable names if we use \"dynamic_rnn\".\r\n\r\n- \"attention\" will be added to variable names if we use \"AttentionWrapper\".\r\n\r\n- \"decoder\" will be added to variable names if we use some instances of \"Decoder\".\r\n\r\n- ...\r\n\r\nLooking forward to more suggestions on this."}