{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22977", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22977/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22977/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22977/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22977", "id": 369982133, "node_id": "MDU6SXNzdWUzNjk5ODIxMzM=", "number": 22977, "title": "gradients of tf.fake_quant_with_min_max_vars function.", "user": {"login": "sicnarf1a", "id": 25295124, "node_id": "MDQ6VXNlcjI1Mjk1MTI0", "avatar_url": "https://avatars0.githubusercontent.com/u/25295124?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sicnarf1a", "html_url": "https://github.com/sicnarf1a", "followers_url": "https://api.github.com/users/sicnarf1a/followers", "following_url": "https://api.github.com/users/sicnarf1a/following{/other_user}", "gists_url": "https://api.github.com/users/sicnarf1a/gists{/gist_id}", "starred_url": "https://api.github.com/users/sicnarf1a/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sicnarf1a/subscriptions", "organizations_url": "https://api.github.com/users/sicnarf1a/orgs", "repos_url": "https://api.github.com/users/sicnarf1a/repos", "events_url": "https://api.github.com/users/sicnarf1a/events{/privacy}", "received_events_url": "https://api.github.com/users/sicnarf1a/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097547147, "node_id": "MDU6TGFiZWwxMDk3NTQ3MTQ3", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:ops", "name": "comp:ops", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-10-15T03:08:01Z", "updated_at": "2018-11-21T17:45:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>OS Platform and Distribution : Linux Ubuntu 16.04</strong></li>\n<li><strong>TensorFlow version : 1.6.0</strong></li>\n<li><strong>Python version : 3.6</strong></li>\n<li><strong>CUDA/cuDNN version : CUDA Version 9.0.176/ CUDNN 7.0.5</strong>:</li>\n<li><strong>GPU model and memory : TITAN Xp/12196MiB</strong></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I assume that <strong>tf.fake_quant_with_min_max_vars function</strong> can not be differentiable due to the fact that quantization should be working based on threshold, such as round, or sign function. And it means that we can not get the gradient of variables due to the nature of chain rule.</p>\n<p>In these days, several tricks are researched, and one of the most popular method is called <strong>'straight-through-estimator'</strong>, literally passing through the gradient itself.</p>\n<p>So I tested <strong>tf.fake_quant_with_min_max_vars</strong> to apply <strong>'straight-through-estimator'</strong> functionality. Most results get a gradient 1. This means it works well. Sometimes, however, the result is incorrect. More detailed errors are described below.</p>\n<h3>Source code / logs</h3>\n<p>Here's some snippet code<br>\n<code>x = tf.cast(np.random.normal(0, 1, (10), tf.float32)</code><br>\n<code>x_q = tf.fake_quant_with_min_max_vars(x, min=tf.reduce_min(x), max=tf.reduce_max(x), num_bits=3)</code><br>\n<code>grad = tf.gradients(x_q, x)</code></p>\n<p>In that case, sometimes <strong>grad</strong> get weird results:<br>\n<code>[array([1., 1., 1., 0., 1., 1., 2., 1., 1., 1.], dtype=float32)]</code><br>\nAnd non-zero gradient values have a common quantization value. In this case, the <strong>x_q</strong> is:<br>\n<code>array([0., 0., 0., -2.289673, 2.289673, 0., -2.289673, 0., 4.570346, 0.], dtype=float32)</code></p>", "body_text": "System information\n\nOS Platform and Distribution : Linux Ubuntu 16.04\nTensorFlow version : 1.6.0\nPython version : 3.6\nCUDA/cuDNN version : CUDA Version 9.0.176/ CUDNN 7.0.5:\nGPU model and memory : TITAN Xp/12196MiB\n\nDescribe the problem\nI assume that tf.fake_quant_with_min_max_vars function can not be differentiable due to the fact that quantization should be working based on threshold, such as round, or sign function. And it means that we can not get the gradient of variables due to the nature of chain rule.\nIn these days, several tricks are researched, and one of the most popular method is called 'straight-through-estimator', literally passing through the gradient itself.\nSo I tested tf.fake_quant_with_min_max_vars to apply 'straight-through-estimator' functionality. Most results get a gradient 1. This means it works well. Sometimes, however, the result is incorrect. More detailed errors are described below.\nSource code / logs\nHere's some snippet code\nx = tf.cast(np.random.normal(0, 1, (10), tf.float32)\nx_q = tf.fake_quant_with_min_max_vars(x, min=tf.reduce_min(x), max=tf.reduce_max(x), num_bits=3)\ngrad = tf.gradients(x_q, x)\nIn that case, sometimes grad get weird results:\n[array([1., 1., 1., 0., 1., 1., 2., 1., 1., 1.], dtype=float32)]\nAnd non-zero gradient values have a common quantization value. In this case, the x_q is:\narray([0., 0., 0., -2.289673, 2.289673, 0., -2.289673, 0., 4.570346, 0.], dtype=float32)", "body": "### System information\r\n- **OS Platform and Distribution : Linux Ubuntu 16.04**\r\n- **TensorFlow version : 1.6.0**\r\n- **Python version : 3.6**\r\n- **CUDA/cuDNN version : CUDA Version 9.0.176/ CUDNN 7.0.5**:\r\n- **GPU model and memory : TITAN Xp/12196MiB**\r\n\r\n### Describe the problem\r\nI assume that **tf.fake_quant_with_min_max_vars function** can not be differentiable due to the fact that quantization should be working based on threshold, such as round, or sign function. And it means that we can not get the gradient of variables due to the nature of chain rule.\r\n\r\nIn these days, several tricks are researched, and one of the most popular method is called **'straight-through-estimator'**, literally passing through the gradient itself.\r\n\r\nSo I tested **tf.fake_quant_with_min_max_vars** to apply **'straight-through-estimator'** functionality. Most results get a gradient 1. This means it works well. Sometimes, however, the result is incorrect. More detailed errors are described below.\r\n\r\n### Source code / logs\r\nHere's some snippet code\r\n`x = tf.cast(np.random.normal(0, 1, (10), tf.float32)`\r\n`x_q = tf.fake_quant_with_min_max_vars(x, min=tf.reduce_min(x), max=tf.reduce_max(x), num_bits=3)`\r\n`grad = tf.gradients(x_q, x)`\r\n\r\nIn that case, sometimes **grad** get weird results:\r\n`[array([1., 1., 1., 0., 1., 1., 2., 1., 1., 1.], dtype=float32)]`\r\nAnd non-zero gradient values have a common quantization value. In this case, the **x_q** is:\r\n`array([0., 0., 0., -2.289673, 2.289673, 0., -2.289673, 0., 4.570346, 0.], dtype=float32)`"}