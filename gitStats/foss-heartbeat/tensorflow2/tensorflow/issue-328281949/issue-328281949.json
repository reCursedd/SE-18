{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19676", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19676/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19676/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19676/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19676", "id": 328281949, "node_id": "MDU6SXNzdWUzMjgyODE5NDk=", "number": 19676, "title": "Error trying to build for macOS with GPU support: \"no toolchain corresponding to 'local_darwin' found for cpu 'darwin' \"", "user": {"login": "ghost", "id": 10137, "node_id": "MDQ6VXNlcjEwMTM3", "avatar_url": "https://avatars3.githubusercontent.com/u/10137?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ghost", "html_url": "https://github.com/ghost", "followers_url": "https://api.github.com/users/ghost/followers", "following_url": "https://api.github.com/users/ghost/following{/other_user}", "gists_url": "https://api.github.com/users/ghost/gists{/gist_id}", "starred_url": "https://api.github.com/users/ghost/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ghost/subscriptions", "organizations_url": "https://api.github.com/users/ghost/orgs", "repos_url": "https://api.github.com/users/ghost/repos", "events_url": "https://api.github.com/users/ghost/events{/privacy}", "received_events_url": "https://api.github.com/users/ghost/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, {"login": "gunan", "id": 7946809, "node_id": "MDQ6VXNlcjc5NDY4MDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/7946809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gunan", "html_url": "https://github.com/gunan", "followers_url": "https://api.github.com/users/gunan/followers", "following_url": "https://api.github.com/users/gunan/following{/other_user}", "gists_url": "https://api.github.com/users/gunan/gists{/gist_id}", "starred_url": "https://api.github.com/users/gunan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gunan/subscriptions", "organizations_url": "https://api.github.com/users/gunan/orgs", "repos_url": "https://api.github.com/users/gunan/repos", "events_url": "https://api.github.com/users/gunan/events{/privacy}", "received_events_url": "https://api.github.com/users/gunan/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-05-31T20:17:30Z", "updated_at": "2018-06-27T18:11:53Z", "closed_at": "2018-06-05T17:49:11Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: N/A</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS 'High Sierra' Version 10.13.4 (17E202)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: source</li>\n<li><strong>TensorFlow version (use command below)</strong>: N/A, attempting to compile at <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/e365deab1333005c8aa186632f160c1bfd4485f8/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/e365deab1333005c8aa186632f160c1bfd4485f8\"><tt>e365dea</tt></a> with minimal local changes (see below)</li>\n<li><strong>Python version</strong>: 2.7.15</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.13.1-homebrew</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: Apple LLVM version 8.1.0 (clang-802.0.42)</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda_9.2.64_mac with cuda_9.2.64.1_mac, cudnn-9.2-osx-x64-v7.1</li>\n<li><strong>GPU model and memory</strong>: NVIDIA GeForce GT 750M with 2 GB device memory, CUDA Compute Capability 3.0</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<pre><code>./configure\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\n</code></pre>\n<h3>Describe the problem</h3>\n<p>This is a re-occurrence of <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"220429375\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/9072\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/9072/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/9072\">#9072</a>, except that the solutions mentioned there (not using clang as the CUDA compiler, using CommandLineTools) do not resolve the problem.</p>\n<p>To configure, I selected the following:</p>\n<pre><code>You have bazel 0.13.1-homebrew installed.\nPlease specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: \n\n\nFound possible Python library paths:\n  /usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\n\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:  \nGoogle Cloud Platform support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \nHadoop File System support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: \nAmazon S3 File System support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: \nApache Kafka Platform support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \nNo XLA JIT support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with GDR support? [y/N]: \nNo GDR support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with VERBS support? [y/N]: \nNo VERBS support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \nNo OpenCL SYCL support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\nCUDA support will be enabled for TensorFlow.\n\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2\n\n\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \n\n\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4\n\n\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n\n\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]3.0\n\n\nDo you want to use clang as CUDA compiler? [y/N]: \nnvcc will be used as CUDA compiler.\n\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \n\n\nDo you wish to build TensorFlow with MPI support? [y/N]: \nNo MPI support will be enabled for TensorFlow.\n\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \n\n\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \nNot configuring the WORKSPACE for Android builds.\n\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=&lt;&gt;\" to your build command. See tools/bazel.rc for more details.\n\t--config=mkl         \t# Build with MKL support.\n\t--config=monolithic  \t# Config for mostly static monolithic build.\nConfiguration finished\n</code></pre>\n<p>To build, I try:</p>\n<pre><code>bazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\n</code></pre>\n<p>However, this results in the error:</p>\n<pre><code>Starting local Bazel server and connecting to it...\n............\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'\nINFO: Elapsed time: 0.903s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (2 packages loaded)\n</code></pre>\n<p>The output of <code>xcode-select -p</code> is:</p>\n<pre><code>/Library/Developer/CommandLineTools\n</code></pre>\n<p>The output of <code>/usr/bin/gcc --version</code> is:</p>\n<pre><code>Configured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\nApple LLVM version 8.1.0 (clang-802.0.42)\nTarget: x86_64-apple-darwin17.5.0\nThread model: posix\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\n</code></pre>\n<p>I am able to build &amp; run the 'deviceQuery' CUDA SDK sample without issue.</p>\n<h3>Source code / logs</h3>\n<p>The only local changes from <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/e365deab1333005c8aa186632f160c1bfd4485f8/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/e365deab1333005c8aa186632f160c1bfd4485f8\"><tt>e365dea</tt></a> I have are:</p>\n<div class=\"highlight highlight-source-diff\"><pre><span class=\"pl-c1\">diff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc</span>\nindex a561d918bd..46c91b4511 100644\n<span class=\"pl-md\">--- a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc</span>\n<span class=\"pl-mi1\">+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc</span>\n<span class=\"pl-mdr\">@@ -69,7 +69,7 @@</span> __global__ void concat_variable_kernel(\n   IntType num_inputs = input_ptr_data.size;\n \n   // verbose declaration needed due to template\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  extern __shared__ __align__(sizeof(T)) unsigned char smem[];</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  extern __shared__ __align__(sizeof(T) &gt; 16 ? sizeof(T) : 16) unsigned char smem[];</span>\n   IntType* smem_col_scan = reinterpret_cast&lt;IntType*&gt;(smem);\n \n   if (useSmem) {\n<span class=\"pl-c1\">diff --git a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc</span>\nindex 5390222b3a..fcbd733614 100644\n<span class=\"pl-md\">--- a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc</span>\n<span class=\"pl-mi1\">+++ b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc</span>\n<span class=\"pl-mdr\">@@ -172,7 +172,7 @@</span> __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNHWCSmall(\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\n   // Holds block plus halo and filter data for blockDim.x depths.\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  extern __shared__ __align__(sizeof(T) &gt; 16 ? sizeof(T) : 16) unsigned char shared_memory[];</span>\n   T* const shared_data = reinterpret_cast&lt;T*&gt;(shared_memory);\n \n   const int num_batches = args.batch;\n<span class=\"pl-mdr\">@@ -452,7 +452,7 @@</span> __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNCHWSmall(\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\n   // Holds block plus halo and filter data for blockDim.z depths.\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  extern __shared__ __align__(sizeof(T) &gt; 16 ? sizeof(T) : 16) unsigned char shared_memory[];</span>\n   T* const shared_data = reinterpret_cast&lt;T*&gt;(shared_memory);\n \n   const int num_batches = args.batch;\n<span class=\"pl-mdr\">@@ -1118,7 +1118,7 @@</span> __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNHWCSmall(\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.z));\n   // Holds block plus halo and filter data for blockDim.x depths.\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  extern __shared__ __align__(sizeof(T) &gt; 16 ? sizeof(T) : 16) unsigned char shared_memory[];</span>\n   T* const shared_data = reinterpret_cast&lt;T*&gt;(shared_memory);\n \n   const int num_batches = args.batch;\n<span class=\"pl-mdr\">@@ -1388,7 +1388,7 @@</span> __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNCHWSmall(\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.x));\n   // Holds block plus halo and filter data for blockDim.z depths.\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  extern __shared__ __align__(sizeof(T) &gt; 16 ? sizeof(T) : 16) unsigned char shared_memory[];</span>\n   T* const shared_data = reinterpret_cast&lt;T*&gt;(shared_memory);\n \n   const int num_batches = args.batch;\n<span class=\"pl-c1\">diff --git a/tensorflow/core/kernels/split_lib_gpu.cu.cc b/tensorflow/core/kernels/split_lib_gpu.cu.cc</span>\nindex 393818730b..58a1294005 100644\n<span class=\"pl-md\">--- a/tensorflow/core/kernels/split_lib_gpu.cu.cc</span>\n<span class=\"pl-mi1\">+++ b/tensorflow/core/kernels/split_lib_gpu.cu.cc</span>\n<span class=\"pl-mdr\">@@ -121,7 +121,7 @@</span> __global__ void split_v_kernel(const T* input_ptr,\n   int num_outputs = output_ptr_data.size;\n \n   // verbose declaration needed due to template\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>  extern __shared__ __align__(sizeof(T)) unsigned char smem[];</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>  extern __shared__ __align__(sizeof(T) &gt; 16 ? sizeof(T) : 16) unsigned char smem[];</span>\n   IntType* smem_col_scan = reinterpret_cast&lt;IntType*&gt;(smem);\n \n   if (useSmem) {\n<span class=\"pl-c1\">diff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl</span>\nindex 2a37c65bc7..43446dd99b 100644\n<span class=\"pl-md\">--- a/third_party/gpus/cuda/BUILD.tpl</span>\n<span class=\"pl-mi1\">+++ b/third_party/gpus/cuda/BUILD.tpl</span>\n<span class=\"pl-mdr\">@@ -110,7 +110,7 @@</span> cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    linkopts = [\"-lgomp\"],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    #linkopts = [\"-lgomp\"],</span>\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )\n<span class=\"pl-c1\">diff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD</span>\nindex 4cb8380938..d025c4f3aa 100644\n<span class=\"pl-md\">--- a/third_party/toolchains/gpus/cuda/BUILD</span>\n<span class=\"pl-mi1\">+++ b/third_party/toolchains/gpus/cuda/BUILD</span>\n<span class=\"pl-mdr\">@@ -115,7 +115,7 @@</span> cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n<span class=\"pl-md\"><span class=\"pl-md\">-</span>    linkopts = [\"-lgomp\"],</span>\n<span class=\"pl-mi1\"><span class=\"pl-mi1\">+</span>    #linkopts = [\"-lgomp\"],</span>\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): N/A\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 'High Sierra' Version 10.13.4 (17E202)\nTensorFlow installed from (source or binary): source\nTensorFlow version (use command below): N/A, attempting to compile at e365dea with minimal local changes (see below)\nPython version: 2.7.15\nBazel version (if compiling from source): 0.13.1-homebrew\nGCC/Compiler version (if compiling from source): Apple LLVM version 8.1.0 (clang-802.0.42)\nCUDA/cuDNN version: cuda_9.2.64_mac with cuda_9.2.64.1_mac, cudnn-9.2-osx-x64-v7.1\nGPU model and memory: NVIDIA GeForce GT 750M with 2 GB device memory, CUDA Compute Capability 3.0\nExact command to reproduce:\n\n./configure\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\n\nDescribe the problem\nThis is a re-occurrence of #9072, except that the solutions mentioned there (not using clang as the CUDA compiler, using CommandLineTools) do not resolve the problem.\nTo configure, I selected the following:\nYou have bazel 0.13.1-homebrew installed.\nPlease specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: \n\n\nFound possible Python library paths:\n  /usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\n\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:  \nGoogle Cloud Platform support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \nHadoop File System support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: \nAmazon S3 File System support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: \nApache Kafka Platform support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \nNo XLA JIT support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with GDR support? [y/N]: \nNo GDR support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with VERBS support? [y/N]: \nNo VERBS support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \nNo OpenCL SYCL support will be enabled for TensorFlow.\n\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\nCUDA support will be enabled for TensorFlow.\n\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2\n\n\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \n\n\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4\n\n\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\n\n\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]3.0\n\n\nDo you want to use clang as CUDA compiler? [y/N]: \nnvcc will be used as CUDA compiler.\n\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \n\n\nDo you wish to build TensorFlow with MPI support? [y/N]: \nNo MPI support will be enabled for TensorFlow.\n\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \n\n\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \nNot configuring the WORKSPACE for Android builds.\n\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\n\t--config=mkl         \t# Build with MKL support.\n\t--config=monolithic  \t# Config for mostly static monolithic build.\nConfiguration finished\n\nTo build, I try:\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\n\nHowever, this results in the error:\nStarting local Bazel server and connecting to it...\n............\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\nERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'\nINFO: Elapsed time: 0.903s\nINFO: 0 processes.\nFAILED: Build did NOT complete successfully (2 packages loaded)\n\nThe output of xcode-select -p is:\n/Library/Developer/CommandLineTools\n\nThe output of /usr/bin/gcc --version is:\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\nApple LLVM version 8.1.0 (clang-802.0.42)\nTarget: x86_64-apple-darwin17.5.0\nThread model: posix\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\n\nI am able to build & run the 'deviceQuery' CUDA SDK sample without issue.\nSource code / logs\nThe only local changes from e365dea I have are:\ndiff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\nindex a561d918bd..46c91b4511 100644\n--- a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\n+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\n@@ -69,7 +69,7 @@ __global__ void concat_variable_kernel(\n   IntType num_inputs = input_ptr_data.size;\n \n   // verbose declaration needed due to template\n-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char smem[];\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\n \n   if (useSmem) {\ndiff --git a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\nindex 5390222b3a..fcbd733614 100644\n--- a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\n+++ b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\n@@ -172,7 +172,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNHWCSmall(\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\n   // Holds block plus halo and filter data for blockDim.x depths.\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\n \n   const int num_batches = args.batch;\n@@ -452,7 +452,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNCHWSmall(\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\n   // Holds block plus halo and filter data for blockDim.z depths.\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\n \n   const int num_batches = args.batch;\n@@ -1118,7 +1118,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNHWCSmall(\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.z));\n   // Holds block plus halo and filter data for blockDim.x depths.\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\n \n   const int num_batches = args.batch;\n@@ -1388,7 +1388,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNCHWSmall(\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.x));\n   // Holds block plus halo and filter data for blockDim.z depths.\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\n \n   const int num_batches = args.batch;\ndiff --git a/tensorflow/core/kernels/split_lib_gpu.cu.cc b/tensorflow/core/kernels/split_lib_gpu.cu.cc\nindex 393818730b..58a1294005 100644\n--- a/tensorflow/core/kernels/split_lib_gpu.cu.cc\n+++ b/tensorflow/core/kernels/split_lib_gpu.cu.cc\n@@ -121,7 +121,7 @@ __global__ void split_v_kernel(const T* input_ptr,\n   int num_outputs = output_ptr_data.size;\n \n   // verbose declaration needed due to template\n-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char smem[];\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\n \n   if (useSmem) {\ndiff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl\nindex 2a37c65bc7..43446dd99b 100644\n--- a/third_party/gpus/cuda/BUILD.tpl\n+++ b/third_party/gpus/cuda/BUILD.tpl\n@@ -110,7 +110,7 @@ cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n-    linkopts = [\"-lgomp\"],\n+    #linkopts = [\"-lgomp\"],\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )\ndiff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD\nindex 4cb8380938..d025c4f3aa 100644\n--- a/third_party/toolchains/gpus/cuda/BUILD\n+++ b/third_party/toolchains/gpus/cuda/BUILD\n@@ -115,7 +115,7 @@ cc_library(\n         \".\",\n         \"cuda/include\",\n     ],\n-    linkopts = [\"-lgomp\"],\n+    #linkopts = [\"-lgomp\"],\n     linkstatic = 1,\n     visibility = [\"//visibility:public\"],\n )", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: N/A\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 'High Sierra' Version 10.13.4 (17E202)\r\n- **TensorFlow installed from (source or binary)**: source\r\n- **TensorFlow version (use command below)**: N/A, attempting to compile at e365deab1333005c8aa186632f160c1bfd4485f8 with minimal local changes (see below)\r\n- **Python version**: 2.7.15\r\n- **Bazel version (if compiling from source)**: 0.13.1-homebrew\r\n- **GCC/Compiler version (if compiling from source)**: Apple LLVM version 8.1.0 (clang-802.0.42)\r\n- **CUDA/cuDNN version**: cuda_9.2.64_mac with cuda_9.2.64.1_mac, cudnn-9.2-osx-x64-v7.1\r\n- **GPU model and memory**: NVIDIA GeForce GT 750M with 2 GB device memory, CUDA Compute Capability 3.0\r\n- **Exact command to reproduce**:\r\n\r\n```\r\n./configure\r\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\n### Describe the problem\r\nThis is a re-occurrence of #9072, except that the solutions mentioned there (not using clang as the CUDA compiler, using CommandLineTools) do not resolve the problem.\r\n\r\nTo configure, I selected the following:\r\n```\r\nYou have bazel 0.13.1-homebrew installed.\r\nPlease specify the location of python. [Default is /usr/local/opt/python@2/bin/python2.7]: \r\n\r\n\r\nFound possible Python library paths:\r\n  /usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages\r\nPlease input the desired Python library path to use.  Default is [/usr/local/Cellar/python@2/2.7.15/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages]\r\n\r\nDo you wish to build TensorFlow with Google Cloud Platform support? [Y/n]:  \r\nGoogle Cloud Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Hadoop File System support? [Y/n]: \r\nHadoop File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Amazon S3 File System support? [Y/n]: \r\nAmazon S3 File System support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with Apache Kafka Platform support? [Y/n]: \r\nApache Kafka Platform support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with XLA JIT support? [y/N]: \r\nNo XLA JIT support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with GDR support? [y/N]: \r\nNo GDR support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with VERBS support? [y/N]: \r\nNo VERBS support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with OpenCL SYCL support? [y/N]: \r\nNo OpenCL SYCL support will be enabled for TensorFlow.\r\n\r\nDo you wish to build TensorFlow with CUDA support? [y/N]: y\r\nCUDA support will be enabled for TensorFlow.\r\n\r\nPlease specify the CUDA SDK version you want to use. [Leave empty to default to CUDA 9.0]: 9.2\r\n\r\n\r\nPlease specify the location where CUDA 9.2 toolkit is installed. Refer to README.md for more details. [Default is /usr/local/cuda]: \r\n\r\n\r\nPlease specify the cuDNN version you want to use. [Leave empty to default to cuDNN 7.0]: 7.1.4\r\n\r\n\r\nPlease specify the location where cuDNN 7 library is installed. Refer to README.md for more details. [Default is /usr/local/cuda]:\r\n\r\n\r\nPlease specify a list of comma-separated Cuda compute capabilities you want to build with.\r\nYou can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.\r\nPlease note that each additional compute capability significantly increases your build time and binary size. [Default is: 3.5,5.2]3.0\r\n\r\n\r\nDo you want to use clang as CUDA compiler? [y/N]: \r\nnvcc will be used as CUDA compiler.\r\n\r\nPlease specify which gcc should be used by nvcc as the host compiler. [Default is /usr/bin/gcc]: \r\n\r\n\r\nDo you wish to build TensorFlow with MPI support? [y/N]: \r\nNo MPI support will be enabled for TensorFlow.\r\n\r\nPlease specify optimization flags to use during compilation when bazel option \"--config=opt\" is specified [Default is -march=native]: \r\n\r\n\r\nWould you like to interactively configure ./WORKSPACE for Android builds? [y/N]: \r\nNot configuring the WORKSPACE for Android builds.\r\n\r\nPreconfigured Bazel build configs. You can use any of the below by adding \"--config=<>\" to your build command. See tools/bazel.rc for more details.\r\n\t--config=mkl         \t# Build with MKL support.\r\n\t--config=monolithic  \t# Config for mostly static monolithic build.\r\nConfiguration finished\r\n```\r\n\r\nTo build, I try:\r\n\r\n```\r\nbazel build --config=opt --config=cuda --save_temps --explain=explain.txt --verbose_explanations --verbose_failures --linkopt=-Wl,-rpath,/usr/local/cuda/lib //tensorflow/tools/pip_package:build_pip_package\r\n```\r\n\r\nHowever, this results in the error:\r\n\r\n```\r\nStarting local Bazel server and connecting to it...\r\n............\r\nWARNING: The following configs were expanded more than once: [cuda]. For repeatable flags, repeats are counted twice and may lead to unexpected behavior.\r\nERROR: Inconsistent crosstool configuration; no toolchain corresponding to 'local_darwin' found for cpu 'darwin'\r\nINFO: Elapsed time: 0.903s\r\nINFO: 0 processes.\r\nFAILED: Build did NOT complete successfully (2 packages loaded)\r\n```\r\n\r\nThe output of `xcode-select -p` is:\r\n\r\n```\r\n/Library/Developer/CommandLineTools\r\n```\r\n\r\nThe output of `/usr/bin/gcc --version` is:\r\n\r\n```\r\nConfigured with: --prefix=/Library/Developer/CommandLineTools/usr --with-gxx-include-dir=/usr/include/c++/4.2.1\r\nApple LLVM version 8.1.0 (clang-802.0.42)\r\nTarget: x86_64-apple-darwin17.5.0\r\nThread model: posix\r\nInstalledDir: /Library/Developer/CommandLineTools/usr/bin\r\n```\r\n\r\nI am able to build & run the 'deviceQuery' CUDA SDK sample without issue.\r\n\r\n### Source code / logs\r\nThe only local changes from e365deab1333005c8aa186632f160c1bfd4485f8 I have are:\r\n\r\n```diff\r\ndiff --git a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\nindex a561d918bd..46c91b4511 100644\r\n--- a/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n+++ b/tensorflow/core/kernels/concat_lib_gpu_impl.cu.cc\r\n@@ -69,7 +69,7 @@ __global__ void concat_variable_kernel(\r\n   IntType num_inputs = input_ptr_data.size;\r\n \r\n   // verbose declaration needed due to template\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char smem[];\r\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\r\n \r\n   if (useSmem) {\r\ndiff --git a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\nindex 5390222b3a..fcbd733614 100644\r\n--- a/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\n+++ b/tensorflow/core/kernels/depthwise_conv_op_gpu.cu.cc\r\n@@ -172,7 +172,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNHWCSmall(\r\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\r\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\r\n   // Holds block plus halo and filter data for blockDim.x depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\n@@ -452,7 +452,7 @@ __global__ __launch_bounds__(1024, 2) void DepthwiseConv2dGPUKernelNCHWSmall(\r\n     const DepthwiseArgs args, const T* input, const T* filter, T* output) {\r\n   assert(CanLaunchDepthwiseConv2dGPUSmall(args));\r\n   // Holds block plus halo and filter data for blockDim.z depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\n@@ -1118,7 +1118,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNHWCSmall(\r\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\r\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.z));\r\n   // Holds block plus halo and filter data for blockDim.x depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\n@@ -1388,7 +1388,7 @@ __launch_bounds__(1024, 2) void DepthwiseConv2dBackpropFilterGPUKernelNCHWSmall(\r\n     const DepthwiseArgs args, const T* output, const T* input, T* filter) {\r\n   assert(CanLaunchDepthwiseConv2dBackpropFilterGPUSmall(args, blockDim.x));\r\n   // Holds block plus halo and filter data for blockDim.z depths.\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char shared_memory[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char shared_memory[];\r\n   T* const shared_data = reinterpret_cast<T*>(shared_memory);\r\n \r\n   const int num_batches = args.batch;\r\ndiff --git a/tensorflow/core/kernels/split_lib_gpu.cu.cc b/tensorflow/core/kernels/split_lib_gpu.cu.cc\r\nindex 393818730b..58a1294005 100644\r\n--- a/tensorflow/core/kernels/split_lib_gpu.cu.cc\r\n+++ b/tensorflow/core/kernels/split_lib_gpu.cu.cc\r\n@@ -121,7 +121,7 @@ __global__ void split_v_kernel(const T* input_ptr,\r\n   int num_outputs = output_ptr_data.size;\r\n \r\n   // verbose declaration needed due to template\r\n-  extern __shared__ __align__(sizeof(T)) unsigned char smem[];\r\n+  extern __shared__ __align__(sizeof(T) > 16 ? sizeof(T) : 16) unsigned char smem[];\r\n   IntType* smem_col_scan = reinterpret_cast<IntType*>(smem);\r\n \r\n   if (useSmem) {\r\ndiff --git a/third_party/gpus/cuda/BUILD.tpl b/third_party/gpus/cuda/BUILD.tpl\r\nindex 2a37c65bc7..43446dd99b 100644\r\n--- a/third_party/gpus/cuda/BUILD.tpl\r\n+++ b/third_party/gpus/cuda/BUILD.tpl\r\n@@ -110,7 +110,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    #linkopts = [\"-lgomp\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\ndiff --git a/third_party/toolchains/gpus/cuda/BUILD b/third_party/toolchains/gpus/cuda/BUILD\r\nindex 4cb8380938..d025c4f3aa 100644\r\n--- a/third_party/toolchains/gpus/cuda/BUILD\r\n+++ b/third_party/toolchains/gpus/cuda/BUILD\r\n@@ -115,7 +115,7 @@ cc_library(\r\n         \".\",\r\n         \"cuda/include\",\r\n     ],\r\n-    linkopts = [\"-lgomp\"],\r\n+    #linkopts = [\"-lgomp\"],\r\n     linkstatic = 1,\r\n     visibility = [\"//visibility:public\"],\r\n )\r\n```"}