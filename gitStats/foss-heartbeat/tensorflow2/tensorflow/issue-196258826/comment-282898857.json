{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/282898857", "html_url": "https://github.com/tensorflow/tensorflow/issues/6381#issuecomment-282898857", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6381", "id": 282898857, "node_id": "MDEyOklzc3VlQ29tbWVudDI4Mjg5ODg1Nw==", "user": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "created_at": "2017-02-28T00:07:01Z", "updated_at": "2017-02-28T00:07:01Z", "author_association": "MEMBER", "body_html": "<p>Oh, if you need online inference (i.e. predict), I highly recommend you check out <a href=\"https://github.com/tensorflow/serving\">TensorFlow Serving</a>. The team that's put this together has solved a number of non-intuitive challenges you might encounter when running inference in an online environment. Their systems absolutely support updating the models while serving requests, and have tooling to do so.</p>", "body_text": "Oh, if you need online inference (i.e. predict), I highly recommend you check out TensorFlow Serving. The team that's put this together has solved a number of non-intuitive challenges you might encounter when running inference in an online environment. Their systems absolutely support updating the models while serving requests, and have tooling to do so.", "body": "Oh, if you need online inference (i.e. predict), I highly recommend you check out [TensorFlow Serving](https://github.com/tensorflow/serving). The team that's put this together has solved a number of non-intuitive challenges you might encounter when running inference in an online environment. Their systems absolutely support updating the models while serving requests, and have tooling to do so."}