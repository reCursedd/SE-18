{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6381", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6381/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6381/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6381/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6381", "id": 196258826, "node_id": "MDU6SXNzdWUxOTYyNTg4MjY=", "number": 6381, "title": "Discussion Online Expanding the Number of Distributed Training Workers ", "user": {"login": "guoying1030", "id": 17998982, "node_id": "MDQ6VXNlcjE3OTk4OTgy", "avatar_url": "https://avatars2.githubusercontent.com/u/17998982?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guoying1030", "html_url": "https://github.com/guoying1030", "followers_url": "https://api.github.com/users/guoying1030/followers", "following_url": "https://api.github.com/users/guoying1030/following{/other_user}", "gists_url": "https://api.github.com/users/guoying1030/gists{/gist_id}", "starred_url": "https://api.github.com/users/guoying1030/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guoying1030/subscriptions", "organizations_url": "https://api.github.com/users/guoying1030/orgs", "repos_url": "https://api.github.com/users/guoying1030/repos", "events_url": "https://api.github.com/users/guoying1030/events{/privacy}", "received_events_url": "https://api.github.com/users/guoying1030/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "saeta", "id": 1284535, "node_id": "MDQ6VXNlcjEyODQ1MzU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1284535?v=4", "gravatar_id": "", "url": "https://api.github.com/users/saeta", "html_url": "https://github.com/saeta", "followers_url": "https://api.github.com/users/saeta/followers", "following_url": "https://api.github.com/users/saeta/following{/other_user}", "gists_url": "https://api.github.com/users/saeta/gists{/gist_id}", "starred_url": "https://api.github.com/users/saeta/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/saeta/subscriptions", "organizations_url": "https://api.github.com/users/saeta/orgs", "repos_url": "https://api.github.com/users/saeta/repos", "events_url": "https://api.github.com/users/saeta/events{/privacy}", "received_events_url": "https://api.github.com/users/saeta/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2016-12-18T05:20:42Z", "updated_at": "2017-06-16T18:39:24Z", "closed_at": "2017-06-16T18:39:24Z", "author_association": "NONE", "body_html": "<p>Tensorflow distributed training adds a new feature:<br>\nWhen we use the distributed tensorflow training task, if the number of workers is too small to start, but not very good convergence. . Especially when used in conjunction with k8s, then tensorflow should support the online expansion of the current number of workers training tasks.</p>\n<p>My idea is: Use python interface, notify the ps server, the new worker task_index and worker addr. such as:</p>\n<p>We pre-start the cluster as follows:</p>\n<h1>On ps0.example.com:</h1>\n<p>$ Python trainer.py <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 0</p>\n<h1>On ps1.example.com:</h1>\n<p>$ Python trainer.py <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 1</p>\n<h1>On worker0.example.com:</h1>\n<p>$ Python trainer.py <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 0</p>\n<h1>On worker1.example.com:</h1>\n<p>$ Python trainer.py <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 <br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 1</p>\n<p>We started the original two ps server ,,, two workers. .<br>\nNow let's add a third worker</p>\n<p>Import tensorflow as tf<br>\nDef main (_):<br>\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps0.example.com:2222') as sess:<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')</p>\n<p>\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps1.example.com:2222') as sess:<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')</p>\n<p>If name == \"main\":<br>\n\u00a0\u00a0\u00a0\u00a0Tf.app.run ()</p>\n<p>Just run this python script, and then start worker2.<br>\nWorker2 startup script:<br>\n$ Python trainer.py<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222<br>\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = worker --task_index = 2</p>\n<p>At this point you should be able to see the worker2 has started training.</p>\n<p>If ps0 or ps1 restart, it should be added worker2:<br>\n$ Python trainer.py<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222<br>\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,<br>\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = ps --task_index = 0</p>\n<p>The design idea is as follows:</p>\n<ol>\n<li>Increase the number of workers online, only need to notify all of the ps server can be.</li>\n<li>Through the use of the python session interface, you can connect a different ps server, and then send an increase in worker instructions can be given to them.</li>\n</ol>\n<p>I've done this at <a href=\"https://github.com/ipdcode/tensorflow\">https://github.com/ipdcode/tensorflow</a>.</p>\n<p>Welcome to the official google and we discuss to see if there is a need to increase this feature.</p>", "body_text": "Tensorflow distributed training adds a new feature:\nWhen we use the distributed tensorflow training task, if the number of workers is too small to start, but not very good convergence. . Especially when used in conjunction with k8s, then tensorflow should support the online expansion of the current number of workers training tasks.\nMy idea is: Use python interface, notify the ps server, the new worker task_index and worker addr. such as:\nWe pre-start the cluster as follows:\nOn ps0.example.com:\n$ Python trainer.py \n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 0\nOn ps1.example.com:\n$ Python trainer.py \n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 1\nOn worker0.example.com:\n$ Python trainer.py \n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 0\nOn worker1.example.com:\n$ Python trainer.py \n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 1\nWe started the original two ps server ,,, two workers. .\nNow let's add a third worker\nImport tensorflow as tf\nDef main (_):\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps0.example.com:2222') as sess:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps1.example.com:2222') as sess:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')\nIf name == \"main\":\n\u00a0\u00a0\u00a0\u00a0Tf.app.run ()\nJust run this python script, and then start worker2.\nWorker2 startup script:\n$ Python trainer.py\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = worker --task_index = 2\nAt this point you should be able to see the worker2 has started training.\nIf ps0 or ps1 restart, it should be added worker2:\n$ Python trainer.py\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = ps --task_index = 0\nThe design idea is as follows:\n\nIncrease the number of workers online, only need to notify all of the ps server can be.\nThrough the use of the python session interface, you can connect a different ps server, and then send an increase in worker instructions can be given to them.\n\nI've done this at https://github.com/ipdcode/tensorflow.\nWelcome to the official google and we discuss to see if there is a need to increase this feature.", "body": "Tensorflow distributed training adds a new feature:\r\nWhen we use the distributed tensorflow training task, if the number of workers is too small to start, but not very good convergence. . Especially when used in conjunction with k8s, then tensorflow should support the online expansion of the current number of workers training tasks.\r\n\r\nMy idea is: Use python interface, notify the ps server, the new worker task_index and worker addr. such as:\r\n\r\nWe pre-start the cluster as follows:\r\n# On ps0.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 0\r\n# On ps1.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = ps --task_index = 1\r\n# On worker0.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 0\r\n# On worker1.example.com:\r\n$ Python trainer.py \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222 \\\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--job_name = worker --task_index = 1\r\n\r\n\r\nWe started the original two ps server ,,, two workers. .\r\nNow let's add a third worker\r\n\r\n\r\nImport tensorflow as tf\r\nDef main (_):\r\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps0.example.com:2222') as sess:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')\r\n\r\n\u00a0\u00a0\u00a0\u00a0With tf.Session ('grpc: //ps1.example.com:2222') as sess:\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Sess.add_onlineworker (2, 'worker2.example.com: 2222')\r\n\r\nIf name == \"main\":\r\n\u00a0\u00a0\u00a0\u00a0Tf.app.run ()\r\n\r\nJust run this python script, and then start worker2.\r\nWorker2 startup script:\r\n$ Python trainer.py\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = worker --task_index = 2\r\n\r\nAt this point you should be able to see the worker2 has started training.\r\n\r\nIf ps0 or ps1 restart, it should be added worker2:\r\n$ Python trainer.py\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--ps_hosts = ps0.example.com: 2222, ps1.example.com: 2222\r\n\u00a0\u00a0\u00a0\u00a0--worker_hosts = worker0.example.com: 2222, worker1.example.com: 2222,\r\n\u00a0\u00a0\u00a0\u00a0\u00a0--worker2.example.com:2222 --job_name = ps --task_index = 0\r\n\r\n\r\nThe design idea is as follows:\r\n1. Increase the number of workers online, only need to notify all of the ps server can be.\r\n2. Through the use of the python session interface, you can connect a different ps server, and then send an increase in worker instructions can be given to them.\r\n\r\n\r\n\r\nI've done this at https://github.com/ipdcode/tensorflow.\r\n\r\nWelcome to the official google and we discuss to see if there is a need to increase this feature.\r\n\r\n\r\n\r\n\r\n"}