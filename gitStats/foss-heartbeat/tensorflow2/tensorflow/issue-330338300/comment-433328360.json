{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/433328360", "html_url": "https://github.com/tensorflow/tensorflow/issues/19838#issuecomment-433328360", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19838", "id": 433328360, "node_id": "MDEyOklzc3VlQ29tbWVudDQzMzMyODM2MA==", "user": {"login": "dvicini", "id": 5487532, "node_id": "MDQ6VXNlcjU0ODc1MzI=", "avatar_url": "https://avatars3.githubusercontent.com/u/5487532?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dvicini", "html_url": "https://github.com/dvicini", "followers_url": "https://api.github.com/users/dvicini/followers", "following_url": "https://api.github.com/users/dvicini/following{/other_user}", "gists_url": "https://api.github.com/users/dvicini/gists{/gist_id}", "starred_url": "https://api.github.com/users/dvicini/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dvicini/subscriptions", "organizations_url": "https://api.github.com/users/dvicini/orgs", "repos_url": "https://api.github.com/users/dvicini/repos", "events_url": "https://api.github.com/users/dvicini/events{/privacy}", "received_events_url": "https://api.github.com/users/dvicini/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-26T08:22:27Z", "updated_at": "2018-10-26T08:23:09Z", "author_association": "NONE", "body_html": "<p>I observed a similar problem. The following code works perfectly well with <code>use_optimize_for_inference = False</code> but gives the error</p>\n<pre><code>tensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op&lt;name=Const; signature= -&gt; output:dtype; attr=value:tensor; attr=dtype:type&gt;; NodeDef: {{node optimized/cond/add/y}} = Const[dtype=DT_FLOAT, value=Tensor&lt;type: float shape: [] values: 1&gt;](optimized/cond/Switch:1)\n</code></pre>\n<p>if I use the optimize for inference routine. The code:</p>\n<pre><code>import tensorflow as tf\nfrom tensorflow.python.tools import optimize_for_inference_lib\nfrom tensorflow.python.tools import freeze_graph\n\n\nuse_optimize_for_inference = True\n\n# Set up a simple test graph\na = tf.Variable(tf.ones(3, dtype=tf.float32))\nx = tf.placeholder(tf.float32, [3], name='x')\nswitch = tf.placeholder(tf.bool, name='switch')\n\ny = a * x\ny = tf.cond(switch, lambda: y + 1.0, lambda: y)\ny = tf.identity(y, 'result')\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # Save the graph to disk\n    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model.pbtxt', as_text=True)\n    saver.save(sess, './model.ckpt')\n\n    # Freeze the trained model\n    output_graph = 'frozen_model.pb'\n    freeze_graph.freeze_graph('./model.pbtxt', '', False, './model.ckpt',\n                              'result', 'save/restore_all', 'save/Const:0', output_graph, True, '')\n\n    # Reload the frozen graph from disk\n    frozen_graph = tf.GraphDef()\n    with tf.gfile.Open(output_graph, \"rb\") as f:\n        frozen_graph.ParseFromString(f.read())\n\n    # optimize for inference\n    input_names = ['x', 'switch']\n    output_names = ['result']\n    new_graph = optimize_for_inference_lib.optimize_for_inference(\n        frozen_graph, input_names, output_names, [tf.float32.as_datatype_enum, tf.bool.as_datatype_enum])\n\n    # Write the optimized graph to disk\n    output_graph_inference = 'frozen_model_inference.pb'\n    with tf.gfile.GFile(output_graph_inference, \"wb\") as f:\n        f.write(new_graph.SerializeToString())\n\n    # Attempt loading the frozen graph: This fails if we optimize the graph for inference\n    print('Loading the saved graph...')\n    optimized_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(output_graph_inference if use_optimize_for_inference else output_graph, \"rb\") as f:\n        optimized_graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(optimized_graph_def, name=\"optimized\")\n\n\n</code></pre>\n<p>The problem is either related to the use of the boolean variable or the conditional. I observed it both in Tensorflow 1.11.0 (Python 3.7) and 1.8.0 (Python 3.6). For now, my solution is to just not use the optimize for inference script, as it doesn't benefit me that much in my use....</p>", "body_text": "I observed a similar problem. The following code works perfectly well with use_optimize_for_inference = False but gives the error\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node optimized/cond/add/y}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>](optimized/cond/Switch:1)\n\nif I use the optimize for inference routine. The code:\nimport tensorflow as tf\nfrom tensorflow.python.tools import optimize_for_inference_lib\nfrom tensorflow.python.tools import freeze_graph\n\n\nuse_optimize_for_inference = True\n\n# Set up a simple test graph\na = tf.Variable(tf.ones(3, dtype=tf.float32))\nx = tf.placeholder(tf.float32, [3], name='x')\nswitch = tf.placeholder(tf.bool, name='switch')\n\ny = a * x\ny = tf.cond(switch, lambda: y + 1.0, lambda: y)\ny = tf.identity(y, 'result')\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    sess.run(tf.global_variables_initializer())\n\n    # Save the graph to disk\n    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model.pbtxt', as_text=True)\n    saver.save(sess, './model.ckpt')\n\n    # Freeze the trained model\n    output_graph = 'frozen_model.pb'\n    freeze_graph.freeze_graph('./model.pbtxt', '', False, './model.ckpt',\n                              'result', 'save/restore_all', 'save/Const:0', output_graph, True, '')\n\n    # Reload the frozen graph from disk\n    frozen_graph = tf.GraphDef()\n    with tf.gfile.Open(output_graph, \"rb\") as f:\n        frozen_graph.ParseFromString(f.read())\n\n    # optimize for inference\n    input_names = ['x', 'switch']\n    output_names = ['result']\n    new_graph = optimize_for_inference_lib.optimize_for_inference(\n        frozen_graph, input_names, output_names, [tf.float32.as_datatype_enum, tf.bool.as_datatype_enum])\n\n    # Write the optimized graph to disk\n    output_graph_inference = 'frozen_model_inference.pb'\n    with tf.gfile.GFile(output_graph_inference, \"wb\") as f:\n        f.write(new_graph.SerializeToString())\n\n    # Attempt loading the frozen graph: This fails if we optimize the graph for inference\n    print('Loading the saved graph...')\n    optimized_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(output_graph_inference if use_optimize_for_inference else output_graph, \"rb\") as f:\n        optimized_graph_def.ParseFromString(f.read())\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(optimized_graph_def, name=\"optimized\")\n\n\n\nThe problem is either related to the use of the boolean variable or the conditional. I observed it both in Tensorflow 1.11.0 (Python 3.7) and 1.8.0 (Python 3.6). For now, my solution is to just not use the optimize for inference script, as it doesn't benefit me that much in my use....", "body": "I observed a similar problem. The following code works perfectly well with `use_optimize_for_inference = False` but gives the error \r\n```\r\ntensorflow.python.framework.errors_impl.InvalidArgumentError: NodeDef expected inputs '' do not match 1 inputs specified; Op<name=Const; signature= -> output:dtype; attr=value:tensor; attr=dtype:type>; NodeDef: {{node optimized/cond/add/y}} = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [] values: 1>](optimized/cond/Switch:1)\r\n```\r\nif I use the optimize for inference routine. The code:\r\n\r\n```\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import optimize_for_inference_lib\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\n\r\nuse_optimize_for_inference = True\r\n\r\n# Set up a simple test graph\r\na = tf.Variable(tf.ones(3, dtype=tf.float32))\r\nx = tf.placeholder(tf.float32, [3], name='x')\r\nswitch = tf.placeholder(tf.bool, name='switch')\r\n\r\ny = a * x\r\ny = tf.cond(switch, lambda: y + 1.0, lambda: y)\r\ny = tf.identity(y, 'result')\r\n\r\nsaver = tf.train.Saver()\r\nwith tf.Session() as sess:\r\n    sess.run(tf.global_variables_initializer())\r\n\r\n    # Save the graph to disk\r\n    tf.train.write_graph(sess.graph.as_graph_def(), '.', 'model.pbtxt', as_text=True)\r\n    saver.save(sess, './model.ckpt')\r\n\r\n    # Freeze the trained model\r\n    output_graph = 'frozen_model.pb'\r\n    freeze_graph.freeze_graph('./model.pbtxt', '', False, './model.ckpt',\r\n                              'result', 'save/restore_all', 'save/Const:0', output_graph, True, '')\r\n\r\n    # Reload the frozen graph from disk\r\n    frozen_graph = tf.GraphDef()\r\n    with tf.gfile.Open(output_graph, \"rb\") as f:\r\n        frozen_graph.ParseFromString(f.read())\r\n\r\n    # optimize for inference\r\n    input_names = ['x', 'switch']\r\n    output_names = ['result']\r\n    new_graph = optimize_for_inference_lib.optimize_for_inference(\r\n        frozen_graph, input_names, output_names, [tf.float32.as_datatype_enum, tf.bool.as_datatype_enum])\r\n\r\n    # Write the optimized graph to disk\r\n    output_graph_inference = 'frozen_model_inference.pb'\r\n    with tf.gfile.GFile(output_graph_inference, \"wb\") as f:\r\n        f.write(new_graph.SerializeToString())\r\n\r\n    # Attempt loading the frozen graph: This fails if we optimize the graph for inference\r\n    print('Loading the saved graph...')\r\n    optimized_graph_def = tf.GraphDef()\r\n    with tf.gfile.GFile(output_graph_inference if use_optimize_for_inference else output_graph, \"rb\") as f:\r\n        optimized_graph_def.ParseFromString(f.read())\r\n    with tf.Graph().as_default() as graph:\r\n        tf.import_graph_def(optimized_graph_def, name=\"optimized\")\r\n\r\n\r\n```\r\nThe problem is either related to the use of the boolean variable or the conditional. I observed it both in Tensorflow 1.11.0 (Python 3.7) and 1.8.0 (Python 3.6). For now, my solution is to just not use the optimize for inference script, as it doesn't benefit me that much in my use...."}