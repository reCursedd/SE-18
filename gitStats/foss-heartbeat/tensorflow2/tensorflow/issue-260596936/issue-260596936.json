{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13310", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13310/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13310/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13310/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13310", "id": 260596936, "node_id": "MDU6SXNzdWUyNjA1OTY5MzY=", "number": 13310, "title": "Feature Request: Mixed Sparse and Dense Tensors", "user": {"login": "wechselstrom", "id": 10787735, "node_id": "MDQ6VXNlcjEwNzg3NzM1", "avatar_url": "https://avatars3.githubusercontent.com/u/10787735?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wechselstrom", "html_url": "https://github.com/wechselstrom", "followers_url": "https://api.github.com/users/wechselstrom/followers", "following_url": "https://api.github.com/users/wechselstrom/following{/other_user}", "gists_url": "https://api.github.com/users/wechselstrom/gists{/gist_id}", "starred_url": "https://api.github.com/users/wechselstrom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wechselstrom/subscriptions", "organizations_url": "https://api.github.com/users/wechselstrom/orgs", "repos_url": "https://api.github.com/users/wechselstrom/repos", "events_url": "https://api.github.com/users/wechselstrom/events{/privacy}", "received_events_url": "https://api.github.com/users/wechselstrom/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "elibixby", "id": 6596957, "node_id": "MDQ6VXNlcjY1OTY5NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6596957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elibixby", "html_url": "https://github.com/elibixby", "followers_url": "https://api.github.com/users/elibixby/followers", "following_url": "https://api.github.com/users/elibixby/following{/other_user}", "gists_url": "https://api.github.com/users/elibixby/gists{/gist_id}", "starred_url": "https://api.github.com/users/elibixby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elibixby/subscriptions", "organizations_url": "https://api.github.com/users/elibixby/orgs", "repos_url": "https://api.github.com/users/elibixby/repos", "events_url": "https://api.github.com/users/elibixby/events{/privacy}", "received_events_url": "https://api.github.com/users/elibixby/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "elibixby", "id": 6596957, "node_id": "MDQ6VXNlcjY1OTY5NTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6596957?v=4", "gravatar_id": "", "url": "https://api.github.com/users/elibixby", "html_url": "https://github.com/elibixby", "followers_url": "https://api.github.com/users/elibixby/followers", "following_url": "https://api.github.com/users/elibixby/following{/other_user}", "gists_url": "https://api.github.com/users/elibixby/gists{/gist_id}", "starred_url": "https://api.github.com/users/elibixby/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/elibixby/subscriptions", "organizations_url": "https://api.github.com/users/elibixby/orgs", "repos_url": "https://api.github.com/users/elibixby/repos", "events_url": "https://api.github.com/users/elibixby/events{/privacy}", "received_events_url": "https://api.github.com/users/elibixby/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 25, "created_at": "2017-09-26T11:57:59Z", "updated_at": "2018-07-11T02:02:08Z", "closed_at": "2018-07-11T02:02:07Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>I am trying to implement a sparse convolution operation. This means I have spatially sparse locations(in this case <em>NHW</em> of <em>NHWC</em> in 2d convolutions)<br>\nand at each of these locations I have a dense vector of values (<em>C</em> of <em>NHWC</em>).<br>\nIn tensorflow there are currently two classes for sparse representations:</p>\n<ol>\n<li><strong>SparseTensor</strong>:<br>\nConsisting of an indices matrix, value and shape vector.<br>\nThis allows for a fully sparse tensor, however it does not fit this use case, because representing the channels as just another sparse dimension, I cannot simply compute a matrix multiplication, with the corresponding kernel parameters. This extremely reduces efficiency. Apart from this, also the storage<br>\nis inefficient, since It redundantly stores the indices for the dense sub tensor.</li>\n<li><strong>IndexedSlices</strong><br>\nConsisting of an indices vector and an arbitrarily shaped value tensor.<br>\nThis is a mixed sparse dense data structure. So the individual sub tensors are stored sparsely. It does however not fit the use case, because the indexing is only a vector and not (as compared to the sparse<br>\ntensor) a matrix. So we can only address only a single index. While it is possible to encode an index vector<br>\nas a scalar index, it imposes this effort on the user, which seems to me to be non optimal.</li>\n</ol>\n<p>So both data structures are inadequate for this use case. While I am talking about a single use case, I cannot imagine, that no one else stumbled across this issue, since its so general.</p>\n<h3>Proposed Solution</h3>\n<p>To solve this issue I propose to introduce a new class <strong>MixedSparseDenseTensor</strong>, which takes an indices matrix (as opposed to IndexedSlices vector), an arbitrarily shaped values tensor (as opposed to SparseTensors vector) and a shape vector. So it would thus be either a generalization of IndexedSlices to multiple dimensional indices or a generalization of SparseTensor to arbitrarily shaped sub tensors.</p>\n<p>The dimensionality of the shape vector should then be the rank of the sparse tensor + the rank of the dense tensor.</p>", "body_text": "Describe the problem\nI am trying to implement a sparse convolution operation. This means I have spatially sparse locations(in this case NHW of NHWC in 2d convolutions)\nand at each of these locations I have a dense vector of values (C of NHWC).\nIn tensorflow there are currently two classes for sparse representations:\n\nSparseTensor:\nConsisting of an indices matrix, value and shape vector.\nThis allows for a fully sparse tensor, however it does not fit this use case, because representing the channels as just another sparse dimension, I cannot simply compute a matrix multiplication, with the corresponding kernel parameters. This extremely reduces efficiency. Apart from this, also the storage\nis inefficient, since It redundantly stores the indices for the dense sub tensor.\nIndexedSlices\nConsisting of an indices vector and an arbitrarily shaped value tensor.\nThis is a mixed sparse dense data structure. So the individual sub tensors are stored sparsely. It does however not fit the use case, because the indexing is only a vector and not (as compared to the sparse\ntensor) a matrix. So we can only address only a single index. While it is possible to encode an index vector\nas a scalar index, it imposes this effort on the user, which seems to me to be non optimal.\n\nSo both data structures are inadequate for this use case. While I am talking about a single use case, I cannot imagine, that no one else stumbled across this issue, since its so general.\nProposed Solution\nTo solve this issue I propose to introduce a new class MixedSparseDenseTensor, which takes an indices matrix (as opposed to IndexedSlices vector), an arbitrarily shaped values tensor (as opposed to SparseTensors vector) and a shape vector. So it would thus be either a generalization of IndexedSlices to multiple dimensional indices or a generalization of SparseTensor to arbitrarily shaped sub tensors.\nThe dimensionality of the shape vector should then be the rank of the sparse tensor + the rank of the dense tensor.", "body": "### Describe the problem\r\nI am trying to implement a sparse convolution operation. This means I have spatially sparse locations(in this case *NHW* of *NHWC* in 2d convolutions)\r\nand at each of these locations I have a dense vector of values (*C* of *NHWC*).\r\nIn tensorflow there are currently two classes for sparse representations:\r\n\r\n1. **SparseTensor**:\r\nConsisting of an indices matrix, value and shape vector.\r\n  This allows for a fully sparse tensor, however it does not fit this use case, because representing the channels as just another sparse dimension, I cannot simply compute a matrix multiplication, with the corresponding kernel parameters. This extremely reduces efficiency. Apart from this, also the storage\r\nis inefficient, since It redundantly stores the indices for the dense sub tensor.\r\n2. **IndexedSlices**\r\nConsisting of an indices vector and an arbitrarily shaped value tensor.\r\nThis is a mixed sparse dense data structure. So the individual sub tensors are stored sparsely. It does however not fit the use case, because the indexing is only a vector and not (as compared to the sparse\r\ntensor) a matrix. So we can only address only a single index. While it is possible to encode an index vector\r\nas a scalar index, it imposes this effort on the user, which seems to me to be non optimal.\r\n\r\nSo both data structures are inadequate for this use case. While I am talking about a single use case, I cannot imagine, that no one else stumbled across this issue, since its so general. \r\n\r\n### Proposed Solution\r\nTo solve this issue I propose to introduce a new class **MixedSparseDenseTensor**, which takes an indices matrix (as opposed to IndexedSlices vector), an arbitrarily shaped values tensor (as opposed to SparseTensors vector) and a shape vector. So it would thus be either a generalization of IndexedSlices to multiple dimensional indices or a generalization of SparseTensor to arbitrarily shaped sub tensors.\r\n\r\nThe dimensionality of the shape vector should then be the rank of the sparse tensor + the rank of the dense tensor.\r\n\r\n"}