{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/355543245", "html_url": "https://github.com/tensorflow/tensorflow/issues/13351#issuecomment-355543245", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13351", "id": 355543245, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NTU0MzI0NQ==", "user": {"login": "djvisentin", "id": 3000831, "node_id": "MDQ6VXNlcjMwMDA4MzE=", "avatar_url": "https://avatars3.githubusercontent.com/u/3000831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/djvisentin", "html_url": "https://github.com/djvisentin", "followers_url": "https://api.github.com/users/djvisentin/followers", "following_url": "https://api.github.com/users/djvisentin/following{/other_user}", "gists_url": "https://api.github.com/users/djvisentin/gists{/gist_id}", "starred_url": "https://api.github.com/users/djvisentin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/djvisentin/subscriptions", "organizations_url": "https://api.github.com/users/djvisentin/orgs", "repos_url": "https://api.github.com/users/djvisentin/repos", "events_url": "https://api.github.com/users/djvisentin/events{/privacy}", "received_events_url": "https://api.github.com/users/djvisentin/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-05T12:25:06Z", "updated_at": "2018-01-05T12:33:37Z", "author_association": "MEMBER", "body_html": "<p>I'm starting to push through changes to fix this. The first one is a refactoring to make subsequent changes easier as well as avoiding infinite recursion when the <code>initial_value</code> contains cycles (note that it also changes the name of <code>_build_initiazier_expr</code> to <code>_try_guard_against_uninitialized_depenencies</code> to clarify what it does). I'll now attempt to address more edge cases.</p>\n<p>Here's what I believe is a minimal reproduction of the bad behaviour in this case:</p>\n<pre><code>p = tf.placeholder(dtype=tf.float32, shape=[])\nv0 = tf.Variable(p)\nv1 = tf.Variable(v0)\nwith tf.Session() as session:\n  session.run(v0.initializer, feed_dict={p: 0})\n  session.run(v1.initializer)  # InvalidArgumentError\n</code></pre>\n<p>The transformation done by <code>_build_initializer_expr</code> makes <code>v1</code> depend on <code>v0.initialized_value()</code> which is a <code>cond</code> of the form:</p>\n<pre><code>cond(\n    is_variable_initialized(v0),\n    v0.read_value,\n    lambda: p)\n</code></pre>\n<p>Because <code>p</code> isn't actually created inside <code>false_fn</code> it will always be evaluated when the <code>cond</code> is -- which is inconsistent with the intended behaviour. If you were to run that <code>session.run(v1.initializer)</code> statement and feed <code>p</code> it would work (but we shouldn't expect the user to do that).</p>\n<p>This adds to the list of edge cases (that I'm aware of) where <code>_build_initializer_expr</code> behaves poorly:</p>\n<ol>\n<li>\n<p>This issues of extraneous propagation of placeholders.</p>\n</li>\n<li>\n<p>Cyclic initializers (now \"fixed\" by having it ignore those graphs).</p>\n</li>\n<li>\n<p>When TF functions are involved. For example:</p>\n</li>\n</ol>\n<pre><code>@function.Defun(tf.float32, shape_func=lambda op: [op.inputs[0].get_shape()])\ndef increment(x):\n  return x + tf.ones_like(x)\n\nv0 = tf.Variable(tf.zeros(shape=[]))\na = increment(v0)\nv = tf.Variable(a)  # NotFoundError: Op type not registered\n</code></pre>\n<ol start=\"4\">\n<li>With most stateful ops. For example:</li>\n</ol>\n<pre><code># Graph rewriting results in wrong values when you have a variable initializer\n# which depends on a non-determinstic op which depends on a variable. Note that\n# we use random_uniform with integer types because the underlying op\n# \"RandomUniformInt\" takes in the minval and maxval as tensors unlike, for\n# example, random_normal with calls \"RandomStandardNormal\" and then transforms\n# it with the mean/stdev (so the non-determinsitic part doesn't actually)\n# depend on the variables and won't be rewritten.\nminval = tf.Variable(0, dtype=tf.int32)\nmaxval = tf.Variable(1000000, dtype=tf.int32)\nr = tf.random_uniform(shape=[], minval=minval, maxval=maxval, dtype=tf.int32)\nv0 = tf.Variable(r)\nv1 = tf.Variable(v0)  # Same results even if you use v0.initialized_value()\nwith tf.Session() as session:\n  session.run([minval.initializer, maxval.initializer])\n  session.run([v0.initializer, v1.initializer])\n  print(session.run([v0, v1])) # Different values.\n\n# _build_initializer_expr copies the random op so there will be two of them.\nprint(sum([op.type == 'RandomUniformInt'\n           for op in tf.get_default_graph().get_operations()]))\n</code></pre>\n<ol start=\"5\">\n<li>If you have an initializer where there is an intermediate op X which takes an argument that must be a reference type, and that argument is a variable. This algorithm will turn the reference type into a value type and cause an error. For example:</li>\n</ol>\n<pre><code>v0 = tf.Variable(tf.zeros(shape=[]))\none = tf.ones_like(v0)\nv0_plus_one = tf.assign(v0, v0.initialized_value() + one)\nv1 = tf.Variable(v0_plus_one)\nwith tf.Session() as session:\n  session.run(v1.initializer)  # InvalidArgumentError\n</code></pre>\n<ol start=\"6\">\n<li>Not a bug, as such, but the implementation will produce an exponentially large number of nodes given a diamond pattern dependency graph (now fixed by memoizing):</li>\n</ol>\n<pre><code># Diamond pattern - exponential growth\n# N =&gt; N + 2^N - 1 Add ops\nN = 4\nv0 = tf.Variable(tf.zeros(shape=[]))\nx = v0\nfor i in range(N):\n  x = tf.add(x, x)\nv1 = tf.Variable(x)\nprint(sum([op.type == 'Add'\n           for op in tf.get_default_graph().get_operations()]))\nwith tf.Session() as session:\n  print(session.run(v1.initializer))\n  print(session.run(v1))\n</code></pre>\n<ol start=\"7\">\n<li>\n<p>The implementation completely ignores control dependencies. Any new ops created simply don't have them.</p>\n</li>\n<li>\n<p>The implementation completely ignores control flow contexts. Any new ops created based on ops in a control flow context aren't added to that context.</p>\n</li>\n<li>\n<p>The implementation will only be able to find a Variable's initialized_value method if that variable is present in the <code>GLOBAL_VARIABLES</code> or <code>LOCAL_VARIABLES</code> collection. This is a weird inconsistency to the end user. It's even possible it could find the wrong Variable (although you'd have to be working really hard to get it to do that).</p>\n</li>\n<li>\n<p>The implementation relies on a hard-coded set of known variable ops. If new variable ops are added it won't recognize them without manual intervention.</p>\n</li>\n<li>\n<p>Even if the caller explicitly passes in a .initialized_value() to a Variable's constructor the <code>_build_initializer_expr</code> will probably create a bunch of new nodes in the graph anyway.</p>\n</li>\n<li>\n<p>It currently doesn't even do anything when the initial value depends on a ResourceVariable.</p>\n</li>\n</ol>\n<p>There are probably even more that I'm not currently aware of.</p>\n<p>I'll start working through these in the coming days. I'm working under the assumption that we can't just remove this transformation since there will already be a significant number of graphs which depend on it. That means most of the work will be to whitelist and/or blacklist the set of <code>initial_value</code> subgraphs for which we can safely apply the transformation.</p>", "body_text": "I'm starting to push through changes to fix this. The first one is a refactoring to make subsequent changes easier as well as avoiding infinite recursion when the initial_value contains cycles (note that it also changes the name of _build_initiazier_expr to _try_guard_against_uninitialized_depenencies to clarify what it does). I'll now attempt to address more edge cases.\nHere's what I believe is a minimal reproduction of the bad behaviour in this case:\np = tf.placeholder(dtype=tf.float32, shape=[])\nv0 = tf.Variable(p)\nv1 = tf.Variable(v0)\nwith tf.Session() as session:\n  session.run(v0.initializer, feed_dict={p: 0})\n  session.run(v1.initializer)  # InvalidArgumentError\n\nThe transformation done by _build_initializer_expr makes v1 depend on v0.initialized_value() which is a cond of the form:\ncond(\n    is_variable_initialized(v0),\n    v0.read_value,\n    lambda: p)\n\nBecause p isn't actually created inside false_fn it will always be evaluated when the cond is -- which is inconsistent with the intended behaviour. If you were to run that session.run(v1.initializer) statement and feed p it would work (but we shouldn't expect the user to do that).\nThis adds to the list of edge cases (that I'm aware of) where _build_initializer_expr behaves poorly:\n\n\nThis issues of extraneous propagation of placeholders.\n\n\nCyclic initializers (now \"fixed\" by having it ignore those graphs).\n\n\nWhen TF functions are involved. For example:\n\n\n@function.Defun(tf.float32, shape_func=lambda op: [op.inputs[0].get_shape()])\ndef increment(x):\n  return x + tf.ones_like(x)\n\nv0 = tf.Variable(tf.zeros(shape=[]))\na = increment(v0)\nv = tf.Variable(a)  # NotFoundError: Op type not registered\n\n\nWith most stateful ops. For example:\n\n# Graph rewriting results in wrong values when you have a variable initializer\n# which depends on a non-determinstic op which depends on a variable. Note that\n# we use random_uniform with integer types because the underlying op\n# \"RandomUniformInt\" takes in the minval and maxval as tensors unlike, for\n# example, random_normal with calls \"RandomStandardNormal\" and then transforms\n# it with the mean/stdev (so the non-determinsitic part doesn't actually)\n# depend on the variables and won't be rewritten.\nminval = tf.Variable(0, dtype=tf.int32)\nmaxval = tf.Variable(1000000, dtype=tf.int32)\nr = tf.random_uniform(shape=[], minval=minval, maxval=maxval, dtype=tf.int32)\nv0 = tf.Variable(r)\nv1 = tf.Variable(v0)  # Same results even if you use v0.initialized_value()\nwith tf.Session() as session:\n  session.run([minval.initializer, maxval.initializer])\n  session.run([v0.initializer, v1.initializer])\n  print(session.run([v0, v1])) # Different values.\n\n# _build_initializer_expr copies the random op so there will be two of them.\nprint(sum([op.type == 'RandomUniformInt'\n           for op in tf.get_default_graph().get_operations()]))\n\n\nIf you have an initializer where there is an intermediate op X which takes an argument that must be a reference type, and that argument is a variable. This algorithm will turn the reference type into a value type and cause an error. For example:\n\nv0 = tf.Variable(tf.zeros(shape=[]))\none = tf.ones_like(v0)\nv0_plus_one = tf.assign(v0, v0.initialized_value() + one)\nv1 = tf.Variable(v0_plus_one)\nwith tf.Session() as session:\n  session.run(v1.initializer)  # InvalidArgumentError\n\n\nNot a bug, as such, but the implementation will produce an exponentially large number of nodes given a diamond pattern dependency graph (now fixed by memoizing):\n\n# Diamond pattern - exponential growth\n# N => N + 2^N - 1 Add ops\nN = 4\nv0 = tf.Variable(tf.zeros(shape=[]))\nx = v0\nfor i in range(N):\n  x = tf.add(x, x)\nv1 = tf.Variable(x)\nprint(sum([op.type == 'Add'\n           for op in tf.get_default_graph().get_operations()]))\nwith tf.Session() as session:\n  print(session.run(v1.initializer))\n  print(session.run(v1))\n\n\n\nThe implementation completely ignores control dependencies. Any new ops created simply don't have them.\n\n\nThe implementation completely ignores control flow contexts. Any new ops created based on ops in a control flow context aren't added to that context.\n\n\nThe implementation will only be able to find a Variable's initialized_value method if that variable is present in the GLOBAL_VARIABLES or LOCAL_VARIABLES collection. This is a weird inconsistency to the end user. It's even possible it could find the wrong Variable (although you'd have to be working really hard to get it to do that).\n\n\nThe implementation relies on a hard-coded set of known variable ops. If new variable ops are added it won't recognize them without manual intervention.\n\n\nEven if the caller explicitly passes in a .initialized_value() to a Variable's constructor the _build_initializer_expr will probably create a bunch of new nodes in the graph anyway.\n\n\nIt currently doesn't even do anything when the initial value depends on a ResourceVariable.\n\n\nThere are probably even more that I'm not currently aware of.\nI'll start working through these in the coming days. I'm working under the assumption that we can't just remove this transformation since there will already be a significant number of graphs which depend on it. That means most of the work will be to whitelist and/or blacklist the set of initial_value subgraphs for which we can safely apply the transformation.", "body": "I'm starting to push through changes to fix this. The first one is a refactoring to make subsequent changes easier as well as avoiding infinite recursion when the `initial_value` contains cycles (note that it also changes the name of `_build_initiazier_expr` to `_try_guard_against_uninitialized_depenencies` to clarify what it does). I'll now attempt to address more edge cases.\r\n\r\nHere's what I believe is a minimal reproduction of the bad behaviour in this case:\r\n\r\n```\r\np = tf.placeholder(dtype=tf.float32, shape=[])\r\nv0 = tf.Variable(p)\r\nv1 = tf.Variable(v0)\r\nwith tf.Session() as session:\r\n  session.run(v0.initializer, feed_dict={p: 0})\r\n  session.run(v1.initializer)  # InvalidArgumentError\r\n```\r\n\r\nThe transformation done by `_build_initializer_expr` makes `v1` depend on `v0.initialized_value()` which is a `cond` of the form:\r\n\r\n```\r\ncond(\r\n    is_variable_initialized(v0),\r\n    v0.read_value,\r\n    lambda: p)\r\n```\r\nBecause `p` isn't actually created inside `false_fn` it will always be evaluated when the `cond` is -- which is inconsistent with the intended behaviour. If you were to run that `session.run(v1.initializer)` statement and feed `p` it would work (but we shouldn't expect the user to do that).\r\n\r\nThis adds to the list of edge cases (that I'm aware of) where `_build_initializer_expr` behaves poorly:\r\n\r\n1) This issues of extraneous propagation of placeholders.\r\n\r\n2) Cyclic initializers (now \"fixed\" by having it ignore those graphs).\r\n\r\n3) When TF functions are involved. For example:\r\n\r\n```\r\n@function.Defun(tf.float32, shape_func=lambda op: [op.inputs[0].get_shape()])\r\ndef increment(x):\r\n  return x + tf.ones_like(x)\r\n\r\nv0 = tf.Variable(tf.zeros(shape=[]))\r\na = increment(v0)\r\nv = tf.Variable(a)  # NotFoundError: Op type not registered\r\n```\r\n\r\n4) With most stateful ops. For example:\r\n\r\n```\r\n# Graph rewriting results in wrong values when you have a variable initializer\r\n# which depends on a non-determinstic op which depends on a variable. Note that\r\n# we use random_uniform with integer types because the underlying op\r\n# \"RandomUniformInt\" takes in the minval and maxval as tensors unlike, for\r\n# example, random_normal with calls \"RandomStandardNormal\" and then transforms\r\n# it with the mean/stdev (so the non-determinsitic part doesn't actually)\r\n# depend on the variables and won't be rewritten.\r\nminval = tf.Variable(0, dtype=tf.int32)\r\nmaxval = tf.Variable(1000000, dtype=tf.int32)\r\nr = tf.random_uniform(shape=[], minval=minval, maxval=maxval, dtype=tf.int32)\r\nv0 = tf.Variable(r)\r\nv1 = tf.Variable(v0)  # Same results even if you use v0.initialized_value()\r\nwith tf.Session() as session:\r\n  session.run([minval.initializer, maxval.initializer])\r\n  session.run([v0.initializer, v1.initializer])\r\n  print(session.run([v0, v1])) # Different values.\r\n\r\n# _build_initializer_expr copies the random op so there will be two of them.\r\nprint(sum([op.type == 'RandomUniformInt'\r\n           for op in tf.get_default_graph().get_operations()]))\r\n```\r\n\r\n5) If you have an initializer where there is an intermediate op X which takes an argument that must be a reference type, and that argument is a variable. This algorithm will turn the reference type into a value type and cause an error. For example:\r\n\r\n```\r\nv0 = tf.Variable(tf.zeros(shape=[]))\r\none = tf.ones_like(v0)\r\nv0_plus_one = tf.assign(v0, v0.initialized_value() + one)\r\nv1 = tf.Variable(v0_plus_one)\r\nwith tf.Session() as session:\r\n  session.run(v1.initializer)  # InvalidArgumentError\r\n```\r\n\r\n6) Not a bug, as such, but the implementation will produce an exponentially large number of nodes given a diamond pattern dependency graph (now fixed by memoizing):\r\n\r\n```\r\n# Diamond pattern - exponential growth\r\n# N => N + 2^N - 1 Add ops\r\nN = 4\r\nv0 = tf.Variable(tf.zeros(shape=[]))\r\nx = v0\r\nfor i in range(N):\r\n  x = tf.add(x, x)\r\nv1 = tf.Variable(x)\r\nprint(sum([op.type == 'Add'\r\n           for op in tf.get_default_graph().get_operations()]))\r\nwith tf.Session() as session:\r\n  print(session.run(v1.initializer))\r\n  print(session.run(v1))\r\n```\r\n\r\n7) The implementation completely ignores control dependencies. Any new ops created simply don't have them.\r\n\r\n8) The implementation completely ignores control flow contexts. Any new ops created based on ops in a control flow context aren't added to that context.\r\n\r\n9) The implementation will only be able to find a Variable's initialized_value method if that variable is present in the `GLOBAL_VARIABLES` or `LOCAL_VARIABLES` collection. This is a weird inconsistency to the end user. It's even possible it could find the wrong Variable (although you'd have to be working really hard to get it to do that).\r\n\r\n10) The implementation relies on a hard-coded set of known variable ops. If new variable ops are added it won't recognize them without manual intervention.\r\n\r\n11) Even if the caller explicitly passes in a .initialized_value() to a Variable's constructor the `_build_initializer_expr` will probably create a bunch of new nodes in the graph anyway.\r\n\r\n12) It currently doesn't even do anything when the initial value depends on a ResourceVariable.\r\n\r\nThere are probably even more that I'm not currently aware of.\r\n\r\nI'll start working through these in the coming days. I'm working under the assumption that we can't just remove this transformation since there will already be a significant number of graphs which depend on it. That means most of the work will be to whitelist and/or blacklist the set of `initial_value` subgraphs for which we can safely apply the transformation.\r\n\r\n  "}