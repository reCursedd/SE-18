{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/289555311", "html_url": "https://github.com/tensorflow/tensorflow/issues/8712#issuecomment-289555311", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8712", "id": 289555311, "node_id": "MDEyOklzc3VlQ29tbWVudDI4OTU1NTMxMQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-27T19:13:18Z", "updated_at": "2017-03-27T19:13:18Z", "author_association": "MEMBER", "body_html": "<p>Could you elaborate a bit on the major overhead? The relative numbers would depend on the computation  being performed. For example, in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java\">label image example</a>, where the image fed is about 600KB (~150K 32-bit floats), creating the <code>Tensor</code> takes about ~100\u03bcs while executing the model takes about 30ms on my machine.</p>\n<p>That said, yes, you're right in that the current feed API requires a copy. It should be precisely one copy (note that <code>FloatBuffer.wrap()</code> etc. should normally not involve an array copy) - from the Java <code>FloatBuffer</code> object to the underlying <code>Tensor</code> object in C memory. If you see more than one copy, do let us know.</p>\n<p>We do not provide direct access to the C memory for safety reasons. For example, there is a private method (<a href=\"https://github.com/tensorflow/tensorflow/blob/2acba51/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L472\"><code>Tensor.buffer()</code></a>) that provides a view of the underlying C memory as a Java <code>ByteBuffer</code>. However, that isn't safe for general use as bad things would happen if say one released native memory (via <code>Tensor.close()</code> perhaps) and before accessing the contents of the buffer (e.g., <code>ByteBuffer buf = t.buffer(); t.close(); doSomethignWith(buf);</code>)</p>\n<p>The other option which I think you're asking about would be to avoid the C <code>Tensor</code> object allocating more memory and instead have it reference the Java <code>Buffer</code> object's underlying memory directly. That might be workable, modulo memory alignment requirements of the various TensorFlow kernels and some care to ensure that it is safe (for example, we wouldn't want the JVM's garbage collector from cleaning up the underlying memory while native code is executing a graph that requires it).</p>\n<p>That might be worth investigating. I was expecting that in practice both the time and space costs of copying feed tensors would be dwarfed by the computation costs of executing models (except perhaps in toy models), like in the label image example. However, if that is not the case, this optimization might be a good idea to pursue. Do you have some more details that you might be able to share on your use case?</p>", "body_text": "Could you elaborate a bit on the major overhead? The relative numbers would depend on the computation  being performed. For example, in label image example, where the image fed is about 600KB (~150K 32-bit floats), creating the Tensor takes about ~100\u03bcs while executing the model takes about 30ms on my machine.\nThat said, yes, you're right in that the current feed API requires a copy. It should be precisely one copy (note that FloatBuffer.wrap() etc. should normally not involve an array copy) - from the Java FloatBuffer object to the underlying Tensor object in C memory. If you see more than one copy, do let us know.\nWe do not provide direct access to the C memory for safety reasons. For example, there is a private method (Tensor.buffer()) that provides a view of the underlying C memory as a Java ByteBuffer. However, that isn't safe for general use as bad things would happen if say one released native memory (via Tensor.close() perhaps) and before accessing the contents of the buffer (e.g., ByteBuffer buf = t.buffer(); t.close(); doSomethignWith(buf);)\nThe other option which I think you're asking about would be to avoid the C Tensor object allocating more memory and instead have it reference the Java Buffer object's underlying memory directly. That might be workable, modulo memory alignment requirements of the various TensorFlow kernels and some care to ensure that it is safe (for example, we wouldn't want the JVM's garbage collector from cleaning up the underlying memory while native code is executing a graph that requires it).\nThat might be worth investigating. I was expecting that in practice both the time and space costs of copying feed tensors would be dwarfed by the computation costs of executing models (except perhaps in toy models), like in the label image example. However, if that is not the case, this optimization might be a good idea to pursue. Do you have some more details that you might be able to share on your use case?", "body": "Could you elaborate a bit on the major overhead? The relative numbers would depend on the computation  being performed. For example, in [label image example](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/examples/LabelImage.java), where the image fed is about 600KB (~150K 32-bit floats), creating the `Tensor` takes about ~100\u03bcs while executing the model takes about 30ms on my machine.\r\n\r\nThat said, yes, you're right in that the current feed API requires a copy. It should be precisely one copy (note that `FloatBuffer.wrap()` etc. should normally not involve an array copy) - from the Java `FloatBuffer` object to the underlying `Tensor` object in C memory. If you see more than one copy, do let us know.\r\n\r\nWe do not provide direct access to the C memory for safety reasons. For example, there is a private method ([`Tensor.buffer()`](https://github.com/tensorflow/tensorflow/blob/2acba51/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L472)) that provides a view of the underlying C memory as a Java `ByteBuffer`. However, that isn't safe for general use as bad things would happen if say one released native memory (via `Tensor.close()` perhaps) and before accessing the contents of the buffer (e.g., `ByteBuffer buf = t.buffer(); t.close(); doSomethignWith(buf);`)\r\n\r\nThe other option which I think you're asking about would be to avoid the C `Tensor` object allocating more memory and instead have it reference the Java `Buffer` object's underlying memory directly. That might be workable, modulo memory alignment requirements of the various TensorFlow kernels and some care to ensure that it is safe (for example, we wouldn't want the JVM's garbage collector from cleaning up the underlying memory while native code is executing a graph that requires it).\r\n\r\nThat might be worth investigating. I was expecting that in practice both the time and space costs of copying feed tensors would be dwarfed by the computation costs of executing models (except perhaps in toy models), like in the label image example. However, if that is not the case, this optimization might be a good idea to pursue. Do you have some more details that you might be able to share on your use case?\r\n\r\n"}