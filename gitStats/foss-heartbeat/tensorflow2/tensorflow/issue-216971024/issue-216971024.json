{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8712", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8712/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8712/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8712/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8712", "id": 216971024, "node_id": "MDU6SXNzdWUyMTY5NzEwMjQ=", "number": 8712, "title": "major memory allocation/copying overhead in Android Inference Library", "user": {"login": "Androbin", "id": 16437156, "node_id": "MDQ6VXNlcjE2NDM3MTU2", "avatar_url": "https://avatars1.githubusercontent.com/u/16437156?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Androbin", "html_url": "https://github.com/Androbin", "followers_url": "https://api.github.com/users/Androbin/followers", "following_url": "https://api.github.com/users/Androbin/following{/other_user}", "gists_url": "https://api.github.com/users/Androbin/gists{/gist_id}", "starred_url": "https://api.github.com/users/Androbin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Androbin/subscriptions", "organizations_url": "https://api.github.com/users/Androbin/orgs", "repos_url": "https://api.github.com/users/Androbin/repos", "events_url": "https://api.github.com/users/Androbin/events{/privacy}", "received_events_url": "https://api.github.com/users/Androbin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 13, "created_at": "2017-03-25T12:28:22Z", "updated_at": "2017-03-31T18:08:06Z", "closed_at": "2017-03-28T17:49:36Z", "author_association": "CONTRIBUTOR", "body_html": "<p>There seems to be a major overhead in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java#L257\"><code>TensorFlowInferenceInterface.feed(...)</code></a><br>\nWhen running the feed-run-fetch cycle, I noticed that feeding took an unreasonable amount of time.</p>\n<p>So I looked at the source code. I realized that I could pass a Buffer instead of an Array, because it would have been wrapped anyway. But either way a new Tensor object is created in <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L128\"><code>Tensor.create(...)</code></a> which includes first <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L204\">allocating the memory</a> and then copying over from the passed Buffer.</p>\n<p>Although this seems to be okay, when doing some Method Tracing, I discovered that when doing one cycle of feeding, running and fetching, it spends 99.99% with feeding, specifically 99.99% of that with creating the Tensor and 99.99% of that with copying over from the Buffer.</p>\n<p>The chart is really packed with <code>FloatBuffer.put(...)</code></p>\n<p>Why isn't it possible to use the just newly created Buffer directly?<br>\nOr why isn't the final Buffer filled directly from the Array?</p>", "body_text": "There seems to be a major overhead in TensorFlowInferenceInterface.feed(...)\nWhen running the feed-run-fetch cycle, I noticed that feeding took an unreasonable amount of time.\nSo I looked at the source code. I realized that I could pass a Buffer instead of an Array, because it would have been wrapped anyway. But either way a new Tensor object is created in Tensor.create(...) which includes first allocating the memory and then copying over from the passed Buffer.\nAlthough this seems to be okay, when doing some Method Tracing, I discovered that when doing one cycle of feeding, running and fetching, it spends 99.99% with feeding, specifically 99.99% of that with creating the Tensor and 99.99% of that with copying over from the Buffer.\nThe chart is really packed with FloatBuffer.put(...)\nWhy isn't it possible to use the just newly created Buffer directly?\nOr why isn't the final Buffer filled directly from the Array?", "body": "There seems to be a major overhead in [`TensorFlowInferenceInterface.feed(...)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/android/java/org/tensorflow/contrib/android/TensorFlowInferenceInterface.java#L257)\r\nWhen running the feed-run-fetch cycle, I noticed that feeding took an unreasonable amount of time.\r\n\r\nSo I looked at the source code. I realized that I could pass a Buffer instead of an Array, because it would have been wrapped anyway. But either way a new Tensor object is created in [`Tensor.create(...)`](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L128) which includes first [allocating the memory](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/java/src/main/java/org/tensorflow/Tensor.java#L204) and then copying over from the passed Buffer.\r\n\r\nAlthough this seems to be okay, when doing some Method Tracing, I discovered that when doing one cycle of feeding, running and fetching, it spends 99.99% with feeding, specifically 99.99% of that with creating the Tensor and 99.99% of that with copying over from the Buffer.\r\n\r\nThe chart is really packed with `FloatBuffer.put(...)`\r\n\r\nWhy isn't it possible to use the just newly created Buffer directly?\r\nOr why isn't the final Buffer filled directly from the Array?"}