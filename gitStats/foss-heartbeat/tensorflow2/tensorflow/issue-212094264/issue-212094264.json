{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8120", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8120/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8120/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8120/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8120", "id": 212094264, "node_id": "MDU6SXNzdWUyMTIwOTQyNjQ=", "number": 8120, "title": "Inconsistency in Variable Creation Methods", "user": {"login": "jaakkopasanen", "id": 8393524, "node_id": "MDQ6VXNlcjgzOTM1MjQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/8393524?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jaakkopasanen", "html_url": "https://github.com/jaakkopasanen", "followers_url": "https://api.github.com/users/jaakkopasanen/followers", "following_url": "https://api.github.com/users/jaakkopasanen/following{/other_user}", "gists_url": "https://api.github.com/users/jaakkopasanen/gists{/gist_id}", "starred_url": "https://api.github.com/users/jaakkopasanen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jaakkopasanen/subscriptions", "organizations_url": "https://api.github.com/users/jaakkopasanen/orgs", "repos_url": "https://api.github.com/users/jaakkopasanen/repos", "events_url": "https://api.github.com/users/jaakkopasanen/events{/privacy}", "received_events_url": "https://api.github.com/users/jaakkopasanen/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, {"login": "ebrevdo", "id": 1794715, "node_id": "MDQ6VXNlcjE3OTQ3MTU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1794715?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ebrevdo", "html_url": "https://github.com/ebrevdo", "followers_url": "https://api.github.com/users/ebrevdo/followers", "following_url": "https://api.github.com/users/ebrevdo/following{/other_user}", "gists_url": "https://api.github.com/users/ebrevdo/gists{/gist_id}", "starred_url": "https://api.github.com/users/ebrevdo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ebrevdo/subscriptions", "organizations_url": "https://api.github.com/users/ebrevdo/orgs", "repos_url": "https://api.github.com/users/ebrevdo/repos", "events_url": "https://api.github.com/users/ebrevdo/events{/privacy}", "received_events_url": "https://api.github.com/users/ebrevdo/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2017-03-06T11:24:33Z", "updated_at": "2017-03-16T21:45:27Z", "closed_at": "2017-03-16T21:45:27Z", "author_association": "NONE", "body_html": "<p>Currently it is not possible to scope all variables cleanly when using RNN cells and Optimizers. RNN cells create variables with tf.get_variable and optimizers create variables with tf.Variable. These two methods don't get along, creating messy variable names when variable scopes are handled differently by the two methods.</p>\n<p>Script for testing different variable creation and scoping combinations.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> -*- coding: utf-8 -*-</span>\n\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">build_loss</span>(<span class=\"pl-smi\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>):\n    y <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>int32<span class=\"pl-pds\">'</span></span>, [<span class=\"pl-c1\">None</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>y<span class=\"pl-pds\">'</span></span>)\n    x <span class=\"pl-k\">=</span> tf.placeholder(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">None</span>, <span class=\"pl-c1\">10</span>], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>x<span class=\"pl-pds\">'</span></span>)\n    cell <span class=\"pl-k\">=</span> tf.contrib.rnn.GRUCell(<span class=\"pl-c1\">128</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> RNN cell</span>\n    <span class=\"pl-k\">if</span> use_get_variable:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create variable with tf.get_variable</span>\n        w <span class=\"pl-k\">=</span> tf.get_variable(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>tf.random_normal([<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">10</span>]))\n    <span class=\"pl-k\">else</span>:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create variable with tf.Variable</span>\n        w <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-v\">initial_value</span><span class=\"pl-k\">=</span>tf.random_normal([<span class=\"pl-c1\">128</span>, <span class=\"pl-c1\">10</span>]), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>w<span class=\"pl-pds\">'</span></span>)\n    rnn_out, _ <span class=\"pl-k\">=</span> tf.nn.dynamic_rnn(cell, x, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>float32<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">time_major</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> RNN</span>\n    rnn_out <span class=\"pl-k\">=</span> rnn_out[:, <span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>, :]  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Last timestep</span>\n    rnn_out <span class=\"pl-k\">=</span> tf.matmul(rnn_out, w)  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Output layer projection</span>\n    loss_op <span class=\"pl-k\">=</span> tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(<span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span>y, <span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span>rnn_out))  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Cross entropy</span>\n    <span class=\"pl-k\">return</span> loss_op\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">inspect</span>(<span class=\"pl-smi\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-smi\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>, <span class=\"pl-smi\">optimizer</span><span class=\"pl-k\">=</span>tf.train.AdamOptimizer):\n    scope <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>scope<span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">if</span> re_enter_scope:\n        <span class=\"pl-k\">with</span> tf.variable_scope(scope) <span class=\"pl-k\">as</span> variable_scope:  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Capture variable scope</span>\n            <span class=\"pl-k\">if</span> re_enter_scope <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span>:\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Save captured object for re-entering scope with captured variable scope object</span>\n                scope <span class=\"pl-k\">=</span> variable_scope\n            <span class=\"pl-k\">elif</span> re_enter_scope <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>original_name_scope<span class=\"pl-pds\">'</span></span>:\n                <span class=\"pl-c\"><span class=\"pl-c\">#</span> Save original name scope of captured variable scope for re-entering the scope with original name scope</span>\n                scope <span class=\"pl-k\">=</span> variable_scope.original_name_scope\n    <span class=\"pl-k\">with</span> tf.variable_scope(scope):  <span class=\"pl-c\"><span class=\"pl-c\">#</span> (Re-)enter scope</span>\n        loss_op <span class=\"pl-k\">=</span> build_loss(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span>use_get_variable)\n        <span class=\"pl-k\">if</span> scope_optimizer:\n            <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create optimizer in the variable scope</span>\n            train_op <span class=\"pl-k\">=</span> optimizer(<span class=\"pl-c1\">0.1</span>).minimize(loss_op)\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> scope_optimizer:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Create optimizer outside of variable scope</span>\n        train_op <span class=\"pl-k\">=</span> optimizer(<span class=\"pl-c1\">0.1</span>).minimize(loss_op)\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Initialize variables</span>\n        sess.run(tf.global_variables_initializer())\n\n    description <span class=\"pl-k\">=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>optimizer <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>NOT<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> scope_optimizer <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>IS<span class=\"pl-pds\">'</span></span>) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span> scoped, <span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> re_enter_scope:\n        description <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>using ORIGINAL scope, <span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">elif</span> re_enter_scope <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span>:\n        description <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>re-entering scope with OBJECT, <span class=\"pl-pds\">'</span></span>\n    <span class=\"pl-k\">elif</span> re_enter_scope <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>original_name_scope<span class=\"pl-pds\">'</span></span>:\n        description <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>re-entering scope with ORIGINAL_NAME_SCOPE, <span class=\"pl-pds\">'</span></span>\n    description <span class=\"pl-k\">+=</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>variables created with <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> (<span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf.get_variable<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">if</span> use_get_variable <span class=\"pl-k\">else</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>tf.Variable<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c1\">print</span>(description)\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span><span class=\"pl-cce\">\\n</span><span class=\"pl-pds\">'</span></span>.join([<span class=\"pl-s\"><span class=\"pl-pds\">'</span>  <span class=\"pl-pds\">'</span></span><span class=\"pl-k\">+</span>var.name <span class=\"pl-k\">for</span> var <span class=\"pl-k\">in</span> tf.global_variables()]))\n    <span class=\"pl-c1\">print</span>()\n\n    tf.reset_default_graph()\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">main</span>():\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Ideally scope everything with re-entered scope, leads to double scoping of gradient variables and having duplicate</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> scope (with unique name) for Adam optimizer beta power variables</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Not scoping optimizer fixes double scoping of RNN variables but leads to having AdamOptimizer beta_powers not</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> scoped</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Using tf.Variable for variable creation when re-entering scope with scope object leads to creation of duplicate</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> scope (with unique name) for non-RNN variables</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Changing scope re-entering method to variable_scope.original_name_scope fixes the problem of duplicate scope</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> when using tf.Variable for variable creation, but creates additional problem of having double slashes in</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> scope hierarchy path with RNN variables.</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>original_name_scope<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Similar problem is observed when using tf.get_variable for variable creation and re-entering scope with</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> original_name_scope but this time double slashes are observed also with non-RNN variables.</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>original_name_scope<span class=\"pl-pds\">'</span></span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Scoping optimizer without re-entering the scope fixes scoping of Adam optimizer beta power variables but does not</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> fix the problem of having double scopes for RNN variables. However re-entering scopes is desired for OOP.</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Using tf.get_variable for variable creation won't help since Optimizer creates variables with tf.Variable and RNN</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> cells create variables with tf.get_variable, these two methods don't get along</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Swapping AdamOptimizer for RMSPropOptimizer fixes the problem with AdamOptimizer's beta power variables</span>\n    inspect(<span class=\"pl-v\">use_get_variable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>, <span class=\"pl-v\">scope_optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">re_enter_scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>object<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span>tf.train.RMSPropOptimizer)\n\n\n<span class=\"pl-k\">if</span> <span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>__main__<span class=\"pl-pds\">'</span></span>:\n    main()</pre></div>\n<p>outputs</p>\n<pre><code>optimizer IS scoped, re-entering scope with OBJECT, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope_1/beta1_power:0\n  scope_1/beta2_power:0\n  scope/scope/w/Adam:0\n  scope/scope/w/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope/w/Adam:0\n  scope/w/Adam_1:0\n  scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.Variable\n  scope_1/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope_1/w/Adam:0\n  scope_1/w/Adam_1:0\n  scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.Variable\n  scope/w:0\n  scope//rnn/gru_cell/gates/weights:0\n  scope//rnn/gru_cell/gates/biases:0\n  scope//rnn/gru_cell/candidate/weights:0\n  scope//rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope/w/Adam:0\n  scope/w/Adam_1:0\n  scope//rnn/gru_cell/gates/weights/Adam:0\n  scope//rnn/gru_cell/gates/weights/Adam_1:0\n  scope//rnn/gru_cell/gates/biases/Adam:0\n  scope//rnn/gru_cell/gates/biases/Adam_1:0\n  scope//rnn/gru_cell/candidate/weights/Adam:0\n  scope//rnn/gru_cell/candidate/weights/Adam_1:0\n  scope//rnn/gru_cell/candidate/biases/Adam:0\n  scope//rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.get_variable\n  scope//w:0\n  scope//rnn/gru_cell/gates/weights:0\n  scope//rnn/gru_cell/gates/biases:0\n  scope//rnn/gru_cell/candidate/weights:0\n  scope//rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope//w/Adam:0\n  scope//w/Adam_1:0\n  scope//rnn/gru_cell/gates/weights/Adam:0\n  scope//rnn/gru_cell/gates/weights/Adam_1:0\n  scope//rnn/gru_cell/gates/biases/Adam:0\n  scope//rnn/gru_cell/gates/biases/Adam_1:0\n  scope//rnn/gru_cell/candidate/weights/Adam:0\n  scope//rnn/gru_cell/candidate/weights/Adam_1:0\n  scope//rnn/gru_cell/candidate/biases/Adam:0\n  scope//rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer IS scoped, using ORIGINAL scope, variables created with tf.Variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope/beta1_power:0\n  scope/beta2_power:0\n  scope/scope/w/Adam:0\n  scope/scope/w/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer IS scoped, using ORIGINAL scope, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope/beta1_power:0\n  scope/beta2_power:0\n  scope/scope/w/Adam:0\n  scope/scope/w/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope/w/RMSProp:0\n  scope/w/RMSProp_1:0\n  scope/rnn/gru_cell/gates/weights/RMSProp:0\n  scope/rnn/gru_cell/gates/weights/RMSProp_1:0\n  scope/rnn/gru_cell/gates/biases/RMSProp:0\n  scope/rnn/gru_cell/gates/biases/RMSProp_1:0\n  scope/rnn/gru_cell/candidate/weights/RMSProp:0\n  scope/rnn/gru_cell/candidate/weights/RMSProp_1:0\n  scope/rnn/gru_cell/candidate/biases/RMSProp:0\n  scope/rnn/gru_cell/candidate/biases/RMSProp_1:0\n</code></pre>\n<p>Workaround for clean scoping is to create optimizer op outside of the variable scope and using eg. <code>RMSPropOptimizer</code>. However this is confusing behaviour, ideally everything should be able to be scoped. Unfortunately there is nothing to get variable names clean when using <code>AdamOptimizer</code>. <code>AdamOptimizer</code> creates variables for beta powers with <code>tf.Variable</code>.</p>\n<p>All of the mentioned problems would probably be fixed by using <code>tf.get_variable</code> everywhere. Until (and if) this change happens users should be instructed to use <code>tf.get_variable</code> only, creating optimizer outside of variable scope and avoid using <code>AdamOptimizer</code> if clean scoping is desired. Double scoping of RNN gradient variables would be tolerable but having several nested classes implementing different models with their own variable scopes lead to extremely messy variable names making debugging more difficult.</p>\n<p>Tested on Ubuntu 16.04 with Tensorflow 1.0.0 installed with pip.</p>\n<p>Related issues:<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"191018655\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/5786\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/5786/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/5786\">#5786</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"194303257\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6189\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6189/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6189\">#6189</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"192800636\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6007\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6007/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6007\">#6007</a></p>", "body_text": "Currently it is not possible to scope all variables cleanly when using RNN cells and Optimizers. RNN cells create variables with tf.get_variable and optimizers create variables with tf.Variable. These two methods don't get along, creating messy variable names when variable scopes are handled differently by the two methods.\nScript for testing different variable creation and scoping combinations.\n# -*- coding: utf-8 -*-\n\nimport tensorflow as tf\n\n\ndef build_loss(use_get_variable=True):\n    y = tf.placeholder('int32', [None], name='y')\n    x = tf.placeholder('float32', shape=[None, None, 10], name='x')\n    cell = tf.contrib.rnn.GRUCell(128)  # RNN cell\n    if use_get_variable:\n        # Create variable with tf.get_variable\n        w = tf.get_variable('w', dtype='float32', initializer=tf.random_normal([128, 10]))\n    else:\n        # Create variable with tf.Variable\n        w = tf.Variable(initial_value=tf.random_normal([128, 10]), dtype='float32', name='w')\n    rnn_out, _ = tf.nn.dynamic_rnn(cell, x, dtype='float32', time_major=False)  # RNN\n    rnn_out = rnn_out[:, -1, :]  # Last timestep\n    rnn_out = tf.matmul(rnn_out, w)  # Output layer projection\n    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=rnn_out))  # Cross entropy\n    return loss_op\n\n\ndef inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope=None, optimizer=tf.train.AdamOptimizer):\n    scope = 'scope'\n    if re_enter_scope:\n        with tf.variable_scope(scope) as variable_scope:  # Capture variable scope\n            if re_enter_scope == 'object':\n                # Save captured object for re-entering scope with captured variable scope object\n                scope = variable_scope\n            elif re_enter_scope == 'original_name_scope':\n                # Save original name scope of captured variable scope for re-entering the scope with original name scope\n                scope = variable_scope.original_name_scope\n    with tf.variable_scope(scope):  # (Re-)enter scope\n        loss_op = build_loss(use_get_variable=use_get_variable)\n        if scope_optimizer:\n            # Create optimizer in the variable scope\n            train_op = optimizer(0.1).minimize(loss_op)\n    if not scope_optimizer:\n        # Create optimizer outside of variable scope\n        train_op = optimizer(0.1).minimize(loss_op)\n    with tf.Session() as sess:\n        # Initialize variables\n        sess.run(tf.global_variables_initializer())\n\n    description = 'optimizer ' + ('NOT' if not scope_optimizer else 'IS') + ' scoped, '\n    if not re_enter_scope:\n        description += 'using ORIGINAL scope, '\n    elif re_enter_scope == 'object':\n        description += 're-entering scope with OBJECT, '\n    elif re_enter_scope == 'original_name_scope':\n        description += 're-entering scope with ORIGINAL_NAME_SCOPE, '\n    description += 'variables created with ' + ('tf.get_variable' if use_get_variable else 'tf.Variable')\n    print(description)\n\n    print('\\n'.join(['  '+var.name for var in tf.global_variables()]))\n    print()\n\n    tf.reset_default_graph()\n\n\ndef main():\n    # Ideally scope everything with re-entered scope, leads to double scoping of gradient variables and having duplicate\n    # scope (with unique name) for Adam optimizer beta power variables\n    inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope='object')\n    # Not scoping optimizer fixes double scoping of RNN variables but leads to having AdamOptimizer beta_powers not\n    # scoped\n    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='object')\n    # Using tf.Variable for variable creation when re-entering scope with scope object leads to creation of duplicate\n    # scope (with unique name) for non-RNN variables\n    inspect(use_get_variable=False, scope_optimizer=False, re_enter_scope='object')\n    # Changing scope re-entering method to variable_scope.original_name_scope fixes the problem of duplicate scope\n    # when using tf.Variable for variable creation, but creates additional problem of having double slashes in\n    # scope hierarchy path with RNN variables.\n    inspect(use_get_variable=False, scope_optimizer=False, re_enter_scope='original_name_scope')\n    # Similar problem is observed when using tf.get_variable for variable creation and re-entering scope with\n    # original_name_scope but this time double slashes are observed also with non-RNN variables.\n    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='original_name_scope')\n    # Scoping optimizer without re-entering the scope fixes scoping of Adam optimizer beta power variables but does not\n    # fix the problem of having double scopes for RNN variables. However re-entering scopes is desired for OOP.\n    inspect(use_get_variable=False, scope_optimizer=True, re_enter_scope=False)\n    # Using tf.get_variable for variable creation won't help since Optimizer creates variables with tf.Variable and RNN\n    # cells create variables with tf.get_variable, these two methods don't get along\n    inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope=False)\n    # Swapping AdamOptimizer for RMSPropOptimizer fixes the problem with AdamOptimizer's beta power variables\n    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='object', optimizer=tf.train.RMSPropOptimizer)\n\n\nif __name__ == '__main__':\n    main()\noutputs\noptimizer IS scoped, re-entering scope with OBJECT, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope_1/beta1_power:0\n  scope_1/beta2_power:0\n  scope/scope/w/Adam:0\n  scope/scope/w/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope/w/Adam:0\n  scope/w/Adam_1:0\n  scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.Variable\n  scope_1/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope_1/w/Adam:0\n  scope_1/w/Adam_1:0\n  scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.Variable\n  scope/w:0\n  scope//rnn/gru_cell/gates/weights:0\n  scope//rnn/gru_cell/gates/biases:0\n  scope//rnn/gru_cell/candidate/weights:0\n  scope//rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope/w/Adam:0\n  scope/w/Adam_1:0\n  scope//rnn/gru_cell/gates/weights/Adam:0\n  scope//rnn/gru_cell/gates/weights/Adam_1:0\n  scope//rnn/gru_cell/gates/biases/Adam:0\n  scope//rnn/gru_cell/gates/biases/Adam_1:0\n  scope//rnn/gru_cell/candidate/weights/Adam:0\n  scope//rnn/gru_cell/candidate/weights/Adam_1:0\n  scope//rnn/gru_cell/candidate/biases/Adam:0\n  scope//rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.get_variable\n  scope//w:0\n  scope//rnn/gru_cell/gates/weights:0\n  scope//rnn/gru_cell/gates/biases:0\n  scope//rnn/gru_cell/candidate/weights:0\n  scope//rnn/gru_cell/candidate/biases:0\n  beta1_power:0\n  beta2_power:0\n  scope//w/Adam:0\n  scope//w/Adam_1:0\n  scope//rnn/gru_cell/gates/weights/Adam:0\n  scope//rnn/gru_cell/gates/weights/Adam_1:0\n  scope//rnn/gru_cell/gates/biases/Adam:0\n  scope//rnn/gru_cell/gates/biases/Adam_1:0\n  scope//rnn/gru_cell/candidate/weights/Adam:0\n  scope//rnn/gru_cell/candidate/weights/Adam_1:0\n  scope//rnn/gru_cell/candidate/biases/Adam:0\n  scope//rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer IS scoped, using ORIGINAL scope, variables created with tf.Variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope/beta1_power:0\n  scope/beta2_power:0\n  scope/scope/w/Adam:0\n  scope/scope/w/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer IS scoped, using ORIGINAL scope, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope/beta1_power:0\n  scope/beta2_power:0\n  scope/scope/w/Adam:0\n  scope/scope/w/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\n\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable\n  scope/w:0\n  scope/rnn/gru_cell/gates/weights:0\n  scope/rnn/gru_cell/gates/biases:0\n  scope/rnn/gru_cell/candidate/weights:0\n  scope/rnn/gru_cell/candidate/biases:0\n  scope/w/RMSProp:0\n  scope/w/RMSProp_1:0\n  scope/rnn/gru_cell/gates/weights/RMSProp:0\n  scope/rnn/gru_cell/gates/weights/RMSProp_1:0\n  scope/rnn/gru_cell/gates/biases/RMSProp:0\n  scope/rnn/gru_cell/gates/biases/RMSProp_1:0\n  scope/rnn/gru_cell/candidate/weights/RMSProp:0\n  scope/rnn/gru_cell/candidate/weights/RMSProp_1:0\n  scope/rnn/gru_cell/candidate/biases/RMSProp:0\n  scope/rnn/gru_cell/candidate/biases/RMSProp_1:0\n\nWorkaround for clean scoping is to create optimizer op outside of the variable scope and using eg. RMSPropOptimizer. However this is confusing behaviour, ideally everything should be able to be scoped. Unfortunately there is nothing to get variable names clean when using AdamOptimizer. AdamOptimizer creates variables for beta powers with tf.Variable.\nAll of the mentioned problems would probably be fixed by using tf.get_variable everywhere. Until (and if) this change happens users should be instructed to use tf.get_variable only, creating optimizer outside of variable scope and avoid using AdamOptimizer if clean scoping is desired. Double scoping of RNN gradient variables would be tolerable but having several nested classes implementing different models with their own variable scopes lead to extremely messy variable names making debugging more difficult.\nTested on Ubuntu 16.04 with Tensorflow 1.0.0 installed with pip.\nRelated issues:\n#5786\n#6189\n#6007", "body": "Currently it is not possible to scope all variables cleanly when using RNN cells and Optimizers. RNN cells create variables with tf.get_variable and optimizers create variables with tf.Variable. These two methods don't get along, creating messy variable names when variable scopes are handled differently by the two methods.\r\n\r\nScript for testing different variable creation and scoping combinations.\r\n```python\r\n# -*- coding: utf-8 -*-\r\n\r\nimport tensorflow as tf\r\n\r\n\r\ndef build_loss(use_get_variable=True):\r\n    y = tf.placeholder('int32', [None], name='y')\r\n    x = tf.placeholder('float32', shape=[None, None, 10], name='x')\r\n    cell = tf.contrib.rnn.GRUCell(128)  # RNN cell\r\n    if use_get_variable:\r\n        # Create variable with tf.get_variable\r\n        w = tf.get_variable('w', dtype='float32', initializer=tf.random_normal([128, 10]))\r\n    else:\r\n        # Create variable with tf.Variable\r\n        w = tf.Variable(initial_value=tf.random_normal([128, 10]), dtype='float32', name='w')\r\n    rnn_out, _ = tf.nn.dynamic_rnn(cell, x, dtype='float32', time_major=False)  # RNN\r\n    rnn_out = rnn_out[:, -1, :]  # Last timestep\r\n    rnn_out = tf.matmul(rnn_out, w)  # Output layer projection\r\n    loss_op = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=rnn_out))  # Cross entropy\r\n    return loss_op\r\n\r\n\r\ndef inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope=None, optimizer=tf.train.AdamOptimizer):\r\n    scope = 'scope'\r\n    if re_enter_scope:\r\n        with tf.variable_scope(scope) as variable_scope:  # Capture variable scope\r\n            if re_enter_scope == 'object':\r\n                # Save captured object for re-entering scope with captured variable scope object\r\n                scope = variable_scope\r\n            elif re_enter_scope == 'original_name_scope':\r\n                # Save original name scope of captured variable scope for re-entering the scope with original name scope\r\n                scope = variable_scope.original_name_scope\r\n    with tf.variable_scope(scope):  # (Re-)enter scope\r\n        loss_op = build_loss(use_get_variable=use_get_variable)\r\n        if scope_optimizer:\r\n            # Create optimizer in the variable scope\r\n            train_op = optimizer(0.1).minimize(loss_op)\r\n    if not scope_optimizer:\r\n        # Create optimizer outside of variable scope\r\n        train_op = optimizer(0.1).minimize(loss_op)\r\n    with tf.Session() as sess:\r\n        # Initialize variables\r\n        sess.run(tf.global_variables_initializer())\r\n\r\n    description = 'optimizer ' + ('NOT' if not scope_optimizer else 'IS') + ' scoped, '\r\n    if not re_enter_scope:\r\n        description += 'using ORIGINAL scope, '\r\n    elif re_enter_scope == 'object':\r\n        description += 're-entering scope with OBJECT, '\r\n    elif re_enter_scope == 'original_name_scope':\r\n        description += 're-entering scope with ORIGINAL_NAME_SCOPE, '\r\n    description += 'variables created with ' + ('tf.get_variable' if use_get_variable else 'tf.Variable')\r\n    print(description)\r\n\r\n    print('\\n'.join(['  '+var.name for var in tf.global_variables()]))\r\n    print()\r\n\r\n    tf.reset_default_graph()\r\n\r\n\r\ndef main():\r\n    # Ideally scope everything with re-entered scope, leads to double scoping of gradient variables and having duplicate\r\n    # scope (with unique name) for Adam optimizer beta power variables\r\n    inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope='object')\r\n    # Not scoping optimizer fixes double scoping of RNN variables but leads to having AdamOptimizer beta_powers not\r\n    # scoped\r\n    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='object')\r\n    # Using tf.Variable for variable creation when re-entering scope with scope object leads to creation of duplicate\r\n    # scope (with unique name) for non-RNN variables\r\n    inspect(use_get_variable=False, scope_optimizer=False, re_enter_scope='object')\r\n    # Changing scope re-entering method to variable_scope.original_name_scope fixes the problem of duplicate scope\r\n    # when using tf.Variable for variable creation, but creates additional problem of having double slashes in\r\n    # scope hierarchy path with RNN variables.\r\n    inspect(use_get_variable=False, scope_optimizer=False, re_enter_scope='original_name_scope')\r\n    # Similar problem is observed when using tf.get_variable for variable creation and re-entering scope with\r\n    # original_name_scope but this time double slashes are observed also with non-RNN variables.\r\n    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='original_name_scope')\r\n    # Scoping optimizer without re-entering the scope fixes scoping of Adam optimizer beta power variables but does not\r\n    # fix the problem of having double scopes for RNN variables. However re-entering scopes is desired for OOP.\r\n    inspect(use_get_variable=False, scope_optimizer=True, re_enter_scope=False)\r\n    # Using tf.get_variable for variable creation won't help since Optimizer creates variables with tf.Variable and RNN\r\n    # cells create variables with tf.get_variable, these two methods don't get along\r\n    inspect(use_get_variable=True, scope_optimizer=True, re_enter_scope=False)\r\n    # Swapping AdamOptimizer for RMSPropOptimizer fixes the problem with AdamOptimizer's beta power variables\r\n    inspect(use_get_variable=True, scope_optimizer=False, re_enter_scope='object', optimizer=tf.train.RMSPropOptimizer)\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\noutputs\r\n```\r\noptimizer IS scoped, re-entering scope with OBJECT, variables created with tf.get_variable\r\n  scope/w:0\r\n  scope/rnn/gru_cell/gates/weights:0\r\n  scope/rnn/gru_cell/gates/biases:0\r\n  scope/rnn/gru_cell/candidate/weights:0\r\n  scope/rnn/gru_cell/candidate/biases:0\r\n  scope_1/beta1_power:0\r\n  scope_1/beta2_power:0\r\n  scope/scope/w/Adam:0\r\n  scope/scope/w/Adam_1:0\r\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\r\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\r\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\r\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\r\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable\r\n  scope/w:0\r\n  scope/rnn/gru_cell/gates/weights:0\r\n  scope/rnn/gru_cell/gates/biases:0\r\n  scope/rnn/gru_cell/candidate/weights:0\r\n  scope/rnn/gru_cell/candidate/biases:0\r\n  beta1_power:0\r\n  beta2_power:0\r\n  scope/w/Adam:0\r\n  scope/w/Adam_1:0\r\n  scope/rnn/gru_cell/gates/weights/Adam:0\r\n  scope/rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope/rnn/gru_cell/gates/biases/Adam:0\r\n  scope/rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope/rnn/gru_cell/candidate/weights/Adam:0\r\n  scope/rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope/rnn/gru_cell/candidate/biases/Adam:0\r\n  scope/rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.Variable\r\n  scope_1/w:0\r\n  scope/rnn/gru_cell/gates/weights:0\r\n  scope/rnn/gru_cell/gates/biases:0\r\n  scope/rnn/gru_cell/candidate/weights:0\r\n  scope/rnn/gru_cell/candidate/biases:0\r\n  beta1_power:0\r\n  beta2_power:0\r\n  scope_1/w/Adam:0\r\n  scope_1/w/Adam_1:0\r\n  scope/rnn/gru_cell/gates/weights/Adam:0\r\n  scope/rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope/rnn/gru_cell/gates/biases/Adam:0\r\n  scope/rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope/rnn/gru_cell/candidate/weights/Adam:0\r\n  scope/rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope/rnn/gru_cell/candidate/biases/Adam:0\r\n  scope/rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.Variable\r\n  scope/w:0\r\n  scope//rnn/gru_cell/gates/weights:0\r\n  scope//rnn/gru_cell/gates/biases:0\r\n  scope//rnn/gru_cell/candidate/weights:0\r\n  scope//rnn/gru_cell/candidate/biases:0\r\n  beta1_power:0\r\n  beta2_power:0\r\n  scope/w/Adam:0\r\n  scope/w/Adam_1:0\r\n  scope//rnn/gru_cell/gates/weights/Adam:0\r\n  scope//rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope//rnn/gru_cell/gates/biases/Adam:0\r\n  scope//rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope//rnn/gru_cell/candidate/weights/Adam:0\r\n  scope//rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope//rnn/gru_cell/candidate/biases/Adam:0\r\n  scope//rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer NOT scoped, re-entering scope with ORIGINAL_NAME_SCOPE, variables created with tf.get_variable\r\n  scope//w:0\r\n  scope//rnn/gru_cell/gates/weights:0\r\n  scope//rnn/gru_cell/gates/biases:0\r\n  scope//rnn/gru_cell/candidate/weights:0\r\n  scope//rnn/gru_cell/candidate/biases:0\r\n  beta1_power:0\r\n  beta2_power:0\r\n  scope//w/Adam:0\r\n  scope//w/Adam_1:0\r\n  scope//rnn/gru_cell/gates/weights/Adam:0\r\n  scope//rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope//rnn/gru_cell/gates/biases/Adam:0\r\n  scope//rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope//rnn/gru_cell/candidate/weights/Adam:0\r\n  scope//rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope//rnn/gru_cell/candidate/biases/Adam:0\r\n  scope//rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer IS scoped, using ORIGINAL scope, variables created with tf.Variable\r\n  scope/w:0\r\n  scope/rnn/gru_cell/gates/weights:0\r\n  scope/rnn/gru_cell/gates/biases:0\r\n  scope/rnn/gru_cell/candidate/weights:0\r\n  scope/rnn/gru_cell/candidate/biases:0\r\n  scope/beta1_power:0\r\n  scope/beta2_power:0\r\n  scope/scope/w/Adam:0\r\n  scope/scope/w/Adam_1:0\r\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\r\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\r\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\r\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\r\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer IS scoped, using ORIGINAL scope, variables created with tf.get_variable\r\n  scope/w:0\r\n  scope/rnn/gru_cell/gates/weights:0\r\n  scope/rnn/gru_cell/gates/biases:0\r\n  scope/rnn/gru_cell/candidate/weights:0\r\n  scope/rnn/gru_cell/candidate/biases:0\r\n  scope/beta1_power:0\r\n  scope/beta2_power:0\r\n  scope/scope/w/Adam:0\r\n  scope/scope/w/Adam_1:0\r\n  scope/scope/rnn/gru_cell/gates/weights/Adam:0\r\n  scope/scope/rnn/gru_cell/gates/weights/Adam_1:0\r\n  scope/scope/rnn/gru_cell/gates/biases/Adam:0\r\n  scope/scope/rnn/gru_cell/gates/biases/Adam_1:0\r\n  scope/scope/rnn/gru_cell/candidate/weights/Adam:0\r\n  scope/scope/rnn/gru_cell/candidate/weights/Adam_1:0\r\n  scope/scope/rnn/gru_cell/candidate/biases/Adam:0\r\n  scope/scope/rnn/gru_cell/candidate/biases/Adam_1:0\r\n\r\noptimizer NOT scoped, re-entering scope with OBJECT, variables created with tf.get_variable\r\n  scope/w:0\r\n  scope/rnn/gru_cell/gates/weights:0\r\n  scope/rnn/gru_cell/gates/biases:0\r\n  scope/rnn/gru_cell/candidate/weights:0\r\n  scope/rnn/gru_cell/candidate/biases:0\r\n  scope/w/RMSProp:0\r\n  scope/w/RMSProp_1:0\r\n  scope/rnn/gru_cell/gates/weights/RMSProp:0\r\n  scope/rnn/gru_cell/gates/weights/RMSProp_1:0\r\n  scope/rnn/gru_cell/gates/biases/RMSProp:0\r\n  scope/rnn/gru_cell/gates/biases/RMSProp_1:0\r\n  scope/rnn/gru_cell/candidate/weights/RMSProp:0\r\n  scope/rnn/gru_cell/candidate/weights/RMSProp_1:0\r\n  scope/rnn/gru_cell/candidate/biases/RMSProp:0\r\n  scope/rnn/gru_cell/candidate/biases/RMSProp_1:0\r\n```\r\n\r\nWorkaround for clean scoping is to create optimizer op outside of the variable scope and using eg. `RMSPropOptimizer`. However this is confusing behaviour, ideally everything should be able to be scoped. Unfortunately there is nothing to get variable names clean when using `AdamOptimizer`. `AdamOptimizer` creates variables for beta powers with `tf.Variable`.\r\n\r\nAll of the mentioned problems would probably be fixed by using `tf.get_variable` everywhere. Until (and if) this change happens users should be instructed to use `tf.get_variable` only, creating optimizer outside of variable scope and avoid using `AdamOptimizer` if clean scoping is desired. Double scoping of RNN gradient variables would be tolerable but having several nested classes implementing different models with their own variable scopes lead to extremely messy variable names making debugging more difficult.\r\n\r\nTested on Ubuntu 16.04 with Tensorflow 1.0.0 installed with pip.\r\n\r\nRelated issues:\r\n#5786 \r\n#6189 \r\n#6007 \r\n"}