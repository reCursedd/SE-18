{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7244", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7244/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7244/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7244/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7244", "id": 205201416, "node_id": "MDU6SXNzdWUyMDUyMDE0MTY=", "number": 7244, "title": "Saver unable to restore from checkpoint file although it had successfully restored previously.", "user": {"login": "kwotsin", "id": 11178344, "node_id": "MDQ6VXNlcjExMTc4MzQ0", "avatar_url": "https://avatars2.githubusercontent.com/u/11178344?v=4", "gravatar_id": "", "url": "https://api.github.com/users/kwotsin", "html_url": "https://github.com/kwotsin", "followers_url": "https://api.github.com/users/kwotsin/followers", "following_url": "https://api.github.com/users/kwotsin/following{/other_user}", "gists_url": "https://api.github.com/users/kwotsin/gists{/gist_id}", "starred_url": "https://api.github.com/users/kwotsin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/kwotsin/subscriptions", "organizations_url": "https://api.github.com/users/kwotsin/orgs", "repos_url": "https://api.github.com/users/kwotsin/repos", "events_url": "https://api.github.com/users/kwotsin/events{/privacy}", "received_events_url": "https://api.github.com/users/kwotsin/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 2, "created_at": "2017-02-03T16:18:26Z", "updated_at": "2017-02-04T09:33:39Z", "closed_at": "2017-02-04T09:33:39Z", "author_association": "CONTRIBUTOR", "body_html": "<p>I am currently seeing a very strange behavior from using tf.train.Saver. Basically I had previously successfully run my code and restored all the variables from the TensorFlow-slim inception-resnet-v2 model and even obtained my losses and predictions. However, after I added some scopes to exclude in <code>variables_to_restore = slim.get_variables_to_restore(exclude = [...])</code> , which I successfully run,  when I returned back to using no excluded scope, my code actually fails. It gave me the following error:</p>\n<p><code>NotFoundError (see above for traceback): Tensor name \"InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/Adam\" not found in checkpoint files ./inception_resnet_v2_2016_08_30.ckpt</code></p>\n<p>Here is the code I run.</p>\n<pre><code>with slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\n\tlogits, end_points = inception_resnet_v2.inception_resnet_v2(image_batch, is_training = False)\n\tpredictions = end_points['Predictions']\n\n#....then the typical building of operations till you get the train_op\n\nvariables_to_restore = slim.get_variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore) #This is the line where the errors appear\ndef restore_fn(sess):\n\treturn saver.restore(sess, checkpoint_file)\nsv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n</code></pre>\n<p>I have even replaced my checkpoint file again and this error persists. Why does this happen?</p>\n<p>I strongly suspect it might have been because the original graph has been edited again as I did not encapsulate most of the definitions within a specified graph like <code>with tf.Graph.as_default() as g:</code>, but at the start of my code I have always done <code>tf.reset_default_graph()</code>.</p>", "body_text": "I am currently seeing a very strange behavior from using tf.train.Saver. Basically I had previously successfully run my code and restored all the variables from the TensorFlow-slim inception-resnet-v2 model and even obtained my losses and predictions. However, after I added some scopes to exclude in variables_to_restore = slim.get_variables_to_restore(exclude = [...]) , which I successfully run,  when I returned back to using no excluded scope, my code actually fails. It gave me the following error:\nNotFoundError (see above for traceback): Tensor name \"InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/Adam\" not found in checkpoint files ./inception_resnet_v2_2016_08_30.ckpt\nHere is the code I run.\nwith slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\n\tlogits, end_points = inception_resnet_v2.inception_resnet_v2(image_batch, is_training = False)\n\tpredictions = end_points['Predictions']\n\n#....then the typical building of operations till you get the train_op\n\nvariables_to_restore = slim.get_variables_to_restore()\nsaver = tf.train.Saver(variables_to_restore) #This is the line where the errors appear\ndef restore_fn(sess):\n\treturn saver.restore(sess, checkpoint_file)\nsv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\n\nI have even replaced my checkpoint file again and this error persists. Why does this happen?\nI strongly suspect it might have been because the original graph has been edited again as I did not encapsulate most of the definitions within a specified graph like with tf.Graph.as_default() as g:, but at the start of my code I have always done tf.reset_default_graph().", "body": "I am currently seeing a very strange behavior from using tf.train.Saver. Basically I had previously successfully run my code and restored all the variables from the TensorFlow-slim inception-resnet-v2 model and even obtained my losses and predictions. However, after I added some scopes to exclude in `variables_to_restore = slim.get_variables_to_restore(exclude = [...])` , which I successfully run,  when I returned back to using no excluded scope, my code actually fails. It gave me the following error:\r\n\r\n`NotFoundError (see above for traceback): Tensor name \"InceptionResnetV2/Repeat_1/block17_13/Branch_1/Conv2d_0c_7x1/BatchNorm/beta/Adam\" not found in checkpoint files ./inception_resnet_v2_2016_08_30.ckpt`\r\n\r\nHere is the code I run.\r\n```\r\nwith slim.arg_scope(inception_resnet_v2.inception_resnet_v2_arg_scope()):\r\n\tlogits, end_points = inception_resnet_v2.inception_resnet_v2(image_batch, is_training = False)\r\n\tpredictions = end_points['Predictions']\r\n\r\n#....then the typical building of operations till you get the train_op\r\n\r\nvariables_to_restore = slim.get_variables_to_restore()\r\nsaver = tf.train.Saver(variables_to_restore) #This is the line where the errors appear\r\ndef restore_fn(sess):\r\n\treturn saver.restore(sess, checkpoint_file)\r\nsv = tf.train.Supervisor(logdir = log_dir, summary_op = None, init_fn = restore_fn)\r\n```\r\n\r\nI have even replaced my checkpoint file again and this error persists. Why does this happen?\r\n\r\nI strongly suspect it might have been because the original graph has been edited again as I did not encapsulate most of the definitions within a specified graph like `with tf.Graph.as_default() as g:`, but at the start of my code I have always done `tf.reset_default_graph()`."}