{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4859", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4859/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4859/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4859/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4859", "id": 181899830, "node_id": "MDU6SXNzdWUxODE4OTk4MzA=", "number": 4859, "title": "incorrect gradient for complex matmul", "user": {"login": "woodshop", "id": 4654379, "node_id": "MDQ6VXNlcjQ2NTQzNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4654379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/woodshop", "html_url": "https://github.com/woodshop", "followers_url": "https://api.github.com/users/woodshop/followers", "following_url": "https://api.github.com/users/woodshop/following{/other_user}", "gists_url": "https://api.github.com/users/woodshop/gists{/gist_id}", "starred_url": "https://api.github.com/users/woodshop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/woodshop/subscriptions", "organizations_url": "https://api.github.com/users/woodshop/orgs", "repos_url": "https://api.github.com/users/woodshop/repos", "events_url": "https://api.github.com/users/woodshop/events{/privacy}", "received_events_url": "https://api.github.com/users/woodshop/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "concretevitamin", "id": 592670, "node_id": "MDQ6VXNlcjU5MjY3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/592670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/concretevitamin", "html_url": "https://github.com/concretevitamin", "followers_url": "https://api.github.com/users/concretevitamin/followers", "following_url": "https://api.github.com/users/concretevitamin/following{/other_user}", "gists_url": "https://api.github.com/users/concretevitamin/gists{/gist_id}", "starred_url": "https://api.github.com/users/concretevitamin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/concretevitamin/subscriptions", "organizations_url": "https://api.github.com/users/concretevitamin/orgs", "repos_url": "https://api.github.com/users/concretevitamin/repos", "events_url": "https://api.github.com/users/concretevitamin/events{/privacy}", "received_events_url": "https://api.github.com/users/concretevitamin/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "concretevitamin", "id": 592670, "node_id": "MDQ6VXNlcjU5MjY3MA==", "avatar_url": "https://avatars3.githubusercontent.com/u/592670?v=4", "gravatar_id": "", "url": "https://api.github.com/users/concretevitamin", "html_url": "https://github.com/concretevitamin", "followers_url": "https://api.github.com/users/concretevitamin/followers", "following_url": "https://api.github.com/users/concretevitamin/following{/other_user}", "gists_url": "https://api.github.com/users/concretevitamin/gists{/gist_id}", "starred_url": "https://api.github.com/users/concretevitamin/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/concretevitamin/subscriptions", "organizations_url": "https://api.github.com/users/concretevitamin/orgs", "repos_url": "https://api.github.com/users/concretevitamin/repos", "events_url": "https://api.github.com/users/concretevitamin/events{/privacy}", "received_events_url": "https://api.github.com/users/concretevitamin/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 8, "created_at": "2016-10-09T17:37:36Z", "updated_at": "2017-06-05T21:40:48Z", "closed_at": "2017-06-05T21:40:47Z", "author_association": "NONE", "body_html": "<p>The <code>MatMul</code> kernel is registered for <code>complex64</code> and <code>complex128</code> on CPU and GPU. However the gradient is incorrect. This issue has not been caught because there is no test for a complex-valued <code>MatMul</code> gradient in <code>tensorflow\\python\\kernel_tests\\matmul_op_test.py</code>.</p>\n<p>When computing the complex gradient we need to apply the conjugate of the input. Several cwise gradient ops in TF were ignoring this step until it was addressed by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17151892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tensorflower-gardener\">@tensorflower-gardener</a> in <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f\"><tt>821063d</tt></a>. However it seems that <code>MatMul</code> snuck through the cracks.</p>\n<p><strong>To Reproduce</strong></p>\n<div class=\"highlight highlight-source-python\"><pre>ops.reset_default_graph()\nsess <span class=\"pl-k\">=</span> tf.Session()\nx <span class=\"pl-k\">=</span> (np.linspace(<span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span>np.linspace(<span class=\"pl-c1\">3</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">6</span>)).reshape(<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>)\ny <span class=\"pl-k\">=</span> (np.linspace(<span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">8</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span>np.linspace(<span class=\"pl-c1\">3</span>, <span class=\"pl-k\">-</span><span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">8</span>)).reshape(<span class=\"pl-c1\">2</span>, <span class=\"pl-c1\">4</span>)\n<span class=\"pl-k\">with</span> sess.as_default():\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/cpu<span class=\"pl-pds\">'</span></span>):\n        x_tf <span class=\"pl-k\">=</span> tf.constant(x, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.complex64, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>)\n        y_tf <span class=\"pl-k\">=</span> tf.constant(y, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.complex64, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>)\n        m <span class=\"pl-k\">=</span> tf.matmul(x_tf, y_tf, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>matmul<span class=\"pl-pds\">\"</span></span>)\n        err <span class=\"pl-k\">=</span> tf.test.compute_gradient_error(x_tf, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>], m, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n        <span class=\"pl-c1\">print</span>(err)\n\n<span class=\"pl-k\">with</span> sess.as_default():\n    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu<span class=\"pl-pds\">'</span></span>):\n        x_tf <span class=\"pl-k\">=</span> tf.constant(x, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.complex64, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>)\n        y_tf <span class=\"pl-k\">=</span> tf.constant(y, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.complex64, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>)\n        m <span class=\"pl-k\">=</span> tf.matmul(x_tf, y_tf, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>matmul<span class=\"pl-pds\">\"</span></span>)\n        err <span class=\"pl-k\">=</span> tf.test.compute_gradient_error(x_tf, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">2</span>], m, [<span class=\"pl-c1\">3</span>, <span class=\"pl-c1\">4</span>])\n        <span class=\"pl-c1\">print</span>(err)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">6.00002098083</span>\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">6.00002098083</span></pre></div>\n<p><strong>Proposed Fix</strong><br>\nReplace <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L697-L714\">these lines</a> with:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>MatMul<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_MatMulGrad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">grad</span>):\n  <span class=\"pl-k\">with</span> ops.control_dependencies([grad.op]):\n      inp0 <span class=\"pl-k\">=</span> math_ops.conj(op.inputs[<span class=\"pl-c1\">0</span>])\n      inp1 <span class=\"pl-k\">=</span> math_ops.conj(op.inputs[<span class=\"pl-c1\">1</span>])\n      t_a <span class=\"pl-k\">=</span> op.get_attr(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>transpose_a<span class=\"pl-pds\">\"</span></span>)\n      t_b <span class=\"pl-k\">=</span> op.get_attr(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>transpose_b<span class=\"pl-pds\">\"</span></span>)\n      <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> t_a <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> t_b:\n        <span class=\"pl-k\">return</span> (math_ops.matmul(grad, inp1, <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n                math_ops.matmul(inp0, grad, <span class=\"pl-v\">transpose_a</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n      <span class=\"pl-k\">elif</span> <span class=\"pl-k\">not</span> t_a <span class=\"pl-k\">and</span> t_b:\n        <span class=\"pl-k\">return</span> (math_ops.matmul(grad, inp1),\n                math_ops.matmul(grad, inp0, <span class=\"pl-v\">transpose_a</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))\n      <span class=\"pl-k\">elif</span> t_a <span class=\"pl-k\">and</span> <span class=\"pl-k\">not</span> t_b:\n        <span class=\"pl-k\">return</span> (math_ops.matmul(op.inp1, grad, <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n                math_ops.matmul(op.inp0, grad))\n      <span class=\"pl-k\">elif</span> t_a <span class=\"pl-k\">and</span> t_b:\n        <span class=\"pl-k\">return</span> (math_ops.matmul(op.inp1, grad, <span class=\"pl-v\">transpose_a</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>),\n                math_ops.matmul(grad, op.inp0, <span class=\"pl-v\">transpose_a</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n                                <span class=\"pl-v\">transpose_b</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>))</pre></div>\n<p>and add a complex gradient test to <code>tensorflow\\python\\kernel_tests\\matmul_op_test.py</code>.</p>", "body_text": "The MatMul kernel is registered for complex64 and complex128 on CPU and GPU. However the gradient is incorrect. This issue has not been caught because there is no test for a complex-valued MatMul gradient in tensorflow\\python\\kernel_tests\\matmul_op_test.py.\nWhen computing the complex gradient we need to apply the conjugate of the input. Several cwise gradient ops in TF were ignoring this step until it was addressed by @girving and @tensorflower-gardener in 821063d. However it seems that MatMul snuck through the cracks.\nTo Reproduce\nops.reset_default_graph()\nsess = tf.Session()\nx = (np.linspace(-3, 3, 6) + 1j*np.linspace(3, -3, 6)).reshape(3, 2)\ny = (np.linspace(-3, 3, 8) + 1j*np.linspace(3, -3, 8)).reshape(2, 4)\nwith sess.as_default():\n    with tf.device('/cpu'):\n        x_tf = tf.constant(x, dtype=tf.complex64, name=\"x\")\n        y_tf = tf.constant(y, dtype=tf.complex64, name=\"y\")\n        m = tf.matmul(x_tf, y_tf, name=\"matmul\")\n        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])\n        print(err)\n\nwith sess.as_default():\n    with tf.device('/gpu'):\n        x_tf = tf.constant(x, dtype=tf.complex64, name=\"x\")\n        y_tf = tf.constant(y, dtype=tf.complex64, name=\"y\")\n        m = tf.matmul(x_tf, y_tf, name=\"matmul\")\n        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])\n        print(err)\n>>> 6.00002098083\n>>> 6.00002098083\nProposed Fix\nReplace these lines with:\n@ops.RegisterGradient(\"MatMul\")\ndef _MatMulGrad(op, grad):\n  with ops.control_dependencies([grad.op]):\n      inp0 = math_ops.conj(op.inputs[0])\n      inp1 = math_ops.conj(op.inputs[1])\n      t_a = op.get_attr(\"transpose_a\")\n      t_b = op.get_attr(\"transpose_b\")\n      if not t_a and not t_b:\n        return (math_ops.matmul(grad, inp1, transpose_b=True),\n                math_ops.matmul(inp0, grad, transpose_a=True))\n      elif not t_a and t_b:\n        return (math_ops.matmul(grad, inp1),\n                math_ops.matmul(grad, inp0, transpose_a=True))\n      elif t_a and not t_b:\n        return (math_ops.matmul(op.inp1, grad, transpose_b=True),\n                math_ops.matmul(op.inp0, grad))\n      elif t_a and t_b:\n        return (math_ops.matmul(op.inp1, grad, transpose_a=True,\n                                transpose_b=True),\n                math_ops.matmul(grad, op.inp0, transpose_a=True,\n                                transpose_b=True))\nand add a complex gradient test to tensorflow\\python\\kernel_tests\\matmul_op_test.py.", "body": "The `MatMul` kernel is registered for `complex64` and `complex128` on CPU and GPU. However the gradient is incorrect. This issue has not been caught because there is no test for a complex-valued `MatMul` gradient in `tensorflow\\python\\kernel_tests\\matmul_op_test.py`.\n\nWhen computing the complex gradient we need to apply the conjugate of the input. Several cwise gradient ops in TF were ignoring this step until it was addressed by @girving and @tensorflower-gardener in https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f. However it seems that `MatMul` snuck through the cracks.\n\n**To Reproduce**\n\n``` python\nops.reset_default_graph()\nsess = tf.Session()\nx = (np.linspace(-3, 3, 6) + 1j*np.linspace(3, -3, 6)).reshape(3, 2)\ny = (np.linspace(-3, 3, 8) + 1j*np.linspace(3, -3, 8)).reshape(2, 4)\nwith sess.as_default():\n    with tf.device('/cpu'):\n        x_tf = tf.constant(x, dtype=tf.complex64, name=\"x\")\n        y_tf = tf.constant(y, dtype=tf.complex64, name=\"y\")\n        m = tf.matmul(x_tf, y_tf, name=\"matmul\")\n        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])\n        print(err)\n\nwith sess.as_default():\n    with tf.device('/gpu'):\n        x_tf = tf.constant(x, dtype=tf.complex64, name=\"x\")\n        y_tf = tf.constant(y, dtype=tf.complex64, name=\"y\")\n        m = tf.matmul(x_tf, y_tf, name=\"matmul\")\n        err = tf.test.compute_gradient_error(x_tf, [3, 2], m, [3, 4])\n        print(err)\n```\n\n``` python\n>>> 6.00002098083\n>>> 6.00002098083\n```\n\n**Proposed Fix**\nReplace [these lines](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/math_grad.py#L697-L714) with:\n\n``` python\n@ops.RegisterGradient(\"MatMul\")\ndef _MatMulGrad(op, grad):\n  with ops.control_dependencies([grad.op]):\n      inp0 = math_ops.conj(op.inputs[0])\n      inp1 = math_ops.conj(op.inputs[1])\n      t_a = op.get_attr(\"transpose_a\")\n      t_b = op.get_attr(\"transpose_b\")\n      if not t_a and not t_b:\n        return (math_ops.matmul(grad, inp1, transpose_b=True),\n                math_ops.matmul(inp0, grad, transpose_a=True))\n      elif not t_a and t_b:\n        return (math_ops.matmul(grad, inp1),\n                math_ops.matmul(grad, inp0, transpose_a=True))\n      elif t_a and not t_b:\n        return (math_ops.matmul(op.inp1, grad, transpose_b=True),\n                math_ops.matmul(op.inp0, grad))\n      elif t_a and t_b:\n        return (math_ops.matmul(op.inp1, grad, transpose_a=True,\n                                transpose_b=True),\n                math_ops.matmul(grad, op.inp0, transpose_a=True,\n                                transpose_b=True))\n```\n\nand add a complex gradient test to `tensorflow\\python\\kernel_tests\\matmul_op_test.py`.\n"}