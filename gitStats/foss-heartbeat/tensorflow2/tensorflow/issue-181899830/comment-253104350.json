{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253104350", "html_url": "https://github.com/tensorflow/tensorflow/issues/4859#issuecomment-253104350", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4859", "id": 253104350, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzEwNDM1MA==", "user": {"login": "woodshop", "id": 4654379, "node_id": "MDQ6VXNlcjQ2NTQzNzk=", "avatar_url": "https://avatars3.githubusercontent.com/u/4654379?v=4", "gravatar_id": "", "url": "https://api.github.com/users/woodshop", "html_url": "https://github.com/woodshop", "followers_url": "https://api.github.com/users/woodshop/followers", "following_url": "https://api.github.com/users/woodshop/following{/other_user}", "gists_url": "https://api.github.com/users/woodshop/gists{/gist_id}", "starred_url": "https://api.github.com/users/woodshop/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/woodshop/subscriptions", "organizations_url": "https://api.github.com/users/woodshop/orgs", "repos_url": "https://api.github.com/users/woodshop/repos", "events_url": "https://api.github.com/users/woodshop/events{/privacy}", "received_events_url": "https://api.github.com/users/woodshop/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-12T02:44:52Z", "updated_at": "2016-10-12T15:11:48Z", "author_association": "NONE", "body_html": "<p>I've considered my question a bit more and I believe this issue should remain open. I would love to hear someone weigh in (perhaps <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=890531\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ibab\">@ibab</a>, who has been involved in resolving several TF issues for complex computations).</p>\n<p>As noted above, the recent commit  <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f\"><tt>821063d</tt></a> by <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=70511\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/girving\">@girving</a> and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=17151892\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tensorflower-gardener\">@tensorflower-gardener</a> changed the python-computed gradients for several complex analytic functions. I believe that <code>tf.matmul</code> needs a similar upgrade. As evidence, TF returns different gradients for scalar <code>matmul</code> and scalar <code>mul</code>, so one of them must be incorrect, right?</p>\n<div class=\"highlight highlight-source-python\"><pre>sess <span class=\"pl-k\">=</span> tf.Session()\nx <span class=\"pl-k\">=</span> (np.random.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span>np.random.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>)).astype(np.complex64)\ny <span class=\"pl-k\">=</span> (np.random.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>) <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1<span class=\"pl-k\">j</span></span><span class=\"pl-k\">*</span> np.random.randn(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">1</span>)).astype(np.complex64)\n<span class=\"pl-k\">with</span> sess.as_default():\n    x_tf <span class=\"pl-k\">=</span> tf.constant(x)\n    y_tf <span class=\"pl-k\">=</span> tf.constant(y) \n    f_mul <span class=\"pl-k\">=</span> x_tf <span class=\"pl-k\">*</span> y_tf\n    f_matmul <span class=\"pl-k\">=</span> tf.matmul(x_tf, y_tf)\n    <span class=\"pl-k\">assert</span> f_mul.eval() <span class=\"pl-k\">==</span> f_matmul.eval()\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>df/dx computed using tf.mul:<span class=\"pl-cce\">\\n\\t</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(sess.run(tf.gradients(f_mul, x_tf))))\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>df/dx computed using tf.matmul:<span class=\"pl-cce\">\\n\\t</span><span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(sess.run(tf.gradients(f_matmul, x_tf))))</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> df<span class=\"pl-k\">/</span>dx computed using tf.mul:\n        [array([[ <span class=\"pl-c1\">2.16470551</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">0.75122446<span class=\"pl-k\">j</span></span>]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>complex64)]\n<span class=\"pl-k\">&gt;&gt;</span><span class=\"pl-k\">&gt;</span> df<span class=\"pl-k\">/</span>dx computed using tf.matmul:\n    [array([[ <span class=\"pl-c1\">2.16470551</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">0.75122446<span class=\"pl-k\">j</span></span>]], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>complex64)]</pre></div>", "body_text": "I've considered my question a bit more and I believe this issue should remain open. I would love to hear someone weigh in (perhaps @ibab, who has been involved in resolving several TF issues for complex computations).\nAs noted above, the recent commit  821063d by @girving and @tensorflower-gardener changed the python-computed gradients for several complex analytic functions. I believe that tf.matmul needs a similar upgrade. As evidence, TF returns different gradients for scalar matmul and scalar mul, so one of them must be incorrect, right?\nsess = tf.Session()\nx = (np.random.randn(1,1) + 1j*np.random.randn(1,1)).astype(np.complex64)\ny = (np.random.randn(1,1) + 1j* np.random.randn(1,1)).astype(np.complex64)\nwith sess.as_default():\n    x_tf = tf.constant(x)\n    y_tf = tf.constant(y) \n    f_mul = x_tf * y_tf\n    f_matmul = tf.matmul(x_tf, y_tf)\n    assert f_mul.eval() == f_matmul.eval()\n    print('df/dx computed using tf.mul:\\n\\t{}'.format(sess.run(tf.gradients(f_mul, x_tf))))\n    print('df/dx computed using tf.matmul:\\n\\t{}'.format(sess.run(tf.gradients(f_matmul, x_tf))))\n>>> df/dx computed using tf.mul:\n        [array([[ 2.16470551+0.75122446j]], dtype=complex64)]\n>>> df/dx computed using tf.matmul:\n    [array([[ 2.16470551-0.75122446j]], dtype=complex64)]", "body": "I've considered my question a bit more and I believe this issue should remain open. I would love to hear someone weigh in (perhaps @ibab, who has been involved in resolving several TF issues for complex computations).\n\nAs noted above, the recent commit  https://github.com/tensorflow/tensorflow/commit/821063df9f0e6a0eec8cb78cb0ddc5c5b2b91b9f by @girving and @tensorflower-gardener changed the python-computed gradients for several complex analytic functions. I believe that `tf.matmul` needs a similar upgrade. As evidence, TF returns different gradients for scalar `matmul` and scalar `mul`, so one of them must be incorrect, right?\n\n``` python\nsess = tf.Session()\nx = (np.random.randn(1,1) + 1j*np.random.randn(1,1)).astype(np.complex64)\ny = (np.random.randn(1,1) + 1j* np.random.randn(1,1)).astype(np.complex64)\nwith sess.as_default():\n    x_tf = tf.constant(x)\n    y_tf = tf.constant(y) \n    f_mul = x_tf * y_tf\n    f_matmul = tf.matmul(x_tf, y_tf)\n    assert f_mul.eval() == f_matmul.eval()\n    print('df/dx computed using tf.mul:\\n\\t{}'.format(sess.run(tf.gradients(f_mul, x_tf))))\n    print('df/dx computed using tf.matmul:\\n\\t{}'.format(sess.run(tf.gradients(f_matmul, x_tf))))\n```\n\n``` python\n>>> df/dx computed using tf.mul:\n        [array([[ 2.16470551+0.75122446j]], dtype=complex64)]\n>>> df/dx computed using tf.matmul:\n    [array([[ 2.16470551-0.75122446j]], dtype=complex64)]\n```\n"}