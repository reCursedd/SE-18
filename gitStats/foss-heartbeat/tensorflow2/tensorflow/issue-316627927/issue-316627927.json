{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18784", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18784/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18784/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/18784/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/18784", "id": 316627927, "node_id": "MDU6SXNzdWUzMTY2Mjc5Mjc=", "number": 18784, "title": "Why is reading a CSV file with a TextLineDataset and decode_csv so slow?", "user": {"login": "recolgan", "id": 11020984, "node_id": "MDQ6VXNlcjExMDIwOTg0", "avatar_url": "https://avatars3.githubusercontent.com/u/11020984?v=4", "gravatar_id": "", "url": "https://api.github.com/users/recolgan", "html_url": "https://github.com/recolgan", "followers_url": "https://api.github.com/users/recolgan/followers", "following_url": "https://api.github.com/users/recolgan/following{/other_user}", "gists_url": "https://api.github.com/users/recolgan/gists{/gist_id}", "starred_url": "https://api.github.com/users/recolgan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/recolgan/subscriptions", "organizations_url": "https://api.github.com/users/recolgan/orgs", "repos_url": "https://api.github.com/users/recolgan/repos", "events_url": "https://api.github.com/users/recolgan/events{/privacy}", "received_events_url": "https://api.github.com/users/recolgan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "rachellim", "id": 9589037, "node_id": "MDQ6VXNlcjk1ODkwMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9589037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rachellim", "html_url": "https://github.com/rachellim", "followers_url": "https://api.github.com/users/rachellim/followers", "following_url": "https://api.github.com/users/rachellim/following{/other_user}", "gists_url": "https://api.github.com/users/rachellim/gists{/gist_id}", "starred_url": "https://api.github.com/users/rachellim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rachellim/subscriptions", "organizations_url": "https://api.github.com/users/rachellim/orgs", "repos_url": "https://api.github.com/users/rachellim/repos", "events_url": "https://api.github.com/users/rachellim/events{/privacy}", "received_events_url": "https://api.github.com/users/rachellim/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rachellim", "id": 9589037, "node_id": "MDQ6VXNlcjk1ODkwMzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/9589037?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rachellim", "html_url": "https://github.com/rachellim", "followers_url": "https://api.github.com/users/rachellim/followers", "following_url": "https://api.github.com/users/rachellim/following{/other_user}", "gists_url": "https://api.github.com/users/rachellim/gists{/gist_id}", "starred_url": "https://api.github.com/users/rachellim/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rachellim/subscriptions", "organizations_url": "https://api.github.com/users/rachellim/orgs", "repos_url": "https://api.github.com/users/rachellim/repos", "events_url": "https://api.github.com/users/rachellim/events{/privacy}", "received_events_url": "https://api.github.com/users/rachellim/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 13, "created_at": "2018-04-23T00:35:32Z", "updated_at": "2018-06-17T02:03:28Z", "closed_at": "2018-06-17T02:03:28Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.7.0</li>\n<li><strong>Python version</strong>: 3.6.0</li>\n<li><strong>Bazel version (if compiling from source)</strong>: N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0.176/7.0.7</li>\n<li><strong>GPU model and memory</strong>: Titan X (12 GB)</li>\n<li><strong>Exact command to reproduce</strong>: See below</li>\n</ul>\n<p>I have a dataset saved on an SSD as a moderately large (~7 million line) .csv file. It's small enough to load into memory, but large enough that it takes a couple minutes to read the entire file into a NumPy array, and I don't always need the entire thing. This seemed like a perfect use case for tf.data.TextLineDataset.</p>\n<p>I attempted to use it similarly to as demonstrated in the documentation with the tf.decode_csv and tf.data.Dataset.map functions, as shown in the code below, but I'm finding it to be unreasonably slow to fetch batches from the TextLineDataset. Of course I'd expect it to be slower than if the data has already been read into memory. But if I compare the total time to read the entire thing into a NumPy array in memory first and process it as a TensorSliceDataset vs. creating a TextLineDataset and reading batches that way, the former is many times faster.</p>\n<p>Am I missing something, or is this an issue with TextLineDataset and/or tf.decode_csv?</p>\n<pre><code>def make_tld(csv_filename, header_lines, delim, batch_size):\n    dataset = tf.data.TextLineDataset(filenames=csv_filename).skip(header_lines)\n\n    def parse_csv(line):\n        cols_types = [[]] * num_cols_  # all required\n        columns = tf.decode_csv(line, record_defaults=cols_types, field_delim=delim)\n        return tf.stack(columns)\n\n    dataset = dataset.map(parse_csv).batch(batch_size)\n    return dataset\n\n\ndef make_tsd(csv_filename, header_lines, delim, batch_size):\n    with open(csv_filename, \"r\") as f:\n        lines = f.readlines()\n\n    data_shape = (len(lines) - header_lines, len(lines[header_lines].strip().split(delim)))\n    data = np.empty(shape=data_shape, dtype=np.float32)\n\n    for idx, line in enumerate(lines[header_lines:]):\n        columns = [float(el) for el in line.strip().split(delim)]\n        data[idx, :] = np.array(columns)\n\n    dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\n    return dataset\n\n\nif __name__ == \"__main__\":\n    batch_size_ = 100\n\n    tld_start = datetime.datetime.now()\n    tld = make_tld(csv_filename_, header_lines_, delim_, batch_size_)\n    tld_next = tld.make_one_shot_iterator().get_next()\n    with tf.Session() as tld_sess:\n        tld_sess.run(tf.global_variables_initializer())\n        try:\n            while True:\n                tld_out = tld_sess.run(tld_next)\n        except tf.errors.OutOfRangeError:\n            print(\"Done\")\n    tld_end = datetime.datetime.now()\n    print(\"TextLineDataset: \" + str(tld_end - tld_start))\n\n    tsd_start = datetime.datetime.now()\n    tsd = make_tsd(csv_filename_, header_lines_, delim_, batch_size_)\n    tsd_next = tsd.make_one_shot_iterator().get_next()\n    with tf.Session() as tsd_sess:\n        tsd_sess.run(tf.global_variables_initializer())\n        try:\n            while True:\n                tsd_out = tsd_sess.run(tsd_next)\n        except tf.errors.OutOfRangeError:\n            print(\"Done\")\n    tsd_end = datetime.datetime.now()\n    print(\"TensorSliceDataset: \" + str(tsd_end - tsd_start))\n</code></pre>\n<p>Output:</p>\n<pre><code>Done\nTextLineDataset: 0:11:24.675474\nDone\nTensorSliceDataset: 0:02:12.061404\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below): 1.7.0\nPython version: 3.6.0\nBazel version (if compiling from source): N/A\nGCC/Compiler version (if compiling from source): N/A\nCUDA/cuDNN version: 9.0.176/7.0.7\nGPU model and memory: Titan X (12 GB)\nExact command to reproduce: See below\n\nI have a dataset saved on an SSD as a moderately large (~7 million line) .csv file. It's small enough to load into memory, but large enough that it takes a couple minutes to read the entire file into a NumPy array, and I don't always need the entire thing. This seemed like a perfect use case for tf.data.TextLineDataset.\nI attempted to use it similarly to as demonstrated in the documentation with the tf.decode_csv and tf.data.Dataset.map functions, as shown in the code below, but I'm finding it to be unreasonably slow to fetch batches from the TextLineDataset. Of course I'd expect it to be slower than if the data has already been read into memory. But if I compare the total time to read the entire thing into a NumPy array in memory first and process it as a TensorSliceDataset vs. creating a TextLineDataset and reading batches that way, the former is many times faster.\nAm I missing something, or is this an issue with TextLineDataset and/or tf.decode_csv?\ndef make_tld(csv_filename, header_lines, delim, batch_size):\n    dataset = tf.data.TextLineDataset(filenames=csv_filename).skip(header_lines)\n\n    def parse_csv(line):\n        cols_types = [[]] * num_cols_  # all required\n        columns = tf.decode_csv(line, record_defaults=cols_types, field_delim=delim)\n        return tf.stack(columns)\n\n    dataset = dataset.map(parse_csv).batch(batch_size)\n    return dataset\n\n\ndef make_tsd(csv_filename, header_lines, delim, batch_size):\n    with open(csv_filename, \"r\") as f:\n        lines = f.readlines()\n\n    data_shape = (len(lines) - header_lines, len(lines[header_lines].strip().split(delim)))\n    data = np.empty(shape=data_shape, dtype=np.float32)\n\n    for idx, line in enumerate(lines[header_lines:]):\n        columns = [float(el) for el in line.strip().split(delim)]\n        data[idx, :] = np.array(columns)\n\n    dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\n    return dataset\n\n\nif __name__ == \"__main__\":\n    batch_size_ = 100\n\n    tld_start = datetime.datetime.now()\n    tld = make_tld(csv_filename_, header_lines_, delim_, batch_size_)\n    tld_next = tld.make_one_shot_iterator().get_next()\n    with tf.Session() as tld_sess:\n        tld_sess.run(tf.global_variables_initializer())\n        try:\n            while True:\n                tld_out = tld_sess.run(tld_next)\n        except tf.errors.OutOfRangeError:\n            print(\"Done\")\n    tld_end = datetime.datetime.now()\n    print(\"TextLineDataset: \" + str(tld_end - tld_start))\n\n    tsd_start = datetime.datetime.now()\n    tsd = make_tsd(csv_filename_, header_lines_, delim_, batch_size_)\n    tsd_next = tsd.make_one_shot_iterator().get_next()\n    with tf.Session() as tsd_sess:\n        tsd_sess.run(tf.global_variables_initializer())\n        try:\n            while True:\n                tsd_out = tsd_sess.run(tsd_next)\n        except tf.errors.OutOfRangeError:\n            print(\"Done\")\n    tsd_end = datetime.datetime.now()\n    print(\"TensorSliceDataset: \" + str(tsd_end - tsd_start))\n\nOutput:\nDone\nTextLineDataset: 0:11:24.675474\nDone\nTensorSliceDataset: 0:02:12.061404", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**: 1.7.0\r\n- **Python version**: 3.6.0\r\n- **Bazel version (if compiling from source)**: N/A\r\n- **GCC/Compiler version (if compiling from source)**: N/A\r\n- **CUDA/cuDNN version**: 9.0.176/7.0.7\r\n- **GPU model and memory**: Titan X (12 GB)\r\n- **Exact command to reproduce**: See below\r\n\r\nI have a dataset saved on an SSD as a moderately large (~7 million line) .csv file. It's small enough to load into memory, but large enough that it takes a couple minutes to read the entire file into a NumPy array, and I don't always need the entire thing. This seemed like a perfect use case for tf.data.TextLineDataset.\r\n\r\nI attempted to use it similarly to as demonstrated in the documentation with the tf.decode_csv and tf.data.Dataset.map functions, as shown in the code below, but I'm finding it to be unreasonably slow to fetch batches from the TextLineDataset. Of course I'd expect it to be slower than if the data has already been read into memory. But if I compare the total time to read the entire thing into a NumPy array in memory first and process it as a TensorSliceDataset vs. creating a TextLineDataset and reading batches that way, the former is many times faster. \r\n\r\nAm I missing something, or is this an issue with TextLineDataset and/or tf.decode_csv?\r\n\r\n\r\n```\r\ndef make_tld(csv_filename, header_lines, delim, batch_size):\r\n    dataset = tf.data.TextLineDataset(filenames=csv_filename).skip(header_lines)\r\n\r\n    def parse_csv(line):\r\n        cols_types = [[]] * num_cols_  # all required\r\n        columns = tf.decode_csv(line, record_defaults=cols_types, field_delim=delim)\r\n        return tf.stack(columns)\r\n\r\n    dataset = dataset.map(parse_csv).batch(batch_size)\r\n    return dataset\r\n\r\n\r\ndef make_tsd(csv_filename, header_lines, delim, batch_size):\r\n    with open(csv_filename, \"r\") as f:\r\n        lines = f.readlines()\r\n\r\n    data_shape = (len(lines) - header_lines, len(lines[header_lines].strip().split(delim)))\r\n    data = np.empty(shape=data_shape, dtype=np.float32)\r\n\r\n    for idx, line in enumerate(lines[header_lines:]):\r\n        columns = [float(el) for el in line.strip().split(delim)]\r\n        data[idx, :] = np.array(columns)\r\n\r\n    dataset = tf.data.Dataset.from_tensor_slices(data).batch(batch_size)\r\n    return dataset\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    batch_size_ = 100\r\n\r\n    tld_start = datetime.datetime.now()\r\n    tld = make_tld(csv_filename_, header_lines_, delim_, batch_size_)\r\n    tld_next = tld.make_one_shot_iterator().get_next()\r\n    with tf.Session() as tld_sess:\r\n        tld_sess.run(tf.global_variables_initializer())\r\n        try:\r\n            while True:\r\n                tld_out = tld_sess.run(tld_next)\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done\")\r\n    tld_end = datetime.datetime.now()\r\n    print(\"TextLineDataset: \" + str(tld_end - tld_start))\r\n\r\n    tsd_start = datetime.datetime.now()\r\n    tsd = make_tsd(csv_filename_, header_lines_, delim_, batch_size_)\r\n    tsd_next = tsd.make_one_shot_iterator().get_next()\r\n    with tf.Session() as tsd_sess:\r\n        tsd_sess.run(tf.global_variables_initializer())\r\n        try:\r\n            while True:\r\n                tsd_out = tsd_sess.run(tsd_next)\r\n        except tf.errors.OutOfRangeError:\r\n            print(\"Done\")\r\n    tsd_end = datetime.datetime.now()\r\n    print(\"TensorSliceDataset: \" + str(tsd_end - tsd_start))\r\n```\r\n\r\nOutput:\r\n```\r\nDone\r\nTextLineDataset: 0:11:24.675474\r\nDone\r\nTensorSliceDataset: 0:02:12.061404\r\n```"}