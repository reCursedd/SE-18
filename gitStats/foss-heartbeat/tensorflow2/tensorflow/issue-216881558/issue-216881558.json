{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8701", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8701/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8701/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8701/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8701", "id": 216881558, "node_id": "MDU6SXNzdWUyMTY4ODE1NTg=", "number": 8701, "title": "InvalidArgumentError using tf.learn and eval", "user": {"login": "dennybritz", "id": 403133, "node_id": "MDQ6VXNlcjQwMzEzMw==", "avatar_url": "https://avatars0.githubusercontent.com/u/403133?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dennybritz", "html_url": "https://github.com/dennybritz", "followers_url": "https://api.github.com/users/dennybritz/followers", "following_url": "https://api.github.com/users/dennybritz/following{/other_user}", "gists_url": "https://api.github.com/users/dennybritz/gists{/gist_id}", "starred_url": "https://api.github.com/users/dennybritz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dennybritz/subscriptions", "organizations_url": "https://api.github.com/users/dennybritz/orgs", "repos_url": "https://api.github.com/users/dennybritz/repos", "events_url": "https://api.github.com/users/dennybritz/events{/privacy}", "received_events_url": "https://api.github.com/users/dennybritz/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-03-24T19:18:55Z", "updated_at": "2017-10-03T09:10:44Z", "closed_at": "2017-06-16T20:43:16Z", "author_association": "CONTRIBUTOR", "body_html": "<p>See <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"216537117\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/google/seq2seq/issues/103\" data-hovercard-type=\"issue\" data-hovercard-url=\"/google/seq2seq/issues/103/hovercard\" href=\"https://github.com/google/seq2seq/issues/103\">google/seq2seq#103</a> for details and user logs.</p>\n<p>TLDR; I'm using tf.learn and for some people the evaluation fails with shape errors. This seems to be some kind of GPU memory sharing issue, as subsequent runs seem to consistently increase the shape size:</p>\n<pre><code>InvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [1280,36240] and labels shape [6272]\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2304,36240] and labels shape [6272]\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [4480,36240] and labels shape [6272]\n</code></pre>\n<p>Evaluation works independently when there is no training in progress. It also doesn't happen when using the CPU only.</p>\n<p>I personally have run into similar issues before then multiple processes were trying to share the GPU, but that shouldn't be the case here.</p>", "body_text": "See google/seq2seq#103 for details and user logs.\nTLDR; I'm using tf.learn and for some people the evaluation fails with shape errors. This seems to be some kind of GPU memory sharing issue, as subsequent runs seem to consistently increase the shape size:\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [1280,36240] and labels shape [6272]\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2304,36240] and labels shape [6272]\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [4480,36240] and labels shape [6272]\n\nEvaluation works independently when there is no training in progress. It also doesn't happen when using the CPU only.\nI personally have run into similar issues before then multiple processes were trying to share the GPU, but that shouldn't be the case here.", "body": "See https://github.com/google/seq2seq/issues/103 for details and user logs.\r\n\r\nTLDR; I'm using tf.learn and for some people the evaluation fails with shape errors. This seems to be some kind of GPU memory sharing issue, as subsequent runs seem to consistently increase the shape size:\r\n\r\n```\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [1280,36240] and labels shape [6272]\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2304,36240] and labels shape [6272]\r\n\r\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [4480,36240] and labels shape [6272]\r\n```\r\n\r\nEvaluation works independently when there is no training in progress. It also doesn't happen when using the CPU only.\r\n\r\nI personally have run into similar issues before then multiple processes were trying to share the GPU, but that shouldn't be the case here."}