{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19233", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19233/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19233/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19233/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19233", "id": 322439961, "node_id": "MDU6SXNzdWUzMjI0Mzk5NjE=", "number": 19233, "title": "`foldl` disallows mixing different types of `elems`", "user": {"login": "shengc", "id": 940628, "node_id": "MDQ6VXNlcjk0MDYyOA==", "avatar_url": "https://avatars1.githubusercontent.com/u/940628?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shengc", "html_url": "https://github.com/shengc", "followers_url": "https://api.github.com/users/shengc/followers", "following_url": "https://api.github.com/users/shengc/following{/other_user}", "gists_url": "https://api.github.com/users/shengc/gists{/gist_id}", "starred_url": "https://api.github.com/users/shengc/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shengc/subscriptions", "organizations_url": "https://api.github.com/users/shengc/orgs", "repos_url": "https://api.github.com/users/shengc/repos", "events_url": "https://api.github.com/users/shengc/events{/privacy}", "received_events_url": "https://api.github.com/users/shengc/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-05-11T21:27:49Z", "updated_at": "2018-11-20T07:53:19Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nyes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Redhat</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nconda-forge</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.6</li>\n<li><strong>Python version</strong>:<br>\n3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nN/A (cpu only)</li>\n<li><strong>GPU model and memory</strong>:<br>\nN/A (cpu only)</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<h3>Source code / logs</h3>\n<p>Look at the following two code snippets.</p>\n<div class=\"highlight highlight-source-python\"><pre>score <span class=\"pl-k\">=</span> tf.foldl(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">s</span>, <span class=\"pl-smi\">x</span>: transitions[x[<span class=\"pl-c1\">0</span>], x[<span class=\"pl-c1\">1</span>]] <span class=\"pl-k\">+</span> x[<span class=\"pl-c1\">2</span>][x[<span class=\"pl-c1\">0</span>]] <span class=\"pl-k\">+</span> s, (label[<span class=\"pl-c1\">1</span>:], label[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], logits), <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>np.zeros((), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float64))</pre></div>\n<div class=\"highlight highlight-source-python\"><pre>score <span class=\"pl-k\">=</span> tf.reduce_sum(tf.map_fn(<span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">x</span>: transitions[x[<span class=\"pl-c1\">0</span>], x[<span class=\"pl-c1\">1</span>]] <span class=\"pl-k\">+</span> x[<span class=\"pl-c1\">2</span>][x[<span class=\"pl-c1\">0</span>]], (label[<span class=\"pl-c1\">1</span>:], label[:<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>], logits), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float64))</pre></div>\n<p>Long story short, the 1st one does not compile, while the second one does. From my view, these 2 operations essentially do the same thing. That said, from the error trace, it seems to me <code>foldl</code> disallows mixing different types for the argument of <code>elems=</code>, since indeed <code>label</code> is of type <code>tf.int32</code>, while <code>logits</code> is of type <code>tf.float64</code>. However the documentation does not indicate that is the case. Could you confirm if that is true ? If yes, can we update the documentation to make it explicit ?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c1\">XXX</span><span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>functional_ops.py <span class=\"pl-k\">in</span> foldl(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, name)\n    <span class=\"pl-c1\">107</span> \n    <span class=\"pl-c1\">108</span>     <span class=\"pl-c\"><span class=\"pl-c\">#</span> Convert elems to tensor array.</span>\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">109</span>     elems <span class=\"pl-k\">=</span> ops.convert_to_tensor(elems, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>elems<span class=\"pl-pds\">\"</span></span>)\n    <span class=\"pl-c1\">110</span>     n <span class=\"pl-k\">=</span> array_ops.shape(elems)[<span class=\"pl-c1\">0</span>]\n    <span class=\"pl-c1\">111</span>     elems_ta <span class=\"pl-k\">=</span> tensor_array_ops.TensorArray(<span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>elems.dtype, <span class=\"pl-v\">size</span><span class=\"pl-k\">=</span>n,\n\n<span class=\"pl-c1\">XXX</span><span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>ops.py <span class=\"pl-k\">in</span> convert_to_tensor(value, dtype, name, preferred_dtype)\n    <span class=\"pl-c1\">944</span>       <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name,\n    <span class=\"pl-c1\">945</span>       <span class=\"pl-v\">preferred_dtype</span><span class=\"pl-k\">=</span>preferred_dtype,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">946</span>       <span class=\"pl-v\">as_ref</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n    <span class=\"pl-c1\">947</span> \n    <span class=\"pl-c1\">948</span> \n\n<span class=\"pl-c1\">XXX</span><span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>framework<span class=\"pl-k\">/</span>ops.py <span class=\"pl-k\">in</span> internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n   <span class=\"pl-c1\">1034</span> \n   <span class=\"pl-c1\">1035</span>     <span class=\"pl-k\">if</span> ret <span class=\"pl-k\">is</span> <span class=\"pl-c1\">None</span>:\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1036</span>       ret <span class=\"pl-k\">=</span> conversion_func(value, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>dtype, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span>name, <span class=\"pl-v\">as_ref</span><span class=\"pl-k\">=</span>as_ref)\n   <span class=\"pl-c1\">1037</span> \n   <span class=\"pl-c1\">1038</span>     <span class=\"pl-k\">if</span> ret <span class=\"pl-k\">is</span> <span class=\"pl-c1\">NotImplemented</span>:\n\n<span class=\"pl-c1\">XXX</span><span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>array_ops.py <span class=\"pl-k\">in</span> _autopacking_conversion_function(v, dtype, name, as_ref)\n   <span class=\"pl-c1\">1018</span>   <span class=\"pl-k\">if</span> dtype <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span> <span class=\"pl-k\">and</span> dtype <span class=\"pl-k\">!=</span> inferred_dtype:\n   <span class=\"pl-c1\">1019</span>     <span class=\"pl-k\">return</span> <span class=\"pl-c1\">NotImplemented</span>\n<span class=\"pl-ii\">-&gt;</span> <span class=\"pl-c1\">1020</span>   <span class=\"pl-k\">return</span> _autopacking_helper(v, inferred_dtype, name <span class=\"pl-k\">or</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>packed<span class=\"pl-pds\">\"</span></span>)\n   <span class=\"pl-c1\">1021</span> \n   <span class=\"pl-c1\">1022</span> \n\n<span class=\"pl-c1\">XXX</span><span class=\"pl-k\">/</span>lib<span class=\"pl-k\">/</span>python3.6<span class=\"pl-k\">/</span>site<span class=\"pl-k\">-</span>packages<span class=\"pl-k\">/</span>tensorflow<span class=\"pl-k\">/</span>python<span class=\"pl-k\">/</span>ops<span class=\"pl-k\">/</span>array_ops.py <span class=\"pl-k\">in</span> _autopacking_helper(list_or_tuple, dtype, name)\n    <span class=\"pl-c1\">960</span>           <span class=\"pl-k\">raise</span> <span class=\"pl-c1\">TypeError</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Cannot convert a list containing a tensor of dtype <span class=\"pl-pds\">\"</span></span>\n    <span class=\"pl-c1\">961</span>                           <span class=\"pl-s\"><span class=\"pl-pds\">\"</span><span class=\"pl-c1\">%s</span> to <span class=\"pl-c1\">%s</span> (Tensor is: <span class=\"pl-c1\">%r</span>)<span class=\"pl-pds\">\"</span></span> <span class=\"pl-k\">%</span> (elem.dtype, dtype,\n<span class=\"pl-ii\">--</span><span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">962</span>                                                         elem))\n    <span class=\"pl-c1\">963</span>         converted_elems.append(elem)\n    <span class=\"pl-c1\">964</span>         must_pack <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span>\n\n<span class=\"pl-c1\">TypeError</span>: Cannot convert a <span class=\"pl-c1\">list</span> containing a tensor of dtype <span class=\"pl-k\">&lt;</span>dtype: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>float64<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span> to <span class=\"pl-k\">&lt;</span>dtype: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>int32<span class=\"pl-pds\">'</span></span><span class=\"pl-k\">&gt;</span> (Tensor <span class=\"pl-k\">is</span>: <span class=\"pl-k\">&lt;</span>tf.Tensor <span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss/BiasAdd:0<span class=\"pl-pds\">'</span></span> shape=(<span class=\"pl-ii\">?</span>, <span class=\"pl-c1\">5</span>) dtype=float64<span class=\"pl-k\">&gt;</span>)</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nyes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Redhat\nTensorFlow installed from (source or binary):\nconda-forge\nTensorFlow version (use command below):\n1.6\nPython version:\n3.6\nBazel version (if compiling from source):\nN/A\nGCC/Compiler version (if compiling from source):\nN/A\nCUDA/cuDNN version:\nN/A (cpu only)\nGPU model and memory:\nN/A (cpu only)\nExact command to reproduce:\n\nDescribe the problem\nSource code / logs\nLook at the following two code snippets.\nscore = tf.foldl(lambda s, x: transitions[x[0], x[1]] + x[2][x[0]] + s, (label[1:], label[:-1], logits), initializer=np.zeros((), dtype=np.float64))\nscore = tf.reduce_sum(tf.map_fn(lambda x: transitions[x[0], x[1]] + x[2][x[0]], (label[1:], label[:-1], logits), dtype=tf.float64))\nLong story short, the 1st one does not compile, while the second one does. From my view, these 2 operations essentially do the same thing. That said, from the error trace, it seems to me foldl disallows mixing different types for the argument of elems=, since indeed label is of type tf.int32, while logits is of type tf.float64. However the documentation does not indicate that is the case. Could you confirm if that is true ? If yes, can we update the documentation to make it explicit ?\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py in foldl(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, name)\n    107 \n    108     # Convert elems to tensor array.\n--> 109     elems = ops.convert_to_tensor(elems, name=\"elems\")\n    110     n = array_ops.shape(elems)[0]\n    111     elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\n\nXXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\n    944       name=name,\n    945       preferred_dtype=preferred_dtype,\n--> 946       as_ref=False)\n    947 \n    948 \n\nXXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\n   1034 \n   1035     if ret is None:\n-> 1036       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n   1037 \n   1038     if ret is NotImplemented:\n\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)\n   1018   if dtype is not None and dtype != inferred_dtype:\n   1019     return NotImplemented\n-> 1020   return _autopacking_helper(v, inferred_dtype, name or \"packed\")\n   1021 \n   1022 \n\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)\n    960           raise TypeError(\"Cannot convert a list containing a tensor of dtype \"\n    961                           \"%s to %s (Tensor is: %r)\" % (elem.dtype, dtype,\n--> 962                                                         elem))\n    963         converted_elems.append(elem)\n    964         must_pack = True\n\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float64'> to <dtype: 'int32'> (Tensor is: <tf.Tensor 'loss/BiasAdd:0' shape=(?, 5) dtype=float64>)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nyes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Redhat\r\n- **TensorFlow installed from (source or binary)**:\r\nconda-forge\r\n- **TensorFlow version (use command below)**:\r\n1.6\r\n- **Python version**: \r\n3.6\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n- **CUDA/cuDNN version**:\r\nN/A (cpu only)\r\n- **GPU model and memory**:\r\nN/A (cpu only)\r\n- **Exact command to reproduce**:\r\n### Describe the problem\r\n### Source code / logs\r\nLook at the following two code snippets. \r\n\r\n```python\r\nscore = tf.foldl(lambda s, x: transitions[x[0], x[1]] + x[2][x[0]] + s, (label[1:], label[:-1], logits), initializer=np.zeros((), dtype=np.float64))\r\n```\r\n\r\n```python\r\nscore = tf.reduce_sum(tf.map_fn(lambda x: transitions[x[0], x[1]] + x[2][x[0]], (label[1:], label[:-1], logits), dtype=tf.float64))\r\n```\r\n\r\nLong story short, the 1st one does not compile, while the second one does. From my view, these 2 operations essentially do the same thing. That said, from the error trace, it seems to me `foldl` disallows mixing different types for the argument of `elems=`, since indeed `label` is of type `tf.int32`, while `logits` is of type `tf.float64`. However the documentation does not indicate that is the case. Could you confirm if that is true ? If yes, can we update the documentation to make it explicit ?\r\n\r\n```python\r\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/functional_ops.py in foldl(fn, elems, initializer, parallel_iterations, back_prop, swap_memory, name)\r\n    107 \r\n    108     # Convert elems to tensor array.\r\n--> 109     elems = ops.convert_to_tensor(elems, name=\"elems\")\r\n    110     n = array_ops.shape(elems)[0]\r\n    111     elems_ta = tensor_array_ops.TensorArray(dtype=elems.dtype, size=n,\r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in convert_to_tensor(value, dtype, name, preferred_dtype)\r\n    944       name=name,\r\n    945       preferred_dtype=preferred_dtype,\r\n--> 946       as_ref=False)\r\n    947 \r\n    948 \r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/framework/ops.py in internal_convert_to_tensor(value, dtype, name, as_ref, preferred_dtype, ctx)\r\n   1034 \r\n   1035     if ret is None:\r\n-> 1036       ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\r\n   1037 \r\n   1038     if ret is NotImplemented:\r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_conversion_function(v, dtype, name, as_ref)\r\n   1018   if dtype is not None and dtype != inferred_dtype:\r\n   1019     return NotImplemented\r\n-> 1020   return _autopacking_helper(v, inferred_dtype, name or \"packed\")\r\n   1021 \r\n   1022 \r\n\r\nXXX/lib/python3.6/site-packages/tensorflow/python/ops/array_ops.py in _autopacking_helper(list_or_tuple, dtype, name)\r\n    960           raise TypeError(\"Cannot convert a list containing a tensor of dtype \"\r\n    961                           \"%s to %s (Tensor is: %r)\" % (elem.dtype, dtype,\r\n--> 962                                                         elem))\r\n    963         converted_elems.append(elem)\r\n    964         must_pack = True\r\n\r\nTypeError: Cannot convert a list containing a tensor of dtype <dtype: 'float64'> to <dtype: 'int32'> (Tensor is: <tf.Tensor 'loss/BiasAdd:0' shape=(?, 5) dtype=float64>)\r\n```"}