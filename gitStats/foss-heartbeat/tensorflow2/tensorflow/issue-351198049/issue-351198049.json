{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21657", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21657/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21657/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21657/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21657", "id": 351198049, "node_id": "MDU6SXNzdWUzNTExOTgwNDk=", "number": 21657, "title": "error when use \"tf.get_variable_scope().reuse_variables()\"", "user": {"login": "LittleFlyFish", "id": 26098258, "node_id": "MDQ6VXNlcjI2MDk4MjU4", "avatar_url": "https://avatars1.githubusercontent.com/u/26098258?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LittleFlyFish", "html_url": "https://github.com/LittleFlyFish", "followers_url": "https://api.github.com/users/LittleFlyFish/followers", "following_url": "https://api.github.com/users/LittleFlyFish/following{/other_user}", "gists_url": "https://api.github.com/users/LittleFlyFish/gists{/gist_id}", "starred_url": "https://api.github.com/users/LittleFlyFish/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LittleFlyFish/subscriptions", "organizations_url": "https://api.github.com/users/LittleFlyFish/orgs", "repos_url": "https://api.github.com/users/LittleFlyFish/repos", "events_url": "https://api.github.com/users/LittleFlyFish/events{/privacy}", "received_events_url": "https://api.github.com/users/LittleFlyFish/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "lukaszkaiser", "id": 684901, "node_id": "MDQ6VXNlcjY4NDkwMQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/684901?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lukaszkaiser", "html_url": "https://github.com/lukaszkaiser", "followers_url": "https://api.github.com/users/lukaszkaiser/followers", "following_url": "https://api.github.com/users/lukaszkaiser/following{/other_user}", "gists_url": "https://api.github.com/users/lukaszkaiser/gists{/gist_id}", "starred_url": "https://api.github.com/users/lukaszkaiser/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lukaszkaiser/subscriptions", "organizations_url": "https://api.github.com/users/lukaszkaiser/orgs", "repos_url": "https://api.github.com/users/lukaszkaiser/repos", "events_url": "https://api.github.com/users/lukaszkaiser/events{/privacy}", "received_events_url": "https://api.github.com/users/lukaszkaiser/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-16T13:08:47Z", "updated_at": "2018-08-19T12:56:13Z", "closed_at": "2018-08-17T17:36:32Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a></p>\n<p>import tensorflow as tf<br>\nimport tensorflow.contrib.slim as slim<br>\nimport numpy as np<br>\nfrom scipy.io import loadmat, savemat<br>\nimport scipy.spatial.distance as ssd<br>\nfrom sbir_sampling import triplet_sampler_asy<br>\nfrom sbir_util import *<br>\nfrom ops import spatial_softmax, reshape_feats<br>\nimport os, errno</p>\n<p>NET_ID = 0 #0 for step3 pre-trained model, 1 for step2 pre-trained model</p>\n<p>def attentionNet(inputs, pool_method='sigmoid'):<br>\nassert(pool_method in ['sigmoid', 'softmax'])<br>\nwith slim.arg_scope([slim.conv2d],<br>\nactivation_fn=tf.nn.relu,<br>\nweights_initializer=tf.truncated_normal_initializer(0.0, 0.1),<br>\nweights_regularizer=slim.l2_regularizer(0.0005),<br>\ntrainable=True):<br>\nnet = slim.conv2d(inputs, 256, [1, 1], padding='SAME', scope='conv1')<br>\nif pool_method == 'sigmoid':<br>\nnet = slim.conv2d(net, 1, [1, 1], activation_fn=tf.nn.sigmoid, scope='conv2')<br>\nelse:<br>\nnet = slim.conv2d(net, 1, [1, 1], activation_fn=None, scope='conv2')<br>\nnet = spatial_softmax(net)<br>\nreturn net</p>\n<p>def sketch_a_net_sbir(inputs, trainable):<br>\nwith slim.arg_scope([slim.conv2d, slim.fully_connected],<br>\nactivation_fn=tf.nn.relu,<br>\nweights_initializer=tf.truncated_normal_initializer(0.0, 0.1),<br>\nweights_regularizer=slim.l2_regularizer(0.0005),<br>\ntrainable=False):<br>\nwith slim.arg_scope([slim.conv2d], padding='VALID'):<br>\n# x = tf.reshape(inputs, shape=[-1, 225, 225, 1])<br>\nconv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')<br>\nconv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')<br>\nconv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')<br>\nconv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')<br>\nconv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')<br>\nconv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')<br>\nconv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', scope='conv5_s1')  # trainable=trainable<br>\nconv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')<br>\nconv5 = slim.flatten(conv5)<br>\nfc6 = slim.fully_connected(conv5, 512, trainable=trainable, scope='fc6_s1')<br>\nfc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')<br>\nfc7 = tf.nn.l2_normalize(fc7, dim=1)<br>\nreturn fc7</p>\n<p>def sketch_a_net_dssa(inputs, trainable):<br>\nwith slim.arg_scope([slim.conv2d, slim.fully_connected],<br>\nactivation_fn=tf.nn.relu,<br>\nweights_initializer=tf.truncated_normal_initializer(0.0, 0.1),<br>\nweights_regularizer=slim.l2_regularizer(0.0005),<br>\ntrainable=False):  # when test 'trainable=True', don't forget to change it<br>\nwith slim.arg_scope([slim.conv2d], padding='VALID'):<br>\n# x = tf.reshape(inputs, shape=[-1, 225, 225, 1])<br>\nconv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')<br>\nconv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')<br>\nconv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')<br>\nconv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')<br>\nconv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')<br>\nconv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')<br>\nconv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', trainable=trainable, scope='conv5_s1')<br>\nconv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')<br>\n# residual attention<br>\natt_mask = attentionNet(conv5, 'softmax')<br>\natt_map = tf.multiply(conv5, att_mask)<br>\natt_f = tf.add(conv5, att_map)<br>\nattended_map = tf.reduce_sum(att_f, reduction_indices=[1, 2])<br>\nattended_map = tf.nn.l2_normalize(attended_map, dim=1)<br>\natt_f = slim.flatten(att_f)<br>\nfc6 = slim.fully_connected(att_f, 512, trainable=trainable, scope='fc6_s1')<br>\nfc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')<br>\nfc7 = tf.nn.l2_normalize(fc7, dim=1)<br>\n# coarse-fine fusion<br>\nfinal_feature_map = tf.concat(1, [fc7, attended_map])<br>\nreturn final_feature_map</p>\n<p>def init_variables(model_file='/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'):<br>\nif NET_ID==0:<br>\npretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1', 'fc7_sketch']<br>\nelse:<br>\npretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1']<br>\nd = np.load(model_file).item()<br>\ninit_ops = []  # a list of operations<br>\nfor var in tf.global_variables():<br>\nfor w_name in pretrained_paras:<br>\nif w_name in var.name:<br>\nprint('Initialise var %s with weight %s' % (var.name, w_name))<br>\ntry:<br>\nif 'weights' in var.name:<br>\n# using assign(src, dst) to assign the weights of pre-trained model to current network<br>\n# init_ops.append(var.assign(d[w_name+'/weights:0']))<br>\ninit_ops.append(var.assign(d[w_name]['weights']))<br>\nelif 'biases' in var.name:<br>\n# init_ops.append(var.assign(d[w_name+'/biases:0']))<br>\ninit_ops.append(var.assign(d[w_name]['biases']))<br>\nexcept KeyError:<br>\nif 'weights' in var.name:<br>\n# using assign(src, dst) to assign the weights of pre-trained model to current network<br>\ninit_ops.append(var.assign(d[w_name+'/weights:0']))<br>\n# init_ops.append(var.assign(d[w_name]['weights']))<br>\nelif 'biases' in var.name:<br>\ninit_ops.append(var.assign(d[w_name+'/biases:0']))<br>\n# init_ops.append(var.assign(d[w_name]['biases']))<br>\nexcept:<br>\nif 'weights' in var.name:<br>\n# using assign(src, dst) to assign the weights of pre-trained model to current network<br>\ninit_ops.append(var.assign(d[w_name][0]))<br>\n# init_ops.append(var.assign(d[w_name]['weights']))<br>\nelif 'biases' in var.name:<br>\ninit_ops.append(var.assign(d[w_name][1]))<br>\n# init_ops.append(var.assign(d[w_name]['biases']))<br>\nreturn init_ops</p>\n<p>def compute_euclidean_distance(x, y):<br>\n\"\"\"<br>\nComputes the euclidean distance between two tensorflow variables<br>\n\"\"\"</p>\n<pre><code>d = tf.square(tf.sub(x, y))\nd = tf.sqrt(tf.reduce_sum(d))  # What about the axis ???\nreturn d\n</code></pre>\n<p>def square_distance(x, y):<br>\nreturn tf.reduce_sum(tf.square(x - y), axis=1)</p>\n<p>def compute_triplet_loss(anchor_feature, positive_feature, negative_feature, margin):<br>\nwith tf.name_scope(\"triplet_loss\"):<br>\nd_p_squared = square_distance(anchor_feature, positive_feature)<br>\nd_n_squared = square_distance(anchor_feature, negative_feature)<br>\nloss = tf.maximum(0., d_p_squared - d_n_squared + margin)<br>\nreturn tf.reduce_mean(loss), tf.reduce_mean(d_p_squared), tf.reduce_mean(d_n_squared)</p>\n<p>def main(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model):</p>\n<pre><code>ITERATIONS = 20000\nVALIDATION_TEST = 200\nperc_train = 0.9\nMARGIN = 0.3\nSAVE_STEP = 200\nmodel_path = \"/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/%s/%s/\" % (subset, net_model)\n#pre_trained_model = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'\npre_step = 0\nif not os.path.exists(model_path):\n    os.makedirs(model_path)\n\n\n# Siamease place holders\ntrain_anchor_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"anchor\")\ntrain_positive_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"positive\")\ntrain_negative_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"negative\")\n\n# Creating the architecturek\nif net_model == 'deep_sbir':\n    train_anchor = sketch_a_net_sbir(tf.cast(train_anchor_data, tf.float32) - mean, True)\n    tf.get_variable_scope().reuse_variables()\n    train_positive = sketch_a_net_sbir(tf.cast(train_positive_data, tf.float32) - mean, True)\n    train_negative = sketch_a_net_sbir(tf.cast(train_negative_data, tf.float32) - mean, True)\nelif net_model == 'DSSA':\n    train_anchor = sketch_a_net_dssa(tf.cast(train_anchor_data, tf.float32) - mean, True)\n    tf.get_variable_scope().reuse_variables()\n    train_positive = sketch_a_net_dssa(tf.cast(train_positive_data, tf.float32) - mean, True)\n    train_negative = sketch_a_net_dssa(tf.cast(train_negative_data, tf.float32) - mean, True)\nelse:\n    print 'Please define the net_model'\n\ninit_ops = init_variables()\nloss, positives, negatives = compute_triplet_loss(train_anchor, train_positive, train_negative, MARGIN)\n\n# Defining training parameters\nbatch = tf.Variable(0)\nlearning_rate = 0.001\ndata_sampler = triplet_sampler_asy.TripletSamplingLayer()\ndata_sampler_te = triplet_sampler_asy.TripletSamplingLayer()\ndata_sampler.setup(sketch_dir, image_dir, triplet_path, mean, hard_ratio, batch_size, phase)\ndata_sampler_te.setup(sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase_te)\noptimizer = tf.train.MomentumOptimizer(momentum=0.9, learning_rate=learning_rate).minimize(loss,\n                                                                                           global_step=batch)\n#validation_prediction = tf.nn.softmax(lenet_validation)\n# saver = tf.train.Saver(max_to_keep=5)\ndst_path = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/log'\nmodel_id = '%s_%s_log.txt' % (subset, net_model)\nfilename = dst_path+'/'+model_id\n# f = open(filename, 'a')\n# Training\nwith tf.Session() as session:\n\n    session.run(tf.global_variables_initializer())\n    session.run(init_ops)\n    for step in range(ITERATIONS):\n        f = open(filename, 'a')\n        batch_anchor, batch_positive, batch_negative = data_sampler.get_next_batch()\n\n        feed_dict = {train_anchor_data: batch_anchor,\n                     train_positive_data: batch_positive,\n                     train_negative_data: batch_negative\n                     }\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        # save_path = saver.save(session, model_path, global_step=step)\n        print(\"Iter %d: Loss Train %f\" % (step+pre_step, l))\n        f.write(\"Iter \"+str(step+pre_step) + \": Loss Train: \" + str(l))\n        f.write(\"\\n\")\n        # train_writer.add_summary(summary, step)\n\n        if step % SAVE_STEP == 0:\n            str_temp = '%smodel-iter%d.npy' % (model_path, step+pre_step)\n            save_dict = {var.name: var.eval(session) for var in tf.global_variables()}\n            np.save(str_temp, save_dict)\n\n        if step % VALIDATION_TEST == 0:\n            batch_anchor, batch_positive, batch_negative = data_sampler_te.get_next_batch()\n\n            feed_dict = {train_anchor_data: batch_anchor,\n                         train_positive_data: batch_positive,\n                         train_negative_data: batch_negative\n                         }\n\n            lv = session.run([loss], feed_dict=feed_dict)\n            # test_writer.add_summary(summary, step)\n            print(\"Loss Validation {0}\".format(lv))\n            f.write(\"Loss Validation: \" + str(lv))\n            f.write(\"\\n\")\n        f.close()\n</code></pre>\n<p>if <strong>name</strong> == '<strong>main</strong>':<br>\n# 'deep_sbir'(the model of cvpr16) or 'DSSA'(the model of iccv17)<br>\nnet_model = 'deep_sbir'<br>\nsubset = 'shoes'<br>\nmean = 250.42<br>\nhard_ratio = 0.75<br>\nbatch_size = 128<br>\nphase = 'TRAIN'<br>\nphase_te = 'TEST'<br>\nbase_path = './data'<br>\nsketch_dir = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase.lower())<br>\nimage_dir = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase.lower())<br>\ntriplet_path = '%s/%s/%s_annotation.json' % (base_path, subset, subset) # pseudo annotations for handbags<br>\nsketch_dir_te = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase_te.lower())<br>\nimage_dir_te = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase_te.lower())<br>\nmain(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model)</p>\n<hr>\n<p>This is my code used above, in the # creating the architecture part, the code \"tf.get_variable_scope().reuse_variables()\" cause an error report :<br>\nVariable fc6_s1/weights/Momentum/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?</p>\n<p>I read some issues about reuse and I know the problem is the sentence trying to use some variable not created. But I am a very bigginer of tensorflow, so I don't know how  to fix this code. If any people can help me, thank you very much !</p>", "body_text": "@lukaszkaiser\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nfrom scipy.io import loadmat, savemat\nimport scipy.spatial.distance as ssd\nfrom sbir_sampling import triplet_sampler_asy\nfrom sbir_util import *\nfrom ops import spatial_softmax, reshape_feats\nimport os, errno\nNET_ID = 0 #0 for step3 pre-trained model, 1 for step2 pre-trained model\ndef attentionNet(inputs, pool_method='sigmoid'):\nassert(pool_method in ['sigmoid', 'softmax'])\nwith slim.arg_scope([slim.conv2d],\nactivation_fn=tf.nn.relu,\nweights_initializer=tf.truncated_normal_initializer(0.0, 0.1),\nweights_regularizer=slim.l2_regularizer(0.0005),\ntrainable=True):\nnet = slim.conv2d(inputs, 256, [1, 1], padding='SAME', scope='conv1')\nif pool_method == 'sigmoid':\nnet = slim.conv2d(net, 1, [1, 1], activation_fn=tf.nn.sigmoid, scope='conv2')\nelse:\nnet = slim.conv2d(net, 1, [1, 1], activation_fn=None, scope='conv2')\nnet = spatial_softmax(net)\nreturn net\ndef sketch_a_net_sbir(inputs, trainable):\nwith slim.arg_scope([slim.conv2d, slim.fully_connected],\nactivation_fn=tf.nn.relu,\nweights_initializer=tf.truncated_normal_initializer(0.0, 0.1),\nweights_regularizer=slim.l2_regularizer(0.0005),\ntrainable=False):\nwith slim.arg_scope([slim.conv2d], padding='VALID'):\n# x = tf.reshape(inputs, shape=[-1, 225, 225, 1])\nconv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')\nconv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')\nconv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')\nconv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')\nconv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')\nconv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')\nconv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', scope='conv5_s1')  # trainable=trainable\nconv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')\nconv5 = slim.flatten(conv5)\nfc6 = slim.fully_connected(conv5, 512, trainable=trainable, scope='fc6_s1')\nfc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')\nfc7 = tf.nn.l2_normalize(fc7, dim=1)\nreturn fc7\ndef sketch_a_net_dssa(inputs, trainable):\nwith slim.arg_scope([slim.conv2d, slim.fully_connected],\nactivation_fn=tf.nn.relu,\nweights_initializer=tf.truncated_normal_initializer(0.0, 0.1),\nweights_regularizer=slim.l2_regularizer(0.0005),\ntrainable=False):  # when test 'trainable=True', don't forget to change it\nwith slim.arg_scope([slim.conv2d], padding='VALID'):\n# x = tf.reshape(inputs, shape=[-1, 225, 225, 1])\nconv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')\nconv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')\nconv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')\nconv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')\nconv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')\nconv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')\nconv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', trainable=trainable, scope='conv5_s1')\nconv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')\n# residual attention\natt_mask = attentionNet(conv5, 'softmax')\natt_map = tf.multiply(conv5, att_mask)\natt_f = tf.add(conv5, att_map)\nattended_map = tf.reduce_sum(att_f, reduction_indices=[1, 2])\nattended_map = tf.nn.l2_normalize(attended_map, dim=1)\natt_f = slim.flatten(att_f)\nfc6 = slim.fully_connected(att_f, 512, trainable=trainable, scope='fc6_s1')\nfc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')\nfc7 = tf.nn.l2_normalize(fc7, dim=1)\n# coarse-fine fusion\nfinal_feature_map = tf.concat(1, [fc7, attended_map])\nreturn final_feature_map\ndef init_variables(model_file='/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'):\nif NET_ID==0:\npretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1', 'fc7_sketch']\nelse:\npretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1']\nd = np.load(model_file).item()\ninit_ops = []  # a list of operations\nfor var in tf.global_variables():\nfor w_name in pretrained_paras:\nif w_name in var.name:\nprint('Initialise var %s with weight %s' % (var.name, w_name))\ntry:\nif 'weights' in var.name:\n# using assign(src, dst) to assign the weights of pre-trained model to current network\n# init_ops.append(var.assign(d[w_name+'/weights:0']))\ninit_ops.append(var.assign(d[w_name]['weights']))\nelif 'biases' in var.name:\n# init_ops.append(var.assign(d[w_name+'/biases:0']))\ninit_ops.append(var.assign(d[w_name]['biases']))\nexcept KeyError:\nif 'weights' in var.name:\n# using assign(src, dst) to assign the weights of pre-trained model to current network\ninit_ops.append(var.assign(d[w_name+'/weights:0']))\n# init_ops.append(var.assign(d[w_name]['weights']))\nelif 'biases' in var.name:\ninit_ops.append(var.assign(d[w_name+'/biases:0']))\n# init_ops.append(var.assign(d[w_name]['biases']))\nexcept:\nif 'weights' in var.name:\n# using assign(src, dst) to assign the weights of pre-trained model to current network\ninit_ops.append(var.assign(d[w_name][0]))\n# init_ops.append(var.assign(d[w_name]['weights']))\nelif 'biases' in var.name:\ninit_ops.append(var.assign(d[w_name][1]))\n# init_ops.append(var.assign(d[w_name]['biases']))\nreturn init_ops\ndef compute_euclidean_distance(x, y):\n\"\"\"\nComputes the euclidean distance between two tensorflow variables\n\"\"\"\nd = tf.square(tf.sub(x, y))\nd = tf.sqrt(tf.reduce_sum(d))  # What about the axis ???\nreturn d\n\ndef square_distance(x, y):\nreturn tf.reduce_sum(tf.square(x - y), axis=1)\ndef compute_triplet_loss(anchor_feature, positive_feature, negative_feature, margin):\nwith tf.name_scope(\"triplet_loss\"):\nd_p_squared = square_distance(anchor_feature, positive_feature)\nd_n_squared = square_distance(anchor_feature, negative_feature)\nloss = tf.maximum(0., d_p_squared - d_n_squared + margin)\nreturn tf.reduce_mean(loss), tf.reduce_mean(d_p_squared), tf.reduce_mean(d_n_squared)\ndef main(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model):\nITERATIONS = 20000\nVALIDATION_TEST = 200\nperc_train = 0.9\nMARGIN = 0.3\nSAVE_STEP = 200\nmodel_path = \"/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/%s/%s/\" % (subset, net_model)\n#pre_trained_model = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'\npre_step = 0\nif not os.path.exists(model_path):\n    os.makedirs(model_path)\n\n\n# Siamease place holders\ntrain_anchor_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"anchor\")\ntrain_positive_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"positive\")\ntrain_negative_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"negative\")\n\n# Creating the architecturek\nif net_model == 'deep_sbir':\n    train_anchor = sketch_a_net_sbir(tf.cast(train_anchor_data, tf.float32) - mean, True)\n    tf.get_variable_scope().reuse_variables()\n    train_positive = sketch_a_net_sbir(tf.cast(train_positive_data, tf.float32) - mean, True)\n    train_negative = sketch_a_net_sbir(tf.cast(train_negative_data, tf.float32) - mean, True)\nelif net_model == 'DSSA':\n    train_anchor = sketch_a_net_dssa(tf.cast(train_anchor_data, tf.float32) - mean, True)\n    tf.get_variable_scope().reuse_variables()\n    train_positive = sketch_a_net_dssa(tf.cast(train_positive_data, tf.float32) - mean, True)\n    train_negative = sketch_a_net_dssa(tf.cast(train_negative_data, tf.float32) - mean, True)\nelse:\n    print 'Please define the net_model'\n\ninit_ops = init_variables()\nloss, positives, negatives = compute_triplet_loss(train_anchor, train_positive, train_negative, MARGIN)\n\n# Defining training parameters\nbatch = tf.Variable(0)\nlearning_rate = 0.001\ndata_sampler = triplet_sampler_asy.TripletSamplingLayer()\ndata_sampler_te = triplet_sampler_asy.TripletSamplingLayer()\ndata_sampler.setup(sketch_dir, image_dir, triplet_path, mean, hard_ratio, batch_size, phase)\ndata_sampler_te.setup(sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase_te)\noptimizer = tf.train.MomentumOptimizer(momentum=0.9, learning_rate=learning_rate).minimize(loss,\n                                                                                           global_step=batch)\n#validation_prediction = tf.nn.softmax(lenet_validation)\n# saver = tf.train.Saver(max_to_keep=5)\ndst_path = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/log'\nmodel_id = '%s_%s_log.txt' % (subset, net_model)\nfilename = dst_path+'/'+model_id\n# f = open(filename, 'a')\n# Training\nwith tf.Session() as session:\n\n    session.run(tf.global_variables_initializer())\n    session.run(init_ops)\n    for step in range(ITERATIONS):\n        f = open(filename, 'a')\n        batch_anchor, batch_positive, batch_negative = data_sampler.get_next_batch()\n\n        feed_dict = {train_anchor_data: batch_anchor,\n                     train_positive_data: batch_positive,\n                     train_negative_data: batch_negative\n                     }\n        _, l = session.run([optimizer, loss], feed_dict=feed_dict)\n        # save_path = saver.save(session, model_path, global_step=step)\n        print(\"Iter %d: Loss Train %f\" % (step+pre_step, l))\n        f.write(\"Iter \"+str(step+pre_step) + \": Loss Train: \" + str(l))\n        f.write(\"\\n\")\n        # train_writer.add_summary(summary, step)\n\n        if step % SAVE_STEP == 0:\n            str_temp = '%smodel-iter%d.npy' % (model_path, step+pre_step)\n            save_dict = {var.name: var.eval(session) for var in tf.global_variables()}\n            np.save(str_temp, save_dict)\n\n        if step % VALIDATION_TEST == 0:\n            batch_anchor, batch_positive, batch_negative = data_sampler_te.get_next_batch()\n\n            feed_dict = {train_anchor_data: batch_anchor,\n                         train_positive_data: batch_positive,\n                         train_negative_data: batch_negative\n                         }\n\n            lv = session.run([loss], feed_dict=feed_dict)\n            # test_writer.add_summary(summary, step)\n            print(\"Loss Validation {0}\".format(lv))\n            f.write(\"Loss Validation: \" + str(lv))\n            f.write(\"\\n\")\n        f.close()\n\nif name == 'main':\n# 'deep_sbir'(the model of cvpr16) or 'DSSA'(the model of iccv17)\nnet_model = 'deep_sbir'\nsubset = 'shoes'\nmean = 250.42\nhard_ratio = 0.75\nbatch_size = 128\nphase = 'TRAIN'\nphase_te = 'TEST'\nbase_path = './data'\nsketch_dir = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase.lower())\nimage_dir = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase.lower())\ntriplet_path = '%s/%s/%s_annotation.json' % (base_path, subset, subset) # pseudo annotations for handbags\nsketch_dir_te = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase_te.lower())\nimage_dir_te = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase_te.lower())\nmain(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model)\n\nThis is my code used above, in the # creating the architecture part, the code \"tf.get_variable_scope().reuse_variables()\" cause an error report :\nVariable fc6_s1/weights/Momentum/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\nI read some issues about reuse and I know the problem is the sentence trying to use some variable not created. But I am a very bigginer of tensorflow, so I don't know how  to fix this code. If any people can help me, thank you very much !", "body": "@lukaszkaiser \r\n\r\nimport tensorflow as tf\r\nimport tensorflow.contrib.slim as slim\r\nimport numpy as np\r\nfrom scipy.io import loadmat, savemat\r\nimport scipy.spatial.distance as ssd\r\nfrom sbir_sampling import triplet_sampler_asy\r\nfrom sbir_util import *\r\nfrom ops import spatial_softmax, reshape_feats\r\nimport os, errno\r\n\r\nNET_ID = 0 #0 for step3 pre-trained model, 1 for step2 pre-trained model\r\n\r\n\r\ndef attentionNet(inputs, pool_method='sigmoid'):\r\n    assert(pool_method in ['sigmoid', 'softmax'])\r\n    with slim.arg_scope([slim.conv2d],\r\n                        activation_fn=tf.nn.relu,\r\n                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),\r\n                        weights_regularizer=slim.l2_regularizer(0.0005),\r\n                        trainable=True):\r\n        net = slim.conv2d(inputs, 256, [1, 1], padding='SAME', scope='conv1')\r\n        if pool_method == 'sigmoid':\r\n            net = slim.conv2d(net, 1, [1, 1], activation_fn=tf.nn.sigmoid, scope='conv2')\r\n        else:\r\n            net = slim.conv2d(net, 1, [1, 1], activation_fn=None, scope='conv2')\r\n            net = spatial_softmax(net)\r\n    return net\r\n\r\n\r\ndef sketch_a_net_sbir(inputs, trainable):\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        activation_fn=tf.nn.relu,\r\n                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),\r\n                        weights_regularizer=slim.l2_regularizer(0.0005),\r\n                        trainable=False):\r\n        with slim.arg_scope([slim.conv2d], padding='VALID'):\r\n            # x = tf.reshape(inputs, shape=[-1, 225, 225, 1])\r\n            conv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')\r\n            conv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')\r\n            conv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')\r\n            conv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')\r\n            conv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')\r\n            conv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')\r\n            conv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', scope='conv5_s1')  # trainable=trainable\r\n            conv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')\r\n            conv5 = slim.flatten(conv5)\r\n            fc6 = slim.fully_connected(conv5, 512, trainable=trainable, scope='fc6_s1')\r\n            fc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')\r\n            fc7 = tf.nn.l2_normalize(fc7, dim=1)\r\n    return fc7\r\n\r\n\r\ndef sketch_a_net_dssa(inputs, trainable):\r\n    with slim.arg_scope([slim.conv2d, slim.fully_connected],\r\n                        activation_fn=tf.nn.relu,\r\n                        weights_initializer=tf.truncated_normal_initializer(0.0, 0.1),\r\n                        weights_regularizer=slim.l2_regularizer(0.0005),\r\n                        trainable=False):  # when test 'trainable=True', don't forget to change it\r\n        with slim.arg_scope([slim.conv2d], padding='VALID'):\r\n            # x = tf.reshape(inputs, shape=[-1, 225, 225, 1])\r\n            conv1 = slim.conv2d(inputs, 64, [15, 15], 3, scope='conv1_s1')\r\n            conv1 = slim.max_pool2d(conv1, [3, 3], scope='pool1')\r\n            conv2 = slim.conv2d(conv1, 128, [5, 5], scope='conv2_s1')\r\n            conv2 = slim.max_pool2d(conv2, [3, 3], scope='pool2')\r\n            conv3 = slim.conv2d(conv2, 256, [3, 3], padding='SAME', scope='conv3_s1')\r\n            conv4 = slim.conv2d(conv3, 256, [3, 3], padding='SAME', scope='conv4_s1')\r\n            conv5 = slim.conv2d(conv4, 256, [3, 3], padding='SAME', trainable=trainable, scope='conv5_s1')\r\n            conv5 = slim.max_pool2d(conv5, [3, 3], scope='pool3')\r\n            # residual attention\r\n            att_mask = attentionNet(conv5, 'softmax')\r\n            att_map = tf.multiply(conv5, att_mask)\r\n            att_f = tf.add(conv5, att_map)\r\n            attended_map = tf.reduce_sum(att_f, reduction_indices=[1, 2])\r\n            attended_map = tf.nn.l2_normalize(attended_map, dim=1)\r\n            att_f = slim.flatten(att_f)\r\n            fc6 = slim.fully_connected(att_f, 512, trainable=trainable, scope='fc6_s1')\r\n            fc7 = slim.fully_connected(fc6, 256, activation_fn=None, trainable=trainable, scope='fc7_sketch')\r\n            fc7 = tf.nn.l2_normalize(fc7, dim=1)\r\n            # coarse-fine fusion\r\n            final_feature_map = tf.concat(1, [fc7, attended_map])\r\n    return final_feature_map\r\n\r\n\r\ndef init_variables(model_file='/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'):\r\n    if NET_ID==0:\r\n        pretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1', 'fc7_sketch']\r\n    else:\r\n        pretrained_paras = ['conv1_s1', 'conv2_s1', 'conv3_s1', 'conv4_s1', 'conv5_s1', 'fc6_s1']\r\n    d = np.load(model_file).item()\r\n    init_ops = []  # a list of operations\r\n    for var in tf.global_variables():\r\n        for w_name in pretrained_paras:\r\n            if w_name in var.name:\r\n                print('Initialise var %s with weight %s' % (var.name, w_name))\r\n                try:\r\n                    if 'weights' in var.name:\r\n                        # using assign(src, dst) to assign the weights of pre-trained model to current network\r\n                        # init_ops.append(var.assign(d[w_name+'/weights:0']))\r\n                        init_ops.append(var.assign(d[w_name]['weights']))\r\n                    elif 'biases' in var.name:\r\n                        # init_ops.append(var.assign(d[w_name+'/biases:0']))\r\n                        init_ops.append(var.assign(d[w_name]['biases']))\r\n                except KeyError:\r\n                     if 'weights' in var.name:\r\n                        # using assign(src, dst) to assign the weights of pre-trained model to current network\r\n                        init_ops.append(var.assign(d[w_name+'/weights:0']))\r\n                        # init_ops.append(var.assign(d[w_name]['weights']))\r\n                     elif 'biases' in var.name:\r\n                        init_ops.append(var.assign(d[w_name+'/biases:0']))\r\n                        # init_ops.append(var.assign(d[w_name]['biases']))\r\n                except:\r\n                     if 'weights' in var.name:\r\n                        # using assign(src, dst) to assign the weights of pre-trained model to current network\r\n                        init_ops.append(var.assign(d[w_name][0]))\r\n                        # init_ops.append(var.assign(d[w_name]['weights']))\r\n                     elif 'biases' in var.name:\r\n                        init_ops.append(var.assign(d[w_name][1]))\r\n                        # init_ops.append(var.assign(d[w_name]['biases']))\r\n    return init_ops\r\n\r\n\r\ndef compute_euclidean_distance(x, y):\r\n    \"\"\"\r\n    Computes the euclidean distance between two tensorflow variables\r\n    \"\"\"\r\n\r\n    d = tf.square(tf.sub(x, y))\r\n    d = tf.sqrt(tf.reduce_sum(d))  # What about the axis ???\r\n    return d\r\n\r\n\r\ndef square_distance(x, y):\r\n    return tf.reduce_sum(tf.square(x - y), axis=1)\r\n\r\n\r\ndef compute_triplet_loss(anchor_feature, positive_feature, negative_feature, margin):\r\n    with tf.name_scope(\"triplet_loss\"):\r\n        d_p_squared = square_distance(anchor_feature, positive_feature)\r\n        d_n_squared = square_distance(anchor_feature, negative_feature)\r\n        loss = tf.maximum(0., d_p_squared - d_n_squared + margin)\r\n        return tf.reduce_mean(loss), tf.reduce_mean(d_p_squared), tf.reduce_mean(d_n_squared)\r\n\r\n\r\ndef main(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model):\r\n\r\n    ITERATIONS = 20000\r\n    VALIDATION_TEST = 200\r\n    perc_train = 0.9\r\n    MARGIN = 0.3\r\n    SAVE_STEP = 200\r\n    model_path = \"/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/%s/%s/\" % (subset, net_model)\r\n    #pre_trained_model = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/model/sketchnet_init.npy'\r\n    pre_step = 0\r\n    if not os.path.exists(model_path):\r\n        os.makedirs(model_path)\r\n\r\n\r\n    # Siamease place holders\r\n    train_anchor_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"anchor\")\r\n    train_positive_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"positive\")\r\n    train_negative_data = tf.placeholder(tf.float32, shape=(None, 225, 225, 1), name=\"negative\")\r\n\r\n    # Creating the architecturek\r\n    if net_model == 'deep_sbir':\r\n        train_anchor = sketch_a_net_sbir(tf.cast(train_anchor_data, tf.float32) - mean, True)\r\n        tf.get_variable_scope().reuse_variables()\r\n        train_positive = sketch_a_net_sbir(tf.cast(train_positive_data, tf.float32) - mean, True)\r\n        train_negative = sketch_a_net_sbir(tf.cast(train_negative_data, tf.float32) - mean, True)\r\n    elif net_model == 'DSSA':\r\n        train_anchor = sketch_a_net_dssa(tf.cast(train_anchor_data, tf.float32) - mean, True)\r\n        tf.get_variable_scope().reuse_variables()\r\n        train_positive = sketch_a_net_dssa(tf.cast(train_positive_data, tf.float32) - mean, True)\r\n        train_negative = sketch_a_net_dssa(tf.cast(train_negative_data, tf.float32) - mean, True)\r\n    else:\r\n        print 'Please define the net_model'\r\n\r\n    init_ops = init_variables()\r\n    loss, positives, negatives = compute_triplet_loss(train_anchor, train_positive, train_negative, MARGIN)\r\n\r\n    # Defining training parameters\r\n    batch = tf.Variable(0)\r\n    learning_rate = 0.001\r\n    data_sampler = triplet_sampler_asy.TripletSamplingLayer()\r\n    data_sampler_te = triplet_sampler_asy.TripletSamplingLayer()\r\n    data_sampler.setup(sketch_dir, image_dir, triplet_path, mean, hard_ratio, batch_size, phase)\r\n    data_sampler_te.setup(sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase_te)\r\n    optimizer = tf.train.MomentumOptimizer(momentum=0.9, learning_rate=learning_rate).minimize(loss,\r\n                                                                                               global_step=batch)\r\n    #validation_prediction = tf.nn.softmax(lenet_validation)\r\n    # saver = tf.train.Saver(max_to_keep=5)\r\n    dst_path = '/home/soe/PycharmProjects/Deep_SBIR_tf-master/log'\r\n    model_id = '%s_%s_log.txt' % (subset, net_model)\r\n    filename = dst_path+'/'+model_id\r\n    # f = open(filename, 'a')\r\n    # Training\r\n    with tf.Session() as session:\r\n\r\n        session.run(tf.global_variables_initializer())\r\n        session.run(init_ops)\r\n        for step in range(ITERATIONS):\r\n            f = open(filename, 'a')\r\n            batch_anchor, batch_positive, batch_negative = data_sampler.get_next_batch()\r\n\r\n            feed_dict = {train_anchor_data: batch_anchor,\r\n                         train_positive_data: batch_positive,\r\n                         train_negative_data: batch_negative\r\n                         }\r\n            _, l = session.run([optimizer, loss], feed_dict=feed_dict)\r\n            # save_path = saver.save(session, model_path, global_step=step)\r\n            print(\"Iter %d: Loss Train %f\" % (step+pre_step, l))\r\n            f.write(\"Iter \"+str(step+pre_step) + \": Loss Train: \" + str(l))\r\n            f.write(\"\\n\")\r\n            # train_writer.add_summary(summary, step)\r\n\r\n            if step % SAVE_STEP == 0:\r\n                str_temp = '%smodel-iter%d.npy' % (model_path, step+pre_step)\r\n                save_dict = {var.name: var.eval(session) for var in tf.global_variables()}\r\n                np.save(str_temp, save_dict)\r\n\r\n            if step % VALIDATION_TEST == 0:\r\n                batch_anchor, batch_positive, batch_negative = data_sampler_te.get_next_batch()\r\n\r\n                feed_dict = {train_anchor_data: batch_anchor,\r\n                             train_positive_data: batch_positive,\r\n                             train_negative_data: batch_negative\r\n                             }\r\n\r\n                lv = session.run([loss], feed_dict=feed_dict)\r\n                # test_writer.add_summary(summary, step)\r\n                print(\"Loss Validation {0}\".format(lv))\r\n                f.write(\"Loss Validation: \" + str(lv))\r\n                f.write(\"\\n\")\r\n            f.close()\r\n\r\n\r\nif __name__ == '__main__':\r\n    # 'deep_sbir'(the model of cvpr16) or 'DSSA'(the model of iccv17)\r\n    net_model = 'deep_sbir'  \r\n    subset = 'shoes'\r\n    mean = 250.42\r\n    hard_ratio = 0.75\r\n    batch_size = 128\r\n    phase = 'TRAIN'\r\n    phase_te = 'TEST'\r\n    base_path = './data'\r\n    sketch_dir = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase.lower())\r\n    image_dir = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase.lower())\r\n    triplet_path = '%s/%s/%s_annotation.json' % (base_path, subset, subset) # pseudo annotations for handbags\r\n    sketch_dir_te = '%s/%s/%s_sketch_db_%s.mat' % (base_path, subset, subset, phase_te.lower())\r\n    image_dir_te = '%s/%s/%s_edge_db_%s.mat' % (base_path, subset, subset, phase_te.lower())\r\n    main(subset, sketch_dir, image_dir, sketch_dir_te, image_dir_te, triplet_path, mean, hard_ratio, batch_size, phase, phase_te, net_model)\r\n\r\n\r\n\r\n\r\n-------------------------------------------------------------------------------------------------------------------------------------------\r\n\r\nThis is my code used above, in the # creating the architecture part, the code \"tf.get_variable_scope().reuse_variables()\" cause an error report : \r\nVariable fc6_s1/weights/Momentum/ does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=tf.AUTO_REUSE in VarScope?\r\n\r\nI read some issues about reuse and I know the problem is the sentence trying to use some variable not created. But I am a very bigginer of tensorflow, so I don't know how  to fix this code. If any people can help me, thank you very much !"}