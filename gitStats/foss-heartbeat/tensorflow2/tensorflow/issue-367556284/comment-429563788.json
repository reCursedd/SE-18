{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/429563788", "html_url": "https://github.com/tensorflow/tensorflow/issues/22804#issuecomment-429563788", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22804", "id": 429563788, "node_id": "MDEyOklzc3VlQ29tbWVudDQyOTU2Mzc4OA==", "user": {"login": "calid", "id": 494405, "node_id": "MDQ6VXNlcjQ5NDQwNQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/494405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/calid", "html_url": "https://github.com/calid", "followers_url": "https://api.github.com/users/calid/followers", "following_url": "https://api.github.com/users/calid/following{/other_user}", "gists_url": "https://api.github.com/users/calid/gists{/gist_id}", "starred_url": "https://api.github.com/users/calid/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/calid/subscriptions", "organizations_url": "https://api.github.com/users/calid/orgs", "repos_url": "https://api.github.com/users/calid/repos", "events_url": "https://api.github.com/users/calid/events{/privacy}", "received_events_url": "https://api.github.com/users/calid/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-13T18:17:49Z", "updated_at": "2018-10-13T18:17:49Z", "author_association": "NONE", "body_html": "<p>I think I isolated the problem:  It looks like the <code>use_multiprocessing=True</code> branch of code is not actually handling the multiprocessing use case correctly, in that it isn't using the appropriate multiprocessing constructs (<code>multiprocessing.Queue</code>, <code>multiprocessing.Event</code>...).</p>\n<p>What's interesting is that in the <code>1.11.0</code> version <code>GeneratorEnqueuer</code> and <code>OrderedEnqueuer</code> handle multiprocessing differently, with <code>GeneratorEnqueuer</code> handling it correctly:</p>\n<p><code>GeneratorEnqueuer.start (on 1.11.0)</code>:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/python/keras/utils/data_utils.py#L733-L743\">tensorflow/tensorflow/python/keras/utils/data_utils.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 733 to 743\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/c19e29306ce1777456b2dbb3a14f511edf7883a8\">c19e293</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L733\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"733\"></td>\n          <td id=\"LC733\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>._use_multiprocessing: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L734\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"734\"></td>\n          <td id=\"LC734\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>._manager <span class=\"pl-k\">=</span> multiprocessing.Manager() </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L735\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"735\"></td>\n          <td id=\"LC735\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.queue <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._manager.Queue(<span class=\"pl-v\">maxsize</span><span class=\"pl-k\">=</span>max_queue_size) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L736\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"736\"></td>\n          <td id=\"LC736\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>._stop_event <span class=\"pl-k\">=</span> multiprocessing.Event() </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L737\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"737\"></td>\n          <td id=\"LC737\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">else</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L738\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"738\"></td>\n          <td id=\"LC738\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c\"><span class=\"pl-c\">#</span> On all OSes, avoid **SYSTEMATIC** error in multithreading mode:</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L739\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"739\"></td>\n          <td id=\"LC739\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c\"><span class=\"pl-c\">#</span> `ValueError: generator already executing`</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L740\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"740\"></td>\n          <td id=\"LC740\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c\"><span class=\"pl-c\">#</span> =&gt; Serialize calls to infinite iterator/generator's next() function</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L741\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"741\"></td>\n          <td id=\"LC741\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.genlock <span class=\"pl-k\">=</span> threading.Lock() </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L742\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"742\"></td>\n          <td id=\"LC742\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.queue <span class=\"pl-k\">=</span> queue.Queue(<span class=\"pl-v\">maxsize</span><span class=\"pl-k\">=</span>max_queue_size) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L743\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"743\"></td>\n          <td id=\"LC743\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>._stop_event <span class=\"pl-k\">=</span> threading.Event() </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>vs <code>OrderedEnqueuer</code> which does not:</p>\n<p><code>OrderedEnqueuer.start (on 1.11.0)</code>:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/python/keras/utils/data_utils.py#L549-L560\">tensorflow/tensorflow/python/keras/utils/data_utils.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 549 to 560\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/c19e29306ce1777456b2dbb3a14f511edf7883a8\">c19e293</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L549\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"549\"></td>\n          <td id=\"LC549\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.use_multiprocessing: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L550\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"550\"></td>\n          <td id=\"LC550\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.executor_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">seqs</span>: multiprocessing.Pool(  <span class=\"pl-c\"><span class=\"pl-c\">#</span> pylint: disable=g-long-lambda</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L551\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"551\"></td>\n          <td id=\"LC551\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">       workers, <span class=\"pl-v\">initializer</span><span class=\"pl-k\">=</span>init_pool, <span class=\"pl-v\">initargs</span><span class=\"pl-k\">=</span>(seqs,)) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L552\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"552\"></td>\n          <td id=\"LC552\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">else</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L553\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"553\"></td>\n          <td id=\"LC553\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">    <span class=\"pl-c\"><span class=\"pl-c\">#</span> We do not need the init since it's threads.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L554\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"554\"></td>\n          <td id=\"LC554\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.executor_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">_</span>: ThreadPool(workers) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L555\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"555\"></td>\n          <td id=\"LC555\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.workers <span class=\"pl-k\">=</span> workers </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L556\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"556\"></td>\n          <td id=\"LC556\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.queue <span class=\"pl-k\">=</span> queue.Queue(max_queue_size) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L557\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"557\"></td>\n          <td id=\"LC557\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.stop_signal <span class=\"pl-k\">=</span> threading.Event() </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L558\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"558\"></td>\n          <td id=\"LC558\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.run_thread <span class=\"pl-k\">=</span> threading.Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._run) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L559\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"559\"></td>\n          <td id=\"LC559\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.run_thread.daemon <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L560\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"560\"></td>\n          <td id=\"LC560\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.run_thread.start() </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>and sure enough on <code>1.11.0</code> only the OrderedEnqueuer tests hang, GeneratorEnqueuer tests do not.</p>\n<p>On master it looks like there was an effort to coalesce the code and eliminate duplication by moving common logic into the SequenceEnqueuer base class.  Unfortunately it looks like the incorrect OrderedEnqueuer version of the code was used, and so now all the multiprocessing tests hang on master.</p>\n<p><code>SequenceEnqueuer.start (on master)</code>:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/573985cacba7ba36032fe3ac55f4ca9b52e08033/tensorflow/python/keras/utils/data_utils.py#L511-L521\">tensorflow/tensorflow/python/keras/utils/data_utils.py</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n        Lines 511 to 521\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/573985cacba7ba36032fe3ac55f4ca9b52e08033\">573985c</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L511\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"511\"></td>\n          <td id=\"LC511\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> <span class=\"pl-c1\">self</span>.use_multiprocessing: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L512\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"512\"></td>\n          <td id=\"LC512\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.executor_fn <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>._get_executor_init(workers) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L513\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"513\"></td>\n          <td id=\"LC513\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">else</span>: </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L514\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"514\"></td>\n          <td id=\"LC514\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c\"><span class=\"pl-c\">#</span> We do not need the init since it's threads.</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L515\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"515\"></td>\n          <td id=\"LC515\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\">   <span class=\"pl-c1\">self</span>.executor_fn <span class=\"pl-k\">=</span> <span class=\"pl-k\">lambda</span> <span class=\"pl-smi\">_</span>: ThreadPool(workers) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L516\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"516\"></td>\n          <td id=\"LC516\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.workers <span class=\"pl-k\">=</span> workers </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L517\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"517\"></td>\n          <td id=\"LC517\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.queue <span class=\"pl-k\">=</span> queue.Queue(max_queue_size) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L518\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"518\"></td>\n          <td id=\"LC518\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.stop_signal <span class=\"pl-k\">=</span> threading.Event() </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L519\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"519\"></td>\n          <td id=\"LC519\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.run_thread <span class=\"pl-k\">=</span> threading.Thread(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>._run) </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L520\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"520\"></td>\n          <td id=\"LC520\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.run_thread.daemon <span class=\"pl-k\">=</span> <span class=\"pl-c1\">True</span> </td>\n        </tr>\n\n        <tr class=\"border-0\">\n          <td id=\"L521\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"521\"></td>\n          <td id=\"LC521\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-c1\">self</span>.run_thread.start() </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>I'm going to work on putting together a PR to use the correct multiprocessing constructs when <code>use_multiprocessing=True</code> (basically to just use the GeneratorEnqueuer version of the multiprocessing  logic).  Let me know if I'm on the right track with this or if I've misinterpreted things.</p>", "body_text": "I think I isolated the problem:  It looks like the use_multiprocessing=True branch of code is not actually handling the multiprocessing use case correctly, in that it isn't using the appropriate multiprocessing constructs (multiprocessing.Queue, multiprocessing.Event...).\nWhat's interesting is that in the 1.11.0 version GeneratorEnqueuer and OrderedEnqueuer handle multiprocessing differently, with GeneratorEnqueuer handling it correctly:\nGeneratorEnqueuer.start (on 1.11.0):\n\n  \n    \n      tensorflow/tensorflow/python/keras/utils/data_utils.py\n    \n    \n        Lines 733 to 743\n      in\n      c19e293\n    \n    \n    \n    \n\n        \n          \n           if self._use_multiprocessing: \n        \n\n        \n          \n             self._manager = multiprocessing.Manager() \n        \n\n        \n          \n             self.queue = self._manager.Queue(maxsize=max_queue_size) \n        \n\n        \n          \n             self._stop_event = multiprocessing.Event() \n        \n\n        \n          \n           else: \n        \n\n        \n          \n             # On all OSes, avoid **SYSTEMATIC** error in multithreading mode: \n        \n\n        \n          \n             # `ValueError: generator already executing` \n        \n\n        \n          \n             # => Serialize calls to infinite iterator/generator's next() function \n        \n\n        \n          \n             self.genlock = threading.Lock() \n        \n\n        \n          \n             self.queue = queue.Queue(maxsize=max_queue_size) \n        \n\n        \n          \n             self._stop_event = threading.Event() \n        \n    \n  \n\n\nvs OrderedEnqueuer which does not:\nOrderedEnqueuer.start (on 1.11.0):\n\n  \n    \n      tensorflow/tensorflow/python/keras/utils/data_utils.py\n    \n    \n        Lines 549 to 560\n      in\n      c19e293\n    \n    \n    \n    \n\n        \n          \n           if self.use_multiprocessing: \n        \n\n        \n          \n             self.executor_fn = lambda seqs: multiprocessing.Pool(  # pylint: disable=g-long-lambda \n        \n\n        \n          \n                 workers, initializer=init_pool, initargs=(seqs,)) \n        \n\n        \n          \n           else: \n        \n\n        \n          \n              # We do not need the init since it's threads. \n        \n\n        \n          \n             self.executor_fn = lambda _: ThreadPool(workers) \n        \n\n        \n          \n           self.workers = workers \n        \n\n        \n          \n           self.queue = queue.Queue(max_queue_size) \n        \n\n        \n          \n           self.stop_signal = threading.Event() \n        \n\n        \n          \n           self.run_thread = threading.Thread(target=self._run) \n        \n\n        \n          \n           self.run_thread.daemon = True \n        \n\n        \n          \n           self.run_thread.start() \n        \n    \n  \n\n\nand sure enough on 1.11.0 only the OrderedEnqueuer tests hang, GeneratorEnqueuer tests do not.\nOn master it looks like there was an effort to coalesce the code and eliminate duplication by moving common logic into the SequenceEnqueuer base class.  Unfortunately it looks like the incorrect OrderedEnqueuer version of the code was used, and so now all the multiprocessing tests hang on master.\nSequenceEnqueuer.start (on master):\n\n  \n    \n      tensorflow/tensorflow/python/keras/utils/data_utils.py\n    \n    \n        Lines 511 to 521\n      in\n      573985c\n    \n    \n    \n    \n\n        \n          \n           if self.use_multiprocessing: \n        \n\n        \n          \n             self.executor_fn = self._get_executor_init(workers) \n        \n\n        \n          \n           else: \n        \n\n        \n          \n             # We do not need the init since it's threads. \n        \n\n        \n          \n             self.executor_fn = lambda _: ThreadPool(workers) \n        \n\n        \n          \n           self.workers = workers \n        \n\n        \n          \n           self.queue = queue.Queue(max_queue_size) \n        \n\n        \n          \n           self.stop_signal = threading.Event() \n        \n\n        \n          \n           self.run_thread = threading.Thread(target=self._run) \n        \n\n        \n          \n           self.run_thread.daemon = True \n        \n\n        \n          \n           self.run_thread.start() \n        \n    \n  \n\n\nI'm going to work on putting together a PR to use the correct multiprocessing constructs when use_multiprocessing=True (basically to just use the GeneratorEnqueuer version of the multiprocessing  logic).  Let me know if I'm on the right track with this or if I've misinterpreted things.", "body": "I think I isolated the problem:  It looks like the `use_multiprocessing=True` branch of code is not actually handling the multiprocessing use case correctly, in that it isn't using the appropriate multiprocessing constructs (`multiprocessing.Queue`, `multiprocessing.Event`...).  \r\n\r\nWhat's interesting is that in the `1.11.0` version `GeneratorEnqueuer` and `OrderedEnqueuer` handle multiprocessing differently, with `GeneratorEnqueuer` handling it correctly: \r\n\r\n`GeneratorEnqueuer.start (on 1.11.0)`:\r\nhttps://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/python/keras/utils/data_utils.py#L733-L743\r\n\r\nvs `OrderedEnqueuer` which does not:\r\n\r\n`OrderedEnqueuer.start (on 1.11.0)`:\r\nhttps://github.com/tensorflow/tensorflow/blob/c19e29306ce1777456b2dbb3a14f511edf7883a8/tensorflow/python/keras/utils/data_utils.py#L549-L560\r\n\r\nand sure enough on `1.11.0` only the OrderedEnqueuer tests hang, GeneratorEnqueuer tests do not.\r\n\r\nOn master it looks like there was an effort to coalesce the code and eliminate duplication by moving common logic into the SequenceEnqueuer base class.  Unfortunately it looks like the incorrect OrderedEnqueuer version of the code was used, and so now all the multiprocessing tests hang on master.\r\n\r\n`SequenceEnqueuer.start (on master)`:\r\nhttps://github.com/tensorflow/tensorflow/blob/573985cacba7ba36032fe3ac55f4ca9b52e08033/tensorflow/python/keras/utils/data_utils.py#L511-L521\r\n\r\n\r\nI'm going to work on putting together a PR to use the correct multiprocessing constructs when `use_multiprocessing=True` (basically to just use the GeneratorEnqueuer version of the multiprocessing  logic).  Let me know if I'm on the right track with this or if I've misinterpreted things."}