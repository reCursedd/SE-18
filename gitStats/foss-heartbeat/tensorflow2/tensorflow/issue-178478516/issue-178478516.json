{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4524", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4524/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4524/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4524/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4524", "id": 178478516, "node_id": "MDU6SXNzdWUxNzg0Nzg1MTY=", "number": 4524, "title": "bidirectional_rnn not taking sequence_length [batch_size]", "user": {"login": "hurshprasad", "id": 798674, "node_id": "MDQ6VXNlcjc5ODY3NA==", "avatar_url": "https://avatars2.githubusercontent.com/u/798674?v=4", "gravatar_id": "", "url": "https://api.github.com/users/hurshprasad", "html_url": "https://github.com/hurshprasad", "followers_url": "https://api.github.com/users/hurshprasad/followers", "following_url": "https://api.github.com/users/hurshprasad/following{/other_user}", "gists_url": "https://api.github.com/users/hurshprasad/gists{/gist_id}", "starred_url": "https://api.github.com/users/hurshprasad/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/hurshprasad/subscriptions", "organizations_url": "https://api.github.com/users/hurshprasad/orgs", "repos_url": "https://api.github.com/users/hurshprasad/repos", "events_url": "https://api.github.com/users/hurshprasad/events{/privacy}", "received_events_url": "https://api.github.com/users/hurshprasad/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-09-21T22:30:46Z", "updated_at": "2016-09-22T05:17:53Z", "closed_at": "2016-09-22T05:17:53Z", "author_association": "NONE", "body_html": "<p>Trying to do variable sequence length with brinn and not able to use placeholder of batch_size as sequence length, instead wants num_steps size even though function description says [batch_size].</p>\n<p>In the code below setting up the graph gives an dimension error. (I am using the latest tensorflow version 0.10.0.)</p>\n<pre><code>import tensorflow as tf\nflags = tf.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_bool(\"use_fp16\", False,\n                  \"Train using 16-bit floats instead of 32bit floats\")\n\ndef data_type():\n    return tf.float16 if FLAGS.use_fp16 else tf.float32\n\nbatch_size = 20\nnum_steps = 40\nhidden_size = 64\nnum_layers = 2\nvocab_size = 1000000\n\nembedding_input = tf.placeholder(tf.int32, [batch_size, num_steps])\nsequence_length = tf.placeholder(tf.int32, [batch_size])\nembeddings = tf.get_variable(\"embedding\",\n                             [vocab_size, 300],dtype=data_type(),trainable=False)\n\nembedding_input = tf.nn.embedding_lookup(embeddings, embedding_input)\n\n\ninitializer = tf.random_uniform_initializer(-1,1)\n\nlstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\nlstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\n\nlstm_fw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell] * num_layers)\nlstm_bw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell] * num_layers)\n\nlstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.9)\nlstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.9)\ninputs = tf.nn.dropout(embedding_input, 0.9, noise_shape=None, seed=None)\n\ninputs = [tf.squeeze(x) for x in tf.split(0, batch_size, inputs)]\noutput, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell,\n                                       lstm_bw_cell,inputs,\n                                       sequence_length=sequence_length,\n                                       dtype=tf.float32)\n</code></pre>\n<p>Traceback (most recent call last):<br>\nFile \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 566, in merge_with<br>\nnew_dims.append(dim.merge_with(other[i]))<br>\nFile \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 133, in merge_with<br>\nself.assert_is_compatible_with(other)<br>\nFile \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 108, in assert_is_compatible_with<br>\n% (self, other))<br>\nValueError: Dimensions 20 and 40 are not compatible</p>", "body_text": "Trying to do variable sequence length with brinn and not able to use placeholder of batch_size as sequence length, instead wants num_steps size even though function description says [batch_size].\nIn the code below setting up the graph gives an dimension error. (I am using the latest tensorflow version 0.10.0.)\nimport tensorflow as tf\nflags = tf.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_bool(\"use_fp16\", False,\n                  \"Train using 16-bit floats instead of 32bit floats\")\n\ndef data_type():\n    return tf.float16 if FLAGS.use_fp16 else tf.float32\n\nbatch_size = 20\nnum_steps = 40\nhidden_size = 64\nnum_layers = 2\nvocab_size = 1000000\n\nembedding_input = tf.placeholder(tf.int32, [batch_size, num_steps])\nsequence_length = tf.placeholder(tf.int32, [batch_size])\nembeddings = tf.get_variable(\"embedding\",\n                             [vocab_size, 300],dtype=data_type(),trainable=False)\n\nembedding_input = tf.nn.embedding_lookup(embeddings, embedding_input)\n\n\ninitializer = tf.random_uniform_initializer(-1,1)\n\nlstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\nlstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\n\nlstm_fw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell] * num_layers)\nlstm_bw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell] * num_layers)\n\nlstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.9)\nlstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.9)\ninputs = tf.nn.dropout(embedding_input, 0.9, noise_shape=None, seed=None)\n\ninputs = [tf.squeeze(x) for x in tf.split(0, batch_size, inputs)]\noutput, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell,\n                                       lstm_bw_cell,inputs,\n                                       sequence_length=sequence_length,\n                                       dtype=tf.float32)\n\nTraceback (most recent call last):\nFile \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 566, in merge_with\nnew_dims.append(dim.merge_with(other[i]))\nFile \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 133, in merge_with\nself.assert_is_compatible_with(other)\nFile \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 108, in assert_is_compatible_with\n% (self, other))\nValueError: Dimensions 20 and 40 are not compatible", "body": "Trying to do variable sequence length with brinn and not able to use placeholder of batch_size as sequence length, instead wants num_steps size even though function description says [batch_size].  \n\nIn the code below setting up the graph gives an dimension error. (I am using the latest tensorflow version 0.10.0.)\n\n```\nimport tensorflow as tf\nflags = tf.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_bool(\"use_fp16\", False,\n                  \"Train using 16-bit floats instead of 32bit floats\")\n\ndef data_type():\n    return tf.float16 if FLAGS.use_fp16 else tf.float32\n\nbatch_size = 20\nnum_steps = 40\nhidden_size = 64\nnum_layers = 2\nvocab_size = 1000000\n\nembedding_input = tf.placeholder(tf.int32, [batch_size, num_steps])\nsequence_length = tf.placeholder(tf.int32, [batch_size])\nembeddings = tf.get_variable(\"embedding\",\n                             [vocab_size, 300],dtype=data_type(),trainable=False)\n\nembedding_input = tf.nn.embedding_lookup(embeddings, embedding_input)\n\n\ninitializer = tf.random_uniform_initializer(-1,1)\n\nlstm_fw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\nlstm_bw_cell = tf.nn.rnn_cell.LSTMCell(num_units=hidden_size,\n                                       initializer=initializer)\n\nlstm_fw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_fw_cell] * num_layers)\nlstm_bw_cell = tf.nn.rnn_cell.MultiRNNCell([lstm_bw_cell] * num_layers)\n\nlstm_fw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_fw_cell, output_keep_prob=0.9)\nlstm_bw_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_bw_cell, output_keep_prob=0.9)\ninputs = tf.nn.dropout(embedding_input, 0.9, noise_shape=None, seed=None)\n\ninputs = [tf.squeeze(x) for x in tf.split(0, batch_size, inputs)]\noutput, _, _ = tf.nn.bidirectional_rnn(lstm_fw_cell,\n                                       lstm_bw_cell,inputs,\n                                       sequence_length=sequence_length,\n                                       dtype=tf.float32)\n```\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 566, in merge_with\n    new_dims.append(dim.merge_with(other[i]))\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 133, in merge_with\n    self.assert_is_compatible_with(other)\n  File \"/usr/local/lib/python3.5/site-packages/tensorflow/python/framework/tensor_shape.py\", line 108, in assert_is_compatible_with\n    % (self, other))\nValueError: Dimensions 20 and 40 are not compatible\n"}