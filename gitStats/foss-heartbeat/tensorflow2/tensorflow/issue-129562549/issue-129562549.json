{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/918", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/918/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/918/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/918/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/918", "id": 129562549, "node_id": "MDU6SXNzdWUxMjk1NjI1NDk=", "number": 918, "title": "bug in machine translation example - learning rate randomly changes", "user": {"login": "thompsonb", "id": 3534106, "node_id": "MDQ6VXNlcjM1MzQxMDY=", "avatar_url": "https://avatars1.githubusercontent.com/u/3534106?v=4", "gravatar_id": "", "url": "https://api.github.com/users/thompsonb", "html_url": "https://github.com/thompsonb", "followers_url": "https://api.github.com/users/thompsonb/followers", "following_url": "https://api.github.com/users/thompsonb/following{/other_user}", "gists_url": "https://api.github.com/users/thompsonb/gists{/gist_id}", "starred_url": "https://api.github.com/users/thompsonb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/thompsonb/subscriptions", "organizations_url": "https://api.github.com/users/thompsonb/orgs", "repos_url": "https://api.github.com/users/thompsonb/repos", "events_url": "https://api.github.com/users/thompsonb/events{/privacy}", "received_events_url": "https://api.github.com/users/thompsonb/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-01-28T19:59:42Z", "updated_at": "2016-02-03T17:02:17Z", "closed_at": "2016-01-29T16:24:21Z", "author_association": "NONE", "body_html": "<p>I'm running into a strange issue with translate.py. I'm getting NaNs in the learning rate, or at least that's what my debug printouts suggest.</p>\n<p>My code is bleeding edge (5572b1a9205d94a0de8dc843f51197ce8bbedf7a) and unmodified except I added print statements like this at the start, and after each model.get_batch and model.step call:<br>\nprint('got a batch: lr = %.9f'% model.learning_rate.eval())</p>\n<p>Each run is different, but here are is one example:</p>\n<p>CUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 200 --max_train_data_size 110000<br>\n...</p>\n<pre><code>Creating 3 layers of 600 units.\nCreated model with fresh parameters.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400\nbefore loop: lr=0.500000000\nReading development and training data (limit: 110000).\n  reading data line 100000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3359 evicted_count=1000 eviction_rate=0.297708 and unsatisfied allocation rate=0.704397\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 22 get requests, put_count=2034 evicted_count=2000 eviction_rate=0.983284 and unsatisfied allocation rate=0\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 46 get requests, put_count=7059 evicted_count=7000 eviction_rate=0.991642 and unsatisfied allocation rate=0\ndid a step: lr = 0.967365444\ngot a batch: lr = 0.967365444\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 44 get requests, put_count=6060 evicted_count=6000 eviction_rate=0.990099 and unsatisfied allocation rate=0\ndid a step: lr = nan\ngot a batch: lr = nan\ndid a step: lr = nan\ngot a batch: lr = nan\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 6 get requests, put_count=3025 evicted_count=3000 eviction_rate=0.991736 and unsatisfied allocation rate=0\n</code></pre>\n<p>And here is another:</p>\n<p>CUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 2 --max_train_data_size 110000</p>\n<pre><code>\nCreating 3 layers of 600 units.\nCreated model with fresh parameters.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400\nbefore loop: lr=0.500000000\nReading development and training data (limit: 110000).\n  reading data line 100000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3316 evicted_count=1000 eviction_rate=0.301568 and unsatisfied allocation rate=0.710024\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8 get requests, put_count=5019 evicted_count=5000 eviction_rate=0.996214 and unsatisfied allocation rate=0\ndid a step: lr = 0.000000000\nglobal step 2 learning rate 0.0000 step-time 4.49 perplexity 14766.28\n  eval: bucket 0 perplexity inf\n  eval: bucket 1 perplexity inf\n  eval: bucket 2 perplexity inf\n  eval: bucket 3 perplexity inf\n</code></pre>\n<p>Can anyone else replicate this, or have any idea how to fix it? Seems like a buffer overflow...</p>", "body_text": "I'm running into a strange issue with translate.py. I'm getting NaNs in the learning rate, or at least that's what my debug printouts suggest.\nMy code is bleeding edge (5572b1a9205d94a0de8dc843f51197ce8bbedf7a) and unmodified except I added print statements like this at the start, and after each model.get_batch and model.step call:\nprint('got a batch: lr = %.9f'% model.learning_rate.eval())\nEach run is different, but here are is one example:\nCUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 200 --max_train_data_size 110000\n...\nCreating 3 layers of 600 units.\nCreated model with fresh parameters.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400\nbefore loop: lr=0.500000000\nReading development and training data (limit: 110000).\n  reading data line 100000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3359 evicted_count=1000 eviction_rate=0.297708 and unsatisfied allocation rate=0.704397\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 22 get requests, put_count=2034 evicted_count=2000 eviction_rate=0.983284 and unsatisfied allocation rate=0\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 46 get requests, put_count=7059 evicted_count=7000 eviction_rate=0.991642 and unsatisfied allocation rate=0\ndid a step: lr = 0.967365444\ngot a batch: lr = 0.967365444\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 44 get requests, put_count=6060 evicted_count=6000 eviction_rate=0.990099 and unsatisfied allocation rate=0\ndid a step: lr = nan\ngot a batch: lr = nan\ndid a step: lr = nan\ngot a batch: lr = nan\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 6 get requests, put_count=3025 evicted_count=3000 eviction_rate=0.991736 and unsatisfied allocation rate=0\n\nAnd here is another:\nCUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 2 --max_train_data_size 110000\n\nCreating 3 layers of 600 units.\nCreated model with fresh parameters.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400\nbefore loop: lr=0.500000000\nReading development and training data (limit: 110000).\n  reading data line 100000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3316 evicted_count=1000 eviction_rate=0.301568 and unsatisfied allocation rate=0.710024\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8 get requests, put_count=5019 evicted_count=5000 eviction_rate=0.996214 and unsatisfied allocation rate=0\ndid a step: lr = 0.000000000\nglobal step 2 learning rate 0.0000 step-time 4.49 perplexity 14766.28\n  eval: bucket 0 perplexity inf\n  eval: bucket 1 perplexity inf\n  eval: bucket 2 perplexity inf\n  eval: bucket 3 perplexity inf\n\nCan anyone else replicate this, or have any idea how to fix it? Seems like a buffer overflow...", "body": "I'm running into a strange issue with translate.py. I'm getting NaNs in the learning rate, or at least that's what my debug printouts suggest. \n\nMy code is bleeding edge (5572b1a9205d94a0de8dc843f51197ce8bbedf7a) and unmodified except I added print statements like this at the start, and after each model.get_batch and model.step call:\nprint('got a batch: lr = %.9f'% model.learning_rate.eval())\n\nEach run is different, but here are is one example:\n\nCUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 200 --max_train_data_size 110000\n...\n\n```\nCreating 3 layers of 600 units.\nCreated model with fresh parameters.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400\nbefore loop: lr=0.500000000\nReading development and training data (limit: 110000).\n  reading data line 100000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3359 evicted_count=1000 eviction_rate=0.297708 and unsatisfied allocation rate=0.704397\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 22 get requests, put_count=2034 evicted_count=2000 eviction_rate=0.983284 and unsatisfied allocation rate=0\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 46 get requests, put_count=7059 evicted_count=7000 eviction_rate=0.991642 and unsatisfied allocation rate=0\ndid a step: lr = 0.967365444\ngot a batch: lr = 0.967365444\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 44 get requests, put_count=6060 evicted_count=6000 eviction_rate=0.990099 and unsatisfied allocation rate=0\ndid a step: lr = nan\ngot a batch: lr = nan\ndid a step: lr = nan\ngot a batch: lr = nan\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 6 get requests, put_count=3025 evicted_count=3000 eviction_rate=0.991736 and unsatisfied allocation rate=0\n```\n\nAnd here is another:\n\nCUDA_VISIBLE_DEVICES=1 python translate.py --data_dir enfr_data --train_dir foo --en_vocab_size=50000 --fr_vocab_size=50000 --size=600 --num_layers=3 --batch_size 128 --steps_per_checkpoint 2 --max_train_data_size 110000\n\n```\n\nCreating 3 layers of 600 units.\nCreated model with fresh parameters.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:73] Allocating 10.57GiB bytes.\nI tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:83] GPU 0 memory begins at 0x2047a0000 extends to 0x4a8b24400\nbefore loop: lr=0.500000000\nReading development and training data (limit: 110000).\n  reading data line 100000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 7642 get requests, put_count=3316 evicted_count=1000 eviction_rate=0.301568 and unsatisfied allocation rate=0.710024\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110\ndid a step: lr = 0.500000000\ngot a batch: lr = 0.500000000\nI tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 8 get requests, put_count=5019 evicted_count=5000 eviction_rate=0.996214 and unsatisfied allocation rate=0\ndid a step: lr = 0.000000000\nglobal step 2 learning rate 0.0000 step-time 4.49 perplexity 14766.28\n  eval: bucket 0 perplexity inf\n  eval: bucket 1 perplexity inf\n  eval: bucket 2 perplexity inf\n  eval: bucket 3 perplexity inf\n```\n\nCan anyone else replicate this, or have any idea how to fix it? Seems like a buffer overflow...\n"}