{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14595", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14595/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14595/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/14595/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/14595", "id": 274296432, "node_id": "MDU6SXNzdWUyNzQyOTY0MzI=", "number": 14595, "title": "Adding a custom Tensorflow Op under Windows/cmake does not work with TF_LoadLibrary", "user": {"login": "stefanseibert", "id": 2681917, "node_id": "MDQ6VXNlcjI2ODE5MTc=", "avatar_url": "https://avatars1.githubusercontent.com/u/2681917?v=4", "gravatar_id": "", "url": "https://api.github.com/users/stefanseibert", "html_url": "https://github.com/stefanseibert", "followers_url": "https://api.github.com/users/stefanseibert/followers", "following_url": "https://api.github.com/users/stefanseibert/following{/other_user}", "gists_url": "https://api.github.com/users/stefanseibert/gists{/gist_id}", "starred_url": "https://api.github.com/users/stefanseibert/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/stefanseibert/subscriptions", "organizations_url": "https://api.github.com/users/stefanseibert/orgs", "repos_url": "https://api.github.com/users/stefanseibert/repos", "events_url": "https://api.github.com/users/stefanseibert/events{/privacy}", "received_events_url": "https://api.github.com/users/stefanseibert/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-11-15T20:38:27Z", "updated_at": "2018-02-14T20:03:21Z", "closed_at": "2018-02-14T20:03:21Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nI have a custom fork (<a href=\"https://github.com/stefanseibert/tensorflow/tree/r1.3\">https://github.com/stefanseibert/tensorflow/tree/r1.3</a>) which is forked from r1.3 and the only modifications are some commented out lines to be able to build AVX support as described in another ticket, and the recent added fix for wide strings.</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nWindows 10, 64 bit, cmake</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nBuilt from source under windows with the cmake setup</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\n1.3.1</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\n3.6</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\nDoes not apply</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nMicrosoft (R) Build Engine version 14.0.25420.1</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nCUDA 8 / cuDNN 5.1`</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nGTX 980 Ti 6GB, 64GB main memory</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:<br>\nDoes not apply</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When using the cmake setup on windows to build Tensorflow from source and using AddUserOps which was added from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=22941064\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/guschmue\">@guschmue</a> some time ago (example like <a href=\"https://gist.github.com/guschmue/2908e4411edc2faef6ddfe87a2ce4a1d\">https://gist.github.com/guschmue/2908e4411edc2faef6ddfe87a2ce4a1d</a>), I am able to build GPU enabled tensorflow ops. I can load the DLL in python with tf.load_op_library() and can actually use it there. I can built GraphDefs with my custom Op and export it as protobuf file.</p>\n<p>When I try to use this graph for inference in another application where I use the C++ API its not possible for me to get the op loaded and registered. Loading the same DLL (as which with the python API succeeds) works on a system level with C/C++ (the DLL is loaded successfully as seen through procmon.exe or dependency walker and TF_LoadLibrary returns with status ok) but when trying to run the Graph afterwards the custom op is not recognized from Tensorflow and Tensorflow errors with \"Not found: Op type not registered...\". Trying to get the OpList afterwards with the Lib handle also returns no ops. So somehow the ops are not seen here even though they are recognized from python side. I tried a lot of different things to circumvent this like described in my Stack Overflow Question here:</p>\n<p><a href=\"https://stackoverflow.com/questions/47309425/tensorflow-op-and-kernel-do-not-register-on-windows-with-cmake\" rel=\"nofollow\">https://stackoverflow.com/questions/47309425/tensorflow-op-and-kernel-do-not-register-on-windows-with-cmake</a></p>\n<p>but none of the approaches worked. It seems like that the op registration which is done after loading the DLL on python side is not properly done when performing the same operation with TF_LoadLibrary from C/C++. Since I cannot get tensorflow built with debug symbols I dont have a callstack where this registration fails unfortunately.</p>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nI have a custom fork (https://github.com/stefanseibert/tensorflow/tree/r1.3) which is forked from r1.3 and the only modifications are some commented out lines to be able to build AVX support as described in another ticket, and the recent added fix for wide strings.\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nWindows 10, 64 bit, cmake\n\n\nTensorFlow installed from (source or binary):\nBuilt from source under windows with the cmake setup\n\n\nTensorFlow version (use command below):\n1.3.1\n\n\nPython version:\n3.6\n\n\nBazel version (if compiling from source):\nDoes not apply\n\n\nGCC/Compiler version (if compiling from source):\nMicrosoft (R) Build Engine version 14.0.25420.1\n\n\nCUDA/cuDNN version:\nCUDA 8 / cuDNN 5.1`\n\n\nGPU model and memory:\nGTX 980 Ti 6GB, 64GB main memory\n\n\nExact command to reproduce:\nDoes not apply\n\n\nDescribe the problem\nWhen using the cmake setup on windows to build Tensorflow from source and using AddUserOps which was added from @guschmue some time ago (example like https://gist.github.com/guschmue/2908e4411edc2faef6ddfe87a2ce4a1d), I am able to build GPU enabled tensorflow ops. I can load the DLL in python with tf.load_op_library() and can actually use it there. I can built GraphDefs with my custom Op and export it as protobuf file.\nWhen I try to use this graph for inference in another application where I use the C++ API its not possible for me to get the op loaded and registered. Loading the same DLL (as which with the python API succeeds) works on a system level with C/C++ (the DLL is loaded successfully as seen through procmon.exe or dependency walker and TF_LoadLibrary returns with status ok) but when trying to run the Graph afterwards the custom op is not recognized from Tensorflow and Tensorflow errors with \"Not found: Op type not registered...\". Trying to get the OpList afterwards with the Lib handle also returns no ops. So somehow the ops are not seen here even though they are recognized from python side. I tried a lot of different things to circumvent this like described in my Stack Overflow Question here:\nhttps://stackoverflow.com/questions/47309425/tensorflow-op-and-kernel-do-not-register-on-windows-with-cmake\nbut none of the approaches worked. It seems like that the op registration which is done after loading the DLL on python side is not properly done when performing the same operation with TF_LoadLibrary from C/C++. Since I cannot get tensorflow built with debug symbols I dont have a callstack where this registration fails unfortunately.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nI have a custom fork (https://github.com/stefanseibert/tensorflow/tree/r1.3) which is forked from r1.3 and the only modifications are some commented out lines to be able to build AVX support as described in another ticket, and the recent added fix for wide strings.\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nWindows 10, 64 bit, cmake\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nBuilt from source under windows with the cmake setup\r\n\r\n- **TensorFlow version (use command below)**:\r\n1.3.1\r\n\r\n- **Python version**: \r\n3.6\r\n\r\n- **Bazel version (if compiling from source)**:\r\nDoes not apply\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nMicrosoft (R) Build Engine version 14.0.25420.1\r\n\r\n- **CUDA/cuDNN version**:\r\nCUDA 8 / cuDNN 5.1`\r\n\r\n- **GPU model and memory**:\r\nGTX 980 Ti 6GB, 64GB main memory\r\n\r\n- **Exact command to reproduce**:\r\nDoes not apply\r\n\r\n### Describe the problem\r\nWhen using the cmake setup on windows to build Tensorflow from source and using AddUserOps which was added from @guschmue some time ago (example like https://gist.github.com/guschmue/2908e4411edc2faef6ddfe87a2ce4a1d), I am able to build GPU enabled tensorflow ops. I can load the DLL in python with tf.load_op_library() and can actually use it there. I can built GraphDefs with my custom Op and export it as protobuf file.\r\n\r\nWhen I try to use this graph for inference in another application where I use the C++ API its not possible for me to get the op loaded and registered. Loading the same DLL (as which with the python API succeeds) works on a system level with C/C++ (the DLL is loaded successfully as seen through procmon.exe or dependency walker and TF_LoadLibrary returns with status ok) but when trying to run the Graph afterwards the custom op is not recognized from Tensorflow and Tensorflow errors with \"Not found: Op type not registered...\". Trying to get the OpList afterwards with the Lib handle also returns no ops. So somehow the ops are not seen here even though they are recognized from python side. I tried a lot of different things to circumvent this like described in my Stack Overflow Question here:\r\n\r\nhttps://stackoverflow.com/questions/47309425/tensorflow-op-and-kernel-do-not-register-on-windows-with-cmake\r\n\r\nbut none of the approaches worked. It seems like that the op registration which is done after loading the DLL on python side is not properly done when performing the same operation with TF_LoadLibrary from C/C++. Since I cannot get tensorflow built with debug symbols I dont have a callstack where this registration fails unfortunately.\r\n"}