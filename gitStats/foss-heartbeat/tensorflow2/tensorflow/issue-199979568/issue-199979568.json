{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6779", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6779/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6779/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6779/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6779", "id": 199979568, "node_id": "MDU6SXNzdWUxOTk5Nzk1Njg=", "number": 6779, "title": "Training Slows Down -- But Speeds up If Entire Net is Reloaded", "user": {"login": "NickShahML", "id": 14891677, "node_id": "MDQ6VXNlcjE0ODkxNjc3", "avatar_url": "https://avatars2.githubusercontent.com/u/14891677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/NickShahML", "html_url": "https://github.com/NickShahML", "followers_url": "https://api.github.com/users/NickShahML/followers", "following_url": "https://api.github.com/users/NickShahML/following{/other_user}", "gists_url": "https://api.github.com/users/NickShahML/gists{/gist_id}", "starred_url": "https://api.github.com/users/NickShahML/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/NickShahML/subscriptions", "organizations_url": "https://api.github.com/users/NickShahML/orgs", "repos_url": "https://api.github.com/users/NickShahML/repos", "events_url": "https://api.github.com/users/NickShahML/events{/privacy}", "received_events_url": "https://api.github.com/users/NickShahML/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-01-11T01:22:27Z", "updated_at": "2017-01-11T18:20:36Z", "closed_at": "2017-01-11T18:20:36Z", "author_association": "NONE", "body_html": "<p>NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>There are a few issues but none that simulate the same problem.</p>\n<h3>Environment info</h3>\n<p>Operating System: Ubuntu 14.04, tf 0.12</p>\n<p>Installed version of CUDA and cuDNN: 8.0<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):</p>\n<p>If installed from binary pip package, provide: tensorflow pip gpu 0.12</p>\n<ol>\n<li>A link to the pip package you installed:</li>\n<li>The output from <code>python -c \"import tensorflow; print(tensorflow.__version__)\"</code>.</li>\n</ol>\n<h3>Problem</h3>\n<p>I am training a network, but after about 4000 backward passes, the training of the network starts to slow down. I have tried reloading the network after saving it. And again, 4000 steps later it starts to slow down. This means that the slow down is independent of what global step the training is on. The network size never changes, which is usually the problem with these slow downs. Average Step times are like this:</p>\n<p><code>2.4, 2.4, 2.4, 4.6, 3.9, 12.2, 4.2, 2.4, 6.7</code></p>\n<p>I am feeding the model with placeholders, so there is no FIFO Queue problems, and feeding it with text data so its pretty light. I have tested this on three separate machines and the same behavior is replicated throughout them. It is the actual tensorflow <code>session.run</code> command that slows it down tremendously.</p>\n<p>When I look at the gpu usage, both gpu's are at 0 percent the entire time (even during the <code>session.run</code> command). They spike up only when the actual forward and backward passes occur but remain dormant the rest of the time. I'm using allocator type 2 in the <code>tf.gradient</code> operation. This perhaps is the most revealing part.</p>\n<p>My <code>htop</code> indicates there's no swap memory problems at all.</p>\n<p>From what I have concluded the problem must be with tensorflow. The fact that You can reload the model and its completely fine for a little bit is really weird. I've also found that running the following command does not help:</p>\n<p><code>sync; echo 3 &gt; /proc/sys/vm/drop_caches</code></p>\n<pre><code>INFO:tensorflow:global step 2001 learning rate 0.0003882 step-time 2.94 perplexity 28.97 loss 3.3662\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\nINFO:tensorflow:16:01:09 01/10/17 EST\nINFO:tensorflow:global step 2251 learning rate 0.0003867 step-time 2.94 perplexity 27.17 loss 3.3022\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.920\nINFO:tensorflow:16:13:25 01/10/17 EST\nINFO:tensorflow:global step 2501 learning rate 0.0003853 step-time 2.94 perplexity 26.03 loss 3.2594\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\nINFO:tensorflow:16:25:48 01/10/17 EST\nINFO:tensorflow:global step 2751 learning rate 0.0003838 step-time 2.97 perplexity 24.80 loss 3.2109\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.956\nINFO:tensorflow:16:44:37 01/10/17 EST\nINFO:tensorflow:global step 3001 learning rate 0.0003824 step-time 4.52 perplexity 23.77 loss 3.1686\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 4.500\nINFO:tensorflow:16:58:05 01/10/17 EST\nINFO:tensorflow:global step 3251 learning rate 0.0003809 step-time 3.23 perplexity 23.10 loss 3.1396\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 3.216\nINFO:tensorflow:17:18:34 01/10/17 EST\nINFO:tensorflow:global step 3501 learning rate 0.0003795 step-time 4.92 perplexity 21.82 loss 3.0828\nINFO:tensorflow:get_batch_step_time 0.015 actual_step_time 4.902\n</code></pre>\n<p>Any help would be greatly appreciated!</p>", "body_text": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nThere are a few issues but none that simulate the same problem.\nEnvironment info\nOperating System: Ubuntu 14.04, tf 0.12\nInstalled version of CUDA and cuDNN: 8.0\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\nIf installed from binary pip package, provide: tensorflow pip gpu 0.12\n\nA link to the pip package you installed:\nThe output from python -c \"import tensorflow; print(tensorflow.__version__)\".\n\nProblem\nI am training a network, but after about 4000 backward passes, the training of the network starts to slow down. I have tried reloading the network after saving it. And again, 4000 steps later it starts to slow down. This means that the slow down is independent of what global step the training is on. The network size never changes, which is usually the problem with these slow downs. Average Step times are like this:\n2.4, 2.4, 2.4, 4.6, 3.9, 12.2, 4.2, 2.4, 6.7\nI am feeding the model with placeholders, so there is no FIFO Queue problems, and feeding it with text data so its pretty light. I have tested this on three separate machines and the same behavior is replicated throughout them. It is the actual tensorflow session.run command that slows it down tremendously.\nWhen I look at the gpu usage, both gpu's are at 0 percent the entire time (even during the session.run command). They spike up only when the actual forward and backward passes occur but remain dormant the rest of the time. I'm using allocator type 2 in the tf.gradient operation. This perhaps is the most revealing part.\nMy htop indicates there's no swap memory problems at all.\nFrom what I have concluded the problem must be with tensorflow. The fact that You can reload the model and its completely fine for a little bit is really weird. I've also found that running the following command does not help:\nsync; echo 3 > /proc/sys/vm/drop_caches\nINFO:tensorflow:global step 2001 learning rate 0.0003882 step-time 2.94 perplexity 28.97 loss 3.3662\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\nINFO:tensorflow:16:01:09 01/10/17 EST\nINFO:tensorflow:global step 2251 learning rate 0.0003867 step-time 2.94 perplexity 27.17 loss 3.3022\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.920\nINFO:tensorflow:16:13:25 01/10/17 EST\nINFO:tensorflow:global step 2501 learning rate 0.0003853 step-time 2.94 perplexity 26.03 loss 3.2594\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\nINFO:tensorflow:16:25:48 01/10/17 EST\nINFO:tensorflow:global step 2751 learning rate 0.0003838 step-time 2.97 perplexity 24.80 loss 3.2109\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.956\nINFO:tensorflow:16:44:37 01/10/17 EST\nINFO:tensorflow:global step 3001 learning rate 0.0003824 step-time 4.52 perplexity 23.77 loss 3.1686\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 4.500\nINFO:tensorflow:16:58:05 01/10/17 EST\nINFO:tensorflow:global step 3251 learning rate 0.0003809 step-time 3.23 perplexity 23.10 loss 3.1396\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 3.216\nINFO:tensorflow:17:18:34 01/10/17 EST\nINFO:tensorflow:global step 3501 learning rate 0.0003795 step-time 4.92 perplexity 21.82 loss 3.0828\nINFO:tensorflow:get_batch_step_time 0.015 actual_step_time 4.902\n\nAny help would be greatly appreciated!", "body": "NOTE: Only file GitHub issues for bugs and feature requests.  All other topics will be closed.\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\n\r\nThere are a few issues but none that simulate the same problem.\r\n\r\n### Environment info\r\nOperating System: Ubuntu 14.04, tf 0.12\r\n\r\nInstalled version of CUDA and cuDNN: 8.0\r\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\r\n\r\nIf installed from binary pip package, provide: tensorflow pip gpu 0.12\r\n\r\n1. A link to the pip package you installed:\r\n2. The output from `python -c \"import tensorflow; print(tensorflow.__version__)\"`.\r\n\r\n### Problem\r\n\r\nI am training a network, but after about 4000 backward passes, the training of the network starts to slow down. I have tried reloading the network after saving it. And again, 4000 steps later it starts to slow down. This means that the slow down is independent of what global step the training is on. The network size never changes, which is usually the problem with these slow downs. Average Step times are like this:\r\n\r\n`2.4, 2.4, 2.4, 4.6, 3.9, 12.2, 4.2, 2.4, 6.7`\r\n\r\nI am feeding the model with placeholders, so there is no FIFO Queue problems, and feeding it with text data so its pretty light. I have tested this on three separate machines and the same behavior is replicated throughout them. It is the actual tensorflow `session.run` command that slows it down tremendously.\r\n\r\nWhen I look at the gpu usage, both gpu's are at 0 percent the entire time (even during the `session.run` command). They spike up only when the actual forward and backward passes occur but remain dormant the rest of the time. I'm using allocator type 2 in the `tf.gradient` operation. This perhaps is the most revealing part.\r\n\r\nMy `htop` indicates there's no swap memory problems at all. \r\n\r\nFrom what I have concluded the problem must be with tensorflow. The fact that You can reload the model and its completely fine for a little bit is really weird. I've also found that running the following command does not help:\r\n\r\n`sync; echo 3 > /proc/sys/vm/drop_caches`\r\n\r\n```\r\nINFO:tensorflow:global step 2001 learning rate 0.0003882 step-time 2.94 perplexity 28.97 loss 3.3662\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\r\nINFO:tensorflow:16:01:09 01/10/17 EST\r\nINFO:tensorflow:global step 2251 learning rate 0.0003867 step-time 2.94 perplexity 27.17 loss 3.3022\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.920\r\nINFO:tensorflow:16:13:25 01/10/17 EST\r\nINFO:tensorflow:global step 2501 learning rate 0.0003853 step-time 2.94 perplexity 26.03 loss 3.2594\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.929\r\nINFO:tensorflow:16:25:48 01/10/17 EST\r\nINFO:tensorflow:global step 2751 learning rate 0.0003838 step-time 2.97 perplexity 24.80 loss 3.2109\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 2.956\r\nINFO:tensorflow:16:44:37 01/10/17 EST\r\nINFO:tensorflow:global step 3001 learning rate 0.0003824 step-time 4.52 perplexity 23.77 loss 3.1686\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 4.500\r\nINFO:tensorflow:16:58:05 01/10/17 EST\r\nINFO:tensorflow:global step 3251 learning rate 0.0003809 step-time 3.23 perplexity 23.10 loss 3.1396\r\nINFO:tensorflow:get_batch_step_time 0.016 actual_step_time 3.216\r\nINFO:tensorflow:17:18:34 01/10/17 EST\r\nINFO:tensorflow:global step 3501 learning rate 0.0003795 step-time 4.92 perplexity 21.82 loss 3.0828\r\nINFO:tensorflow:get_batch_step_time 0.015 actual_step_time 4.902\r\n```\r\n\r\nAny help would be greatly appreciated!\r\n"}