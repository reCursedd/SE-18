{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111259193", "pull_request_review_id": 32492732, "id": 111259193, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMTI1OTE5Mw==", "diff_hunk": "@@ -0,0 +1,775 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// LRN = Local Response Normalization\n+// See docs in ../ops/nn_ops.cc. This opkernel uses MKL library, create MKL\n+// layout and primitives, use MKL dnn primitives to compute local\n+// response normalization\n+\n+#ifdef INTEL_MKL\n+\n+#define EIGEN_USE_THREADS\n+#include <vector>\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/kernels/ops_util.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+#include \"tensorflow/core/util/tensor_format.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"third_party/mkl/include/mkl_dnn.h\"\n+#include \"third_party/mkl/include/mkl_dnn_types.h\"\n+\n+#if !defined(IS_MOBILE_PLATFORM)\n+#include \"tensorflow/core/util/work_sharder.h\"\n+#endif\n+\n+namespace tensorflow {\n+\n+namespace {\n+// Create a depth-by-depth band matrix with 1s along a swath of size (2 *\n+// depth_radius + 1) around the diagonal.\n+template <typename T>\n+void GetBandMatrix(int depth, int depth_radius,\n+                   Eigen::Tensor<T, 2, Eigen::RowMajor>* result) {\n+  result->setZero();\n+  for (int row = 0; row < depth; ++row) {\n+    const int begin = std::max<int>(0, row - depth_radius);\n+    const int end = std::min<int>(depth, row + depth_radius + 1);\n+    Eigen::DSizes<Eigen::DenseIndex, 2> start(row, begin);\n+    Eigen::DSizes<Eigen::DenseIndex, 2> sizes(1, end - begin);\n+    result->slice(start, sizes).setConstant(T(1));\n+  }\n+}\n+\n+}  // namespace\n+\n+template <typename T>\n+class MklLRNOp : public OpKernel {\n+ public:\n+  ~MklLRNOp() {}\n+\n+  explicit MklLRNOp(OpKernelConstruction* context) : OpKernel(context) {\n+    int64 depth_radius64;\n+    OP_REQUIRES_OK(context, context->GetAttr(\"depth_radius\", &depth_radius64));\n+    OP_REQUIRES(context, FastBoundsCheck(depth_radius64,\n+                                         std::numeric_limits<int>::max()),\n+                errors::InvalidArgument(\"depth_radius = \", depth_radius64,\n+                                        \" larger than int max\"));\n+    depth_radius_ = static_cast<size_t>(depth_radius64);\n+\n+    OP_REQUIRES_OK(context, context->GetAttr(\"bias\", &bias_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"alpha\", &alpha_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"beta\", &beta_));\n+    workspace_enabled_ = false;\n+    context->GetAttr(\"workspace_enabled\", &workspace_enabled_);\n+  }\n+\n+  void Compute(OpKernelContext* context) override {\n+    MklLRNOpContext mkl_context;\n+\n+    const Tensor& input = MklGetInput(context, 0);\n+    GetMklShape(context, 0, &mkl_context.input_shape);\n+    bool input_in_mkl_format = mkl_context.input_shape.IsMklTensor();\n+\n+    // Sanity checks\n+    mkl_context.in_dims = input_in_mkl_format\n+                              ? mkl_context.input_shape.GetDimension()\n+                              : input.dims();\n+    OP_REQUIRES(context, mkl_context.in_dims == 4,\n+                errors::InvalidArgument(\"input must be 4-dimensional\"));\n+    OP_REQUIRES(context, FastBoundsCheck(input.NumElements(),\n+                                         std::numeric_limits<int>::max()),\n+                errors::InvalidArgument(\"argument to LRN too large\"));\n+\n+    if (!input_in_mkl_format) {\n+      mkl_context.MklDefaultToEigen(context, depth_radius_, bias_, alpha_,\n+                                    beta_, input);\n+      return;\n+    }\n+\n+    if (input_in_mkl_format) {\n+      // MKL supports normalization over channel dimension only\n+      if (mkl_context.input_shape.tf_dim_idx(mkl_context.in_dims - 1) ==\n+          MklDims::C) {\n+        mkl_context.lt_input =\n+            static_cast<dnnLayout_t>(mkl_context.input_shape.GetCurLayout());\n+        workspace_enabled_ = true;\n+        /* std::cout<<\" the mkl lrn is enabled!!!!!!!!!!\"<<std::endl; */\n+      } else {\n+        /* std::vector<const Tensor *> temp_input(1, &input);\n+        std::vector< MklShape> mkl_input_shapes(1, mkl_context.input_shape);\n+        std::vector<Tensor> temp_out = ConvertMklToTF<T>(context,\n+                                                      temp_input,\n+                                                      mkl_input_shapes); */\n+        mkl_context.MklDefaultToEigen(context, depth_radius_, bias_, alpha_,\n+                                      beta_, input);\n+        return;\n+      }\n+    }\n+\n+    int kernel_size = 2 * depth_radius_ + 1;\n+\n+    CHECK_EQ(dnnLRNCreateForward_F32(&mkl_context.lrn_fwd, NULL,\n+                                     mkl_context.lt_input, kernel_size,\n+                                     static_cast<float>(alpha_ * kernel_size),\n+                                     beta_, bias_),\n+             E_SUCCESS);\n+\n+    // Allocate output tensor and shape\n+    Tensor* output = nullptr;\n+    Tensor* workspace = nullptr;\n+\n+    // Convert Inputs if needed\n+    Tensor mkl_tmp_input_buf_tensor;\n+    mkl_context.MklPrepareLRNInputs(context, &mkl_tmp_input_buf_tensor);\n+\n+    // Allocate Layer Outputs\n+    mkl_context.MklAllocateOutputs(context, &output, &workspace,\n+                                   workspace_enabled_);\n+\n+    Tensor mkl_tmp_workspace_buf_tensor;\n+    mkl_context.MklPrepareLRNOutputs(context, output, workspace,\n+                                     &mkl_tmp_workspace_buf_tensor,\n+                                     workspace_enabled_);\n+\n+    // Execute LRN.\n+    CHECK_EQ(dnnExecute_F32(mkl_context.lrn_fwd, mkl_context.lrn_res),\n+             E_SUCCESS);\n+\n+    // Release MKL resources.\n+    mkl_context.MklCleanup();\n+  }\n+\n+ private:\n+  typedef struct {\n+    size_t in_dims;\n+    size_t in_sizes[4];\n+    size_t in_strides[4];\n+    size_t out_sizes[4];\n+    size_t out_strides[4];\n+    MklShape input_shape;\n+    dnnPrimitive_t lrn_fwd = nullptr;\n+    dnnPrimitive_t convert_input = nullptr;\n+    /* dnnPrimitive_t convert_output; */\n+    dnnLayout_t lt_input = nullptr;\n+    /* dnnLayout_t lt_output; */\n+    dnnLayout_t lt_internal_input = nullptr;\n+    dnnLayout_t lt_internal_workspace = nullptr;\n+    dnnLayout_t lt_internal_output = nullptr;\n+    void* lrn_res[dnnResourceNumber];\n+\n+    // Convert Inputs if needed\n+    void MklPrepareLRNInputs(OpKernelContext* context,\n+                             Tensor* mkl_tmp_input_buf_tensor) {\n+      const Tensor& input = MklGetInput(context, 0);\n+      void* mkl_buf_input =\n+          const_cast<void*>(static_cast<const void*>(input.flat<T>().data()));\n+\n+      CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(&lt_internal_input, lrn_fwd,\n+                                                dnnResourceSrc),\n+               E_SUCCESS);\n+\n+      void* mkl_buf_convert_input = nullptr;\n+      bool mkl_convert_input = false;\n+      mkl_convert_input = !dnnLayoutCompare_F32(lt_internal_input, lt_input);\n+\n+      if (mkl_convert_input) {\n+        CHECK_EQ(dnnConversionCreate_F32(&convert_input, lt_input,\n+                                         lt_internal_input),\n+                 E_SUCCESS);\n+        AllocTmpBuffer(context, mkl_tmp_input_buf_tensor, lt_internal_input,\n+                       &mkl_buf_convert_input);\n+        CHECK_EQ(dnnConversionExecute_F32(convert_input, mkl_buf_input,\n+                                          mkl_buf_convert_input),\n+                 E_SUCCESS);\n+        dnnDelete_F32(convert_input);\n+      }\n+\n+      lrn_res[dnnResourceSrc] =\n+          (mkl_convert_input) ? mkl_buf_convert_input : mkl_buf_input;\n+    }\n+\n+    // Allocate Layer Outputs\n+    void MklAllocateOutputs(OpKernelContext* context, Tensor** output,\n+                            Tensor** workspace, bool workspace_enabled_) {\n+      TensorShape mkl_output_tf_shape; /* First tensor */\n+      MklShape mkl_output_mkl_shape;   /* Second tensor */\n+\n+      mkl_output_mkl_shape.SetMklTensor(true);\n+      mkl_output_mkl_shape.SetMklLayout(lrn_fwd, dnnResourceDst);\n+      mkl_output_mkl_shape.SetTfLayout(in_dims, input_shape.GetSizes(),\n+                                       input_shape.GetStrides());\n+      mkl_output_mkl_shape.SetTfDimOrder(in_dims,\n+                                         input_shape.GetTfToMklDimMap());\n+      mkl_output_tf_shape.AddDim(\n+          dnnLayoutGetMemorySize_F32(\n+              static_cast<dnnLayout_t>(mkl_output_mkl_shape.GetMklLayout())) /\n+          sizeof(T));\n+      AllocateOutputSetMklShape(context, 0, output,\n+                                mkl_output_tf_shape /* First tensor */,\n+                                mkl_output_mkl_shape /* Second Tensor */);\n+\n+      if (workspace_enabled_) {\n+        TensorShape mkl_workspace_tf_shape; /* First tensor */\n+        MklShape mkl_workspace_mkl_shape;   /* Second tensor */\n+        mkl_workspace_mkl_shape.SetMklTensor(false);\n+        mkl_workspace_mkl_shape.SetMklLayout(lrn_fwd, dnnResourceWorkspace);\n+        // Assumes workspace has same TF layout and TF dim order as input\n+        mkl_workspace_mkl_shape.SetTfLayout(in_dims, input_shape.GetSizes(),\n+                                            input_shape.GetStrides());\n+        mkl_workspace_mkl_shape.SetTfDimOrder(in_dims,\n+                                              input_shape.GetTfToMklDimMap());\n+        mkl_workspace_tf_shape.AddDim(\n+            dnnLayoutGetMemorySize_F32(static_cast<dnnLayout_t>(\n+                mkl_workspace_mkl_shape.GetMklLayout())) /\n+            sizeof(T));\n+        AllocateOutputSetMklShape(context, 1, workspace,\n+                                  mkl_workspace_tf_shape /* First tensor */,\n+                                  mkl_workspace_mkl_shape /* Second Tensor */);\n+      }\n+    }\n+\n+    void MklPrepareLRNOutputs(OpKernelContext* context, Tensor* output,\n+                              Tensor* workspace,\n+                              Tensor* mkl_tmp_workspace_buf_tensor,\n+                              bool workspace_enabled_) {\n+      CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(&lt_internal_workspace, lrn_fwd,\n+                                                dnnResourceWorkspace),\n+               E_SUCCESS);\n+\n+      CHECK_EQ(dnnLayoutCreateFromPrimitive_F32(&lt_internal_output, lrn_fwd,\n+                                                dnnResourceDst),\n+               E_SUCCESS);\n+\n+      void* mkl_buf_output =\n+          const_cast<void*>(static_cast<const void*>(output->flat<T>().data()));\n+      lrn_res[dnnResourceDst] = mkl_buf_output;\n+\n+      void* mkl_buf_workspace = nullptr;\n+      if (workspace_enabled_) {\n+        mkl_buf_workspace = const_cast<void*>(\n+            static_cast<const void*>(workspace->flat<T>().data()));\n+      } else {\n+        AllocTmpBuffer(context, mkl_tmp_workspace_buf_tensor,\n+                       lt_internal_workspace, &mkl_buf_workspace);\n+      }\n+      lrn_res[dnnResourceWorkspace] = mkl_buf_workspace;\n+    }\n+\n+    // Fallback implementation - Taken from lrn_op.cc", "path": "tensorflow/core/kernels/mkl_lrn_op.cc", "position": 267, "original_position": 274, "commit_id": "b5ef5bfcb39a0ba0cef4e1f7e9d766344f918ab2", "original_commit_id": "67f9925ef9ceed02892c200a3122092ab497943a", "user": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "body": "Could you make lrn_op.h and expose this function, so that we could reuse this code instead of making a copy?", "created_at": "2017-04-12T21:00:33Z", "updated_at": "2017-04-13T23:37:54Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8968#discussion_r111259193", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8968", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111259193"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8968#discussion_r111259193"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8968"}}, "body_html": "<p>Could you make lrn_op.h and expose this function, so that we could reuse this code instead of making a copy?</p>", "body_text": "Could you make lrn_op.h and expose this function, so that we could reuse this code instead of making a copy?"}