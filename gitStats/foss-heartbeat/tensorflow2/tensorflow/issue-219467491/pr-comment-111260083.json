{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111260083", "pull_request_review_id": 32493716, "id": 111260083, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDExMTI2MDA4Mw==", "diff_hunk": "@@ -0,0 +1,455 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+#ifdef INTEL_MKL\n+\n+#include <limits>\n+#include <vector>\n+\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor.h\"\n+#include \"tensorflow/core/framework/tensor_types.h\"\n+#include \"tensorflow/core/framework/types.h\"\n+#include \"tensorflow/core/kernels/bounds_check.h\"\n+#include \"tensorflow/core/kernels/concat_lib.h\"\n+#include \"tensorflow/core/lib/core/status.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+#include \"third_party/mkl/include/mkl_dnn_types.h\"\n+#include \"third_party/mkl/include/mkl_dnn.h\"\n+#include \"tensorflow/core/util/mkl_util.h\"\n+\n+namespace tensorflow {\n+typedef Eigen::ThreadPoolDevice CPUDevice;\n+\n+enum AxisArgumentName { NAME_IS_AXIS, NAME_IS_CONCAT_DIM };\n+\n+// --------------------------------------------------------------------------\n+//                      Eigen Concat Op\n+// --------------------------------------------------------------------------\n+template <typename Device, typename T, AxisArgumentName AxisArgName>\n+class EigenConcatBaseOp : public OpKernel {\n+ public:\n+  typedef std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>\n+      ConstMatrixVector;\n+\n+  explicit EigenConcatBaseOp(OpKernelConstruction* c) : OpKernel(c) {}\n+\n+  // Although, we modify Compute for this call to accept one extra param,\n+  // we need to have empty Compute because Compute is pure virtual function.\n+  void Compute(OpKernelContext* c) {}\n+\n+  void Compute(OpKernelContext* c, const std::vector<Tensor>& values) {\n+    const Tensor* concat_dim_tensor;\n+    const char* axis_attribute_name =\n+        AxisArgName == NAME_IS_AXIS\n+            ? \"axis\"\n+            : AxisArgName == NAME_IS_CONCAT_DIM ? \"concat_dim\" : \"<invalid>\";\n+    OP_REQUIRES_OK(c, c->input(axis_attribute_name, &concat_dim_tensor));\n+    OP_REQUIRES(c, IsLegacyScalar(concat_dim_tensor->shape()),\n+                errors::InvalidArgument(\n+                    axis_attribute_name,\n+                    \" tensor should be a scalar integer, but got shape \",\n+                    concat_dim_tensor->shape().DebugString()));\n+    const int32 concat_dim =\n+        internal::SubtleMustCopy(concat_dim_tensor->scalar<int32>()());\n+    // Instead of accessing values from context, we use input to Compute.\n+    const int N = values.size();\n+    const int input_dims = values[0].dims();\n+    const TensorShape& input_shape = values[0].shape();\n+\n+    int32 axis = concat_dim < 0 ? concat_dim + input_dims : concat_dim;\n+    OP_REQUIRES(c, (0 <= axis && axis < input_dims) ||\n+                       (allow_legacy_scalars() && concat_dim == 0),\n+                errors::InvalidArgument(\n+                    \"ConcatOp : Expected concatenating dimensions in the range \"\n+                    \"[\",\n+                    -input_dims, \", \", input_dims, \"), but got \", concat_dim));\n+    // Note that we reduce the concat of n-dimensional tensors into a two\n+    // dimensional concat. Assuming the dimensions of any input/output\n+    // tensor are {x0, x1,...,xn-1, y0, y1,...,ym-1}, where the concat is along\n+    // the dimension indicated with size y0, we flatten it to {x, y}, where y =\n+    // Prod_i(yi) and x = ((n > 0) ? Prod_i(xi) : 1).\n+    ConstMatrixVector inputs_flat;\n+    inputs_flat.reserve(N);\n+    int64 inputs_flat_dim0 = 1;\n+    for (int d = 0; d < axis; ++d) {\n+      inputs_flat_dim0 *= input_shape.dim_size(d);\n+    }\n+    int64 output_concat_dim = 0;\n+    const bool input_is_scalar = IsLegacyScalar(input_shape);\n+    for (int i = 0; i < N; ++i) {\n+      const auto in = values[i];\n+      const bool in_is_scalar = IsLegacyScalar(in.shape());\n+      OP_REQUIRES(\n+          c, in.dims() == input_dims || (input_is_scalar && in_is_scalar),\n+          errors::InvalidArgument(\n+              \"ConcatOp : Ranks of all input tensors should match: shape[0] = \",\n+              input_shape.DebugString(), \" vs. shape[\", i, \"] = \",\n+              in.shape().DebugString()));\n+      for (int j = 0; j < input_dims; ++j) {\n+        if (j == axis) {\n+          continue;\n+        }\n+        OP_REQUIRES(\n+            c, in.dim_size(j) == input_shape.dim_size(j),\n+            errors::InvalidArgument(\n+                \"ConcatOp : Dimensions of inputs should match: shape[0] = \",\n+                input_shape.DebugString(), \" vs. shape[\", i, \"] = \",\n+                in.shape().DebugString()));\n+      }\n+      if (in.NumElements() > 0) {\n+        int64 inputs_flat_dim1 = in.NumElements() / inputs_flat_dim0;\n+        inputs_flat.emplace_back(new typename TTypes<T, 2>::ConstMatrix(\n+            in.shaped<T, 2>({inputs_flat_dim0, inputs_flat_dim1})));\n+      }\n+      // TODO(irving): Remove check once !allow_legacy_scalars().\n+      output_concat_dim += in.dims() > 0 ? in.dim_size(axis) : 1;\n+    }\n+\n+    TensorShape output_shape(input_shape);\n+    // TODO(irving): Remove rank 0 case once !allow_legacy_scalars().\n+    if (output_shape.dims() == 0) {\n+      output_shape.AddDim(output_concat_dim);\n+    } else {\n+      output_shape.set_dim(axis, output_concat_dim);\n+    }\n+    Tensor* output = nullptr;\n+    OP_REQUIRES_OK(c, c->allocate_output(0, output_shape, &output));\n+    if (output->NumElements() > 0) {\n+      int64 output_dim1 = output->NumElements() / inputs_flat_dim0;\n+      auto output_flat = output->shaped<T, 2>({inputs_flat_dim0, output_dim1});\n+      ConcatCPU<T>(c->device(), inputs_flat, &output_flat);\n+    }\n+  }\n+};\n+\n+// --------------------------------------------------------------------------\n+//                      Mkl Concat Op\n+// --------------------------------------------------------------------------\n+\n+template <typename Device, typename T, AxisArgumentName AxisArgName>\n+class MklConcatOp : public OpKernel {\n+ private:\n+  TensorFormat data_format_;\n+  EigenConcatBaseOp<Device, T, AxisArgName> eigen_concat_op_;\n+\n+ public:\n+  typedef std::vector<std::unique_ptr<typename TTypes<T, 2>::ConstMatrix>>\n+    ConstMatrixVector;\n+\n+  explicit MklConcatOp(OpKernelConstruction* c) : OpKernel(c),\n+                                                  eigen_concat_op_(c) {}\n+\n+  void Compute(OpKernelContext* context) override {\n+    MklConcatOpContext mkl_context;\n+\n+    // Get input tensors.\n+    OpInputList input_tensors;\n+    GetMklInputList(context, \"values\", &input_tensors);\n+    const int N = input_tensors.size();\n+    // Get MKL shapes.\n+    MklShapeList input_shapes(N);\n+    GetMklShapeList(context, \"values\", &input_shapes);\n+\n+    // If this is Concat, then concat_dim is 0th input.\n+    // If this is ConcatV2, then axis is Nth input.\n+    const Tensor& concat_dim_tensor = AxisArgName == NAME_IS_CONCAT_DIM ?\n+                                      MklGetInput(context, 0) :\n+                                      MklGetInput(context, N);\n+\n+    // Sanity checks\n+    OP_REQUIRES(\n+      context, IsLegacyScalar(concat_dim_tensor.shape()),\n+      errors::InvalidArgument(\n+        \"Concat dim tensor should be a scalar integer, but got shape \",\n+        concat_dim_tensor.shape().DebugString()));\n+    int32 concat_dim =\n+      internal::SubtleMustCopy(concat_dim_tensor.scalar<int32>()());\n+\n+    MklShape& inpshape0 = input_shapes[0];\n+\n+    // Check that all tensors are Mkl, if not we call Eigen version.\n+    bool invoke_eigen = false;\n+    bool is_concat_dim_channel = true;\n+    if (!AreAllMklTensors(input_shapes)) {\n+      invoke_eigen = true;\n+    }\n+\n+    // Check that total number of dimensions is 4, if not call Eigen.\n+    if (!invoke_eigen) {", "path": "tensorflow/core/kernels/mkl_concat_op.cc", "position": 196, "original_position": 194, "commit_id": "b5ef5bfcb39a0ba0cef4e1f7e9d766344f918ab2", "original_commit_id": "67f9925ef9ceed02892c200a3122092ab497943a", "user": {"login": "nhasabni", "id": 22304502, "node_id": "MDQ6VXNlcjIyMzA0NTAy", "avatar_url": "https://avatars0.githubusercontent.com/u/22304502?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nhasabni", "html_url": "https://github.com/nhasabni", "followers_url": "https://api.github.com/users/nhasabni/followers", "following_url": "https://api.github.com/users/nhasabni/following{/other_user}", "gists_url": "https://api.github.com/users/nhasabni/gists{/gist_id}", "starred_url": "https://api.github.com/users/nhasabni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nhasabni/subscriptions", "organizations_url": "https://api.github.com/users/nhasabni/orgs", "repos_url": "https://api.github.com/users/nhasabni/repos", "events_url": "https://api.github.com/users/nhasabni/events{/privacy}", "received_events_url": "https://api.github.com/users/nhasabni/received_events", "type": "User", "site_admin": false}, "body": "No, the decision cannot be made in graph rewrite pass. We cannot find out concat_dim in graph rewrite. Also, we do not know if input tensors are in Mkl layout or Tensorflow layout in graph rewrite.", "created_at": "2017-04-12T21:04:40Z", "updated_at": "2017-04-13T23:37:54Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/8968#discussion_r111260083", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8968", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/111260083"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/8968#discussion_r111260083"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/8968"}}, "body_html": "<p>No, the decision cannot be made in graph rewrite pass. We cannot find out concat_dim in graph rewrite. Also, we do not know if input tensors are in Mkl layout or Tensorflow layout in graph rewrite.</p>", "body_text": "No, the decision cannot be made in graph rewrite pass. We cannot find out concat_dim in graph rewrite. Also, we do not know if input tensors are in Mkl layout or Tensorflow layout in graph rewrite.", "in_reply_to_id": 111256532}