{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22532", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22532/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22532/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/22532/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/22532", "id": 364005672, "node_id": "MDU6SXNzdWUzNjQwMDU2NzI=", "number": 22532, "title": "Tensorflow quantization on Windows", "user": {"login": "monInspiration", "id": 36672680, "node_id": "MDQ6VXNlcjM2NjcyNjgw", "avatar_url": "https://avatars3.githubusercontent.com/u/36672680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/monInspiration", "html_url": "https://github.com/monInspiration", "followers_url": "https://api.github.com/users/monInspiration/followers", "following_url": "https://api.github.com/users/monInspiration/following{/other_user}", "gists_url": "https://api.github.com/users/monInspiration/gists{/gist_id}", "starred_url": "https://api.github.com/users/monInspiration/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/monInspiration/subscriptions", "organizations_url": "https://api.github.com/users/monInspiration/orgs", "repos_url": "https://api.github.com/users/monInspiration/repos", "events_url": "https://api.github.com/users/monInspiration/events{/privacy}", "received_events_url": "https://api.github.com/users/monInspiration/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "wt-huang", "id": 42785337, "node_id": "MDQ6VXNlcjQyNzg1MzM3", "avatar_url": "https://avatars0.githubusercontent.com/u/42785337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wt-huang", "html_url": "https://github.com/wt-huang", "followers_url": "https://api.github.com/users/wt-huang/followers", "following_url": "https://api.github.com/users/wt-huang/following{/other_user}", "gists_url": "https://api.github.com/users/wt-huang/gists{/gist_id}", "starred_url": "https://api.github.com/users/wt-huang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wt-huang/subscriptions", "organizations_url": "https://api.github.com/users/wt-huang/orgs", "repos_url": "https://api.github.com/users/wt-huang/repos", "events_url": "https://api.github.com/users/wt-huang/events{/privacy}", "received_events_url": "https://api.github.com/users/wt-huang/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2018-09-26T12:34:03Z", "updated_at": "2018-09-28T02:08:03Z", "closed_at": "2018-09-28T02:08:03Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: no</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04, Windows 10</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: -</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.10.1 CPU</li>\n<li><strong>Python version</strong>: 3.6</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I've freezed my model and got .pb file. Then I've quantize my model using tocoConverter on Linux, as it's not supported on Windows. I've got quantized_model.tflite. I can load it and get predictions on Linux, but I have issues to make it on Windows, as my project requires. I've tried to load it using tf.contrib.lite.Interpreter using this code:<br>\n`import numpy as np<br>\nimport tensorflow as tf</p>\n<p>interpreter=tf.contrib.lite.Interpreter(model_path=\"quantized_model.tflite\")<br>\ninterpreter.allocate_tensors()</p>\n<p>input_details = interpreter.get_input_details()<br>\noutput_details = interpreter.get_output_details()</p>\n<p>input_shape = input_details[0]['shape']</p>\n<p>input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)<br>\ninterpreter.set_tensor(input_details[0]['index'],input_data)</p>\n<p>interpreter.invoke()<br>\noutput_data = interpreter.get_tensor(output_details[0]['index'])<br>\nprint(output_data)</p>\n<p><em>//ImportError: No module named 'tensorflow.contrib.lite.python.Interpreter</em>`</p>\n<p>But it failed with \"No module named 'tensorflow.contrib.lite.python.interpreter\" error. I always get this errors on Windows, when trying to use something from tf.contrib.lite. Maybe there is a way to load this on Windows? Or can you advice alternative options to quantize a model on Windows?</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): no\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04, Windows 10\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: -\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): 1.10.1 CPU\nPython version: 3.6\n\nDescribe the problem\nI've freezed my model and got .pb file. Then I've quantize my model using tocoConverter on Linux, as it's not supported on Windows. I've got quantized_model.tflite. I can load it and get predictions on Linux, but I have issues to make it on Windows, as my project requires. I've tried to load it using tf.contrib.lite.Interpreter using this code:\n`import numpy as np\nimport tensorflow as tf\ninterpreter=tf.contrib.lite.Interpreter(model_path=\"quantized_model.tflite\")\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\ninput_shape = input_details[0]['shape']\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\ninterpreter.set_tensor(input_details[0]['index'],input_data)\ninterpreter.invoke()\noutput_data = interpreter.get_tensor(output_details[0]['index'])\nprint(output_data)\n//ImportError: No module named 'tensorflow.contrib.lite.python.Interpreter`\nBut it failed with \"No module named 'tensorflow.contrib.lite.python.interpreter\" error. I always get this errors on Windows, when trying to use something from tf.contrib.lite. Maybe there is a way to load this on Windows? Or can you advice alternative options to quantize a model on Windows?", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: no\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04, Windows 10\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: -\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: 1.10.1 CPU\r\n- **Python version**: 3.6\r\n\r\n### Describe the problem\r\nI've freezed my model and got .pb file. Then I've quantize my model using tocoConverter on Linux, as it's not supported on Windows. I've got quantized_model.tflite. I can load it and get predictions on Linux, but I have issues to make it on Windows, as my project requires. I've tried to load it using tf.contrib.lite.Interpreter using this code:\r\n`import numpy as np\r\nimport tensorflow as tf\r\n\r\ninterpreter=tf.contrib.lite.Interpreter(model_path=\"quantized_model.tflite\")\r\ninterpreter.allocate_tensors()\r\n\r\ninput_details = interpreter.get_input_details()\r\noutput_details = interpreter.get_output_details()\r\n\r\ninput_shape = input_details[0]['shape']\r\n\r\ninput_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\r\ninterpreter.set_tensor(input_details[0]['index'],input_data)\r\n\r\ninterpreter.invoke()\r\noutput_data = interpreter.get_tensor(output_details[0]['index'])\r\nprint(output_data)\r\n\r\n*//ImportError: No module named 'tensorflow.contrib.lite.python.Interpreter*`\r\n\r\nBut it failed with \"No module named 'tensorflow.contrib.lite.python.interpreter\" error. I always get this errors on Windows, when trying to use something from tf.contrib.lite. Maybe there is a way to load this on Windows? Or can you advice alternative options to quantize a model on Windows?\r\n"}