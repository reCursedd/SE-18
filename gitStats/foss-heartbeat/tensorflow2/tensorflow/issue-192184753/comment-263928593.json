{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/263928593", "html_url": "https://github.com/tensorflow/tensorflow/issues/5922#issuecomment-263928593", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5922", "id": 263928593, "node_id": "MDEyOklzc3VlQ29tbWVudDI2MzkyODU5Mw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-30T16:55:31Z", "updated_at": "2016-11-30T16:55:31Z", "author_association": "MEMBER", "body_html": "<p>Those are some critical details.  By reducing the embedding size, you reduced its ability to distinguish between words dramatically.  Also, you may not have trained long enough.  The tutorial suggests you should train for a full epoch (340,000 steps at batch 64) before expecting the model to work...with the original embedding size.  Nonetheless, the perplexity scores suggest that it learned something.</p>\n<p>One rationality test would be to retry queries from your eval set on the inference version you say cannot translate basic terms.  Do you get the same overall perplexity score as at the end of training?  If not, if its much worse, then you've somehow set up the inference model incorrectly.  If so, then either your sample test queries are not good (try full sentences, not single words), or the model isn't capable enough or trained long enough.</p>\n<p>I'm not an expert on translation models.  Perhaps someone else reading this thread has more insight.</p>", "body_text": "Those are some critical details.  By reducing the embedding size, you reduced its ability to distinguish between words dramatically.  Also, you may not have trained long enough.  The tutorial suggests you should train for a full epoch (340,000 steps at batch 64) before expecting the model to work...with the original embedding size.  Nonetheless, the perplexity scores suggest that it learned something.\nOne rationality test would be to retry queries from your eval set on the inference version you say cannot translate basic terms.  Do you get the same overall perplexity score as at the end of training?  If not, if its much worse, then you've somehow set up the inference model incorrectly.  If so, then either your sample test queries are not good (try full sentences, not single words), or the model isn't capable enough or trained long enough.\nI'm not an expert on translation models.  Perhaps someone else reading this thread has more insight.", "body": "Those are some critical details.  By reducing the embedding size, you reduced its ability to distinguish between words dramatically.  Also, you may not have trained long enough.  The tutorial suggests you should train for a full epoch (340,000 steps at batch 64) before expecting the model to work...with the original embedding size.  Nonetheless, the perplexity scores suggest that it learned something. \r\n\r\nOne rationality test would be to retry queries from your eval set on the inference version you say cannot translate basic terms.  Do you get the same overall perplexity score as at the end of training?  If not, if its much worse, then you've somehow set up the inference model incorrectly.  If so, then either your sample test queries are not good (try full sentences, not single words), or the model isn't capable enough or trained long enough.\r\n\r\nI'm not an expert on translation models.  Perhaps someone else reading this thread has more insight."}