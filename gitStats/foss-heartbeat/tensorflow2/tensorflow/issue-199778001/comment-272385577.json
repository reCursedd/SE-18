{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/272385577", "html_url": "https://github.com/tensorflow/tensorflow/issues/6761#issuecomment-272385577", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6761", "id": 272385577, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MjM4NTU3Nw==", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-13T08:14:27Z", "updated_at": "2017-01-13T08:17:21Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a>  adagrad problem also find out the cause.  I think it is my code problem, I should use addtional scope and set variable reused in that scope, before using tf.contrib.layers.optimize_loss.<br>\nThough tf v0.12 can work but I think tv v1.0's behavior is ok not a bug also. Thanks lukaszkaiser and jart!</p>\n<p>Below code can reproduce this, it works with tf v0.12 fail v1.0 without using addtional scope for train and evaluate before layers.optimize.<br>\n<a href=\"https://github.com/chenghuige/tensorflow-example/blob/master/tests/adagrad_optimize_scope.py\">https://github.com/chenghuige/tensorflow-example/blob/master/tests/adagrad_optimize_scope.py</a></p>", "body_text": "@lukaszkaiser  adagrad problem also find out the cause.  I think it is my code problem, I should use addtional scope and set variable reused in that scope, before using tf.contrib.layers.optimize_loss.\nThough tf v0.12 can work but I think tv v1.0's behavior is ok not a bug also. Thanks lukaszkaiser and jart!\nBelow code can reproduce this, it works with tf v0.12 fail v1.0 without using addtional scope for train and evaluate before layers.optimize.\nhttps://github.com/chenghuige/tensorflow-example/blob/master/tests/adagrad_optimize_scope.py", "body": "@lukaszkaiser  adagrad problem also find out the cause.  I think it is my code problem, I should use addtional scope and set variable reused in that scope, before using tf.contrib.layers.optimize_loss.\r\nThough tf v0.12 can work but I think tv v1.0's behavior is ok not a bug also. Thanks lukaszkaiser and jart!\r\n\r\nBelow code can reproduce this, it works with tf v0.12 fail v1.0 without using addtional scope for train and evaluate before layers.optimize.\r\nhttps://github.com/chenghuige/tensorflow-example/blob/master/tests/adagrad_optimize_scope.py"}