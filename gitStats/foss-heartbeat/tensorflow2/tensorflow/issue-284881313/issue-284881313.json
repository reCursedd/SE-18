{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15688", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15688/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15688/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15688/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15688", "id": 284881313, "node_id": "MDU6SXNzdWUyODQ4ODEzMTM=", "number": 15688, "title": "Reduced accuracy with retrained Inception v3 model on Android ", "user": {"login": "dailystudio", "id": 898525, "node_id": "MDQ6VXNlcjg5ODUyNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/898525?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dailystudio", "html_url": "https://github.com/dailystudio", "followers_url": "https://api.github.com/users/dailystudio/followers", "following_url": "https://api.github.com/users/dailystudio/following{/other_user}", "gists_url": "https://api.github.com/users/dailystudio/gists{/gist_id}", "starred_url": "https://api.github.com/users/dailystudio/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dailystudio/subscriptions", "organizations_url": "https://api.github.com/users/dailystudio/orgs", "repos_url": "https://api.github.com/users/dailystudio/repos", "events_url": "https://api.github.com/users/dailystudio/events{/privacy}", "received_events_url": "https://api.github.com/users/dailystudio/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jart", "id": 49262, "node_id": "MDQ6VXNlcjQ5MjYy", "avatar_url": "https://avatars1.githubusercontent.com/u/49262?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jart", "html_url": "https://github.com/jart", "followers_url": "https://api.github.com/users/jart/followers", "following_url": "https://api.github.com/users/jart/following{/other_user}", "gists_url": "https://api.github.com/users/jart/gists{/gist_id}", "starred_url": "https://api.github.com/users/jart/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jart/subscriptions", "organizations_url": "https://api.github.com/users/jart/orgs", "repos_url": "https://api.github.com/users/jart/repos", "events_url": "https://api.github.com/users/jart/events{/privacy}", "received_events_url": "https://api.github.com/users/jart/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2017-12-28T09:58:43Z", "updated_at": "2018-04-27T03:09:55Z", "closed_at": "2018-04-26T15:48:38Z", "author_association": "NONE", "body_html": "<p>I have follow instructions on TensorFlow website and the source code from examples on Github to retrain my own image classifier model which is basing on Inception v3.</p>\n<p>The result is, for same picture, if I use python script for prediction I got the right category with confidence 93.3%. But I use Android Inference interface can only get the right category with 81.3% confidence.</p>\n<p>I think the problem comes from the way that how to use the model.</p>\n<p>In Github code, <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java</a></p>\n<p>Here is a snippet of comments to indicate how to user Inception v3 model:</p>\n<div class=\"highlight highlight-source-java\"><pre>  <span class=\"pl-c\"><span class=\"pl-c\">//</span> These are the settings for the original v1 Inception model. If you want to</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> use a model that's been produced from the TensorFlow for Poets codelab,</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> you'll need to set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128,</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> INPUT_NAME = \"Mul\", and OUTPUT_NAME = \"final_result\".</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> You'll also need to update the MODEL_FILE and LABEL_FILE paths to point to</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> the ones you produced.</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> To use v3 Inception model, strip the DecodeJpeg Op from your retrained</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> model first:</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span></span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> python strip_unused.py \\</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> --input_graph=&lt;retrained-pb-file&gt; \\</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> --output_graph=&lt;your-stripped-pb-file&gt; \\</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> --input_node_names=\"Mul\" \\</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> --output_node_names=\"final_result\" \\</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">//</span> --input_binary=true</span></pre></div>\n<p>We start from Input named \"Mul\" because DecodeJpeg is NOT supported in Android. So, we need to decode bitmap and resize it to 299 x 299 and flatten it with Android way.  I think that is the difference between python script and Android Inference interface. In python script, we use tf.gFile to get the content of image and direct pass to the start node \"DecodeJpeg\"</p>\n<p>I review the graph of retrained model, node \"Mul\" is not the direct successor of DecodeJpeg. There are four nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" between \"Mul\" and \"DecodeJpeg\". I think it does the same thing I mentioned to preprocess the image. I think may be we could pass input data a little bit earlier than \"Mul\".</p>\n<p>First, I strip the mode with follow command:</p>\n<div class=\"highlight highlight-source-shell\"><pre>strip_unused \\\n--input_graph=tf_files/retrained_graph.pb \\\n--output_graph=tf_files/stripped_retrained_graph..pb \\\n--input_node_names=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Cast<span class=\"pl-pds\">\"</span></span> \\\n--output_node_names=<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>final_result<span class=\"pl-pds\">\"</span></span> \\\n--input_binary=true</pre></div>\n<p>Then I change the recognizeImage() to pass input to node Cast</p>\n<div class=\"highlight highlight-source-java\"><pre>  <span class=\"pl-k\">@Override</span>\n  <span class=\"pl-k\">public</span> <span class=\"pl-k\">List&lt;<span class=\"pl-smi\">Recognition</span>&gt;</span> recognizeImage(<span class=\"pl-k\">final</span> <span class=\"pl-smi\">Bitmap</span> bitmap) {\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> Log this method so that it can be analyzed with systrace.</span>\n    <span class=\"pl-smi\">Trace</span><span class=\"pl-k\">.</span>beginSection(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>recognizeImage<span class=\"pl-pds\">\"</span></span>);\n\n    <span class=\"pl-smi\">Trace</span><span class=\"pl-k\">.</span>beginSection(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>preprocessBitmap<span class=\"pl-pds\">\"</span></span>);\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> Preprocess the image data from 0-255 int to normalized float based</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> on the provided parameters.</span>\n    <span class=\"pl-k\">int</span>[] origIntValues <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">int</span>[bitmap<span class=\"pl-k\">.</span>getWidth() <span class=\"pl-k\">*</span> bitmap<span class=\"pl-k\">.</span>getHeight()];\n    <span class=\"pl-k\">float</span>[] flatValues <span class=\"pl-k\">=</span> <span class=\"pl-k\">new</span> <span class=\"pl-smi\">float</span>[bitmap<span class=\"pl-k\">.</span>getWidth() <span class=\"pl-k\">*</span> bitmap<span class=\"pl-k\">.</span>getHeight() <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span>];\n    bitmap<span class=\"pl-k\">.</span>getPixels(origIntValues, <span class=\"pl-c1\">0</span>, bitmap<span class=\"pl-k\">.</span>getWidth(), <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, bitmap<span class=\"pl-k\">.</span>getWidth(), bitmap<span class=\"pl-k\">.</span>getHeight());\n    <span class=\"pl-k\">for</span> (<span class=\"pl-k\">int</span> i <span class=\"pl-k\">=</span> <span class=\"pl-c1\">0</span>; i <span class=\"pl-k\">&lt;</span> origIntValues<span class=\"pl-k\">.</span>length; <span class=\"pl-k\">++</span>i) {\n        <span class=\"pl-k\">final</span> <span class=\"pl-k\">int</span> val <span class=\"pl-k\">=</span> origIntValues[i];\n        flatValues[i <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">0</span>] <span class=\"pl-k\">=</span> ((val <span class=\"pl-k\">&gt;&gt;</span> <span class=\"pl-c1\">16</span>) <span class=\"pl-k\">&amp;</span> <span class=\"pl-c1\">0xFF</span>);\n        flatValues[i <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>] <span class=\"pl-k\">=</span> ((val <span class=\"pl-k\">&gt;&gt;</span> <span class=\"pl-c1\">8</span>) <span class=\"pl-k\">&amp;</span> <span class=\"pl-c1\">0xFF</span>) ;\n        flatValues[i <span class=\"pl-k\">*</span> <span class=\"pl-c1\">3</span> <span class=\"pl-k\">+</span> <span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> (val <span class=\"pl-k\">&amp;</span> <span class=\"pl-c1\">0xFF</span>);\n     }\n    <span class=\"pl-smi\">Trace</span><span class=\"pl-k\">.</span>endSection();\n\n    <span class=\"pl-c\"><span class=\"pl-c\">//</span> Copy the input data into TensorFlow.</span>\n    <span class=\"pl-smi\">Trace</span><span class=\"pl-k\">.</span>beginSection(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>feed<span class=\"pl-pds\">\"</span></span>);\n    inferenceInterface<span class=\"pl-k\">.</span>feed(inputName, flatValues, <span class=\"pl-k\">new</span> <span class=\"pl-smi\">long</span>[] { bitmap<span class=\"pl-k\">.</span>getHeight(), bitmap<span class=\"pl-k\">.</span>getWidth() , <span class=\"pl-c1\">3</span>  });\n    <span class=\"pl-smi\">Trace</span><span class=\"pl-k\">.</span>endSection();</pre></div>\n<p>Here the inputNames is \"Cast\", not \"Mul\". After that I get the exact the same result as python script.</p>\n<p>Conclusion, I think the way currently we used on Android side to preprocess image is NOT doing the same tasks as the nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" do. I suggest to update the code of Android example to fix this problem.</p>", "body_text": "I have follow instructions on TensorFlow website and the source code from examples on Github to retrain my own image classifier model which is basing on Inception v3.\nThe result is, for same picture, if I use python script for prediction I got the right category with confidence 93.3%. But I use Android Inference interface can only get the right category with 81.3% confidence.\nI think the problem comes from the way that how to use the model.\nIn Github code, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java\nHere is a snippet of comments to indicate how to user Inception v3 model:\n  // These are the settings for the original v1 Inception model. If you want to\n  // use a model that's been produced from the TensorFlow for Poets codelab,\n  // you'll need to set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128,\n  // INPUT_NAME = \"Mul\", and OUTPUT_NAME = \"final_result\".\n  // You'll also need to update the MODEL_FILE and LABEL_FILE paths to point to\n  // the ones you produced.\n  //\n  // To use v3 Inception model, strip the DecodeJpeg Op from your retrained\n  // model first:\n  //\n  // python strip_unused.py \\\n  // --input_graph=<retrained-pb-file> \\\n  // --output_graph=<your-stripped-pb-file> \\\n  // --input_node_names=\"Mul\" \\\n  // --output_node_names=\"final_result\" \\\n  // --input_binary=true\nWe start from Input named \"Mul\" because DecodeJpeg is NOT supported in Android. So, we need to decode bitmap and resize it to 299 x 299 and flatten it with Android way.  I think that is the difference between python script and Android Inference interface. In python script, we use tf.gFile to get the content of image and direct pass to the start node \"DecodeJpeg\"\nI review the graph of retrained model, node \"Mul\" is not the direct successor of DecodeJpeg. There are four nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" between \"Mul\" and \"DecodeJpeg\". I think it does the same thing I mentioned to preprocess the image. I think may be we could pass input data a little bit earlier than \"Mul\".\nFirst, I strip the mode with follow command:\nstrip_unused \\\n--input_graph=tf_files/retrained_graph.pb \\\n--output_graph=tf_files/stripped_retrained_graph..pb \\\n--input_node_names=\"Cast\" \\\n--output_node_names=\"final_result\" \\\n--input_binary=true\nThen I change the recognizeImage() to pass input to node Cast\n  @Override\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\n    // Log this method so that it can be analyzed with systrace.\n    Trace.beginSection(\"recognizeImage\");\n\n    Trace.beginSection(\"preprocessBitmap\");\n    // Preprocess the image data from 0-255 int to normalized float based\n    // on the provided parameters.\n    int[] origIntValues = new int[bitmap.getWidth() * bitmap.getHeight()];\n    float[] flatValues = new float[bitmap.getWidth() * bitmap.getHeight() * 3];\n    bitmap.getPixels(origIntValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\n    for (int i = 0; i < origIntValues.length; ++i) {\n        final int val = origIntValues[i];\n        flatValues[i * 3 + 0] = ((val >> 16) & 0xFF);\n        flatValues[i * 3 + 1] = ((val >> 8) & 0xFF) ;\n        flatValues[i * 3 + 2] = (val & 0xFF);\n     }\n    Trace.endSection();\n\n    // Copy the input data into TensorFlow.\n    Trace.beginSection(\"feed\");\n    inferenceInterface.feed(inputName, flatValues, new long[] { bitmap.getHeight(), bitmap.getWidth() , 3  });\n    Trace.endSection();\nHere the inputNames is \"Cast\", not \"Mul\". After that I get the exact the same result as python script.\nConclusion, I think the way currently we used on Android side to preprocess image is NOT doing the same tasks as the nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" do. I suggest to update the code of Android example to fix this problem.", "body": "I have follow instructions on TensorFlow website and the source code from examples on Github to retrain my own image classifier model which is basing on Inception v3.\r\n\r\nThe result is, for same picture, if I use python script for prediction I got the right category with confidence 93.3%. But I use Android Inference interface can only get the right category with 81.3% confidence.\r\n\r\nI think the problem comes from the way that how to use the model.\r\n\r\nIn Github code, https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/android/src/org/tensorflow/demo/ClassifierActivity.java\r\n\r\nHere is a snippet of comments to indicate how to user Inception v3 model:\r\n```java\r\n  // These are the settings for the original v1 Inception model. If you want to\r\n  // use a model that's been produced from the TensorFlow for Poets codelab,\r\n  // you'll need to set IMAGE_SIZE = 299, IMAGE_MEAN = 128, IMAGE_STD = 128,\r\n  // INPUT_NAME = \"Mul\", and OUTPUT_NAME = \"final_result\".\r\n  // You'll also need to update the MODEL_FILE and LABEL_FILE paths to point to\r\n  // the ones you produced.\r\n  //\r\n  // To use v3 Inception model, strip the DecodeJpeg Op from your retrained\r\n  // model first:\r\n  //\r\n  // python strip_unused.py \\\r\n  // --input_graph=<retrained-pb-file> \\\r\n  // --output_graph=<your-stripped-pb-file> \\\r\n  // --input_node_names=\"Mul\" \\\r\n  // --output_node_names=\"final_result\" \\\r\n  // --input_binary=true\r\n```\r\nWe start from Input named \"Mul\" because DecodeJpeg is NOT supported in Android. So, we need to decode bitmap and resize it to 299 x 299 and flatten it with Android way.  I think that is the difference between python script and Android Inference interface. In python script, we use tf.gFile to get the content of image and direct pass to the start node \"DecodeJpeg\"\r\n\r\nI review the graph of retrained model, node \"Mul\" is not the direct successor of DecodeJpeg. There are four nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" between \"Mul\" and \"DecodeJpeg\". I think it does the same thing I mentioned to preprocess the image. I think may be we could pass input data a little bit earlier than \"Mul\".\r\n\r\nFirst, I strip the mode with follow command:\r\n```shell\r\nstrip_unused \\\r\n--input_graph=tf_files/retrained_graph.pb \\\r\n--output_graph=tf_files/stripped_retrained_graph..pb \\\r\n--input_node_names=\"Cast\" \\\r\n--output_node_names=\"final_result\" \\\r\n--input_binary=true\r\n```\r\nThen I change the recognizeImage() to pass input to node Cast\r\n```Java\r\n  @Override\r\n  public List<Recognition> recognizeImage(final Bitmap bitmap) {\r\n    // Log this method so that it can be analyzed with systrace.\r\n    Trace.beginSection(\"recognizeImage\");\r\n\r\n    Trace.beginSection(\"preprocessBitmap\");\r\n    // Preprocess the image data from 0-255 int to normalized float based\r\n    // on the provided parameters.\r\n    int[] origIntValues = new int[bitmap.getWidth() * bitmap.getHeight()];\r\n    float[] flatValues = new float[bitmap.getWidth() * bitmap.getHeight() * 3];\r\n    bitmap.getPixels(origIntValues, 0, bitmap.getWidth(), 0, 0, bitmap.getWidth(), bitmap.getHeight());\r\n    for (int i = 0; i < origIntValues.length; ++i) {\r\n        final int val = origIntValues[i];\r\n        flatValues[i * 3 + 0] = ((val >> 16) & 0xFF);\r\n        flatValues[i * 3 + 1] = ((val >> 8) & 0xFF) ;\r\n        flatValues[i * 3 + 2] = (val & 0xFF);\r\n     }\r\n    Trace.endSection();\r\n\r\n    // Copy the input data into TensorFlow.\r\n    Trace.beginSection(\"feed\");\r\n    inferenceInterface.feed(inputName, flatValues, new long[] { bitmap.getHeight(), bitmap.getWidth() , 3  });\r\n    Trace.endSection();\r\n```\r\nHere the inputNames is \"Cast\", not \"Mul\". After that I get the exact the same result as python script. \r\n\r\nConclusion, I think the way currently we used on Android side to preprocess image is NOT doing the same tasks as the nodes \"Cast\", \"ExpandDims\", \"ResizeBilinear\", \"Sub\" do. I suggest to update the code of Android example to fix this problem.\r\n"}