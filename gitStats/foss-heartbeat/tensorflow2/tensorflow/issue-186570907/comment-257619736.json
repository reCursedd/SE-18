{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/257619736", "html_url": "https://github.com/tensorflow/tensorflow/pull/5329#issuecomment-257619736", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5329", "id": 257619736, "node_id": "MDEyOklzc3VlQ29tbWVudDI1NzYxOTczNg==", "user": {"login": "goodfeli", "id": 387866, "node_id": "MDQ6VXNlcjM4Nzg2Ng==", "avatar_url": "https://avatars3.githubusercontent.com/u/387866?v=4", "gravatar_id": "", "url": "https://api.github.com/users/goodfeli", "html_url": "https://github.com/goodfeli", "followers_url": "https://api.github.com/users/goodfeli/followers", "following_url": "https://api.github.com/users/goodfeli/following{/other_user}", "gists_url": "https://api.github.com/users/goodfeli/gists{/gist_id}", "starred_url": "https://api.github.com/users/goodfeli/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/goodfeli/subscriptions", "organizations_url": "https://api.github.com/users/goodfeli/orgs", "repos_url": "https://api.github.com/users/goodfeli/repos", "events_url": "https://api.github.com/users/goodfeli/events{/privacy}", "received_events_url": "https://api.github.com/users/goodfeli/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-01T16:45:55Z", "updated_at": "2016-11-01T16:45:55Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a>: tl;dr This looks like the correct way to implement the full Hessian matrix. You wouldn't want to use _hessian_vector_product to do it.</p>\n<p>Long version:<br>\nFor most problems, the Hessian matrix would be too big to store, so we use _hessian_vector_product to compute only a small Krylov subspace of the Hessian matrix. However, it can be useful to compute the full Hessian matrix for a small problem, for debugging or research purposes.</p>\n<p>_hessian_vector_product takes the product between the Hessian \"matrix\" and a \"vector\". I use scare quotes here because the \"vector\" can actually be a list of tensors of arbitrary shape. We imagine that we reshape and concatenate these tensors to get one large 1-D tensor. It's not possible to call _hessian_vector_product with just a scalar <code>1</code> or an identity matrix as its argument; the argument needs to match the shapes of the parameter list.</p>\n<p>To compute a Hessian matrix of shape [n, n] using _hessian_vector_product, you would need to call _hessian_vector_product n times. On call <code>i</code>, you pass a \"vector\" that is all zeros except for position <code>i</code>, and is 1 at position <code>i</code>. This is not very efficient because you have to construct those one-hot vectors.</p>\n<p>Computing the full Hessian is not a very common operation, but if someone wants to do it, this PR looks like the right way to do it.</p>", "body_text": "@vrv: tl;dr This looks like the correct way to implement the full Hessian matrix. You wouldn't want to use _hessian_vector_product to do it.\nLong version:\nFor most problems, the Hessian matrix would be too big to store, so we use _hessian_vector_product to compute only a small Krylov subspace of the Hessian matrix. However, it can be useful to compute the full Hessian matrix for a small problem, for debugging or research purposes.\n_hessian_vector_product takes the product between the Hessian \"matrix\" and a \"vector\". I use scare quotes here because the \"vector\" can actually be a list of tensors of arbitrary shape. We imagine that we reshape and concatenate these tensors to get one large 1-D tensor. It's not possible to call _hessian_vector_product with just a scalar 1 or an identity matrix as its argument; the argument needs to match the shapes of the parameter list.\nTo compute a Hessian matrix of shape [n, n] using _hessian_vector_product, you would need to call _hessian_vector_product n times. On call i, you pass a \"vector\" that is all zeros except for position i, and is 1 at position i. This is not very efficient because you have to construct those one-hot vectors.\nComputing the full Hessian is not a very common operation, but if someone wants to do it, this PR looks like the right way to do it.", "body": "@vrv: tl;dr This looks like the correct way to implement the full Hessian matrix. You wouldn't want to use _hessian_vector_product to do it.\n\nLong version:\nFor most problems, the Hessian matrix would be too big to store, so we use _hessian_vector_product to compute only a small Krylov subspace of the Hessian matrix. However, it can be useful to compute the full Hessian matrix for a small problem, for debugging or research purposes.\n\n_hessian_vector_product takes the product between the Hessian \"matrix\" and a \"vector\". I use scare quotes here because the \"vector\" can actually be a list of tensors of arbitrary shape. We imagine that we reshape and concatenate these tensors to get one large 1-D tensor. It's not possible to call _hessian_vector_product with just a scalar `1` or an identity matrix as its argument; the argument needs to match the shapes of the parameter list.\n\nTo compute a Hessian matrix of shape [n, n] using _hessian_vector_product, you would need to call _hessian_vector_product n times. On call `i`, you pass a \"vector\" that is all zeros except for position `i`, and is 1 at position `i`. This is not very efficient because you have to construct those one-hot vectors.\n\nComputing the full Hessian is not a very common operation, but if someone wants to do it, this PR looks like the right way to do it.\n"}