{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6048", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6048/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6048/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6048/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6048", "id": 193227405, "node_id": "MDU6SXNzdWUxOTMyMjc0MDU=", "number": 6048, "title": "CUDA_ERROR_OUT_OF_MEMORY (Memory Available)", "user": {"login": "camj256", "id": 6239949, "node_id": "MDQ6VXNlcjYyMzk5NDk=", "avatar_url": "https://avatars3.githubusercontent.com/u/6239949?v=4", "gravatar_id": "", "url": "https://api.github.com/users/camj256", "html_url": "https://github.com/camj256", "followers_url": "https://api.github.com/users/camj256/followers", "following_url": "https://api.github.com/users/camj256/following{/other_user}", "gists_url": "https://api.github.com/users/camj256/gists{/gist_id}", "starred_url": "https://api.github.com/users/camj256/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/camj256/subscriptions", "organizations_url": "https://api.github.com/users/camj256/orgs", "repos_url": "https://api.github.com/users/camj256/repos", "events_url": "https://api.github.com/users/camj256/events{/privacy}", "received_events_url": "https://api.github.com/users/camj256/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-12-02T21:56:06Z", "updated_at": "2018-04-06T19:06:07Z", "closed_at": "2017-01-16T03:09:08Z", "author_association": "NONE", "body_html": "<p>Tensorflow is failing like so - very odd since I have memory available and it sees that. This runs fine in CPU only.</p>\n<p>Ubuntu 16.04, Cuda 8.0, CUDNN 5.1 for 8.0, Nvidia 367.57 driver, tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl. The rest you can see in the log. I have also tried with CUDNN 5.0 with the same result. Cuda 7.5 works for me but is very slow.</p>\n<p><strong>Log:</strong></p>\n<pre><code>I tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\nExceptions: 0\n1403\n1403\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:02:00.0\nTotal memory: 1.99GiB\nFree memory: 1.43GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2c47900\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:01:00.0\nTotal memory: 1.99GiB\nFree memory: 1.41GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 198.83M (208486400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nTraceback (most recent call last):\n  File \"build.py\", line 84, in &lt;module&gt;\n    model = tflearn.DNN(net)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py\", line 63, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py\", line 135, in __init__\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1021, in build\n    raise ValueError(\"No variables to save\")\nValueError: No variables to save\n</code></pre>\n<p><strong>Code:</strong></p>\n<pre><code>net = tflearn.input_data(shape=[None, 5])\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net)\n\n# Training\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y, n_epoch=10000, batch_size=64, show_metric=True)\n\nfor x in range(0,50):\n    rand = randint(0,len(X))\n    print(model.predict([X[rand]]), Y[rand])\n\nmodel.save(\"model.tfl\")\n</code></pre>", "body_text": "Tensorflow is failing like so - very odd since I have memory available and it sees that. This runs fine in CPU only.\nUbuntu 16.04, Cuda 8.0, CUDNN 5.1 for 8.0, Nvidia 367.57 driver, tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl. The rest you can see in the log. I have also tried with CUDNN 5.0 with the same result. Cuda 7.5 works for me but is very slow.\nLog:\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\nExceptions: 0\n1403\n1403\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:02:00.0\nTotal memory: 1.99GiB\nFree memory: 1.43GiB\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2c47900\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \nname: GeForce GTX 660\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\npciBusID 0000:01:00.0\nTotal memory: 1.99GiB\nFree memory: 1.41GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 198.83M (208486400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nTraceback (most recent call last):\n  File \"build.py\", line 84, in <module>\n    model = tflearn.DNN(net)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py\", line 63, in __init__\n    best_val_accuracy=best_val_accuracy)\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py\", line 135, in __init__\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\n    self.build()\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1021, in build\n    raise ValueError(\"No variables to save\")\nValueError: No variables to save\n\nCode:\nnet = tflearn.input_data(shape=[None, 5])\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.fully_connected(net, 64)\nnet = tflearn.fully_connected(net, 2, activation='softmax')\nnet = tflearn.regression(net)\n\n# Training\nmodel = tflearn.DNN(net)\nmodel.fit(X, Y, n_epoch=10000, batch_size=64, show_metric=True)\n\nfor x in range(0,50):\n    rand = randint(0,len(X))\n    print(model.predict([X[rand]]), Y[rand])\n\nmodel.save(\"model.tfl\")", "body": "Tensorflow is failing like so - very odd since I have memory available and it sees that. This runs fine in CPU only.\r\n\r\nUbuntu 16.04, Cuda 8.0, CUDNN 5.1 for 8.0, Nvidia 367.57 driver, tensorflow_gpu-0.12.0rc0-cp27-none-linux_x86_64.whl. The rest you can see in the log. I have also tried with CUDNN 5.0 with the same result. Cuda 7.5 works for me but is very slow.\r\n\r\n**Log:**\r\n```\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcublas.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcudnn.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcufft.so locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcuda.so.1 locally\r\nI tensorflow/stream_executor/dso_loader.cc:128] successfully opened CUDA library libcurand.so locally\r\nExceptions: 0\r\n1403\r\n1403\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\r\npciBusID 0000:02:00.0\r\nTotal memory: 1.99GiB\r\nFree memory: 1.43GiB\r\nW tensorflow/stream_executor/cuda/cuda_driver.cc:590] creating context when one is currently active; existing: 0x2c47900\r\nI tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 1 with properties: \r\nname: GeForce GTX 660\r\nmajor: 3 minor: 0 memoryClockRate (GHz) 1.0975\r\npciBusID 0000:01:00.0\r\nTotal memory: 1.99GiB\r\nFree memory: 1.41GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 0 and 1\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:777] Peer access not supported between device ordinals 1 and 0\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 1 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y N \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 1:   N Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 660, pci bus id: 0000:02:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 660, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 198.83M (208486400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nTraceback (most recent call last):\r\n  File \"build.py\", line 84, in <module>\r\n    model = tflearn.DNN(net)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/models/dnn.py\", line 63, in __init__\r\n    best_val_accuracy=best_val_accuracy)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tflearn/helpers/trainer.py\", line 135, in __init__\r\n    keep_checkpoint_every_n_hours=keep_checkpoint_every_n_hours)\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1000, in __init__\r\n    self.build()\r\n  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/training/saver.py\", line 1021, in build\r\n    raise ValueError(\"No variables to save\")\r\nValueError: No variables to save\r\n```\r\n\r\n**Code:**\r\n```\r\nnet = tflearn.input_data(shape=[None, 5])\r\nnet = tflearn.fully_connected(net, 64)\r\nnet = tflearn.fully_connected(net, 64)\r\nnet = tflearn.fully_connected(net, 2, activation='softmax')\r\nnet = tflearn.regression(net)\r\n\r\n# Training\r\nmodel = tflearn.DNN(net)\r\nmodel.fit(X, Y, n_epoch=10000, batch_size=64, show_metric=True)\r\n\r\nfor x in range(0,50):\r\n    rand = randint(0,len(X))\r\n    print(model.predict([X[rand]]), Y[rand])\r\n\r\nmodel.save(\"model.tfl\")\r\n```"}