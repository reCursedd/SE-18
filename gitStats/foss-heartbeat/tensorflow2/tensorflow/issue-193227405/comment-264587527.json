{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/264587527", "html_url": "https://github.com/tensorflow/tensorflow/issues/6048#issuecomment-264587527", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6048", "id": 264587527, "node_id": "MDEyOklzc3VlQ29tbWVudDI2NDU4NzUyNw==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-12-02T23:10:18Z", "updated_at": "2016-12-02T23:11:17Z", "author_association": "MEMBER", "body_html": "<p>The python stack trace doesn't seem to accurately identify where the failing memory allocation is happening.  In general, nothing in a TF program should be calling CudaDriver::DeviceAllocate directly, except the initial allocation of most of the device memory into a pool that will later be sub-allocated.  Your GPUs are rather small.  I can't tell whether the initial allocation is failing, or something later.</p>\n<p>You might try adjusting the fraction of visible memory that TF tries to take in its initial allocation.<br>\nFor example: Assume that you have 12GB of GPU memory and want to allocate ~4GB:</p>\n<p>gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)</p>\n<p>sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))</p>", "body_text": "The python stack trace doesn't seem to accurately identify where the failing memory allocation is happening.  In general, nothing in a TF program should be calling CudaDriver::DeviceAllocate directly, except the initial allocation of most of the device memory into a pool that will later be sub-allocated.  Your GPUs are rather small.  I can't tell whether the initial allocation is failing, or something later.\nYou might try adjusting the fraction of visible memory that TF tries to take in its initial allocation.\nFor example: Assume that you have 12GB of GPU memory and want to allocate ~4GB:\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))", "body": "The python stack trace doesn't seem to accurately identify where the failing memory allocation is happening.  In general, nothing in a TF program should be calling CudaDriver::DeviceAllocate directly, except the initial allocation of most of the device memory into a pool that will later be sub-allocated.  Your GPUs are rather small.  I can't tell whether the initial allocation is failing, or something later.  \r\n\r\nYou might try adjusting the fraction of visible memory that TF tries to take in its initial allocation.\r\nFor example: Assume that you have 12GB of GPU memory and want to allocate ~4GB:\r\n\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\r\n\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))"}