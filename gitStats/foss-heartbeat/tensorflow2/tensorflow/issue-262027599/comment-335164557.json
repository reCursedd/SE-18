{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/335164557", "html_url": "https://github.com/tensorflow/tensorflow/issues/13441#issuecomment-335164557", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13441", "id": 335164557, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNTE2NDU1Nw==", "user": {"login": "LiuZichuan", "id": 22128160, "node_id": "MDQ6VXNlcjIyMTI4MTYw", "avatar_url": "https://avatars2.githubusercontent.com/u/22128160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiuZichuan", "html_url": "https://github.com/LiuZichuan", "followers_url": "https://api.github.com/users/LiuZichuan/followers", "following_url": "https://api.github.com/users/LiuZichuan/following{/other_user}", "gists_url": "https://api.github.com/users/LiuZichuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiuZichuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiuZichuan/subscriptions", "organizations_url": "https://api.github.com/users/LiuZichuan/orgs", "repos_url": "https://api.github.com/users/LiuZichuan/repos", "events_url": "https://api.github.com/users/LiuZichuan/events{/privacy}", "received_events_url": "https://api.github.com/users/LiuZichuan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-09T13:52:07Z", "updated_at": "2017-10-10T04:49:30Z", "author_association": "NONE", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=29663194\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/cy89\">@cy89</a> , below is the code that reproduces the error. You can copy and run directly on your PC. The system I use is ubuntu 14.04 with Python3.4. The Tensorflow version is v1.1.<br>\nThis phenomena is that the program get stuck at <code>sess.run(optimizer, feeds)</code>. But when I comment the batch normalization operation <code>logits = tf.contrib.layers.batch_norm(logits, is_training=True)</code> and <code>tmp = tf.contrib.layers.batch_norm(tmp, is_training=True)</code>, it run correctly.</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport sys\nimport math\nimport time\nimport nets.CTPNetCL_MSRA_TD500 as ctp\nimport scipy.io as io\nimport cv2\nimport os\nimport utils.preprocessing as pp\nfrom cmath import isnan\n\ndef weight_variable(shape, mean=0, stddev=1):\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.01, shape=shape)\n    return tf.Variable(initial)\n\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\n    if cell_type == 'LSTM': \n        forward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\n        backward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\n    elif cell_type == 'GRU':\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\n    else :\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\n    \n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\n    outputs = tf.concat(output_tuple[0], 2)\n    return outputs\n\nH = 768\nW = 768\nC = 3\nbatch_size = 1\nstride = 16\nlearning_rate = 0.0001\ndecay_rate = 0.9\nsave_period = 3\nprob_period = 3\ndecay_steps = 1000\n\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\n\nweight1 = weight_variable([3, 3, 3, 4608])\nb1 = weight_variable([4608])\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\ntmp = tf.nn.bias_add(tmp, b1)\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\ntmp = tf.nn.relu(tmp)\nshape = np.shape(tmp)\n# row major\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\n# reshape to RNN\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\n# feed to RNN\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\n# reshape back to 2D feature map\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\n# outputs to Logits\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\nb2 = weight_variable([2])\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\nlogits = tf.nn.bias_add(logits, b2)\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\n# loss\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\n# cost\ncost = tf.reduce_mean(loss)\n\nglobal_step = tf.Variable(0, trainable=False)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\n\ninit = tf.global_variables_initializer()\n\n# testing data\n_inputs = np.random.randn(batch_size, H, W, C)\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\n_labels_negative = 1 - _labels_positive\n_labels = np.stack((_labels_negative, _labels_positive), -1)\n\nfeeds = {inputs: _inputs, labels: _labels}\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(optimizer, feeds)\n</code></pre>\n<p>The log message  before getting stuck is shown below:</p>\n<pre><code>Using TensorFlow backend.\nloading dataset 'MSRA_TD500'.\nloading finished: 2.849345s\nloading vgg-16 pretrained model from /home/tensor_v1/project/CRNN-Tensorflow/CRNN/nets/vgg16/vgg16.npy.\nnpy file loaded\n2017-10-09 21:48:58.425429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-10-09 21:48:58.425860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:03:00.0\nTotal memory: 7.92GiB\nFree memory: 7.76GiB\n2017-10-09 21:48:58.425934: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0xbcaf9d0\n2017-10-09 21:48:58.643478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-10-09 21:48:58.643903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:04:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\n2017-10-09 21:48:58.644864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \n2017-10-09 21:48:58.644880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \n2017-10-09 21:48:58.644886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \n2017-10-09 21:48:58.644897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\n2017-10-09 21:48:58.644904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)\n\n</code></pre>", "body_text": "Hi @cy89 , below is the code that reproduces the error. You can copy and run directly on your PC. The system I use is ubuntu 14.04 with Python3.4. The Tensorflow version is v1.1.\nThis phenomena is that the program get stuck at sess.run(optimizer, feeds). But when I comment the batch normalization operation logits = tf.contrib.layers.batch_norm(logits, is_training=True) and tmp = tf.contrib.layers.batch_norm(tmp, is_training=True), it run correctly.\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport math\nimport time\nimport nets.CTPNetCL_MSRA_TD500 as ctp\nimport scipy.io as io\nimport cv2\nimport os\nimport utils.preprocessing as pp\nfrom cmath import isnan\n\ndef weight_variable(shape, mean=0, stddev=1):\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.01, shape=shape)\n    return tf.Variable(initial)\n\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\n    if cell_type == 'LSTM': \n        forward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\n        backward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\n    elif cell_type == 'GRU':\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\n    else :\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\n    \n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\n    outputs = tf.concat(output_tuple[0], 2)\n    return outputs\n\nH = 768\nW = 768\nC = 3\nbatch_size = 1\nstride = 16\nlearning_rate = 0.0001\ndecay_rate = 0.9\nsave_period = 3\nprob_period = 3\ndecay_steps = 1000\n\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\n\nweight1 = weight_variable([3, 3, 3, 4608])\nb1 = weight_variable([4608])\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\ntmp = tf.nn.bias_add(tmp, b1)\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\ntmp = tf.nn.relu(tmp)\nshape = np.shape(tmp)\n# row major\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\n# reshape to RNN\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\n# feed to RNN\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\n# reshape back to 2D feature map\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\n# outputs to Logits\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\nb2 = weight_variable([2])\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\nlogits = tf.nn.bias_add(logits, b2)\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\n# loss\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\n# cost\ncost = tf.reduce_mean(loss)\n\nglobal_step = tf.Variable(0, trainable=False)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\n\ninit = tf.global_variables_initializer()\n\n# testing data\n_inputs = np.random.randn(batch_size, H, W, C)\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\n_labels_negative = 1 - _labels_positive\n_labels = np.stack((_labels_negative, _labels_positive), -1)\n\nfeeds = {inputs: _inputs, labels: _labels}\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(optimizer, feeds)\n\nThe log message  before getting stuck is shown below:\nUsing TensorFlow backend.\nloading dataset 'MSRA_TD500'.\nloading finished: 2.849345s\nloading vgg-16 pretrained model from /home/tensor_v1/project/CRNN-Tensorflow/CRNN/nets/vgg16/vgg16.npy.\nnpy file loaded\n2017-10-09 21:48:58.425429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-10-09 21:48:58.425860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:03:00.0\nTotal memory: 7.92GiB\nFree memory: 7.76GiB\n2017-10-09 21:48:58.425934: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0xbcaf9d0\n2017-10-09 21:48:58.643478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2017-10-09 21:48:58.643903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \nname: GeForce GTX 1080\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\npciBusID 0000:04:00.0\nTotal memory: 7.92GiB\nFree memory: 7.81GiB\n2017-10-09 21:48:58.644864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \n2017-10-09 21:48:58.644880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \n2017-10-09 21:48:58.644886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \n2017-10-09 21:48:58.644897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\n2017-10-09 21:48:58.644904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)", "body": "Hi @cy89 , below is the code that reproduces the error. You can copy and run directly on your PC. The system I use is ubuntu 14.04 with Python3.4. The Tensorflow version is v1.1. \r\nThis phenomena is that the program get stuck at `sess.run(optimizer, feeds)`. But when I comment the batch normalization operation `logits = tf.contrib.layers.batch_norm(logits, is_training=True)` and `tmp = tf.contrib.layers.batch_norm(tmp, is_training=True)`, it run correctly.\r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport math\r\nimport time\r\nimport nets.CTPNetCL_MSRA_TD500 as ctp\r\nimport scipy.io as io\r\nimport cv2\r\nimport os\r\nimport utils.preprocessing as pp\r\nfrom cmath import isnan\r\n\r\ndef weight_variable(shape, mean=0, stddev=1):\r\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\r\n    return tf.Variable(initial)\r\n\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.01, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\r\n    if cell_type == 'LSTM': \r\n        forward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n        backward_cell = tf.contrib.rnn.core_rnn_cell.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n    elif cell_type == 'GRU':\r\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \r\n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\r\n    else :\r\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\r\n    \r\n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\r\n    outputs = tf.concat(output_tuple[0], 2)\r\n    return outputs\r\n\r\nH = 768\r\nW = 768\r\nC = 3\r\nbatch_size = 1\r\nstride = 16\r\nlearning_rate = 0.0001\r\ndecay_rate = 0.9\r\nsave_period = 3\r\nprob_period = 3\r\ndecay_steps = 1000\r\n\r\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\r\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\r\n\r\nweight1 = weight_variable([3, 3, 3, 4608])\r\nb1 = weight_variable([4608])\r\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\r\ntmp = tf.nn.bias_add(tmp, b1)\r\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\r\ntmp = tf.nn.relu(tmp)\r\nshape = np.shape(tmp)\r\n# row major\r\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\r\n# reshape to RNN\r\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\r\n# feed to RNN\r\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\r\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\r\n# reshape back to 2D feature map\r\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\r\n# outputs to Logits\r\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\r\nb2 = weight_variable([2])\r\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\r\nlogits = tf.nn.bias_add(logits, b2)\r\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\r\n# loss\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\r\n# cost\r\ncost = tf.reduce_mean(loss)\r\n\r\nglobal_step = tf.Variable(0, trainable=False)\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n# testing data\r\n_inputs = np.random.randn(batch_size, H, W, C)\r\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\r\n_labels_negative = 1 - _labels_positive\r\n_labels = np.stack((_labels_negative, _labels_positive), -1)\r\n\r\nfeeds = {inputs: _inputs, labels: _labels}\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(optimizer, feeds)\r\n```\r\nThe log message  before getting stuck is shown below:\r\n```\r\nUsing TensorFlow backend.\r\nloading dataset 'MSRA_TD500'.\r\nloading finished: 2.849345s\r\nloading vgg-16 pretrained model from /home/tensor_v1/project/CRNN-Tensorflow/CRNN/nets/vgg16/vgg16.npy.\r\nnpy file loaded\r\n2017-10-09 21:48:58.425429: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-09 21:48:58.425860: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 0 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:03:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.76GiB\r\n2017-10-09 21:48:58.425934: W tensorflow/stream_executor/cuda/cuda_driver.cc:485] creating context when one is currently active; existing: 0xbcaf9d0\r\n2017-10-09 21:48:58.643478: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2017-10-09 21:48:58.643903: I tensorflow/core/common_runtime/gpu/gpu_device.cc:887] Found device 1 with properties: \r\nname: GeForce GTX 1080\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.835\r\npciBusID 0000:04:00.0\r\nTotal memory: 7.92GiB\r\nFree memory: 7.81GiB\r\n2017-10-09 21:48:58.644864: I tensorflow/core/common_runtime/gpu/gpu_device.cc:908] DMA: 0 1 \r\n2017-10-09 21:48:58.644880: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 0:   Y Y \r\n2017-10-09 21:48:58.644886: I tensorflow/core/common_runtime/gpu/gpu_device.cc:918] 1:   Y Y \r\n2017-10-09 21:48:58.644897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1080, pci bus id: 0000:03:00.0)\r\n2017-10-09 21:48:58.644904: I tensorflow/core/common_runtime/gpu/gpu_device.cc:977] Creating TensorFlow device (/gpu:1) -> (device: 1, name: GeForce GTX 1080, pci bus id: 0000:04:00.0)\r\n\r\n```"}