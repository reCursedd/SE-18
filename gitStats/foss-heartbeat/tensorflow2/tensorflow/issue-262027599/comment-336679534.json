{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336679534", "html_url": "https://github.com/tensorflow/tensorflow/issues/13441#issuecomment-336679534", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13441", "id": 336679534, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjY3OTUzNA==", "user": {"login": "LiuZichuan", "id": 22128160, "node_id": "MDQ6VXNlcjIyMTI4MTYw", "avatar_url": "https://avatars2.githubusercontent.com/u/22128160?v=4", "gravatar_id": "", "url": "https://api.github.com/users/LiuZichuan", "html_url": "https://github.com/LiuZichuan", "followers_url": "https://api.github.com/users/LiuZichuan/followers", "following_url": "https://api.github.com/users/LiuZichuan/following{/other_user}", "gists_url": "https://api.github.com/users/LiuZichuan/gists{/gist_id}", "starred_url": "https://api.github.com/users/LiuZichuan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/LiuZichuan/subscriptions", "organizations_url": "https://api.github.com/users/LiuZichuan/orgs", "repos_url": "https://api.github.com/users/LiuZichuan/repos", "events_url": "https://api.github.com/users/LiuZichuan/events{/privacy}", "received_events_url": "https://api.github.com/users/LiuZichuan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-15T01:36:18Z", "updated_at": "2017-10-15T01:36:18Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a> I have update the program, which is shown below:</p>\n<pre><code>import tensorflow as tf\nimport numpy as np\nimport sys\nimport math\nimport time\n# import cv2\nimport os\n\n\ndef weight_variable(shape, mean=0, stddev=1):\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.01, shape=shape)\n    return tf.Variable(initial)\n\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\n    if cell_type == 'LSTM': \n        forward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\n        backward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\n    elif cell_type == 'GRU':\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\n    else :\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\n    \n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\n    outputs = tf.concat(output_tuple[0], 2)\n    return outputs\n\nH = 768\nW = 768\nC = 3\nbatch_size = 1\nstride = 16\nlearning_rate = 0.0001\ndecay_rate = 0.9\nsave_period = 3\nprob_period = 3\ndecay_steps = 1000\n\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\n# vgg = vgg16.Vgg16()\n# vgg.build(inputs)\n# tmp = vgg.outputs\n\n\nweight1 = weight_variable([3, 3, 3, 4608])\nb1 = weight_variable([4608])\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\ntmp = tf.nn.bias_add(tmp, b1)\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\ntmp = tf.nn.relu(tmp)\nshape = np.shape(tmp)\n# row major\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\n# reshape to RNN\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\n# feed to RNN\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\n# reshape back to 2D feature map\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\n# outputs to Logits\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\nb2 = weight_variable([2])\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\nlogits = tf.nn.bias_add(logits, b2)\n# logits = tf.layers.batch_normalization(logits, training=True)\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\n# loss\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\n# cost\ncost = tf.reduce_mean(loss)\n\nglobal_step = tf.Variable(0, trainable=False)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\n\ninit = tf.global_variables_initializer()\n\n# testing data\n_inputs = np.random.randn(batch_size, H, W, C)\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\n_labels_negative = 1 - _labels_positive\n_labels = np.stack((_labels_negative, _labels_positive), -1)\n\nfeeds = {inputs: _inputs, labels: _labels}\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(optimizer, feeds)\nprint('finished.')\n\n</code></pre>", "body_text": "@reedwm I have update the program, which is shown below:\nimport tensorflow as tf\nimport numpy as np\nimport sys\nimport math\nimport time\n# import cv2\nimport os\n\n\ndef weight_variable(shape, mean=0, stddev=1):\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    initial = tf.constant(0.01, shape=shape)\n    return tf.Variable(initial)\n\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\n    if cell_type == 'LSTM': \n        forward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\n        backward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\n    elif cell_type == 'GRU':\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\n    else :\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\n    \n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\n    outputs = tf.concat(output_tuple[0], 2)\n    return outputs\n\nH = 768\nW = 768\nC = 3\nbatch_size = 1\nstride = 16\nlearning_rate = 0.0001\ndecay_rate = 0.9\nsave_period = 3\nprob_period = 3\ndecay_steps = 1000\n\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\n# vgg = vgg16.Vgg16()\n# vgg.build(inputs)\n# tmp = vgg.outputs\n\n\nweight1 = weight_variable([3, 3, 3, 4608])\nb1 = weight_variable([4608])\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\ntmp = tf.nn.bias_add(tmp, b1)\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\ntmp = tf.nn.relu(tmp)\nshape = np.shape(tmp)\n# row major\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\n# reshape to RNN\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\n# feed to RNN\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\n# reshape back to 2D feature map\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\n# outputs to Logits\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\nb2 = weight_variable([2])\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\nlogits = tf.nn.bias_add(logits, b2)\n# logits = tf.layers.batch_normalization(logits, training=True)\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\n# loss\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\n# cost\ncost = tf.reduce_mean(loss)\n\nglobal_step = tf.Variable(0, trainable=False)\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nwith tf.control_dependencies(update_ops):\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\n\ninit = tf.global_variables_initializer()\n\n# testing data\n_inputs = np.random.randn(batch_size, H, W, C)\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\n_labels_negative = 1 - _labels_positive\n_labels = np.stack((_labels_negative, _labels_positive), -1)\n\nfeeds = {inputs: _inputs, labels: _labels}\n\nwith tf.Session() as sess:\n    sess.run(init)\n    sess.run(optimizer, feeds)\nprint('finished.')", "body": "@reedwm I have update the program, which is shown below: \r\n```\r\nimport tensorflow as tf\r\nimport numpy as np\r\nimport sys\r\nimport math\r\nimport time\r\n# import cv2\r\nimport os\r\n\r\n\r\ndef weight_variable(shape, mean=0, stddev=1):\r\n    initial = tf.truncated_normal(shape, mean=mean, stddev=stddev)\r\n    return tf.Variable(initial)\r\n\r\n\r\ndef bias_variable(shape):\r\n    initial = tf.constant(0.01, shape=shape)\r\n    return tf.Variable(initial)\r\n\r\ndef birnn_1d_layer(inputs, seq_len, num_hidden=None, is_truple=True, cell_type='LSTM', time_major=True, scope=None):\r\n    if cell_type == 'LSTM': \r\n        forward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n        backward_cell = tf.contrib.rnn.LSTMCell(num_hidden, state_is_tuple=is_truple)\r\n    elif cell_type == 'GRU':\r\n        forward_cell = tf.contrib.rnn.GRUCell(num_hidden)    \r\n        backward_cell = tf.contrib.rnn.GRUCell(num_hidden)\r\n    else :\r\n        exit(\"birnn_1d_layer: Unrecognized cell type.\")\r\n    \r\n    output_tuple = tf.nn.bidirectional_dynamic_rnn(forward_cell, backward_cell, inputs, seq_len, dtype=tf.float32, time_major=time_major, scope=scope)\r\n    outputs = tf.concat(output_tuple[0], 2)\r\n    return outputs\r\n\r\nH = 768\r\nW = 768\r\nC = 3\r\nbatch_size = 1\r\nstride = 16\r\nlearning_rate = 0.0001\r\ndecay_rate = 0.9\r\nsave_period = 3\r\nprob_period = 3\r\ndecay_steps = 1000\r\n\r\ninputs = tf.placeholder(tf.float32, [batch_size, H, W, C])\r\nlabels = tf.placeholder(tf.float32, [batch_size, H/stride, W/stride, 2])\r\n# vgg = vgg16.Vgg16()\r\n# vgg.build(inputs)\r\n# tmp = vgg.outputs\r\n\r\n\r\nweight1 = weight_variable([3, 3, 3, 4608])\r\nb1 = weight_variable([4608])\r\ntmp = tf.nn.conv2d(inputs, weight1, strides=(1,16,16,1), padding='SAME')\r\ntmp = tf.nn.bias_add(tmp, b1)\r\ntmp = tf.contrib.layers.batch_norm(tmp, is_training=True)\r\ntmp = tf.nn.relu(tmp)\r\nshape = np.shape(tmp)\r\n# row major\r\ntmp = tf.transpose(tmp, [2, 0, 1, 3])\r\n# reshape to RNN\r\ntmp = tf.reshape(tmp, [shape[2].value, -1, shape[3].value])\r\n# feed to RNN\r\nseqlens = np.array([shape[2].value] *(shape[0].value*shape[1].value))\r\noutputs = birnn_1d_layer(tmp, seqlens, num_hidden=256)\r\n# reshape back to 2D feature map\r\noutputs = tf.transpose(tf.reshape(outputs, [shape[2].value, shape[0].value, shape[1].value, -1]), [1,2,0,3])\r\n# outputs to Logits\r\nweight2 = weight_variable([3, 3, np.shape(outputs)[3].value, 2])\r\nb2 = weight_variable([2])\r\nlogits = tf.nn.conv2d(outputs, weight2, strides=(1,1,1,1), padding='SAME')\r\nlogits = tf.nn.bias_add(logits, b2)\r\n# logits = tf.layers.batch_normalization(logits, training=True)\r\nlogits = tf.contrib.layers.batch_norm(logits, is_training=True)\r\n# loss\r\nloss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits, dim=-1)\r\n# cost\r\ncost = tf.reduce_mean(loss)\r\n\r\nglobal_step = tf.Variable(0, trainable=False)\r\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\nwith tf.control_dependencies(update_ops):\r\n    optimizer = tf.train.AdamOptimizer(learning_rate=tf.train.exponential_decay(learning_rate, global_step, decay_steps, decay_rate)).minimize(cost, global_step=global_step)\r\n\r\ninit = tf.global_variables_initializer()\r\n\r\n# testing data\r\n_inputs = np.random.randn(batch_size, H, W, C)\r\n_labels_positive = np.random.randint(0, 1, [batch_size, np.int32(H/stride), np.int32(W/stride)]);\r\n_labels_negative = 1 - _labels_positive\r\n_labels = np.stack((_labels_negative, _labels_positive), -1)\r\n\r\nfeeds = {inputs: _inputs, labels: _labels}\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    sess.run(optimizer, feeds)\r\nprint('finished.')\r\n\r\n```"}