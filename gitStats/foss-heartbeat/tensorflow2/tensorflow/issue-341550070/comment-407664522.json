{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/407664522", "html_url": "https://github.com/tensorflow/tensorflow/issues/20842#issuecomment-407664522", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20842", "id": 407664522, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNzY2NDUyMg==", "user": {"login": "ORippler", "id": 24656669, "node_id": "MDQ6VXNlcjI0NjU2NjY5", "avatar_url": "https://avatars0.githubusercontent.com/u/24656669?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ORippler", "html_url": "https://github.com/ORippler", "followers_url": "https://api.github.com/users/ORippler/followers", "following_url": "https://api.github.com/users/ORippler/following{/other_user}", "gists_url": "https://api.github.com/users/ORippler/gists{/gist_id}", "starred_url": "https://api.github.com/users/ORippler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ORippler/subscriptions", "organizations_url": "https://api.github.com/users/ORippler/orgs", "repos_url": "https://api.github.com/users/ORippler/repos", "events_url": "https://api.github.com/users/ORippler/events{/privacy}", "received_events_url": "https://api.github.com/users/ORippler/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-25T07:43:05Z", "updated_at": "2018-07-25T07:43:49Z", "author_association": "NONE", "body_html": "<p>Okay, one last thing:<br>\nIs there a way to intermingle <code>tf.layers</code> and <code>tf.contrib.layers</code> with <code>tf.keras.layers</code> and get the expected <code>tf.keras.Model</code> behavior? Because i think this was broken upon in 1.9.0. I tried wrapping the above MWE by subclassing <code>tf.keras.Model()</code>. Code is attached, but expected behavior of <code>tf.keras</code>, which is parameter-sharing by sharing the <code>Model</code> object is not achieved.</p>\n<p>In 1.8.0, it was possible to do this simply by wrapping <code>tf.layers</code> and <code>tf.contrib.layers</code> inside a <code>Lambda</code> layer, as I did in the initial comment.</p>\n<pre><code>import tensorflow as tf\n\nConv3D = tf.keras.layers.Conv3D\nLambda = tf.keras.layers.Lambda\nActivation = tf.keras.layers.Activation\ntf_instance_norm = tf.contrib.layers.instance_norm    \n\nclass MyModel(tf.keras.Model):\n\n  def __init__(self, n_filters=1, kernel=3, padding='same', strides=1):\n    super(MyModel, self).__init__(name='my_model')\n    self.conv = Conv3D(n_filters, kernel, padding=padding, strides=strides, data_format='channels_first')\n    self.instance_norm = Lambda(lambda x: tf_instance_norm(inputs=x, data_format='NCHW'))\n\n  def call(self, inputs):\n    x = self.conv(inputs)\n    x = self.instance_norm(x)\n    x = self.conv(x)\n    x = self.instance_norm(x)\n    return x\n\nwith tf.variable_scope('reuse', reuse=tf.AUTO_REUSE) as scope:\n\n    model = MyModel()\n    \n    outputs_1 = model(tf.zeros(shape=[1,1]+[128]*3))\n    vars_conv_1 = [x.name for x in tf.global_variables(scope='reuse')]\n    outputs_2 = model(tf.zeros(shape=[1,1]+[128]*3))\n    vars_conv_2 = [x.name for x in tf.global_variables(scope='reuse')]\n    print(vars_conv_1)\n    print(vars_conv_2)\n</code></pre>", "body_text": "Okay, one last thing:\nIs there a way to intermingle tf.layers and tf.contrib.layers with tf.keras.layers and get the expected tf.keras.Model behavior? Because i think this was broken upon in 1.9.0. I tried wrapping the above MWE by subclassing tf.keras.Model(). Code is attached, but expected behavior of tf.keras, which is parameter-sharing by sharing the Model object is not achieved.\nIn 1.8.0, it was possible to do this simply by wrapping tf.layers and tf.contrib.layers inside a Lambda layer, as I did in the initial comment.\nimport tensorflow as tf\n\nConv3D = tf.keras.layers.Conv3D\nLambda = tf.keras.layers.Lambda\nActivation = tf.keras.layers.Activation\ntf_instance_norm = tf.contrib.layers.instance_norm    \n\nclass MyModel(tf.keras.Model):\n\n  def __init__(self, n_filters=1, kernel=3, padding='same', strides=1):\n    super(MyModel, self).__init__(name='my_model')\n    self.conv = Conv3D(n_filters, kernel, padding=padding, strides=strides, data_format='channels_first')\n    self.instance_norm = Lambda(lambda x: tf_instance_norm(inputs=x, data_format='NCHW'))\n\n  def call(self, inputs):\n    x = self.conv(inputs)\n    x = self.instance_norm(x)\n    x = self.conv(x)\n    x = self.instance_norm(x)\n    return x\n\nwith tf.variable_scope('reuse', reuse=tf.AUTO_REUSE) as scope:\n\n    model = MyModel()\n    \n    outputs_1 = model(tf.zeros(shape=[1,1]+[128]*3))\n    vars_conv_1 = [x.name for x in tf.global_variables(scope='reuse')]\n    outputs_2 = model(tf.zeros(shape=[1,1]+[128]*3))\n    vars_conv_2 = [x.name for x in tf.global_variables(scope='reuse')]\n    print(vars_conv_1)\n    print(vars_conv_2)", "body": "Okay, one last thing:\r\nIs there a way to intermingle `tf.layers` and `tf.contrib.layers` with `tf.keras.layers` and get the expected `tf.keras.Model` behavior? Because i think this was broken upon in 1.9.0. I tried wrapping the above MWE by subclassing `tf.keras.Model()`. Code is attached, but expected behavior of `tf.keras`, which is parameter-sharing by sharing the `Model` object is not achieved.\r\n\r\nIn 1.8.0, it was possible to do this simply by wrapping `tf.layers` and `tf.contrib.layers` inside a `Lambda` layer, as I did in the initial comment.\r\n\r\n```\r\nimport tensorflow as tf\r\n\r\nConv3D = tf.keras.layers.Conv3D\r\nLambda = tf.keras.layers.Lambda\r\nActivation = tf.keras.layers.Activation\r\ntf_instance_norm = tf.contrib.layers.instance_norm    \r\n\r\nclass MyModel(tf.keras.Model):\r\n\r\n  def __init__(self, n_filters=1, kernel=3, padding='same', strides=1):\r\n    super(MyModel, self).__init__(name='my_model')\r\n    self.conv = Conv3D(n_filters, kernel, padding=padding, strides=strides, data_format='channels_first')\r\n    self.instance_norm = Lambda(lambda x: tf_instance_norm(inputs=x, data_format='NCHW'))\r\n\r\n  def call(self, inputs):\r\n    x = self.conv(inputs)\r\n    x = self.instance_norm(x)\r\n    x = self.conv(x)\r\n    x = self.instance_norm(x)\r\n    return x\r\n\r\nwith tf.variable_scope('reuse', reuse=tf.AUTO_REUSE) as scope:\r\n\r\n    model = MyModel()\r\n    \r\n    outputs_1 = model(tf.zeros(shape=[1,1]+[128]*3))\r\n    vars_conv_1 = [x.name for x in tf.global_variables(scope='reuse')]\r\n    outputs_2 = model(tf.zeros(shape=[1,1]+[128]*3))\r\n    vars_conv_2 = [x.name for x in tf.global_variables(scope='reuse')]\r\n    print(vars_conv_1)\r\n    print(vars_conv_2)\r\n```"}