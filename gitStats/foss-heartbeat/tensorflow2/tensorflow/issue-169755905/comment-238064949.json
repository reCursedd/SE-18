{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/238064949", "html_url": "https://github.com/tensorflow/tensorflow/issues/3678#issuecomment-238064949", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3678", "id": 238064949, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODA2NDk0OQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-07T05:39:09Z", "updated_at": "2016-08-07T05:39:09Z", "author_association": "CONTRIBUTOR", "body_html": "<p>UVM is implemented through page-fault between CPU and GPU. It makes GPU program easier to be adopted, but not necessarily as fast as possible. Without more investigation, we are not sure this is suitable for high-performance machine learning. Note that the typical bandwidth across PCIE is often about two orders of magnitude slower than accessing the GPU dram itself.</p>\n<p>If the model is indeed too large and cannot fit into GPU memory, it may make sense to load parts of the model in parallel, instead of relying the page faults in the kernels to page in the data.</p>", "body_text": "UVM is implemented through page-fault between CPU and GPU. It makes GPU program easier to be adopted, but not necessarily as fast as possible. Without more investigation, we are not sure this is suitable for high-performance machine learning. Note that the typical bandwidth across PCIE is often about two orders of magnitude slower than accessing the GPU dram itself.\nIf the model is indeed too large and cannot fit into GPU memory, it may make sense to load parts of the model in parallel, instead of relying the page faults in the kernels to page in the data.", "body": "UVM is implemented through page-fault between CPU and GPU. It makes GPU program easier to be adopted, but not necessarily as fast as possible. Without more investigation, we are not sure this is suitable for high-performance machine learning. Note that the typical bandwidth across PCIE is often about two orders of magnitude slower than accessing the GPU dram itself. \n\nIf the model is indeed too large and cannot fit into GPU memory, it may make sense to load parts of the model in parallel, instead of relying the page faults in the kernels to page in the data. \n"}