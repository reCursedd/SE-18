{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3678", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3678/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3678/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3678/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/3678", "id": 169755905, "node_id": "MDU6SXNzdWUxNjk3NTU5MDU=", "number": 3678, "title": "New Feature:  Pascal, Cuda 8, Unified memory", "user": {"login": "deeplearning-ai-research", "id": 20675387, "node_id": "MDQ6VXNlcjIwNjc1Mzg3", "avatar_url": "https://avatars1.githubusercontent.com/u/20675387?v=4", "gravatar_id": "", "url": "https://api.github.com/users/deeplearning-ai-research", "html_url": "https://github.com/deeplearning-ai-research", "followers_url": "https://api.github.com/users/deeplearning-ai-research/followers", "following_url": "https://api.github.com/users/deeplearning-ai-research/following{/other_user}", "gists_url": "https://api.github.com/users/deeplearning-ai-research/gists{/gist_id}", "starred_url": "https://api.github.com/users/deeplearning-ai-research/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/deeplearning-ai-research/subscriptions", "organizations_url": "https://api.github.com/users/deeplearning-ai-research/orgs", "repos_url": "https://api.github.com/users/deeplearning-ai-research/repos", "events_url": "https://api.github.com/users/deeplearning-ai-research/events{/privacy}", "received_events_url": "https://api.github.com/users/deeplearning-ai-research/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2016-08-06T17:02:39Z", "updated_at": "2018-06-16T01:49:01Z", "closed_at": "2016-08-26T23:39:23Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>As Cuda 8 enables unified memory for Pascal GPU, combining CPU and GPU on the same address level, and enhancing the memory size available for GPU (with limited latency), using CPU RAM.</p>\n<ol>\n<li>Is there a possibility to have larger than GPU ram NN+data (lower than CPU ram) for training in tensor flow ? (it would help reducing distributed computing/network latency) ?</li>\n</ol>\n<p>ie, using the idea of  Oversubscribe GPU memory for large dataset/models.</p>\n<p>here, CUDA API :<br>\n<a href=\"http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying\" rel=\"nofollow\">http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying</a></p>\n<p>Example of 64GB allocation on GPU:</p>\n<pre><code>void foo() {\n// Allocate 64 GB on GPU, using CPU RAM\nchar *data;\nsize_t size = 64*1024*1024*1024;\ncudaMallocManaged(&amp;data, size);\n}\n</code></pre>", "body_text": "Hi,\nAs Cuda 8 enables unified memory for Pascal GPU, combining CPU and GPU on the same address level, and enhancing the memory size available for GPU (with limited latency), using CPU RAM.\n\nIs there a possibility to have larger than GPU ram NN+data (lower than CPU ram) for training in tensor flow ? (it would help reducing distributed computing/network latency) ?\n\nie, using the idea of  Oversubscribe GPU memory for large dataset/models.\nhere, CUDA API :\nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying\nExample of 64GB allocation on GPU:\nvoid foo() {\n// Allocate 64 GB on GPU, using CPU RAM\nchar *data;\nsize_t size = 64*1024*1024*1024;\ncudaMallocManaged(&data, size);\n}", "body": "Hi,\n\nAs Cuda 8 enables unified memory for Pascal GPU, combining CPU and GPU on the same address level, and enhancing the memory size available for GPU (with limited latency), using CPU RAM.\n\n1) Is there a possibility to have larger than GPU ram NN+data (lower than CPU ram) for training in tensor flow ? (it would help reducing distributed computing/network latency) ?\n\nie, using the idea of  Oversubscribe GPU memory for large dataset/models.\n\nhere, CUDA API :  \nhttp://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#um-simplifying\n\nExample of 64GB allocation on GPU:\n\n```\nvoid foo() {\n// Allocate 64 GB on GPU, using CPU RAM\nchar *data;\nsize_t size = 64*1024*1024*1024;\ncudaMallocManaged(&data, size);\n}\n```\n"}