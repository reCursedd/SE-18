{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/238619031", "html_url": "https://github.com/tensorflow/tensorflow/issues/3678#issuecomment-238619031", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/3678", "id": 238619031, "node_id": "MDEyOklzc3VlQ29tbWVudDIzODYxOTAzMQ==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-09T17:00:35Z", "updated_at": "2016-08-09T17:00:35Z", "author_association": "CONTRIBUTOR", "body_html": "<p>My understanding was that 80GB/s was between devices. The CPU/GPU communication through PCIE used by UVM is only a small fraction of that. And they are both much smaller than the 720GB/s when GPU accessing its own memory.</p>\n<p>This is an active research area, and we might still find a good use of UVM down the road. But the current belief is that it is better to page in/out memory with CPU in parallel, while the compute engine on GPU accesses its own memory in full speed. A good example is the \"swap_memory\" option in \"tf.while_loop\", which swaps the temporary memory created in the loop to the host, when the device memory is under pressure.</p>", "body_text": "My understanding was that 80GB/s was between devices. The CPU/GPU communication through PCIE used by UVM is only a small fraction of that. And they are both much smaller than the 720GB/s when GPU accessing its own memory.\nThis is an active research area, and we might still find a good use of UVM down the road. But the current belief is that it is better to page in/out memory with CPU in parallel, while the compute engine on GPU accesses its own memory in full speed. A good example is the \"swap_memory\" option in \"tf.while_loop\", which swaps the temporary memory created in the loop to the host, when the device memory is under pressure.", "body": "My understanding was that 80GB/s was between devices. The CPU/GPU communication through PCIE used by UVM is only a small fraction of that. And they are both much smaller than the 720GB/s when GPU accessing its own memory.\n\nThis is an active research area, and we might still find a good use of UVM down the road. But the current belief is that it is better to page in/out memory with CPU in parallel, while the compute engine on GPU accesses its own memory in full speed. A good example is the \"swap_memory\" option in \"tf.while_loop\", which swaps the temporary memory created in the loop to the host, when the device memory is under pressure. \n"}