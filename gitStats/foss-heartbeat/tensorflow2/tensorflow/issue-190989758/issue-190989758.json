{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5783", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5783/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5783/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5783/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/5783", "id": 190989758, "node_id": "MDU6SXNzdWUxOTA5ODk3NTg=", "number": 5783, "title": "My tensorboard appears many undefined events and charts", "user": {"login": "fengsky401", "id": 11943769, "node_id": "MDQ6VXNlcjExOTQzNzY5", "avatar_url": "https://avatars0.githubusercontent.com/u/11943769?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fengsky401", "html_url": "https://github.com/fengsky401", "followers_url": "https://api.github.com/users/fengsky401/followers", "following_url": "https://api.github.com/users/fengsky401/following{/other_user}", "gists_url": "https://api.github.com/users/fengsky401/gists{/gist_id}", "starred_url": "https://api.github.com/users/fengsky401/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fengsky401/subscriptions", "organizations_url": "https://api.github.com/users/fengsky401/orgs", "repos_url": "https://api.github.com/users/fengsky401/repos", "events_url": "https://api.github.com/users/fengsky401/events{/privacy}", "received_events_url": "https://api.github.com/users/fengsky401/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2016-11-22T12:43:35Z", "updated_at": "2016-11-23T05:10:53Z", "closed_at": "2016-11-22T20:52:34Z", "author_association": "NONE", "body_html": "<p>I only summary my loss as xentropy_mean in training ,but in tensorboard ,I had not find the xentropy_mean chart but many other charts I dont know. I don't know where I wrote wrong, and what's the matter indeed.<br>\nI wrote the inference(), training() and loss() in this file<br>\n`<br>\nfrom <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import math</p>\n<p>import tensorflow.python.platform<br>\nimport tensorflow as tf</p>\n<p>NUM_CLASSES = 16</p>\n<p>IMAGE_SIZE = 28<br>\nIMAGE_PIXELS = 784</p>\n<p>def inference(images, hidden1_units, hidden2_units):</p>\n<p>with tf.name_scope('hidden1'):<br>\nweights = tf.Variable(<br>\ntf.truncated_normal([IMAGE_PIXELS, hidden1_units],<br>\nstddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),<br>\nname='weights')<br>\nbiases = tf.Variable(tf.zeros([hidden1_units]),<br>\nname='biases')<br>\nhidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)</p>\n<p>with tf.name_scope('hidden2'):<br>\nweights = tf.Variable(<br>\ntf.truncated_normal([hidden1_units, hidden2_units],<br>\nstddev=1.0 / math.sqrt(float(hidden1_units))),<br>\nname='weights')<br>\nbiases = tf.Variable(tf.zeros([hidden2_units]),<br>\nname='biases')<br>\nhidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)</p>\n<p>with tf.name_scope('softmax_linear'):<br>\nweights = tf.Variable(<br>\ntf.truncated_normal([hidden2_units, NUM_CLASSES],<br>\nstddev=1.0 / math.sqrt(float(hidden2_units))),<br>\nname='weights')<br>\nbiases = tf.Variable(tf.zeros([NUM_CLASSES]),<br>\nname='biases')<br>\nlogits = tf.matmul(hidden2, weights) + biases<br>\nreturn logits</p>\n<p>def loss(logits, labels):</p>\n<p>batch_size = tf.size(labels)<br>\nlabels = tf.expand_dims(labels, 1)<br>\nindices = tf.expand_dims(tf.range(0, batch_size), 1)<br>\nconcated = tf.concat(1, [indices, labels])<br>\nprint('Done2')<br>\nonehot_labels = tf.sparse_to_dense(<br>\nconcated, tf.pack([batch_size, 16]), 1.0, 0.0)<br>\nprint('Done1')<br>\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,<br>\nonehot_labels,<br>\nname='xentropy')<br>\nloss = tf.reduce_mean(cross_entropy, name='xentropy_mean')</p>\n<p>return loss</p>\n<p>def training(loss, learning_rate):<br>\ntf.summary.scalar(loss.op.name, loss)<br>\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)<br>\nglobal_step = tf.Variable(0, name='global_step', trainable=False)<br>\ntrain_op = optimizer.minimize(loss, global_step=global_step)</p>\n<p>return train_op</p>\n<p>def evaluation(logits, labels):</p>\n<p>correct = tf.nn.in_top_k(logits, labels, 1)<br>\nreturn tf.reduce_sum(tf.cast(correct, tf.int32))<br>\n`</p>\n<p>and training process in this file</p>\n<p>`from <strong>future</strong> import absolute_import<br>\nfrom <strong>future</strong> import division<br>\nfrom <strong>future</strong> import print_function</p>\n<p>import argparse<br>\nimport os.path<br>\nimport sys<br>\nimport time<br>\nimport numpy as np</p>\n<p>import tensorflow as tf</p>\n<p>import mnist</p>\n<p>TRAIN_FILE = 'train.tfrecords'<br>\nVALIDATION_FILE = 'validation.tfrecords'<br>\nTEST_FILE='test.tfrecords'</p>\n<p>flags = tf.app.flags<br>\nFLAGS = flags.FLAGS</p>\n<p>flags.DEFINE_string('train_dir', '/home/queenie/image2tfrecord/tfrecords-28-gray/', 'Directory to put the training data.')<br>\nflags.DEFINE_string('filename', 'train.tfrecords', 'Directory to put the training data.')<br>\nflags.DEFINE_integer('batch_size', 100, 'Batch size.  '<br>\n'Must divide evenly into the dataset sizes.')<br>\nflags.DEFINE_integer('num_epochs', None, 'Batch size.  '<br>\n'Must divide evenly into the dataset sizes.')<br>\nflags.DEFINE_integer('hidden1', 128,'balabala')<br>\nflags.DEFINE_integer('hidden2', 32,'balabala')<br>\nflags.DEFINE_integer('learning_rate', 0.01,'balabala')<br>\nflags.DEFINE_integer('max_steps', 50000,'balabala')</p>\n<p>def placeholder_inputs(batch_size):<br>\nimages_placeholder=tf.placeholder(tf.float32,shape=(batch_size,mnist.IMAGE_PIXELS))<br>\nlabels_placeholder=tf.placeholder(tf.int32,shape=(batch_size))<br>\nreturn images_placeholder,labels_placeholder</p>\n<p>def fill_feed_dict(images_feed,labels_feed,images_pl,labels_pl):</p>\n<p>feed_dict={<br>\nimages_pl:images_feed,<br>\nlabels_pl:labels_feed,<br>\n}<br>\nreturn feed_dict</p>\n<p>def read_and_decode(filename_queue):<br>\nreader = tf.TFRecordReader()<br>\n_, serialized_example = reader.read(filename_queue)<br>\nfeatures = tf.parse_single_example(<br>\nserialized_example,<br>\n# Defaults are not specified since both keys are required.<br>\nfeatures={<br>\n'image_raw': tf.FixedLenFeature([], tf.string),<br>\n'label': tf.FixedLenFeature([], tf.int64),<br>\n})</p>\n<p>image = tf.decode_raw(features['image_raw'], tf.uint8)<br>\nimage.set_shape([mnist.IMAGE_PIXELS])</p>\n<p>image = tf.cast(image, tf.float32) * (1. / 255) - 0.5</p>\n<p>label = tf.cast(features['label'], tf.int32)</p>\n<p>return image, label</p>\n<p>def do_eval(sess,eval_correct):<br>\ntrue_count=0<br>\nfor step in xrange(FLAGS.batch_size):<br>\n#print(sess.run(eval_correct))<br>\ntrue_count+=sess.run(eval_correct)</p>\n<pre><code>precision=float(true_count)/FLAGS.batch_size/FLAGS.batch_size\nprint('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n</code></pre>\n<p>(FLAGS.batch_size, true_count, precision))<br>\nreturn precision</p>\n<p>def inputs(train, batch_size, num_epochs):</p>\n<p>if not num_epochs: num_epochs = None<br>\nif train=='train':<br>\nfilename=os.path.join(FLAGS.train_dir,TRAIN_FILE)<br>\nelif train=='validation':<br>\nfilename=os.path.join(FLAGS.train_dir,VALIDATION_FILE)<br>\nelse:<br>\nfilename=os.path.join(FLAGS.train_dir,TEST_FILE)</p>\n<p>with tf.name_scope('input'):<br>\nfilename_queue = tf.train.string_input_producer(<br>\n[filename], num_epochs=None)<br>\nimage, label = read_and_decode(filename_queue)<br>\nimages, sparse_labels = tf.train.shuffle_batch(<br>\n[image, label], batch_size=batch_size, num_threads=2,<br>\ncapacity=1000 + 3 * batch_size,</p>\n<pre><code>    min_after_dequeue=1000)\n\nreturn images, sparse_labels\n</code></pre>\n<p>def run_training():<br>\nwith tf.Graph().as_default():<br>\nimages, labels = inputs(train='train', batch_size=FLAGS.batch_size,<br>\nnum_epochs=FLAGS.num_epochs)</p>\n<pre><code>images_valid,labels_valid=inputs(train='validation', batch_size=FLAGS.batch_size,\n                         num_epochs=FLAGS.num_epochs)\n\nimages_test,labels_test=inputs(train='test', batch_size=FLAGS.batch_size,\n                         num_epochs=FLAGS.num_epochs)\n\nlogits = mnist.inference(images,\n                         FLAGS.hidden1,\n                         FLAGS.hidden2)\n\n\nvalid_prediction=mnist.inference(images_valid,FLAGS.hidden1,FLAGS.hidden2)\n\ntest_prediction=mnist.inference(images_test,FLAGS.hidden1,FLAGS.hidden2)\n\n\n\nloss = mnist.loss(logits, labels)\n\n\ntrain_op = mnist.training(loss, FLAGS.learning_rate)\n\neval_correct=mnist.evaluation(logits,labels)\n\neval_correct_valid=mnist.evaluation(valid_prediction,labels_valid)\n\neval_correct_test=mnist.evaluation(test_prediction,labels_test)\n\nsummary_op=tf.merge_all_summaries()\n\ninit_op = tf.group(tf.initialize_all_variables(),\n                   tf.initialize_local_variables())\n\nsaver = tf.train.Saver()\nsess = tf.Session()\n\n\n\nsess.run(init_op)\n\nsummary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n\n\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\ntry:\n  step = 0\n  train_precision=0\n  validation_precision=0\n  test_precision=0\n  #while not coord.should_stop():\n  while not coord.should_stop():\n    start_time = time.time()\n\n \n    _, loss_value,images_see,labels_see = sess.run([train_op, loss,images,labels])\n\n    duration = time.time() - start_time\n\n\n   \n    if step % 100 == 0:\n      print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value,\n                                                 duration))\n      precision_tr=do_eval(sess,eval_correct)\n      summary_str=sess.run(summary_op)\n      summary_writer.add_summary(summary_str,step)\n\n\n\n    if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n      checkpoint_file = os.path.join(FLAGS.train_dir, 'model.ckpt')\n      saver.save(sess, checkpoint_file, global_step=step)\n      print('Train:')\n      do_eval(sess,eval_correct)\n      print('Validation:')\n      do_eval(sess,eval_correct_valid)\n      print('Test:')\n      do_eval(sess,eval_correct_test)\n    \n\n    step += 1\n\nexcept tf.errors.OutOfRangeError:\n  print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\nfinally:\n  coord.request_stop()\n\n\ncoord.join(threads)\nsess.close()\n</code></pre>\n<p>run_training()<br>\n`</p>\n<p>and the tensorboard is as the picture</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/11943769/20524207/53013746-b0f4-11e6-8f25-d8ae5d80b27f.png\"><img src=\"https://cloud.githubusercontent.com/assets/11943769/20524207/53013746-b0f4-11e6-8f25-d8ae5d80b27f.png\" alt=\"2016-11-22 20-43-03\" style=\"max-width:100%;\"></a></p>", "body_text": "I only summary my loss as xentropy_mean in training ,but in tensorboard ,I had not find the xentropy_mean chart but many other charts I dont know. I don't know where I wrote wrong, and what's the matter indeed.\nI wrote the inference(), training() and loss() in this file\n`\nfrom future import absolute_import\nfrom future import division\nfrom future import print_function\nimport math\nimport tensorflow.python.platform\nimport tensorflow as tf\nNUM_CLASSES = 16\nIMAGE_SIZE = 28\nIMAGE_PIXELS = 784\ndef inference(images, hidden1_units, hidden2_units):\nwith tf.name_scope('hidden1'):\nweights = tf.Variable(\ntf.truncated_normal([IMAGE_PIXELS, hidden1_units],\nstddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\nname='weights')\nbiases = tf.Variable(tf.zeros([hidden1_units]),\nname='biases')\nhidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\nwith tf.name_scope('hidden2'):\nweights = tf.Variable(\ntf.truncated_normal([hidden1_units, hidden2_units],\nstddev=1.0 / math.sqrt(float(hidden1_units))),\nname='weights')\nbiases = tf.Variable(tf.zeros([hidden2_units]),\nname='biases')\nhidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\nwith tf.name_scope('softmax_linear'):\nweights = tf.Variable(\ntf.truncated_normal([hidden2_units, NUM_CLASSES],\nstddev=1.0 / math.sqrt(float(hidden2_units))),\nname='weights')\nbiases = tf.Variable(tf.zeros([NUM_CLASSES]),\nname='biases')\nlogits = tf.matmul(hidden2, weights) + biases\nreturn logits\ndef loss(logits, labels):\nbatch_size = tf.size(labels)\nlabels = tf.expand_dims(labels, 1)\nindices = tf.expand_dims(tf.range(0, batch_size), 1)\nconcated = tf.concat(1, [indices, labels])\nprint('Done2')\nonehot_labels = tf.sparse_to_dense(\nconcated, tf.pack([batch_size, 16]), 1.0, 0.0)\nprint('Done1')\ncross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\nonehot_labels,\nname='xentropy')\nloss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\nreturn loss\ndef training(loss, learning_rate):\ntf.summary.scalar(loss.op.name, loss)\noptimizer = tf.train.GradientDescentOptimizer(learning_rate)\nglobal_step = tf.Variable(0, name='global_step', trainable=False)\ntrain_op = optimizer.minimize(loss, global_step=global_step)\nreturn train_op\ndef evaluation(logits, labels):\ncorrect = tf.nn.in_top_k(logits, labels, 1)\nreturn tf.reduce_sum(tf.cast(correct, tf.int32))\n`\nand training process in this file\n`from future import absolute_import\nfrom future import division\nfrom future import print_function\nimport argparse\nimport os.path\nimport sys\nimport time\nimport numpy as np\nimport tensorflow as tf\nimport mnist\nTRAIN_FILE = 'train.tfrecords'\nVALIDATION_FILE = 'validation.tfrecords'\nTEST_FILE='test.tfrecords'\nflags = tf.app.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('train_dir', '/home/queenie/image2tfrecord/tfrecords-28-gray/', 'Directory to put the training data.')\nflags.DEFINE_string('filename', 'train.tfrecords', 'Directory to put the training data.')\nflags.DEFINE_integer('batch_size', 100, 'Batch size.  '\n'Must divide evenly into the dataset sizes.')\nflags.DEFINE_integer('num_epochs', None, 'Batch size.  '\n'Must divide evenly into the dataset sizes.')\nflags.DEFINE_integer('hidden1', 128,'balabala')\nflags.DEFINE_integer('hidden2', 32,'balabala')\nflags.DEFINE_integer('learning_rate', 0.01,'balabala')\nflags.DEFINE_integer('max_steps', 50000,'balabala')\ndef placeholder_inputs(batch_size):\nimages_placeholder=tf.placeholder(tf.float32,shape=(batch_size,mnist.IMAGE_PIXELS))\nlabels_placeholder=tf.placeholder(tf.int32,shape=(batch_size))\nreturn images_placeholder,labels_placeholder\ndef fill_feed_dict(images_feed,labels_feed,images_pl,labels_pl):\nfeed_dict={\nimages_pl:images_feed,\nlabels_pl:labels_feed,\n}\nreturn feed_dict\ndef read_and_decode(filename_queue):\nreader = tf.TFRecordReader()\n_, serialized_example = reader.read(filename_queue)\nfeatures = tf.parse_single_example(\nserialized_example,\n# Defaults are not specified since both keys are required.\nfeatures={\n'image_raw': tf.FixedLenFeature([], tf.string),\n'label': tf.FixedLenFeature([], tf.int64),\n})\nimage = tf.decode_raw(features['image_raw'], tf.uint8)\nimage.set_shape([mnist.IMAGE_PIXELS])\nimage = tf.cast(image, tf.float32) * (1. / 255) - 0.5\nlabel = tf.cast(features['label'], tf.int32)\nreturn image, label\ndef do_eval(sess,eval_correct):\ntrue_count=0\nfor step in xrange(FLAGS.batch_size):\n#print(sess.run(eval_correct))\ntrue_count+=sess.run(eval_correct)\nprecision=float(true_count)/FLAGS.batch_size/FLAGS.batch_size\nprint('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n\n(FLAGS.batch_size, true_count, precision))\nreturn precision\ndef inputs(train, batch_size, num_epochs):\nif not num_epochs: num_epochs = None\nif train=='train':\nfilename=os.path.join(FLAGS.train_dir,TRAIN_FILE)\nelif train=='validation':\nfilename=os.path.join(FLAGS.train_dir,VALIDATION_FILE)\nelse:\nfilename=os.path.join(FLAGS.train_dir,TEST_FILE)\nwith tf.name_scope('input'):\nfilename_queue = tf.train.string_input_producer(\n[filename], num_epochs=None)\nimage, label = read_and_decode(filename_queue)\nimages, sparse_labels = tf.train.shuffle_batch(\n[image, label], batch_size=batch_size, num_threads=2,\ncapacity=1000 + 3 * batch_size,\n    min_after_dequeue=1000)\n\nreturn images, sparse_labels\n\ndef run_training():\nwith tf.Graph().as_default():\nimages, labels = inputs(train='train', batch_size=FLAGS.batch_size,\nnum_epochs=FLAGS.num_epochs)\nimages_valid,labels_valid=inputs(train='validation', batch_size=FLAGS.batch_size,\n                         num_epochs=FLAGS.num_epochs)\n\nimages_test,labels_test=inputs(train='test', batch_size=FLAGS.batch_size,\n                         num_epochs=FLAGS.num_epochs)\n\nlogits = mnist.inference(images,\n                         FLAGS.hidden1,\n                         FLAGS.hidden2)\n\n\nvalid_prediction=mnist.inference(images_valid,FLAGS.hidden1,FLAGS.hidden2)\n\ntest_prediction=mnist.inference(images_test,FLAGS.hidden1,FLAGS.hidden2)\n\n\n\nloss = mnist.loss(logits, labels)\n\n\ntrain_op = mnist.training(loss, FLAGS.learning_rate)\n\neval_correct=mnist.evaluation(logits,labels)\n\neval_correct_valid=mnist.evaluation(valid_prediction,labels_valid)\n\neval_correct_test=mnist.evaluation(test_prediction,labels_test)\n\nsummary_op=tf.merge_all_summaries()\n\ninit_op = tf.group(tf.initialize_all_variables(),\n                   tf.initialize_local_variables())\n\nsaver = tf.train.Saver()\nsess = tf.Session()\n\n\n\nsess.run(init_op)\n\nsummary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\n\n\ncoord = tf.train.Coordinator()\nthreads = tf.train.start_queue_runners(sess=sess, coord=coord)\n\ntry:\n  step = 0\n  train_precision=0\n  validation_precision=0\n  test_precision=0\n  #while not coord.should_stop():\n  while not coord.should_stop():\n    start_time = time.time()\n\n \n    _, loss_value,images_see,labels_see = sess.run([train_op, loss,images,labels])\n\n    duration = time.time() - start_time\n\n\n   \n    if step % 100 == 0:\n      print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value,\n                                                 duration))\n      precision_tr=do_eval(sess,eval_correct)\n      summary_str=sess.run(summary_op)\n      summary_writer.add_summary(summary_str,step)\n\n\n\n    if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\n      checkpoint_file = os.path.join(FLAGS.train_dir, 'model.ckpt')\n      saver.save(sess, checkpoint_file, global_step=step)\n      print('Train:')\n      do_eval(sess,eval_correct)\n      print('Validation:')\n      do_eval(sess,eval_correct_valid)\n      print('Test:')\n      do_eval(sess,eval_correct_test)\n    \n\n    step += 1\n\nexcept tf.errors.OutOfRangeError:\n  print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\nfinally:\n  coord.request_stop()\n\n\ncoord.join(threads)\nsess.close()\n\nrun_training()\n`\nand the tensorboard is as the picture", "body": "I only summary my loss as xentropy_mean in training ,but in tensorboard ,I had not find the xentropy_mean chart but many other charts I dont know. I don't know where I wrote wrong, and what's the matter indeed.\r\nI wrote the inference(), training() and loss() in this file\r\n`\r\nfrom __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport math\r\n\r\nimport tensorflow.python.platform\r\nimport tensorflow as tf\r\n\r\n\r\nNUM_CLASSES = 16\r\n\r\n\r\nIMAGE_SIZE = 28\r\nIMAGE_PIXELS = 784\r\n\r\n\r\ndef inference(images, hidden1_units, hidden2_units):\r\n\r\n\r\n\r\n\r\n  with tf.name_scope('hidden1'):\r\n    weights = tf.Variable(\r\n        tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\r\n                            stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))),\r\n        name='weights')\r\n    biases = tf.Variable(tf.zeros([hidden1_units]),\r\n                         name='biases')\r\n    hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\r\n\r\n  with tf.name_scope('hidden2'):\r\n    weights = tf.Variable(\r\n        tf.truncated_normal([hidden1_units, hidden2_units],\r\n                            stddev=1.0 / math.sqrt(float(hidden1_units))),\r\n        name='weights')\r\n    biases = tf.Variable(tf.zeros([hidden2_units]),\r\n                         name='biases')\r\n    hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\r\n\r\n  with tf.name_scope('softmax_linear'):\r\n    weights = tf.Variable(\r\n        tf.truncated_normal([hidden2_units, NUM_CLASSES],\r\n                            stddev=1.0 / math.sqrt(float(hidden2_units))),\r\n        name='weights')\r\n    biases = tf.Variable(tf.zeros([NUM_CLASSES]),\r\n                         name='biases')\r\n    logits = tf.matmul(hidden2, weights) + biases\r\n  return logits\r\n\r\n\r\ndef loss(logits, labels):\r\n \r\n  batch_size = tf.size(labels)\r\n  labels = tf.expand_dims(labels, 1)\r\n  indices = tf.expand_dims(tf.range(0, batch_size), 1)\r\n  concated = tf.concat(1, [indices, labels])\r\n  print('Done2')\r\n  onehot_labels = tf.sparse_to_dense(\r\n      concated, tf.pack([batch_size, 16]), 1.0, 0.0)\r\n  print('Done1')\r\n  cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits,\r\n                                                          onehot_labels,\r\n                                                          name='xentropy')\r\n  loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\r\n  \r\n  return loss\r\n\r\n\r\ndef training(loss, learning_rate):\r\n  tf.summary.scalar(loss.op.name, loss)\r\n  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n  global_step = tf.Variable(0, name='global_step', trainable=False)\r\n  train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n  return train_op\r\n\r\n\r\ndef evaluation(logits, labels):\r\n  \r\n\r\n  correct = tf.nn.in_top_k(logits, labels, 1)\r\n  return tf.reduce_sum(tf.cast(correct, tf.int32))\r\n`\r\n\r\nand training process in this file\r\n\r\n`from __future__ import absolute_import\r\nfrom __future__ import division\r\nfrom __future__ import print_function\r\n\r\nimport argparse\r\nimport os.path\r\nimport sys\r\nimport time\r\nimport numpy as np\r\n\r\nimport tensorflow as tf\r\n\r\nimport mnist\r\n\r\n\r\n\r\n\r\n\r\nTRAIN_FILE = 'train.tfrecords'\r\nVALIDATION_FILE = 'validation.tfrecords'\r\nTEST_FILE='test.tfrecords'\r\n\r\n\r\nflags = tf.app.flags\r\nFLAGS = flags.FLAGS\r\n\r\nflags.DEFINE_string('train_dir', '/home/queenie/image2tfrecord/tfrecords-28-gray/', 'Directory to put the training data.')\r\nflags.DEFINE_string('filename', 'train.tfrecords', 'Directory to put the training data.')\r\nflags.DEFINE_integer('batch_size', 100, 'Batch size.  '\r\n                     'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_integer('num_epochs', None, 'Batch size.  '\r\n                     'Must divide evenly into the dataset sizes.')\r\nflags.DEFINE_integer('hidden1', 128,'balabala')\r\nflags.DEFINE_integer('hidden2', 32,'balabala')\r\nflags.DEFINE_integer('learning_rate', 0.01,'balabala')\r\nflags.DEFINE_integer('max_steps', 50000,'balabala')\r\n\r\n\r\ndef placeholder_inputs(batch_size):\r\n  images_placeholder=tf.placeholder(tf.float32,shape=(batch_size,mnist.IMAGE_PIXELS))\r\n  labels_placeholder=tf.placeholder(tf.int32,shape=(batch_size))\r\n  return images_placeholder,labels_placeholder\r\n\r\ndef fill_feed_dict(images_feed,labels_feed,images_pl,labels_pl):\r\n  \r\n  feed_dict={\r\n  images_pl:images_feed,\r\n  labels_pl:labels_feed,\r\n  }\r\n  return feed_dict\r\n\r\ndef read_and_decode(filename_queue):\r\n  reader = tf.TFRecordReader()\r\n  _, serialized_example = reader.read(filename_queue)\r\n  features = tf.parse_single_example(\r\n      serialized_example,\r\n      # Defaults are not specified since both keys are required.\r\n      features={\r\n          'image_raw': tf.FixedLenFeature([], tf.string),\r\n          'label': tf.FixedLenFeature([], tf.int64),\r\n      })\r\n\r\n\r\n  image = tf.decode_raw(features['image_raw'], tf.uint8)\r\n  image.set_shape([mnist.IMAGE_PIXELS])\r\n\r\n \r\n\r\n \r\n  image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\r\n\r\n  label = tf.cast(features['label'], tf.int32)\r\n\r\n  return image, label\r\n\r\n\r\ndef do_eval(sess,eval_correct):\r\n\ttrue_count=0\r\n\tfor step in xrange(FLAGS.batch_size):\r\n\t\t#print(sess.run(eval_correct))\r\n\t\ttrue_count+=sess.run(eval_correct)\r\n\r\n\tprecision=float(true_count)/FLAGS.batch_size/FLAGS.batch_size\r\n\tprint('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\r\n(FLAGS.batch_size, true_count, precision))\r\n\treturn precision\r\n\r\n\r\ndef inputs(train, batch_size, num_epochs):\r\n\r\n  if not num_epochs: num_epochs = None\r\n  if train=='train':\r\n  \tfilename=os.path.join(FLAGS.train_dir,TRAIN_FILE)\r\n  elif train=='validation':\r\n  \tfilename=os.path.join(FLAGS.train_dir,VALIDATION_FILE)\r\n  else:\r\n  \tfilename=os.path.join(FLAGS.train_dir,TEST_FILE)\r\n\r\n\r\n \r\n  with tf.name_scope('input'):\r\n    filename_queue = tf.train.string_input_producer(\r\n        [filename], num_epochs=None)\r\n    image, label = read_and_decode(filename_queue)\r\n    images, sparse_labels = tf.train.shuffle_batch(\r\n        [image, label], batch_size=batch_size, num_threads=2,\r\n        capacity=1000 + 3 * batch_size,\r\n        \r\n        min_after_dequeue=1000)\r\n\r\n    return images, sparse_labels\r\n\r\n\r\ndef run_training():\r\n  with tf.Graph().as_default():\r\n    images, labels = inputs(train='train', batch_size=FLAGS.batch_size,\r\n                            num_epochs=FLAGS.num_epochs)\r\n\r\n    images_valid,labels_valid=inputs(train='validation', batch_size=FLAGS.batch_size,\r\n                             num_epochs=FLAGS.num_epochs)\r\n    \r\n    images_test,labels_test=inputs(train='test', batch_size=FLAGS.batch_size,\r\n                             num_epochs=FLAGS.num_epochs)\r\n\r\n    logits = mnist.inference(images,\r\n                             FLAGS.hidden1,\r\n                             FLAGS.hidden2)\r\n    \r\n\r\n    valid_prediction=mnist.inference(images_valid,FLAGS.hidden1,FLAGS.hidden2)\r\n\r\n    test_prediction=mnist.inference(images_test,FLAGS.hidden1,FLAGS.hidden2)\r\n\r\n\r\n\r\n    loss = mnist.loss(logits, labels)\r\n\r\n    \r\n    train_op = mnist.training(loss, FLAGS.learning_rate)\r\n\r\n    eval_correct=mnist.evaluation(logits,labels)\r\n\r\n    eval_correct_valid=mnist.evaluation(valid_prediction,labels_valid)\r\n\r\n    eval_correct_test=mnist.evaluation(test_prediction,labels_test)\r\n\r\n    summary_op=tf.merge_all_summaries()\r\n   \r\n    init_op = tf.group(tf.initialize_all_variables(),\r\n                       tf.initialize_local_variables())\r\n\r\n    saver = tf.train.Saver()\r\n    sess = tf.Session()\r\n\r\n    \r\n  \r\n    sess.run(init_op)\r\n\r\n    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir, sess.graph)\r\n\r\n\r\n    coord = tf.train.Coordinator()\r\n    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\r\n\r\n    try:\r\n      step = 0\r\n      train_precision=0\r\n      validation_precision=0\r\n      test_precision=0\r\n      #while not coord.should_stop():\r\n      while not coord.should_stop():\r\n        start_time = time.time()\r\n\r\n     \r\n        _, loss_value,images_see,labels_see = sess.run([train_op, loss,images,labels])\r\n\r\n        duration = time.time() - start_time\r\n\r\n\r\n       \r\n        if step % 100 == 0:\r\n          print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value,\r\n                                                     duration))\r\n          precision_tr=do_eval(sess,eval_correct)\r\n          summary_str=sess.run(summary_op)\r\n          summary_writer.add_summary(summary_str,step)\r\n    \r\n\r\n\r\n        if (step + 1) % 1000 == 0 or (step + 1) == FLAGS.max_steps:\r\n          checkpoint_file = os.path.join(FLAGS.train_dir, 'model.ckpt')\r\n          saver.save(sess, checkpoint_file, global_step=step)\r\n          print('Train:')\r\n          do_eval(sess,eval_correct)\r\n          print('Validation:')\r\n          do_eval(sess,eval_correct_valid)\r\n          print('Test:')\r\n          do_eval(sess,eval_correct_test)\r\n        \r\n\r\n        step += 1\r\n\r\n    except tf.errors.OutOfRangeError:\r\n      print('Done training for %d epochs, %d steps.' % (FLAGS.num_epochs, step))\r\n    finally:\r\n      coord.request_stop()\r\n\r\n   \r\n    coord.join(threads)\r\n    sess.close()\r\n\r\n\r\nrun_training()\r\n`\r\n\r\nand the tensorboard is as the picture\r\n\r\n![2016-11-22 20-43-03](https://cloud.githubusercontent.com/assets/11943769/20524207/53013746-b0f4-11e6-8f25-d8ae5d80b27f.png)\r\n\r\n"}