{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19648", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19648/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19648/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19648/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19648", "id": 327752324, "node_id": "MDU6SXNzdWUzMjc3NTIzMjQ=", "number": 19648, "title": "kernel_constraint=maxnorm(3) raises error with eager execution", "user": {"login": "doriang102", "id": 8105797, "node_id": "MDQ6VXNlcjgxMDU3OTc=", "avatar_url": "https://avatars2.githubusercontent.com/u/8105797?v=4", "gravatar_id": "", "url": "https://api.github.com/users/doriang102", "html_url": "https://github.com/doriang102", "followers_url": "https://api.github.com/users/doriang102/followers", "following_url": "https://api.github.com/users/doriang102/following{/other_user}", "gists_url": "https://api.github.com/users/doriang102/gists{/gist_id}", "starred_url": "https://api.github.com/users/doriang102/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/doriang102/subscriptions", "organizations_url": "https://api.github.com/users/doriang102/orgs", "repos_url": "https://api.github.com/users/doriang102/repos", "events_url": "https://api.github.com/users/doriang102/events{/privacy}", "received_events_url": "https://api.github.com/users/doriang102/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 736653459, "node_id": "MDU6TGFiZWw3MzY2NTM0NTk=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:eager", "name": "comp:eager", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-05-30T14:18:01Z", "updated_at": "2018-06-01T16:16:08Z", "closed_at": "2018-06-01T16:15:30Z", "author_association": "NONE", "body_html": "<p>I posted this already on Stackoverflow and it was suggested this is indeed a bug:<br>\n<a href=\"https://stackoverflow.com/questions/50594025/how-to-include-kernel-constraints-in-tensorflow-eager-conv2d\" rel=\"nofollow\">https://stackoverflow.com/questions/50594025/how-to-include-kernel-constraints-in-tensorflow-eager-conv2d</a></p>\n<p>I'm having trouble using <code>kernel_constraint=maxnorm(3)</code> within keras when using Tensorflow eager execution. This works fine when not using the standard <code>Sequential</code> method outside of eager execution, but seems to fail with an error here (it seems to be because of a multiplication step <code>*=</code> which I don't know if there is a substitute for in this context).</p>\n<p><strong>Question:</strong> Is there a workaround to incorporate the maximum $L^2$ norm functionality within the Eager Tensorflow execution framework? Below are more details.</p>\n<p>Here is how I activate <code>tensorflow</code> eager.</p>\n<pre><code>from __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nfrom keras.datasets import cifar10\ntf.enable_eager_execution()\n</code></pre>\n<p>The following code works fine</p>\n<p><strong>Works:</strong></p>\n<pre><code>class ObjectDet(tf.keras.Model):\n    def __init__(self):\n        super(ObjectDet,self).__init__()\n        self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\n        self.layer2=tf.keras.layers.Dropout(0.2)\n        self.layer3=tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')\n        self.layer4=tf.keras.layers.MaxPooling2D(pool_size=(2,2))\n        self.layer5=tf.keras.layers.Flatten()\n        self.layer6=tf.keras.layers.Dense(512, activation='relu')\n        self.layer7=tf.keras.layers.Dropout(0.1)\n        self.layer8=tf.keras.layers.Dense(10, activation='softmax')\n\n    def call(self, input):\n        \"\"\"Run the model.\"\"\"\n        result = self.layer1(input)\n        result = self.layer2(result)\n        result = self.layer3(result)\n        result = self.layer4(result)\n        result = self.layer5(result)\n        result = self.layer6(result)\n        result = self.layer7(result)\n        result = self.layer8(result)\n    \n        return result\n\n\n\ndef loss(model, x, y):\n  prediction = model(x)\n  return cross_entropy(prediction,y)\n\ndef grad(model, inputs, targets):\n  with tf.GradientTape() as tape:\n    loss_value = loss(model, inputs, targets)\n  return tape.gradient(loss_value, model.variables)\n\n\nx, y = iter(train_ds).next()\nprint(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n\n# Training loop\nfor (i, (x, y)) in enumerate(train_ds):\n  # Calculate derivatives of the input function with respect to its parameters.\n  grads = grad(model, x, y)\n  # Apply the gradient to the model\n  \n  optimizer.apply_gradients(zip(grads, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\n  if i % 200 == 0:\n    pass\n    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n</code></pre>\n<p><strong>Does not work:</strong></p>\n<p>If I replace</p>\n<pre><code>self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\n</code></pre>\n<p>with</p>\n<pre><code>self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu',kernel_constraint=maxnorm(3))\n</code></pre>\n<p>I obtain the error:</p>\n<pre><code>RuntimeErrorTraceback (most recent call last)\n&lt;ipython-input-74-629273c4a534&gt; in &lt;module&gt;()\n     19 \n     20   optimizer.apply_gradients(zip(grads, model.variables),\n---&gt; 21                             global_step=tf.train.get_or_create_global_step())\n     22   if i % 200 == 0:\n     23     pass\n\n/home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\n    615           scope_name = var.op.name\n    616         with ops.name_scope(\"update_\" + scope_name), ops.colocate_with(var):\n--&gt; 617           update_ops.append(processor.update_op(self, grad))\n    618       if global_step is None:\n    619         apply_updates = self._finish(update_ops, name)\n\n/home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in update_op(self, optimizer, g)\n    166     if self._v.constraint is not None:\n    167       with ops.control_dependencies([update_op]):\n--&gt; 168         return self._v.assign(self._v.constraint(self._v))\n    169     else:\n    170       return update_op\n\n/home/dgoldma1/.local/lib/python2.7/site-packages/keras/constraints.pyc in __call__(self, w)\n     51         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n     52         desired = K.clip(norms, 0, self.max_value)\n---&gt; 53         w *= (desired / (K.epsilon() + norms))\n     54         return w\n     55 \n\n/home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.pyc in __imul__(self, unused_other)\n    931 \n    932   def __imul__(self, unused_other):\n--&gt; 933     raise RuntimeError(\"Variable *= value not supported. Use \"\n    934                        \"variable.assign_mul(value) to modify the variable \"\n    935                        \"value and variable = variable * value to get a new \"\n\nRuntimeError: Variable *= value not supported. Use variable.assign_mul(value) to modify the variable value and variable = variable * value to get a new Tensor object.\n</code></pre>\n<p>Thanks!</p>", "body_text": "I posted this already on Stackoverflow and it was suggested this is indeed a bug:\nhttps://stackoverflow.com/questions/50594025/how-to-include-kernel-constraints-in-tensorflow-eager-conv2d\nI'm having trouble using kernel_constraint=maxnorm(3) within keras when using Tensorflow eager execution. This works fine when not using the standard Sequential method outside of eager execution, but seems to fail with an error here (it seems to be because of a multiplication step *= which I don't know if there is a substitute for in this context).\nQuestion: Is there a workaround to incorporate the maximum $L^2$ norm functionality within the Eager Tensorflow execution framework? Below are more details.\nHere is how I activate tensorflow eager.\nfrom __future__ import absolute_import, division, print_function\nimport tensorflow as tf\nimport tensorflow.contrib.eager as tfe\nfrom keras.datasets import cifar10\ntf.enable_eager_execution()\n\nThe following code works fine\nWorks:\nclass ObjectDet(tf.keras.Model):\n    def __init__(self):\n        super(ObjectDet,self).__init__()\n        self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\n        self.layer2=tf.keras.layers.Dropout(0.2)\n        self.layer3=tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')\n        self.layer4=tf.keras.layers.MaxPooling2D(pool_size=(2,2))\n        self.layer5=tf.keras.layers.Flatten()\n        self.layer6=tf.keras.layers.Dense(512, activation='relu')\n        self.layer7=tf.keras.layers.Dropout(0.1)\n        self.layer8=tf.keras.layers.Dense(10, activation='softmax')\n\n    def call(self, input):\n        \"\"\"Run the model.\"\"\"\n        result = self.layer1(input)\n        result = self.layer2(result)\n        result = self.layer3(result)\n        result = self.layer4(result)\n        result = self.layer5(result)\n        result = self.layer6(result)\n        result = self.layer7(result)\n        result = self.layer8(result)\n    \n        return result\n\n\n\ndef loss(model, x, y):\n  prediction = model(x)\n  return cross_entropy(prediction,y)\n\ndef grad(model, inputs, targets):\n  with tf.GradientTape() as tape:\n    loss_value = loss(model, inputs, targets)\n  return tape.gradient(loss_value, model.variables)\n\n\nx, y = iter(train_ds).next()\nprint(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\n\n# Training loop\nfor (i, (x, y)) in enumerate(train_ds):\n  # Calculate derivatives of the input function with respect to its parameters.\n  grads = grad(model, x, y)\n  # Apply the gradient to the model\n  \n  optimizer.apply_gradients(zip(grads, model.variables),\n                            global_step=tf.train.get_or_create_global_step())\n  if i % 200 == 0:\n    pass\n    print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\n\nDoes not work:\nIf I replace\nself.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\n\nwith\nself.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu',kernel_constraint=maxnorm(3))\n\nI obtain the error:\nRuntimeErrorTraceback (most recent call last)\n<ipython-input-74-629273c4a534> in <module>()\n     19 \n     20   optimizer.apply_gradients(zip(grads, model.variables),\n---> 21                             global_step=tf.train.get_or_create_global_step())\n     22   if i % 200 == 0:\n     23     pass\n\n/home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\n    615           scope_name = var.op.name\n    616         with ops.name_scope(\"update_\" + scope_name), ops.colocate_with(var):\n--> 617           update_ops.append(processor.update_op(self, grad))\n    618       if global_step is None:\n    619         apply_updates = self._finish(update_ops, name)\n\n/home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in update_op(self, optimizer, g)\n    166     if self._v.constraint is not None:\n    167       with ops.control_dependencies([update_op]):\n--> 168         return self._v.assign(self._v.constraint(self._v))\n    169     else:\n    170       return update_op\n\n/home/dgoldma1/.local/lib/python2.7/site-packages/keras/constraints.pyc in __call__(self, w)\n     51         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\n     52         desired = K.clip(norms, 0, self.max_value)\n---> 53         w *= (desired / (K.epsilon() + norms))\n     54         return w\n     55 \n\n/home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.pyc in __imul__(self, unused_other)\n    931 \n    932   def __imul__(self, unused_other):\n--> 933     raise RuntimeError(\"Variable *= value not supported. Use \"\n    934                        \"variable.assign_mul(value) to modify the variable \"\n    935                        \"value and variable = variable * value to get a new \"\n\nRuntimeError: Variable *= value not supported. Use variable.assign_mul(value) to modify the variable value and variable = variable * value to get a new Tensor object.\n\nThanks!", "body": "I posted this already on Stackoverflow and it was suggested this is indeed a bug:\r\nhttps://stackoverflow.com/questions/50594025/how-to-include-kernel-constraints-in-tensorflow-eager-conv2d\r\n\r\nI'm having trouble using `kernel_constraint=maxnorm(3)` within keras when using Tensorflow eager execution. This works fine when not using the standard `Sequential` method outside of eager execution, but seems to fail with an error here (it seems to be because of a multiplication step `*=` which I don't know if there is a substitute for in this context). \r\n\r\n**Question:** Is there a workaround to incorporate the maximum $L^2$ norm functionality within the Eager Tensorflow execution framework? Below are more details.\r\n\r\nHere is how I activate `tensorflow` eager. \r\n\r\n\r\n\r\n    from __future__ import absolute_import, division, print_function\r\n    import tensorflow as tf\r\n    import tensorflow.contrib.eager as tfe\r\n    from keras.datasets import cifar10\r\n    tf.enable_eager_execution()\r\n\r\n\r\n\r\nThe following code works fine\r\n\r\n\r\n**Works:**\r\n\r\n\r\n    class ObjectDet(tf.keras.Model):\r\n        def __init__(self):\r\n            super(ObjectDet,self).__init__()\r\n            self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\r\n            self.layer2=tf.keras.layers.Dropout(0.2)\r\n            self.layer3=tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')\r\n            self.layer4=tf.keras.layers.MaxPooling2D(pool_size=(2,2))\r\n            self.layer5=tf.keras.layers.Flatten()\r\n            self.layer6=tf.keras.layers.Dense(512, activation='relu')\r\n            self.layer7=tf.keras.layers.Dropout(0.1)\r\n            self.layer8=tf.keras.layers.Dense(10, activation='softmax')\r\n\r\n        def call(self, input):\r\n            \"\"\"Run the model.\"\"\"\r\n            result = self.layer1(input)\r\n            result = self.layer2(result)\r\n            result = self.layer3(result)\r\n            result = self.layer4(result)\r\n            result = self.layer5(result)\r\n            result = self.layer6(result)\r\n            result = self.layer7(result)\r\n            result = self.layer8(result)\r\n        \r\n            return result\r\n\r\n\r\n\r\n    def loss(model, x, y):\r\n      prediction = model(x)\r\n      return cross_entropy(prediction,y)\r\n    \r\n    def grad(model, inputs, targets):\r\n      with tf.GradientTape() as tape:\r\n        loss_value = loss(model, inputs, targets)\r\n      return tape.gradient(loss_value, model.variables)\r\n    \r\n    \r\n    x, y = iter(train_ds).next()\r\n    print(\"Initial loss: {:.3f}\".format(loss(model, x, y)))\r\n    \r\n    # Training loop\r\n    for (i, (x, y)) in enumerate(train_ds):\r\n      # Calculate derivatives of the input function with respect to its parameters.\r\n      grads = grad(model, x, y)\r\n      # Apply the gradient to the model\r\n      \r\n      optimizer.apply_gradients(zip(grads, model.variables),\r\n                                global_step=tf.train.get_or_create_global_step())\r\n      if i % 200 == 0:\r\n        pass\r\n        print(\"Loss at step {:04d}: {:.3f}\".format(i, loss(model, x, y)))\r\n\r\n\r\n\r\n**Does not work:**\r\n\r\nIf I replace \r\n\r\n    self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu')\r\n\r\nwith\r\n\r\n    self.layer1= tf.keras.layers.Conv2D(32, (3, 3), input_shape=(32,32,3), padding='same', activation='relu',kernel_constraint=maxnorm(3))\r\n\r\nI obtain the error:\r\n\r\n\r\n    RuntimeErrorTraceback (most recent call last)\r\n    <ipython-input-74-629273c4a534> in <module>()\r\n         19 \r\n         20   optimizer.apply_gradients(zip(grads, model.variables),\r\n    ---> 21                             global_step=tf.train.get_or_create_global_step())\r\n         22   if i % 200 == 0:\r\n         23     pass\r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in apply_gradients(self, grads_and_vars, global_step, name)\r\n        615           scope_name = var.op.name\r\n        616         with ops.name_scope(\"update_\" + scope_name), ops.colocate_with(var):\r\n    --> 617           update_ops.append(processor.update_op(self, grad))\r\n        618       if global_step is None:\r\n        619         apply_updates = self._finish(update_ops, name)\r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/training/optimizer.pyc in update_op(self, optimizer, g)\r\n        166     if self._v.constraint is not None:\r\n        167       with ops.control_dependencies([update_op]):\r\n    --> 168         return self._v.assign(self._v.constraint(self._v))\r\n        169     else:\r\n        170       return update_op\r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/keras/constraints.pyc in __call__(self, w)\r\n         51         norms = K.sqrt(K.sum(K.square(w), axis=self.axis, keepdims=True))\r\n         52         desired = K.clip(norms, 0, self.max_value)\r\n    ---> 53         w *= (desired / (K.epsilon() + norms))\r\n         54         return w\r\n         55 \r\n    \r\n    /home/dgoldma1/.local/lib/python2.7/site-packages/tensorflow/python/ops/resource_variable_ops.pyc in __imul__(self, unused_other)\r\n        931 \r\n        932   def __imul__(self, unused_other):\r\n    --> 933     raise RuntimeError(\"Variable *= value not supported. Use \"\r\n        934                        \"variable.assign_mul(value) to modify the variable \"\r\n        935                        \"value and variable = variable * value to get a new \"\r\n    \r\n    RuntimeError: Variable *= value not supported. Use variable.assign_mul(value) to modify the variable value and variable = variable * value to get a new Tensor object.\r\n\r\n\r\nThanks!\r\n\r\n"}