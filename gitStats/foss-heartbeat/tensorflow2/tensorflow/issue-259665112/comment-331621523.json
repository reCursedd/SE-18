{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/331621523", "html_url": "https://github.com/tensorflow/tensorflow/issues/13222#issuecomment-331621523", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13222", "id": 331621523, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMTYyMTUyMw==", "user": {"login": "shamanDevel", "id": 1770337, "node_id": "MDQ6VXNlcjE3NzAzMzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1770337?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shamanDevel", "html_url": "https://github.com/shamanDevel", "followers_url": "https://api.github.com/users/shamanDevel/followers", "following_url": "https://api.github.com/users/shamanDevel/following{/other_user}", "gists_url": "https://api.github.com/users/shamanDevel/gists{/gist_id}", "starred_url": "https://api.github.com/users/shamanDevel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shamanDevel/subscriptions", "organizations_url": "https://api.github.com/users/shamanDevel/orgs", "repos_url": "https://api.github.com/users/shamanDevel/repos", "events_url": "https://api.github.com/users/shamanDevel/events{/privacy}", "received_events_url": "https://api.github.com/users/shamanDevel/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-23T09:05:53Z", "updated_at": "2017-09-23T09:05:53Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi, yes, it was me who added the GPU implementation for the SVD.<br>\nIt calls cuSOLVER internally (<a href=\"http://docs.nvidia.com/cuda/cusolver/index.html#cuds-lt-t-gt-gesvd\" rel=\"nofollow\">http://docs.nvidia.com/cuda/cusolver/index.html#cuds-lt-t-gt-gesvd</a>). I'm not aware of any direct comparisons between numpy and cuSOLVER.</p>\n<p>I'm always a bit suspicious about these timing comparisons of individual operations. GPU operations are inherently asynchronous. So all comparisons with a CPU implementation involve some overhead.<br>\nIn your case, this is a synchronization hidded when running the SVD: Tensorflow waits for the operation to be finished. And the variables are probably also in CPU memory?</p>\n<p>Further, there is another synchronization hidden in the SVD operation: It has to check if the solver has thrown an error. This also involves a memcpy from GPU to CPU. In larger scenarios, the check is delayed until the data is on the CPU (no overhead), but here, sess.run() has to wait for that as well.</p>\n<p>Running a single SVD on the GPU is probably not that optimal when compared to a CPU implementation when all the data is in CPU memory.<br>\nIt first becomes useful if all your data is only in GPU memory and the memcpy would be the largest bottleneck. That's what I experienced in my usecases.</p>", "body_text": "Hi, yes, it was me who added the GPU implementation for the SVD.\nIt calls cuSOLVER internally (http://docs.nvidia.com/cuda/cusolver/index.html#cuds-lt-t-gt-gesvd). I'm not aware of any direct comparisons between numpy and cuSOLVER.\nI'm always a bit suspicious about these timing comparisons of individual operations. GPU operations are inherently asynchronous. So all comparisons with a CPU implementation involve some overhead.\nIn your case, this is a synchronization hidded when running the SVD: Tensorflow waits for the operation to be finished. And the variables are probably also in CPU memory?\nFurther, there is another synchronization hidden in the SVD operation: It has to check if the solver has thrown an error. This also involves a memcpy from GPU to CPU. In larger scenarios, the check is delayed until the data is on the CPU (no overhead), but here, sess.run() has to wait for that as well.\nRunning a single SVD on the GPU is probably not that optimal when compared to a CPU implementation when all the data is in CPU memory.\nIt first becomes useful if all your data is only in GPU memory and the memcpy would be the largest bottleneck. That's what I experienced in my usecases.", "body": "Hi, yes, it was me who added the GPU implementation for the SVD.\r\nIt calls cuSOLVER internally (http://docs.nvidia.com/cuda/cusolver/index.html#cuds-lt-t-gt-gesvd). I'm not aware of any direct comparisons between numpy and cuSOLVER.\r\n\r\nI'm always a bit suspicious about these timing comparisons of individual operations. GPU operations are inherently asynchronous. So all comparisons with a CPU implementation involve some overhead.\r\nIn your case, this is a synchronization hidded when running the SVD: Tensorflow waits for the operation to be finished. And the variables are probably also in CPU memory?\r\n\r\nFurther, there is another synchronization hidden in the SVD operation: It has to check if the solver has thrown an error. This also involves a memcpy from GPU to CPU. In larger scenarios, the check is delayed until the data is on the CPU (no overhead), but here, sess.run() has to wait for that as well.\r\n\r\nRunning a single SVD on the GPU is probably not that optimal when compared to a CPU implementation when all the data is in CPU memory.\r\nIt first becomes useful if all your data is only in GPU memory and the memcpy would be the largest bottleneck. That's what I experienced in my usecases."}