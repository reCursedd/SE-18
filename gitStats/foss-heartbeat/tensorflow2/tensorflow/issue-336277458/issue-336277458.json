{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20346", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20346/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20346/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20346/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20346", "id": 336277458, "node_id": "MDU6SXNzdWUzMzYyNzc0NTg=", "number": 20346, "title": "Tensorflow Program Hangs on pthread_cond_wait", "user": {"login": "benbotto", "id": 14243981, "node_id": "MDQ6VXNlcjE0MjQzOTgx", "avatar_url": "https://avatars1.githubusercontent.com/u/14243981?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benbotto", "html_url": "https://github.com/benbotto", "followers_url": "https://api.github.com/users/benbotto/followers", "following_url": "https://api.github.com/users/benbotto/following{/other_user}", "gists_url": "https://api.github.com/users/benbotto/gists{/gist_id}", "starred_url": "https://api.github.com/users/benbotto/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benbotto/subscriptions", "organizations_url": "https://api.github.com/users/benbotto/orgs", "repos_url": "https://api.github.com/users/benbotto/repos", "events_url": "https://api.github.com/users/benbotto/events{/privacy}", "received_events_url": "https://api.github.com/users/benbotto/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rohan100jain", "id": 144114, "node_id": "MDQ6VXNlcjE0NDExNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/144114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rohan100jain", "html_url": "https://github.com/rohan100jain", "followers_url": "https://api.github.com/users/rohan100jain/followers", "following_url": "https://api.github.com/users/rohan100jain/following{/other_user}", "gists_url": "https://api.github.com/users/rohan100jain/gists{/gist_id}", "starred_url": "https://api.github.com/users/rohan100jain/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rohan100jain/subscriptions", "organizations_url": "https://api.github.com/users/rohan100jain/orgs", "repos_url": "https://api.github.com/users/rohan100jain/repos", "events_url": "https://api.github.com/users/rohan100jain/events{/privacy}", "received_events_url": "https://api.github.com/users/rohan100jain/received_events", "type": "User", "site_admin": false}, {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 9, "created_at": "2018-06-27T15:24:30Z", "updated_at": "2018-11-16T15:19:28Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04.4 LTS, xenial</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:  tensorflow-gpu (1.8.0)</li>\n<li><strong>Python version</strong>: Python 3.5.2</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0.176/7.1.3.16</li>\n<li><strong>GPU model and memory</strong>: GeForce GTX 850M with 2GB of memory</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.</p>\n<h3>Source code / logs</h3>\n<p>I don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: <a href=\"https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best\">https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best</a>  My code is single threaded.</p>\n<p>When last the program hung I took a stack trace.  That's available here: <a href=\"https://pastebin.com/LiPhz2CE\" rel=\"nofollow\">https://pastebin.com/LiPhz2CE</a></p>\n<p>This looks similar to this report: <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148393708\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1947\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1947/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1947\">#1947</a></p>\n<p>Let me know if more information is needed.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04.4 LTS, xenial\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below):  tensorflow-gpu (1.8.0)\nPython version: Python 3.5.2\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: 9.0.176/7.1.3.16\nGPU model and memory: GeForce GTX 850M with 2GB of memory\nExact command to reproduce:\n\nDescribe the problem\nI have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.\nSource code / logs\nI don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best  My code is single threaded.\nWhen last the program hung I took a stack trace.  That's available here: https://pastebin.com/LiPhz2CE\nThis looks similar to this report: #1947\nLet me know if more information is needed.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04.4 LTS, xenial\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**:  tensorflow-gpu (1.8.0)\r\n- **Python version**: Python 3.5.2\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: 9.0.176/7.1.3.16\r\n- **GPU model and memory**: GeForce GTX 850M with 2GB of memory\r\n- **Exact command to reproduce**:\r\n\r\n### Describe the problem\r\nI have a program using Tensorflow and it consistently hangs after a week or so, arbitrarily.  The GPU utilization goes to 0%, and the program stops training.  A stack trace shows that the program is stuck waiting on pthread_cond_wait.\r\n\r\n### Source code / logs\r\nI don't know that the source code will be of much use since it's not a concise example that reproduces the problem, but the code is available here: https://github.com/benbotto/bsy-dqn-atari/tree/breakout-best  My code is single threaded.\r\n\r\nWhen last the program hung I took a stack trace.  That's available here: https://pastebin.com/LiPhz2CE\r\n\r\nThis looks similar to this report: https://github.com/tensorflow/tensorflow/issues/1947\r\n \r\nLet me know if more information is needed."}