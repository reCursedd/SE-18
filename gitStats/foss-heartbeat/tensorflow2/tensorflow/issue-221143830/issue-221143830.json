{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9154", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9154/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9154/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9154/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9154", "id": 221143830, "node_id": "MDU6SXNzdWUyMjExNDM4MzA=", "number": 9154, "title": "consecutive calls of saver.restore(sess,path) slows down", "user": {"login": "amityaffliction", "id": 8777275, "node_id": "MDQ6VXNlcjg3NzcyNzU=", "avatar_url": "https://avatars2.githubusercontent.com/u/8777275?v=4", "gravatar_id": "", "url": "https://api.github.com/users/amityaffliction", "html_url": "https://github.com/amityaffliction", "followers_url": "https://api.github.com/users/amityaffliction/followers", "following_url": "https://api.github.com/users/amityaffliction/following{/other_user}", "gists_url": "https://api.github.com/users/amityaffliction/gists{/gist_id}", "starred_url": "https://api.github.com/users/amityaffliction/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/amityaffliction/subscriptions", "organizations_url": "https://api.github.com/users/amityaffliction/orgs", "repos_url": "https://api.github.com/users/amityaffliction/repos", "events_url": "https://api.github.com/users/amityaffliction/events{/privacy}", "received_events_url": "https://api.github.com/users/amityaffliction/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2017-04-12T03:45:50Z", "updated_at": "2017-04-12T16:59:58Z", "closed_at": "2017-04-12T16:59:58Z", "author_association": "NONE", "body_html": "<h3>You must complete this information or else your issue will be closed</h3>\n<ul>\n<li><em>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?</em>: no</li>\n<li><em>TensorFlow installed from (source or binary)?</em>: binary</li>\n<li><em>TensorFlow version</em>: 0.12.1</li>\n<li><em>Bazel version (if compiling from source)</em>:</li>\n<li><em>CUDA/cuDNN version</em>: cpu</li>\n</ul>\n<h3>Describe the problem clearly</h3>\n<p>Hi I was building and evaluating ensemble models about 45 very simple neural networks.</p>\n<p>when evaluating, I noticed that consecutive calls to <code>saver.restore(sess, path)</code> slows down</p>\n<p>at first it spent about 0.08 seconds but after 45 calls to  <code>saver.restore()</code>, time spent on restore increased to 0.5 seconds. It kept increasing to 1 second and beyond.</p>\n<p>Is anyone having the same problem? In the source code, I called <code>test_model()</code> consecutively. Other part didn't slow down but only the part with saver.restore() did</p>\n<h3>Source Code / Logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">test_model</span>(<span class=\"pl-smi\">model</span>, <span class=\"pl-smi\">batch_gen</span>, <span class=\"pl-smi\">batch_num</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">num_class</span>, <span class=\"pl-smi\">model_id</span>):\n\n    saver <span class=\"pl-k\">=</span> tf.train.Saver()\n    start_time <span class=\"pl-k\">=</span> time.time()\n\n    <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>## Testing model : <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(model_id))\n    <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n        ot <span class=\"pl-k\">=</span> time.time()\n        sess.run(tf.global_variables_initializer())\n        <span class=\"pl-c\"><span class=\"pl-c\">#</span> Load latest model to evaluate</span>\n        checkpoint_dir <span class=\"pl-k\">=</span> <span class=\"pl-c1\">CHECKPOINTS_DIR</span><span class=\"pl-k\">+</span><span class=\"pl-c1\">str</span>(model_id)<span class=\"pl-k\">+</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>/<span class=\"pl-pds\">'</span></span>\n\n        ckpt <span class=\"pl-k\">=</span> tf.train.get_checkpoint_state(os.path.dirname(checkpoint_dir))\n\n        \n        <span class=\"pl-k\">if</span> ckpt <span class=\"pl-k\">and</span> ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        <span class=\"pl-k\">else</span>:\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span># No trained weight found<span class=\"pl-pds\">'</span></span>)\n            <span class=\"pl-k\">return</span>\n        nt <span class=\"pl-k\">=</span> time.time()\n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>1 : <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">'</span></span>.format(nt<span class=\"pl-k\">-</span>ot))\n\n        vote_list <span class=\"pl-k\">=</span> []\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-v\">xrange</span>(batch_num):\n            X_batch, Y_batch <span class=\"pl-k\">=</span> batch_gen.next()\n            _, loss_batch, softmax_batch <span class=\"pl-k\">=</span> sess.run([model.optimizer, model.loss, model.softmax], <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{model.input: X_batch, model.output:Y_batch}) \n            \n            vote_batch <span class=\"pl-k\">=</span> dense_to_one_hot(np.argmax(softmax_batch,<span class=\"pl-c1\">1</span>), num_class)\n            vote_list.append(vote_batch)\n\n        total_vote_list <span class=\"pl-k\">=</span> np.concatenate(vote_list, <span class=\"pl-c1\">0</span>)\n   \n        <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span># time elapsed :<span class=\"pl-c1\">{<span class=\"pl-k\">:.1f</span>}</span> seconds<span class=\"pl-pds\">'</span></span>.format(time.time() <span class=\"pl-k\">-</span> start_time ))\n    <span class=\"pl-k\">return</span> total_vote_list</pre></div>\n<p>------------------EDIT------------------</p>\n<p>Problem was not <code>saver.restore()</code> but consecutive calling of</p>\n<div class=\"highlight highlight-source-python\"><pre>sess.run(tf.global_variables_initializer())</pre></div>\n<p>session is suppose to free all the memories right?<br>\nso I think there has to be no performance slow down but there is when making multiple sessions</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> time\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\na <span class=\"pl-k\">=</span> tf.Variable([<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">2</span>,<span class=\"pl-c1\">3</span>,<span class=\"pl-c1\">4</span>,<span class=\"pl-c1\">5</span>])\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">test</span>():\n    <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1000</span>):\n        <span class=\"pl-k\">with</span> tf.Session() <span class=\"pl-k\">as</span> sess:\n            ot <span class=\"pl-k\">=</span> time.time()\n            sess.run(tf.global_variables_initializer())\n            nt <span class=\"pl-k\">=</span> time.time()\n            <span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>test : <span class=\"pl-c1\">{<span class=\"pl-k\">:.3f</span>}</span><span class=\"pl-pds\">'</span></span>.format(nt<span class=\"pl-k\">-</span>ot))</pre></div>\n<p>running time of <code>tf.global_varaiables_initializer()</code> op slowly increases</p>", "body_text": "You must complete this information or else your issue will be closed\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow)?: no\nTensorFlow installed from (source or binary)?: binary\nTensorFlow version: 0.12.1\nBazel version (if compiling from source):\nCUDA/cuDNN version: cpu\n\nDescribe the problem clearly\nHi I was building and evaluating ensemble models about 45 very simple neural networks.\nwhen evaluating, I noticed that consecutive calls to saver.restore(sess, path) slows down\nat first it spent about 0.08 seconds but after 45 calls to  saver.restore(), time spent on restore increased to 0.5 seconds. It kept increasing to 1 second and beyond.\nIs anyone having the same problem? In the source code, I called test_model() consecutively. Other part didn't slow down but only the part with saver.restore() did\nSource Code / Logs\ndef test_model(model, batch_gen, batch_num, batch_size, num_class, model_id):\n\n    saver = tf.train.Saver()\n    start_time = time.time()\n\n    print('## Testing model : {}'.format(model_id))\n    with tf.Session() as sess:\n        ot = time.time()\n        sess.run(tf.global_variables_initializer())\n        # Load latest model to evaluate\n        checkpoint_dir = CHECKPOINTS_DIR+str(model_id)+'/'\n\n        ckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint_dir))\n\n        \n        if ckpt and ckpt.model_checkpoint_path:\n            saver.restore(sess, ckpt.model_checkpoint_path)\n        else:\n            print('# No trained weight found')\n            return\n        nt = time.time()\n        print('1 : {}'.format(nt-ot))\n\n        vote_list = []\n        for i in xrange(batch_num):\n            X_batch, Y_batch = batch_gen.next()\n            _, loss_batch, softmax_batch = sess.run([model.optimizer, model.loss, model.softmax], feed_dict={model.input: X_batch, model.output:Y_batch}) \n            \n            vote_batch = dense_to_one_hot(np.argmax(softmax_batch,1), num_class)\n            vote_list.append(vote_batch)\n\n        total_vote_list = np.concatenate(vote_list, 0)\n   \n        print('# time elapsed :{:.1f} seconds'.format(time.time() - start_time ))\n    return total_vote_list\n------------------EDIT------------------\nProblem was not saver.restore() but consecutive calling of\nsess.run(tf.global_variables_initializer())\nsession is suppose to free all the memories right?\nso I think there has to be no performance slow down but there is when making multiple sessions\nimport time\nimport tensorflow as tf\n\na = tf.Variable([1,2,3,4,5])\n\ndef test():\n    for i in range(1000):\n        with tf.Session() as sess:\n            ot = time.time()\n            sess.run(tf.global_variables_initializer())\n            nt = time.time()\n            print('test : {:.3f}'.format(nt-ot))\nrunning time of tf.global_varaiables_initializer() op slowly increases", "body": "### You must complete this information or else your issue will be closed\r\n- *Have I written custom code (as opposed to using a stock example script provided in TensorFlow)?*: no\r\n- *TensorFlow installed from (source or binary)?*: binary\r\n- *TensorFlow version*: 0.12.1\r\n- *Bazel version (if compiling from source)*:\r\n- *CUDA/cuDNN version*: cpu\r\n\r\n### Describe the problem clearly\r\nHi I was building and evaluating ensemble models about 45 very simple neural networks.\r\n\r\nwhen evaluating, I noticed that consecutive calls to ```saver.restore(sess, path)``` slows down\r\n\r\nat first it spent about 0.08 seconds but after 45 calls to  ```saver.restore()```, time spent on restore increased to 0.5 seconds. It kept increasing to 1 second and beyond. \r\n\r\nIs anyone having the same problem? In the source code, I called ```test_model()``` consecutively. Other part didn't slow down but only the part with saver.restore() did\r\n\r\n### Source Code / Logs\r\n```python\r\ndef test_model(model, batch_gen, batch_num, batch_size, num_class, model_id):\r\n\r\n    saver = tf.train.Saver()\r\n    start_time = time.time()\r\n\r\n    print('## Testing model : {}'.format(model_id))\r\n    with tf.Session() as sess:\r\n        ot = time.time()\r\n        sess.run(tf.global_variables_initializer())\r\n        # Load latest model to evaluate\r\n        checkpoint_dir = CHECKPOINTS_DIR+str(model_id)+'/'\r\n\r\n        ckpt = tf.train.get_checkpoint_state(os.path.dirname(checkpoint_dir))\r\n\r\n        \r\n        if ckpt and ckpt.model_checkpoint_path:\r\n            saver.restore(sess, ckpt.model_checkpoint_path)\r\n        else:\r\n            print('# No trained weight found')\r\n            return\r\n        nt = time.time()\r\n        print('1 : {}'.format(nt-ot))\r\n\r\n        vote_list = []\r\n        for i in xrange(batch_num):\r\n            X_batch, Y_batch = batch_gen.next()\r\n            _, loss_batch, softmax_batch = sess.run([model.optimizer, model.loss, model.softmax], feed_dict={model.input: X_batch, model.output:Y_batch}) \r\n            \r\n            vote_batch = dense_to_one_hot(np.argmax(softmax_batch,1), num_class)\r\n            vote_list.append(vote_batch)\r\n\r\n        total_vote_list = np.concatenate(vote_list, 0)\r\n   \r\n        print('# time elapsed :{:.1f} seconds'.format(time.time() - start_time ))\r\n    return total_vote_list\r\n```\r\n\r\n------------------EDIT------------------\r\n\r\nProblem was not ```saver.restore()``` but consecutive calling of\r\n```python\r\nsess.run(tf.global_variables_initializer())\r\n````\r\nsession is suppose to free all the memories right?\r\nso I think there has to be no performance slow down but there is when making multiple sessions\r\n\r\n```python\r\nimport time\r\nimport tensorflow as tf\r\n\r\na = tf.Variable([1,2,3,4,5])\r\n\r\ndef test():\r\n    for i in range(1000):\r\n        with tf.Session() as sess:\r\n            ot = time.time()\r\n            sess.run(tf.global_variables_initializer())\r\n            nt = time.time()\r\n            print('test : {:.3f}'.format(nt-ot))\r\n```\r\nrunning time of `tf.global_varaiables_initializer()` op slowly increases"}