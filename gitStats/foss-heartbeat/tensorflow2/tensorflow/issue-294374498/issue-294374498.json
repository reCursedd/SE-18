{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16772", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16772/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16772/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16772/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16772", "id": 294374498, "node_id": "MDU6SXNzdWUyOTQzNzQ0OTg=", "number": 16772, "title": "XLA with frozen protobuf: Tuples do not have a rank error", "user": {"login": "ALEXKIRNAS", "id": 17690151, "node_id": "MDQ6VXNlcjE3NjkwMTUx", "avatar_url": "https://avatars3.githubusercontent.com/u/17690151?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ALEXKIRNAS", "html_url": "https://github.com/ALEXKIRNAS", "followers_url": "https://api.github.com/users/ALEXKIRNAS/followers", "following_url": "https://api.github.com/users/ALEXKIRNAS/following{/other_user}", "gists_url": "https://api.github.com/users/ALEXKIRNAS/gists{/gist_id}", "starred_url": "https://api.github.com/users/ALEXKIRNAS/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ALEXKIRNAS/subscriptions", "organizations_url": "https://api.github.com/users/ALEXKIRNAS/orgs", "repos_url": "https://api.github.com/users/ALEXKIRNAS/repos", "events_url": "https://api.github.com/users/ALEXKIRNAS/events{/privacy}", "received_events_url": "https://api.github.com/users/ALEXKIRNAS/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "shivaniag", "id": 16565716, "node_id": "MDQ6VXNlcjE2NTY1NzE2", "avatar_url": "https://avatars1.githubusercontent.com/u/16565716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shivaniag", "html_url": "https://github.com/shivaniag", "followers_url": "https://api.github.com/users/shivaniag/followers", "following_url": "https://api.github.com/users/shivaniag/following{/other_user}", "gists_url": "https://api.github.com/users/shivaniag/gists{/gist_id}", "starred_url": "https://api.github.com/users/shivaniag/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shivaniag/subscriptions", "organizations_url": "https://api.github.com/users/shivaniag/orgs", "repos_url": "https://api.github.com/users/shivaniag/repos", "events_url": "https://api.github.com/users/shivaniag/events{/privacy}", "received_events_url": "https://api.github.com/users/shivaniag/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-02-05T12:22:53Z", "updated_at": "2018-11-14T19:15:10Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes, model inference script.</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: 16.04.3 LTS (Xenial Xerus)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source <strong>with XLA support</strong></li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.4.1</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.5.4</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609</li>\n<li><strong>CUDA/cuDNN version</strong>: cuda-8.0/cuDNN-6.0.21</li>\n<li><strong>GPU model and memory</strong>: Tesla K80, 12 Gb</li>\n<li><strong>Exact command to reproduce</strong>: <code>CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python name_of_script.py</code></li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to run model (SSD with MobileNetV1 from <a href=\"https://github.com/tensorflow/models/tree/master/research/object_detection\">tf-models</a>) with XLA optimization. I am trained this model on my own data set and generated <strong>frozen</strong> protobuf file with provide script <a href=\"https://github.com/tensorflow/models/blob/master/research/object_detection/export_inference_graph.py\">export_inference_graph.py</a>.<br>\n<strong>Note</strong>: Script that run inference works fine <strong>without</strong> XLA. Models from <code>keras.application</code> works fine <strong>with</strong> XLA, so I think there are some problems with support of frozen protobufs.</p>\n<h3>Source code / logs</h3>\n<p>Source code:<br>\nAll test completed with next session config:</p>\n<pre><code>gpu_options = tf.GPUOptions(\n            allocator_type='BFC',\n            allow_growth=True,\n            per_process_gpu_memory_fraction=True\n        )\n\nconfig = tf.ConfigProto(\n            allow_soft_placement=True,\n            gpu_options=gpu_options\n       )\n\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n</code></pre>\n<p>Log:</p>\n<pre><code>(tf-models-env) alexkirnas@host:~/scrips/tfDetector$ CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python ssd_model_inference_time_test_random.py              \n/tf-models-env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `n\np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n2018-02-05 10:55:01.159354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning\n NUMA node zero\n2018-02-05 10:55:01.160137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:00:1e.0\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\n2018-02-05 10:55:01.160163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability\n: 3.7)\n2018-02-05 10:55:03.544862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -&gt; (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability\n: 3.7)\n2018-02-05 10:55:06.752946: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x7f6454024500 executing computations on platform CUDA. Devices:\n2018-02-05 10:55:06.752997: I tensorflow/compiler/xla/service/service.cc:170]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n2018-02-05 10:55:06.757697: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: pipeli\nne start, before CallInliner]: /tmp/hlo_graph_0.GS9qJI.dot\n2018-02-05 10:55:06.757963: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: after\nCallInliner, before simplification]: /tmp/hlo_graph_1.KFEf5N.dot\n\n....\nLot of such lines as above\n.... \n\n2018-02-05 10:55:45.106105: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after\n cse, before dce]: /tmp/hlo_graph_401.V0lBLH.dot\n2018-02-05 10:55:45.112071: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after\n dce, pipeline end]: /tmp/hlo_graph_402.I2Gnpl.dot\n2018-02-05 10:55:45.117834: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [fusion: pipeline st\nart, before fusion]: /tmp/hlo_graph_403.Jmq33Y.dot\n2018-02-05 10:55:45.122108: F tensorflow/compiler/xla/shape_util.cc:118] Check failed: !ShapeUtil::IsTuple(shape) Tuples do not have a rank\nAborted (core dumped)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes, model inference script.\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): 16.04.3 LTS (Xenial Xerus)\nTensorFlow installed from (source or binary): Source with XLA support\nTensorFlow version (use command below): 1.4.1\nPython version: 3.6\nBazel version (if compiling from source): 0.5.4\nGCC/Compiler version (if compiling from source): c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\nCUDA/cuDNN version: cuda-8.0/cuDNN-6.0.21\nGPU model and memory: Tesla K80, 12 Gb\nExact command to reproduce: CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python name_of_script.py\n\nDescribe the problem\nI am trying to run model (SSD with MobileNetV1 from tf-models) with XLA optimization. I am trained this model on my own data set and generated frozen protobuf file with provide script export_inference_graph.py.\nNote: Script that run inference works fine without XLA. Models from keras.application works fine with XLA, so I think there are some problems with support of frozen protobufs.\nSource code / logs\nSource code:\nAll test completed with next session config:\ngpu_options = tf.GPUOptions(\n            allocator_type='BFC',\n            allow_growth=True,\n            per_process_gpu_memory_fraction=True\n        )\n\nconfig = tf.ConfigProto(\n            allow_soft_placement=True,\n            gpu_options=gpu_options\n       )\n\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\nLog:\n(tf-models-env) alexkirnas@host:~/scrips/tfDetector$ CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python ssd_model_inference_time_test_random.py              \n/tf-models-env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `n\np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\n2018-02-05 10:55:01.159354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning\n NUMA node zero\n2018-02-05 10:55:01.160137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\npciBusID: 0000:00:1e.0\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\n2018-02-05 10:55:01.160163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability\n: 3.7)\n2018-02-05 10:55:03.544862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability\n: 3.7)\n2018-02-05 10:55:06.752946: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x7f6454024500 executing computations on platform CUDA. Devices:\n2018-02-05 10:55:06.752997: I tensorflow/compiler/xla/service/service.cc:170]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n2018-02-05 10:55:06.757697: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: pipeli\nne start, before CallInliner]: /tmp/hlo_graph_0.GS9qJI.dot\n2018-02-05 10:55:06.757963: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: after\nCallInliner, before simplification]: /tmp/hlo_graph_1.KFEf5N.dot\n\n....\nLot of such lines as above\n.... \n\n2018-02-05 10:55:45.106105: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after\n cse, before dce]: /tmp/hlo_graph_401.V0lBLH.dot\n2018-02-05 10:55:45.112071: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after\n dce, pipeline end]: /tmp/hlo_graph_402.I2Gnpl.dot\n2018-02-05 10:55:45.117834: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [fusion: pipeline st\nart, before fusion]: /tmp/hlo_graph_403.Jmq33Y.dot\n2018-02-05 10:55:45.122108: F tensorflow/compiler/xla/shape_util.cc:118] Check failed: !ShapeUtil::IsTuple(shape) Tuples do not have a rank\nAborted (core dumped)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes, model inference script.\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: 16.04.3 LTS (Xenial Xerus)\r\n- **TensorFlow installed from (source or binary)**: Source **with XLA support**\r\n- **TensorFlow version (use command below)**: 1.4.1\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**: 0.5.4\r\n- **GCC/Compiler version (if compiling from source)**: c++ (Ubuntu 5.4.0-6ubuntu1~16.04.4) 5.4.0 20160609\r\n- **CUDA/cuDNN version**: cuda-8.0/cuDNN-6.0.21\r\n- **GPU model and memory**: Tesla K80, 12 Gb\r\n- **Exact command to reproduce**: `CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python name_of_script.py`\r\n\r\n### Describe the problem\r\nI am trying to run model (SSD with MobileNetV1 from [tf-models](https://github.com/tensorflow/models/tree/master/research/object_detection)) with XLA optimization. I am trained this model on my own data set and generated **frozen** protobuf file with provide script [export_inference_graph.py](https://github.com/tensorflow/models/blob/master/research/object_detection/export_inference_graph.py). \r\n**Note**: Script that run inference works fine **without** XLA. Models from `keras.application` works fine **with** XLA, so I think there are some problems with support of frozen protobufs. \r\n\r\n### Source code / logs\r\nSource code:\r\nAll test completed with next session config:\r\n```\r\ngpu_options = tf.GPUOptions(\r\n            allocator_type='BFC',\r\n            allow_growth=True,\r\n            per_process_gpu_memory_fraction=True\r\n        )\r\n\r\nconfig = tf.ConfigProto(\r\n            allow_soft_placement=True,\r\n            gpu_options=gpu_options\r\n       )\r\n\r\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n```\r\n\r\nLog:\r\n```\r\n(tf-models-env) alexkirnas@host:~/scrips/tfDetector$ CUDA_VISIBLE_DEVICES='0'  TF_XLA_FLAGS=--xla_generate_hlo_graph=.* python ssd_model_inference_time_test_random.py              \r\n/tf-models-env/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `n\r\np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\r\n  from ._conv import register_converters as _register_converters\r\n2018-02-05 10:55:01.159354: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning\r\n NUMA node zero\r\n2018-02-05 10:55:01.160137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties:\r\nname: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\npciBusID: 0000:00:1e.0\r\ntotalMemory: 11.17GiB freeMemory: 11.11GiB\r\n2018-02-05 10:55:01.160163: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability\r\n: 3.7)\r\n2018-02-05 10:55:03.544862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 0000:00:1e.0, compute capability\r\n: 3.7)\r\n2018-02-05 10:55:06.752946: I tensorflow/compiler/xla/service/service.cc:162] XLA service 0x7f6454024500 executing computations on platform CUDA. Devices:\r\n2018-02-05 10:55:06.752997: I tensorflow/compiler/xla/service/service.cc:170]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\r\n2018-02-05 10:55:06.757697: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: pipeli\r\nne start, before CallInliner]: /tmp/hlo_graph_0.GS9qJI.dot\r\n2018-02-05 10:55:06.757963: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_22[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v9 [optimization: after\r\nCallInliner, before simplification]: /tmp/hlo_graph_1.KFEf5N.dot\r\n\r\n....\r\nLot of such lines as above\r\n.... \r\n\r\n2018-02-05 10:55:45.106105: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after\r\n cse, before dce]: /tmp/hlo_graph_401.V0lBLH.dot\r\n2018-02-05 10:55:45.112071: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [optimization: after\r\n dce, pipeline end]: /tmp/hlo_graph_402.I2Gnpl.dot\r\n2018-02-05 10:55:45.117834: I tensorflow/compiler/xla/service/hlo_graph_dumper.cc:1254] computation cluster_9[_XlaCompiledKernel=true,_XlaNumConstantArgs=1,_XlaNumResourceArgs=0].v163 [fusion: pipeline st\r\nart, before fusion]: /tmp/hlo_graph_403.Jmq33Y.dot\r\n2018-02-05 10:55:45.122108: F tensorflow/compiler/xla/shape_util.cc:118] Check failed: !ShapeUtil::IsTuple(shape) Tuples do not have a rank\r\nAborted (core dumped)\r\n```"}