{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312958774", "html_url": "https://github.com/tensorflow/tensorflow/pull/11262#issuecomment-312958774", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11262", "id": 312958774, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjk1ODc3NA==", "user": {"login": "DavidNorman", "id": 606831, "node_id": "MDQ6VXNlcjYwNjgzMQ==", "avatar_url": "https://avatars2.githubusercontent.com/u/606831?v=4", "gravatar_id": "", "url": "https://api.github.com/users/DavidNorman", "html_url": "https://github.com/DavidNorman", "followers_url": "https://api.github.com/users/DavidNorman/followers", "following_url": "https://api.github.com/users/DavidNorman/following{/other_user}", "gists_url": "https://api.github.com/users/DavidNorman/gists{/gist_id}", "starred_url": "https://api.github.com/users/DavidNorman/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/DavidNorman/subscriptions", "organizations_url": "https://api.github.com/users/DavidNorman/orgs", "repos_url": "https://api.github.com/users/DavidNorman/repos", "events_url": "https://api.github.com/users/DavidNorman/events{/privacy}", "received_events_url": "https://api.github.com/users/DavidNorman/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-04T21:42:15Z", "updated_at": "2017-07-04T21:42:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>So, in terms of the effectiveness of this change, I think it is quite important to have a way for non-CPU/GPU devices to be tested against all of XLA.    My device actually fails quite a few of the tests because they are written with the assumption that devices support the full intersection of CPU/GPU types (which out device currently does not).  I will extend type support on our device to unsigned integer types, eliminating most of the problems, but I will probably have to submit another pull request in the future that allows devices to opt out of double precision float tests.  While our compiler supports doubles, it isn't on the timeline to support them in the graph framework.</p>", "body_text": "So, in terms of the effectiveness of this change, I think it is quite important to have a way for non-CPU/GPU devices to be tested against all of XLA.    My device actually fails quite a few of the tests because they are written with the assumption that devices support the full intersection of CPU/GPU types (which out device currently does not).  I will extend type support on our device to unsigned integer types, eliminating most of the problems, but I will probably have to submit another pull request in the future that allows devices to opt out of double precision float tests.  While our compiler supports doubles, it isn't on the timeline to support them in the graph framework.", "body": "So, in terms of the effectiveness of this change, I think it is quite important to have a way for non-CPU/GPU devices to be tested against all of XLA.    My device actually fails quite a few of the tests because they are written with the assumption that devices support the full intersection of CPU/GPU types (which out device currently does not).  I will extend type support on our device to unsigned integer types, eliminating most of the problems, but I will probably have to submit another pull request in the future that allows devices to opt out of double precision float tests.  While our compiler supports doubles, it isn't on the timeline to support them in the graph framework."}