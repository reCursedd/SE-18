{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/251544111", "html_url": "https://github.com/tensorflow/tensorflow/issues/4744#issuecomment-251544111", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744", "id": 251544111, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MTU0NDExMQ==", "user": {"login": "tatatodd", "id": 5453737, "node_id": "MDQ6VXNlcjU0NTM3Mzc=", "avatar_url": "https://avatars3.githubusercontent.com/u/5453737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatatodd", "html_url": "https://github.com/tatatodd", "followers_url": "https://api.github.com/users/tatatodd/followers", "following_url": "https://api.github.com/users/tatatodd/following{/other_user}", "gists_url": "https://api.github.com/users/tatatodd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatatodd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatatodd/subscriptions", "organizations_url": "https://api.github.com/users/tatatodd/orgs", "repos_url": "https://api.github.com/users/tatatodd/repos", "events_url": "https://api.github.com/users/tatatodd/events{/privacy}", "received_events_url": "https://api.github.com/users/tatatodd/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-04T23:37:02Z", "updated_at": "2016-10-04T23:37:02Z", "author_association": "MEMBER", "body_html": "<p>@hfutxrg have you seen the following documentation:<br>\n<a href=\"https://github.com/tensorflow/models/tree/master/inception\">https://github.com/tensorflow/models/tree/master/inception</a><br>\n<a href=\"https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch\">https://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch</a></p>\n<p>In particular the following section may be useful:</p>\n<hr>\n<p>A crucial aspect of training a network of this size is training speed in terms of wall-clock time. The training speed is dictated by many factors -- most importantly the batch size and the learning rate schedule. Both of these parameters are heavily coupled to the hardware set up.</p>\n<p>Generally speaking, a batch size is a difficult parameter to tune as it requires balancing memory demands of the model, memory available on the GPU and speed of computation. Generally speaking, employing larger batch sizes leads to more efficient computation and potentially more efficient training steps.</p>\n<p>We have tested several hardware setups for training this model from scratch but we emphasize that depending your hardware set up, you may need to adapt the batch size and learning rate schedule.</p>\n<p>Please see the comments in inception_train.py for a few selected learning rate plans based on some selected hardware setups.</p>", "body_text": "@hfutxrg have you seen the following documentation:\nhttps://github.com/tensorflow/models/tree/master/inception\nhttps://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch\nIn particular the following section may be useful:\n\nA crucial aspect of training a network of this size is training speed in terms of wall-clock time. The training speed is dictated by many factors -- most importantly the batch size and the learning rate schedule. Both of these parameters are heavily coupled to the hardware set up.\nGenerally speaking, a batch size is a difficult parameter to tune as it requires balancing memory demands of the model, memory available on the GPU and speed of computation. Generally speaking, employing larger batch sizes leads to more efficient computation and potentially more efficient training steps.\nWe have tested several hardware setups for training this model from scratch but we emphasize that depending your hardware set up, you may need to adapt the batch size and learning rate schedule.\nPlease see the comments in inception_train.py for a few selected learning rate plans based on some selected hardware setups.", "body": "@hfutxrg have you seen the following documentation:\nhttps://github.com/tensorflow/models/tree/master/inception\nhttps://github.com/tensorflow/models/tree/master/inception#how-to-train-from-scratch\n\nIn particular the following section may be useful:\n\n---\n\nA crucial aspect of training a network of this size is training speed in terms of wall-clock time. The training speed is dictated by many factors -- most importantly the batch size and the learning rate schedule. Both of these parameters are heavily coupled to the hardware set up.\n\nGenerally speaking, a batch size is a difficult parameter to tune as it requires balancing memory demands of the model, memory available on the GPU and speed of computation. Generally speaking, employing larger batch sizes leads to more efficient computation and potentially more efficient training steps.\n\nWe have tested several hardware setups for training this model from scratch but we emphasize that depending your hardware set up, you may need to adapt the batch size and learning rate schedule.\n\nPlease see the comments in inception_train.py for a few selected learning rate plans based on some selected hardware setups.\n"}