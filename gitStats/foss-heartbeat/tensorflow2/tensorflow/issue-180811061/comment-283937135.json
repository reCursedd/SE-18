{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/283937135", "html_url": "https://github.com/tensorflow/tensorflow/issues/4744#issuecomment-283937135", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744", "id": 283937135, "node_id": "MDEyOklzc3VlQ29tbWVudDI4MzkzNzEzNQ==", "user": {"login": "shashank879", "id": 9903431, "node_id": "MDQ6VXNlcjk5MDM0MzE=", "avatar_url": "https://avatars2.githubusercontent.com/u/9903431?v=4", "gravatar_id": "", "url": "https://api.github.com/users/shashank879", "html_url": "https://github.com/shashank879", "followers_url": "https://api.github.com/users/shashank879/followers", "following_url": "https://api.github.com/users/shashank879/following{/other_user}", "gists_url": "https://api.github.com/users/shashank879/gists{/gist_id}", "starred_url": "https://api.github.com/users/shashank879/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/shashank879/subscriptions", "organizations_url": "https://api.github.com/users/shashank879/orgs", "repos_url": "https://api.github.com/users/shashank879/repos", "events_url": "https://api.github.com/users/shashank879/events{/privacy}", "received_events_url": "https://api.github.com/users/shashank879/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-03T12:03:28Z", "updated_at": "2017-03-03T12:15:23Z", "author_association": "NONE", "body_html": "<p>I am also experiencing a similar issue. I get almost same total training time while training my network (not cifar) on a single GPU (TitanX 12GB) and 4 GPUs(1080 8GB each), and selecting a proper batch_size via trial and error, single GPU outperforms the 4 GPUs. I anyone is willing to look into it I can provide the necessary details.</p>", "body_text": "I am also experiencing a similar issue. I get almost same total training time while training my network (not cifar) on a single GPU (TitanX 12GB) and 4 GPUs(1080 8GB each), and selecting a proper batch_size via trial and error, single GPU outperforms the 4 GPUs. I anyone is willing to look into it I can provide the necessary details.", "body": "I am also experiencing a similar issue. I get almost same total training time while training my network (not cifar) on a single GPU (TitanX 12GB) and 4 GPUs(1080 8GB each), and selecting a proper batch_size via trial and error, single GPU outperforms the 4 GPUs. I anyone is willing to look into it I can provide the necessary details."}