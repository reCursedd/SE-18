{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/252088066", "html_url": "https://github.com/tensorflow/tensorflow/issues/4744#issuecomment-252088066", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744", "id": 252088066, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MjA4ODA2Ng==", "user": {"login": "poxvoculi", "id": 15676913, "node_id": "MDQ6VXNlcjE1Njc2OTEz", "avatar_url": "https://avatars2.githubusercontent.com/u/15676913?v=4", "gravatar_id": "", "url": "https://api.github.com/users/poxvoculi", "html_url": "https://github.com/poxvoculi", "followers_url": "https://api.github.com/users/poxvoculi/followers", "following_url": "https://api.github.com/users/poxvoculi/following{/other_user}", "gists_url": "https://api.github.com/users/poxvoculi/gists{/gist_id}", "starred_url": "https://api.github.com/users/poxvoculi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/poxvoculi/subscriptions", "organizations_url": "https://api.github.com/users/poxvoculi/orgs", "repos_url": "https://api.github.com/users/poxvoculi/repos", "events_url": "https://api.github.com/users/poxvoculi/events{/privacy}", "received_events_url": "https://api.github.com/users/poxvoculi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-06T21:04:09Z", "updated_at": "2016-10-06T21:04:09Z", "author_association": "MEMBER", "body_html": "<p>The phenomenon reported is interesting.  It's possible there's a bug in the form of a computational inaccuracy somewhere, but it also seems possible (and without further evidence more likely) that this is a natural consequence of SGD learning and the hyper-parameters you've chosen.  In particular, as you've increased the number of GPUs, you've also increased the batch size linearly.</p>\n<p>There is not a simple, direct relationship between samples/sec and training time.</p>\n<p>It's not the case that 1 batch of 64 is likely to provide equivalent progress to the learning goal as 4 batches of 16.  The learning path from initialized parameters to fully-trained parameters is through a high-dimensional space, and likely very twisty.  Computing the gradient from a larger mini-batch gives a higher expectation of a good approximation of the true gradient at a given point, but doesn't generally justify a much larger learning step, i.e. a larger learning rate, because moving just a little bit can change the true gradient, regardless of whether one's approximation from the last point was accurate.</p>\n<p>Picking a minibatch size is a pragmatic compromise, best done empirically.  Suppose for the sake of argument that computing the gradient for a minibatch of size k takes k times longer than for a single sample, and that the expected goodness of the gradient improves by 1/(k+1) whenever you increase the minibatch size by 1.  In this case it seems pretty clear that the best SGD minibatch size is 1, because the expected advantage in gradient accuracy is worth less than being able to take steps faster.  In practice, especially with SIMD devices, the time to compute a minibatch is much less than k times larger than a simple sample, so the best minibatch size will usually be larger than 1.</p>\n<p>Consider your edge cases of batch sizes 64 and 16.  At 150 samples per sec, the large batch was 0.43 sec/step, and at 52 samples per sec the small batch was 0.31 secs/step, correct?   Using the small batch, your reported training time was 657 secs or 2120 steps.  Using the large batch the training time was 1104 sec or 2700 steps.   So, the larger batch in this case seems worse, both because the wall clock time is longer, and because the number of steps to completion is larger.</p>\n<p>By the preceding argument, I'm less concerned with the inverse relation between samples/sec and training time than with steps/sec and training time.   What determines when training terminates?  Are you using the same learning rate in both cases?   It strikes me as rather suspicious that you're able to train a model in &lt; 3000 steps, is this just a retraining?</p>\n<p>You might try varying only the number of GPUs, without varying the batch size, to see what effect it has on steps to completion.  Make sure you process the same inputs, in the same order.</p>", "body_text": "The phenomenon reported is interesting.  It's possible there's a bug in the form of a computational inaccuracy somewhere, but it also seems possible (and without further evidence more likely) that this is a natural consequence of SGD learning and the hyper-parameters you've chosen.  In particular, as you've increased the number of GPUs, you've also increased the batch size linearly.\nThere is not a simple, direct relationship between samples/sec and training time.\nIt's not the case that 1 batch of 64 is likely to provide equivalent progress to the learning goal as 4 batches of 16.  The learning path from initialized parameters to fully-trained parameters is through a high-dimensional space, and likely very twisty.  Computing the gradient from a larger mini-batch gives a higher expectation of a good approximation of the true gradient at a given point, but doesn't generally justify a much larger learning step, i.e. a larger learning rate, because moving just a little bit can change the true gradient, regardless of whether one's approximation from the last point was accurate.\nPicking a minibatch size is a pragmatic compromise, best done empirically.  Suppose for the sake of argument that computing the gradient for a minibatch of size k takes k times longer than for a single sample, and that the expected goodness of the gradient improves by 1/(k+1) whenever you increase the minibatch size by 1.  In this case it seems pretty clear that the best SGD minibatch size is 1, because the expected advantage in gradient accuracy is worth less than being able to take steps faster.  In practice, especially with SIMD devices, the time to compute a minibatch is much less than k times larger than a simple sample, so the best minibatch size will usually be larger than 1.\nConsider your edge cases of batch sizes 64 and 16.  At 150 samples per sec, the large batch was 0.43 sec/step, and at 52 samples per sec the small batch was 0.31 secs/step, correct?   Using the small batch, your reported training time was 657 secs or 2120 steps.  Using the large batch the training time was 1104 sec or 2700 steps.   So, the larger batch in this case seems worse, both because the wall clock time is longer, and because the number of steps to completion is larger.\nBy the preceding argument, I'm less concerned with the inverse relation between samples/sec and training time than with steps/sec and training time.   What determines when training terminates?  Are you using the same learning rate in both cases?   It strikes me as rather suspicious that you're able to train a model in < 3000 steps, is this just a retraining?\nYou might try varying only the number of GPUs, without varying the batch size, to see what effect it has on steps to completion.  Make sure you process the same inputs, in the same order.", "body": "The phenomenon reported is interesting.  It's possible there's a bug in the form of a computational inaccuracy somewhere, but it also seems possible (and without further evidence more likely) that this is a natural consequence of SGD learning and the hyper-parameters you've chosen.  In particular, as you've increased the number of GPUs, you've also increased the batch size linearly.  \n\nThere is not a simple, direct relationship between samples/sec and training time.\n\nIt's not the case that 1 batch of 64 is likely to provide equivalent progress to the learning goal as 4 batches of 16.  The learning path from initialized parameters to fully-trained parameters is through a high-dimensional space, and likely very twisty.  Computing the gradient from a larger mini-batch gives a higher expectation of a good approximation of the true gradient at a given point, but doesn't generally justify a much larger learning step, i.e. a larger learning rate, because moving just a little bit can change the true gradient, regardless of whether one's approximation from the last point was accurate.\n\nPicking a minibatch size is a pragmatic compromise, best done empirically.  Suppose for the sake of argument that computing the gradient for a minibatch of size k takes k times longer than for a single sample, and that the expected goodness of the gradient improves by 1/(k+1) whenever you increase the minibatch size by 1.  In this case it seems pretty clear that the best SGD minibatch size is 1, because the expected advantage in gradient accuracy is worth less than being able to take steps faster.  In practice, especially with SIMD devices, the time to compute a minibatch is much less than k times larger than a simple sample, so the best minibatch size will usually be larger than 1.\n\nConsider your edge cases of batch sizes 64 and 16.  At 150 samples per sec, the large batch was 0.43 sec/step, and at 52 samples per sec the small batch was 0.31 secs/step, correct?   Using the small batch, your reported training time was 657 secs or 2120 steps.  Using the large batch the training time was 1104 sec or 2700 steps.   So, the larger batch in this case seems worse, both because the wall clock time is longer, and because the number of steps to completion is larger.\n\nBy the preceding argument, I'm less concerned with the inverse relation between samples/sec and training time than with steps/sec and training time.   What determines when training terminates?  Are you using the same learning rate in both cases?   It strikes me as rather suspicious that you're able to train a model in < 3000 steps, is this just a retraining?\n\nYou might try varying only the number of GPUs, without varying the batch size, to see what effect it has on steps to completion.  Make sure you process the same inputs, in the same order.\n"}