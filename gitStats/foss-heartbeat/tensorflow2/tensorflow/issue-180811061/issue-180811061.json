{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4744/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4744", "id": 180811061, "node_id": "MDU6SXNzdWUxODA4MTEwNjE=", "number": 4744, "title": "Poor performance with multi-GPU", "user": {"login": "renganxu", "id": 3160803, "node_id": "MDQ6VXNlcjMxNjA4MDM=", "avatar_url": "https://avatars1.githubusercontent.com/u/3160803?v=4", "gravatar_id": "", "url": "https://api.github.com/users/renganxu", "html_url": "https://github.com/renganxu", "followers_url": "https://api.github.com/users/renganxu/followers", "following_url": "https://api.github.com/users/renganxu/following{/other_user}", "gists_url": "https://api.github.com/users/renganxu/gists{/gist_id}", "starred_url": "https://api.github.com/users/renganxu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/renganxu/subscriptions", "organizations_url": "https://api.github.com/users/renganxu/orgs", "repos_url": "https://api.github.com/users/renganxu/repos", "events_url": "https://api.github.com/users/renganxu/events{/privacy}", "received_events_url": "https://api.github.com/users/renganxu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 6, "created_at": "2016-10-04T05:39:10Z", "updated_at": "2017-03-03T12:15:23Z", "closed_at": "2016-10-14T22:28:18Z", "author_association": "NONE", "body_html": "<p>I tried to train the example inception-v3 network on multiple GPUs. The following commands are used:<br>\n/* use 4 GPUs */<br>\nbazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=64 --max_steps=2000 --train_dir=... --data_dir=...</p>\n<p>/*use 2 GPUs */<br>\nexport CUDA_VISIBLE_DEVICES=0,1<br>\nbazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --max_steps=2000 --train_dir=... --data_dir=...</p>\n<p>/*use 1 GPU */<br>\nexport CUDA_VISIBLE_DEVICES=0<br>\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=16 --max_steps=2000 --train_dir=... --data_dir=...</p>\n<p>The performance results are as follows:<br>\nGPUs   Training time(s)       samples/sec<br>\n1           657                          52.7<br>\n2           844                          97.3<br>\n4           1104                        150</p>\n<p>The training time was got from the log file by subtracting the first time from the last time and therefore excluded the start-up time. For example<br>\n2016-10-03 15:28:23.239148: step 0, loss = 13.08 (4.6 examples/sec; 13.916 sec/batch)<br>\n....<br>\n2016-10-03 15:46:47.830959: step 1990, loss = 12.21 (174.6 examples/sec; 0.367 sec/batch)</p>\n<p>Now the part that I do not understand is that the samples/sec scales normally with increasing number of GPUs, but the training time does not reduce. Instead the training also increases with more GPUs.</p>\n<p>Has anyone observed the similar case?</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I tried to set --input_queue_memory_factor=0 as in the post <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"148496849\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/models/issues/47\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/47/hovercard\" href=\"https://github.com/tensorflow/models/issues/47\">tensorflow/models#47</a>, but it does not help.</p>\n<p>Also the post <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"175696993\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/4272\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/4272/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/4272\">#4272</a> has similar performance issue, but it was not solved.</p>\n<h3>Environment info</h3>\n<p>Operating System: Redhat 7.2</p>\n<p>Installed version of CUDA and cuDNN:<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -&gt; libcudart.so.8.0<br>\nlrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -&gt; libcudart.so.8.0.27<br>\n-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27<br>\n-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a<br>\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so<br>\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5<br>\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3<br>\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a</p>\n<p>If installed from source, provide</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>): <a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/a63b0cbcabc79531e155a0663a08656debf2fe07/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/a63b0cbcabc79531e155a0663a08656debf2fe07\"><tt>a63b0cb</tt></a></li>\n<li>The output of <code>bazel version</code>:</li>\n</ol>\n<p>.<br>\nBuild label: 0.3.1- (@non-git)<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Thu Sep 29 22:19:27 2016 (1475187567)<br>\nBuild timestamp: 1475187567<br>\nBuild timestamp as int: 1475187567</p>\n<h3>Logs or other output that would be helpful</h3>\n<p>By the way, before the actual computation, the overhead of the initialization took &gt;7minutes, is that normal? And I also got many warnings:<br>\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)<br>\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)<br>\nWARNING:tensorflow:Passing a <code>GraphDef</code> to the SummaryWriter is deprecated. Pass a <code>Graph</code> object instead, such as <code>sess.graph</code>.</p>\n<p>Will all these warnings impact the performance?</p>", "body_text": "I tried to train the example inception-v3 network on multiple GPUs. The following commands are used:\n/* use 4 GPUs */\nbazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=64 --max_steps=2000 --train_dir=... --data_dir=...\n/*use 2 GPUs */\nexport CUDA_VISIBLE_DEVICES=0,1\nbazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --max_steps=2000 --train_dir=... --data_dir=...\n/*use 1 GPU */\nexport CUDA_VISIBLE_DEVICES=0\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=16 --max_steps=2000 --train_dir=... --data_dir=...\nThe performance results are as follows:\nGPUs   Training time(s)       samples/sec\n1           657                          52.7\n2           844                          97.3\n4           1104                        150\nThe training time was got from the log file by subtracting the first time from the last time and therefore excluded the start-up time. For example\n2016-10-03 15:28:23.239148: step 0, loss = 13.08 (4.6 examples/sec; 13.916 sec/batch)\n....\n2016-10-03 15:46:47.830959: step 1990, loss = 12.21 (174.6 examples/sec; 0.367 sec/batch)\nNow the part that I do not understand is that the samples/sec scales normally with increasing number of GPUs, but the training time does not reduce. Instead the training also increases with more GPUs.\nHas anyone observed the similar case?\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI tried to set --input_queue_memory_factor=0 as in the post tensorflow/models#47, but it does not help.\nAlso the post #4272 has similar performance issue, but it was not solved.\nEnvironment info\nOperating System: Redhat 7.2\nInstalled version of CUDA and cuDNN:\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a\nIf installed from source, provide\n\nThe commit hash (git rev-parse HEAD): a63b0cb\nThe output of bazel version:\n\n.\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Sep 29 22:19:27 2016 (1475187567)\nBuild timestamp: 1475187567\nBuild timestamp as int: 1475187567\nLogs or other output that would be helpful\nBy the way, before the actual computation, the overhead of the initialization took >7minutes, is that normal? And I also got many warnings:\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\nWARNING:tensorflow:Passing a GraphDef to the SummaryWriter is deprecated. Pass a Graph object instead, such as sess.graph.\nWill all these warnings impact the performance?", "body": "I tried to train the example inception-v3 network on multiple GPUs. The following commands are used:\n/\\* use 4 GPUs */\nbazel-bin/inception/imagenet_train --num_gpus=4 --batch_size=64 --max_steps=2000 --train_dir=... --data_dir=...\n\n/*use 2 GPUs */\nexport CUDA_VISIBLE_DEVICES=0,1\nbazel-bin/inception/imagenet_train --num_gpus=2 --batch_size=32 --max_steps=2000 --train_dir=... --data_dir=...\n\n/*use 1 GPU */\nexport CUDA_VISIBLE_DEVICES=0\nbazel-bin/inception/imagenet_train --num_gpus=1 --batch_size=16 --max_steps=2000 --train_dir=... --data_dir=...\n\nThe performance results are as follows:\nGPUs   Training time(s)       samples/sec\n1           657                          52.7\n2           844                          97.3\n4           1104                        150\n\nThe training time was got from the log file by subtracting the first time from the last time and therefore excluded the start-up time. For example\n2016-10-03 15:28:23.239148: step 0, loss = 13.08 (4.6 examples/sec; 13.916 sec/batch)\n....\n2016-10-03 15:46:47.830959: step 1990, loss = 12.21 (174.6 examples/sec; 0.367 sec/batch)\n\nNow the part that I do not understand is that the samples/sec scales normally with increasing number of GPUs, but the training time does not reduce. Instead the training also increases with more GPUs. \n\nHas anyone observed the similar case? \n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\n\nI tried to set --input_queue_memory_factor=0 as in the post https://github.com/tensorflow/models/issues/47, but it does not help.\n\nAlso the post https://github.com/tensorflow/tensorflow/issues/4272 has similar performance issue, but it was not solved.\n### Environment info\n\nOperating System: Redhat 7.2\n\nInstalled version of CUDA and cuDNN: \n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root   560184 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudadevrt.a\nlrwxrwxrwx 1 root root       16 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so -> libcudart.so.8.0\nlrwxrwxrwx 1 root root       19 Sep 12 15:13 /usr/local/cuda-8.0/lib64/libcudart.so.8.0 -> libcudart.so.8.0.27\n-rwxr-xr-x 1 root root   394472 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart.so.8.0.27\n-rw-r--r-- 1 root root   737516 May  8 02:00 /usr/local/cuda-8.0/lib64/libcudart_static.a\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5\n-rwxr-xr-x 1 root root 60696704 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn.so.5.1.3\n-rw-r--r-- 1 root root 59715990 Sep 24 09:12 /usr/local/cuda-8.0/lib64/libcudnn_static.a\n\nIf installed from source, provide \n1. The commit hash (`git rev-parse HEAD`): a63b0cbcabc79531e155a0663a08656debf2fe07\n2. The output of `bazel version`: \n\n.\nBuild label: 0.3.1- (@non-git)\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Thu Sep 29 22:19:27 2016 (1475187567)\nBuild timestamp: 1475187567\nBuild timestamp as int: 1475187567\n### Logs or other output that would be helpful\n\nBy the way, before the actual computation, the overhead of the initialization took >7minutes, is that normal? And I also got many warnings:\nWARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\nWARNING:tensorflow:tf.variable_op_scope(values, name, default_name) is deprecated, use tf.variable_scope(name, default_name, values)\nWARNING:tensorflow:Passing a `GraphDef` to the SummaryWriter is deprecated. Pass a `Graph` object instead, such as `sess.graph`.\n\nWill all these warnings impact the performance?\n"}