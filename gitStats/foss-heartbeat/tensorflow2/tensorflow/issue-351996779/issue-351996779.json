{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21726", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21726/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21726/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21726/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21726", "id": 351996779, "node_id": "MDU6SXNzdWUzNTE5OTY3Nzk=", "number": 21726, "title": "assign_moving_average function explodes when smoothing bounded inputs", "user": {"login": "MCMcCallum", "id": 18222703, "node_id": "MDQ6VXNlcjE4MjIyNzAz", "avatar_url": "https://avatars2.githubusercontent.com/u/18222703?v=4", "gravatar_id": "", "url": "https://api.github.com/users/MCMcCallum", "html_url": "https://github.com/MCMcCallum", "followers_url": "https://api.github.com/users/MCMcCallum/followers", "following_url": "https://api.github.com/users/MCMcCallum/following{/other_user}", "gists_url": "https://api.github.com/users/MCMcCallum/gists{/gist_id}", "starred_url": "https://api.github.com/users/MCMcCallum/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/MCMcCallum/subscriptions", "organizations_url": "https://api.github.com/users/MCMcCallum/orgs", "repos_url": "https://api.github.com/users/MCMcCallum/repos", "events_url": "https://api.github.com/users/MCMcCallum/events{/privacy}", "received_events_url": "https://api.github.com/users/MCMcCallum/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-20T06:07:55Z", "updated_at": "2018-11-10T18:50:43Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>Problem encountered when running custom code</li>\n<li>Linux Ubuntu 16.04 LTS</li>\n<li>TensorFlow installed from source</li>\n<li>TensorFlow version 1.8.0</li>\n<li>Python 3.5.2</li>\n<li>Bazel Version 0.11.1</li>\n<li>GCC 4.9.4</li>\n<li>CUDA 9.1.85, CuDNN 7.1.2</li>\n<li>GPU: Gtx1070, 8GB memory</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>In training a network consisting of the architecture described below, I encountered a problem with batch normalization layers. I found that the Keras batch normalization moving average parameters were unstable. While decreasing loss was observed over several iterations, it would regularly explode to a very high value. In observing the batch normalization weights in Tensorboard I notice that the moving mean in batch normalization layers explode at the same iteration of this loss explosion, while the batch statistics remain well behaved as shown in the picture below. This doesn't add up as the moving averages are a stable function of the batch statistics.</p>\n<p>I noticed that the only Keras code employed in updating these parameters is the <code>assign_moving_average</code> function in <code>tensorflow/tensorflow/python/training/moving_averages.py</code>. Looking deeper this function in turn uses the <code>assign_sub</code> function in <code>tensorflow/tensorflow/python/ops/state_ops.py</code> which by default does not use any locking and hence its behaviour may be undefined. Perhaps this is the cause, and if so, should the default be to ignore locking? How certain is it that this wont run into undefined behaviour?</p>\n<p>When I replace the Tensorflow <code>assign_moving_average</code> function with a simple handcrafted <code>(1-self.momentum)*mean + (self.momentum)*self.moving_mean</code> in the Keras BatchNormalization object, this problem never occurs.</p>\n<h3>Source code / logs</h3>\n<p>For reference I am training a three layer convolutional network with three max pooling layers and three batch normalization layers, followed by two dense layers and two batch normalization layers at the output. The network employs a triplet loss and so the weights are each employed three times prior to back propogation. I hope to get a chance some time in the future to reproduce this problem in a simple example that I can share here.</p>\n<p>The <code>decay</code> parameter to the tensorflow <code>assign_moving_average</code> function in this case is 0.99.</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://user-images.githubusercontent.com/18222703/44322682-51db8c00-a403-11e8-9d1b-f8554b626412.png\"><img src=\"https://user-images.githubusercontent.com/18222703/44322682-51db8c00-a403-11e8-9d1b-f8554b626412.png\" alt=\"batchnormweightexplosion\" style=\"max-width:100%;\"></a></p>", "body_text": "System information\n\nProblem encountered when running custom code\nLinux Ubuntu 16.04 LTS\nTensorFlow installed from source\nTensorFlow version 1.8.0\nPython 3.5.2\nBazel Version 0.11.1\nGCC 4.9.4\nCUDA 9.1.85, CuDNN 7.1.2\nGPU: Gtx1070, 8GB memory\n\nDescribe the problem\nIn training a network consisting of the architecture described below, I encountered a problem with batch normalization layers. I found that the Keras batch normalization moving average parameters were unstable. While decreasing loss was observed over several iterations, it would regularly explode to a very high value. In observing the batch normalization weights in Tensorboard I notice that the moving mean in batch normalization layers explode at the same iteration of this loss explosion, while the batch statistics remain well behaved as shown in the picture below. This doesn't add up as the moving averages are a stable function of the batch statistics.\nI noticed that the only Keras code employed in updating these parameters is the assign_moving_average function in tensorflow/tensorflow/python/training/moving_averages.py. Looking deeper this function in turn uses the assign_sub function in tensorflow/tensorflow/python/ops/state_ops.py which by default does not use any locking and hence its behaviour may be undefined. Perhaps this is the cause, and if so, should the default be to ignore locking? How certain is it that this wont run into undefined behaviour?\nWhen I replace the Tensorflow assign_moving_average function with a simple handcrafted (1-self.momentum)*mean + (self.momentum)*self.moving_mean in the Keras BatchNormalization object, this problem never occurs.\nSource code / logs\nFor reference I am training a three layer convolutional network with three max pooling layers and three batch normalization layers, followed by two dense layers and two batch normalization layers at the output. The network employs a triplet loss and so the weights are each employed three times prior to back propogation. I hope to get a chance some time in the future to reproduce this problem in a simple example that I can share here.\nThe decay parameter to the tensorflow assign_moving_average function in this case is 0.99.", "body": "### System information\r\n- Problem encountered when running custom code\r\n- Linux Ubuntu 16.04 LTS\r\n- TensorFlow installed from source\r\n- TensorFlow version 1.8.0\r\n- Python 3.5.2 \r\n- Bazel Version 0.11.1\r\n- GCC 4.9.4\r\n- CUDA 9.1.85, CuDNN 7.1.2\r\n- GPU: Gtx1070, 8GB memory\r\n\r\n### Describe the problem\r\nIn training a network consisting of the architecture described below, I encountered a problem with batch normalization layers. I found that the Keras batch normalization moving average parameters were unstable. While decreasing loss was observed over several iterations, it would regularly explode to a very high value. In observing the batch normalization weights in Tensorboard I notice that the moving mean in batch normalization layers explode at the same iteration of this loss explosion, while the batch statistics remain well behaved as shown in the picture below. This doesn't add up as the moving averages are a stable function of the batch statistics.\r\n\r\nI noticed that the only Keras code employed in updating these parameters is the `assign_moving_average` function in `tensorflow/tensorflow/python/training/moving_averages.py`. Looking deeper this function in turn uses the `assign_sub` function in `tensorflow/tensorflow/python/ops/state_ops.py` which by default does not use any locking and hence its behaviour may be undefined. Perhaps this is the cause, and if so, should the default be to ignore locking? How certain is it that this wont run into undefined behaviour?\r\n\r\nWhen I replace the Tensorflow `assign_moving_average` function with a simple handcrafted `(1-self.momentum)*mean + (self.momentum)*self.moving_mean` in the Keras BatchNormalization object, this problem never occurs.\r\n\r\n### Source code / logs\r\nFor reference I am training a three layer convolutional network with three max pooling layers and three batch normalization layers, followed by two dense layers and two batch normalization layers at the output. The network employs a triplet loss and so the weights are each employed three times prior to back propogation. I hope to get a chance some time in the future to reproduce this problem in a simple example that I can share here.\r\n\r\nThe `decay` parameter to the tensorflow `assign_moving_average` function in this case is 0.99.\r\n\r\n![batchnormweightexplosion](https://user-images.githubusercontent.com/18222703/44322682-51db8c00-a403-11e8-9d1b-f8554b626412.png)\r\n"}