{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/273256884", "html_url": "https://github.com/tensorflow/tensorflow/issues/6903#issuecomment-273256884", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6903", "id": 273256884, "node_id": "MDEyOklzc3VlQ29tbWVudDI3MzI1Njg4NA==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-17T18:33:16Z", "updated_at": "2017-01-17T18:33:58Z", "author_association": "CONTRIBUTOR", "body_html": "<p>It's counter-intuitive, but it would be hard to fix without significantly changing the underlying framework.</p>\n<p>Technically <code>tf.gradients</code> creates a backprop graph, and the result of running this graph can be different from mathematical gradient. For instance, it can produce <code>None</code> values which indicate that given input is not connected to output (as opposed to 0 which means it's connected, but has no numeric effect on it).</p>\n<p>Here, the graph you get is below, and when you feed <code>b</code> that severs the forward prop chain of computation, but there are still connections from the backprop chain</p>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/22034230/72b25b9c-dca0-11e6-989f-188533343e05.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/22034230/72b25b9c-dca0-11e6-989f-188533343e05.png\" alt=\"gradients\" style=\"max-width:100%;\"></a></p>", "body_text": "It's counter-intuitive, but it would be hard to fix without significantly changing the underlying framework.\nTechnically tf.gradients creates a backprop graph, and the result of running this graph can be different from mathematical gradient. For instance, it can produce None values which indicate that given input is not connected to output (as opposed to 0 which means it's connected, but has no numeric effect on it).\nHere, the graph you get is below, and when you feed b that severs the forward prop chain of computation, but there are still connections from the backprop chain", "body": "It's counter-intuitive, but it would be hard to fix without significantly changing the underlying framework.\r\n\r\nTechnically `tf.gradients` creates a backprop graph, and the result of running this graph can be different from mathematical gradient. For instance, it can produce `None` values which indicate that given input is not connected to output (as opposed to 0 which means it's connected, but has no numeric effect on it).\r\n\r\nHere, the graph you get is below, and when you feed `b` that severs the forward prop chain of computation, but there are still connections from the backprop chain\r\n\r\n![gradients](https://cloud.githubusercontent.com/assets/23068/22034230/72b25b9c-dca0-11e6-989f-188533343e05.png)\r\n"}