{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/274120931", "html_url": "https://github.com/tensorflow/tensorflow/issues/6903#issuecomment-274120931", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6903", "id": 274120931, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NDEyMDkzMQ==", "user": {"login": "gaohuazuo", "id": 10446514, "node_id": "MDQ6VXNlcjEwNDQ2NTE0", "avatar_url": "https://avatars0.githubusercontent.com/u/10446514?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gaohuazuo", "html_url": "https://github.com/gaohuazuo", "followers_url": "https://api.github.com/users/gaohuazuo/followers", "following_url": "https://api.github.com/users/gaohuazuo/following{/other_user}", "gists_url": "https://api.github.com/users/gaohuazuo/gists{/gist_id}", "starred_url": "https://api.github.com/users/gaohuazuo/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gaohuazuo/subscriptions", "organizations_url": "https://api.github.com/users/gaohuazuo/orgs", "repos_url": "https://api.github.com/users/gaohuazuo/repos", "events_url": "https://api.github.com/users/gaohuazuo/events{/privacy}", "received_events_url": "https://api.github.com/users/gaohuazuo/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-20T16:50:29Z", "updated_at": "2017-01-20T16:50:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Even when feeding 0 to gradients, <code>d_b / d_a</code> still sticks around while being redundant. The only way I can think of is to construct a copy of the graph and simply don't mix gradients with feeding.</p>\n<p>BTW, seems that the gradient of <code>cond</code> can be optimized harder.</p>\n<pre><code>a = tf.constant(0., name='a')\np = tf.constant(True, name='p')\nb = tf.cond(p, lambda: tf.square(a), lambda: tf.constant(0.), name='b')\nc = tf.square(b, name='c')\ng, = tf.gradients(c, a, name='g')\n</code></pre>\n<p><a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/10446514/22157412/8ca1aa10-df72-11e6-9780-5641eef6f9c3.png\"><img src=\"https://cloud.githubusercontent.com/assets/10446514/22157412/8ca1aa10-df72-11e6-9780-5641eef6f9c3.png\" alt=\"graph-run\" style=\"max-width:100%;\"></a></p>\n<p>This is a trace from <code>g.eval(feed_dict={p: False})</code>. The <code>c_grad</code> subgraph is not actually useful, but anyway get executed (not shaded in tensorboard).</p>", "body_text": "Even when feeding 0 to gradients, d_b / d_a still sticks around while being redundant. The only way I can think of is to construct a copy of the graph and simply don't mix gradients with feeding.\nBTW, seems that the gradient of cond can be optimized harder.\na = tf.constant(0., name='a')\np = tf.constant(True, name='p')\nb = tf.cond(p, lambda: tf.square(a), lambda: tf.constant(0.), name='b')\nc = tf.square(b, name='c')\ng, = tf.gradients(c, a, name='g')\n\n\nThis is a trace from g.eval(feed_dict={p: False}). The c_grad subgraph is not actually useful, but anyway get executed (not shaded in tensorboard).", "body": "Even when feeding 0 to gradients, `d_b / d_a` still sticks around while being redundant. The only way I can think of is to construct a copy of the graph and simply don't mix gradients with feeding.\r\n\r\nBTW, seems that the gradient of `cond` can be optimized harder.\r\n\r\n```\r\na = tf.constant(0., name='a')\r\np = tf.constant(True, name='p')\r\nb = tf.cond(p, lambda: tf.square(a), lambda: tf.constant(0.), name='b')\r\nc = tf.square(b, name='c')\r\ng, = tf.gradients(c, a, name='g')\r\n```\r\n\r\n![graph-run](https://cloud.githubusercontent.com/assets/10446514/22157412/8ca1aa10-df72-11e6-9780-5641eef6f9c3.png)\r\n\r\nThis is a trace from `g.eval(feed_dict={p: False})`. The `c_grad` subgraph is not actually useful, but anyway get executed (not shaded in tensorboard)."}