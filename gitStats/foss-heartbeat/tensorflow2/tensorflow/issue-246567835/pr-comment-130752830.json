{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/130752830", "pull_request_review_id": 53662768, "id": 130752830, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMDc1MjgzMA==", "diff_hunk": "@@ -0,0 +1,249 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/linalg_ops.cc.\n+// TODO(shamanDevel): Enable complex inputs. This will require additional tests\n+//                    and OP_REQUIRES.\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+\n+#include <algorithm>\n+\n+#include \"tensorflow/core/framework/kernel_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/kernels/linalg_ops_common.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+#include \"tensorflow/core/kernels/cuda_solvers.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+// I need to transpose V afterwards\n+#include <vector>\n+#include \"transpose_functor.h\"\n+\n+// Logging\n+#include <stdio.h>\n+\n+namespace tensorflow {\n+\n+static const char kErrMsg[] =\n+    \"Singular Value Decomposition was not successful. The input might not be \"\n+    \"valid.\";\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+template <class Scalar>\n+class SvdOpGpu : public AsyncOpKernel {\n+ public:\n+  explicit SvdOpGpu(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"compute_uv\", &compute_uv_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"full_matrices\", &full_matrices_));\n+  }\n+\n+  void ComputeAsync(OpKernelContext* context, DoneCallback done) final {\n+    const Tensor& input = context->input(0);\n+    const int ndims = input.dims();\n+    const int64 m = input.dim_size(ndims - 2);\n+    const int64 n = input.dim_size(ndims - 1);\n+    const int64 p = std::min(m, n);\n+\n+    // This is a limitation of cuSolver\n+    OP_REQUIRES_ASYNC(\n+        context, m >= n,\n+        errors::InvalidArgument(\n+            \"GPU implementation of SVD only supports m>=n, but m=\", m, \", n=\",\n+            n),\n+        done);\n+\n+    // Validate inputs.\n+    OP_REQUIRES_ASYNC(\n+        context, ndims >= 2,\n+        errors::InvalidArgument(\"Input must have rank >= 2, got \", ndims),\n+        done);\n+\n+    // output tensors.\n+    Tensor* outputU = NULL;\n+    Tensor* outputS = NULL;\n+    Tensor outputVT;\n+    Tensor* outputV = NULL;\n+\n+    // compute  shapes\n+    TensorShape shapeRaw = input.shape();\n+    shapeRaw.RemoveDim(shapeRaw.dims() - 1);\n+    shapeRaw.RemoveDim(shapeRaw.dims() - 1);\n+    TensorShape shapeS = shapeRaw;\n+    TensorShape shapeU = shapeRaw;\n+    TensorShape shapeVT = shapeRaw;\n+    TensorShape shapeV = shapeRaw;\n+    shapeS.AddDim(p);\n+    if (compute_uv_) {\n+      if (full_matrices_) {\n+        shapeU.AddDim(m);\n+        shapeU.AddDim(m);\n+        shapeVT.AddDim(n);\n+        shapeVT.AddDim(n);\n+        shapeV.AddDim(n);\n+        shapeV.AddDim(n);\n+      } else {\n+        shapeU.AddDim(m);\n+        shapeU.AddDim(p);\n+        shapeVT.AddDim(p);\n+        shapeVT.AddDim(n);\n+        shapeV.AddDim(n);\n+        shapeV.AddDim(p);\n+      }\n+    } else {\n+      shapeU = TensorShape({0});\n+      shapeV = TensorShape({0});\n+    }\n+\n+    // allocate output\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(0, shapeS, &outputS),\n+                         done);\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(1, shapeU, &outputU),\n+                         done);\n+    OP_REQUIRES_OK_ASYNC(context, context->allocate_output(2, shapeV, &outputV),\n+                         done);\n+    if (compute_uv_) {\n+      OP_REQUIRES_OK_ASYNC(\n+          context, context->allocate_temp(input.dtype(), shapeVT, &outputVT),\n+          done);\n+    }\n+\n+    if (n == 0 || m == 0) {\n+      // If X is an empty matrix (0 rows, 0 col), X * X' == X.\n+      // Therefore, we return X.\n+      done();\n+      return;\n+    }\n+\n+    // Copy and reshape input tensor\n+    // SVD modifies the input, so I need to copy it.\n+    Tensor inputCopy;\n+    OP_REQUIRES_OK_ASYNC(\n+        context,\n+        context->allocate_temp(input.dtype(), input.shape(), &inputCopy), done);\n+\n+    cudaMemcpy(inputCopy.flat<Scalar>().data(), input.flat<Scalar>().data(),\n+               input.NumElements() * sizeof(Scalar), cudaMemcpyDeviceToDevice);\n+\n+    auto input_reshaped = inputCopy.template flat_inner_dims<Scalar, 3>();\n+\n+    // Reshape output tensors\n+    auto outputS_reshaped = outputS->template flat_inner_dims<Scalar, 2>();\n+    Scalar* outputU_reshaped_ptr = NULL;\n+    Scalar* outputVT_reshaped_ptr = NULL;\n+    if (compute_uv_) {\n+      auto outputU_reshaped = outputU->template flat_inner_dims<Scalar, 3>();\n+      auto outputVT_reshaped = outputVT.template flat_inner_dims<Scalar, 3>();\n+      outputU_reshaped_ptr = outputU_reshaped.data();\n+      outputVT_reshaped_ptr = outputVT_reshaped.data();\n+    }\n+\n+    // Launch a SVD kernel for each matrix in the batch.", "path": "tensorflow/core/kernels/svd_op_gpu.cu.cc", "position": null, "original_position": 160, "commit_id": "bfda71b1104c7d1d0210439c22cded14c87c7db7", "original_commit_id": "61a77cb6c1b65678f41e60bbd9a93c1052fe8aa8", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "nit: s/n/an/", "created_at": "2017-08-01T23:18:44Z", "updated_at": "2017-09-06T05:32:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11878#discussion_r130752830", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11878", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/130752830"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11878#discussion_r130752830"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11878"}}, "body_html": "<p>nit: s/n/an/</p>", "body_text": "nit: s/n/an/"}