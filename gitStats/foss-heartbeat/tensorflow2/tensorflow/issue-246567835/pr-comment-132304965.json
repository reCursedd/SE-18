{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/132304965", "pull_request_review_id": 55372009, "id": 132304965, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDEzMjMwNDk2NQ==", "diff_hunk": "@@ -0,0 +1,343 @@\n+/* Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+==============================================================================*/\n+\n+// See docs in ../ops/linalg_ops.cc.\n+// TODO(shamanDevel): Enable complex inputs. This will require additional tests\n+//                    and OP_REQUIRES.\n+#if GOOGLE_CUDA\n+#define EIGEN_USE_GPU\n+\n+#include <algorithm>\n+#include <vector>\n+\n+#include \"tensorflow/core/framework/kernel_def_builder.h\"\n+#include \"tensorflow/core/framework/op_kernel.h\"\n+#include \"tensorflow/core/framework/register_types.h\"\n+#include \"tensorflow/core/framework/tensor_shape.h\"\n+#include \"tensorflow/core/kernels/linalg_ops_common.h\"\n+#include \"tensorflow/core/lib/core/errors.h\"\n+#include \"tensorflow/core/platform/logging.h\"\n+#include \"tensorflow/core/platform/types.h\"\n+\n+#include \"tensorflow/core/kernels/cuda_solvers.h\"\n+#include \"tensorflow/core/platform/stream_executor.h\"\n+#include \"third_party/eigen3/unsupported/Eigen/CXX11/Tensor\"\n+\n+// I need to transpose V afterwards\n+#include \"tensorflow/core/kernels/transpose_functor.h\"\n+\n+// Logging\n+#include <stdio.h>\n+\n+namespace tensorflow {\n+\n+static const char kErrMsg[] =\n+    \"Singular Value Decomposition was not successful. The input might not be \"\n+    \"valid.\";\n+\n+typedef Eigen::GpuDevice GPUDevice;\n+\n+// Scalar: The input scalar type (can be complex)\n+// SScalar: The output type for the singular value,\n+//   same as Scalar if real, or the real version if Scalar is complex\n+template <class Scalar, class SScalar>\n+class SvdOpGpu : public AsyncOpKernel {\n+ public:\n+  explicit SvdOpGpu(OpKernelConstruction* context) : AsyncOpKernel(context) {\n+    OP_REQUIRES_OK(context, context->GetAttr(\"compute_uv\", &compute_uv_));\n+    OP_REQUIRES_OK(context, context->GetAttr(\"full_matrices\", &full_matrices_));\n+  }\n+\n+  void RunSVD(OpKernelContext* context, DoneCallback done, int64 m, int64 n,\n+              int64 p, int64 batch_size, Scalar* input_ptr,\n+              SScalar* outputS_ptr, Scalar* outputU_ptr, Scalar* outputVT_ptr,\n+              int* dev_info_ptr, CudaSolver& solver) {\n+    for (int64 i = 0; i < batch_size; ++i) {\n+      int lda = m;\n+      int ldu = m;\n+      int ldvt = n;\n+      Scalar* input = input_ptr + i * m * n;\n+      SScalar* outputS = outputS_ptr + i * p;\n+      Scalar* outputU = NULL;\n+      Scalar* outputVT = NULL;\n+      signed char jobu = 'N';\n+      signed char jobvt = 'N';\n+\n+      if (compute_uv_) {\n+        if (full_matrices_) {\n+          outputU = outputU_ptr + i * m * m;\n+          outputVT = outputVT_ptr + i * n * n;\n+          jobu = 'A';\n+          jobvt = 'A';\n+        } else {\n+          outputU = outputU_ptr + i * m * p;\n+          outputVT = outputVT_ptr + i * n * p;\n+          jobu = 'S';\n+          jobvt = 'S';\n+        }\n+      }\n+\n+      OP_REQUIRES_OK_ASYNC(\n+          context, solver.Gesvd(jobu, jobvt, m, n, input, lda, outputS, outputU,\n+                                ldu, outputVT, ldvt, dev_info_ptr + i),\n+          done);\n+    }\n+  }\n+\n+  void CheckResult(OpKernelContext* context, DoneCallback done,\n+                   const std::vector<DeviceLapackInfo>& dev_info,\n+                   CudaSolver& solver, Tensor& catch1, Tensor& catch2) {\n+    auto info_checker = [context, dev_info, done, catch1, catch2](\n+        const Status& status, const std::vector<HostLapackInfo>& /* unused */) {\n+      Status full_status = status;\n+      if (!full_status.ok()) {\n+        full_status.Update(errors::InvalidArgument(kErrMsg));\n+      }\n+      OP_REQUIRES_OK_ASYNC(context, full_status, done);\n+      done();\n+    };\n+\n+    OP_REQUIRES_OK_ASYNC(context, solver.CopyLapackInfoToHostAsync(\n+                                      dev_info, std::move(info_checker)),\n+                         done);\n+  }\n+\n+  // The SVD if m >= n\n+  void PerformSVD_MgeqN(OpKernelContext* context, DoneCallback done, int64 m,\n+                        int64 n, int64 p, const gtl::ArraySlice<int32>& perm,\n+                        const Tensor& M, Tensor* S, Tensor* U, Tensor* V) {\n+    TensorShape shapeRaw = M.shape();\n+    shapeRaw.RemoveDim(shapeRaw.dims() - 1);\n+    shapeRaw.RemoveDim(shapeRaw.dims() - 1);\n+\n+    // Transpose M, because cuSolver expects it to be column-major\n+    TensorShape input_shape = shapeRaw;\n+    input_shape.AddDim(n);\n+    input_shape.AddDim(m);\n+    Tensor input_copy;\n+    OP_REQUIRES_OK_ASYNC(\n+        context, context->allocate_temp(M.dtype(), input_shape, &input_copy),\n+        done);\n+    auto device = context->eigen_device<GPUDevice>();\n+    DoTranspose(device, M, perm, &input_copy);", "path": "tensorflow/core/kernels/svd_op_gpu.cu.cc", "position": null, "original_position": 134, "commit_id": "bfda71b1104c7d1d0210439c22cded14c87c7db7", "original_commit_id": "159a70bf1647b81f674129eb267941ad9217f29d", "user": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "body": "You need to check the return value from DoTranpose. Wrap in OP_REQUIRES_OK macro.", "created_at": "2017-08-09T20:58:12Z", "updated_at": "2017-09-06T05:32:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/11878#discussion_r132304965", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11878", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/132304965"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/11878#discussion_r132304965"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/11878"}}, "body_html": "<p>You need to check the return value from DoTranpose. Wrap in OP_REQUIRES_OK macro.</p>", "body_text": "You need to check the return value from DoTranpose. Wrap in OP_REQUIRES_OK macro."}