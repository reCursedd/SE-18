{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16864", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16864/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16864/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16864/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16864", "id": 295502049, "node_id": "MDU6SXNzdWUyOTU1MDIwNDk=", "number": 16864, "title": "CPU execution of ops after gradient clipping on windows", "user": {"login": "Dhruv-Mohan", "id": 11293826, "node_id": "MDQ6VXNlcjExMjkzODI2", "avatar_url": "https://avatars2.githubusercontent.com/u/11293826?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Dhruv-Mohan", "html_url": "https://github.com/Dhruv-Mohan", "followers_url": "https://api.github.com/users/Dhruv-Mohan/followers", "following_url": "https://api.github.com/users/Dhruv-Mohan/following{/other_user}", "gists_url": "https://api.github.com/users/Dhruv-Mohan/gists{/gist_id}", "starred_url": "https://api.github.com/users/Dhruv-Mohan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Dhruv-Mohan/subscriptions", "organizations_url": "https://api.github.com/users/Dhruv-Mohan/orgs", "repos_url": "https://api.github.com/users/Dhruv-Mohan/repos", "events_url": "https://api.github.com/users/Dhruv-Mohan/events{/privacy}", "received_events_url": "https://api.github.com/users/Dhruv-Mohan/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "rmlarsen", "id": 16907534, "node_id": "MDQ6VXNlcjE2OTA3NTM0", "avatar_url": "https://avatars2.githubusercontent.com/u/16907534?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rmlarsen", "html_url": "https://github.com/rmlarsen", "followers_url": "https://api.github.com/users/rmlarsen/followers", "following_url": "https://api.github.com/users/rmlarsen/following{/other_user}", "gists_url": "https://api.github.com/users/rmlarsen/gists{/gist_id}", "starred_url": "https://api.github.com/users/rmlarsen/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rmlarsen/subscriptions", "organizations_url": "https://api.github.com/users/rmlarsen/orgs", "repos_url": "https://api.github.com/users/rmlarsen/repos", "events_url": "https://api.github.com/users/rmlarsen/events{/privacy}", "received_events_url": "https://api.github.com/users/rmlarsen/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2018-02-08T12:56:53Z", "updated_at": "2018-11-14T19:15:15Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Windows10</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Binary</li>\n<li><strong>TensorFlow version (use command below)</strong>:1.5.0</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:N/A</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:N/A</li>\n<li><strong>CUDA/cuDNN version</strong>:CUDA 9.0, cuDNN 7</li>\n<li><strong>GPU model and memory</strong>: Titan xp, 12gb</li>\n<li><strong>Exact command to reproduce</strong>: Script attached below</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Adding gradient clipping as follows places certain ops on the CPU greatly increasing training time on windows.</p>\n<div class=\"highlight highlight-source-python\"><pre>    grads <span class=\"pl-k\">=</span> tf.gradients(loss, tf.trainable_variables())\n    omnomed_grads, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(grads, <span class=\"pl-c1\">0.5</span>)\n    train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(omnomed_grads,  tf.trainable_variables()))</pre></div>\n<p>I worte a <a href=\"https://github.com/Dhruv-Mohan/G_clip_issue/blob/master/Gptest.py\">script</a>  to perform a few tests:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">OS</th>\n<th align=\"center\">TF Version</th>\n<th align=\"center\">Gradient clipping</th>\n<th align=\"center\">Average runtime (m:s.ms)</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">Windows10</td>\n<td align=\"center\">1.6</td>\n<td align=\"center\">True</td>\n<td align=\"center\">2:13.57</td>\n</tr>\n<tr>\n<td align=\"center\">Windows10</td>\n<td align=\"center\">1.6</td>\n<td align=\"center\">False</td>\n<td align=\"center\">0:16.24</td>\n</tr>\n<tr>\n<td align=\"center\">Windows10</td>\n<td align=\"center\">1.5</td>\n<td align=\"center\">True</td>\n<td align=\"center\">2:13.46</td>\n</tr>\n<tr>\n<td align=\"center\">Windows10</td>\n<td align=\"center\">1.5</td>\n<td align=\"center\">False</td>\n<td align=\"center\">0:16.52</td>\n</tr>\n<tr>\n<td align=\"center\">Windows10</td>\n<td align=\"center\">1.2</td>\n<td align=\"center\">True</td>\n<td align=\"center\">0:24.64</td>\n</tr>\n<tr>\n<td align=\"center\">Windows10</td>\n<td align=\"center\">1.2</td>\n<td align=\"center\">False</td>\n<td align=\"center\">0:23.80</td>\n</tr>\n<tr>\n<td align=\"center\">Linux mint 18.1</td>\n<td align=\"center\">1.5</td>\n<td align=\"center\">True</td>\n<td align=\"center\">0:23.45</td>\n</tr>\n<tr>\n<td align=\"center\">Linux mint 18.1</td>\n<td align=\"center\">1.5</td>\n<td align=\"center\">False</td>\n<td align=\"center\">0:21.29</td>\n</tr>\n</tbody>\n</table>\n<p>Curiously the issue isn't present on TF1.2</p>\n<p>Forcing operation placement on the GPU:</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">with</span> tf.device(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>/gpu:0<span class=\"pl-pds\">'</span></span>):\n        grads <span class=\"pl-k\">=</span> tf.gradients(loss, tf.trainable_variables())\n        omnomed_grads, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(grads, <span class=\"pl-c1\">0.5</span>)\n        train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(omnomed_grads,  tf.trainable_variables()))</pre></div>\n<p>results in the following error on windows:<br>\n<code>Cannot assign a device for operation 'gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.</code><br>\nThe error generated is similar to <a href=\"https://github.com/tensorflow/models/issues/2803\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/2803/hovercard\">#2803</a> and <a href=\"https://github.com/tensorflow/models/issues/3118\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/models/issues/3118/hovercard\">#3118</a></p>\n<p>Setting soft device placement in session configuration results in similar runtime.</p>\n<h3>Source code / logs</h3>\n<p>I have attached the device placement logs of windows 10 running TF 1.6 <a href=\"https://github.com/tensorflow/tensorflow/files/1707039/Windows_gradclip.txt\">with</a> and <a href=\"https://github.com/tensorflow/tensorflow/files/1707040/Windows_nogradclip.txt\">without</a> gradient clipping as well as the device placement logs from Linux mint <a href=\"https://github.com/tensorflow/tensorflow/files/1707038/Mint_gradclip.txt\">with</a> gradient clipping</p>\n<p>A few discrepancies with respect to op placement while gradient clipping are highlighted below:</p>\n<table>\n<thead>\n<tr>\n<th align=\"center\">Windows10</th>\n<th align=\"center\">Linux mint</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"center\">gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:CPU:0</td>\n<td align=\"center\">gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:GPU:0</td>\n</tr>\n<tr>\n<td align=\"center\">global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:CPU:0</td>\n<td align=\"center\">global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0</td>\n</tr>\n<tr>\n<td align=\"center\">gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:CPU:0</td>\n<td align=\"center\">gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0</td>\n</tr>\n<tr>\n<td align=\"center\">gradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:CPU:0</td>\n<td align=\"center\">gradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0</td>\n</tr>\n<tr>\n<td align=\"center\">clip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:CPU:0</td>\n<td align=\"center\">clip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:GPU:0</td>\n</tr>\n</tbody>\n</table>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Windows10\nTensorFlow installed from (source or binary): Binary\nTensorFlow version (use command below):1.5.0\nPython version: 3.6\nBazel version (if compiling from source):N/A\nGCC/Compiler version (if compiling from source):N/A\nCUDA/cuDNN version:CUDA 9.0, cuDNN 7\nGPU model and memory: Titan xp, 12gb\nExact command to reproduce: Script attached below\n\nDescribe the problem\nAdding gradient clipping as follows places certain ops on the CPU greatly increasing training time on windows.\n    grads = tf.gradients(loss, tf.trainable_variables())\n    omnomed_grads, _ = tf.clip_by_global_norm(grads, 0.5)\n    train_op = optimizer.apply_gradients(zip(omnomed_grads,  tf.trainable_variables()))\nI worte a script  to perform a few tests:\n\n\n\nOS\nTF Version\nGradient clipping\nAverage runtime (m:s.ms)\n\n\n\n\nWindows10\n1.6\nTrue\n2:13.57\n\n\nWindows10\n1.6\nFalse\n0:16.24\n\n\nWindows10\n1.5\nTrue\n2:13.46\n\n\nWindows10\n1.5\nFalse\n0:16.52\n\n\nWindows10\n1.2\nTrue\n0:24.64\n\n\nWindows10\n1.2\nFalse\n0:23.80\n\n\nLinux mint 18.1\n1.5\nTrue\n0:23.45\n\n\nLinux mint 18.1\n1.5\nFalse\n0:21.29\n\n\n\nCuriously the issue isn't present on TF1.2\nForcing operation placement on the GPU:\n    with tf.device('/gpu:0'):\n        grads = tf.gradients(loss, tf.trainable_variables())\n        omnomed_grads, _ = tf.clip_by_global_norm(grads, 0.5)\n        train_op = optimizer.apply_gradients(zip(omnomed_grads,  tf.trainable_variables()))\nresults in the following error on windows:\nCannot assign a device for operation 'gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.\nThe error generated is similar to #2803 and #3118\nSetting soft device placement in session configuration results in similar runtime.\nSource code / logs\nI have attached the device placement logs of windows 10 running TF 1.6 with and without gradient clipping as well as the device placement logs from Linux mint with gradient clipping\nA few discrepancies with respect to op placement while gradient clipping are highlighted below:\n\n\n\nWindows10\nLinux mint\n\n\n\n\ngradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:CPU:0\ngradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:GPU:0\n\n\nglobal_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:CPU:0\nglobal_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0\n\n\ngradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:CPU:0\ngradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0\n\n\ngradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:CPU:0\ngradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0\n\n\nclip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:CPU:0\nclip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:GPU:0", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Windows10\r\n- **TensorFlow installed from (source or binary)**: Binary\r\n- **TensorFlow version (use command below)**:1.5.0\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:N/A\r\n- **GCC/Compiler version (if compiling from source)**:N/A\r\n- **CUDA/cuDNN version**:CUDA 9.0, cuDNN 7\r\n- **GPU model and memory**: Titan xp, 12gb\r\n- **Exact command to reproduce**: Script attached below\r\n\r\n### Describe the problem\r\nAdding gradient clipping as follows places certain ops on the CPU greatly increasing training time on windows.\r\n```python\r\n    grads = tf.gradients(loss, tf.trainable_variables())\r\n    omnomed_grads, _ = tf.clip_by_global_norm(grads, 0.5)\r\n    train_op = optimizer.apply_gradients(zip(omnomed_grads,  tf.trainable_variables()))\r\n```\r\nI worte a [script](https://github.com/Dhruv-Mohan/G_clip_issue/blob/master/Gptest.py)  to perform a few tests:\r\n\r\n| OS        | TF Version           | Gradient clipping  | Average runtime (m:s.ms) |\r\n| :-------------: |:-------------:| :-----:| :--------: |\r\n| Windows10           | 1.6  | True   | 2:13.57    |\r\n| Windows10           | 1.6  |  False | 0:16.24    |\r\n| Windows10           | 1.5  | True   | 2:13.46    |\r\n| Windows10           | 1.5  |  False | 0:16.52    |\r\n| Windows10           | 1.2  |  True  | 0:24.64    |\r\n| Windows10           | 1.2  |  False | 0:23.80    |\r\n| Linux mint 18.1     | 1.5  | True   | 0:23.45    |\r\n|Linux mint 18.1      | 1.5  |  False | 0:21.29    |\r\n\r\nCuriously the issue isn't present on TF1.2\r\n\r\nForcing operation placement on the GPU:\r\n```python\r\n    with tf.device('/gpu:0'):\r\n        grads = tf.gradients(loss, tf.trainable_variables())\r\n        omnomed_grads, _ = tf.clip_by_global_norm(grads, 0.5)\r\n        train_op = optimizer.apply_gradients(zip(omnomed_grads,  tf.trainable_variables()))\r\n```\r\nresults in the following error on windows:\r\n `Cannot assign a device for operation 'gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax': Could not satisfy explicit device specification '/device:GPU:0' because no supported kernel for GPU devices is available.`\r\nThe error generated is similar to [#2803](https://github.com/tensorflow/models/issues/2803) and [#3118](https://github.com/tensorflow/models/issues/3118)\r\n\r\nSetting soft device placement in session configuration results in similar runtime.\r\n\r\n### Source code / logs\r\nI have attached the device placement logs of windows 10 running TF 1.6 [with](https://github.com/tensorflow/tensorflow/files/1707039/Windows_gradclip.txt) and [without](https://github.com/tensorflow/tensorflow/files/1707040/Windows_nogradclip.txt) gradient clipping as well as the device placement logs from Linux mint [with](https://github.com/tensorflow/tensorflow/files/1707038/Mint_gradclip.txt) gradient clipping\r\n\r\n\r\n\r\nA few discrepancies with respect to op placement while gradient clipping are highlighted below:\r\n\r\n| Windows10        |  Linux mint        |\r\n| :-------------: |:-------------:| \r\n| gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:CPU:0     | gradients/softmax_cross_entropy_with_logits_grad/LogSoftmax: (LogSoftmax): /job:localhost/replica:0/task:0/device:GPU:0 | \r\n| global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:CPU:0      | global_norm/L2Loss_6: (L2Loss): /job:localhost/replica:0/task:0/device:GPU:0      | \r\n|gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:CPU:0 | gradients/conv2d/Conv2D_grad/Conv2DBackpropFilter: (Conv2DBackpropFilter): /job:localhost/replica:0/task:0/device:GPU:0      | \r\n| gradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:CPU:0   | gradients/conv2d/BiasAdd_grad/BiasAddGrad: (BiasAddGrad): /job:localhost/replica:0/task:0/device:GPU:0   |\r\n| clip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:CPU:0 | clip_by_global_norm/mul_8: (Mul): /job:localhost/replica:0/task:0/device:GPU:0 |\r\n\r\n\r\n\r\n\r\n"}