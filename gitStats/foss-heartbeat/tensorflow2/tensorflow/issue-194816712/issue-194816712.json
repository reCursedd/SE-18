{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6251", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6251/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6251/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6251/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6251", "id": 194816712, "node_id": "MDU6SXNzdWUxOTQ4MTY3MTI=", "number": 6251, "title": "tensorflow crash when training in distribution", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586558, "node_id": "MDU6TGFiZWw0MDQ1ODY1NTg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:community%20support", "name": "stat:community support", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-12-11T06:45:25Z", "updated_at": "2017-06-16T17:27:02Z", "closed_at": "2017-06-16T17:27:02Z", "author_association": "NONE", "body_html": "<p>I'am train my model in distribution, running 12 workers and 2 ps server.<br>\nBut the worker crash and dumping core files continuously.</p>\n<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>nothing</p>\n<h3>Environment info</h3>\n<p>Operating System:<br>\ncentos 6.3 with cpu<br>\ntensorflow version 0.12</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<pre><code>if __name__ == \"__main__\":\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)):\n        # Read TFRecords files for training\n        filename_queue = tf.train.string_input_producer(\n            tf.train.match_filenames_once(FLAGS.train),\n            num_epochs=epoch_number)\n        serialized_example = read_and_decode(filename_queue)\n        batch_serialized_example = tf.train.shuffle_batch(\n            [serialized_example],\n            batch_size=batch_size,\n            num_threads=thread_number,\n            capacity=capacity,\n            min_after_dequeue=min_after_dequeue)\n        features = tf.parse_example(\n            batch_serialized_example,\n            features={\n                \"label\": tf.FixedLenFeature([], tf.float32),\n                \"ids\": tf.VarLenFeature(tf.int64),\n                \"values\": tf.VarLenFeature(tf.float32),\n            })\n        batch_labels = features[\"label\"]\n        batch_ids = features[\"ids\"]\n        batch_values = features[\"values\"]\n\n        # Read TFRecords file for validatioin\n        validate_filename_queue = tf.train.string_input_producer(\n            tf.train.match_filenames_once(FLAGS.eval),\n            num_epochs=epoch_number)\n        validate_serialized_example = read_and_decode(validate_filename_queue)\n        validate_batch_serialized_example = tf.train.shuffle_batch(\n            [validate_serialized_example],\n            batch_size=validate_batch_size,\n            num_threads=thread_number,\n            capacity=capacity,\n            min_after_dequeue=min_after_dequeue)\n        validate_features = tf.parse_example(\n            validate_batch_serialized_example,\n            features={\n                \"label\": tf.FixedLenFeature([], tf.float32),\n                \"ids\": tf.VarLenFeature(tf.int64),\n                \"values\": tf.VarLenFeature(tf.float32),\n            })\n        validate_batch_labels = features[\"label\"]\n        validate_batch_ids = features[\"ids\"]\n        validate_batch_values = features[\"values\"]\n        logits = inference(batch_ids, batch_values)\n        batch_labels = tf.to_int64(batch_labels)\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                       batch_labels)\n        loss = tf.reduce_mean(cross_entropy, name='loss')\n\n        print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\n        if FLAGS.optimizer == \"sgd\":\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"momentum\":\n            # optimizer = tf.train.MomentumOptimizer(learning_rate)\n            print(\"Not support optimizer: {} yet, exit now\".format(FLAGS.optimizer))\n            exit(1)\n        elif FLAGS.optimizer == \"adadelta\":\n            optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"adagrad\":\n            optimizer = tf.train.AdagradOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"adam\":\n            optimizer = tf.train.AdamOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"ftrl\":\n            optimizer = tf.train.FtrlOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"rmsprop\":\n            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n        else:\n            print(\"Unknow optimizer: {}, exit now\".format(FLAGS.optimizer))\n            exit(1)\n\n        #with tf.device(\"/cpu:0\"):\n        global_step = tf.Variable(0, name='global_step', trainable=False)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n\n        # Compute accuracy\n        tf.get_variable_scope().reuse_variables()\n        accuracy_logits = inference(validate_batch_ids, validate_batch_values)\n        validate_softmax = tf.nn.softmax(accuracy_logits)\n        validate_batch_labels = tf.to_int64(validate_batch_labels)\n        correct_prediction = tf.equal(\n            tf.argmax(validate_softmax, 1), validate_batch_labels)\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        # Compute auc\n        validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\n        sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\n        derived_size = tf.shape(validate_batch_labels)[0]\n        indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n        concated = tf.concat(1, [indices, sparse_labels])\n        outshape = tf.pack([derived_size, LABEL_SIZE])\n        new_validate_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n        _, auc_op = tf.contrib.metrics.streaming_auc(validate_softmax,\n                                                     new_validate_batch_labels)\n\n        # Define inference op\n        sparse_index = tf.placeholder(tf.int64)\n        sparse_ids = tf.placeholder(tf.int64)\n        sparse_values = tf.placeholder(tf.float32)\n        sparse_shape = tf.placeholder(tf.int64)\n        inference_ids = tf.SparseTensor(sparse_index, sparse_ids, sparse_shape)\n        inference_values = tf.SparseTensor(sparse_index, sparse_values, sparse_shape)\n        inference_logits = inference(inference_ids, inference_values)\n        inference_softmax = tf.nn.softmax(inference_logits)\n        inference_op = tf.argmax(inference_softmax, 1)\n\n        # Initialize saver and summary\n        #checkpoint_file = checkpoint_dir + \"checkpoint.ckpt\"\n        steps_to_validate = FLAGS.steps_to_validate\n        init_op = tf.initialize_all_variables()\n\n        saver = tf.train.Saver(max_to_keep = 2)\n        keys_placeholder = tf.placeholder(\"float\")\n        keys = tf.identity(keys_placeholder)\n        tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\n        tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\n                                                    'softmax': inference_softmax.name,\n                                                    'prediction': inference_op.name}))\n        tf.scalar_summary('loss', loss)\n        tf.scalar_summary('accuracy', accuracy)\n        tf.scalar_summary('auc', auc_op)\n        summary_op = tf.merge_all_summaries()\n\n\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"./train_process/\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=60)\n\n    # Create session to run graph\n    with sv.managed_session(server.target) as sess:\n        if mode == \"train\" or mode == \"train_from_scratch\":\n            while not sv.should_stop():\n                # Get coordinator and run queues to read data\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n                start_time = datetime.datetime.now()\n\n                try:\n                    while not coord.should_stop():\n                        _, loss_value, step = sess.run([train_op, loss, global_step])\n                        if step % steps_to_validate == 0:\n                            accuracy_value, auc_value, summary_value = sess.run(\n                                [accuracy, auc_op, summary_op])\n                            end_time = datetime.datetime.now()\n                            print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\n                                end_time - start_time,\n                                FLAGS.task_index,\n                                step, loss_value, accuracy_value,\n                                auc_value))\n\n                            start_time = end_time\n                except tf.errors.OutOfRangeError:\n                    print(\"Done training after reading all data\")\n                finally:\n                    coord.request_stop()\n                    print(\"coord stopped\")\n\n                # Wait for threads to exit\n                coord.join(threads)\n</code></pre>\n<h3>logs</h3>\n<p>gdb python core.xxxxx , and then 'bt', shows that</p>\n<pre><code>\n#0  0x00007f5943d3f166 in pthread_detach () from /opt/glibc-2.17/lib/libpthread.so.0\n#1  0x00007f58fc8c8ab5 in std::thread::detach() ()\n    at /opt/install/gcc-build/x86_64-redhat-linux/libstdc++-v3/include/x86_64-redhat-linux/bits/gthr-default.h:674\n#2  0x00007f58fec7f0bf in tensorflow::(anonymous namespace)::PosixEnv::SchedClosure(std::function&lt;void ()()&gt;) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007f58fea8e981 in tensorflow::SchedClosure(std::function&lt;void ()()&gt;) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007f58fd5f72cd in tensorflow::Master::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequest const*, tensorflow::RunStepResponse*, std::function&lt;void ()(tensorflow::Status const&amp;)&gt;) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007f58fd5f10f9 in tensorflow::GrpcMasterService::RunStepHandler(tensorflow::Call&lt;tensorflow::GrpcMasterService, tensorflow::grpc::MasterService::AsyncService, tensorflow::RunStepRequest, tensorflow::RunStepResponse&gt;*) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007f58fd5f2c4c in tensorflow::GrpcMasterService::HandleRPCsLoop() ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007f58fc8c8b80 in execute_native_thread_routine_compat ()\n    at ../../../../../gcc-6.2.0/libstdc++-v3/src/c++11/thread.cc:110\n#8  0x00007f5943d3df83 in start_thread () from /opt/glibc-2.17/lib/libpthread.so.0\n#9  0x00007f59433668bd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\n\n</code></pre>", "body_text": "I'am train my model in distribution, running 12 workers and 2 ps server.\nBut the worker crash and dumping core files continuously.\nWhat related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nnothing\nEnvironment info\nOperating System:\ncentos 6.3 with cpu\ntensorflow version 0.12\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nif __name__ == \"__main__\":\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n  server = tf.train.Server(cluster,\n                           job_name=FLAGS.job_name,\n                           task_index=FLAGS.task_index)\n  if FLAGS.job_name == \"ps\":\n    server.join()\n  elif FLAGS.job_name == \"worker\":\n    with tf.device(tf.train.replica_device_setter(\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n            cluster=cluster)):\n        # Read TFRecords files for training\n        filename_queue = tf.train.string_input_producer(\n            tf.train.match_filenames_once(FLAGS.train),\n            num_epochs=epoch_number)\n        serialized_example = read_and_decode(filename_queue)\n        batch_serialized_example = tf.train.shuffle_batch(\n            [serialized_example],\n            batch_size=batch_size,\n            num_threads=thread_number,\n            capacity=capacity,\n            min_after_dequeue=min_after_dequeue)\n        features = tf.parse_example(\n            batch_serialized_example,\n            features={\n                \"label\": tf.FixedLenFeature([], tf.float32),\n                \"ids\": tf.VarLenFeature(tf.int64),\n                \"values\": tf.VarLenFeature(tf.float32),\n            })\n        batch_labels = features[\"label\"]\n        batch_ids = features[\"ids\"]\n        batch_values = features[\"values\"]\n\n        # Read TFRecords file for validatioin\n        validate_filename_queue = tf.train.string_input_producer(\n            tf.train.match_filenames_once(FLAGS.eval),\n            num_epochs=epoch_number)\n        validate_serialized_example = read_and_decode(validate_filename_queue)\n        validate_batch_serialized_example = tf.train.shuffle_batch(\n            [validate_serialized_example],\n            batch_size=validate_batch_size,\n            num_threads=thread_number,\n            capacity=capacity,\n            min_after_dequeue=min_after_dequeue)\n        validate_features = tf.parse_example(\n            validate_batch_serialized_example,\n            features={\n                \"label\": tf.FixedLenFeature([], tf.float32),\n                \"ids\": tf.VarLenFeature(tf.int64),\n                \"values\": tf.VarLenFeature(tf.float32),\n            })\n        validate_batch_labels = features[\"label\"]\n        validate_batch_ids = features[\"ids\"]\n        validate_batch_values = features[\"values\"]\n        logits = inference(batch_ids, batch_values)\n        batch_labels = tf.to_int64(batch_labels)\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\n                                                                       batch_labels)\n        loss = tf.reduce_mean(cross_entropy, name='loss')\n\n        print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\n        if FLAGS.optimizer == \"sgd\":\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"momentum\":\n            # optimizer = tf.train.MomentumOptimizer(learning_rate)\n            print(\"Not support optimizer: {} yet, exit now\".format(FLAGS.optimizer))\n            exit(1)\n        elif FLAGS.optimizer == \"adadelta\":\n            optimizer = tf.train.AdadeltaOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"adagrad\":\n            optimizer = tf.train.AdagradOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"adam\":\n            optimizer = tf.train.AdamOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"ftrl\":\n            optimizer = tf.train.FtrlOptimizer(learning_rate)\n        elif FLAGS.optimizer == \"rmsprop\":\n            optimizer = tf.train.RMSPropOptimizer(learning_rate)\n        else:\n            print(\"Unknow optimizer: {}, exit now\".format(FLAGS.optimizer))\n            exit(1)\n\n        #with tf.device(\"/cpu:0\"):\n        global_step = tf.Variable(0, name='global_step', trainable=False)\n        train_op = optimizer.minimize(loss, global_step=global_step)\n\n        # Compute accuracy\n        tf.get_variable_scope().reuse_variables()\n        accuracy_logits = inference(validate_batch_ids, validate_batch_values)\n        validate_softmax = tf.nn.softmax(accuracy_logits)\n        validate_batch_labels = tf.to_int64(validate_batch_labels)\n        correct_prediction = tf.equal(\n            tf.argmax(validate_softmax, 1), validate_batch_labels)\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        # Compute auc\n        validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\n        sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\n        derived_size = tf.shape(validate_batch_labels)[0]\n        indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\n        concated = tf.concat(1, [indices, sparse_labels])\n        outshape = tf.pack([derived_size, LABEL_SIZE])\n        new_validate_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\n        _, auc_op = tf.contrib.metrics.streaming_auc(validate_softmax,\n                                                     new_validate_batch_labels)\n\n        # Define inference op\n        sparse_index = tf.placeholder(tf.int64)\n        sparse_ids = tf.placeholder(tf.int64)\n        sparse_values = tf.placeholder(tf.float32)\n        sparse_shape = tf.placeholder(tf.int64)\n        inference_ids = tf.SparseTensor(sparse_index, sparse_ids, sparse_shape)\n        inference_values = tf.SparseTensor(sparse_index, sparse_values, sparse_shape)\n        inference_logits = inference(inference_ids, inference_values)\n        inference_softmax = tf.nn.softmax(inference_logits)\n        inference_op = tf.argmax(inference_softmax, 1)\n\n        # Initialize saver and summary\n        #checkpoint_file = checkpoint_dir + \"checkpoint.ckpt\"\n        steps_to_validate = FLAGS.steps_to_validate\n        init_op = tf.initialize_all_variables()\n\n        saver = tf.train.Saver(max_to_keep = 2)\n        keys_placeholder = tf.placeholder(\"float\")\n        keys = tf.identity(keys_placeholder)\n        tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\n        tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\n                                                    'softmax': inference_softmax.name,\n                                                    'prediction': inference_op.name}))\n        tf.scalar_summary('loss', loss)\n        tf.scalar_summary('accuracy', accuracy)\n        tf.scalar_summary('auc', auc_op)\n        summary_op = tf.merge_all_summaries()\n\n\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\n                             logdir=\"./train_process/\",\n                             init_op=init_op,\n                             summary_op=summary_op,\n                             saver=saver,\n                             global_step=global_step,\n                             save_model_secs=60)\n\n    # Create session to run graph\n    with sv.managed_session(server.target) as sess:\n        if mode == \"train\" or mode == \"train_from_scratch\":\n            while not sv.should_stop():\n                # Get coordinator and run queues to read data\n                coord = tf.train.Coordinator()\n                threads = tf.train.start_queue_runners(coord=coord, sess=sess)\n\n                start_time = datetime.datetime.now()\n\n                try:\n                    while not coord.should_stop():\n                        _, loss_value, step = sess.run([train_op, loss, global_step])\n                        if step % steps_to_validate == 0:\n                            accuracy_value, auc_value, summary_value = sess.run(\n                                [accuracy, auc_op, summary_op])\n                            end_time = datetime.datetime.now()\n                            print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\n                                end_time - start_time,\n                                FLAGS.task_index,\n                                step, loss_value, accuracy_value,\n                                auc_value))\n\n                            start_time = end_time\n                except tf.errors.OutOfRangeError:\n                    print(\"Done training after reading all data\")\n                finally:\n                    coord.request_stop()\n                    print(\"coord stopped\")\n\n                # Wait for threads to exit\n                coord.join(threads)\n\nlogs\ngdb python core.xxxxx , and then 'bt', shows that\n\n#0  0x00007f5943d3f166 in pthread_detach () from /opt/glibc-2.17/lib/libpthread.so.0\n#1  0x00007f58fc8c8ab5 in std::thread::detach() ()\n    at /opt/install/gcc-build/x86_64-redhat-linux/libstdc++-v3/include/x86_64-redhat-linux/bits/gthr-default.h:674\n#2  0x00007f58fec7f0bf in tensorflow::(anonymous namespace)::PosixEnv::SchedClosure(std::function<void ()()>) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#3  0x00007f58fea8e981 in tensorflow::SchedClosure(std::function<void ()()>) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#4  0x00007f58fd5f72cd in tensorflow::Master::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequest const*, tensorflow::RunStepResponse*, std::function<void ()(tensorflow::Status const&)>) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#5  0x00007f58fd5f10f9 in tensorflow::GrpcMasterService::RunStepHandler(tensorflow::Call<tensorflow::GrpcMasterService, tensorflow::grpc::MasterService::AsyncService, tensorflow::RunStepRequest, tensorflow::RunStepResponse>*) ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#6  0x00007f58fd5f2c4c in tensorflow::GrpcMasterService::HandleRPCsLoop() ()\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\n#7  0x00007f58fc8c8b80 in execute_native_thread_routine_compat ()\n    at ../../../../../gcc-6.2.0/libstdc++-v3/src/c++11/thread.cc:110\n#8  0x00007f5943d3df83 in start_thread () from /opt/glibc-2.17/lib/libpthread.so.0\n#9  0x00007f59433668bd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113", "body": "I'am train my model in distribution, running 12 workers and 2 ps server.\r\nBut the worker crash and dumping core files continuously.\r\n\r\n\r\n### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nnothing\r\n\r\n### Environment info\r\nOperating System:\r\ncentos 6.3 with cpu\r\ntensorflow version 0.12\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\n```\r\nif __name__ == \"__main__\":\r\n  ps_hosts = FLAGS.ps_hosts.split(\",\")\r\n  worker_hosts = FLAGS.worker_hosts.split(\",\")\r\n  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\r\n  server = tf.train.Server(cluster,\r\n                           job_name=FLAGS.job_name,\r\n                           task_index=FLAGS.task_index)\r\n  if FLAGS.job_name == \"ps\":\r\n    server.join()\r\n  elif FLAGS.job_name == \"worker\":\r\n    with tf.device(tf.train.replica_device_setter(\r\n            worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\r\n            cluster=cluster)):\r\n        # Read TFRecords files for training\r\n        filename_queue = tf.train.string_input_producer(\r\n            tf.train.match_filenames_once(FLAGS.train),\r\n            num_epochs=epoch_number)\r\n        serialized_example = read_and_decode(filename_queue)\r\n        batch_serialized_example = tf.train.shuffle_batch(\r\n            [serialized_example],\r\n            batch_size=batch_size,\r\n            num_threads=thread_number,\r\n            capacity=capacity,\r\n            min_after_dequeue=min_after_dequeue)\r\n        features = tf.parse_example(\r\n            batch_serialized_example,\r\n            features={\r\n                \"label\": tf.FixedLenFeature([], tf.float32),\r\n                \"ids\": tf.VarLenFeature(tf.int64),\r\n                \"values\": tf.VarLenFeature(tf.float32),\r\n            })\r\n        batch_labels = features[\"label\"]\r\n        batch_ids = features[\"ids\"]\r\n        batch_values = features[\"values\"]\r\n\r\n        # Read TFRecords file for validatioin\r\n        validate_filename_queue = tf.train.string_input_producer(\r\n            tf.train.match_filenames_once(FLAGS.eval),\r\n            num_epochs=epoch_number)\r\n        validate_serialized_example = read_and_decode(validate_filename_queue)\r\n        validate_batch_serialized_example = tf.train.shuffle_batch(\r\n            [validate_serialized_example],\r\n            batch_size=validate_batch_size,\r\n            num_threads=thread_number,\r\n            capacity=capacity,\r\n            min_after_dequeue=min_after_dequeue)\r\n        validate_features = tf.parse_example(\r\n            validate_batch_serialized_example,\r\n            features={\r\n                \"label\": tf.FixedLenFeature([], tf.float32),\r\n                \"ids\": tf.VarLenFeature(tf.int64),\r\n                \"values\": tf.VarLenFeature(tf.float32),\r\n            })\r\n        validate_batch_labels = features[\"label\"]\r\n        validate_batch_ids = features[\"ids\"]\r\n        validate_batch_values = features[\"values\"]\r\n        logits = inference(batch_ids, batch_values)\r\n        batch_labels = tf.to_int64(batch_labels)\r\n        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits,\r\n                                                                       batch_labels)\r\n        loss = tf.reduce_mean(cross_entropy, name='loss')\r\n\r\n        print(\"Use the optimizer: {}\".format(FLAGS.optimizer))\r\n        if FLAGS.optimizer == \"sgd\":\r\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"momentum\":\r\n            # optimizer = tf.train.MomentumOptimizer(learning_rate)\r\n            print(\"Not support optimizer: {} yet, exit now\".format(FLAGS.optimizer))\r\n            exit(1)\r\n        elif FLAGS.optimizer == \"adadelta\":\r\n            optimizer = tf.train.AdadeltaOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"adagrad\":\r\n            optimizer = tf.train.AdagradOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"adam\":\r\n            optimizer = tf.train.AdamOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"ftrl\":\r\n            optimizer = tf.train.FtrlOptimizer(learning_rate)\r\n        elif FLAGS.optimizer == \"rmsprop\":\r\n            optimizer = tf.train.RMSPropOptimizer(learning_rate)\r\n        else:\r\n            print(\"Unknow optimizer: {}, exit now\".format(FLAGS.optimizer))\r\n            exit(1)\r\n\r\n        #with tf.device(\"/cpu:0\"):\r\n        global_step = tf.Variable(0, name='global_step', trainable=False)\r\n        train_op = optimizer.minimize(loss, global_step=global_step)\r\n\r\n        # Compute accuracy\r\n        tf.get_variable_scope().reuse_variables()\r\n        accuracy_logits = inference(validate_batch_ids, validate_batch_values)\r\n        validate_softmax = tf.nn.softmax(accuracy_logits)\r\n        validate_batch_labels = tf.to_int64(validate_batch_labels)\r\n        correct_prediction = tf.equal(\r\n            tf.argmax(validate_softmax, 1), validate_batch_labels)\r\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\r\n\r\n        # Compute auc\r\n        validate_batch_labels = tf.cast(validate_batch_labels, tf.int32)\r\n        sparse_labels = tf.reshape(validate_batch_labels, [-1, 1])\r\n        derived_size = tf.shape(validate_batch_labels)[0]\r\n        indices = tf.reshape(tf.range(0, derived_size, 1), [-1, 1])\r\n        concated = tf.concat(1, [indices, sparse_labels])\r\n        outshape = tf.pack([derived_size, LABEL_SIZE])\r\n        new_validate_batch_labels = tf.sparse_to_dense(concated, outshape, 1.0, 0.0)\r\n        _, auc_op = tf.contrib.metrics.streaming_auc(validate_softmax,\r\n                                                     new_validate_batch_labels)\r\n\r\n        # Define inference op\r\n        sparse_index = tf.placeholder(tf.int64)\r\n        sparse_ids = tf.placeholder(tf.int64)\r\n        sparse_values = tf.placeholder(tf.float32)\r\n        sparse_shape = tf.placeholder(tf.int64)\r\n        inference_ids = tf.SparseTensor(sparse_index, sparse_ids, sparse_shape)\r\n        inference_values = tf.SparseTensor(sparse_index, sparse_values, sparse_shape)\r\n        inference_logits = inference(inference_ids, inference_values)\r\n        inference_softmax = tf.nn.softmax(inference_logits)\r\n        inference_op = tf.argmax(inference_softmax, 1)\r\n\r\n        # Initialize saver and summary\r\n        #checkpoint_file = checkpoint_dir + \"checkpoint.ckpt\"\r\n        steps_to_validate = FLAGS.steps_to_validate\r\n        init_op = tf.initialize_all_variables()\r\n\r\n        saver = tf.train.Saver(max_to_keep = 2)\r\n        keys_placeholder = tf.placeholder(\"float\")\r\n        keys = tf.identity(keys_placeholder)\r\n        tf.add_to_collection(\"inputs\", json.dumps({'key': keys_placeholder.name}))\r\n        tf.add_to_collection(\"outputs\", json.dumps({'key': keys.name,\r\n                                                    'softmax': inference_softmax.name,\r\n                                                    'prediction': inference_op.name}))\r\n        tf.scalar_summary('loss', loss)\r\n        tf.scalar_summary('accuracy', accuracy)\r\n        tf.scalar_summary('auc', auc_op)\r\n        summary_op = tf.merge_all_summaries()\r\n\r\n\r\n    sv = tf.train.Supervisor(is_chief=(FLAGS.task_index == 0),\r\n                             logdir=\"./train_process/\",\r\n                             init_op=init_op,\r\n                             summary_op=summary_op,\r\n                             saver=saver,\r\n                             global_step=global_step,\r\n                             save_model_secs=60)\r\n\r\n    # Create session to run graph\r\n    with sv.managed_session(server.target) as sess:\r\n        if mode == \"train\" or mode == \"train_from_scratch\":\r\n            while not sv.should_stop():\r\n                # Get coordinator and run queues to read data\r\n                coord = tf.train.Coordinator()\r\n                threads = tf.train.start_queue_runners(coord=coord, sess=sess)\r\n\r\n                start_time = datetime.datetime.now()\r\n\r\n                try:\r\n                    while not coord.should_stop():\r\n                        _, loss_value, step = sess.run([train_op, loss, global_step])\r\n                        if step % steps_to_validate == 0:\r\n                            accuracy_value, auc_value, summary_value = sess.run(\r\n                                [accuracy, auc_op, summary_op])\r\n                            end_time = datetime.datetime.now()\r\n                            print(\"[{}] Task: {}, Step: {}, loss: {}, accuracy: {}, auc: {}\".format(\r\n                                end_time - start_time,\r\n                                FLAGS.task_index,\r\n                                step, loss_value, accuracy_value,\r\n                                auc_value))\r\n\r\n                            start_time = end_time\r\n                except tf.errors.OutOfRangeError:\r\n                    print(\"Done training after reading all data\")\r\n                finally:\r\n                    coord.request_stop()\r\n                    print(\"coord stopped\")\r\n\r\n                # Wait for threads to exit\r\n                coord.join(threads)\r\n```\r\n\r\n### logs\r\n\r\ngdb python core.xxxxx , and then 'bt', shows that\r\n\r\n```\r\n\r\n#0  0x00007f5943d3f166 in pthread_detach () from /opt/glibc-2.17/lib/libpthread.so.0\r\n#1  0x00007f58fc8c8ab5 in std::thread::detach() ()\r\n    at /opt/install/gcc-build/x86_64-redhat-linux/libstdc++-v3/include/x86_64-redhat-linux/bits/gthr-default.h:674\r\n#2  0x00007f58fec7f0bf in tensorflow::(anonymous namespace)::PosixEnv::SchedClosure(std::function<void ()()>) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#3  0x00007f58fea8e981 in tensorflow::SchedClosure(std::function<void ()()>) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#4  0x00007f58fd5f72cd in tensorflow::Master::RunStep(tensorflow::CallOptions*, tensorflow::RunStepRequest const*, tensorflow::RunStepResponse*, std::function<void ()(tensorflow::Status const&)>) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#5  0x00007f58fd5f10f9 in tensorflow::GrpcMasterService::RunStepHandler(tensorflow::Call<tensorflow::GrpcMasterService, tensorflow::grpc::MasterService::AsyncService, tensorflow::RunStepRequest, tensorflow::RunStepResponse>*) ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#6  0x00007f58fd5f2c4c in tensorflow::GrpcMasterService::HandleRPCsLoop() ()\r\n   from /home/serving/anaconda2/lib/python2.7/site-packages/tensorflow/python/_pywrap_tensorflow.so\r\n#7  0x00007f58fc8c8b80 in execute_native_thread_routine_compat ()\r\n    at ../../../../../gcc-6.2.0/libstdc++-v3/src/c++11/thread.cc:110\r\n#8  0x00007f5943d3df83 in start_thread () from /opt/glibc-2.17/lib/libpthread.so.0\r\n#9  0x00007f59433668bd in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:113\r\n\r\n```"}