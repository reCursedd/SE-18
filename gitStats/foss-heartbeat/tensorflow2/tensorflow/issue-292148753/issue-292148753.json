{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16504", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16504/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16504/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16504/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16504", "id": 292148753, "node_id": "MDU6SXNzdWUyOTIxNDg3NTM=", "number": 16504, "title": "Issue propagating gradients through tf.while_loop", "user": {"login": "fjanoos", "id": 923438, "node_id": "MDQ6VXNlcjkyMzQzOA==", "avatar_url": "https://avatars0.githubusercontent.com/u/923438?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fjanoos", "html_url": "https://github.com/fjanoos", "followers_url": "https://api.github.com/users/fjanoos/followers", "following_url": "https://api.github.com/users/fjanoos/following{/other_user}", "gists_url": "https://api.github.com/users/fjanoos/gists{/gist_id}", "starred_url": "https://api.github.com/users/fjanoos/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fjanoos/subscriptions", "organizations_url": "https://api.github.com/users/fjanoos/orgs", "repos_url": "https://api.github.com/users/fjanoos/repos", "events_url": "https://api.github.com/users/fjanoos/events{/privacy}", "received_events_url": "https://api.github.com/users/fjanoos/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "skye", "id": 88808, "node_id": "MDQ6VXNlcjg4ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/88808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/skye", "html_url": "https://github.com/skye", "followers_url": "https://api.github.com/users/skye/followers", "following_url": "https://api.github.com/users/skye/following{/other_user}", "gists_url": "https://api.github.com/users/skye/gists{/gist_id}", "starred_url": "https://api.github.com/users/skye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/skye/subscriptions", "organizations_url": "https://api.github.com/users/skye/orgs", "repos_url": "https://api.github.com/users/skye/repos", "events_url": "https://api.github.com/users/skye/events{/privacy}", "received_events_url": "https://api.github.com/users/skye/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 15, "created_at": "2018-01-27T20:10:04Z", "updated_at": "2018-04-28T18:26:46Z", "closed_at": "2018-04-28T18:26:46Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu wheezy</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\npip</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\ntf.VERSION = 1.4.0<br>\ntf.GIT_VERSION = v1.4.0-4-g9283868<br>\ntf.COMPILER_VERSION = v1.4.0-4-g9283868<br>\nSanity check: array([1], dtype=int32)</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\n3.5</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I've found a few issues when trying to propagate gradients through tf.while_loops.</p>\n<p>One issue is ops like this inside the loop body break gradients<br>\n<code>k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )</code></p>\n<p>but more concerningly, conjoined conditions such as this:<br>\n<code>tf.logical_and( k &lt; max_its-1, chg_w &gt; tol  )</code><br>\nor even this<br>\n<code> tf.cast( max_its-k, DTYPE) *(chg_w - tol)</code><br>\nbreaks the differentiablity across the while loop.</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre>tf.reset_default_graph()\nsess <span class=\"pl-k\">=</span> tf.InteractiveSession()\ng <span class=\"pl-k\">=</span> tf.Graph().as_default()\n\nmax_its <span class=\"pl-k\">=</span> <span class=\"pl-c1\">10</span>\ntol <span class=\"pl-k\">=</span> <span class=\"pl-c1\">1e-3</span>\n\nc <span class=\"pl-k\">=</span> tf.constant( np.arange(<span class=\"pl-c1\">100</span>), <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DTYPE</span>)\nw <span class=\"pl-k\">=</span> tf.Variable( <span class=\"pl-v\">initial_value</span><span class=\"pl-k\">=</span>np.ones(<span class=\"pl-c1\">100</span>),  <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DTYPE</span>)<span class=\"pl-k\">/</span><span class=\"pl-c1\">100</span>\nk <span class=\"pl-k\">=</span> tf.Variable( <span class=\"pl-c1\">0</span>, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.int32 )\nchg_w <span class=\"pl-k\">=</span> tf.constant( np.inf, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">DTYPE</span> )\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_eg_step</span>( <span class=\"pl-smi\">k</span>, <span class=\"pl-smi\">w</span>, <span class=\"pl-smi\">chg_w</span>): \n    grad <span class=\"pl-k\">=</span> tf.gradients( <span class=\"pl-k\">-</span>tf.reduce_sum( w <span class=\"pl-k\">*</span> c ) , w )[<span class=\"pl-c1\">0</span>]\n    w_n <span class=\"pl-k\">=</span> w <span class=\"pl-k\">*</span> tf.exp( <span class=\"pl-k\">-</span><span class=\"pl-c1\">0.1</span>  <span class=\"pl-k\">*</span> grad )\n    w_n <span class=\"pl-k\">=</span> w_n <span class=\"pl-k\">/</span> tf.reduce_sum( w_n ) \n    chg_w <span class=\"pl-k\">=</span> tf.reduce_sum( tf.abs( w_n <span class=\"pl-k\">-</span> w) ) <span class=\"pl-k\">/</span> tf.reduce_sum( tf.abs( w ) )\n    k <span class=\"pl-k\">=</span> k <span class=\"pl-k\">+</span> <span class=\"pl-c1\">1</span>\n    <span class=\"pl-k\">**</span><span class=\"pl-c\"><span class=\"pl-c\">#</span> !! this busts the differentiablity !!</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )**</span>\n    <span class=\"pl-k\">return</span> k, w_n, chg_w\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_continue_cond</span>( <span class=\"pl-smi\">k</span>, <span class=\"pl-smi\">w</span>, <span class=\"pl-smi\">chg_w</span>, <span class=\"pl-k\">*</span><span class=\"pl-smi\">args</span> ):\n    <span class=\"pl-k\">**</span><span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">NOTE</span> either of this conjoined conditions</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span>        tf.logical_and( k &lt; max_its-1, chg_w &gt; tol  )</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> OR     tf.cast( max_its-k, DTYPE) *(chg_w - tol)</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> do no propagate gradients correctly**</span>\n    <span class=\"pl-k\">return</span>  k <span class=\"pl-k\">&lt;</span> max_its <span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.logical_and( k &lt; max_its-1, chg_w &gt; tol  )</span>\n\nk, w, chg_w <span class=\"pl-k\">=</span> tf.while_loop(\n    <span class=\"pl-v\">cond</span><span class=\"pl-k\">=</span>_continue_cond,  <span class=\"pl-v\">body</span><span class=\"pl-k\">=</span>_eg_step,\n    <span class=\"pl-v\">loop_vars</span><span class=\"pl-k\">=</span>[k, w, chg_w],\n    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>while_loop<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">parallel_iterations</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>\n)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> see if the gradient is propagated</span>\ntf.gradients( w, c)\n</pre></div>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu wheezy\n\n\nTensorFlow installed from (source or binary):\npip\n\n\nTensorFlow version (use command below):\ntf.VERSION = 1.4.0\ntf.GIT_VERSION = v1.4.0-4-g9283868\ntf.COMPILER_VERSION = v1.4.0-4-g9283868\nSanity check: array([1], dtype=int32)\n\n\nPython version:\n3.5\n\n\nDescribe the problem\nI've found a few issues when trying to propagate gradients through tf.while_loops.\nOne issue is ops like this inside the loop body break gradients\nk = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )\nbut more concerningly, conjoined conditions such as this:\ntf.logical_and( k < max_its-1, chg_w > tol  )\nor even this\n tf.cast( max_its-k, DTYPE) *(chg_w - tol)\nbreaks the differentiablity across the while loop.\nSource code / logs\ntf.reset_default_graph()\nsess = tf.InteractiveSession()\ng = tf.Graph().as_default()\n\nmax_its = 10\ntol = 1e-3\n\nc = tf.constant( np.arange(100), dtype=DTYPE)\nw = tf.Variable( initial_value=np.ones(100),  dtype=DTYPE)/100\nk = tf.Variable( 0, dtype=tf.int32 )\nchg_w = tf.constant( np.inf, dtype=DTYPE )\n\n\ndef _eg_step( k, w, chg_w): \n    grad = tf.gradients( -tf.reduce_sum( w * c ) , w )[0]\n    w_n = w * tf.exp( -0.1  * grad )\n    w_n = w_n / tf.reduce_sum( w_n ) \n    chg_w = tf.reduce_sum( tf.abs( w_n - w) ) / tf.reduce_sum( tf.abs( w ) )\n    k = k + 1\n    **# !! this busts the differentiablity !!\n    # k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )**\n    return k, w_n, chg_w\n\ndef _continue_cond( k, w, chg_w, *args ):\n    **# NOTE either of this conjoined conditions\n    #        tf.logical_and( k < max_its-1, chg_w > tol  )\n    # OR     tf.cast( max_its-k, DTYPE) *(chg_w - tol)\n    # do no propagate gradients correctly**\n    return  k < max_its # tf.logical_and( k < max_its-1, chg_w > tol  )\n\nk, w, chg_w = tf.while_loop(\n    cond=_continue_cond,  body=_eg_step,\n    loop_vars=[k, w, chg_w],\n    name='while_loop', parallel_iterations=1\n)\n\n# see if the gradient is propagated\ntf.gradients( w, c)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nUbuntu wheezy\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\npip\r\n\r\n- **TensorFlow version (use command below)**:\r\ntf.VERSION = 1.4.0\r\ntf.GIT_VERSION = v1.4.0-4-g9283868\r\ntf.COMPILER_VERSION = v1.4.0-4-g9283868\r\nSanity check: array([1], dtype=int32)\r\n\r\n- **Python version**: \r\n3.5\r\n\r\n### Describe the problem\r\n\r\nI've found a few issues when trying to propagate gradients through tf.while_loops.\r\n\r\nOne issue is ops like this inside the loop body break gradients \r\n```k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )```\r\n\r\nbut more concerningly, conjoined conditions such as this:\r\n```tf.logical_and( k < max_its-1, chg_w > tol  )```\r\nor even this\r\n``` tf.cast( max_its-k, DTYPE) *(chg_w - tol)```\r\nbreaks the differentiablity across the while loop.\r\n\r\n\r\n### Source code / logs\r\n``` python\r\ntf.reset_default_graph()\r\nsess = tf.InteractiveSession()\r\ng = tf.Graph().as_default()\r\n\r\nmax_its = 10\r\ntol = 1e-3\r\n\r\nc = tf.constant( np.arange(100), dtype=DTYPE)\r\nw = tf.Variable( initial_value=np.ones(100),  dtype=DTYPE)/100\r\nk = tf.Variable( 0, dtype=tf.int32 )\r\nchg_w = tf.constant( np.inf, dtype=DTYPE )\r\n\r\n\r\ndef _eg_step( k, w, chg_w): \r\n    grad = tf.gradients( -tf.reduce_sum( w * c ) , w )[0]\r\n    w_n = w * tf.exp( -0.1  * grad )\r\n    w_n = w_n / tf.reduce_sum( w_n ) \r\n    chg_w = tf.reduce_sum( tf.abs( w_n - w) ) / tf.reduce_sum( tf.abs( w ) )\r\n    k = k + 1\r\n    **# !! this busts the differentiablity !!\r\n    # k = tf.Print( k + 1, [k + 1, eta, loss( w_n ), chg_w, G_inf], 'EG:: k, eta, loss(w), chg_w, G_inf = ' )**\r\n    return k, w_n, chg_w\r\n\r\ndef _continue_cond( k, w, chg_w, *args ):\r\n    **# NOTE either of this conjoined conditions\r\n    #        tf.logical_and( k < max_its-1, chg_w > tol  )\r\n    # OR     tf.cast( max_its-k, DTYPE) *(chg_w - tol)\r\n    # do no propagate gradients correctly**\r\n    return  k < max_its # tf.logical_and( k < max_its-1, chg_w > tol  )\r\n\r\nk, w, chg_w = tf.while_loop(\r\n    cond=_continue_cond,  body=_eg_step,\r\n    loop_vars=[k, w, chg_w],\r\n    name='while_loop', parallel_iterations=1\r\n)\r\n\r\n# see if the gradient is propagated\r\ntf.gradients( w, c)\r\n\r\n```"}