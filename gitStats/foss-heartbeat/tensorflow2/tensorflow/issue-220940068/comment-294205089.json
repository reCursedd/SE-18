{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/294205089", "html_url": "https://github.com/tensorflow/tensorflow/issues/9136#issuecomment-294205089", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9136", "id": 294205089, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDIwNTA4OQ==", "user": {"login": "asimshankar", "id": 16018, "node_id": "MDQ6VXNlcjE2MDE4", "avatar_url": "https://avatars2.githubusercontent.com/u/16018?v=4", "gravatar_id": "", "url": "https://api.github.com/users/asimshankar", "html_url": "https://github.com/asimshankar", "followers_url": "https://api.github.com/users/asimshankar/followers", "following_url": "https://api.github.com/users/asimshankar/following{/other_user}", "gists_url": "https://api.github.com/users/asimshankar/gists{/gist_id}", "starred_url": "https://api.github.com/users/asimshankar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/asimshankar/subscriptions", "organizations_url": "https://api.github.com/users/asimshankar/orgs", "repos_url": "https://api.github.com/users/asimshankar/repos", "events_url": "https://api.github.com/users/asimshankar/events{/privacy}", "received_events_url": "https://api.github.com/users/asimshankar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-14T18:09:20Z", "updated_at": "2017-04-14T18:11:05Z", "author_association": "MEMBER", "body_html": "<p>Thanks for the detailed report and stacktraces, this helps a lot and is much appreciated.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> pointed out that we might have a bug when graphs are extended in a distributed session while some operations (in this case the enqueue operation) are in progress (See <a href=\"https://github.com/tensorflow/tensorflow/blob/87cdfafd44ff5e332fd820608783432fea83a4c9/tensorflow/core/distributed_runtime/master_session.cc#L1038\"><code>master_session.cc:1038</code></a> - that code predates the queue runners).</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1450614\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/suharshs\">@suharshs</a> : Would you have the bandwidth to look into that TODO?</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=3336429\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jdonier\">@jdonier</a> : In the mean time, a workaround for you would be to ensure that the graph isn't modified after the queue runners are started. For example, your snippet above could be rewritten as:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">import</span> time\n\ncluster <span class=\"pl-k\">=</span> tf.train.ClusterSpec({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu1<span class=\"pl-pds\">\"</span></span> : [<span class=\"pl-s\"><span class=\"pl-pds\">'</span>localhost:2222<span class=\"pl-pds\">'</span></span>]})\nserver <span class=\"pl-k\">=</span> tf.train.Server(cluster, <span class=\"pl-v\">job_name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>cpu1<span class=\"pl-pds\">\"</span></span>, <span class=\"pl-v\">task_index</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0</span>)\n\n<span class=\"pl-k\">with</span> tf.Graph().as_default() <span class=\"pl-k\">as</span> graph:\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Queue</span>\n    input_queue <span class=\"pl-k\">=</span> tf.train.input_producer(tf.constant([<span class=\"pl-c1\">0</span>.], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32))\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Useless variable</span>\n    variable <span class=\"pl-k\">=</span> tf.Variable(<span class=\"pl-c1\">1</span>., <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">trainable</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>variable<span class=\"pl-pds\">\"</span></span>)\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> Session and queue runners</span>\n    session <span class=\"pl-k\">=</span> tf.Session(<span class=\"pl-v\">target</span><span class=\"pl-k\">=</span>server.target)\n    session.run(tf.global_variables_initializer())\n\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> CHANGE FROM PREVIOUS SNIPPET: Assign operations</span>\n    assign2 <span class=\"pl-k\">=</span> tf.assign(variable, <span class=\"pl-c1\">2</span>)\n    assign3 <span class=\"pl-k\">=</span> tf.assign(variable, <span class=\"pl-c1\">3</span>)\n\n    tf.train.start_queue_runners(session)\n\n<span class=\"pl-c1\">print</span>(session.run(variable))\n<span class=\"pl-c1\">print</span>(session.run(assign2))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Freely sleep</span>\ntime.sleep(<span class=\"pl-c1\">1</span>)\n\n<span class=\"pl-c1\">print</span>(session.run(variable))\n<span class=\"pl-c1\">print</span>(session.run(assign3))</pre></div>\n<p>FYI <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=170179\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jhseu\">@jhseu</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1284535\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/saeta\">@saeta</a> who might like to know about this too.</p>", "body_text": "Thanks for the detailed report and stacktraces, this helps a lot and is much appreciated.\n@mrry pointed out that we might have a bug when graphs are extended in a distributed session while some operations (in this case the enqueue operation) are in progress (See master_session.cc:1038 - that code predates the queue runners).\n@suharshs : Would you have the bandwidth to look into that TODO?\n@jdonier : In the mean time, a workaround for you would be to ensure that the graph isn't modified after the queue runners are started. For example, your snippet above could be rewritten as:\nimport tensorflow as tf\nimport time\n\ncluster = tf.train.ClusterSpec({\"cpu1\" : ['localhost:2222']})\nserver = tf.train.Server(cluster, job_name=\"cpu1\", task_index=0)\n\nwith tf.Graph().as_default() as graph:\n    # Queue\n    input_queue = tf.train.input_producer(tf.constant([0.], dtype=tf.float32))\n\n    # Useless variable\n    variable = tf.Variable(1., dtype=tf.float32, trainable=False, name=\"variable\")\n\n    # Session and queue runners\n    session = tf.Session(target=server.target)\n    session.run(tf.global_variables_initializer())\n\n    # CHANGE FROM PREVIOUS SNIPPET: Assign operations\n    assign2 = tf.assign(variable, 2)\n    assign3 = tf.assign(variable, 3)\n\n    tf.train.start_queue_runners(session)\n\nprint(session.run(variable))\nprint(session.run(assign2))\n\n# Freely sleep\ntime.sleep(1)\n\nprint(session.run(variable))\nprint(session.run(assign3))\nFYI @jhseu @saeta who might like to know about this too.", "body": "Thanks for the detailed report and stacktraces, this helps a lot and is much appreciated.\r\n\r\n@mrry pointed out that we might have a bug when graphs are extended in a distributed session while some operations (in this case the enqueue operation) are in progress (See [`master_session.cc:1038`](https://github.com/tensorflow/tensorflow/blob/87cdfafd44ff5e332fd820608783432fea83a4c9/tensorflow/core/distributed_runtime/master_session.cc#L1038) - that code predates the queue runners).\r\n\r\n@suharshs : Would you have the bandwidth to look into that TODO?\r\n\r\n@jdonier : In the mean time, a workaround for you would be to ensure that the graph isn't modified after the queue runners are started. For example, your snippet above could be rewritten as:\r\n\r\n```python\r\nimport tensorflow as tf\r\nimport time\r\n\r\ncluster = tf.train.ClusterSpec({\"cpu1\" : ['localhost:2222']})\r\nserver = tf.train.Server(cluster, job_name=\"cpu1\", task_index=0)\r\n\r\nwith tf.Graph().as_default() as graph:\r\n    # Queue\r\n    input_queue = tf.train.input_producer(tf.constant([0.], dtype=tf.float32))\r\n\r\n    # Useless variable\r\n    variable = tf.Variable(1., dtype=tf.float32, trainable=False, name=\"variable\")\r\n\r\n    # Session and queue runners\r\n    session = tf.Session(target=server.target)\r\n    session.run(tf.global_variables_initializer())\r\n\r\n    # CHANGE FROM PREVIOUS SNIPPET: Assign operations\r\n    assign2 = tf.assign(variable, 2)\r\n    assign3 = tf.assign(variable, 3)\r\n\r\n    tf.train.start_queue_runners(session)\r\n\r\nprint(session.run(variable))\r\nprint(session.run(assign2))\r\n\r\n# Freely sleep\r\ntime.sleep(1)\r\n\r\nprint(session.run(variable))\r\nprint(session.run(assign3))\r\n```\r\n\r\nFYI @jhseu @saeta who might like to know about this too."}