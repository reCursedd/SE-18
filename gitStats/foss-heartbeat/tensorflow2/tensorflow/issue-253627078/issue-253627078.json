{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12681", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12681/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12681/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12681/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12681", "id": 253627078, "node_id": "MDU6SXNzdWUyNTM2MjcwNzg=", "number": 12681, "title": "Feature request: dilated pooling", "user": {"login": "rcasero", "id": 1219084, "node_id": "MDQ6VXNlcjEyMTkwODQ=", "avatar_url": "https://avatars3.githubusercontent.com/u/1219084?v=4", "gravatar_id": "", "url": "https://api.github.com/users/rcasero", "html_url": "https://github.com/rcasero", "followers_url": "https://api.github.com/users/rcasero/followers", "following_url": "https://api.github.com/users/rcasero/following{/other_user}", "gists_url": "https://api.github.com/users/rcasero/gists{/gist_id}", "starred_url": "https://api.github.com/users/rcasero/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/rcasero/subscriptions", "organizations_url": "https://api.github.com/users/rcasero/orgs", "repos_url": "https://api.github.com/users/rcasero/repos", "events_url": "https://api.github.com/users/rcasero/events{/privacy}", "received_events_url": "https://api.github.com/users/rcasero/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 7, "created_at": "2017-08-29T11:46:32Z", "updated_at": "2018-07-24T12:22:45Z", "closed_at": "2018-01-19T05:26:44Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I posted initially in keras-users, but <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a> suggested that this would need to be implemented first at TensorFlow level</p>\n<p><a href=\"https://groups.google.com/forum/#!topic/keras-users/ZVtI8Ef6508\" rel=\"nofollow\">https://groups.google.com/forum/#!topic/keras-users/ZVtI8Ef6508</a></p>\n<p>Similarly to dilated convolution, dilated pooling uses a dilated kernel. For example, dilated kernel with pool_size=(3,2), dilation_rate=(2,4) (* = non-zero elements: this is where values are sampled to compute max pooling)</p>\n<pre><code>    * 0 0 0 *\n    0 0 0 0 0\n    * 0 0 0 *\n    0 0 0 0 0\n    * 0 0 0 *\n</code></pre>\n<p>This was proposed in</p>\n<p>Li H, Zhao R, Wang X. Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification. arXiv preprint arXiv:14124526. 2014.</p>\n<p>and it's used by DeepCell</p>\n<p>Van Valen et al. (2016), PLoS Comput Biol, \"Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments\"\u200b <a href=\"http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177\" rel=\"nofollow\">http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177</a></p>\n<p>DeepCell is state of the art for cell segmentation, but it's implemented with Keras 1, and ad-hoc code based on a bit outdated Theano.</p>\n<p><a href=\"https://github.com/CovertLab/DeepCell\">https://github.com/CovertLab/DeepCell</a></p>\n<p>To the best of my knowledge, there's a lack of testing of whether dilation in pooling improves segmentation. I have written one in Keras, by shifting the kernel and slicing the image, then running regular 2D pooling on each slice, and reassembling the results (code at the end of post, but probably not very interesting). However, this approach is probably quite inefficient.</p>\n<p>The question would be whether any TF developer would be willing to extend the current tf.contrib.keras.backend.pool2d</p>\n<pre><code>pool2d(\n    x,\n    pool_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    pool_mode='max'\n)\n\n</code></pre>\n<p>with a <code>dilation_rate</code> argument, and the functionality at that level. From there, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a> is happy to modify the Keras API to include this option.</p>\n<p>Code for Keras implementation of dilated 2D pooling</p>\n<pre><code>    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format, dilation_rate):\n\n        # inputs for tests\n        inputs = K.variable(np.reshape(range(1,4*3*5*8+1), (4, 3, 5, 8)))########\n        inputs = K.variable(np.reshape(range(1,1*1*5*8+1), (1, 1, 5, 8)))########\n        \n        if data_format == 'channels_first': # (batch,chan,row,col)\n            nbatch = K.get_variable_shape(inputs)[0]\n            #nchan = K.get_variable_shape(inputs)[1]\n            nrows = K.get_variable_shape(inputs)[2]\n            ncols = K.get_variable_shape(inputs)[3]\n        elif data_format == 'channels_last': # (batch,row,col,chan)\n            nbatch = K.get_variable_shape(inputs)[0]\n            #nchan = K.get_variable_shape(inputs)[1]\n            nrows = K.get_variable_shape(inputs)[2]\n            ncols = K.get_variable_shape(inputs)[3]\n        else:\n            raise ValueError('Expected data format to be channels_first or channels_last')\n\n        # number of blocks to split the input into. Each dilation (row or \n        # column) goes into a separate block\n        nblocks = dilation_rate\n        \n        # size of each block we are going to split the input images in\n        block_sz = (int(np.ceil(nrows / dilation_rate[0])), \n                    int(np.ceil(ncols / dilation_rate[1])))\n\n        # pad inputs so that they can be split into equal blocks\n        padded_size = np.multiply(block_sz, nblocks)\n        padding_len = ((0, padded_size[0] - nrows), (0, padded_size[1] - ncols))\n        inputs = K.spatial_2d_padding(inputs, padding=padding_len, data_format=data_format)\n \n        # split the inputs into blocks\n        split_inputs = []\n        for row_offset in range(nblocks[0]):\n            for col_offset in range(nblocks[1]):\n                if data_format == 'channels_first': # (batch,chan,row,col)\n                    split_inputs = split_inputs + [inputs[:, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]]]\n                elif data_format == 'channels_last': # (batch,row,col,chan)\n                    split_inputs = split_inputs + [inputs[:, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :]]\n        split_inputs = K.concatenate(split_inputs, axis=0)                        \n    \n        # pool each block without dilation\n        split_inputs = K.pool2d(split_inputs, pool_size, strides=(1,1),\n                                padding='same', data_format=data_format,\n                                pool_mode='max')\n\n        # reassemble blocks\n        output = np.zeros(shape=list(inputs.shape.eval()), dtype=inputs.dtype)\n        for idx in range(nbatch*nblocks[0]*nblocks[1]):\n            multi_index = np.unravel_index(idx, dims=(nblocks[0], nblocks[1], nbatch))\n            row_offset = multi_index[0]\n            col_offset = multi_index[1]\n            batch = multi_index[2]\n            if data_format == 'channels_first': # (batch,chan,row,col)\n                output[batch, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]] = split_inputs[idx, :, :, :].eval()\n            elif data_format == 'channels_last': # (batch,row,col,chan)\n                output[batch, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :] = split_inputs[idx, :, :, :].eval()\n        output = K.variable(output)\n        \n        # remove padding\n        if padding == 'valid':\n            \n            if data_format == 'channels_first': # (batch,chan,row,col)\n                output = output[:, :, (pool_size[0]-1)*dilation_rate[0]:nrows, \n                                (pool_size[1]-1)*dilation_rate[1]:ncols]\n            elif data_format == 'channels_last': # (batch,row,col,chan)\n                output = output[:, (pool_size[0]-1)*dilation_rate[0]:nrows, \n                                (pool_size[1]-1)*dilation_rate[1]:ncols, :]\n                    \n        elif padding == 'same':\n        \n            if data_format == 'channels_first': # (batch,chan,row,col)\n                output = output[:, :, 0:nrows, 0:ncols]\n            elif data_format == 'channels_last': # (batch,row,col,chan)\n                output = output[:, 0:nrows, 0:ncols, :]\n                \n        else:\n            \n            raise NotImplementedError\n\n        # return tensor\n        return output\n\n</code></pre>", "body_text": "Hi,\nI posted initially in keras-users, but @fchollet suggested that this would need to be implemented first at TensorFlow level\nhttps://groups.google.com/forum/#!topic/keras-users/ZVtI8Ef6508\nSimilarly to dilated convolution, dilated pooling uses a dilated kernel. For example, dilated kernel with pool_size=(3,2), dilation_rate=(2,4) (* = non-zero elements: this is where values are sampled to compute max pooling)\n    * 0 0 0 *\n    0 0 0 0 0\n    * 0 0 0 *\n    0 0 0 0 0\n    * 0 0 0 *\n\nThis was proposed in\nLi H, Zhao R, Wang X. Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification. arXiv preprint arXiv:14124526. 2014.\nand it's used by DeepCell\nVan Valen et al. (2016), PLoS Comput Biol, \"Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments\"\u200b http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177\nDeepCell is state of the art for cell segmentation, but it's implemented with Keras 1, and ad-hoc code based on a bit outdated Theano.\nhttps://github.com/CovertLab/DeepCell\nTo the best of my knowledge, there's a lack of testing of whether dilation in pooling improves segmentation. I have written one in Keras, by shifting the kernel and slicing the image, then running regular 2D pooling on each slice, and reassembling the results (code at the end of post, but probably not very interesting). However, this approach is probably quite inefficient.\nThe question would be whether any TF developer would be willing to extend the current tf.contrib.keras.backend.pool2d\npool2d(\n    x,\n    pool_size,\n    strides=(1, 1),\n    padding='valid',\n    data_format=None,\n    pool_mode='max'\n)\n\n\nwith a dilation_rate argument, and the functionality at that level. From there, @fchollet is happy to modify the Keras API to include this option.\nCode for Keras implementation of dilated 2D pooling\n    def _pooling_function(self, inputs, pool_size, strides,\n                          padding, data_format, dilation_rate):\n\n        # inputs for tests\n        inputs = K.variable(np.reshape(range(1,4*3*5*8+1), (4, 3, 5, 8)))########\n        inputs = K.variable(np.reshape(range(1,1*1*5*8+1), (1, 1, 5, 8)))########\n        \n        if data_format == 'channels_first': # (batch,chan,row,col)\n            nbatch = K.get_variable_shape(inputs)[0]\n            #nchan = K.get_variable_shape(inputs)[1]\n            nrows = K.get_variable_shape(inputs)[2]\n            ncols = K.get_variable_shape(inputs)[3]\n        elif data_format == 'channels_last': # (batch,row,col,chan)\n            nbatch = K.get_variable_shape(inputs)[0]\n            #nchan = K.get_variable_shape(inputs)[1]\n            nrows = K.get_variable_shape(inputs)[2]\n            ncols = K.get_variable_shape(inputs)[3]\n        else:\n            raise ValueError('Expected data format to be channels_first or channels_last')\n\n        # number of blocks to split the input into. Each dilation (row or \n        # column) goes into a separate block\n        nblocks = dilation_rate\n        \n        # size of each block we are going to split the input images in\n        block_sz = (int(np.ceil(nrows / dilation_rate[0])), \n                    int(np.ceil(ncols / dilation_rate[1])))\n\n        # pad inputs so that they can be split into equal blocks\n        padded_size = np.multiply(block_sz, nblocks)\n        padding_len = ((0, padded_size[0] - nrows), (0, padded_size[1] - ncols))\n        inputs = K.spatial_2d_padding(inputs, padding=padding_len, data_format=data_format)\n \n        # split the inputs into blocks\n        split_inputs = []\n        for row_offset in range(nblocks[0]):\n            for col_offset in range(nblocks[1]):\n                if data_format == 'channels_first': # (batch,chan,row,col)\n                    split_inputs = split_inputs + [inputs[:, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]]]\n                elif data_format == 'channels_last': # (batch,row,col,chan)\n                    split_inputs = split_inputs + [inputs[:, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :]]\n        split_inputs = K.concatenate(split_inputs, axis=0)                        \n    \n        # pool each block without dilation\n        split_inputs = K.pool2d(split_inputs, pool_size, strides=(1,1),\n                                padding='same', data_format=data_format,\n                                pool_mode='max')\n\n        # reassemble blocks\n        output = np.zeros(shape=list(inputs.shape.eval()), dtype=inputs.dtype)\n        for idx in range(nbatch*nblocks[0]*nblocks[1]):\n            multi_index = np.unravel_index(idx, dims=(nblocks[0], nblocks[1], nbatch))\n            row_offset = multi_index[0]\n            col_offset = multi_index[1]\n            batch = multi_index[2]\n            if data_format == 'channels_first': # (batch,chan,row,col)\n                output[batch, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]] = split_inputs[idx, :, :, :].eval()\n            elif data_format == 'channels_last': # (batch,row,col,chan)\n                output[batch, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :] = split_inputs[idx, :, :, :].eval()\n        output = K.variable(output)\n        \n        # remove padding\n        if padding == 'valid':\n            \n            if data_format == 'channels_first': # (batch,chan,row,col)\n                output = output[:, :, (pool_size[0]-1)*dilation_rate[0]:nrows, \n                                (pool_size[1]-1)*dilation_rate[1]:ncols]\n            elif data_format == 'channels_last': # (batch,row,col,chan)\n                output = output[:, (pool_size[0]-1)*dilation_rate[0]:nrows, \n                                (pool_size[1]-1)*dilation_rate[1]:ncols, :]\n                    \n        elif padding == 'same':\n        \n            if data_format == 'channels_first': # (batch,chan,row,col)\n                output = output[:, :, 0:nrows, 0:ncols]\n            elif data_format == 'channels_last': # (batch,row,col,chan)\n                output = output[:, 0:nrows, 0:ncols, :]\n                \n        else:\n            \n            raise NotImplementedError\n\n        # return tensor\n        return output", "body": "Hi,\r\n\r\nI posted initially in keras-users, but @fchollet suggested that this would need to be implemented first at TensorFlow level\r\n\r\nhttps://groups.google.com/forum/#!topic/keras-users/ZVtI8Ef6508\r\n\r\nSimilarly to dilated convolution, dilated pooling uses a dilated kernel. For example, dilated kernel with pool_size=(3,2), dilation_rate=(2,4) (* = non-zero elements: this is where values are sampled to compute max pooling)\r\n        \r\n        * 0 0 0 *\r\n        0 0 0 0 0\r\n        * 0 0 0 *\r\n        0 0 0 0 0\r\n        * 0 0 0 *\r\n\r\nThis was proposed in \r\n\r\nLi H, Zhao R, Wang X. Highly efficient forward and backward propagation of convolutional neural networks for pixelwise classification. arXiv preprint arXiv:14124526. 2014.\r\n\r\nand it's used by DeepCell\r\n\r\nVan Valen et al. (2016), PLoS Comput Biol, \"Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments\"\u200b http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005177\r\n\r\nDeepCell is state of the art for cell segmentation, but it's implemented with Keras 1, and ad-hoc code based on a bit outdated Theano.\r\n\r\nhttps://github.com/CovertLab/DeepCell\r\n\r\nTo the best of my knowledge, there's a lack of testing of whether dilation in pooling improves segmentation. I have written one in Keras, by shifting the kernel and slicing the image, then running regular 2D pooling on each slice, and reassembling the results (code at the end of post, but probably not very interesting). However, this approach is probably quite inefficient. \r\n\r\nThe question would be whether any TF developer would be willing to extend the current tf.contrib.keras.backend.pool2d\r\n\r\n```\r\npool2d(\r\n    x,\r\n    pool_size,\r\n    strides=(1, 1),\r\n    padding='valid',\r\n    data_format=None,\r\n    pool_mode='max'\r\n)\r\n\r\n```\r\n\r\nwith a `dilation_rate` argument, and the functionality at that level. From there, @fchollet is happy to modify the Keras API to include this option.\r\n\r\n\r\nCode for Keras implementation of dilated 2D pooling\r\n\r\n```\r\n    def _pooling_function(self, inputs, pool_size, strides,\r\n                          padding, data_format, dilation_rate):\r\n\r\n        # inputs for tests\r\n        inputs = K.variable(np.reshape(range(1,4*3*5*8+1), (4, 3, 5, 8)))########\r\n        inputs = K.variable(np.reshape(range(1,1*1*5*8+1), (1, 1, 5, 8)))########\r\n        \r\n        if data_format == 'channels_first': # (batch,chan,row,col)\r\n            nbatch = K.get_variable_shape(inputs)[0]\r\n            #nchan = K.get_variable_shape(inputs)[1]\r\n            nrows = K.get_variable_shape(inputs)[2]\r\n            ncols = K.get_variable_shape(inputs)[3]\r\n        elif data_format == 'channels_last': # (batch,row,col,chan)\r\n            nbatch = K.get_variable_shape(inputs)[0]\r\n            #nchan = K.get_variable_shape(inputs)[1]\r\n            nrows = K.get_variable_shape(inputs)[2]\r\n            ncols = K.get_variable_shape(inputs)[3]\r\n        else:\r\n            raise ValueError('Expected data format to be channels_first or channels_last')\r\n\r\n        # number of blocks to split the input into. Each dilation (row or \r\n        # column) goes into a separate block\r\n        nblocks = dilation_rate\r\n        \r\n        # size of each block we are going to split the input images in\r\n        block_sz = (int(np.ceil(nrows / dilation_rate[0])), \r\n                    int(np.ceil(ncols / dilation_rate[1])))\r\n\r\n        # pad inputs so that they can be split into equal blocks\r\n        padded_size = np.multiply(block_sz, nblocks)\r\n        padding_len = ((0, padded_size[0] - nrows), (0, padded_size[1] - ncols))\r\n        inputs = K.spatial_2d_padding(inputs, padding=padding_len, data_format=data_format)\r\n \r\n        # split the inputs into blocks\r\n        split_inputs = []\r\n        for row_offset in range(nblocks[0]):\r\n            for col_offset in range(nblocks[1]):\r\n                if data_format == 'channels_first': # (batch,chan,row,col)\r\n                    split_inputs = split_inputs + [inputs[:, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]]]\r\n                elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                    split_inputs = split_inputs + [inputs[:, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :]]\r\n        split_inputs = K.concatenate(split_inputs, axis=0)                        \r\n    \r\n        # pool each block without dilation\r\n        split_inputs = K.pool2d(split_inputs, pool_size, strides=(1,1),\r\n                                padding='same', data_format=data_format,\r\n                                pool_mode='max')\r\n\r\n        # reassemble blocks\r\n        output = np.zeros(shape=list(inputs.shape.eval()), dtype=inputs.dtype)\r\n        for idx in range(nbatch*nblocks[0]*nblocks[1]):\r\n            multi_index = np.unravel_index(idx, dims=(nblocks[0], nblocks[1], nbatch))\r\n            row_offset = multi_index[0]\r\n            col_offset = multi_index[1]\r\n            batch = multi_index[2]\r\n            if data_format == 'channels_first': # (batch,chan,row,col)\r\n                output[batch, :, row_offset::dilation_rate[0], col_offset::dilation_rate[1]] = split_inputs[idx, :, :, :].eval()\r\n            elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                output[batch, row_offset::dilation_rate[0], col_offset::dilation_rate[1], :] = split_inputs[idx, :, :, :].eval()\r\n        output = K.variable(output)\r\n        \r\n        # remove padding\r\n        if padding == 'valid':\r\n            \r\n            if data_format == 'channels_first': # (batch,chan,row,col)\r\n                output = output[:, :, (pool_size[0]-1)*dilation_rate[0]:nrows, \r\n                                (pool_size[1]-1)*dilation_rate[1]:ncols]\r\n            elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                output = output[:, (pool_size[0]-1)*dilation_rate[0]:nrows, \r\n                                (pool_size[1]-1)*dilation_rate[1]:ncols, :]\r\n                    \r\n        elif padding == 'same':\r\n        \r\n            if data_format == 'channels_first': # (batch,chan,row,col)\r\n                output = output[:, :, 0:nrows, 0:ncols]\r\n            elif data_format == 'channels_last': # (batch,row,col,chan)\r\n                output = output[:, 0:nrows, 0:ncols, :]\r\n                \r\n        else:\r\n            \r\n            raise NotImplementedError\r\n\r\n        # return tensor\r\n        return output\r\n\r\n```"}