{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276127833", "html_url": "https://github.com/tensorflow/tensorflow/issues/4552#issuecomment-276127833", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552", "id": 276127833, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjEyNzgzMw==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-30T17:26:45Z", "updated_at": "2017-01-30T17:28:31Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Memory efficient execution is an open research area, and it's good to have a concrete application in mind to have a practical solution. For instance I found that \"out of memory\" was often caused by bad execution order, rather than executing too many of them concurrently.</p>\n<p>IE consider computational graph below<br>\n<a target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://cloud.githubusercontent.com/assets/23068/22432653/7dc7bf20-e6ca-11e6-879c-14880a4d26d3.png\"><img src=\"https://cloud.githubusercontent.com/assets/23068/22432653/7dc7bf20-e6ca-11e6-879c-14880a4d26d3.png\" alt=\"caterpillar\" style=\"max-width:100%;\"></a></p>\n<p>TF schedules things as soon as they are ready to execute, so if \"square\" op takes long to compute, it'll compute all the \"circles\", and then keep them in memory, and then compute the squares. This needs O(n) memory where <code>n</code> is the length of chain, whereas memory-optimal execution need O(1) memory and will go \"circle-&gt;square-&gt;circle-&gt;square\".</p>\n<p>The general problem is known in computer science as \"one-shot black pebbling\" first formalized in <a href=\"http://graal.ens-lyon.fr/~lmarchal/scheduling/sethi_complete_register_allocation.pdf\" rel=\"nofollow\">http://graal.ens-lyon.fr/~lmarchal/scheduling/sethi_complete_register_allocation.pdf</a></p>\n<p>It is provably <a href=\"https://arxiv.org/abs/1109.4910\" rel=\"nofollow\">hard</a> to compute, but things can look better for concrete models.</p>\n<p>Another set of memory-constrained applications can be enabled by discarding some intermediate values and recomputing them later. That's corresponds to dropping \"one-shot\" in the \"one-shot pebbling game\", and in deep learning this comes up in approaches like training neural networks where gradients don't fit in <a href=\"https://arxiv.org/abs/1604.06174\" rel=\"nofollow\">memory</a></p>\n<p>Regarding your proposal, it sounds like you are proposing to add memory swapping to more ops in TensorFlow?</p>\n<p>This is what the implementation does with it right now:<br>\n<div class=\"border rounded-1 my-2\">\n  <div class=\"f6 px-3 py-2 lh-condensed border-bottom bg-gray-light\">\n    <p class=\"mb-0 text-bold\">\n      <a href=\"https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L215\">tensorflow/tensorflow/core/kernels/stack_ops.cc</a>\n    </p>\n    <p class=\"mb-0 text-gray-light\">\n         Line 215\n      in\n      <a data-pjax=\"true\" class=\"commit-tease-sha\" href=\"/tensorflow/tensorflow/commit/64edd34ce69b4a8033af5d217cb8894105297d8a\">64edd34</a>\n    </p>\n    </div>\n    <div itemprop=\"text\" class=\"blob-wrapper blob-wrapper-embedded data\">\n    <table class=\"highlight tab-size mb-0 js-file-line-container\" data-tab-size=\"8\">\n\n        <tbody><tr class=\"border-0\">\n          <td id=\"L215\" class=\"blob-num border-0 px-3 py-0 bg-white js-line-number\" data-line-number=\"215\"></td>\n          <td id=\"LC215\" class=\"blob-code border-0 px-3 py-0 bg-white blob-code-inner js-file-line\"> <span class=\"pl-k\">if</span> (swap_memory_ &amp;&amp; !alloc_attrs.<span class=\"pl-c1\">on_host</span>() &amp;&amp; </td>\n        </tr>\n    </tbody></table>\n  </div>\n</div>\n</p>\n<p>There's a trade off between transfer and recomputation. For instance, I found that on Titan X Pascal it's 7-10x faster to recompute a large cheap op like <code>mul</code> or <code>concat</code> on GPU rather than fetch previous result from main memory.</p>\n<p>To summarize, you can reduce memory by picking a better execution order, by recomputing parts of the graph on demand, or by swapping things out to CPU memory. Can you be more specific with the model you are trying to run to make sure that swapping is indeed the correct thing here?</p>", "body_text": "Memory efficient execution is an open research area, and it's good to have a concrete application in mind to have a practical solution. For instance I found that \"out of memory\" was often caused by bad execution order, rather than executing too many of them concurrently.\nIE consider computational graph below\n\nTF schedules things as soon as they are ready to execute, so if \"square\" op takes long to compute, it'll compute all the \"circles\", and then keep them in memory, and then compute the squares. This needs O(n) memory where n is the length of chain, whereas memory-optimal execution need O(1) memory and will go \"circle->square->circle->square\".\nThe general problem is known in computer science as \"one-shot black pebbling\" first formalized in http://graal.ens-lyon.fr/~lmarchal/scheduling/sethi_complete_register_allocation.pdf\nIt is provably hard to compute, but things can look better for concrete models.\nAnother set of memory-constrained applications can be enabled by discarding some intermediate values and recomputing them later. That's corresponds to dropping \"one-shot\" in the \"one-shot pebbling game\", and in deep learning this comes up in approaches like training neural networks where gradients don't fit in memory\nRegarding your proposal, it sounds like you are proposing to add memory swapping to more ops in TensorFlow?\nThis is what the implementation does with it right now:\n\n  \n    \n      tensorflow/tensorflow/core/kernels/stack_ops.cc\n    \n    \n         Line 215\n      in\n      64edd34\n    \n    \n    \n    \n\n        \n          \n           if (swap_memory_ && !alloc_attrs.on_host() && \n        \n    \n  \n\n\nThere's a trade off between transfer and recomputation. For instance, I found that on Titan X Pascal it's 7-10x faster to recompute a large cheap op like mul or concat on GPU rather than fetch previous result from main memory.\nTo summarize, you can reduce memory by picking a better execution order, by recomputing parts of the graph on demand, or by swapping things out to CPU memory. Can you be more specific with the model you are trying to run to make sure that swapping is indeed the correct thing here?", "body": "Memory efficient execution is an open research area, and it's good to have a concrete application in mind to have a practical solution. For instance I found that \"out of memory\" was often caused by bad execution order, rather than executing too many of them concurrently.\r\n\r\nIE consider computational graph below\r\n![caterpillar](https://cloud.githubusercontent.com/assets/23068/22432653/7dc7bf20-e6ca-11e6-879c-14880a4d26d3.png)\r\n\r\nTF schedules things as soon as they are ready to execute, so if \"square\" op takes long to compute, it'll compute all the \"circles\", and then keep them in memory, and then compute the squares. This needs O(n) memory where `n` is the length of chain, whereas memory-optimal execution need O(1) memory and will go \"circle->square->circle->square\".\r\n\r\nThe general problem is known in computer science as \"one-shot black pebbling\" first formalized in http://graal.ens-lyon.fr/~lmarchal/scheduling/sethi_complete_register_allocation.pdf\r\n\r\nIt is provably [hard](https://arxiv.org/abs/1109.4910) to compute, but things can look better for concrete models.\r\n\r\nAnother set of memory-constrained applications can be enabled by discarding some intermediate values and recomputing them later. That's corresponds to dropping \"one-shot\" in the \"one-shot pebbling game\", and in deep learning this comes up in approaches like training neural networks where gradients don't fit in [memory](https://arxiv.org/abs/1604.06174)\r\n\r\nRegarding your proposal, it sounds like you are proposing to add memory swapping to more ops in TensorFlow?\r\n\r\nThis is what the implementation does with it right now:\r\nhttps://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L215\r\n\r\nThere's a trade off between transfer and recomputation. For instance, I found that on Titan X Pascal it's 7-10x faster to recompute a large cheap op like `mul` or `concat` on GPU rather than fetch previous result from main memory.\r\n\r\nTo summarize, you can reduce memory by picking a better execution order, by recomputing parts of the graph on demand, or by swapping things out to CPU memory. Can you be more specific with the model you are trying to run to make sure that swapping is indeed the correct thing here?"}