{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276150140", "html_url": "https://github.com/tensorflow/tensorflow/issues/4552#issuecomment-276150140", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552", "id": 276150140, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjE1MDE0MA==", "user": {"login": "vrv", "id": 463737, "node_id": "MDQ6VXNlcjQ2MzczNw==", "avatar_url": "https://avatars0.githubusercontent.com/u/463737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/vrv", "html_url": "https://github.com/vrv", "followers_url": "https://api.github.com/users/vrv/followers", "following_url": "https://api.github.com/users/vrv/following{/other_user}", "gists_url": "https://api.github.com/users/vrv/gists{/gist_id}", "starred_url": "https://api.github.com/users/vrv/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/vrv/subscriptions", "organizations_url": "https://api.github.com/users/vrv/orgs", "repos_url": "https://api.github.com/users/vrv/repos", "events_url": "https://api.github.com/users/vrv/events{/privacy}", "received_events_url": "https://api.github.com/users/vrv/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-30T18:38:03Z", "updated_at": "2017-01-30T18:38:03Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5205204\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alquraishi\">@alquraishi</a> it wasn't easy to determine whether executing an op would OOM in the current framework, since you're already in the execution of the OpKernel when you find out you run out of memory, and the framework doesn't really handle re-executing an OpKernel well (it may be possible, though).</p>\n<p>I think there are two possible approaches to solve this (none of which I'm too involved in at the moment, but something I've thought about):</p>\n<ul>\n<li>\n<p>Once we get C++ shape inference in the runtime (right now it just backs the python shape inference framework), you would be able to know at runtime the approximate output memory requirements of every operation, and so you could indeed know ahead of time whether an operation could run.  This wouldn't include temporary memories required by libraries like cudnn), but it would probably be the low hanging fruit optimization.</p>\n</li>\n<li>\n<p>XLA's compiled approach needs to know the sizes of tensors at compile time, providing the information to the memory and instruction scheduler for XLA to possibly do the right thing.  I'm more optimistic about this approach working well, since the standard executor is very dynamic, and XLA might have a better time of accomplishing this goal. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5453737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tatatodd\">@tatatodd</a>  might have more to say on this.</p>\n</li>\n<li>\n<p>It would be also nice to have the recompuatation/memory use tradeoff in the above solutions.</p>\n</li>\n</ul>", "body_text": "@alquraishi it wasn't easy to determine whether executing an op would OOM in the current framework, since you're already in the execution of the OpKernel when you find out you run out of memory, and the framework doesn't really handle re-executing an OpKernel well (it may be possible, though).\nI think there are two possible approaches to solve this (none of which I'm too involved in at the moment, but something I've thought about):\n\n\nOnce we get C++ shape inference in the runtime (right now it just backs the python shape inference framework), you would be able to know at runtime the approximate output memory requirements of every operation, and so you could indeed know ahead of time whether an operation could run.  This wouldn't include temporary memories required by libraries like cudnn), but it would probably be the low hanging fruit optimization.\n\n\nXLA's compiled approach needs to know the sizes of tensors at compile time, providing the information to the memory and instruction scheduler for XLA to possibly do the right thing.  I'm more optimistic about this approach working well, since the standard executor is very dynamic, and XLA might have a better time of accomplishing this goal. @tatatodd  might have more to say on this.\n\n\nIt would be also nice to have the recompuatation/memory use tradeoff in the above solutions.", "body": "@alquraishi it wasn't easy to determine whether executing an op would OOM in the current framework, since you're already in the execution of the OpKernel when you find out you run out of memory, and the framework doesn't really handle re-executing an OpKernel well (it may be possible, though).\r\n\r\nI think there are two possible approaches to solve this (none of which I'm too involved in at the moment, but something I've thought about):\r\n\r\n* Once we get C++ shape inference in the runtime (right now it just backs the python shape inference framework), you would be able to know at runtime the approximate output memory requirements of every operation, and so you could indeed know ahead of time whether an operation could run.  This wouldn't include temporary memories required by libraries like cudnn), but it would probably be the low hanging fruit optimization.\r\n\r\n* XLA's compiled approach needs to know the sizes of tensors at compile time, providing the information to the memory and instruction scheduler for XLA to possibly do the right thing.  I'm more optimistic about this approach working well, since the standard executor is very dynamic, and XLA might have a better time of accomplishing this goal. @tatatodd  might have more to say on this.\r\n\r\n* It would be also nice to have the recompuatation/memory use tradeoff in the above solutions."}