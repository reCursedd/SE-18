{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/276153946", "html_url": "https://github.com/tensorflow/tensorflow/issues/4552#issuecomment-276153946", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552", "id": 276153946, "node_id": "MDEyOklzc3VlQ29tbWVudDI3NjE1Mzk0Ng==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2017-01-30T18:51:57Z", "updated_at": "2017-01-30T18:52:51Z", "author_association": "CONTRIBUTOR", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5205204\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/alquraishi\">@alquraishi</a> estimating memory needed is hard, but simple heuristic might be to use an upper bound instead of true memory requirement. IE, suppose the largest op output is 1GB. Then you assume that once memory hits 11GB on TitanX, any new op executed will cause OOM and output needs to be placed on CPU.</p>\n<p>Hence, once you hit 11GB, all GPU kernels place their output in main RAM until GPU RAM is reclaimed. This will avoid OOM crash at the expense of introducing a bunch of GPU-&gt;CPU and CPU-&gt;GPU memory copies needed to run GPU kernels.</p>\n<p>This is a \"run-time\" approach mirroring what dynamic_rnn does. A competing approach is static graph analysis which would determine which ops are likely to hit this scenario, and pin those ops to CPU device. This means CPU kernel implementation will be used, and extra GPU&lt;-&gt;CPU memory copies are avoided.</p>\n<p>The later could be done purely on client side, by doing a profiling run, examining <code>RunMetadata</code>, and then creating new graph with <code>tf.device</code> annotations to place things on CPU or GPU</p>", "body_text": "@alquraishi estimating memory needed is hard, but simple heuristic might be to use an upper bound instead of true memory requirement. IE, suppose the largest op output is 1GB. Then you assume that once memory hits 11GB on TitanX, any new op executed will cause OOM and output needs to be placed on CPU.\nHence, once you hit 11GB, all GPU kernels place their output in main RAM until GPU RAM is reclaimed. This will avoid OOM crash at the expense of introducing a bunch of GPU->CPU and CPU->GPU memory copies needed to run GPU kernels.\nThis is a \"run-time\" approach mirroring what dynamic_rnn does. A competing approach is static graph analysis which would determine which ops are likely to hit this scenario, and pin those ops to CPU device. This means CPU kernel implementation will be used, and extra GPU<->CPU memory copies are avoided.\nThe later could be done purely on client side, by doing a profiling run, examining RunMetadata, and then creating new graph with tf.device annotations to place things on CPU or GPU", "body": "@alquraishi estimating memory needed is hard, but simple heuristic might be to use an upper bound instead of true memory requirement. IE, suppose the largest op output is 1GB. Then you assume that once memory hits 11GB on TitanX, any new op executed will cause OOM and output needs to be placed on CPU.\r\n\r\nHence, once you hit 11GB, all GPU kernels place their output in main RAM until GPU RAM is reclaimed. This will avoid OOM crash at the expense of introducing a bunch of GPU->CPU and CPU->GPU memory copies needed to run GPU kernels.\r\n\r\nThis is a \"run-time\" approach mirroring what dynamic_rnn does. A competing approach is static graph analysis which would determine which ops are likely to hit this scenario, and pin those ops to CPU device. This means CPU kernel implementation will be used, and extra GPU<->CPU memory copies are avoided.\r\n\r\nThe later could be done purely on client side, by doing a profiling run, examining `RunMetadata`, and then creating new graph with `tf.device` annotations to place things on CPU or GPU"}