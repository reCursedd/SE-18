{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4552/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4552", "id": 178861823, "node_id": "MDU6SXNzdWUxNzg4NjE4MjM=", "number": 4552, "title": "Feature request: better handling of concurrent execution to prevent unnecessary out of memory failures ", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 10, "created_at": "2016-09-23T12:16:39Z", "updated_at": "2017-06-16T16:45:57Z", "closed_at": "2017-06-16T16:45:57Z", "author_association": "NONE", "body_html": "<p>I have a model in which one operation happens to be very memory intensive and cannot be executed at once. I thought that by splitting the operation manually and executing parts of it individually (all on the GPU), I would get around the OOM problem. Unfortunately, it appears that TF aggressively tries to schedule as many ops to run in parallel as possible, even if that results in an OOM failure. I describe my specific issue further in this <a href=\"http://stackoverflow.com/questions/39643250/best-way-to-split-computation-too-large-to-fit-into-memory\" rel=\"nofollow\">Stack Overflow post</a>.</p>\n<p>It seems to me that this problem should be reasonably easy to solve, and is rather a serious current limitation of TF. The basic principle is that a computation should not fail if it's possible, in some sequential order, to execute it in memory. I'm not talking about situations in which TF has to be clever about splitting a seemingly atomic operations in parts. That I understand can be quite difficult and requires \"art\" on the part of the programmer. What I'm describing is a situation in which the computation graph, <em>as described</em>, can in fact be executed without running out of memory, but because TF is (overly) aggressive in scheduling independent ops concurrently, it runs out of memory.</p>\n<p>The solution seems rather straight-forward. Before putting yet another op onto the GPU (or whatever hardware), TF should check if that would cause the memory to run out. If so, it should check if there's a cyclic dependency on the current op getting executed, i.e. it would be impossible for it to move forward unless the current op is executed. If so then that's a legitimate problem and it would fail with an OOM error. On the other hand, if it's possible for it to wait until other ops are executed before trying again, then it should simply wait until more memory is available. I.e. it should have a way to run independent operations sequentially.</p>\n<p>I'm not familiar enough with the internals of TF to know how difficult a change like this would be, but as it currently stands, it seems like it's preventing a large swath of models from getting executed that otherwise could get executed.</p>\n<p>On a related note, the dynamic_rnn op already seems to do this, where it's able to shuffle memory back and forth until the computation is done, and so perhaps there's an existing partial solution that I'm not aware of?</p>", "body_text": "I have a model in which one operation happens to be very memory intensive and cannot be executed at once. I thought that by splitting the operation manually and executing parts of it individually (all on the GPU), I would get around the OOM problem. Unfortunately, it appears that TF aggressively tries to schedule as many ops to run in parallel as possible, even if that results in an OOM failure. I describe my specific issue further in this Stack Overflow post.\nIt seems to me that this problem should be reasonably easy to solve, and is rather a serious current limitation of TF. The basic principle is that a computation should not fail if it's possible, in some sequential order, to execute it in memory. I'm not talking about situations in which TF has to be clever about splitting a seemingly atomic operations in parts. That I understand can be quite difficult and requires \"art\" on the part of the programmer. What I'm describing is a situation in which the computation graph, as described, can in fact be executed without running out of memory, but because TF is (overly) aggressive in scheduling independent ops concurrently, it runs out of memory.\nThe solution seems rather straight-forward. Before putting yet another op onto the GPU (or whatever hardware), TF should check if that would cause the memory to run out. If so, it should check if there's a cyclic dependency on the current op getting executed, i.e. it would be impossible for it to move forward unless the current op is executed. If so then that's a legitimate problem and it would fail with an OOM error. On the other hand, if it's possible for it to wait until other ops are executed before trying again, then it should simply wait until more memory is available. I.e. it should have a way to run independent operations sequentially.\nI'm not familiar enough with the internals of TF to know how difficult a change like this would be, but as it currently stands, it seems like it's preventing a large swath of models from getting executed that otherwise could get executed.\nOn a related note, the dynamic_rnn op already seems to do this, where it's able to shuffle memory back and forth until the computation is done, and so perhaps there's an existing partial solution that I'm not aware of?", "body": "I have a model in which one operation happens to be very memory intensive and cannot be executed at once. I thought that by splitting the operation manually and executing parts of it individually (all on the GPU), I would get around the OOM problem. Unfortunately, it appears that TF aggressively tries to schedule as many ops to run in parallel as possible, even if that results in an OOM failure. I describe my specific issue further in this [Stack Overflow post](http://stackoverflow.com/questions/39643250/best-way-to-split-computation-too-large-to-fit-into-memory).\n\nIt seems to me that this problem should be reasonably easy to solve, and is rather a serious current limitation of TF. The basic principle is that a computation should not fail if it's possible, in some sequential order, to execute it in memory. I'm not talking about situations in which TF has to be clever about splitting a seemingly atomic operations in parts. That I understand can be quite difficult and requires \"art\" on the part of the programmer. What I'm describing is a situation in which the computation graph, _as described_, can in fact be executed without running out of memory, but because TF is (overly) aggressive in scheduling independent ops concurrently, it runs out of memory.\n\nThe solution seems rather straight-forward. Before putting yet another op onto the GPU (or whatever hardware), TF should check if that would cause the memory to run out. If so, it should check if there's a cyclic dependency on the current op getting executed, i.e. it would be impossible for it to move forward unless the current op is executed. If so then that's a legitimate problem and it would fail with an OOM error. On the other hand, if it's possible for it to wait until other ops are executed before trying again, then it should simply wait until more memory is available. I.e. it should have a way to run independent operations sequentially.\n\nI'm not familiar enough with the internals of TF to know how difficult a change like this would be, but as it currently stands, it seems like it's preventing a large swath of models from getting executed that otherwise could get executed.\n\nOn a related note, the dynamic_rnn op already seems to do this, where it's able to shuffle memory back and forth until the computation is done, and so perhaps there's an existing partial solution that I'm not aware of?\n"}