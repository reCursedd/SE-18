{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13179", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13179/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13179/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13179/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/13179", "id": 259150398, "node_id": "MDU6SXNzdWUyNTkxNTAzOTg=", "number": 13179, "title": "Function categorical_column_with_identity with big number as num_buckets parameter causes training hang and crash", "user": {"login": "batizty", "id": 6157242, "node_id": "MDQ6VXNlcjYxNTcyNDI=", "avatar_url": "https://avatars3.githubusercontent.com/u/6157242?v=4", "gravatar_id": "", "url": "https://api.github.com/users/batizty", "html_url": "https://github.com/batizty", "followers_url": "https://api.github.com/users/batizty/followers", "following_url": "https://api.github.com/users/batizty/following{/other_user}", "gists_url": "https://api.github.com/users/batizty/gists{/gist_id}", "starred_url": "https://api.github.com/users/batizty/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/batizty/subscriptions", "organizations_url": "https://api.github.com/users/batizty/orgs", "repos_url": "https://api.github.com/users/batizty/repos", "events_url": "https://api.github.com/users/batizty/events{/privacy}", "received_events_url": "https://api.github.com/users/batizty/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ispirmustafa", "id": 19293677, "node_id": "MDQ6VXNlcjE5MjkzNjc3", "avatar_url": "https://avatars1.githubusercontent.com/u/19293677?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ispirmustafa", "html_url": "https://github.com/ispirmustafa", "followers_url": "https://api.github.com/users/ispirmustafa/followers", "following_url": "https://api.github.com/users/ispirmustafa/following{/other_user}", "gists_url": "https://api.github.com/users/ispirmustafa/gists{/gist_id}", "starred_url": "https://api.github.com/users/ispirmustafa/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ispirmustafa/subscriptions", "organizations_url": "https://api.github.com/users/ispirmustafa/orgs", "repos_url": "https://api.github.com/users/ispirmustafa/repos", "events_url": "https://api.github.com/users/ispirmustafa/events{/privacy}", "received_events_url": "https://api.github.com/users/ispirmustafa/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 6, "created_at": "2017-09-20T12:47:42Z", "updated_at": "2018-01-10T00:08:47Z", "closed_at": "2018-01-10T00:08:47Z", "author_association": "NONE", "body_html": "<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS 10.12.6</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: ('v1.3.0-rc2-20-g0787eee', '1.3.0')</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: not use GPU</li>\n<li><strong>GPU model and memory</strong>: no GPU</li>\n<li><strong>Exact command to reproduce</strong>:</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Describe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.</p>\n<p>When I try to use use_id or item_id as an feature and process it through function <code>categorical_column_with_identity</code>, because of the id category will be very big out of upper limit of int32, the training job will hang a few minutes then crash.  The crash without any useful information to reminder that this problem was caused by the <code>num_buckets=${a_big_number}</code>.</p>\n<p>Maybe this kinds of feature like uid or item_id should not be used like this, and I believe if there are some diagnostics information will be better.</p>\n<p>And any suggestions about how to process this kinds of feature like uid or item_id, these features are very big number. I think this problem appeared in many cases.</p>\n<p>Thanks</p>\n<h3>Source code / logs</h3>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> coding: utf-8</span>\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow <span class=\"pl-k\">import</span> feature_column <span class=\"pl-k\">as</span> fc\n\nf <span class=\"pl-k\">=</span> fc.embedding_column(\n    fc.categorical_column_with_identity(<span class=\"pl-v\">key</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>vid<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">num_buckets</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1500000000</span>),\n    <span class=\"pl-v\">dimension</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\ne <span class=\"pl-k\">=</span> tf.estimator.DNNClassifier(\n    <span class=\"pl-v\">feature_columns</span><span class=\"pl-k\">=</span>[f],\n    <span class=\"pl-v\">hidden_units</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">10</span>])\n\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">input_fn</span>():\n    <span class=\"pl-k\">return</span> (\n        {<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vid<span class=\"pl-pds\">\"</span></span>: tf.identity(tf.constant(\n            [<span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">100</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">10000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">1000</span>, <span class=\"pl-c1\">100</span>],\n            <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vid<span class=\"pl-pds\">\"</span></span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>vid_trainning<span class=\"pl-pds\">\"</span></span>)},\n        tf.constant([<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">1</span>])\n    )\n\ne.train(input_fn, <span class=\"pl-v\">steps</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)</pre></div>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.12.6\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): ('v1.3.0-rc2-20-g0787eee', '1.3.0')\nPython version: 2.7\nBazel version (if compiling from source):\nCUDA/cuDNN version: not use GPU\nGPU model and memory: no GPU\nExact command to reproduce:\n\nDescribe the problem\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\nWhen I try to use use_id or item_id as an feature and process it through function categorical_column_with_identity, because of the id category will be very big out of upper limit of int32, the training job will hang a few minutes then crash.  The crash without any useful information to reminder that this problem was caused by the num_buckets=${a_big_number}.\nMaybe this kinds of feature like uid or item_id should not be used like this, and I believe if there are some diagnostics information will be better.\nAnd any suggestions about how to process this kinds of feature like uid or item_id, these features are very big number. I think this problem appeared in many cases.\nThanks\nSource code / logs\n# coding: utf-8\nimport tensorflow as tf\nfrom tensorflow import feature_column as fc\n\nf = fc.embedding_column(\n    fc.categorical_column_with_identity(key='vid', num_buckets=1500000000),\n    dimension=10)\ne = tf.estimator.DNNClassifier(\n    feature_columns=[f],\n    hidden_units=[10])\n\n\ndef input_fn():\n    return (\n        {\"vid\": tf.identity(tf.constant(\n            [1000, 1000, 100, 1000, 10000, 1000, 1000, 1000, 100],\n            name=\"vid\"), name=\"vid_trainning\")},\n        tf.constant([1, 0, 1, 0, 0, 0, 0, 0, 1])\n    )\n\ne.train(input_fn, steps=10)", "body": "------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.12.6\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: ('v1.3.0-rc2-20-g0787eee', '1.3.0')\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: not use GPU\r\n- **GPU model and memory**: no GPU\r\n- **Exact command to reproduce**: \r\n\r\n### Describe the problem\r\nDescribe the problem clearly here. Be sure to convey here why it's a bug in TensorFlow or a feature request.\r\n\r\nWhen I try to use use_id or item_id as an feature and process it through function ```categorical_column_with_identity```, because of the id category will be very big out of upper limit of int32, the training job will hang a few minutes then crash.  The crash without any useful information to reminder that this problem was caused by the ```num_buckets=${a_big_number}```.\r\n\r\nMaybe this kinds of feature like uid or item_id should not be used like this, and I believe if there are some diagnostics information will be better.\r\n\r\nAnd any suggestions about how to process this kinds of feature like uid or item_id, these features are very big number. I think this problem appeared in many cases.\r\n\r\nThanks\r\n\r\n### Source code / logs\r\n```python\r\n# coding: utf-8\r\nimport tensorflow as tf\r\nfrom tensorflow import feature_column as fc\r\n\r\nf = fc.embedding_column(\r\n    fc.categorical_column_with_identity(key='vid', num_buckets=1500000000),\r\n    dimension=10)\r\ne = tf.estimator.DNNClassifier(\r\n    feature_columns=[f],\r\n    hidden_units=[10])\r\n\r\n\r\ndef input_fn():\r\n    return (\r\n        {\"vid\": tf.identity(tf.constant(\r\n            [1000, 1000, 100, 1000, 10000, 1000, 1000, 1000, 100],\r\n            name=\"vid\"), name=\"vid_trainning\")},\r\n        tf.constant([1, 0, 1, 0, 0, 0, 0, 0, 1])\r\n    )\r\n\r\ne.train(input_fn, steps=10)\r\n```\r\n"}