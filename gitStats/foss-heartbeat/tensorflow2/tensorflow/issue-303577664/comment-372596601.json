{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/372596601", "html_url": "https://github.com/tensorflow/tensorflow/issues/17566#issuecomment-372596601", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17566", "id": 372596601, "node_id": "MDEyOklzc3VlQ29tbWVudDM3MjU5NjYwMQ==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-03-13T09:13:35Z", "updated_at": "2018-03-13T09:13:50Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a>, yeah, e.g. CUDA 9.0 and CUDA 9.1 are quite different in the respects we care about.</p>\n<p>For cudnn, I have not seen a statement specifying their level of backwards compatibility.  I would <em>naively expect</em> that if you build against cudnn x.y and run with cudnn x.z for z &gt;= y, it <em>probably will</em> work.  But to be comfortable with blessing that I'd want a statement in writing from nvidia.  (Perhaps such a statement already exists.)</p>\n<p>Whether or not it should be a fatal error in TF vs a \"good luck, you're on your own\" warning (like we do for known-broken ptxas versions), I don't have an opinion on.</p>", "body_text": "@martinwicke, yeah, e.g. CUDA 9.0 and CUDA 9.1 are quite different in the respects we care about.\nFor cudnn, I have not seen a statement specifying their level of backwards compatibility.  I would naively expect that if you build against cudnn x.y and run with cudnn x.z for z >= y, it probably will work.  But to be comfortable with blessing that I'd want a statement in writing from nvidia.  (Perhaps such a statement already exists.)\nWhether or not it should be a fatal error in TF vs a \"good luck, you're on your own\" warning (like we do for known-broken ptxas versions), I don't have an opinion on.", "body": "@martinwicke, yeah, e.g. CUDA 9.0 and CUDA 9.1 are quite different in the respects we care about.\r\n\r\nFor cudnn, I have not seen a statement specifying their level of backwards compatibility.  I would *naively expect* that if you build against cudnn x.y and run with cudnn x.z for z >= y, it *probably will* work.  But to be comfortable with blessing that I'd want a statement in writing from nvidia.  (Perhaps such a statement already exists.)\r\n\r\nWhether or not it should be a fatal error in TF vs a \"good luck, you're on your own\" warning (like we do for known-broken ptxas versions), I don't have an opinion on."}