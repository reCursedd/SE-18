{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/182636306", "pull_request_review_id": 113470181, "id": 182636306, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE4MjYzNjMwNg==", "diff_hunk": "@@ -80,7 +80,8 @@ def _apply_sparse_shared(self, grad, var, indices, scatter_add):\n     with ops.control_dependencies([m_t]):\n       m_t = scatter_add(m, indices, m_scaled_g_values)\n       # m_bar = (1 - beta1) * g_t + beta1 * m_t\n-      m_bar = m_scaled_g_values + beta1_t * m_t\n+      state_ops.assign(m_t, m_t * beta1_t, use_locking=self._use_locking)", "path": "tensorflow/contrib/opt/python/training/nadam_optimizer.py", "position": 5, "original_position": 5, "commit_id": "a8fde5075113e7ac5f404a39eb2eb14c83e86f84", "original_commit_id": "a8fde5075113e7ac5f404a39eb2eb14c83e86f84", "user": {"login": "strategist333", "id": 246351, "node_id": "MDQ6VXNlcjI0NjM1MQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/246351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/strategist333", "html_url": "https://github.com/strategist333", "followers_url": "https://api.github.com/users/strategist333/followers", "following_url": "https://api.github.com/users/strategist333/following{/other_user}", "gists_url": "https://api.github.com/users/strategist333/gists{/gist_id}", "starred_url": "https://api.github.com/users/strategist333/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/strategist333/subscriptions", "organizations_url": "https://api.github.com/users/strategist333/orgs", "repos_url": "https://api.github.com/users/strategist333/repos", "events_url": "https://api.github.com/users/strategist333/events{/privacy}", "received_events_url": "https://api.github.com/users/strategist333/received_events", "type": "User", "site_admin": false}, "body": "This change does not make any sense to me. Assuming you wanted to maintain the original logic (m_bar = m_scaled_g_values + beta1_t * m_t), these two lines do not do that at all.\r\n\r\nThe first line creates an assign op that is never used, as the reference is not captured. So it will never be run.\r\nThe second line just does the same as line 81, adding m_scaled_g_values to m once more.\r\n\r\nHave you tested this logic?", "created_at": "2018-04-19T05:26:49Z", "updated_at": "2018-04-19T05:27:05Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/15665#discussion_r182636306", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15665", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/182636306"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/15665#discussion_r182636306"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15665"}}, "body_html": "<p>This change does not make any sense to me. Assuming you wanted to maintain the original logic (m_bar = m_scaled_g_values + beta1_t * m_t), these two lines do not do that at all.</p>\n<p>The first line creates an assign op that is never used, as the reference is not captured. So it will never be run.<br>\nThe second line just does the same as line 81, adding m_scaled_g_values to m once more.</p>\n<p>Have you tested this logic?</p>", "body_text": "This change does not make any sense to me. Assuming you wanted to maintain the original logic (m_bar = m_scaled_g_values + beta1_t * m_t), these two lines do not do that at all.\nThe first line creates an assign op that is never used, as the reference is not captured. So it will never be run.\nThe second line just does the same as line 81, adding m_scaled_g_values to m once more.\nHave you tested this logic?"}