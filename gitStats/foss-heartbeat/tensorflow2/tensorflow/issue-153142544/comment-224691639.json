{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/224691639", "html_url": "https://github.com/tensorflow/tensorflow/issues/2226#issuecomment-224691639", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2226", "id": 224691639, "node_id": "MDEyOklzc3VlQ29tbWVudDIyNDY5MTYzOQ==", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-08T18:53:01Z", "updated_at": "2016-06-08T18:53:22Z", "author_association": "CONTRIBUTOR", "body_html": "<p>At some point I was debugging tests failing when running with -copt=avx2 instead of avx. We couldn't get rid of the numeric difference and ended up just raising tolerances on tests. What happens is that there's some small operation that produces small difference in the range of ulp or two and then it blows up. In particular, I noticed forward pass during MNIST training to be relatively stable, but during backward pass, tiny difference blow up to billions of ulps</p>", "body_text": "At some point I was debugging tests failing when running with -copt=avx2 instead of avx. We couldn't get rid of the numeric difference and ended up just raising tolerances on tests. What happens is that there's some small operation that produces small difference in the range of ulp or two and then it blows up. In particular, I noticed forward pass during MNIST training to be relatively stable, but during backward pass, tiny difference blow up to billions of ulps", "body": "At some point I was debugging tests failing when running with -copt=avx2 instead of avx. We couldn't get rid of the numeric difference and ended up just raising tolerances on tests. What happens is that there's some small operation that produces small difference in the range of ulp or two and then it blows up. In particular, I noticed forward pass during MNIST training to be relatively stable, but during backward pass, tiny difference blow up to billions of ulps\n"}