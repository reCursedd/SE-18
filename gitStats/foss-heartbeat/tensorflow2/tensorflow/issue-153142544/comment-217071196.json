{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217071196", "html_url": "https://github.com/tensorflow/tensorflow/issues/2226#issuecomment-217071196", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2226", "id": 217071196, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzA3MTE5Ng==", "user": {"login": "alexlee-gk", "id": 839426, "node_id": "MDQ6VXNlcjgzOTQyNg==", "avatar_url": "https://avatars1.githubusercontent.com/u/839426?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexlee-gk", "html_url": "https://github.com/alexlee-gk", "followers_url": "https://api.github.com/users/alexlee-gk/followers", "following_url": "https://api.github.com/users/alexlee-gk/following{/other_user}", "gists_url": "https://api.github.com/users/alexlee-gk/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexlee-gk/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexlee-gk/subscriptions", "organizations_url": "https://api.github.com/users/alexlee-gk/orgs", "repos_url": "https://api.github.com/users/alexlee-gk/repos", "events_url": "https://api.github.com/users/alexlee-gk/events{/privacy}", "received_events_url": "https://api.github.com/users/alexlee-gk/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-05T03:43:46Z", "updated_at": "2016-05-05T03:43:46Z", "author_association": "NONE", "body_html": "<p>I have condensed my code into a single script that builds a small convnet and trains it for 1000 iterations. The network achieves a test accuracy of 97.7% and 9.8% when I use the CPU and GPU respectively.</p>\n<p>I have also attached weights for initializing the network; this particular initialization seems to be particularly bad even though the non-bias weights were initialized using a truncated normal distribution.</p>\n<p>Another thing to note is that this behavior happens with a particular bad choice of hyperparameters (learning rate 0.1 and momentum 0.1).</p>\n<p>I'm aware that a lot of things could be better in the script, such as using a lower learning rate, higher momentum, dropout, batch norm, etc. However, I'm interested in this particular setting since this is what we developed for a class that I'm teaching (CS188 at UC Berkeley), and the behavior of our online autograder differs from that of the student's local autograder. Another puzzling thing is that the online autograder is the one getting ~10% accuracy even though it is not using the GPU. This autograder is running Ubuntu 14.04 on aws.</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/files/249904/convnet_determinism_check.zip\">convnet_determinism_check.zip</a></p>", "body_text": "I have condensed my code into a single script that builds a small convnet and trains it for 1000 iterations. The network achieves a test accuracy of 97.7% and 9.8% when I use the CPU and GPU respectively.\nI have also attached weights for initializing the network; this particular initialization seems to be particularly bad even though the non-bias weights were initialized using a truncated normal distribution.\nAnother thing to note is that this behavior happens with a particular bad choice of hyperparameters (learning rate 0.1 and momentum 0.1).\nI'm aware that a lot of things could be better in the script, such as using a lower learning rate, higher momentum, dropout, batch norm, etc. However, I'm interested in this particular setting since this is what we developed for a class that I'm teaching (CS188 at UC Berkeley), and the behavior of our online autograder differs from that of the student's local autograder. Another puzzling thing is that the online autograder is the one getting ~10% accuracy even though it is not using the GPU. This autograder is running Ubuntu 14.04 on aws.\nconvnet_determinism_check.zip", "body": "I have condensed my code into a single script that builds a small convnet and trains it for 1000 iterations. The network achieves a test accuracy of 97.7% and 9.8% when I use the CPU and GPU respectively.\n\nI have also attached weights for initializing the network; this particular initialization seems to be particularly bad even though the non-bias weights were initialized using a truncated normal distribution.\n\nAnother thing to note is that this behavior happens with a particular bad choice of hyperparameters (learning rate 0.1 and momentum 0.1).\n\nI'm aware that a lot of things could be better in the script, such as using a lower learning rate, higher momentum, dropout, batch norm, etc. However, I'm interested in this particular setting since this is what we developed for a class that I'm teaching (CS188 at UC Berkeley), and the behavior of our online autograder differs from that of the student's local autograder. Another puzzling thing is that the online autograder is the one getting ~10% accuracy even though it is not using the GPU. This autograder is running Ubuntu 14.04 on aws.\n\n[convnet_determinism_check.zip](https://github.com/tensorflow/tensorflow/files/249904/convnet_determinism_check.zip)\n"}