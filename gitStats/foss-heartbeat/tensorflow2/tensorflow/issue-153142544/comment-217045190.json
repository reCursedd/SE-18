{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/217045190", "html_url": "https://github.com/tensorflow/tensorflow/issues/2226#issuecomment-217045190", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2226", "id": 217045190, "node_id": "MDEyOklzc3VlQ29tbWVudDIxNzA0NTE5MA==", "user": {"login": "zheng-xq", "id": 15736910, "node_id": "MDQ6VXNlcjE1NzM2OTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/15736910?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zheng-xq", "html_url": "https://github.com/zheng-xq", "followers_url": "https://api.github.com/users/zheng-xq/followers", "following_url": "https://api.github.com/users/zheng-xq/following{/other_user}", "gists_url": "https://api.github.com/users/zheng-xq/gists{/gist_id}", "starred_url": "https://api.github.com/users/zheng-xq/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zheng-xq/subscriptions", "organizations_url": "https://api.github.com/users/zheng-xq/orgs", "repos_url": "https://api.github.com/users/zheng-xq/repos", "events_url": "https://api.github.com/users/zheng-xq/events{/privacy}", "received_events_url": "https://api.github.com/users/zheng-xq/received_events", "type": "User", "site_admin": false}, "created_at": "2016-05-05T00:17:45Z", "updated_at": "2016-05-05T00:17:45Z", "author_association": "CONTRIBUTOR", "body_html": "<p>The GPU kernel for reduce_sum is known to be non-deterministic since it<br>\nuses the atomic operations. When the number of elements is large, the<br>\ndifference could be quite large.</p>\n<p>If your model is trained with dropout in it, it tends to be less likely to<br>\nbe affected by the noise.</p>\n<p>Otherwise, it is a good idea to have a small example where we can observe<br>\nthe 98% -&gt; 10% difference.</p>\n<p>On Wed, May 4, 2016 at 5:03 PM, alexlee-gk <a href=\"mailto:notifications@github.com\">notifications@github.com</a> wrote:</p>\n<blockquote>\n<p>The evaluated value of the l1 or l2 loss of variables differ sometimes<br>\nwhen using the GPU. This seems to happen when the variables are \"large\".</p>\n<p>Even though the difference is not that much in the example below, these<br>\ndifferences have resulted in a 98% test accuracy on a network trained on<br>\nthe CPU, but a 10% test accuracy on the same network trained on the GPU.<br>\nEnvironment info</p>\n<p>Operating System: Ubuntu 14.04</p>\n<p>Installed version of CUDA and cuDNN: 7.5 and 7.0 cuda_info.txt<br>\n<a href=\"https://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt\">https://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt</a><br>\nGPU: Titan X</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.<br>\npip install --upgrade<br>\n<a href=\"https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\" rel=\"nofollow\">https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl</a></li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<br>\n<em>version</em>)\".<br>\npython -c \"import tensorflow; print(tensorflow.<em>version</em>)\"I<br>\ntensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA<br>\nlibrary libcublas.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA<br>\nlibrary libcudnn.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA<br>\nlibrary libcufft.so locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA<br>\nlibrary libcuda.so.1 locally<br>\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA<br>\nlibrary libcurand.so locally<br>\n0.8.0<br>\nSteps to reproduce</li>\n</ol>\n<p>import tensorflow as tf<br>\nW1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))<br>\nW2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))<br>\nsess = tf.InteractiveSession()<br>\nsess.run(tf.initialize_all_variables())<br>\nl2 = tf.reduce_sum(W1 * W1) + tf.reduce_sum(W2 * W2)<br>\nfor i in range(100):<br>\nprint('%.10f' % sess.run(l2))</p>\n<p>\u2014<br>\nYou are receiving this because you are subscribed to this thread.<br>\nReply to this email directly or view it on GitHub<br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"153142544\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/2226\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/2226/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/2226\">#2226</a></p>\n</blockquote>", "body_text": "The GPU kernel for reduce_sum is known to be non-deterministic since it\nuses the atomic operations. When the number of elements is large, the\ndifference could be quite large.\nIf your model is trained with dropout in it, it tends to be less likely to\nbe affected by the noise.\nOtherwise, it is a good idea to have a small example where we can observe\nthe 98% -> 10% difference.\nOn Wed, May 4, 2016 at 5:03 PM, alexlee-gk notifications@github.com wrote:\n\nThe evaluated value of the l1 or l2 loss of variables differ sometimes\nwhen using the GPU. This seems to happen when the variables are \"large\".\nEven though the difference is not that much in the example below, these\ndifferences have resulted in a 98% test accuracy on a network trained on\nthe CPU, but a 10% test accuracy on the same network trained on the GPU.\nEnvironment info\nOperating System: Ubuntu 14.04\nInstalled version of CUDA and cuDNN: 7.5 and 7.0 cuda_info.txt\nhttps://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt\nGPU: Titan X\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\npip install --upgrade\nhttps://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\nThe output from python -c \"import tensorflow; print(tensorflow.\nversion)\".\npython -c \"import tensorflow; print(tensorflow.version)\"I\ntensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcublas.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcudnn.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcufft.so locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcuda.so.1 locally\nI tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\nlibrary libcurand.so locally\n0.8.0\nSteps to reproduce\n\nimport tensorflow as tf\nW1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\nW2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\nsess = tf.InteractiveSession()\nsess.run(tf.initialize_all_variables())\nl2 = tf.reduce_sum(W1 * W1) + tf.reduce_sum(W2 * W2)\nfor i in range(100):\nprint('%.10f' % sess.run(l2))\n\u2014\nYou are receiving this because you are subscribed to this thread.\nReply to this email directly or view it on GitHub\n#2226", "body": "The GPU kernel for reduce_sum is known to be non-deterministic since it\nuses the atomic operations. When the number of elements is large, the\ndifference could be quite large.\n\nIf your model is trained with dropout in it, it tends to be less likely to\nbe affected by the noise.\n\nOtherwise, it is a good idea to have a small example where we can observe\nthe 98% -> 10% difference.\n\nOn Wed, May 4, 2016 at 5:03 PM, alexlee-gk notifications@github.com wrote:\n\n> The evaluated value of the l1 or l2 loss of variables differ sometimes\n> when using the GPU. This seems to happen when the variables are \"large\".\n> \n> Even though the difference is not that much in the example below, these\n> differences have resulted in a 98% test accuracy on a network trained on\n> the CPU, but a 10% test accuracy on the same network trained on the GPU.\n> Environment info\n> \n> Operating System: Ubuntu 14.04\n> \n> Installed version of CUDA and cuDNN: 7.5 and 7.0 cuda_info.txt\n> https://github.com/tensorflow/tensorflow/files/249768/cuda_info.txt\n> GPU: Titan X\n> \n> If installed from binary pip package, provide:\n> 1. Which pip package you installed.\n> pip install --upgrade\n> https://storage.googleapis.com/tensorflow/linux/gpu/tensorflow-0.8.0-cp27-none-linux_x86_64.whl\n> 2. The output from python -c \"import tensorflow; print(tensorflow.\n> _version_)\".\n> python -c \"import tensorflow; print(tensorflow._version_)\"I\n> tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcublas.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcudnn.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcufft.so locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcuda.so.1 locally\n> I tensorflow/stream_executor/dso_loader.cc:105] successfully opened CUDA\n> library libcurand.so locally\n> 0.8.0\n> Steps to reproduce\n> \n> import tensorflow as tf\n> W1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n> W2 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1))\n> sess = tf.InteractiveSession()\n> sess.run(tf.initialize_all_variables())\n> l2 = tf.reduce_sum(W1 \\* W1) + tf.reduce_sum(W2 \\* W2)\n> for i in range(100):\n>     print('%.10f' % sess.run(l2))\n> \n> \u2014\n> You are receiving this because you are subscribed to this thread.\n> Reply to this email directly or view it on GitHub\n> https://github.com/tensorflow/tensorflow/issues/2226\n"}