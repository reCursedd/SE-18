{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2015", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2015/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2015/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2015/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2015", "id": 149354780, "node_id": "MDU6SXNzdWUxNDkzNTQ3ODA=", "number": 2015, "title": "recurrent layer on top of convolution layer fails with GPU", "user": {"login": "alphaf52", "id": 4263114, "node_id": "MDQ6VXNlcjQyNjMxMTQ=", "avatar_url": "https://avatars0.githubusercontent.com/u/4263114?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alphaf52", "html_url": "https://github.com/alphaf52", "followers_url": "https://api.github.com/users/alphaf52/followers", "following_url": "https://api.github.com/users/alphaf52/following{/other_user}", "gists_url": "https://api.github.com/users/alphaf52/gists{/gist_id}", "starred_url": "https://api.github.com/users/alphaf52/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alphaf52/subscriptions", "organizations_url": "https://api.github.com/users/alphaf52/orgs", "repos_url": "https://api.github.com/users/alphaf52/repos", "events_url": "https://api.github.com/users/alphaf52/events{/privacy}", "received_events_url": "https://api.github.com/users/alphaf52/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "benoitsteiner", "id": 6969686, "node_id": "MDQ6VXNlcjY5Njk2ODY=", "avatar_url": "https://avatars0.githubusercontent.com/u/6969686?v=4", "gravatar_id": "", "url": "https://api.github.com/users/benoitsteiner", "html_url": "https://github.com/benoitsteiner", "followers_url": "https://api.github.com/users/benoitsteiner/followers", "following_url": "https://api.github.com/users/benoitsteiner/following{/other_user}", "gists_url": "https://api.github.com/users/benoitsteiner/gists{/gist_id}", "starred_url": "https://api.github.com/users/benoitsteiner/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/benoitsteiner/subscriptions", "organizations_url": "https://api.github.com/users/benoitsteiner/orgs", "repos_url": "https://api.github.com/users/benoitsteiner/repos", "events_url": "https://api.github.com/users/benoitsteiner/events{/privacy}", "received_events_url": "https://api.github.com/users/benoitsteiner/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2016-04-19T05:46:43Z", "updated_at": "2017-02-09T22:02:16Z", "closed_at": "2016-08-12T01:49:23Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Operating System: Ubuntu 15.04</p>\n<p>Installed version of CUDA and cuDNN:<br>\nCUDA 7.5<br>\ncuDNN 5.0.4<br>\n(please attach the output of <code>ls -l /path/to/cuda/lib/libcud*</code>):<br>\n-rw-r--r-- 1 root root 189170 Oct 10  2015 /usr/local/cuda/lib/libcudadevrt.a<br>\nlrwxrwxrwx 1 root root     16 Oct 10  2015 /usr/local/cuda/lib/libcudart.so -&gt; libcudart.so.7.5<br>\nlrwxrwxrwx 1 root root     19 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5 -&gt; libcudart.so.7.5.18<br>\n-rwxr-xr-x 1 root root 311596 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5.18<br>\n-rw-r--r-- 1 root root 558020 Oct 10  2015 /usr/local/cuda/lib/libcudart_static.a</p>\n<p>If installed from binary pip package, provide:</p>\n<ol>\n<li>Which pip package you installed.<br>\npip list<br>\ntensorflow (0.7.1)</li>\n<li>The output from python -c \"import tensorflow; print(tensorflow.<strong>version</strong>)\".<br>\n0.7.1</li>\n</ol>\n<h3>Problem description</h3>\n<p>I simply try to put a recurrent layer on top of a convolution layer, which directly manipulate the inputs. It is ok with cpu, but fails with gpu. When i remove either the convolution layer or the recurrent layer, it works well with gpu.</p>\n<p>The error message is</p>\n<blockquote>\n<p>python: external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223: static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, false&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long int&gt;, 16&gt;, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, const Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, const Eigen::DimensionList&lt;long int, 1ul&gt;, const Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, const Eigen::TensorMap&lt;Eigen::Tensor&lt;const float, 1, 1, long int&gt;, 16&gt; &gt; &gt; &gt; &gt;]: Assertion `cudaGetLastError() == cudaSuccess' failed.</p>\n</blockquote>\n<p>I run some black-box test (as i don't know a better way to debug it), and found that it raises error when it try to compute gradients with the statement<br>\n<code>grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm) </code></p>\n<p>The following is a simplified version of the code which has the same problem. (There ought to be a softmax layer on top of the recurrent layer, but i replace it with sum to make it simpler)</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\n\nimport sys\nsys.path.append('/home/jiahua/tensorflow')\nimport math\nimport time\n\nfrom tensorflow.contrib.ctc import ctc_ops\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import rnn\nfrom conv_batch_normalizer import ConvolutionalBatchNormalizer\n\nimport load_data\n\nclass Model(object):\n\n  def __init__(self, is_training, config):\n    self._batch_size = batch_size = config.batch_size\n    self._freq_size = freq_size = config.freq_size\n    self._hidden_size = hidden_size = config.hidden_size\n    self._seq_max_length = seq_max_length = config.seq_max_length\n\n    num_channels = 10\n    filter_dimension = 2\n    conv_stride = 2\n\n    self._inputs = inputs = tf.placeholder(tf.float32, [batch_size, seq_max_length, freq_size]) \n    self._sequence_lengths = sequence_lengths = tf.placeholder(tf.int32, [batch_size])\n    inputs = tf.reshape(inputs, [batch_size, seq_max_length, freq_size, 1])\n\n    padding_size = filter_dimension - (seq_max_length - 1)  % conv_stride - 1 \n    print 'padding_size', padding_size\n    paddings = [[0, 0], [0, padding_size], [0, 0], [0, 0]]\n    inputs = tf.pad(inputs, paddings)\n\n    if is_training and config.keep_prob &lt; 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n    print 'inputs', tf.Tensor.get_shape(inputs)\n\n    parameters = []\n    # conv1\n    with tf.name_scope('conv1') as scope:\n      kernel = tf.Variable(tf.truncated_normal([filter_dimension, freq_size, 1, num_channels], dtype=tf.float32, stddev=1e-1), name='weights')\n      conv = tf.nn.conv2d(inputs, kernel, [1, conv_stride, conv_stride, 1], padding='VALID')\n      biases = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases')\n      bias = tf.nn.bias_add(conv, biases)\n      conv1 = tf.nn.relu(bias, name=scope)\n      parameters += [kernel, biases]\n\n    # batch normalization\n    print 'conv1', tf.Tensor.get_shape(conv1)\n\n    inter1 = tf.reshape(conv1, [batch_size, -1, 1, num_channels])\n    print 'inter1', tf.Tensor.get_shape(inter1)\n\n    new_length = (seq_max_length + padding_size - filter_dimension) / conv_stride + 1\n    print 'new length = ', new_length \n\n    # rnn\n    rnn_inputs = tf.reshape(inter1, [batch_size, new_length, num_channels])\n    print 'rnn_inputs', tf.Tensor.get_shape(rnn_inputs)\n\n    lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n    if is_training and config.keep_prob &lt; 1:\n      lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n    cell = rnn_cell.MultiRNNCell([lstm_cell] * 2)\n\n    self._initial_state = cell.zero_state(batch_size, tf.float32)\n\n    rnn_outputs = []\n    state = self._initial_state\n    with tf.variable_scope(\"rnn\"):\n      for time_step in range(new_length):\n        if time_step &gt; 0: tf.get_variable_scope().reuse_variables()\n        (cell_output, state) = cell(rnn_inputs[:, time_step, :], state)\n        rnn_outputs.append(cell_output)\n\n    print 'rnn_outputs ', tf.Tensor.get_shape(rnn_outputs[0]) , ' * ', len(rnn_outputs)\n    self._final_state = state\n\n    self._cost = cost = tf.reduce_sum(tf.add_n(rnn_outputs)) / batch_size\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n\n    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n\n  def assign_lr(self, session, lr_value):\n    session.run(tf.assign(self._lr, lr_value))\n\n  @property\n  def input_data(self):\n    return self._input_data\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def predict(self):\n    return self._predict\n\n\nclass Config(object):\n  batch_size = 2\n  freq_size = 2\n  hidden_size = 2\n  seq_max_length = 5\n  init_scale = 1.0\n  lr_decay = 0.5\n  max_epoch = 10\n  max_max_epoch = 100\n  keep_prob = 0.9\n  max_grad_norm = 5\n  learning_rate = 1\n\n\ndef run_epoch(session, m, data, eval_op):\n  start_time = time.time()\n  iters = 0\n  costs = 0.0\n  state = m._initial_state.eval()\n  for (inputs, labels_indices, labels_values, labels_shape, seq_lens) in data:\n    cost, state, _ = session.run([m._cost, m._final_state, eval_op],\n                                 {m._inputs: inputs,\n                                  m._initial_state: state})\n    costs += cost\n    iters += 1\n    if iters % 1 == 0:\n      print iters\n      print \"time: %f\" % (time.time() - start_time)\n      print \"cost: %f\" % cost\n  print 'finish'\n  print \"total time: %f\" % (time.time() - start_time)\n  return costs / iters\n  return 0.0\n\n\ndef main():\n  config = Config()\n  with tf.Graph().as_default(), tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n      m = Model(is_training=True, config=config)\n    init_op = tf.initialize_all_variables()\n    session.run(init_op)\n\n    train_data = [([[[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]], [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]], [[0, 0], [0, 1], [1, 0], [1, 1]], [0, 1, 1, 2], [2, 2], [2, 2])]\n    for i in range(config.max_max_epoch):\n      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n      print '#iter %d start lr: %f' % (i, config.learning_rate * lr_decay)\n      m.assign_lr(session, config.learning_rate * lr_decay)\n\n      print 'average loss: %f' % (run_epoch(session, m, train_data, m._train_op))\n\n\nif __name__ == \"__main__\":\n  main()\n</code></pre>\n<h3>What have you tried?</h3>\n<p>Below is the stack status printed using gdb python, in case you want to look at it. It seems that there is a problem when it tries to launch a cuda kernel to compute the l2loss.</p>\n<blockquote>\n<h1>0  0x00007ffff7826267 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:55</h1>\n<h1>1  0x00007ffff7827eca in __GI_abort () at abort.c:89</h1>\n<h1>2  0x00007ffff781f03d in <strong>assert_fail_base (fmt=0x7ffff7981028 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=file@entry=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=line@entry=223, function=function@entry=0x7fffe3c04180 &lt;Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&gt; const, Eigen::GpuDevice, false&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&gt; const&amp;, Eigen::GpuDevice const&amp;)::__PRETTY_FUNCTION</strong>&gt; \"static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, false&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen:\"...) at assert.c:92</h1>\n<h1>3  0x00007ffff781f0f2 in <strong>GI___assert_fail (assertion=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=223, function=0x7fffe3c04180 &lt;Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&gt; const, Eigen::GpuDevice, false&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&gt; const&amp;, Eigen::GpuDevice const&amp;)::__PRETTY_FUNCTION</strong>&gt; \"static void Eigen::internal::TensorExecutor&lt;Expression, Eigen::GpuDevice, false&gt;::run(const Expression&amp;, const Eigen::GpuDevice&amp;) [with Expression = const Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen:\"...) at assert.c:101</h1>\n<h1>4  0x00007fffe2487e26 in Eigen::internal::TensorExecutor&lt;Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&gt; const, Eigen::GpuDevice, false&gt;::run(Eigen::TensorAssignOp&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&gt; const&amp;, Eigen::GpuDevice const&amp;) (expr=..., device=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223</h1>\n<h1>5  0x00007fffe24879ea in Eigen::TensorDevice&lt;Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;, Eigen::GpuDevice&gt;::operator=Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; &gt;(Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op&lt;float, Eigen::TensorReductionOpEigen::internal::SumReducer&lt;float, Eigen::DimensionList&lt;long, 1ul&gt; const, Eigen::TensorCwiseUnaryOp&lt;Eigen::internal::scalar_square_op, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt; const&gt; const&gt; const&gt; const&amp;) (this=0x7fff357f91c0, other=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35</h1>\n<h1>6  0x00007fffe248785a in tensorflow::functor::L2Loss&lt;Eigen::GpuDevice, float&gt;::operator()(Eigen::GpuDevice const&amp;, Eigen::TensorMap&lt;Eigen::Tensor&lt;float const, 1, 1, long&gt;, 16&gt;, Eigen::TensorMap&lt;Eigen::TensorFixedSize&lt;float, Eigen::Sizes&lt;&gt;, 1, long&gt;, 16&gt;) (this=0x7fff357f9250, d=..., input=..., output=...) at ./tensorflow/core/kernels/l2loss_op.h:32</h1>\n<h1>7  0x00007fffe247ee64 in tensorflow::L2LossOp&lt;Eigen::GpuDevice, float&gt;::Compute (this=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/kernels/l2loss_op.cc:45</h1>\n<h1>8  0x00007fffe293d927 in tensorflow::BaseGPUDevice::Compute (this=0x3dc5b60, op_kernel=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/common_runtime/gpu/gpu_device.cc:388</h1>\n<h1>9  0x00007fffe2b61689 in tensorflow::(anonymous namespace)::ExecutorState::Process (this=0x3facbc0, tagged_node=..., scheduled_usec=0) at tensorflow/core/common_runtime/executor.cc:1092</h1>\n<h1>10 0x00007fffe2b6cf89 in std::_Mem_fn&lt;void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)&gt;::operator()&lt;tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&amp;, long long&amp;, void&gt; (this=0x7fff200008c0, __object=0x3facbc0) at /usr/include/c++/4.9/functional:569</h1>\n<h1>11 0x00007fffe2b6c3dc in std::_Bind&lt;std::_Mem_fn&lt;void (tensorflow::(anonymous namespace)::ExecutorState::<em>)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)&gt;(tensorflow::(anonymous namespace)::ExecutorState</em>, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)&gt;::__call&lt;void, 0ul, 1ul, 2ul&gt;(&lt;unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f&gt;, std::_Index_tuple&lt;0ul, 1ul, 2ul&gt;) (this=0x7fff200008c0, __args=&lt;unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f&gt;) at /usr/include/c++/4.9/functional:1264</h1>\n<h1>12 0x00007fffe2b6aa38 in std::_Bind&lt;std::_Mem_fn&lt;void (tensorflow::(anonymous namespace)::ExecutorState::<em>)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)&gt;(tensorflow::(anonymous namespace)::ExecutorState</em>, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)&gt;::operator()&lt;, void&gt;(void) (this=0x7fff200008c0) at /usr/include/c++/4.9/functional:1323</h1>\n<h1>13 0x00007fffe2b68c54 in std::_Function_handler&lt;void(), std::_Bind&lt;std::_Mem_fn&lt;void (tensorflow::(anonymous namespace)::ExecutorState::<em>)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)&gt;(tensorflow::(anonymous namespace)::ExecutorState</em>, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)&gt; &gt;::_M_invoke(const std::_Any_data &amp;) (__functor=...) at /usr/include/c++/4.9/functional:2039</h1>\n<h1>14 0x00007fffe0b01732 in std::function&lt;void ()&gt;::operator()() const (this=0x7fff357f9dd0) at /usr/include/c++/4.9/functional:2439</h1>\n<h1>15 0x00007fffe2da977a in tensorflow::thread::ThreadPool::Impl::WorkerLoop (this=0x3ddaa90) at tensorflow/core/lib/core/threadpool.cc:196</h1>\n<h1>16 0x00007fffe2da91bb in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&amp;, std::string const&amp;, int)::{lambda()<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"115886302\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1\">#1</a>}::operator()() const () at tensorflow/core/lib/core/threadpool.cc:123</h1>\n<h1>17 0x00007fffe2da9b93 in std::_Function_handler&lt;void(), tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&amp;, const string&amp;, int)::&lt;lambda()&gt; &gt;::_M_invoke(const std::_Any_data &amp;) (__functor=...) at /usr/include/c++/4.9/functional:2039</h1>\n<h1>18 0x00007fffe0b01732 in std::function&lt;void ()&gt;::operator()() const (this=0x3ddba58) at /usr/include/c++/4.9/functional:2439</h1>\n<h1>19 0x00007fffe2dcb472 in std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt;::_M_invoke&lt;&gt;(std::_Index_tuple&lt;&gt;) (this=0x3ddba58) at /usr/include/c++/4.9/functional:1700</h1>\n<h1>20 0x00007fffe2dcb3b7 in std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt;::operator()() (this=0x3ddba58) at /usr/include/c++/4.9/functional:1688</h1>\n<h1>21 0x00007fffe2dcb334 in std::thread::_Impl&lt;std::_Bind_simple&lt;std::function&lt;void ()&gt; ()&gt; &gt;::_M_run() (this=0x3ddba40) at /usr/include/c++/4.9/thread:115</h1>\n<h1>22 0x00007fffd4e4bf20 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6</h1>\n<h1>23 0x00007ffff7bc26aa in start_thread (arg=0x7fff357fa700) at pthread_create.c:333</h1>\n<h1>24 0x00007ffff78f7eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109</h1>\n</blockquote>", "body_text": "Environment info\nOperating System: Ubuntu 15.04\nInstalled version of CUDA and cuDNN:\nCUDA 7.5\ncuDNN 5.0.4\n(please attach the output of ls -l /path/to/cuda/lib/libcud*):\n-rw-r--r-- 1 root root 189170 Oct 10  2015 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Oct 10  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Oct 10  2015 /usr/local/cuda/lib/libcudart_static.a\nIf installed from binary pip package, provide:\n\nWhich pip package you installed.\npip list\ntensorflow (0.7.1)\nThe output from python -c \"import tensorflow; print(tensorflow.version)\".\n0.7.1\n\nProblem description\nI simply try to put a recurrent layer on top of a convolution layer, which directly manipulate the inputs. It is ok with cpu, but fails with gpu. When i remove either the convolution layer or the recurrent layer, it works well with gpu.\nThe error message is\n\npython: external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, const Eigen::TensorReductionOpEigen::internal::SumReducer<float, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> > > > >]: Assertion `cudaGetLastError() == cudaSuccess' failed.\n\nI run some black-box test (as i don't know a better way to debug it), and found that it raises error when it try to compute gradients with the statement\ngrads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), config.max_grad_norm) \nThe following is a simplified version of the code which has the same problem. (There ought to be a softmax layer on top of the recurrent layer, but i replace it with sum to make it simpler)\nimport numpy as np\nimport tensorflow as tf\n\nimport sys\nsys.path.append('/home/jiahua/tensorflow')\nimport math\nimport time\n\nfrom tensorflow.contrib.ctc import ctc_ops\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import rnn\nfrom conv_batch_normalizer import ConvolutionalBatchNormalizer\n\nimport load_data\n\nclass Model(object):\n\n  def __init__(self, is_training, config):\n    self._batch_size = batch_size = config.batch_size\n    self._freq_size = freq_size = config.freq_size\n    self._hidden_size = hidden_size = config.hidden_size\n    self._seq_max_length = seq_max_length = config.seq_max_length\n\n    num_channels = 10\n    filter_dimension = 2\n    conv_stride = 2\n\n    self._inputs = inputs = tf.placeholder(tf.float32, [batch_size, seq_max_length, freq_size]) \n    self._sequence_lengths = sequence_lengths = tf.placeholder(tf.int32, [batch_size])\n    inputs = tf.reshape(inputs, [batch_size, seq_max_length, freq_size, 1])\n\n    padding_size = filter_dimension - (seq_max_length - 1)  % conv_stride - 1 \n    print 'padding_size', padding_size\n    paddings = [[0, 0], [0, padding_size], [0, 0], [0, 0]]\n    inputs = tf.pad(inputs, paddings)\n\n    if is_training and config.keep_prob < 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n    print 'inputs', tf.Tensor.get_shape(inputs)\n\n    parameters = []\n    # conv1\n    with tf.name_scope('conv1') as scope:\n      kernel = tf.Variable(tf.truncated_normal([filter_dimension, freq_size, 1, num_channels], dtype=tf.float32, stddev=1e-1), name='weights')\n      conv = tf.nn.conv2d(inputs, kernel, [1, conv_stride, conv_stride, 1], padding='VALID')\n      biases = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases')\n      bias = tf.nn.bias_add(conv, biases)\n      conv1 = tf.nn.relu(bias, name=scope)\n      parameters += [kernel, biases]\n\n    # batch normalization\n    print 'conv1', tf.Tensor.get_shape(conv1)\n\n    inter1 = tf.reshape(conv1, [batch_size, -1, 1, num_channels])\n    print 'inter1', tf.Tensor.get_shape(inter1)\n\n    new_length = (seq_max_length + padding_size - filter_dimension) / conv_stride + 1\n    print 'new length = ', new_length \n\n    # rnn\n    rnn_inputs = tf.reshape(inter1, [batch_size, new_length, num_channels])\n    print 'rnn_inputs', tf.Tensor.get_shape(rnn_inputs)\n\n    lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n    if is_training and config.keep_prob < 1:\n      lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n    cell = rnn_cell.MultiRNNCell([lstm_cell] * 2)\n\n    self._initial_state = cell.zero_state(batch_size, tf.float32)\n\n    rnn_outputs = []\n    state = self._initial_state\n    with tf.variable_scope(\"rnn\"):\n      for time_step in range(new_length):\n        if time_step > 0: tf.get_variable_scope().reuse_variables()\n        (cell_output, state) = cell(rnn_inputs[:, time_step, :], state)\n        rnn_outputs.append(cell_output)\n\n    print 'rnn_outputs ', tf.Tensor.get_shape(rnn_outputs[0]) , ' * ', len(rnn_outputs)\n    self._final_state = state\n\n    self._cost = cost = tf.reduce_sum(tf.add_n(rnn_outputs)) / batch_size\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n\n    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n\n  def assign_lr(self, session, lr_value):\n    session.run(tf.assign(self._lr, lr_value))\n\n  @property\n  def input_data(self):\n    return self._input_data\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def predict(self):\n    return self._predict\n\n\nclass Config(object):\n  batch_size = 2\n  freq_size = 2\n  hidden_size = 2\n  seq_max_length = 5\n  init_scale = 1.0\n  lr_decay = 0.5\n  max_epoch = 10\n  max_max_epoch = 100\n  keep_prob = 0.9\n  max_grad_norm = 5\n  learning_rate = 1\n\n\ndef run_epoch(session, m, data, eval_op):\n  start_time = time.time()\n  iters = 0\n  costs = 0.0\n  state = m._initial_state.eval()\n  for (inputs, labels_indices, labels_values, labels_shape, seq_lens) in data:\n    cost, state, _ = session.run([m._cost, m._final_state, eval_op],\n                                 {m._inputs: inputs,\n                                  m._initial_state: state})\n    costs += cost\n    iters += 1\n    if iters % 1 == 0:\n      print iters\n      print \"time: %f\" % (time.time() - start_time)\n      print \"cost: %f\" % cost\n  print 'finish'\n  print \"total time: %f\" % (time.time() - start_time)\n  return costs / iters\n  return 0.0\n\n\ndef main():\n  config = Config()\n  with tf.Graph().as_default(), tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n      m = Model(is_training=True, config=config)\n    init_op = tf.initialize_all_variables()\n    session.run(init_op)\n\n    train_data = [([[[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]], [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]], [[0, 0], [0, 1], [1, 0], [1, 1]], [0, 1, 1, 2], [2, 2], [2, 2])]\n    for i in range(config.max_max_epoch):\n      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n      print '#iter %d start lr: %f' % (i, config.learning_rate * lr_decay)\n      m.assign_lr(session, config.learning_rate * lr_decay)\n\n      print 'average loss: %f' % (run_epoch(session, m, train_data, m._train_op))\n\n\nif __name__ == \"__main__\":\n  main()\n\nWhat have you tried?\nBelow is the stack status printed using gdb python, in case you want to look at it. It seems that there is a problem when it tries to launch a cuda kernel to compute the l2loss.\n\n0  0x00007ffff7826267 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:55\n1  0x00007ffff7827eca in __GI_abort () at abort.c:89\n2  0x00007ffff781f03d in assert_fail_base (fmt=0x7ffff7981028 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=file@entry=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=line@entry=223, function=function@entry=0x7fffe3c04180 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen:\"...) at assert.c:92\n3  0x00007ffff781f0f2 in GI___assert_fail (assertion=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=223, function=0x7fffe3c04180 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen:\"...) at assert.c:101\n4  0x00007fffe2487e26 in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&) (expr=..., device=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223\n5  0x00007fffe24879ea in Eigen::TensorDevice<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::GpuDevice>::operator=Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> >(Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const&) (this=0x7fff357f91c0, other=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35\n6  0x00007fffe248785a in tensorflow::functor::L2Loss<Eigen::GpuDevice, float>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>) (this=0x7fff357f9250, d=..., input=..., output=...) at ./tensorflow/core/kernels/l2loss_op.h:32\n7  0x00007fffe247ee64 in tensorflow::L2LossOp<Eigen::GpuDevice, float>::Compute (this=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/kernels/l2loss_op.cc:45\n8  0x00007fffe293d927 in tensorflow::BaseGPUDevice::Compute (this=0x3dc5b60, op_kernel=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/common_runtime/gpu/gpu_device.cc:388\n9  0x00007fffe2b61689 in tensorflow::(anonymous namespace)::ExecutorState::Process (this=0x3facbc0, tagged_node=..., scheduled_usec=0) at tensorflow/core/common_runtime/executor.cc:1092\n10 0x00007fffe2b6cf89 in std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)>::operator()<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&, long long&, void> (this=0x7fff200008c0, __object=0x3facbc0) at /usr/include/c++/4.9/functional:569\n11 0x00007fffe2b6c3dc in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::__call<void, 0ul, 1ul, 2ul>(<unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f>, std::_Index_tuple<0ul, 1ul, 2ul>) (this=0x7fff200008c0, __args=<unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f>) at /usr/include/c++/4.9/functional:1264\n12 0x00007fffe2b6aa38 in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::operator()<, void>(void) (this=0x7fff200008c0) at /usr/include/c++/4.9/functional:1323\n13 0x00007fffe2b68c54 in std::_Function_handler<void(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n14 0x00007fffe0b01732 in std::function<void ()>::operator()() const (this=0x7fff357f9dd0) at /usr/include/c++/4.9/functional:2439\n15 0x00007fffe2da977a in tensorflow::thread::ThreadPool::Impl::WorkerLoop (this=0x3ddaa90) at tensorflow/core/lib/core/threadpool.cc:196\n16 0x00007fffe2da91bb in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int)::{lambda()#1}::operator()() const () at tensorflow/core/lib/core/threadpool.cc:123\n17 0x00007fffe2da9b93 in std::_Function_handler<void(), tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&, const string&, int)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n18 0x00007fffe0b01732 in std::function<void ()>::operator()() const (this=0x3ddba58) at /usr/include/c++/4.9/functional:2439\n19 0x00007fffe2dcb472 in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x3ddba58) at /usr/include/c++/4.9/functional:1700\n20 0x00007fffe2dcb3b7 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x3ddba58) at /usr/include/c++/4.9/functional:1688\n21 0x00007fffe2dcb334 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x3ddba40) at /usr/include/c++/4.9/thread:115\n22 0x00007fffd4e4bf20 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n23 0x00007ffff7bc26aa in start_thread (arg=0x7fff357fa700) at pthread_create.c:333\n24 0x00007ffff78f7eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109", "body": "### Environment info\n\nOperating System: Ubuntu 15.04\n\nInstalled version of CUDA and cuDNN: \nCUDA 7.5\ncuDNN 5.0.4\n(please attach the output of `ls -l /path/to/cuda/lib/libcud*`):\n-rw-r--r-- 1 root root 189170 Oct 10  2015 /usr/local/cuda/lib/libcudadevrt.a\nlrwxrwxrwx 1 root root     16 Oct 10  2015 /usr/local/cuda/lib/libcudart.so -> libcudart.so.7.5\nlrwxrwxrwx 1 root root     19 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5 -> libcudart.so.7.5.18\n-rwxr-xr-x 1 root root 311596 Oct 10  2015 /usr/local/cuda/lib/libcudart.so.7.5.18\n-rw-r--r-- 1 root root 558020 Oct 10  2015 /usr/local/cuda/lib/libcudart_static.a\n\nIf installed from binary pip package, provide:\n1. Which pip package you installed.\n   pip list\n   tensorflow (0.7.1)\n2. The output from python -c \"import tensorflow; print(tensorflow.**version**)\".\n   0.7.1\n### Problem description\n\nI simply try to put a recurrent layer on top of a convolution layer, which directly manipulate the inputs. It is ok with cpu, but fails with gpu. When i remove either the convolution layer or the recurrent layer, it works well with gpu. \n\nThe error message is \n\n> python: external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223: static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long int>, 16>, const Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, const Eigen::TensorReductionOpEigen::internal::SumReducer<float, const Eigen::DimensionList<long int, 1ul>, const Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<const float>, const Eigen::TensorMap<Eigen::Tensor<const float, 1, 1, long int>, 16> > > > >]: Assertion `cudaGetLastError() == cudaSuccess' failed.\n\nI run some black-box test (as i don't know a better way to debug it), and found that it raises error when it try to compute gradients with the statement \n`grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n`\n\nThe following is a simplified version of the code which has the same problem. (There ought to be a softmax layer on top of the recurrent layer, but i replace it with sum to make it simpler)\n\n```\nimport numpy as np\nimport tensorflow as tf\n\nimport sys\nsys.path.append('/home/jiahua/tensorflow')\nimport math\nimport time\n\nfrom tensorflow.contrib.ctc import ctc_ops\nfrom tensorflow.python.ops import rnn_cell\nfrom tensorflow.python.ops import rnn\nfrom conv_batch_normalizer import ConvolutionalBatchNormalizer\n\nimport load_data\n\nclass Model(object):\n\n  def __init__(self, is_training, config):\n    self._batch_size = batch_size = config.batch_size\n    self._freq_size = freq_size = config.freq_size\n    self._hidden_size = hidden_size = config.hidden_size\n    self._seq_max_length = seq_max_length = config.seq_max_length\n\n    num_channels = 10\n    filter_dimension = 2\n    conv_stride = 2\n\n    self._inputs = inputs = tf.placeholder(tf.float32, [batch_size, seq_max_length, freq_size]) \n    self._sequence_lengths = sequence_lengths = tf.placeholder(tf.int32, [batch_size])\n    inputs = tf.reshape(inputs, [batch_size, seq_max_length, freq_size, 1])\n\n    padding_size = filter_dimension - (seq_max_length - 1)  % conv_stride - 1 \n    print 'padding_size', padding_size\n    paddings = [[0, 0], [0, padding_size], [0, 0], [0, 0]]\n    inputs = tf.pad(inputs, paddings)\n\n    if is_training and config.keep_prob < 1:\n      inputs = tf.nn.dropout(inputs, config.keep_prob)\n    print 'inputs', tf.Tensor.get_shape(inputs)\n\n    parameters = []\n    # conv1\n    with tf.name_scope('conv1') as scope:\n      kernel = tf.Variable(tf.truncated_normal([filter_dimension, freq_size, 1, num_channels], dtype=tf.float32, stddev=1e-1), name='weights')\n      conv = tf.nn.conv2d(inputs, kernel, [1, conv_stride, conv_stride, 1], padding='VALID')\n      biases = tf.Variable(tf.constant(0.0, shape=[num_channels], dtype=tf.float32), trainable=True, name='biases')\n      bias = tf.nn.bias_add(conv, biases)\n      conv1 = tf.nn.relu(bias, name=scope)\n      parameters += [kernel, biases]\n\n    # batch normalization\n    print 'conv1', tf.Tensor.get_shape(conv1)\n\n    inter1 = tf.reshape(conv1, [batch_size, -1, 1, num_channels])\n    print 'inter1', tf.Tensor.get_shape(inter1)\n\n    new_length = (seq_max_length + padding_size - filter_dimension) / conv_stride + 1\n    print 'new length = ', new_length \n\n    # rnn\n    rnn_inputs = tf.reshape(inter1, [batch_size, new_length, num_channels])\n    print 'rnn_inputs', tf.Tensor.get_shape(rnn_inputs)\n\n    lstm_cell = rnn_cell.BasicLSTMCell(hidden_size, forget_bias=0.0)\n    if is_training and config.keep_prob < 1:\n      lstm_cell = rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=config.keep_prob)\n    cell = rnn_cell.MultiRNNCell([lstm_cell] * 2)\n\n    self._initial_state = cell.zero_state(batch_size, tf.float32)\n\n    rnn_outputs = []\n    state = self._initial_state\n    with tf.variable_scope(\"rnn\"):\n      for time_step in range(new_length):\n        if time_step > 0: tf.get_variable_scope().reuse_variables()\n        (cell_output, state) = cell(rnn_inputs[:, time_step, :], state)\n        rnn_outputs.append(cell_output)\n\n    print 'rnn_outputs ', tf.Tensor.get_shape(rnn_outputs[0]) , ' * ', len(rnn_outputs)\n    self._final_state = state\n\n    self._cost = cost = tf.reduce_sum(tf.add_n(rnn_outputs)) / batch_size\n\n    if not is_training:\n      return\n\n    self._lr = tf.Variable(0.0, trainable=False)\n    tvars = tf.trainable_variables()\n    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),\n                                      config.max_grad_norm)\n\n    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n    self._train_op = optimizer.apply_gradients(zip(grads, tvars))\n\n\n  def assign_lr(self, session, lr_value):\n    session.run(tf.assign(self._lr, lr_value))\n\n  @property\n  def input_data(self):\n    return self._input_data\n\n  @property\n  def cost(self):\n    return self._cost\n\n  @property\n  def final_state(self):\n    return self._final_state\n\n  @property\n  def lr(self):\n    return self._lr\n\n  @property\n  def train_op(self):\n    return self._train_op\n\n  @property\n  def predict(self):\n    return self._predict\n\n\nclass Config(object):\n  batch_size = 2\n  freq_size = 2\n  hidden_size = 2\n  seq_max_length = 5\n  init_scale = 1.0\n  lr_decay = 0.5\n  max_epoch = 10\n  max_max_epoch = 100\n  keep_prob = 0.9\n  max_grad_norm = 5\n  learning_rate = 1\n\n\ndef run_epoch(session, m, data, eval_op):\n  start_time = time.time()\n  iters = 0\n  costs = 0.0\n  state = m._initial_state.eval()\n  for (inputs, labels_indices, labels_values, labels_shape, seq_lens) in data:\n    cost, state, _ = session.run([m._cost, m._final_state, eval_op],\n                                 {m._inputs: inputs,\n                                  m._initial_state: state})\n    costs += cost\n    iters += 1\n    if iters % 1 == 0:\n      print iters\n      print \"time: %f\" % (time.time() - start_time)\n      print \"cost: %f\" % cost\n  print 'finish'\n  print \"total time: %f\" % (time.time() - start_time)\n  return costs / iters\n  return 0.0\n\n\ndef main():\n  config = Config()\n  with tf.Graph().as_default(), tf.Session() as session:\n    initializer = tf.random_uniform_initializer(-config.init_scale,\n                                                config.init_scale)\n    with tf.variable_scope(\"model\", reuse=None, initializer=initializer):\n      m = Model(is_training=True, config=config)\n    init_op = tf.initialize_all_variables()\n    session.run(init_op)\n\n    train_data = [([[[0, 1], [1, 2], [2, 3], [3, 4], [4, 5]], [[1, 2], [2, 3], [3, 4], [4, 5], [5, 6]]], [[0, 0], [0, 1], [1, 0], [1, 1]], [0, 1, 1, 2], [2, 2], [2, 2])]\n    for i in range(config.max_max_epoch):\n      lr_decay = config.lr_decay ** max(i - config.max_epoch, 0.0)\n      print '#iter %d start lr: %f' % (i, config.learning_rate * lr_decay)\n      m.assign_lr(session, config.learning_rate * lr_decay)\n\n      print 'average loss: %f' % (run_epoch(session, m, train_data, m._train_op))\n\n\nif __name__ == \"__main__\":\n  main()\n```\n### What have you tried?\n\nBelow is the stack status printed using gdb python, in case you want to look at it. It seems that there is a problem when it tries to launch a cuda kernel to compute the l2loss.\n\n> # 0  0x00007ffff7826267 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:55\n> # 1  0x00007ffff7827eca in __GI_abort () at abort.c:89\n> # 2  0x00007ffff781f03d in **assert_fail_base (fmt=0x7ffff7981028 \"%s%s%s:%u: %s%sAssertion `%s' failed.\\n%n\", assertion=assertion@entry=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=file@entry=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=line@entry=223, function=function@entry=0x7fffe3c04180 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION**> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen:\"...) at assert.c:92\n> # 3  0x00007ffff781f0f2 in **GI___assert_fail (assertion=0x7fffe3c03fe8 \"cudaGetLastError() == cudaSuccess\", file=0x7fffe3c03f80 \"external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h\", line=223, function=0x7fffe3c04180 <Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&)::__PRETTY_FUNCTION**> \"static void Eigen::internal::TensorExecutor<Expression, Eigen::GpuDevice, false>::run(const Expression&, const Eigen::GpuDevice&) [with Expression = const Eigen::TensorAssignOp<Eigen::TensorMap<Eigen:\"...) at assert.c:101\n> # 4  0x00007fffe2487e26 in Eigen::internal::TensorExecutor<Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const, Eigen::GpuDevice, false>::run(Eigen::TensorAssignOp<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const> const&, Eigen::GpuDevice const&) (expr=..., device=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorExecutor.h:223\n> # 5  0x00007fffe24879ea in Eigen::TensorDevice<Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>, Eigen::GpuDevice>::operator=Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> >(Eigen::TensorCwiseUnaryOpEigen::internal::scalar_multiple_op<float, Eigen::TensorReductionOpEigen::internal::SumReducer<float, Eigen::DimensionList<long, 1ul> const, Eigen::TensorCwiseUnaryOp<Eigen::internal::scalar_square_op<float const>, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16> const> const> const> const&) (this=0x7fff357f91c0, other=...) at external/eigen_archive/eigen-eigen-3f653ace7d28/unsupported/Eigen/CXX11/src/Tensor/TensorDevice.h:35\n> # 6  0x00007fffe248785a in tensorflow::functor::L2Loss<Eigen::GpuDevice, float>::operator()(Eigen::GpuDevice const&, Eigen::TensorMap<Eigen::Tensor<float const, 1, 1, long>, 16>, Eigen::TensorMap<Eigen::TensorFixedSize<float, Eigen::Sizes<>, 1, long>, 16>) (this=0x7fff357f9250, d=..., input=..., output=...) at ./tensorflow/core/kernels/l2loss_op.h:32\n> # 7  0x00007fffe247ee64 in tensorflow::L2LossOp<Eigen::GpuDevice, float>::Compute (this=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/kernels/l2loss_op.cc:45\n> # 8  0x00007fffe293d927 in tensorflow::BaseGPUDevice::Compute (this=0x3dc5b60, op_kernel=0x402f9e0, context=0x7fff357f9b20) at tensorflow/core/common_runtime/gpu/gpu_device.cc:388\n> # 9  0x00007fffe2b61689 in tensorflow::(anonymous namespace)::ExecutorState::Process (this=0x3facbc0, tagged_node=..., scheduled_usec=0) at tensorflow/core/common_runtime/executor.cc:1092\n> # 10 0x00007fffe2b6cf89 in std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long)>::operator()<tensorflow::(anonymous namespace)::ExecutorState::TaggedNode&, long long&, void> (this=0x7fff200008c0, __object=0x3facbc0) at /usr/include/c++/4.9/functional:569\n> # 11 0x00007fffe2b6c3dc in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::__call<void, 0ul, 1ul, 2ul>(<unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f>, std::_Index_tuple<0ul, 1ul, 2ul>) (this=0x7fff200008c0, __args=<unknown type in /home/jiahua/tensorflow/bazel-bin/speech/model.runfiles/tensorflow/python/_pywrap_tensorflow.so, CU 0xdb55fb8, DIE 0xdbd4a7f>) at /usr/include/c++/4.9/functional:1264\n> # 12 0x00007fffe2b6aa38 in std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>::operator()<, void>(void) (this=0x7fff200008c0) at /usr/include/c++/4.9/functional:1323\n> # 13 0x00007fffe2b68c54 in std::_Function_handler<void(), std::_Bind<std::_Mem_fn<void (tensorflow::(anonymous namespace)::ExecutorState::*)(tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)>(tensorflow::(anonymous namespace)::ExecutorState*, tensorflow::(anonymous namespace)::ExecutorState::TaggedNode, long long int)> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n> # 14 0x00007fffe0b01732 in std::function<void ()>::operator()() const (this=0x7fff357f9dd0) at /usr/include/c++/4.9/functional:2439\n> # 15 0x00007fffe2da977a in tensorflow::thread::ThreadPool::Impl::WorkerLoop (this=0x3ddaa90) at tensorflow/core/lib/core/threadpool.cc:196\n> # 16 0x00007fffe2da91bb in tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, tensorflow::ThreadOptions const&, std::string const&, int)::{lambda()#1}::operator()() const () at tensorflow/core/lib/core/threadpool.cc:123\n> # 17 0x00007fffe2da9b93 in std::_Function_handler<void(), tensorflow::thread::ThreadPool::Impl::Impl(tensorflow::Env*, const tensorflow::ThreadOptions&, const string&, int)::<lambda()> >::_M_invoke(const std::_Any_data &) (__functor=...) at /usr/include/c++/4.9/functional:2039\n> # 18 0x00007fffe0b01732 in std::function<void ()>::operator()() const (this=0x3ddba58) at /usr/include/c++/4.9/functional:2439\n> # 19 0x00007fffe2dcb472 in std::_Bind_simple<std::function<void ()> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x3ddba58) at /usr/include/c++/4.9/functional:1700\n> # 20 0x00007fffe2dcb3b7 in std::_Bind_simple<std::function<void ()> ()>::operator()() (this=0x3ddba58) at /usr/include/c++/4.9/functional:1688\n> # 21 0x00007fffe2dcb334 in std::thread::_Impl<std::_Bind_simple<std::function<void ()> ()> >::_M_run() (this=0x3ddba40) at /usr/include/c++/4.9/thread:115\n> # 22 0x00007fffd4e4bf20 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6\n> # 23 0x00007ffff7bc26aa in start_thread (arg=0x7fff357fa700) at pthread_create.c:333\n> # 24 0x00007ffff78f7eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109\n"}