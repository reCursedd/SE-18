{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/356025132", "html_url": "https://github.com/tensorflow/tensorflow/issues/9141#issuecomment-356025132", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9141", "id": 356025132, "node_id": "MDEyOklzc3VlQ29tbWVudDM1NjAyNTEzMg==", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-08T16:54:46Z", "updated_at": "2018-01-08T16:55:28Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>For BatchNorm, we do have an op in XLA, and lately Justin Lebar tried it with the GPU backend of XLA using cuDNN, but concluded that raw XLA is actually faster than cuDNN for this operation, IIRC</p>\n</blockquote>\n<p>Just to be clear, what I saw was that on our models, using cudnn batchnorm was roughly the same speed as our XLA implementation.  However, our lowering of these particular XLA ops to PTX is not very good at the moment -- I think it could be made significantly faster, at which point we would beat cudnn.</p>\n<p>I don't see why batch re-normalization would be different, although someone would have to benchmark.</p>\n<p>Note that the comparison I was making was between everything-compiled-with-xla+cudnn-batchnorm vs everything-compiled-with-xla+pure-xla-batchnorm.  If you tried instead compiling just the one TF batchnorm (or batch-renorm) op with XLA, then XLA wouldn't be able to fuse the op's inputs or outputs, and you might indeed find that cudnn is faster than XLA.</p>\n<p>I'm going to unassign myself since it doesn't sound like there's work that we need to do here that's separate from the regular XLA performance work we're already doing.  (This is a good thing -- part of the promise of XLA is that we don't have to open a bug every time we have a new op that we want to optimize.)</p>", "body_text": "For BatchNorm, we do have an op in XLA, and lately Justin Lebar tried it with the GPU backend of XLA using cuDNN, but concluded that raw XLA is actually faster than cuDNN for this operation, IIRC\n\nJust to be clear, what I saw was that on our models, using cudnn batchnorm was roughly the same speed as our XLA implementation.  However, our lowering of these particular XLA ops to PTX is not very good at the moment -- I think it could be made significantly faster, at which point we would beat cudnn.\nI don't see why batch re-normalization would be different, although someone would have to benchmark.\nNote that the comparison I was making was between everything-compiled-with-xla+cudnn-batchnorm vs everything-compiled-with-xla+pure-xla-batchnorm.  If you tried instead compiling just the one TF batchnorm (or batch-renorm) op with XLA, then XLA wouldn't be able to fuse the op's inputs or outputs, and you might indeed find that cudnn is faster than XLA.\nI'm going to unassign myself since it doesn't sound like there's work that we need to do here that's separate from the regular XLA performance work we're already doing.  (This is a good thing -- part of the promise of XLA is that we don't have to open a bug every time we have a new op that we want to optimize.)", "body": "> For BatchNorm, we do have an op in XLA, and lately Justin Lebar tried it with the GPU backend of XLA using cuDNN, but concluded that raw XLA is actually faster than cuDNN for this operation, IIRC\r\n\r\nJust to be clear, what I saw was that on our models, using cudnn batchnorm was roughly the same speed as our XLA implementation.  However, our lowering of these particular XLA ops to PTX is not very good at the moment -- I think it could be made significantly faster, at which point we would beat cudnn.\r\n\r\nI don't see why batch re-normalization would be different, although someone would have to benchmark.\r\n\r\nNote that the comparison I was making was between everything-compiled-with-xla+cudnn-batchnorm vs everything-compiled-with-xla+pure-xla-batchnorm.  If you tried instead compiling just the one TF batchnorm (or batch-renorm) op with XLA, then XLA wouldn't be able to fuse the op's inputs or outputs, and you might indeed find that cudnn is faster than XLA.\r\n\r\nI'm going to unassign myself since it doesn't sound like there's work that we need to do here that's separate from the regular XLA performance work we're already doing.  (This is a good thing -- part of the promise of XLA is that we don't have to open a bug every time we have a new op that we want to optimize.)\r\n  "}