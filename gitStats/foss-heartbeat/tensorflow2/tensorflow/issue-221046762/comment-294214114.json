{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/294214114", "html_url": "https://github.com/tensorflow/tensorflow/issues/9141#issuecomment-294214114", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9141", "id": 294214114, "node_id": "MDEyOklzc3VlQ29tbWVudDI5NDIxNDExNA==", "user": {"login": "broune", "id": 1933380, "node_id": "MDQ6VXNlcjE5MzMzODA=", "avatar_url": "https://avatars3.githubusercontent.com/u/1933380?v=4", "gravatar_id": "", "url": "https://api.github.com/users/broune", "html_url": "https://github.com/broune", "followers_url": "https://api.github.com/users/broune/followers", "following_url": "https://api.github.com/users/broune/following{/other_user}", "gists_url": "https://api.github.com/users/broune/gists{/gist_id}", "starred_url": "https://api.github.com/users/broune/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/broune/subscriptions", "organizations_url": "https://api.github.com/users/broune/orgs", "repos_url": "https://api.github.com/users/broune/repos", "events_url": "https://api.github.com/users/broune/events{/privacy}", "received_events_url": "https://api.github.com/users/broune/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-14T18:55:29Z", "updated_at": "2017-04-14T18:55:29Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We are aware that batch normalization can be improved on GPU (and presumably on CPU). We are currently working on enabling fusion nodes with multiple outputs (also for other reasons), and this may help here, since some batchnorm division ops are not getting fused because they are needed by several other ops, but those ops, including the divide, could be combined into a single fusion node if fusion nodes could have multiple outputs. That would prevent the intermediate data from getting written to memory. The division nodes in particular are not getting fused because the XLA heuristic is that these are expensive on GPU and so they are not duplicated into the consuming fusion nodes.</p>\n<p>There also seems to be a problem where a broadcast op is not fused into a fusion node that does a reduce, so the broadcast ends up being written to memory and then read back, which doesn't seem good, especially since the source data for the broadcast is small. I'll pass that problem on.</p>\n<p>We may in future consider adding a dedicated batchnorm op to XLA, since batchnorm is too large of a subgraph to reasonably pattern match, though not clear at this point if that will end up being useful.</p>", "body_text": "We are aware that batch normalization can be improved on GPU (and presumably on CPU). We are currently working on enabling fusion nodes with multiple outputs (also for other reasons), and this may help here, since some batchnorm division ops are not getting fused because they are needed by several other ops, but those ops, including the divide, could be combined into a single fusion node if fusion nodes could have multiple outputs. That would prevent the intermediate data from getting written to memory. The division nodes in particular are not getting fused because the XLA heuristic is that these are expensive on GPU and so they are not duplicated into the consuming fusion nodes.\nThere also seems to be a problem where a broadcast op is not fused into a fusion node that does a reduce, so the broadcast ends up being written to memory and then read back, which doesn't seem good, especially since the source data for the broadcast is small. I'll pass that problem on.\nWe may in future consider adding a dedicated batchnorm op to XLA, since batchnorm is too large of a subgraph to reasonably pattern match, though not clear at this point if that will end up being useful.", "body": "We are aware that batch normalization can be improved on GPU (and presumably on CPU). We are currently working on enabling fusion nodes with multiple outputs (also for other reasons), and this may help here, since some batchnorm division ops are not getting fused because they are needed by several other ops, but those ops, including the divide, could be combined into a single fusion node if fusion nodes could have multiple outputs. That would prevent the intermediate data from getting written to memory. The division nodes in particular are not getting fused because the XLA heuristic is that these are expensive on GPU and so they are not duplicated into the consuming fusion nodes.\r\n\r\nThere also seems to be a problem where a broadcast op is not fused into a fusion node that does a reduce, so the broadcast ends up being written to memory and then read back, which doesn't seem good, especially since the source data for the broadcast is small. I'll pass that problem on.\r\n\r\nWe may in future consider adding a dedicated batchnorm op to XLA, since batchnorm is too large of a subgraph to reasonably pattern match, though not clear at this point if that will end up being useful."}