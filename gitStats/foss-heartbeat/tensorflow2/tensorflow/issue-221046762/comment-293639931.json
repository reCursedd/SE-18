{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/293639931", "html_url": "https://github.com/tensorflow/tensorflow/issues/9141#issuecomment-293639931", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9141", "id": 293639931, "node_id": "MDEyOklzc3VlQ29tbWVudDI5MzYzOTkzMQ==", "user": {"login": "Caenorst", "id": 10882588, "node_id": "MDQ6VXNlcjEwODgyNTg4", "avatar_url": "https://avatars1.githubusercontent.com/u/10882588?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Caenorst", "html_url": "https://github.com/Caenorst", "followers_url": "https://api.github.com/users/Caenorst/followers", "following_url": "https://api.github.com/users/Caenorst/following{/other_user}", "gists_url": "https://api.github.com/users/Caenorst/gists{/gist_id}", "starred_url": "https://api.github.com/users/Caenorst/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Caenorst/subscriptions", "organizations_url": "https://api.github.com/users/Caenorst/orgs", "repos_url": "https://api.github.com/users/Caenorst/repos", "events_url": "https://api.github.com/users/Caenorst/events{/privacy}", "received_events_url": "https://api.github.com/users/Caenorst/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-12T16:49:35Z", "updated_at": "2017-04-12T16:49:35Z", "author_association": "NONE", "body_html": "<p>From my understanding, Fused Batch Norm is mostly faster because of CuDNN implementation.</p>\n<p>CuDNN implementation return the variance (while it internally compute the inverse of stddev to normalize), but Batch Renormalization apply the moving average on the the stddev, so you would have to apply again rsqrt to find the inverse of stddev and the stddev, which is I guess the slowest part of the batch norm.</p>\n<p>So unless Nvidia decide to do a CuDNN implementation of batch renormalization, XLA should be the way to go.</p>", "body_text": "From my understanding, Fused Batch Norm is mostly faster because of CuDNN implementation.\nCuDNN implementation return the variance (while it internally compute the inverse of stddev to normalize), but Batch Renormalization apply the moving average on the the stddev, so you would have to apply again rsqrt to find the inverse of stddev and the stddev, which is I guess the slowest part of the batch norm.\nSo unless Nvidia decide to do a CuDNN implementation of batch renormalization, XLA should be the way to go.", "body": "From my understanding, Fused Batch Norm is mostly faster because of CuDNN implementation.\r\n\r\nCuDNN implementation return the variance (while it internally compute the inverse of stddev to normalize), but Batch Renormalization apply the moving average on the the stddev, so you would have to apply again rsqrt to find the inverse of stddev and the stddev, which is I guess the slowest part of the batch norm.\r\n\r\nSo unless Nvidia decide to do a CuDNN implementation of batch renormalization, XLA should be the way to go."}