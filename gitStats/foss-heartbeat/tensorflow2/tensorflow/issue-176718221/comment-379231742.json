{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/379231742", "html_url": "https://github.com/tensorflow/tensorflow/issues/4361#issuecomment-379231742", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4361", "id": 379231742, "node_id": "MDEyOklzc3VlQ29tbWVudDM3OTIzMTc0Mg==", "user": {"login": "drewanye", "id": 12620642, "node_id": "MDQ6VXNlcjEyNjIwNjQy", "avatar_url": "https://avatars3.githubusercontent.com/u/12620642?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drewanye", "html_url": "https://github.com/drewanye", "followers_url": "https://api.github.com/users/drewanye/followers", "following_url": "https://api.github.com/users/drewanye/following{/other_user}", "gists_url": "https://api.github.com/users/drewanye/gists{/gist_id}", "starred_url": "https://api.github.com/users/drewanye/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drewanye/subscriptions", "organizations_url": "https://api.github.com/users/drewanye/orgs", "repos_url": "https://api.github.com/users/drewanye/repos", "events_url": "https://api.github.com/users/drewanye/events{/privacy}", "received_events_url": "https://api.github.com/users/drewanye/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-06T11:57:12Z", "updated_at": "2018-04-06T12:00:17Z", "author_association": "NONE", "body_html": "<p>I solve the problem of reusing batch_normalization by specifying reuse=False when first creating bn(I use slim, but it's same to tf.layers.batch_normalization):</p>\n<pre><code>scope = tf.get_variable_scope()\nbn1 = slim.batch_norm(input1, decay=0.9, reuse=False, scope=scope, is_training=is_training)\nbn2 = slim.batch_norm(input2, decay=0.9, reuse=True, scope=scope, is_training=is_training)\n</code></pre>\n<p>You have to specify reuse=False at your first time to create parameters in batch normalization. Or you will get the error info:<br>\nVariable cnn/block1/conv1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?</p>", "body_text": "I solve the problem of reusing batch_normalization by specifying reuse=False when first creating bn(I use slim, but it's same to tf.layers.batch_normalization):\nscope = tf.get_variable_scope()\nbn1 = slim.batch_norm(input1, decay=0.9, reuse=False, scope=scope, is_training=is_training)\nbn2 = slim.batch_norm(input2, decay=0.9, reuse=True, scope=scope, is_training=is_training)\n\nYou have to specify reuse=False at your first time to create parameters in batch normalization. Or you will get the error info:\nVariable cnn/block1/conv1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?", "body": "I solve the problem of reusing batch_normalization by specifying reuse=False when first creating bn(I use slim, but it's same to tf.layers.batch_normalization):\r\n```\r\nscope = tf.get_variable_scope()\r\nbn1 = slim.batch_norm(input1, decay=0.9, reuse=False, scope=scope, is_training=is_training)\r\nbn2 = slim.batch_norm(input2, decay=0.9, reuse=True, scope=scope, is_training=is_training)\r\n```\r\nYou have to specify reuse=False at your first time to create parameters in batch normalization. Or you will get the error info:\r\nVariable cnn/block1/conv1/beta does not exist, or was not created with tf.get_variable(). Did you mean to set reuse=None in VarScope?\r\n\r\n"}