{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/395341214", "html_url": "https://github.com/tensorflow/tensorflow/issues/4361#issuecomment-395341214", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4361", "id": 395341214, "node_id": "MDEyOklzc3VlQ29tbWVudDM5NTM0MTIxNA==", "user": {"login": "qingchenwuhou", "id": 26782793, "node_id": "MDQ6VXNlcjI2NzgyNzkz", "avatar_url": "https://avatars2.githubusercontent.com/u/26782793?v=4", "gravatar_id": "", "url": "https://api.github.com/users/qingchenwuhou", "html_url": "https://github.com/qingchenwuhou", "followers_url": "https://api.github.com/users/qingchenwuhou/followers", "following_url": "https://api.github.com/users/qingchenwuhou/following{/other_user}", "gists_url": "https://api.github.com/users/qingchenwuhou/gists{/gist_id}", "starred_url": "https://api.github.com/users/qingchenwuhou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/qingchenwuhou/subscriptions", "organizations_url": "https://api.github.com/users/qingchenwuhou/orgs", "repos_url": "https://api.github.com/users/qingchenwuhou/repos", "events_url": "https://api.github.com/users/qingchenwuhou/events{/privacy}", "received_events_url": "https://api.github.com/users/qingchenwuhou/received_events", "type": "User", "site_admin": false}, "created_at": "2018-06-07T08:41:11Z", "updated_at": "2018-06-07T11:23:14Z", "author_association": "NONE", "body_html": "<p>I obey <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7198141\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/wjiangcmu\">@wjiangcmu</a> 's advice, it works.<br>\nthe code:<br>\n33         self.is_training = tf.placeholder(tf.bool, name='MODE')<br>\n// first use:<br>\n94         self.img_bn1 = tf.cond(self.is_training,<br>\n95                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = False),<br>\n96                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True))</p>\n<p>// add update_ops before second ruse, and filter out unrelated update_ops(unrelated moving mean and variance)<br>\n126         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)<br>\n127         print('update_ops')<br>\n128         for key in update_ops:<br>\n129             print(key)<br>\n131         i2t_update_extra_ops = [elem for elem in update_ops if 'text_feature/attention' not in elem.name]</p>\n<p>// second use:<br>\n132         self.img_neg_bn1 = batch_norm(self.img_neg_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True)</p>\n<p>// weight update and dependent extra_ops(moving mean and variance)<br>\n242         self.i2t_optimizer = tf.train.GradientDescentOptimizer(learning_rate )<br>\n243         i2t_update_grads = self.i2t_optimizer.minimize(self.i2t_loss)<br>\n244<br>\n245         i2t_train_ops = [i2t_update_grads] + i2t_update_extra_ops<br>\n246         self.i2t_updates = tf.group(*i2t_train_ops)</p>\n<p>in addition,  in order to update each batch_norm only once, according to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2537736\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/bsautermeister\">@bsautermeister</a> 's \"UPDATE_OPS in combination with reuse varscope\", I add the update_ops before the second use each batch_norm, and filter out unrelated update_ops.</p>\n<p>Hope this will be helpful for others.</p>", "body_text": "I obey @wjiangcmu 's advice, it works.\nthe code:\n33         self.is_training = tf.placeholder(tf.bool, name='MODE')\n// first use:\n94         self.img_bn1 = tf.cond(self.is_training,\n95                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = False),\n96                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True))\n// add update_ops before second ruse, and filter out unrelated update_ops(unrelated moving mean and variance)\n126         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n127         print('update_ops')\n128         for key in update_ops:\n129             print(key)\n131         i2t_update_extra_ops = [elem for elem in update_ops if 'text_feature/attention' not in elem.name]\n// second use:\n132         self.img_neg_bn1 = batch_norm(self.img_neg_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True)\n// weight update and dependent extra_ops(moving mean and variance)\n242         self.i2t_optimizer = tf.train.GradientDescentOptimizer(learning_rate )\n243         i2t_update_grads = self.i2t_optimizer.minimize(self.i2t_loss)\n244\n245         i2t_train_ops = [i2t_update_grads] + i2t_update_extra_ops\n246         self.i2t_updates = tf.group(*i2t_train_ops)\nin addition,  in order to update each batch_norm only once, according to @bsautermeister 's \"UPDATE_OPS in combination with reuse varscope\", I add the update_ops before the second use each batch_norm, and filter out unrelated update_ops.\nHope this will be helpful for others.", "body": "I obey @wjiangcmu 's advice, it works.\r\nthe code:\r\n33         self.is_training = tf.placeholder(tf.bool, name='MODE')\r\n// first use:\r\n 94         self.img_bn1 = tf.cond(self.is_training,\r\n 95                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = False),\r\n 96                                 lambda: batch_norm(self.img_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True))\r\n\r\n// add update_ops before second ruse, and filter out unrelated update_ops(unrelated moving mean and variance)\r\n126         update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n127         print('update_ops')\r\n128         for key in update_ops:\r\n129             print(key)\r\n131         i2t_update_extra_ops = [elem for elem in update_ops if 'text_feature/attention' not in elem.name]\r\n\r\n// second use:\r\n132         self.img_neg_bn1 = batch_norm(self.img_neg_fc1, is_training=self.is_training, center=True, scale=True, activation_fn=None, decay=0.9, scope='discriminator/img_bn1', reuse = True)\r\n\r\n// weight update and dependent extra_ops(moving mean and variance)\r\n242         self.i2t_optimizer = tf.train.GradientDescentOptimizer(learning_rate )\r\n243         i2t_update_grads = self.i2t_optimizer.minimize(self.i2t_loss)\r\n244 \r\n245         i2t_train_ops = [i2t_update_grads] + i2t_update_extra_ops\r\n246         self.i2t_updates = tf.group(*i2t_train_ops)\r\n\r\nin addition,  in order to update each batch_norm only once, according to @bsautermeister 's \"UPDATE_OPS in combination with reuse varscope\", I add the update_ops before the second use each batch_norm, and filter out unrelated update_ops.\r\n\r\nHope this will be helpful for others."}