{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4361", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4361/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4361/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4361/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4361", "id": 176718221, "node_id": "MDU6SXNzdWUxNzY3MTgyMjE=", "number": 4361, "title": "Update tf.contrib.layers.batch_norm() docs", "user": {"login": "bsautermeister", "id": 2537736, "node_id": "MDQ6VXNlcjI1Mzc3MzY=", "avatar_url": "https://avatars2.githubusercontent.com/u/2537736?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bsautermeister", "html_url": "https://github.com/bsautermeister", "followers_url": "https://api.github.com/users/bsautermeister/followers", "following_url": "https://api.github.com/users/bsautermeister/following{/other_user}", "gists_url": "https://api.github.com/users/bsautermeister/gists{/gist_id}", "starred_url": "https://api.github.com/users/bsautermeister/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bsautermeister/subscriptions", "organizations_url": "https://api.github.com/users/bsautermeister/orgs", "repos_url": "https://api.github.com/users/bsautermeister/repos", "events_url": "https://api.github.com/users/bsautermeister/events{/privacy}", "received_events_url": "https://api.github.com/users/bsautermeister/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2016-09-13T18:43:34Z", "updated_at": "2018-06-07T11:23:14Z", "closed_at": "2017-02-13T17:42:10Z", "author_association": "NONE", "body_html": "<p><em>Tensorflow version that I use : 0.10 (pip package)</em></p>\n<hr>\n<p>I took heavy use of <em><a href=\"https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100\">tf.contrib.layers.batch_norm()</a></em> the last weeks.</p>\n<p>After facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:</p>\n<ul>\n<li><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"133980206\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1122\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1122/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1122\">#1122</a></li>\n<li><a href=\"http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\" rel=\"nofollow\">http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow</a></li>\n</ul>\n<p>I would suggest to do following improvements to make it more clear:</p>\n<p><strong>1) Update example in doc-string:</strong></p>\n<p>The example tells in case we use <em>update_collections</em> on its defaults, we have to include this:</p>\n<pre><code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.group(update_ops)\n    total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n</code></pre>\n<p>But this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:</p>\n<pre><code>from tensorflow.python import control_flow_ops\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.tuple(update_ops)\n    total_loss = control_flow_ops.with_dependencies(updates, total_loss)\n</code></pre>\n<p>As a side question, why do we apply it to the <em>total_loss</em>, and not to the train_op directly, as described in the doc-string text. Added a dependency to <em>total_loss</em> works, but grouping it with the <em>train_op</em> would make the example more clear in my opinion, because we do batch-statistic updates only during training.</p>\n<p><strong>2) <em>UPDATE_OPS</em> in combination with reuse varscope:</strong></p>\n<p>This is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to <em>UPDATE_OPS</em> nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?<br>\nOr is it required to filter the update-ops after collecting them with <code>update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)</code>, so that each one is executed just once?</p>\n<p>To sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:</p>\n<pre><code>if not reuse:\n    # Collect the updates to be computed later.\n    ops.add_to_collections(updates_collections, update_moving_mean)\n    ops.add_to_collections(updates_collections, update_moving_variance)\n</code></pre>\n<p>In my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling <code>tf.get_collection(tf.GraphKeys.UPDATE_OPS)</code>. As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.</p>\n<p><strong>3) Handling of <em>is_training</em> parameter:</strong></p>\n<p>I have seen a lot of examples people doing something like this in their code to handle the <em>is_training</em> parameter:</p>\n<pre><code>def batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=True)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=False)\n    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return bn\n</code></pre>\n<p>As far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.</p>\n<p><strong>4) Usage on Multi-GPU configuration</strong></p>\n<p>a) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).</p>\n<p>b) When I use <em>tf.contrib.batch_norm()</em> within a multi-GPU system, I get an error like this:</p>\n<pre><code>InvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': \nCould not satisfy explicit device specification '/device:GPU:1' because no supported kernel \nfor GPU devices is available.\n...\n</code></pre>\n<p>Hence, to we have to wrap evey <em>batch_norm()</em> call with <em>tf.device(\"/cpu:0\")</em>? I guess this might have bad impacts on performance, right?</p>\n<p>Thanks!</p>\n<p><em>PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know...</em></p>", "body_text": "Tensorflow version that I use : 0.10 (pip package)\n\nI took heavy use of tf.contrib.layers.batch_norm() the last weeks.\nAfter facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:\n\n#1122\nhttp://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n\nI would suggest to do following improvements to make it more clear:\n1) Update example in doc-string:\nThe example tells in case we use update_collections on its defaults, we have to include this:\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.group(update_ops)\n    total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n\nBut this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:\nfrom tensorflow.python import control_flow_ops\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.tuple(update_ops)\n    total_loss = control_flow_ops.with_dependencies(updates, total_loss)\n\nAs a side question, why do we apply it to the total_loss, and not to the train_op directly, as described in the doc-string text. Added a dependency to total_loss works, but grouping it with the train_op would make the example more clear in my opinion, because we do batch-statistic updates only during training.\n2) UPDATE_OPS in combination with reuse varscope:\nThis is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to UPDATE_OPS nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?\nOr is it required to filter the update-ops after collecting them with update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS), so that each one is executed just once?\nTo sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:\nif not reuse:\n    # Collect the updates to be computed later.\n    ops.add_to_collections(updates_collections, update_moving_mean)\n    ops.add_to_collections(updates_collections, update_moving_variance)\n\nIn my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling tf.get_collection(tf.GraphKeys.UPDATE_OPS). As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.\n3) Handling of is_training parameter:\nI have seen a lot of examples people doing something like this in their code to handle the is_training parameter:\ndef batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=True)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=False)\n    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return bn\n\nAs far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.\n4) Usage on Multi-GPU configuration\na) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).\nb) When I use tf.contrib.batch_norm() within a multi-GPU system, I get an error like this:\nInvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': \nCould not satisfy explicit device specification '/device:GPU:1' because no supported kernel \nfor GPU devices is available.\n...\n\nHence, to we have to wrap evey batch_norm() call with tf.device(\"/cpu:0\")? I guess this might have bad impacts on performance, right?\nThanks!\nPS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know...", "body": "_Tensorflow version that I use : 0.10 (pip package)_\n\n---\n\nI took heavy use of _[tf.contrib.layers.batch_norm()](https://github.com/tensorflow/tensorflow/blob/b826b79718e3e93148c3545e7aa3f90891744cc0/tensorflow/contrib/layers/python/layers/layers.py#L100)_ the last weeks. \n\nAfter facing some problems on how to use it correctly, I figured out that there are many devs out there who are confused as well, such as here:\n- https://github.com/tensorflow/tensorflow/issues/1122\n- http://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow\n\nI would suggest to do following improvements to make it more clear:\n\n**1) Update example in doc-string:**\n\nThe example tells in case we use _update_collections_ on its defaults, we have to include this:\n\n```\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.group(update_ops)\n    total_loss = control_flow_ops.with_dependencies([updates], total_loss)\n```\n\nBut this is actually not working or deprecated, as it throws errors. Instead, we have to do some tiny changes. I would suggest to update the docs as follows:\n\n```\nfrom tensorflow.python import control_flow_ops\n\nupdate_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\nif update_ops:\n    updates = tf.tuple(update_ops)\n    total_loss = control_flow_ops.with_dependencies(updates, total_loss)\n```\n\nAs a side question, why do we apply it to the _total_loss_, and not to the train_op directly, as described in the doc-string text. Added a dependency to _total_loss_ works, but grouping it with the _train_op_ would make the example more clear in my opinion, because we do batch-statistic updates only during training.\n\n**2) _UPDATE_OPS_ in combination with reuse varscope:**\n\nThis is related to the question above. Let's say we have a model with which reuses an convolutional encoder (and also its batch-norm-layers) several times. Even when we reuse these layers, the update operation for the batch-statistics is added to _UPDATE_OPS_ nevertheless. Personally, I'm not sure if this is a bug, or if this is really what should be done?\nOr is it required to filter the update-ops after collecting them with `update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)`, so that each one is executed just once?\n\nTo sum this up: Am I wrong that lines 213-215 should not be executed when reuse=True? So changing it to:\n\n```\nif not reuse:\n    # Collect the updates to be computed later.\n    ops.add_to_collections(updates_collections, update_moving_mean)\n    ops.add_to_collections(updates_collections, update_moving_variance)\n```\n\nIn my case, I'm using a Conv-LSTM-Conv_tp architecture, where I reuse the Conv/Conv_tp for each timestep. When I increase the number of timesteps in the LSTM, the number of update-ops increases in proportionally, while the number of model-parameters stays constant because they get reused. Currently, I'm getting 420 update-ops when calling `tf.get_collection(tf.GraphKeys.UPDATE_OPS)`. As the performance feels super slow when I use batch-norm, I guess this high number of update-ops cannot be right.\n\n**3) Handling of _is_training_ parameter:**\n\nI have seen a lot of examples people doing something like this in their code to handle the _is_training_ parameter:\n\n```\ndef batch_norm_layer(x,train_phase,scope_bn):\n    bn_train = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=True)\n    bn_inference = batch_norm(x, decay=0.999, center=True, scale=True,\n    updates_collections=None,\n    is_training=False)\n    bn = tf.cond(train_phase, lambda: bn_train, lambda: bn_inference)\n    return bn\n```\n\nAs far as I know, this was really required in the past, because is_training was just a Boolean. But since the param can be a Bool-Tensor as well, this is not required anymore. Since many devs are still ding this workaound, added a comment to the doc-string that this is not required anymore could be helpful.\n\n**4) Usage on Multi-GPU configuration**\n\na) When I optimize my code for multi-GPU systems (as in the CIFAR10 example) the number of update-ops increases with the factor of num_gpus (might be related to 2) ).\n\nb) When I use _tf.contrib.batch_norm()_ within a multi-GPU system, I get an error like this:\n\n```\nInvalidArgumentError: Cannot assign a device to node 'tower_1/inference/ConvStack/x_bn_9/moments/sufficient_statistics/SparseToDense': \nCould not satisfy explicit device specification '/device:GPU:1' because no supported kernel \nfor GPU devices is available.\n...\n```\n\nHence, to we have to wrap evey _batch_norm()_ call with _tf.device(\"/cpu:0\")_? I guess this might have bad impacts on performance, right?\n\nThanks!\n\n_PS: Sorry in case this question would fits better to StackOverflow. As it is a combination of suggested improvements and questions. Just let me know..._\n"}