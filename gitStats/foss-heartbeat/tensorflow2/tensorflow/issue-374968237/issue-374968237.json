{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23348", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23348/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23348/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23348/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23348", "id": 374968237, "node_id": "MDU6SXNzdWUzNzQ5NjgyMzc=", "number": 23348, "title": "CPU memory slowly filling, during inference, cannot seem to find the reason", "user": {"login": "StevenPuttemans", "id": 4621239, "node_id": "MDQ6VXNlcjQ2MjEyMzk=", "avatar_url": "https://avatars0.githubusercontent.com/u/4621239?v=4", "gravatar_id": "", "url": "https://api.github.com/users/StevenPuttemans", "html_url": "https://github.com/StevenPuttemans", "followers_url": "https://api.github.com/users/StevenPuttemans/followers", "following_url": "https://api.github.com/users/StevenPuttemans/following{/other_user}", "gists_url": "https://api.github.com/users/StevenPuttemans/gists{/gist_id}", "starred_url": "https://api.github.com/users/StevenPuttemans/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/StevenPuttemans/subscriptions", "organizations_url": "https://api.github.com/users/StevenPuttemans/orgs", "repos_url": "https://api.github.com/users/StevenPuttemans/repos", "events_url": "https://api.github.com/users/StevenPuttemans/events{/privacy}", "received_events_url": "https://api.github.com/users/StevenPuttemans/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "azaks2", "id": 40365382, "node_id": "MDQ6VXNlcjQwMzY1Mzgy", "avatar_url": "https://avatars2.githubusercontent.com/u/40365382?v=4", "gravatar_id": "", "url": "https://api.github.com/users/azaks2", "html_url": "https://github.com/azaks2", "followers_url": "https://api.github.com/users/azaks2/followers", "following_url": "https://api.github.com/users/azaks2/following{/other_user}", "gists_url": "https://api.github.com/users/azaks2/gists{/gist_id}", "starred_url": "https://api.github.com/users/azaks2/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/azaks2/subscriptions", "organizations_url": "https://api.github.com/users/azaks2/orgs", "repos_url": "https://api.github.com/users/azaks2/repos", "events_url": "https://api.github.com/users/azaks2/events{/privacy}", "received_events_url": "https://api.github.com/users/azaks2/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-10-29T11:19:04Z", "updated_at": "2018-11-12T16:30:49Z", "closed_at": null, "author_association": "NONE", "body_html": "<p><strong>System information</strong></p>\n<ul>\n<li>We have made a specific adaptation of a inference script, that reads in tensorflow records, pushes the data through the network and then stores the result.</li>\n<li>Linux Ubuntu 18.04</li>\n<li>TensorFlow installed from (source or binary): default Ubuntu repositories</li>\n<li>TensorFlow version (use command below): 1.3</li>\n<li>Python version: 2.7</li>\n<li>CUDA/cuDNN version: Cuda 8.0 CuDNN 6.0</li>\n<li>GPU model and memory: NVIDIA P5000 - 16GB</li>\n</ul>\n<p><strong>Describe the current behavior</strong></p>\n<p>The script runs slow but fine. However, CPU memory of the system is slowly clogging up and system will fail.</p>\n<p><strong>Describe the expected behavior</strong></p>\n<p>We expected the code to run through all data parts without issues.</p>\n<p><strong>Code to reproduce the issue</strong></p>\n<pre><code>\"\"\"Applies model to all examples in a train record.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport constants\nimport model\nimport util\n\n_RESOLUTIONS = ['5cm', '9cm', '19cm']\n_INPUT_FEATURE = 'input_sdf'\n_TARGET_FEATURE = 'target_df'\n_TARGET_SEM_FEATURE = 'target_sem'\n_TRAIN_FEATURE = 'samples'\n_TRAIN_SEM_FEATURE = 'samples_sem'\n\nflags = tf.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('base_dir', '',\n                    'Root directory. Expects a directory containing the model.')\nflags.DEFINE_string('model_checkpoint', '',\n                    'Model checkpoint to use (empty for latest).')\nflags.DEFINE_string('data_filepattern', '/tmp/data/train_*.tfrecords',\n                    'Training data file pattern.')\nflags.DEFINE_string('output_folder', FLAGS.base_dir + '/data/vox5-9-19-dim3D_edited/',\n                    'Folder in which to put the new tfrecords.')\n\n# Parameters for applying model.\nflags.DEFINE_integer('height_input', 64, 'Input block y dim.')\nflags.DEFINE_integer('hierarchy_level', 1, 'Hierachy level (1: finest level).')\nflags.DEFINE_bool('is_base_level', True, 'If base level of hierarchy.')\nflags.DEFINE_integer('num_quant_levels', 256, 'Number of quantization bins.')\nflags.DEFINE_integer('num_total_hierarchy_levels', 1,\n                     'Number of total hierarchy levels.')\nflags.DEFINE_integer('stored_dim_block', 64,\n                     'Stored data block x/z dim, high-resolution.')\nflags.DEFINE_integer('stored_height_block', 64,\n                     'Stored data block y dim, high-resolution.')\nflags.DEFINE_integer('p_norm', 1, 'P-norm loss (0 to disable).')\nflags.DEFINE_bool('predict_semantics', True,\n                  'Also predict semantic labels per-voxel.')\nflags.DEFINE_float('temperature', 100.0, 'Softmax temperature for sampling.')\n\n\ndef read_record(filename):\n    for serialized_example in tf.python_io.tf_record_iterator(filename):\n\tsamples_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\n                               FLAGS.stored_dim_block // 2, 2])\n        samples_sem_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\n                                   FLAGS.stored_dim_block // 2], dtype=np.uint8)\n        if not FLAGS.is_base_level:\n            key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]\n            key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]\n            spec = {\n                'data':\n                    tf.FixedLenFeature((), tf.string),\n                key_samples:\n                    tf.FixedLenFeature((FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\n                                        FLAGS.stored_dim_block // 2, 1), tf.float32),\n                key_samples_sem:\n                    tf.FixedLenFeature((), tf.string)\n            }\n            example = tf.parse_single_example(serialized_example, spec)\n            samples_lo = example[key_samples]\n            samples_sem_lo = example[key_samples_sem]\n            samples_sem_lo = tf.decode_raw(samples_sem_lo, tf.uint8)\n            samples_sem_lo = tf.reshape(samples_sem_lo, [\n                FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2, FLAGS.stored_dim_block // 2])\n\n            serialized_example = example['data']\n\t\n        # Parse sequence example.\n        key_input = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _INPUT_FEATURE\n        key_target = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_FEATURE\n        key_target_sem = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_SEM_FEATURE\n\t\n        sequence_features_spec = {\n            key_input:\n                tf.FixedLenFeature(\n                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),\n                    tf.float32),\n            key_target:\n                tf.FixedLenFeature(\n                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),\n                    tf.float32),\n            key_target_sem:\n                tf.FixedLenFeature((), tf.string)\n        }\n\tprint \"did it\"\n        example = tf.parse_single_example(serialized_example, sequence_features_spec)\n\tprint \"did it\"\n        input_sdf_tensor = example[key_input]\n\t\n        input_sdf = input_sdf_tensor.eval()\n        input_sdf = np.squeeze(input_sdf, axis = 3)\n        input_sdf = util.preprocess_sdf(input_sdf, constants.TRUNCATION)\n\n        yield input_sdf, samples_lo, samples_sem_lo, serialized_example\n\n\ndef predict_from_model(logit_groups_geometry, logit_groups_semantics,\n                       temperature):\n    \"\"\"Reconstruct predicted geometry and semantics from model output.\"\"\"\n    predictions_geometry_list = []\n    for logit_group in logit_groups_geometry:\n        if FLAGS.p_norm &gt; 0:\n            predictions_geometry_list.append(logit_group[:, :, :, :, 0])\n        else:\n            logit_group_shape = logit_group.shape_as_list()\n            logit_group = tf.reshape(logit_group, [-1, logit_group_shape[-1]])\n            samples = tf.multinomial(temperature * logit_group, 1)\n            predictions_geometry_list.append(\n                tf.reshape(samples, logit_group_shape[:-1]))\n    predictions_semantics_list = []\n    if FLAGS.predict_semantics:\n        for logit_group in logit_groups_semantics:\n            predictions_semantics_list.append(tf.argmax(logit_group, 4))\n    else:\n        predictions_semantics_list = [\n                                         tf.zeros(shape=predictions_geometry_list[0].shape, dtype=tf.uint8)\n                                     ] * len(predictions_geometry_list)\n    return predictions_geometry_list, predictions_semantics_list\n\n\ndef create_dfs_from_output(output_df):\n    \"\"\"Rescales model output to distance fields (in voxel units).\"\"\"\n    if FLAGS.p_norm &gt; 0:\n        output_df = constants.TRUNCATION * (output_df[0, :, :, :, 0] + 1)\n    else:\n        output_df = (output_df[0, :, :, :, 0] + 1) * 0.5 * (\n            FLAGS.num_quant_levels - 1)\n        output_df = util.dequantize(output_df, FLAGS.num_quant_levels,\n                                    constants.TRUNCATION)\n    return output_df\n\n\ndef create_model(scene_dim_x, scene_dim_y, scene_dim_z):\n    \"\"\"Init model graph for scene.\"\"\"\n    input_placeholder = tf.placeholder(\n        tf.float32,\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],\n        name='pl_scan')\n    target_placeholder = tf.placeholder(\n        tf.float32,\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],\n        name='pl_target')\n    target_lo_placeholder = tf.placeholder(\n        tf.float32,\n        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2, 2],\n        name='pl_target_lo')\n    target_sem_placeholder = tf.placeholder(\n        tf.uint8,\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x],\n        name='pl_target_sem')\n    target_sem_lo_placeholder = tf.placeholder(\n        tf.uint8,\n        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2],\n        name='pl_target_sem_lo')\n    # No previous level input if at base level.\n    if FLAGS.is_base_level:\n        target_scan_low_resolution = None\n        target_semantics_low_resolution = None\n    else:\n        target_scan_low_resolution = target_lo_placeholder\n        target_semantics_low_resolution = target_sem_lo_placeholder\n    logits = model.model(\n        input_scan=input_placeholder,\n        target_scan_low_resolution=target_scan_low_resolution,\n        target_scan=target_placeholder,\n        target_semantics_low_resolution=target_semantics_low_resolution,\n        target_semantics=target_sem_placeholder,\n        num_quant_levels=FLAGS.num_quant_levels,\n        predict_semantics=FLAGS.predict_semantics,\n        use_p_norm=FLAGS.p_norm &gt; 0)\n    return (input_placeholder, target_placeholder, target_lo_placeholder,\n            target_sem_placeholder, target_sem_lo_placeholder, logits)\n\n\ndef main(_):\n    model_path = FLAGS.base_dir\n    output_folder = FLAGS.output_folder\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    if FLAGS.model_checkpoint:\n        checkpoint_path = os.path.join(model_path, FLAGS.model_checkpoint)\n        print \"Used model: \" + str(checkpoint_path)\n    else:\n        checkpoint_path = tf.train.latest_checkpoint(model_path)\n        print \"Used model: \" + str(checkpoint_path)\t\n\n    # Loop over all tfrecords in training data\n    data_filepattern = FLAGS.data_filepattern\n    if not isinstance(data_filepattern, list):\n        data_filepattern = [data_filepattern]\n    # Get filenames matching filespec.\n    tf.logging.info('data_filepattern: %s', data_filepattern)\n    filenames = []\n    for p in data_filepattern:\n        filenames.extend(tf.gfile.Glob(p))\n    tf.logging.info('filenames: %s', filenames)\n\n    for filename in filenames:\n        tf.reset_default_graph()\n\n        # Init model.\n        (input_placeholder, target_placeholder, target_lo_placeholder,\n         target_sem_placeholder, target_sem_lo_placeholder, logits) = create_model(\n            FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block)\n        logit_groups_geometry = logits['logits_geometry']\n        logit_groups_semantics = logits['logits_semantics']\n        feature_groups = logits['features']\n\n        predictions_geometry_list, predictions_semantics_list = predict_from_model(\n            logit_groups_geometry, logit_groups_semantics, FLAGS.temperature)\n\n\n        config = tf.ConfigProto(device_count={'GPU': 0})\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n\n        with tf.Session(config=config) as session:\n            session.run(init_op)\n\t    \n            assign_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n                checkpoint_path, tf.contrib.framework.get_variables_to_restore())\n            assign_fn(session)\n   \n            filename_out = os.path.join(output_folder, os.path.basename(filename))\n            print(filename_out)\n            writer = tf.python_io.TFRecordWriter(filename_out)\n\n            for input_sdf, samples_lo, samples_sem_lo, serialized_example in read_record(filename):             \n\n\t\t# Make batch size 1 to input to model.\n                input_sdf = input_sdf[np.newaxis, :, :, :, :]\n                samples_lo = samples_lo[np.newaxis, :, :, :, :]\n                samples_sem_lo = samples_sem_lo[np.newaxis, :, :, :]\n                output_df = np.ones(shape=input_sdf.shape)\n                output_df[:, :, :, :, 0] *= constants.TRUNCATION  # Fill with truncation, known values.\n                output_sem = np.zeros(\n                    shape=[1, FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block],\n                    dtype=np.uint8)\n\n                # First get features.\n                feed_dict = {\n                    input_placeholder: input_sdf,\n                    target_lo_placeholder: samples_lo,\n                    target_placeholder: output_df,\n                    target_sem_lo_placeholder: samples_sem_lo,\n                    target_sem_placeholder: output_sem\n                }\n\t\t\n                # Cache these features.\n                feature_groups_ = session.run(feature_groups, feed_dict)\n\n                for n in range(8):\n                    tf.logging.info('Predicting group [%d/%d]', n + 1, 8)\n                    # Predict\n                    feed_dict[feature_groups[n]] = feature_groups_[n]\n                    predictions = session.run(\n                        {\n                            'prediction_geometry': predictions_geometry_list[n],\n                            'prediction_semantics': predictions_semantics_list[n]\n                        },\n                        feed_dict=feed_dict)\n                    prediction_geometry = predictions['prediction_geometry']\n                    prediction_semantics = predictions['prediction_semantics']\n                    # Put into [-1,1] for next group.\n                    if FLAGS.p_norm == 0:\n                        prediction_geometry = prediction_geometry.astype(np.float32) / (\n                            (FLAGS.num_quant_levels - 1) / 2.0) - 1.0\n\n                    util.assign_voxel_group(output_df, prediction_geometry,\n                                            n + 1)\n                    if FLAGS.predict_semantics:\n                        util.assign_voxel_group(output_sem,\n                                                prediction_semantics, n + 1)\n\n                # Final outputs.\n                output_sem = output_sem[0]\n                output_df = create_dfs_from_output(output_df)  # Fill with truncation, known values.\n\n                # Export predictions\n                key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]\n                key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]\n                out_feature = {\n                    'data': util.bytes_feature(serialized_example),\n                    key_samples: util.float_feature(output_df.flatten().tolist()),\n                    key_samples_sem: util.bytes_feature(output_sem.flatten().tobytes())\n                }\n                example = tf.train.Example(features=tf.train.Features(feature=out_feature))\n                writer.write(example.SerializeToString())\n        session.close()\n\nif __name__ == '__main__':\n    tf.app.run(main)\n</code></pre>\n<p>There are no errors reported. Any help welcome.</p>", "body_text": "System information\n\nWe have made a specific adaptation of a inference script, that reads in tensorflow records, pushes the data through the network and then stores the result.\nLinux Ubuntu 18.04\nTensorFlow installed from (source or binary): default Ubuntu repositories\nTensorFlow version (use command below): 1.3\nPython version: 2.7\nCUDA/cuDNN version: Cuda 8.0 CuDNN 6.0\nGPU model and memory: NVIDIA P5000 - 16GB\n\nDescribe the current behavior\nThe script runs slow but fine. However, CPU memory of the system is slowly clogging up and system will fail.\nDescribe the expected behavior\nWe expected the code to run through all data parts without issues.\nCode to reproduce the issue\n\"\"\"Applies model to all examples in a train record.\n\n\"\"\"\n\nimport os\nimport numpy as np\nimport tensorflow as tf\n\nimport constants\nimport model\nimport util\n\n_RESOLUTIONS = ['5cm', '9cm', '19cm']\n_INPUT_FEATURE = 'input_sdf'\n_TARGET_FEATURE = 'target_df'\n_TARGET_SEM_FEATURE = 'target_sem'\n_TRAIN_FEATURE = 'samples'\n_TRAIN_SEM_FEATURE = 'samples_sem'\n\nflags = tf.flags\nFLAGS = flags.FLAGS\nflags.DEFINE_string('base_dir', '',\n                    'Root directory. Expects a directory containing the model.')\nflags.DEFINE_string('model_checkpoint', '',\n                    'Model checkpoint to use (empty for latest).')\nflags.DEFINE_string('data_filepattern', '/tmp/data/train_*.tfrecords',\n                    'Training data file pattern.')\nflags.DEFINE_string('output_folder', FLAGS.base_dir + '/data/vox5-9-19-dim3D_edited/',\n                    'Folder in which to put the new tfrecords.')\n\n# Parameters for applying model.\nflags.DEFINE_integer('height_input', 64, 'Input block y dim.')\nflags.DEFINE_integer('hierarchy_level', 1, 'Hierachy level (1: finest level).')\nflags.DEFINE_bool('is_base_level', True, 'If base level of hierarchy.')\nflags.DEFINE_integer('num_quant_levels', 256, 'Number of quantization bins.')\nflags.DEFINE_integer('num_total_hierarchy_levels', 1,\n                     'Number of total hierarchy levels.')\nflags.DEFINE_integer('stored_dim_block', 64,\n                     'Stored data block x/z dim, high-resolution.')\nflags.DEFINE_integer('stored_height_block', 64,\n                     'Stored data block y dim, high-resolution.')\nflags.DEFINE_integer('p_norm', 1, 'P-norm loss (0 to disable).')\nflags.DEFINE_bool('predict_semantics', True,\n                  'Also predict semantic labels per-voxel.')\nflags.DEFINE_float('temperature', 100.0, 'Softmax temperature for sampling.')\n\n\ndef read_record(filename):\n    for serialized_example in tf.python_io.tf_record_iterator(filename):\n\tsamples_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\n                               FLAGS.stored_dim_block // 2, 2])\n        samples_sem_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\n                                   FLAGS.stored_dim_block // 2], dtype=np.uint8)\n        if not FLAGS.is_base_level:\n            key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]\n            key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]\n            spec = {\n                'data':\n                    tf.FixedLenFeature((), tf.string),\n                key_samples:\n                    tf.FixedLenFeature((FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\n                                        FLAGS.stored_dim_block // 2, 1), tf.float32),\n                key_samples_sem:\n                    tf.FixedLenFeature((), tf.string)\n            }\n            example = tf.parse_single_example(serialized_example, spec)\n            samples_lo = example[key_samples]\n            samples_sem_lo = example[key_samples_sem]\n            samples_sem_lo = tf.decode_raw(samples_sem_lo, tf.uint8)\n            samples_sem_lo = tf.reshape(samples_sem_lo, [\n                FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2, FLAGS.stored_dim_block // 2])\n\n            serialized_example = example['data']\n\t\n        # Parse sequence example.\n        key_input = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _INPUT_FEATURE\n        key_target = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_FEATURE\n        key_target_sem = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_SEM_FEATURE\n\t\n        sequence_features_spec = {\n            key_input:\n                tf.FixedLenFeature(\n                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),\n                    tf.float32),\n            key_target:\n                tf.FixedLenFeature(\n                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),\n                    tf.float32),\n            key_target_sem:\n                tf.FixedLenFeature((), tf.string)\n        }\n\tprint \"did it\"\n        example = tf.parse_single_example(serialized_example, sequence_features_spec)\n\tprint \"did it\"\n        input_sdf_tensor = example[key_input]\n\t\n        input_sdf = input_sdf_tensor.eval()\n        input_sdf = np.squeeze(input_sdf, axis = 3)\n        input_sdf = util.preprocess_sdf(input_sdf, constants.TRUNCATION)\n\n        yield input_sdf, samples_lo, samples_sem_lo, serialized_example\n\n\ndef predict_from_model(logit_groups_geometry, logit_groups_semantics,\n                       temperature):\n    \"\"\"Reconstruct predicted geometry and semantics from model output.\"\"\"\n    predictions_geometry_list = []\n    for logit_group in logit_groups_geometry:\n        if FLAGS.p_norm > 0:\n            predictions_geometry_list.append(logit_group[:, :, :, :, 0])\n        else:\n            logit_group_shape = logit_group.shape_as_list()\n            logit_group = tf.reshape(logit_group, [-1, logit_group_shape[-1]])\n            samples = tf.multinomial(temperature * logit_group, 1)\n            predictions_geometry_list.append(\n                tf.reshape(samples, logit_group_shape[:-1]))\n    predictions_semantics_list = []\n    if FLAGS.predict_semantics:\n        for logit_group in logit_groups_semantics:\n            predictions_semantics_list.append(tf.argmax(logit_group, 4))\n    else:\n        predictions_semantics_list = [\n                                         tf.zeros(shape=predictions_geometry_list[0].shape, dtype=tf.uint8)\n                                     ] * len(predictions_geometry_list)\n    return predictions_geometry_list, predictions_semantics_list\n\n\ndef create_dfs_from_output(output_df):\n    \"\"\"Rescales model output to distance fields (in voxel units).\"\"\"\n    if FLAGS.p_norm > 0:\n        output_df = constants.TRUNCATION * (output_df[0, :, :, :, 0] + 1)\n    else:\n        output_df = (output_df[0, :, :, :, 0] + 1) * 0.5 * (\n            FLAGS.num_quant_levels - 1)\n        output_df = util.dequantize(output_df, FLAGS.num_quant_levels,\n                                    constants.TRUNCATION)\n    return output_df\n\n\ndef create_model(scene_dim_x, scene_dim_y, scene_dim_z):\n    \"\"\"Init model graph for scene.\"\"\"\n    input_placeholder = tf.placeholder(\n        tf.float32,\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],\n        name='pl_scan')\n    target_placeholder = tf.placeholder(\n        tf.float32,\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],\n        name='pl_target')\n    target_lo_placeholder = tf.placeholder(\n        tf.float32,\n        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2, 2],\n        name='pl_target_lo')\n    target_sem_placeholder = tf.placeholder(\n        tf.uint8,\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x],\n        name='pl_target_sem')\n    target_sem_lo_placeholder = tf.placeholder(\n        tf.uint8,\n        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2],\n        name='pl_target_sem_lo')\n    # No previous level input if at base level.\n    if FLAGS.is_base_level:\n        target_scan_low_resolution = None\n        target_semantics_low_resolution = None\n    else:\n        target_scan_low_resolution = target_lo_placeholder\n        target_semantics_low_resolution = target_sem_lo_placeholder\n    logits = model.model(\n        input_scan=input_placeholder,\n        target_scan_low_resolution=target_scan_low_resolution,\n        target_scan=target_placeholder,\n        target_semantics_low_resolution=target_semantics_low_resolution,\n        target_semantics=target_sem_placeholder,\n        num_quant_levels=FLAGS.num_quant_levels,\n        predict_semantics=FLAGS.predict_semantics,\n        use_p_norm=FLAGS.p_norm > 0)\n    return (input_placeholder, target_placeholder, target_lo_placeholder,\n            target_sem_placeholder, target_sem_lo_placeholder, logits)\n\n\ndef main(_):\n    model_path = FLAGS.base_dir\n    output_folder = FLAGS.output_folder\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    if FLAGS.model_checkpoint:\n        checkpoint_path = os.path.join(model_path, FLAGS.model_checkpoint)\n        print \"Used model: \" + str(checkpoint_path)\n    else:\n        checkpoint_path = tf.train.latest_checkpoint(model_path)\n        print \"Used model: \" + str(checkpoint_path)\t\n\n    # Loop over all tfrecords in training data\n    data_filepattern = FLAGS.data_filepattern\n    if not isinstance(data_filepattern, list):\n        data_filepattern = [data_filepattern]\n    # Get filenames matching filespec.\n    tf.logging.info('data_filepattern: %s', data_filepattern)\n    filenames = []\n    for p in data_filepattern:\n        filenames.extend(tf.gfile.Glob(p))\n    tf.logging.info('filenames: %s', filenames)\n\n    for filename in filenames:\n        tf.reset_default_graph()\n\n        # Init model.\n        (input_placeholder, target_placeholder, target_lo_placeholder,\n         target_sem_placeholder, target_sem_lo_placeholder, logits) = create_model(\n            FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block)\n        logit_groups_geometry = logits['logits_geometry']\n        logit_groups_semantics = logits['logits_semantics']\n        feature_groups = logits['features']\n\n        predictions_geometry_list, predictions_semantics_list = predict_from_model(\n            logit_groups_geometry, logit_groups_semantics, FLAGS.temperature)\n\n\n        config = tf.ConfigProto(device_count={'GPU': 0})\n        init_op = tf.group(tf.global_variables_initializer(),\n                           tf.local_variables_initializer())\n\n        with tf.Session(config=config) as session:\n            session.run(init_op)\n\t    \n            assign_fn = tf.contrib.framework.assign_from_checkpoint_fn(\n                checkpoint_path, tf.contrib.framework.get_variables_to_restore())\n            assign_fn(session)\n   \n            filename_out = os.path.join(output_folder, os.path.basename(filename))\n            print(filename_out)\n            writer = tf.python_io.TFRecordWriter(filename_out)\n\n            for input_sdf, samples_lo, samples_sem_lo, serialized_example in read_record(filename):             \n\n\t\t# Make batch size 1 to input to model.\n                input_sdf = input_sdf[np.newaxis, :, :, :, :]\n                samples_lo = samples_lo[np.newaxis, :, :, :, :]\n                samples_sem_lo = samples_sem_lo[np.newaxis, :, :, :]\n                output_df = np.ones(shape=input_sdf.shape)\n                output_df[:, :, :, :, 0] *= constants.TRUNCATION  # Fill with truncation, known values.\n                output_sem = np.zeros(\n                    shape=[1, FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block],\n                    dtype=np.uint8)\n\n                # First get features.\n                feed_dict = {\n                    input_placeholder: input_sdf,\n                    target_lo_placeholder: samples_lo,\n                    target_placeholder: output_df,\n                    target_sem_lo_placeholder: samples_sem_lo,\n                    target_sem_placeholder: output_sem\n                }\n\t\t\n                # Cache these features.\n                feature_groups_ = session.run(feature_groups, feed_dict)\n\n                for n in range(8):\n                    tf.logging.info('Predicting group [%d/%d]', n + 1, 8)\n                    # Predict\n                    feed_dict[feature_groups[n]] = feature_groups_[n]\n                    predictions = session.run(\n                        {\n                            'prediction_geometry': predictions_geometry_list[n],\n                            'prediction_semantics': predictions_semantics_list[n]\n                        },\n                        feed_dict=feed_dict)\n                    prediction_geometry = predictions['prediction_geometry']\n                    prediction_semantics = predictions['prediction_semantics']\n                    # Put into [-1,1] for next group.\n                    if FLAGS.p_norm == 0:\n                        prediction_geometry = prediction_geometry.astype(np.float32) / (\n                            (FLAGS.num_quant_levels - 1) / 2.0) - 1.0\n\n                    util.assign_voxel_group(output_df, prediction_geometry,\n                                            n + 1)\n                    if FLAGS.predict_semantics:\n                        util.assign_voxel_group(output_sem,\n                                                prediction_semantics, n + 1)\n\n                # Final outputs.\n                output_sem = output_sem[0]\n                output_df = create_dfs_from_output(output_df)  # Fill with truncation, known values.\n\n                # Export predictions\n                key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]\n                key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]\n                out_feature = {\n                    'data': util.bytes_feature(serialized_example),\n                    key_samples: util.float_feature(output_df.flatten().tolist()),\n                    key_samples_sem: util.bytes_feature(output_sem.flatten().tobytes())\n                }\n                example = tf.train.Example(features=tf.train.Features(feature=out_feature))\n                writer.write(example.SerializeToString())\n        session.close()\n\nif __name__ == '__main__':\n    tf.app.run(main)\n\nThere are no errors reported. Any help welcome.", "body": "**System information**\r\n- We have made a specific adaptation of a inference script, that reads in tensorflow records, pushes the data through the network and then stores the result.\r\n- Linux Ubuntu 18.04\r\n- TensorFlow installed from (source or binary): default Ubuntu repositories\r\n- TensorFlow version (use command below): 1.3\r\n- Python version: 2.7\r\n- CUDA/cuDNN version: Cuda 8.0 CuDNN 6.0\r\n- GPU model and memory: NVIDIA P5000 - 16GB\r\n\r\n**Describe the current behavior**\r\n\r\nThe script runs slow but fine. However, CPU memory of the system is slowly clogging up and system will fail.\r\n\r\n**Describe the expected behavior**\r\n\r\nWe expected the code to run through all data parts without issues.\r\n\r\n**Code to reproduce the issue**\r\n\r\n```\r\n\"\"\"Applies model to all examples in a train record.\r\n\r\n\"\"\"\r\n\r\nimport os\r\nimport numpy as np\r\nimport tensorflow as tf\r\n\r\nimport constants\r\nimport model\r\nimport util\r\n\r\n_RESOLUTIONS = ['5cm', '9cm', '19cm']\r\n_INPUT_FEATURE = 'input_sdf'\r\n_TARGET_FEATURE = 'target_df'\r\n_TARGET_SEM_FEATURE = 'target_sem'\r\n_TRAIN_FEATURE = 'samples'\r\n_TRAIN_SEM_FEATURE = 'samples_sem'\r\n\r\nflags = tf.flags\r\nFLAGS = flags.FLAGS\r\nflags.DEFINE_string('base_dir', '',\r\n                    'Root directory. Expects a directory containing the model.')\r\nflags.DEFINE_string('model_checkpoint', '',\r\n                    'Model checkpoint to use (empty for latest).')\r\nflags.DEFINE_string('data_filepattern', '/tmp/data/train_*.tfrecords',\r\n                    'Training data file pattern.')\r\nflags.DEFINE_string('output_folder', FLAGS.base_dir + '/data/vox5-9-19-dim3D_edited/',\r\n                    'Folder in which to put the new tfrecords.')\r\n\r\n# Parameters for applying model.\r\nflags.DEFINE_integer('height_input', 64, 'Input block y dim.')\r\nflags.DEFINE_integer('hierarchy_level', 1, 'Hierachy level (1: finest level).')\r\nflags.DEFINE_bool('is_base_level', True, 'If base level of hierarchy.')\r\nflags.DEFINE_integer('num_quant_levels', 256, 'Number of quantization bins.')\r\nflags.DEFINE_integer('num_total_hierarchy_levels', 1,\r\n                     'Number of total hierarchy levels.')\r\nflags.DEFINE_integer('stored_dim_block', 64,\r\n                     'Stored data block x/z dim, high-resolution.')\r\nflags.DEFINE_integer('stored_height_block', 64,\r\n                     'Stored data block y dim, high-resolution.')\r\nflags.DEFINE_integer('p_norm', 1, 'P-norm loss (0 to disable).')\r\nflags.DEFINE_bool('predict_semantics', True,\r\n                  'Also predict semantic labels per-voxel.')\r\nflags.DEFINE_float('temperature', 100.0, 'Softmax temperature for sampling.')\r\n\r\n\r\ndef read_record(filename):\r\n    for serialized_example in tf.python_io.tf_record_iterator(filename):\r\n\tsamples_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\r\n                               FLAGS.stored_dim_block // 2, 2])\r\n        samples_sem_lo = np.zeros([FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\r\n                                   FLAGS.stored_dim_block // 2], dtype=np.uint8)\r\n        if not FLAGS.is_base_level:\r\n            key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]\r\n            key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level]\r\n            spec = {\r\n                'data':\r\n                    tf.FixedLenFeature((), tf.string),\r\n                key_samples:\r\n                    tf.FixedLenFeature((FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2,\r\n                                        FLAGS.stored_dim_block // 2, 1), tf.float32),\r\n                key_samples_sem:\r\n                    tf.FixedLenFeature((), tf.string)\r\n            }\r\n            example = tf.parse_single_example(serialized_example, spec)\r\n            samples_lo = example[key_samples]\r\n            samples_sem_lo = example[key_samples_sem]\r\n            samples_sem_lo = tf.decode_raw(samples_sem_lo, tf.uint8)\r\n            samples_sem_lo = tf.reshape(samples_sem_lo, [\r\n                FLAGS.stored_dim_block // 2, FLAGS.stored_height_block // 2, FLAGS.stored_dim_block // 2])\r\n\r\n            serialized_example = example['data']\r\n\t\r\n        # Parse sequence example.\r\n        key_input = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _INPUT_FEATURE\r\n        key_target = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_FEATURE\r\n        key_target_sem = _RESOLUTIONS[FLAGS.hierarchy_level - 1] + '_' + _TARGET_SEM_FEATURE\r\n\t\r\n        sequence_features_spec = {\r\n            key_input:\r\n                tf.FixedLenFeature(\r\n                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),\r\n                    tf.float32),\r\n            key_target:\r\n                tf.FixedLenFeature(\r\n                    (FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block, 1),\r\n                    tf.float32),\r\n            key_target_sem:\r\n                tf.FixedLenFeature((), tf.string)\r\n        }\r\n\tprint \"did it\"\r\n        example = tf.parse_single_example(serialized_example, sequence_features_spec)\r\n\tprint \"did it\"\r\n        input_sdf_tensor = example[key_input]\r\n\t\r\n        input_sdf = input_sdf_tensor.eval()\r\n        input_sdf = np.squeeze(input_sdf, axis = 3)\r\n        input_sdf = util.preprocess_sdf(input_sdf, constants.TRUNCATION)\r\n\r\n        yield input_sdf, samples_lo, samples_sem_lo, serialized_example\r\n\r\n\r\ndef predict_from_model(logit_groups_geometry, logit_groups_semantics,\r\n                       temperature):\r\n    \"\"\"Reconstruct predicted geometry and semantics from model output.\"\"\"\r\n    predictions_geometry_list = []\r\n    for logit_group in logit_groups_geometry:\r\n        if FLAGS.p_norm > 0:\r\n            predictions_geometry_list.append(logit_group[:, :, :, :, 0])\r\n        else:\r\n            logit_group_shape = logit_group.shape_as_list()\r\n            logit_group = tf.reshape(logit_group, [-1, logit_group_shape[-1]])\r\n            samples = tf.multinomial(temperature * logit_group, 1)\r\n            predictions_geometry_list.append(\r\n                tf.reshape(samples, logit_group_shape[:-1]))\r\n    predictions_semantics_list = []\r\n    if FLAGS.predict_semantics:\r\n        for logit_group in logit_groups_semantics:\r\n            predictions_semantics_list.append(tf.argmax(logit_group, 4))\r\n    else:\r\n        predictions_semantics_list = [\r\n                                         tf.zeros(shape=predictions_geometry_list[0].shape, dtype=tf.uint8)\r\n                                     ] * len(predictions_geometry_list)\r\n    return predictions_geometry_list, predictions_semantics_list\r\n\r\n\r\ndef create_dfs_from_output(output_df):\r\n    \"\"\"Rescales model output to distance fields (in voxel units).\"\"\"\r\n    if FLAGS.p_norm > 0:\r\n        output_df = constants.TRUNCATION * (output_df[0, :, :, :, 0] + 1)\r\n    else:\r\n        output_df = (output_df[0, :, :, :, 0] + 1) * 0.5 * (\r\n            FLAGS.num_quant_levels - 1)\r\n        output_df = util.dequantize(output_df, FLAGS.num_quant_levels,\r\n                                    constants.TRUNCATION)\r\n    return output_df\r\n\r\n\r\ndef create_model(scene_dim_x, scene_dim_y, scene_dim_z):\r\n    \"\"\"Init model graph for scene.\"\"\"\r\n    input_placeholder = tf.placeholder(\r\n        tf.float32,\r\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],\r\n        name='pl_scan')\r\n    target_placeholder = tf.placeholder(\r\n        tf.float32,\r\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x, 2],\r\n        name='pl_target')\r\n    target_lo_placeholder = tf.placeholder(\r\n        tf.float32,\r\n        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2, 2],\r\n        name='pl_target_lo')\r\n    target_sem_placeholder = tf.placeholder(\r\n        tf.uint8,\r\n        shape=[1, scene_dim_z, scene_dim_y, scene_dim_x],\r\n        name='pl_target_sem')\r\n    target_sem_lo_placeholder = tf.placeholder(\r\n        tf.uint8,\r\n        shape=[1, scene_dim_z // 2, scene_dim_y // 2, scene_dim_x // 2],\r\n        name='pl_target_sem_lo')\r\n    # No previous level input if at base level.\r\n    if FLAGS.is_base_level:\r\n        target_scan_low_resolution = None\r\n        target_semantics_low_resolution = None\r\n    else:\r\n        target_scan_low_resolution = target_lo_placeholder\r\n        target_semantics_low_resolution = target_sem_lo_placeholder\r\n    logits = model.model(\r\n        input_scan=input_placeholder,\r\n        target_scan_low_resolution=target_scan_low_resolution,\r\n        target_scan=target_placeholder,\r\n        target_semantics_low_resolution=target_semantics_low_resolution,\r\n        target_semantics=target_sem_placeholder,\r\n        num_quant_levels=FLAGS.num_quant_levels,\r\n        predict_semantics=FLAGS.predict_semantics,\r\n        use_p_norm=FLAGS.p_norm > 0)\r\n    return (input_placeholder, target_placeholder, target_lo_placeholder,\r\n            target_sem_placeholder, target_sem_lo_placeholder, logits)\r\n\r\n\r\ndef main(_):\r\n    model_path = FLAGS.base_dir\r\n    output_folder = FLAGS.output_folder\r\n    if not os.path.exists(output_folder):\r\n        os.makedirs(output_folder)\r\n\r\n    if FLAGS.model_checkpoint:\r\n        checkpoint_path = os.path.join(model_path, FLAGS.model_checkpoint)\r\n        print \"Used model: \" + str(checkpoint_path)\r\n    else:\r\n        checkpoint_path = tf.train.latest_checkpoint(model_path)\r\n        print \"Used model: \" + str(checkpoint_path)\t\r\n\r\n    # Loop over all tfrecords in training data\r\n    data_filepattern = FLAGS.data_filepattern\r\n    if not isinstance(data_filepattern, list):\r\n        data_filepattern = [data_filepattern]\r\n    # Get filenames matching filespec.\r\n    tf.logging.info('data_filepattern: %s', data_filepattern)\r\n    filenames = []\r\n    for p in data_filepattern:\r\n        filenames.extend(tf.gfile.Glob(p))\r\n    tf.logging.info('filenames: %s', filenames)\r\n\r\n    for filename in filenames:\r\n        tf.reset_default_graph()\r\n\r\n        # Init model.\r\n        (input_placeholder, target_placeholder, target_lo_placeholder,\r\n         target_sem_placeholder, target_sem_lo_placeholder, logits) = create_model(\r\n            FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block)\r\n        logit_groups_geometry = logits['logits_geometry']\r\n        logit_groups_semantics = logits['logits_semantics']\r\n        feature_groups = logits['features']\r\n\r\n        predictions_geometry_list, predictions_semantics_list = predict_from_model(\r\n            logit_groups_geometry, logit_groups_semantics, FLAGS.temperature)\r\n\r\n\r\n        config = tf.ConfigProto(device_count={'GPU': 0})\r\n        init_op = tf.group(tf.global_variables_initializer(),\r\n                           tf.local_variables_initializer())\r\n\r\n        with tf.Session(config=config) as session:\r\n            session.run(init_op)\r\n\t    \r\n            assign_fn = tf.contrib.framework.assign_from_checkpoint_fn(\r\n                checkpoint_path, tf.contrib.framework.get_variables_to_restore())\r\n            assign_fn(session)\r\n   \r\n            filename_out = os.path.join(output_folder, os.path.basename(filename))\r\n            print(filename_out)\r\n            writer = tf.python_io.TFRecordWriter(filename_out)\r\n\r\n            for input_sdf, samples_lo, samples_sem_lo, serialized_example in read_record(filename):             \r\n\r\n\t\t# Make batch size 1 to input to model.\r\n                input_sdf = input_sdf[np.newaxis, :, :, :, :]\r\n                samples_lo = samples_lo[np.newaxis, :, :, :, :]\r\n                samples_sem_lo = samples_sem_lo[np.newaxis, :, :, :]\r\n                output_df = np.ones(shape=input_sdf.shape)\r\n                output_df[:, :, :, :, 0] *= constants.TRUNCATION  # Fill with truncation, known values.\r\n                output_sem = np.zeros(\r\n                    shape=[1, FLAGS.stored_dim_block, FLAGS.stored_height_block, FLAGS.stored_dim_block],\r\n                    dtype=np.uint8)\r\n\r\n                # First get features.\r\n                feed_dict = {\r\n                    input_placeholder: input_sdf,\r\n                    target_lo_placeholder: samples_lo,\r\n                    target_placeholder: output_df,\r\n                    target_sem_lo_placeholder: samples_sem_lo,\r\n                    target_sem_placeholder: output_sem\r\n                }\r\n\t\t\r\n                # Cache these features.\r\n                feature_groups_ = session.run(feature_groups, feed_dict)\r\n\r\n                for n in range(8):\r\n                    tf.logging.info('Predicting group [%d/%d]', n + 1, 8)\r\n                    # Predict\r\n                    feed_dict[feature_groups[n]] = feature_groups_[n]\r\n                    predictions = session.run(\r\n                        {\r\n                            'prediction_geometry': predictions_geometry_list[n],\r\n                            'prediction_semantics': predictions_semantics_list[n]\r\n                        },\r\n                        feed_dict=feed_dict)\r\n                    prediction_geometry = predictions['prediction_geometry']\r\n                    prediction_semantics = predictions['prediction_semantics']\r\n                    # Put into [-1,1] for next group.\r\n                    if FLAGS.p_norm == 0:\r\n                        prediction_geometry = prediction_geometry.astype(np.float32) / (\r\n                            (FLAGS.num_quant_levels - 1) / 2.0) - 1.0\r\n\r\n                    util.assign_voxel_group(output_df, prediction_geometry,\r\n                                            n + 1)\r\n                    if FLAGS.predict_semantics:\r\n                        util.assign_voxel_group(output_sem,\r\n                                                prediction_semantics, n + 1)\r\n\r\n                # Final outputs.\r\n                output_sem = output_sem[0]\r\n                output_df = create_dfs_from_output(output_df)  # Fill with truncation, known values.\r\n\r\n                # Export predictions\r\n                key_samples = 'samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]\r\n                key_samples_sem = 'sem_samples_' + _RESOLUTIONS[FLAGS.hierarchy_level - 1]\r\n                out_feature = {\r\n                    'data': util.bytes_feature(serialized_example),\r\n                    key_samples: util.float_feature(output_df.flatten().tolist()),\r\n                    key_samples_sem: util.bytes_feature(output_sem.flatten().tobytes())\r\n                }\r\n                example = tf.train.Example(features=tf.train.Features(feature=out_feature))\r\n                writer.write(example.SerializeToString())\r\n        session.close()\r\n\r\nif __name__ == '__main__':\r\n    tf.app.run(main)\r\n```\r\n\r\nThere are no errors reported. Any help welcome."}