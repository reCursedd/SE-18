{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/368507769", "html_url": "https://github.com/tensorflow/tensorflow/issues/17274#issuecomment-368507769", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17274", "id": 368507769, "node_id": "MDEyOklzc3VlQ29tbWVudDM2ODUwNzc2OQ==", "user": {"login": "lissyx", "id": 1645737, "node_id": "MDQ6VXNlcjE2NDU3Mzc=", "avatar_url": "https://avatars0.githubusercontent.com/u/1645737?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lissyx", "html_url": "https://github.com/lissyx", "followers_url": "https://api.github.com/users/lissyx/followers", "following_url": "https://api.github.com/users/lissyx/following{/other_user}", "gists_url": "https://api.github.com/users/lissyx/gists{/gist_id}", "starred_url": "https://api.github.com/users/lissyx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lissyx/subscriptions", "organizations_url": "https://api.github.com/users/lissyx/orgs", "repos_url": "https://api.github.com/users/lissyx/repos", "events_url": "https://api.github.com/users/lissyx/events{/privacy}", "received_events_url": "https://api.github.com/users/lissyx/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-26T13:43:53Z", "updated_at": "2018-02-26T13:43:53Z", "author_association": "CONTRIBUTOR", "body_html": "<pre><code>#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.tools import freeze_graph\n\nimport re\nimport os\nimport sys\n\nif len(sys.argv) != 2:\n    print(\"%s: graphName\" % (sys.argv[0]))\n    print(\"will produce: graphName.{pb,pbtxt} and graphName.dot\")\n    print(\"got:\", sys.argv)\n    sys.exit(1)\n\ngraph_pb   = os.path.abspath(\"%s.pb\" % sys.argv[1])\ngraph_ckpt = os.path.abspath(\"%s.ckpt\" % sys.argv[1])\ntemp_pb    = os.path.abspath(\"%s.non-frozen.pb\" % sys.argv[1])\nconfig_pb  = os.path.abspath(\"%s.pbtxt\" % sys.argv[1])\ngraph_dot  = os.path.abspath(\"%s.dot\" % sys.argv[1])\n\nuse_rnn = False\n\ng = tf.Graph()\nwith g.as_default() as g:\n    n_input    = 26\n    n_context  = 9\n    n_cell_dim = 2\n    n_hidden_3 = 2 * n_cell_dim\n\n    batch_x = tf.placeholder(tf.float32, [None, None, n_input + 2*n_input*n_context], name='garbage_batch_x')\n    seq_length = tf.placeholder(tf.int32, [None], name='garbage_seq_length')\n\n    batch_x_shape = tf.shape(batch_x, name='garbage_batch_x_shape')\n    batch_x = tf.transpose(batch_x, [1, 0, 2], name='garbage_batch_x_transpose')\n    batch_x = tf.reshape(batch_x, [-1, n_input + 2*n_input*n_context], name='garbage_batch_x_reshape')\n    layer_input = tf.reshape(batch_x, [-1, batch_x_shape[0], n_hidden_3], name='garbage_layer_input')\n\n    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n\n    rnn_fw_cell = tf.contrib.rnn.BasicRNNCell(n_cell_dim)\n    rnn_bw_cell = tf.contrib.rnn.BasicRNNCell(n_cell_dim)\n\n    outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_fw_cell if use_rnn else lstm_fw_cell,\n                                                             cell_bw=rnn_bw_cell if use_rnn else lstm_bw_cell,\n                                                             inputs=layer_input,\n                                                             dtype=tf.float32,\n                                                             time_major=True,\n                                                             sequence_length=None)\n\n    outputs = tf.concat(outputs, 2, name='garbage_outputs')\n\nsession_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\nwith tf.Session(graph=g, config=session_config) as sess:\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n\n    r = sess.run(init_op)\n    #sess.run(outputs)\n\n    tf.train.write_graph(g, os.path.dirname(temp_pb), os.path.basename(temp_pb), as_text=False)\n\n    # Save the variables to disk.\n    save_path = saver.save(sess, graph_ckpt)\n    print(\"Model saved in path: %s\" % save_path)\n\n    # Freeze graph\n    input_graph_path = temp_pb\n    input_saver_def_path = ''\n    input_binary = True\n    output_node_names = 'garbage_outputs'\n    restore_op_name = 'save/restore_all'\n    filename_tensor_name = 'save/Const:0'\n    output_graph_path = graph_pb\n    clear_devices = False\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n                               input_binary, save_path, output_node_names,\n                               restore_op_name, filename_tensor_name,\n                               output_graph_path, clear_devices, '')\n</code></pre>", "body_text": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function\n\nimport tensorflow as tf\nfrom tensorflow.python.tools import freeze_graph\n\nimport re\nimport os\nimport sys\n\nif len(sys.argv) != 2:\n    print(\"%s: graphName\" % (sys.argv[0]))\n    print(\"will produce: graphName.{pb,pbtxt} and graphName.dot\")\n    print(\"got:\", sys.argv)\n    sys.exit(1)\n\ngraph_pb   = os.path.abspath(\"%s.pb\" % sys.argv[1])\ngraph_ckpt = os.path.abspath(\"%s.ckpt\" % sys.argv[1])\ntemp_pb    = os.path.abspath(\"%s.non-frozen.pb\" % sys.argv[1])\nconfig_pb  = os.path.abspath(\"%s.pbtxt\" % sys.argv[1])\ngraph_dot  = os.path.abspath(\"%s.dot\" % sys.argv[1])\n\nuse_rnn = False\n\ng = tf.Graph()\nwith g.as_default() as g:\n    n_input    = 26\n    n_context  = 9\n    n_cell_dim = 2\n    n_hidden_3 = 2 * n_cell_dim\n\n    batch_x = tf.placeholder(tf.float32, [None, None, n_input + 2*n_input*n_context], name='garbage_batch_x')\n    seq_length = tf.placeholder(tf.int32, [None], name='garbage_seq_length')\n\n    batch_x_shape = tf.shape(batch_x, name='garbage_batch_x_shape')\n    batch_x = tf.transpose(batch_x, [1, 0, 2], name='garbage_batch_x_transpose')\n    batch_x = tf.reshape(batch_x, [-1, n_input + 2*n_input*n_context], name='garbage_batch_x_reshape')\n    layer_input = tf.reshape(batch_x, [-1, batch_x_shape[0], n_hidden_3], name='garbage_layer_input')\n\n    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\n\n    rnn_fw_cell = tf.contrib.rnn.BasicRNNCell(n_cell_dim)\n    rnn_bw_cell = tf.contrib.rnn.BasicRNNCell(n_cell_dim)\n\n    outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_fw_cell if use_rnn else lstm_fw_cell,\n                                                             cell_bw=rnn_bw_cell if use_rnn else lstm_bw_cell,\n                                                             inputs=layer_input,\n                                                             dtype=tf.float32,\n                                                             time_major=True,\n                                                             sequence_length=None)\n\n    outputs = tf.concat(outputs, 2, name='garbage_outputs')\n\nsession_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\n\nwith tf.Session(graph=g, config=session_config) as sess:\n    init_op = tf.global_variables_initializer()\n    saver = tf.train.Saver()\n\n    r = sess.run(init_op)\n    #sess.run(outputs)\n\n    tf.train.write_graph(g, os.path.dirname(temp_pb), os.path.basename(temp_pb), as_text=False)\n\n    # Save the variables to disk.\n    save_path = saver.save(sess, graph_ckpt)\n    print(\"Model saved in path: %s\" % save_path)\n\n    # Freeze graph\n    input_graph_path = temp_pb\n    input_saver_def_path = ''\n    input_binary = True\n    output_node_names = 'garbage_outputs'\n    restore_op_name = 'save/restore_all'\n    filename_tensor_name = 'save/Const:0'\n    output_graph_path = graph_pb\n    clear_devices = False\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\n                               input_binary, save_path, output_node_names,\n                               restore_op_name, filename_tensor_name,\n                               output_graph_path, clear_devices, '')", "body": "```\r\n#!/usr/bin/env python\r\n# -*- coding: utf-8 -*-\r\nfrom __future__ import absolute_import, division, print_function\r\n\r\nimport tensorflow as tf\r\nfrom tensorflow.python.tools import freeze_graph\r\n\r\nimport re\r\nimport os\r\nimport sys\r\n\r\nif len(sys.argv) != 2:\r\n    print(\"%s: graphName\" % (sys.argv[0]))\r\n    print(\"will produce: graphName.{pb,pbtxt} and graphName.dot\")\r\n    print(\"got:\", sys.argv)\r\n    sys.exit(1)\r\n\r\ngraph_pb   = os.path.abspath(\"%s.pb\" % sys.argv[1])\r\ngraph_ckpt = os.path.abspath(\"%s.ckpt\" % sys.argv[1])\r\ntemp_pb    = os.path.abspath(\"%s.non-frozen.pb\" % sys.argv[1])\r\nconfig_pb  = os.path.abspath(\"%s.pbtxt\" % sys.argv[1])\r\ngraph_dot  = os.path.abspath(\"%s.dot\" % sys.argv[1])\r\n\r\nuse_rnn = False\r\n\r\ng = tf.Graph()\r\nwith g.as_default() as g:\r\n    n_input    = 26\r\n    n_context  = 9\r\n    n_cell_dim = 2\r\n    n_hidden_3 = 2 * n_cell_dim\r\n\r\n    batch_x = tf.placeholder(tf.float32, [None, None, n_input + 2*n_input*n_context], name='garbage_batch_x')\r\n    seq_length = tf.placeholder(tf.int32, [None], name='garbage_seq_length')\r\n\r\n    batch_x_shape = tf.shape(batch_x, name='garbage_batch_x_shape')\r\n    batch_x = tf.transpose(batch_x, [1, 0, 2], name='garbage_batch_x_transpose')\r\n    batch_x = tf.reshape(batch_x, [-1, n_input + 2*n_input*n_context], name='garbage_batch_x_reshape')\r\n    layer_input = tf.reshape(batch_x, [-1, batch_x_shape[0], n_hidden_3], name='garbage_layer_input')\r\n\r\n    lstm_fw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\r\n    lstm_bw_cell = tf.contrib.rnn.BasicLSTMCell(n_cell_dim, forget_bias=1.0, state_is_tuple=True)\r\n\r\n    rnn_fw_cell = tf.contrib.rnn.BasicRNNCell(n_cell_dim)\r\n    rnn_bw_cell = tf.contrib.rnn.BasicRNNCell(n_cell_dim)\r\n\r\n    outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_fw_cell if use_rnn else lstm_fw_cell,\r\n                                                             cell_bw=rnn_bw_cell if use_rnn else lstm_bw_cell,\r\n                                                             inputs=layer_input,\r\n                                                             dtype=tf.float32,\r\n                                                             time_major=True,\r\n                                                             sequence_length=None)\r\n\r\n    outputs = tf.concat(outputs, 2, name='garbage_outputs')\r\n\r\nsession_config = tf.ConfigProto(allow_soft_placement=True, log_device_placement=False)\r\n\r\nwith tf.Session(graph=g, config=session_config) as sess:\r\n    init_op = tf.global_variables_initializer()\r\n    saver = tf.train.Saver()\r\n\r\n    r = sess.run(init_op)\r\n    #sess.run(outputs)\r\n\r\n    tf.train.write_graph(g, os.path.dirname(temp_pb), os.path.basename(temp_pb), as_text=False)\r\n\r\n    # Save the variables to disk.\r\n    save_path = saver.save(sess, graph_ckpt)\r\n    print(\"Model saved in path: %s\" % save_path)\r\n\r\n    # Freeze graph\r\n    input_graph_path = temp_pb\r\n    input_saver_def_path = ''\r\n    input_binary = True\r\n    output_node_names = 'garbage_outputs'\r\n    restore_op_name = 'save/restore_all'\r\n    filename_tensor_name = 'save/Const:0'\r\n    output_graph_path = graph_pb\r\n    clear_devices = False\r\n    freeze_graph.freeze_graph(input_graph_path, input_saver_def_path,\r\n                               input_binary, save_path, output_node_names,\r\n                               restore_op_name, filename_tensor_name,\r\n                               output_graph_path, clear_devices, '')\r\n```"}