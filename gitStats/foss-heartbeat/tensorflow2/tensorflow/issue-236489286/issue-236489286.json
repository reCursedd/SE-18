{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10765", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10765/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10765/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10765/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/10765", "id": 236489286, "node_id": "MDU6SXNzdWUyMzY0ODkyODY=", "number": 10765, "title": "[Performance] contirb.seq2seq.attention_wrapper slower due to using matmul instead of reduce_sum", "user": {"login": "chenghuige", "id": 6323467, "node_id": "MDQ6VXNlcjYzMjM0Njc=", "avatar_url": "https://avatars0.githubusercontent.com/u/6323467?v=4", "gravatar_id": "", "url": "https://api.github.com/users/chenghuige", "html_url": "https://github.com/chenghuige", "followers_url": "https://api.github.com/users/chenghuige/followers", "following_url": "https://api.github.com/users/chenghuige/following{/other_user}", "gists_url": "https://api.github.com/users/chenghuige/gists{/gist_id}", "starred_url": "https://api.github.com/users/chenghuige/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/chenghuige/subscriptions", "organizations_url": "https://api.github.com/users/chenghuige/orgs", "repos_url": "https://api.github.com/users/chenghuige/repos", "events_url": "https://api.github.com/users/chenghuige/events{/privacy}", "received_events_url": "https://api.github.com/users/chenghuige/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 2, "created_at": "2017-06-16T13:52:17Z", "updated_at": "2018-09-05T01:51:53Z", "closed_at": "2018-09-05T01:51:53Z", "author_association": "NONE", "body_html": "<p>tf version '1.2.0-rc0' contirb.seq2seq.attention_wrapper is great, it make using attention much easier.<br>\nHowever I found using attention_wrapper will be much slower then tf version 1.0.<br>\nAfter some experiment I found it is due to using matmul instead of reduce_sum.</p>\n<p>from attetntion_wrapper.py 731</p>\n<pre><code>  expanded_alignments = array_ops.expand_dims(alignments, 1)\n  attention_mechanism_values = self._attention_mechanism.values\n  context = math_ops.matmul(expanded_alignments, attention_mechanism_values)\n  context = array_ops.squeeze(context, [1])\n</code></pre>\n<p>Using above code for one of my application got 2.2 batch/s, after changing to use reduce_sum(as tf version 1.0 did), the speed is 3.4 batch/s, improve a lot.</p>\n<pre><code>  expanded_alignments = array_ops.expand_dims(alignments, 2)\n  attention_mechanism_values = self._attention_mechanism.values\n  context = math_ops.reduce_sum(expanded_alignments * attention_mechanism_values, [1])\n</code></pre>", "body_text": "tf version '1.2.0-rc0' contirb.seq2seq.attention_wrapper is great, it make using attention much easier.\nHowever I found using attention_wrapper will be much slower then tf version 1.0.\nAfter some experiment I found it is due to using matmul instead of reduce_sum.\nfrom attetntion_wrapper.py 731\n  expanded_alignments = array_ops.expand_dims(alignments, 1)\n  attention_mechanism_values = self._attention_mechanism.values\n  context = math_ops.matmul(expanded_alignments, attention_mechanism_values)\n  context = array_ops.squeeze(context, [1])\n\nUsing above code for one of my application got 2.2 batch/s, after changing to use reduce_sum(as tf version 1.0 did), the speed is 3.4 batch/s, improve a lot.\n  expanded_alignments = array_ops.expand_dims(alignments, 2)\n  attention_mechanism_values = self._attention_mechanism.values\n  context = math_ops.reduce_sum(expanded_alignments * attention_mechanism_values, [1])", "body": "tf version '1.2.0-rc0' contirb.seq2seq.attention_wrapper is great, it make using attention much easier.\r\nHowever I found using attention_wrapper will be much slower then tf version 1.0.\r\nAfter some experiment I found it is due to using matmul instead of reduce_sum.\r\n\r\nfrom attetntion_wrapper.py 731\r\n      \r\n      expanded_alignments = array_ops.expand_dims(alignments, 1)\r\n      attention_mechanism_values = self._attention_mechanism.values\r\n      context = math_ops.matmul(expanded_alignments, attention_mechanism_values)\r\n      context = array_ops.squeeze(context, [1])\r\n\r\nUsing above code for one of my application got 2.2 batch/s, after changing to use reduce_sum(as tf version 1.0 did), the speed is 3.4 batch/s, improve a lot.\r\n\r\n      expanded_alignments = array_ops.expand_dims(alignments, 2)\r\n      attention_mechanism_values = self._attention_mechanism.values\r\n      context = math_ops.reduce_sum(expanded_alignments * attention_mechanism_values, [1])"}