{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/299055559", "html_url": "https://github.com/tensorflow/tensorflow/issues/7679#issuecomment-299055559", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7679", "id": 299055559, "node_id": "MDEyOklzc3VlQ29tbWVudDI5OTA1NTU1OQ==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-03T22:40:10Z", "updated_at": "2017-05-03T22:43:46Z", "author_association": "MEMBER", "body_html": "<p>The <a href=\"https://www.tensorflow.org/performance/benchmarks\" rel=\"nofollow\">benchmarks</a> have been published along with the code.  Because all of you can do math, I want to address something in this thread.  InceptionV3 with batch size 32 no longer scales at x58.  What happened is that we got faster across the board but our gains for 1 GPU were higher than at 64 GPUs and that lowered the scaling factor a little.  Our images/sec, which we did not share, at the conference was ~1,496 and our current number is 1608 images/sec for batch size 32 per GPU.</p>\n<p>I apologize for the delay and look forward to focusing on time to accuracy, async training with accuracy, and other interesting problems.  If I missed something let me know.  I personally ran all of the tests and made every attempt to not grab the \"best number\".  I run each test 5 times and averaged the results.  I suspect I have run 5-10K tests in the past few months (I should sit down and make a more accurate guess) and spent some absurd amount of money.  Thank you for being patient.  In the future, I will do what I can to avoid these types of delays.  I am closing this issue but you can still comment, I think?</p>\n<p>I also learned a lot about scaling on Google Cloud and AWS.  If anyone wants to share amusing stories or has questions let me know.  I documented my environment, which I doubt was ideal but they mostly worked well for what I was doing.</p>\n<p><strong>Final item</strong>.  While, I say in the document that you need to use code post TF 1.1.  If you want to run the scripts with TF 1.1, you will get slightly worse results, but you can do it by commending out this line: <code>  config.gpu_options.force_gpu_compatible = FLAGS.force_gpu_compatible</code>.  And if you just want to jump to the <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py\">scripts</a></p>\n<p>EDIT:  slightly wrong link to master vs. root.  fixed</p>", "body_text": "The benchmarks have been published along with the code.  Because all of you can do math, I want to address something in this thread.  InceptionV3 with batch size 32 no longer scales at x58.  What happened is that we got faster across the board but our gains for 1 GPU were higher than at 64 GPUs and that lowered the scaling factor a little.  Our images/sec, which we did not share, at the conference was ~1,496 and our current number is 1608 images/sec for batch size 32 per GPU.\nI apologize for the delay and look forward to focusing on time to accuracy, async training with accuracy, and other interesting problems.  If I missed something let me know.  I personally ran all of the tests and made every attempt to not grab the \"best number\".  I run each test 5 times and averaged the results.  I suspect I have run 5-10K tests in the past few months (I should sit down and make a more accurate guess) and spent some absurd amount of money.  Thank you for being patient.  In the future, I will do what I can to avoid these types of delays.  I am closing this issue but you can still comment, I think?\nI also learned a lot about scaling on Google Cloud and AWS.  If anyone wants to share amusing stories or has questions let me know.  I documented my environment, which I doubt was ideal but they mostly worked well for what I was doing.\nFinal item.  While, I say in the document that you need to use code post TF 1.1.  If you want to run the scripts with TF 1.1, you will get slightly worse results, but you can do it by commending out this line:   config.gpu_options.force_gpu_compatible = FLAGS.force_gpu_compatible.  And if you just want to jump to the scripts\nEDIT:  slightly wrong link to master vs. root.  fixed", "body": "The [benchmarks](https://www.tensorflow.org/performance/benchmarks) have been published along with the code.  Because all of you can do math, I want to address something in this thread.  InceptionV3 with batch size 32 no longer scales at x58.  What happened is that we got faster across the board but our gains for 1 GPU were higher than at 64 GPUs and that lowered the scaling factor a little.  Our images/sec, which we did not share, at the conference was ~1,496 and our current number is 1608 images/sec for batch size 32 per GPU.  \r\n\r\nI apologize for the delay and look forward to focusing on time to accuracy, async training with accuracy, and other interesting problems.  If I missed something let me know.  I personally ran all of the tests and made every attempt to not grab the \"best number\".  I run each test 5 times and averaged the results.  I suspect I have run 5-10K tests in the past few months (I should sit down and make a more accurate guess) and spent some absurd amount of money.  Thank you for being patient.  In the future, I will do what I can to avoid these types of delays.  I am closing this issue but you can still comment, I think?\r\n\r\nI also learned a lot about scaling on Google Cloud and AWS.  If anyone wants to share amusing stories or has questions let me know.  I documented my environment, which I doubt was ideal but they mostly worked well for what I was doing. \r\n\r\n**Final item**.  While, I say in the document that you need to use code post TF 1.1.  If you want to run the scripts with TF 1.1, you will get slightly worse results, but you can do it by commending out this line: `  config.gpu_options.force_gpu_compatible = FLAGS.force_gpu_compatible`.  And if you just want to jump to the [scripts](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/tf_cnn_benchmarks.py)        \r\n\r\nEDIT:  slightly wrong link to master vs. root.  fixed\r\n\r\n"}