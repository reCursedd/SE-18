{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/243301958", "html_url": "https://github.com/tensorflow/tensorflow/issues/4022#issuecomment-243301958", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4022", "id": 243301958, "node_id": "MDEyOklzc3VlQ29tbWVudDI0MzMwMTk1OA==", "user": {"login": "alexbeloi", "id": 9807648, "node_id": "MDQ6VXNlcjk4MDc2NDg=", "avatar_url": "https://avatars2.githubusercontent.com/u/9807648?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alexbeloi", "html_url": "https://github.com/alexbeloi", "followers_url": "https://api.github.com/users/alexbeloi/followers", "following_url": "https://api.github.com/users/alexbeloi/following{/other_user}", "gists_url": "https://api.github.com/users/alexbeloi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alexbeloi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alexbeloi/subscriptions", "organizations_url": "https://api.github.com/users/alexbeloi/orgs", "repos_url": "https://api.github.com/users/alexbeloi/repos", "events_url": "https://api.github.com/users/alexbeloi/events{/privacy}", "received_events_url": "https://api.github.com/users/alexbeloi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-08-30T00:53:13Z", "updated_at": "2016-08-30T00:53:13Z", "author_association": "NONE", "body_html": "<p>Perhaps I am mistaken or am misunderstanding the documentation/implementation, but it I now believe that even the 'correct script' in my initial post is wrong (output not matching documentation).</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> [0, 1, 2, 3, 4 ,...]</span>\nx <span class=\"pl-k\">=</span> tf.range(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">10</span>, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>x<span class=\"pl-pds\">\"</span></span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> A queue that outputs 0,1,2,3,...</span>\nrange_q <span class=\"pl-k\">=</span> tf.train.range_input_producer(<span class=\"pl-v\">limit</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>, <span class=\"pl-v\">shuffle</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\nslice_end <span class=\"pl-k\">=</span> range_q.dequeue()\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....</span>\ny <span class=\"pl-k\">=</span> tf.slice(x, [<span class=\"pl-c1\">0</span>], [slice_end], <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>)\n\nbatched_data <span class=\"pl-k\">=</span> tf.train.batch(\n    <span class=\"pl-v\">tensors</span><span class=\"pl-k\">=</span>[y],\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>,\n    <span class=\"pl-v\">dynamic_pad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y_batch<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-v\">enqueue_many</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\n)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the graph</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.contrib.learn takes care of starting the queues for us</span>\nres <span class=\"pl-k\">=</span> tf.contrib.learn.run_n({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: batched_data}, <span class=\"pl-v\">n</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Print the result</span>\n<span class=\"pl-c1\">print</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Batch shape: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(res[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>].shape))\n<span class=\"pl-c1\">print</span>(res[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>])</pre></div>\n<p>Output:</p>\n<pre><code>Batch shape: (5, 4)\n[[0 0 0 0]\n [1 0 0 0]\n [1 2 0 0]\n [1 2 3 0]\n [1 2 3 4]]\n</code></pre>\n<p>I believe this output should be expected if and only if the <code>enqueue_many</code> flag is set to <code>True</code>, as per the documentation, this means that the <code>tensors</code> in <code>tensors_list</code> are interpreted as having first dimension indexing the examples. Which is the case for <code>y</code> as it is defined.</p>\n<p>However, with <code>enqueue_many=True</code> we get the following output:</p>\n<pre><code>Batch shape: (5,)\n[1 1 2 1 2]\n</code></pre>\n<p>I believe passing a list of tensors of varying lengths to <code>tf.train.batch</code> should look like this (with <code>tensors=y</code> not <code>tensors=[y]</code>), however the output is not as expected.</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\ny <span class=\"pl-k\">=</span> [tf.constant(<span class=\"pl-c1\">range</span>(n)) <span class=\"pl-k\">for</span> n <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">10</span>)]\n\nbatched_data <span class=\"pl-k\">=</span> tf.train.batch(\n    <span class=\"pl-v\">tensors</span><span class=\"pl-k\">=</span>y,    <span class=\"pl-c\"><span class=\"pl-c\">#</span>tensors=[y]</span>\n    <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">5</span>,\n    <span class=\"pl-v\">dynamic_pad</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>,\n    <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y_batch<span class=\"pl-pds\">\"</span></span>,\n    <span class=\"pl-v\">enqueue_many</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>\n)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Run the graph</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> tf.contrib.learn takes care of starting the queues for us</span>\nres <span class=\"pl-k\">=</span> tf.contrib.learn.run_n({<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>: batched_data}, <span class=\"pl-v\">n</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">None</span>)\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Print the result</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> print(\"Batch shape: {}\".format(res[0][\"y\"].shape))</span>\n<span class=\"pl-c1\">print</span>(res[<span class=\"pl-c1\">0</span>][<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>y<span class=\"pl-pds\">\"</span></span>])</pre></div>\n<p>Output:</p>\n<pre><code>[array([[0],\n       [0],\n       [0],\n       [0],\n       [0]], dtype=int32), array([[0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1]], dtype=int32), array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]], dtype=int32), array([[0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3]], dtype=int32), array([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]], dtype=int32), array([[0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)]\n</code></pre>\n<p>It almost looks each example is being queued <code>batch_size</code> number of times, or perhaps being dequeued improperly.</p>", "body_text": "Perhaps I am mistaken or am misunderstanding the documentation/implementation, but it I now believe that even the 'correct script' in my initial post is wrong (output not matching documentation).\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\n\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=5, shuffle=False)\nslice_end = range_q.dequeue()\n\n# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nbatched_data = tf.train.batch(\n    tensors=[y],\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\",\n    enqueue_many=False\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\nprint(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\nOutput:\nBatch shape: (5, 4)\n[[0 0 0 0]\n [1 0 0 0]\n [1 2 0 0]\n [1 2 3 0]\n [1 2 3 4]]\n\nI believe this output should be expected if and only if the enqueue_many flag is set to True, as per the documentation, this means that the tensors in tensors_list are interpreted as having first dimension indexing the examples. Which is the case for y as it is defined.\nHowever, with enqueue_many=True we get the following output:\nBatch shape: (5,)\n[1 1 2 1 2]\n\nI believe passing a list of tensors of varying lengths to tf.train.batch should look like this (with tensors=y not tensors=[y]), however the output is not as expected.\nimport tensorflow as tf\n\ny = [tf.constant(range(n)) for n in range(1,10)]\n\nbatched_data = tf.train.batch(\n    tensors=y,    #tensors=[y]\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\",\n    enqueue_many=False\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\n# print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\nOutput:\n[array([[0],\n       [0],\n       [0],\n       [0],\n       [0]], dtype=int32), array([[0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1]], dtype=int32), array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]], dtype=int32), array([[0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3]], dtype=int32), array([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]], dtype=int32), array([[0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)]\n\nIt almost looks each example is being queued batch_size number of times, or perhaps being dequeued improperly.", "body": "Perhaps I am mistaken or am misunderstanding the documentation/implementation, but it I now believe that even the 'correct script' in my initial post is wrong (output not matching documentation). \n\n``` python\nimport tensorflow as tf\n\n# [0, 1, 2, 3, 4 ,...]\nx = tf.range(1, 10, name=\"x\")\n\n# A queue that outputs 0,1,2,3,...\nrange_q = tf.train.range_input_producer(limit=5, shuffle=False)\nslice_end = range_q.dequeue()\n\n# Slice x to variable length, i.e. [0], [0, 1], [0, 1, 2], ....\ny = tf.slice(x, [0], [slice_end], name=\"y\")\n\nbatched_data = tf.train.batch(\n    tensors=[y],\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\",\n    enqueue_many=False\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\nprint(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\n```\n\nOutput:\n\n```\nBatch shape: (5, 4)\n[[0 0 0 0]\n [1 0 0 0]\n [1 2 0 0]\n [1 2 3 0]\n [1 2 3 4]]\n```\n\nI believe this output should be expected if and only if the `enqueue_many` flag is set to `True`, as per the documentation, this means that the `tensors` in `tensors_list` are interpreted as having first dimension indexing the examples. Which is the case for `y` as it is defined.\n\nHowever, with `enqueue_many=True` we get the following output:\n\n```\nBatch shape: (5,)\n[1 1 2 1 2]\n```\n\nI believe passing a list of tensors of varying lengths to `tf.train.batch` should look like this (with `tensors=y` not `tensors=[y]`), however the output is not as expected.\n\n``` python\nimport tensorflow as tf\n\ny = [tf.constant(range(n)) for n in range(1,10)]\n\nbatched_data = tf.train.batch(\n    tensors=y,    #tensors=[y]\n    batch_size=5,\n    dynamic_pad=True,\n    name=\"y_batch\",\n    enqueue_many=False\n)\n\n# Run the graph\n# tf.contrib.learn takes care of starting the queues for us\nres = tf.contrib.learn.run_n({\"y\": batched_data}, n=1, feed_dict=None)\n\n# Print the result\n# print(\"Batch shape: {}\".format(res[0][\"y\"].shape))\nprint(res[0][\"y\"])\n```\n\nOutput:\n\n```\n[array([[0],\n       [0],\n       [0],\n       [0],\n       [0]], dtype=int32), array([[0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1],\n       [0, 1]], dtype=int32), array([[0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2],\n       [0, 1, 2]], dtype=int32), array([[0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3],\n       [0, 1, 2, 3]], dtype=int32), array([[0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4],\n       [0, 1, 2, 3, 4]], dtype=int32), array([[0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5],\n       [0, 1, 2, 3, 4, 5]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6],\n       [0, 1, 2, 3, 4, 5, 6]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7],\n       [0, 1, 2, 3, 4, 5, 6, 7]], dtype=int32), array([[0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8],\n       [0, 1, 2, 3, 4, 5, 6, 7, 8]], dtype=int32)]\n```\n\nIt almost looks each example is being queued `batch_size` number of times, or perhaps being dequeued improperly.\n"}