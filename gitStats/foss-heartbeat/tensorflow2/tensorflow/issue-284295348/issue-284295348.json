{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15599", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15599/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15599/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15599/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/15599", "id": 284295348, "node_id": "MDExOlB1bGxSZXF1ZXN0MTU5OTYyMzM1", "number": 15599, "title": "Freeze pb model will not remove is_training flag of bn attr and it is moreover still set as true", "user": {"login": "HwMohanLiu", "id": 34759453, "node_id": "MDQ6VXNlcjM0NzU5NDUz", "avatar_url": "https://avatars3.githubusercontent.com/u/34759453?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HwMohanLiu", "html_url": "https://github.com/HwMohanLiu", "followers_url": "https://api.github.com/users/HwMohanLiu/followers", "following_url": "https://api.github.com/users/HwMohanLiu/following{/other_user}", "gists_url": "https://api.github.com/users/HwMohanLiu/gists{/gist_id}", "starred_url": "https://api.github.com/users/HwMohanLiu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HwMohanLiu/subscriptions", "organizations_url": "https://api.github.com/users/HwMohanLiu/orgs", "repos_url": "https://api.github.com/users/HwMohanLiu/repos", "events_url": "https://api.github.com/users/HwMohanLiu/events{/privacy}", "received_events_url": "https://api.github.com/users/HwMohanLiu/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 4, "created_at": "2017-12-23T08:54:29Z", "updated_at": "2017-12-28T19:06:42Z", "closed_at": "2017-12-28T19:06:42Z", "author_association": "NONE", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/15599", "html_url": "https://github.com/tensorflow/tensorflow/pull/15599", "diff_url": "https://github.com/tensorflow/tensorflow/pull/15599.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/15599.patch"}, "body_html": "<p>Using /tensorflow/python/framework/graph_util_impl.py # <strong>convert_variables_to_constants</strong> to freeze graph to pb model will not remove <strong>is_training</strong> flag of <strong>batch_normalization</strong> as well as <strong>fused batch_normalization</strong> ops (tf.layers.batch_normalziation) in the inference graph.  Moreover, the <strong>is_training</strong> flag is still set as true. This issue will not cause any errors to use the pb model on Android. However, it will cause converting errors to tflite model by toco. Is it possible to optimize this conversion tool to resolve the problem in further versions.</p>\n<p><em>Following is the node output of the pb model during loading:<br>\nname: \"convolution_layer/block_layer1/batch_normalization/FusedBatchNorm\"<br>\nop: \"FusedBatchNorm\"<br>\ninput: \"convolution_layer/block_layer1/conv2d/Conv2D\"<br>\ninput: \"batch_normalization/gamma/read\"<br>\ninput: \"batch_normalization/beta/read\"<br>\ninput: \"convolution_layer/block_layer1/batch_normalization/Const\"<br>\ninput: \"convolution_layer/block_layer1/batch_normalization/Const_1\"<br>\nattr {<br>\nkey: \"T\"<br>\nvalue {<br>\ntype: DT_FLOAT<br>\n}<br>\n}<br>\nattr {<br>\nkey: \"data_format\"<br>\nvalue {<br>\ns: \"NHWC\"<br>\n}<br>\n}<br>\nattr {<br>\nkey: \"epsilon\"<br>\nvalue {<br>\nf: 0.0010000000475<br>\n}<br>\n}<br>\n<strong>attr {<br>\nkey: \"is_training\"<br>\nvalue {<br>\nb: true<br>\n}</strong><br>\n}</em></p>", "body_text": "Using /tensorflow/python/framework/graph_util_impl.py # convert_variables_to_constants to freeze graph to pb model will not remove is_training flag of batch_normalization as well as fused batch_normalization ops (tf.layers.batch_normalziation) in the inference graph.  Moreover, the is_training flag is still set as true. This issue will not cause any errors to use the pb model on Android. However, it will cause converting errors to tflite model by toco. Is it possible to optimize this conversion tool to resolve the problem in further versions.\nFollowing is the node output of the pb model during loading:\nname: \"convolution_layer/block_layer1/batch_normalization/FusedBatchNorm\"\nop: \"FusedBatchNorm\"\ninput: \"convolution_layer/block_layer1/conv2d/Conv2D\"\ninput: \"batch_normalization/gamma/read\"\ninput: \"batch_normalization/beta/read\"\ninput: \"convolution_layer/block_layer1/batch_normalization/Const\"\ninput: \"convolution_layer/block_layer1/batch_normalization/Const_1\"\nattr {\nkey: \"T\"\nvalue {\ntype: DT_FLOAT\n}\n}\nattr {\nkey: \"data_format\"\nvalue {\ns: \"NHWC\"\n}\n}\nattr {\nkey: \"epsilon\"\nvalue {\nf: 0.0010000000475\n}\n}\nattr {\nkey: \"is_training\"\nvalue {\nb: true\n}\n}", "body": "Using /tensorflow/python/framework/graph_util_impl.py # **convert_variables_to_constants** to freeze graph to pb model will not remove **is_training** flag of **batch_normalization** as well as **fused batch_normalization** ops (tf.layers.batch_normalziation) in the inference graph.  Moreover, the **is_training** flag is still set as true. This issue will not cause any errors to use the pb model on Android. However, it will cause converting errors to tflite model by toco. Is it possible to optimize this conversion tool to resolve the problem in further versions. \r\n\r\n_Following is the node output of the pb model during loading:\r\nname: \"convolution_layer/block_layer1/batch_normalization/FusedBatchNorm\"\r\nop: \"FusedBatchNorm\"\r\ninput: \"convolution_layer/block_layer1/conv2d/Conv2D\"\r\ninput: \"batch_normalization/gamma/read\"\r\ninput: \"batch_normalization/beta/read\"\r\ninput: \"convolution_layer/block_layer1/batch_normalization/Const\"\r\ninput: \"convolution_layer/block_layer1/batch_normalization/Const_1\"\r\nattr {\r\n  key: \"T\"\r\n  value {\r\n    type: DT_FLOAT\r\n  }\r\n}\r\nattr {\r\n  key: \"data_format\"\r\n  value {\r\n    s: \"NHWC\"\r\n  }\r\n}\r\nattr {\r\n  key: \"epsilon\"\r\n  value {\r\n    f: 0.0010000000475\r\n  }\r\n}\r\n**attr {\r\n  key: \"is_training\"\r\n  value {\r\n    b: true\r\n  }**\r\n}_"}