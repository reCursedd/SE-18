{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/428032951", "html_url": "https://github.com/tensorflow/tensorflow/issues/20698#issuecomment-428032951", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20698", "id": 428032951, "node_id": "MDEyOklzc3VlQ29tbWVudDQyODAzMjk1MQ==", "user": {"login": "gabrielibagon", "id": 12654145, "node_id": "MDQ6VXNlcjEyNjU0MTQ1", "avatar_url": "https://avatars2.githubusercontent.com/u/12654145?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gabrielibagon", "html_url": "https://github.com/gabrielibagon", "followers_url": "https://api.github.com/users/gabrielibagon/followers", "following_url": "https://api.github.com/users/gabrielibagon/following{/other_user}", "gists_url": "https://api.github.com/users/gabrielibagon/gists{/gist_id}", "starred_url": "https://api.github.com/users/gabrielibagon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gabrielibagon/subscriptions", "organizations_url": "https://api.github.com/users/gabrielibagon/orgs", "repos_url": "https://api.github.com/users/gabrielibagon/repos", "events_url": "https://api.github.com/users/gabrielibagon/events{/privacy}", "received_events_url": "https://api.github.com/users/gabrielibagon/received_events", "type": "User", "site_admin": false}, "created_at": "2018-10-09T01:50:12Z", "updated_at": "2018-10-11T00:59:51Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=8582703\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/JanRuettinger\">@JanRuettinger</a> <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=5393732\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ricoms\">@ricoms</a> Sorry for the delayed response.</p>\n<p>I drafted up a toy example using MNIST in order to train a model with two inputs and two outputs. The model is simply two identical models fused together, which takes in two copies of the MNIST data (two inputs) and outputs a prediction for each (two outputs). You can adapt this to more complex models and input pipelines.</p>\n<p><strong>Note</strong>: This is still using  <code>tf-nightly-gpu</code> version <code>1.12.0-dev20180918</code>. I assume this will work in tensorflow 1.12 and above.</p>\n<pre><code>batch_size = 512\n\n# -- Data Setup -- #\n(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\ny_train = tf.keras.utils.to_categorical(y_train)\nx_train, x_test = x_train / 255.0, x_test / 255.0\n# Create two inputs and two outputs (for demonstration)\nx_train1 = x_train2 = x_train\ny_train1 = y_train2 = y_train\n\n# -- Dataset API -- #\n# Create a Dataset for multiple inputs and Dataset for multiple outputs\ninput_set = tf.data.Dataset.from_tensor_slices((x_train1, x_train2))\noutput_set = tf.data.Dataset.from_tensor_slices((y_train1, y_train2))\n# Create Dataset pipeline\ninput_set = input_set.batch(batch_size).repeat()\noutput_set = output_set.batch(batch_size).repeat()\n# Group the input and output dataset\ndataset = tf.data.Dataset.zip((input_set, output_set))\n# Initialize the iterator to be passed to the model.fit() function\ndata_iter = dataset.make_one_shot_iterator()\n\n# -- Model Definition -- #\n# Multiple Inputs\ninput1 = tf.keras.layers.Input(shape=(28,28))\ninput2 = tf.keras.layers.Input(shape=(28,28))\n# Input 1 Pathway\nx1 = tf.keras.layers.Flatten()(input1)\nx1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(x1)\nx1 = tf.keras.layers.Dropout(0.2)(x1)\n# Input 2 Pathway\nx2 = tf.keras.layers.Flatten()(input2)\nx2 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(x2)\nx2 = tf.keras.layers.Dropout(0.2)(x2)\n# Multiple Outputs\noutput1 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x1)\noutput2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x2)\n# Create Model\nmodel = tf.keras.models.Model(inputs=[input1, input2], outputs=[output1, output2])\n# Compile\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n\n# -- Train -- #\nmodel.fit(data_iter, steps_per_epoch=len(x_train)//batch_size, epochs=5)\n</code></pre>\n<p><strong>Update</strong>: As <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=37063658\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jashshopin\">@jashshopin</a> mentions below, the <code>dataset</code> object can be passed directly to <code>model.fit()</code> if you have no need for an iterator.</p>", "body_text": "@JanRuettinger @ricoms Sorry for the delayed response.\nI drafted up a toy example using MNIST in order to train a model with two inputs and two outputs. The model is simply two identical models fused together, which takes in two copies of the MNIST data (two inputs) and outputs a prediction for each (two outputs). You can adapt this to more complex models and input pipelines.\nNote: This is still using  tf-nightly-gpu version 1.12.0-dev20180918. I assume this will work in tensorflow 1.12 and above.\nbatch_size = 512\n\n# -- Data Setup -- #\n(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\ny_train = tf.keras.utils.to_categorical(y_train)\nx_train, x_test = x_train / 255.0, x_test / 255.0\n# Create two inputs and two outputs (for demonstration)\nx_train1 = x_train2 = x_train\ny_train1 = y_train2 = y_train\n\n# -- Dataset API -- #\n# Create a Dataset for multiple inputs and Dataset for multiple outputs\ninput_set = tf.data.Dataset.from_tensor_slices((x_train1, x_train2))\noutput_set = tf.data.Dataset.from_tensor_slices((y_train1, y_train2))\n# Create Dataset pipeline\ninput_set = input_set.batch(batch_size).repeat()\noutput_set = output_set.batch(batch_size).repeat()\n# Group the input and output dataset\ndataset = tf.data.Dataset.zip((input_set, output_set))\n# Initialize the iterator to be passed to the model.fit() function\ndata_iter = dataset.make_one_shot_iterator()\n\n# -- Model Definition -- #\n# Multiple Inputs\ninput1 = tf.keras.layers.Input(shape=(28,28))\ninput2 = tf.keras.layers.Input(shape=(28,28))\n# Input 1 Pathway\nx1 = tf.keras.layers.Flatten()(input1)\nx1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(x1)\nx1 = tf.keras.layers.Dropout(0.2)(x1)\n# Input 2 Pathway\nx2 = tf.keras.layers.Flatten()(input2)\nx2 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(x2)\nx2 = tf.keras.layers.Dropout(0.2)(x2)\n# Multiple Outputs\noutput1 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x1)\noutput2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x2)\n# Create Model\nmodel = tf.keras.models.Model(inputs=[input1, input2], outputs=[output1, output2])\n# Compile\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\n\n# -- Train -- #\nmodel.fit(data_iter, steps_per_epoch=len(x_train)//batch_size, epochs=5)\n\nUpdate: As @jashshopin mentions below, the dataset object can be passed directly to model.fit() if you have no need for an iterator.", "body": "@JanRuettinger @ricoms Sorry for the delayed response.\r\n\r\nI drafted up a toy example using MNIST in order to train a model with two inputs and two outputs. The model is simply two identical models fused together, which takes in two copies of the MNIST data (two inputs) and outputs a prediction for each (two outputs). You can adapt this to more complex models and input pipelines.\r\n\r\n**Note**: This is still using  `tf-nightly-gpu` version `1.12.0-dev20180918`. I assume this will work in tensorflow 1.12 and above.\r\n\r\n```\r\nbatch_size = 512\r\n\r\n# -- Data Setup -- #\r\n(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\r\ny_train = tf.keras.utils.to_categorical(y_train)\r\nx_train, x_test = x_train / 255.0, x_test / 255.0\r\n# Create two inputs and two outputs (for demonstration)\r\nx_train1 = x_train2 = x_train\r\ny_train1 = y_train2 = y_train\r\n\r\n# -- Dataset API -- #\r\n# Create a Dataset for multiple inputs and Dataset for multiple outputs\r\ninput_set = tf.data.Dataset.from_tensor_slices((x_train1, x_train2))\r\noutput_set = tf.data.Dataset.from_tensor_slices((y_train1, y_train2))\r\n# Create Dataset pipeline\r\ninput_set = input_set.batch(batch_size).repeat()\r\noutput_set = output_set.batch(batch_size).repeat()\r\n# Group the input and output dataset\r\ndataset = tf.data.Dataset.zip((input_set, output_set))\r\n# Initialize the iterator to be passed to the model.fit() function\r\ndata_iter = dataset.make_one_shot_iterator()\r\n\r\n# -- Model Definition -- #\r\n# Multiple Inputs\r\ninput1 = tf.keras.layers.Input(shape=(28,28))\r\ninput2 = tf.keras.layers.Input(shape=(28,28))\r\n# Input 1 Pathway\r\nx1 = tf.keras.layers.Flatten()(input1)\r\nx1 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(x1)\r\nx1 = tf.keras.layers.Dropout(0.2)(x1)\r\n# Input 2 Pathway\r\nx2 = tf.keras.layers.Flatten()(input2)\r\nx2 = tf.keras.layers.Dense(512, activation=tf.nn.relu)(x2)\r\nx2 = tf.keras.layers.Dropout(0.2)(x2)\r\n# Multiple Outputs\r\noutput1 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x1)\r\noutput2 = tf.keras.layers.Dense(10, activation=tf.nn.softmax)(x2)\r\n# Create Model\r\nmodel = tf.keras.models.Model(inputs=[input1, input2], outputs=[output1, output2])\r\n# Compile\r\nmodel.compile(optimizer='adam', loss='categorical_crossentropy')\r\n\r\n# -- Train -- #\r\nmodel.fit(data_iter, steps_per_epoch=len(x_train)//batch_size, epochs=5)\r\n```\r\n\r\n**Update**: As @jashshopin mentions below, the `dataset` object can be passed directly to `model.fit()` if you have no need for an iterator."}