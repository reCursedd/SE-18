{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20698", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20698/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20698/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20698/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/20698", "id": 340247849, "node_id": "MDU6SXNzdWUzNDAyNDc4NDk=", "number": 20698, "title": "tf.keras multi input models don't work when using tf.data.Dataset", "user": {"login": "lgeiger", "id": 13285808, "node_id": "MDQ6VXNlcjEzMjg1ODA4", "avatar_url": "https://avatars1.githubusercontent.com/u/13285808?v=4", "gravatar_id": "", "url": "https://api.github.com/users/lgeiger", "html_url": "https://github.com/lgeiger", "followers_url": "https://api.github.com/users/lgeiger/followers", "following_url": "https://api.github.com/users/lgeiger/following{/other_user}", "gists_url": "https://api.github.com/users/lgeiger/gists{/gist_id}", "starred_url": "https://api.github.com/users/lgeiger/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/lgeiger/subscriptions", "organizations_url": "https://api.github.com/users/lgeiger/orgs", "repos_url": "https://api.github.com/users/lgeiger/repos", "events_url": "https://api.github.com/users/lgeiger/events{/privacy}", "received_events_url": "https://api.github.com/users/lgeiger/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 22, "created_at": "2018-07-11T13:42:33Z", "updated_at": "2018-11-16T00:54:22Z", "closed_at": "2018-11-14T21:44:49Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Please go to Stack Overflow for help and support:</p>\n<p><a href=\"https://stackoverflow.com/questions/tagged/tensorflow\" rel=\"nofollow\">https://stackoverflow.com/questions/tagged/tensorflow</a></p>\n<p>If you open a GitHub issue, here is our policy:</p>\n<ol>\n<li>It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).</li>\n<li>The form below must be filled out.</li>\n<li>It shouldn't be a TensorBoard issue. Those go <a href=\"https://github.com/tensorflow/tensorboard/issues\">here</a>.</li>\n</ol>\n<p><strong>Here's why we have that policy</strong>: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.</p>\n<hr>\n<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: macOS 10.13.5 and Debian GNU/Linux 9 (stretch)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: v1.9.0-rc2-359-g95cfd8b3d9 1.10.0-dev20180711 also reproduces on v1.9.0</li>\n<li><strong>Python version</strong>: 3.6.5 and 3.5.3</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: None</li>\n<li><strong>GPU model and memory</strong>: None</li>\n<li><strong>Exact command to reproduce</strong>: see below</li>\n</ul>\n<p>You can collect some of this information using our environment capture script:</p>\n<p><a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh</a></p>\n<p>You can obtain the TensorFlow version with</p>\n<p>python -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<h3>Describe the problem</h3>\n<p><code>tf.keras</code> multi input models don't work when used together with <code>tf.data.Dataset</code> due to input broken validation checks. This problem reproduces both on tf@1.9.0 and the latest nightly.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=710255\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fchollet\">@fchollet</a> Do you have any ideas what's going on here, or am I missing something obvious?</p>\n<h3>Source code / logs</h3>\n<h4>Multi input model</h4>\n<p>Consider the following toy model:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n<span class=\"pl-k\">from</span> tensorflow <span class=\"pl-k\">import</span> keras\n\ndata_a <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">455</span>, <span class=\"pl-c1\">350</span>, <span class=\"pl-c1\">560</span>, <span class=\"pl-c1\">700</span>, <span class=\"pl-c1\">800</span>, <span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">250</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\nlabels <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">455</span>, <span class=\"pl-c1\">350</span>, <span class=\"pl-c1\">560</span>, <span class=\"pl-c1\">700</span>, <span class=\"pl-c1\">800</span>, <span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">250</span>, <span class=\"pl-c1\">300</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\ndata_b <span class=\"pl-k\">=</span> np.array([<span class=\"pl-c1\">200</span>, <span class=\"pl-c1\">255</span>, <span class=\"pl-c1\">350</span>, <span class=\"pl-c1\">470</span>, <span class=\"pl-c1\">600</span>, <span class=\"pl-c1\">300</span>, <span class=\"pl-c1\">344</span>, <span class=\"pl-c1\">322</span>], <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>np.float32)\ndata_a <span class=\"pl-k\">=</span> np.reshape(data_a, (<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\ndata_b <span class=\"pl-k\">=</span> np.reshape(data_b, (<span class=\"pl-c1\">8</span>, <span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>))\n\nx <span class=\"pl-k\">=</span> keras.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x<span class=\"pl-pds\">'</span></span>)\ny <span class=\"pl-k\">=</span> keras.layers.Input(<span class=\"pl-v\">shape</span><span class=\"pl-k\">=</span>(<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">1</span>), <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y<span class=\"pl-pds\">'</span></span>)\nadmi <span class=\"pl-k\">=</span> keras.layers.LSTM(<span class=\"pl-c1\">40</span>, <span class=\"pl-v\">return_sequences</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)(x)\npla <span class=\"pl-k\">=</span> keras.layers.LSTM(<span class=\"pl-c1\">40</span>, <span class=\"pl-v\">return_sequences</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)(y)\nout <span class=\"pl-k\">=</span> keras.layers.concatenate([admi, pla], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\noutput <span class=\"pl-k\">=</span> keras.layers.Dense(<span class=\"pl-c1\">1</span>, <span class=\"pl-v\">activation</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>sigmoid<span class=\"pl-pds\">'</span></span>)(out)\nmodel <span class=\"pl-k\">=</span> keras.models.Model(<span class=\"pl-v\">inputs</span><span class=\"pl-k\">=</span>[x, y], <span class=\"pl-v\">outputs</span><span class=\"pl-k\">=</span>output)\nmodel.compile(<span class=\"pl-v\">optimizer</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>adam<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">loss</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>binary_crossentropy<span class=\"pl-pds\">'</span></span>, <span class=\"pl-v\">metrics</span><span class=\"pl-k\">=</span>[<span class=\"pl-s\"><span class=\"pl-pds\">'</span>accuracy<span class=\"pl-pds\">'</span></span>])</pre></div>\n<h4>Using <code>numpy</code> data</h4>\n<p>When fitting using <code>numpy</code> data this works as expected when passing a list or dictionary of inputs:</p>\n<div class=\"highlight highlight-source-python\"><pre>model.fit([data_a, data_b], labels, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)\nmodel.fit({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x<span class=\"pl-pds\">'</span></span>: data_a, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y<span class=\"pl-pds\">'</span></span>: data_b}, labels, <span class=\"pl-v\">batch_size</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">2</span>, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>)</pre></div>\n<h4>Using <code>tf.data.Dataset.from_tensor_slices</code> dictionary</h4>\n<p>When trying the same with a <code>tf.data.Dataset</code> the following fails due to incorrect input validation:</p>\n<div class=\"highlight highlight-source-python\"><pre>dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x<span class=\"pl-pds\">'</span></span>: data_a, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y<span class=\"pl-pds\">'</span></span>: data_b}, labels)).batch(<span class=\"pl-c1\">2</span>).repeat()\nmodel.fit(dataset, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)</pre></div>\n<div class=\"highlight highlight-text-python-traceback\"><pre>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-6-d35bacd274cc&gt; in &lt;module&gt;()\n      <span class=\"pl-c1\">1</span> dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x<span class=\"pl-pds\">'</span></span>: data_a, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y<span class=\"pl-pds\">'</span></span>: data_b}, labels)).batch(<span class=\"pl-c1\">2</span>).repeat()\n----&gt; 2 model.fit(dataset, epochs=10, steps_per_epoch=4)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n   1276         steps_name='steps_per_epoch',\n   1277         steps=steps_per_epoch,\n-&gt; 1278         validation_split=validation_split)\n   1279 \n   1280     # Prepare validation data.\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\n    <span class=\"pl-c1\">915</span>           feed_output_shapes,\n    <span class=\"pl-c1\">916</span>           check_batch_axis<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Don't enforce the batch size.</span>\n--&gt; 917           exception_prefix='target')\n    <span class=\"pl-c1\">918</span> \n    <span class=\"pl-c1\">919</span>       <span class=\"pl-c\"><span class=\"pl-c\">#</span> Generate sample-wise weight values given the `sample_weight` and</span>\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n    <span class=\"pl-c1\">180</span>                            <span class=\"pl-s\"><span class=\"pl-pds\">'</span>: expected <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span> names[i] <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span> to have <span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">+</span>\n    <span class=\"pl-c1\">181</span>                            <span class=\"pl-c1\">str</span>(<span class=\"pl-c1\">len</span>(shape)) <span class=\"pl-k\">+</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span> dimensions, but got array <span class=\"pl-pds\">'</span></span>\n--&gt; 182                            'with shape ' + str(data_shape))\n    <span class=\"pl-c1\">183</span>         <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> check_batch_axis:\n    <span class=\"pl-c1\">184</span>           data_shape <span class=\"pl-k\">=</span> data_shape[<span class=\"pl-c1\">1</span>:]\n\n<span class=\"pl-en\">ValueError</span>: <span class=\"pl-s\">Error when checking target: expected dense to have 2 dimensions, but got array with shape (None,)</span></pre></div>\n<h4>Using <code>tf.data.Dataset.from_generator</code> dictionary</h4>\n<p>However using the same network together with <code>tf.data.Dataset.from_generator</code> works. Probably because less validation is done:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">generator</span>():\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> np.random.permutation(<span class=\"pl-c1\">8</span>):\n            <span class=\"pl-k\">yield</span> {<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x<span class=\"pl-pds\">'</span></span>: data_a[i], <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y<span class=\"pl-pds\">'</span></span>: data_b[i]}, labels[i]\n\ndataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(generator, ({<span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_x<span class=\"pl-pds\">'</span></span>: tf.float32, <span class=\"pl-s\"><span class=\"pl-pds\">'</span>input_y<span class=\"pl-pds\">'</span></span>: tf.float32}, tf.float32)).batch(<span class=\"pl-c1\">2</span>)\nmodel.fit(dataset, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)</pre></div>\n<h4>Using <code>tf.data.Dataset</code> tuple</h4>\n<p>Passing the multi-input as a tuple to the model both datasets generated with <code>from_tensor_slices</code> and <code>from_generator</code> fail:</p>\n<div class=\"highlight highlight-source-python\"><pre>dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(<span class=\"pl-c1\">2</span>).repeat()\nmodel.fit(dataset, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)</pre></div>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">def</span> <span class=\"pl-en\">generator</span>():\n    <span class=\"pl-k\">while</span> <span class=\"pl-c1\">True</span>:\n        <span class=\"pl-k\">for</span> i <span class=\"pl-k\">in</span> np.random.permutation(<span class=\"pl-c1\">8</span>):\n            <span class=\"pl-k\">yield</span> (data_a[i], data_b[i]), labels[i]\n\ndataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_generator(generator, ((tf.float32, tf.float32), tf.float32)).batch(<span class=\"pl-c1\">2</span>)\nmodel.fit(dataset, <span class=\"pl-v\">epochs</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">10</span>, <span class=\"pl-v\">steps_per_epoch</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">4</span>)</pre></div>\n<div class=\"highlight highlight-text-python-traceback\"><pre>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n&lt;ipython-input-7-512a95f0c2a7&gt; in &lt;module&gt;()\n      <span class=\"pl-c1\">1</span> dataset <span class=\"pl-k\">=</span> tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(<span class=\"pl-c1\">2</span>).repeat()\n----&gt; 2 model.fit(dataset, epochs=10, steps_per_epoch=4)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n   1276         steps_name='steps_per_epoch',\n   1277         steps=steps_per_epoch,\n-&gt; 1278         validation_split=validation_split)\n   1279 \n   1280     # Prepare validation data.\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\n    <span class=\"pl-c1\">876</span>         feed_input_shapes,\n    <span class=\"pl-c1\">877</span>         check_batch_axis<span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>,  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Don't enforce the batch size.</span>\n--&gt; 878         exception_prefix='input')\n    <span class=\"pl-c1\">879</span> \n    <span class=\"pl-c1\">880</span>     <span class=\"pl-k\">if</span> y <span class=\"pl-k\">is</span> <span class=\"pl-k\">not</span> <span class=\"pl-c1\">None</span>:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n    <span class=\"pl-c1\">141</span>     data <span class=\"pl-k\">=</span> data.values <span class=\"pl-k\">if</span> data.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>DataFrame<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> data\n    <span class=\"pl-c1\">142</span>     data <span class=\"pl-k\">=</span> [data]\n--&gt; 143   data = [standardize_single_array(x) for x in data]\n    <span class=\"pl-c1\">144</span> \n    <span class=\"pl-c1\">145</span>   <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(data) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">len</span>(names):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in &lt;listcomp&gt;(.0)\n    <span class=\"pl-c1\">141</span>     data <span class=\"pl-k\">=</span> data.values <span class=\"pl-k\">if</span> data.<span class=\"pl-c1\">__class__</span>.<span class=\"pl-c1\">__name__</span> <span class=\"pl-k\">==</span> <span class=\"pl-s\"><span class=\"pl-pds\">'</span>DataFrame<span class=\"pl-pds\">'</span></span> <span class=\"pl-k\">else</span> data\n    <span class=\"pl-c1\">142</span>     data <span class=\"pl-k\">=</span> [data]\n--&gt; 143   data = [standardize_single_array(x) for x in data]\n    <span class=\"pl-c1\">144</span> \n    <span class=\"pl-c1\">145</span>   <span class=\"pl-k\">if</span> <span class=\"pl-c1\">len</span>(data) <span class=\"pl-k\">!=</span> <span class=\"pl-c1\">len</span>(names):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_single_array(x)\n     <span class=\"pl-c1\">79</span>   <span class=\"pl-k\">elif</span> tensor_util.is_tensor(x):\n     <span class=\"pl-c1\">80</span>     <span class=\"pl-k\">return</span> x\n---&gt; 81   elif x.ndim == 1:\n     <span class=\"pl-c1\">82</span>     x <span class=\"pl-k\">=</span> np.expand_dims(x, <span class=\"pl-c1\">1</span>)\n     <span class=\"pl-c1\">83</span>   <span class=\"pl-k\">return</span> x\n\n<span class=\"pl-en\">AttributeError</span>: <span class=\"pl-s\">'tuple' object has no attribute 'ndim'</span></pre></div>", "body_text": "Please go to Stack Overflow for help and support:\nhttps://stackoverflow.com/questions/tagged/tensorflow\nIf you open a GitHub issue, here is our policy:\n\nIt must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\nThe form below must be filled out.\nIt shouldn't be a TensorBoard issue. Those go here.\n\nHere's why we have that policy: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\n\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): macOS 10.13.5 and Debian GNU/Linux 9 (stretch)\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): v1.9.0-rc2-359-g95cfd8b3d9 1.10.0-dev20180711 also reproduces on v1.9.0\nPython version: 3.6.5 and 3.5.3\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version: None\nGPU model and memory: None\nExact command to reproduce: see below\n\nYou can collect some of this information using our environment capture script:\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\nYou can obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nDescribe the problem\ntf.keras multi input models don't work when used together with tf.data.Dataset due to input broken validation checks. This problem reproduces both on tf@1.9.0 and the latest nightly.\n@fchollet Do you have any ideas what's going on here, or am I missing something obvious?\nSource code / logs\nMulti input model\nConsider the following toy model:\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\n\ndata_a = np.array([300, 455, 350, 560, 700, 800, 200, 250], dtype=np.float32)\nlabels = np.array([455, 350, 560, 700, 800, 200, 250, 300], dtype=np.float32)\ndata_b = np.array([200, 255, 350, 470, 600, 300, 344, 322], dtype=np.float32)\ndata_a = np.reshape(data_a, (8, 1, 1))\ndata_b = np.reshape(data_b, (8, 1, 1))\n\nx = keras.layers.Input(shape=(1, 1), name='input_x')\ny = keras.layers.Input(shape=(1, 1), name='input_y')\nadmi = keras.layers.LSTM(40, return_sequences=False)(x)\npla = keras.layers.LSTM(40, return_sequences=False)(y)\nout = keras.layers.concatenate([admi, pla], axis=-1)\noutput = keras.layers.Dense(1, activation='sigmoid')(out)\nmodel = keras.models.Model(inputs=[x, y], outputs=output)\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nUsing numpy data\nWhen fitting using numpy data this works as expected when passing a list or dictionary of inputs:\nmodel.fit([data_a, data_b], labels, batch_size=2, epochs=10)\nmodel.fit({'input_x': data_a, 'input_y': data_b}, labels, batch_size=2, epochs=10)\nUsing tf.data.Dataset.from_tensor_slices dictionary\nWhen trying the same with a tf.data.Dataset the following fails due to incorrect input validation:\ndataset = tf.data.Dataset.from_tensor_slices(({'input_x': data_a, 'input_y': data_b}, labels)).batch(2).repeat()\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-6-d35bacd274cc> in <module>()\n      1 dataset = tf.data.Dataset.from_tensor_slices(({'input_x': data_a, 'input_y': data_b}, labels)).batch(2).repeat()\n----> 2 model.fit(dataset, epochs=10, steps_per_epoch=4)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n   1276         steps_name='steps_per_epoch',\n   1277         steps=steps_per_epoch,\n-> 1278         validation_split=validation_split)\n   1279 \n   1280     # Prepare validation data.\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\n    915           feed_output_shapes,\n    916           check_batch_axis=False,  # Don't enforce the batch size.\n--> 917           exception_prefix='target')\n    918 \n    919       # Generate sample-wise weight values given the `sample_weight` and\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n    180                            ': expected ' + names[i] + ' to have ' +\n    181                            str(len(shape)) + ' dimensions, but got array '\n--> 182                            'with shape ' + str(data_shape))\n    183         if not check_batch_axis:\n    184           data_shape = data_shape[1:]\n\nValueError: Error when checking target: expected dense to have 2 dimensions, but got array with shape (None,)\nUsing tf.data.Dataset.from_generator dictionary\nHowever using the same network together with tf.data.Dataset.from_generator works. Probably because less validation is done:\ndef generator():\n    while True:\n        for i in np.random.permutation(8):\n            yield {'input_x': data_a[i], 'input_y': data_b[i]}, labels[i]\n\ndataset = tf.data.Dataset.from_generator(generator, ({'input_x': tf.float32, 'input_y': tf.float32}, tf.float32)).batch(2)\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\nUsing tf.data.Dataset tuple\nPassing the multi-input as a tuple to the model both datasets generated with from_tensor_slices and from_generator fail:\ndataset = tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(2).repeat()\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\ndef generator():\n    while True:\n        for i in np.random.permutation(8):\n            yield (data_a[i], data_b[i]), labels[i]\n\ndataset = tf.data.Dataset.from_generator(generator, ((tf.float32, tf.float32), tf.float32)).batch(2)\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\n---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-7-512a95f0c2a7> in <module>()\n      1 dataset = tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(2).repeat()\n----> 2 model.fit(dataset, epochs=10, steps_per_epoch=4)\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\n   1276         steps_name='steps_per_epoch',\n   1277         steps=steps_per_epoch,\n-> 1278         validation_split=validation_split)\n   1279 \n   1280     # Prepare validation data.\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\n    876         feed_input_shapes,\n    877         check_batch_axis=False,  # Don't enforce the batch size.\n--> 878         exception_prefix='input')\n    879 \n    880     if y is not None:\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\n    141     data = data.values if data.__class__.__name__ == 'DataFrame' else data\n    142     data = [data]\n--> 143   data = [standardize_single_array(x) for x in data]\n    144 \n    145   if len(data) != len(names):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in <listcomp>(.0)\n    141     data = data.values if data.__class__.__name__ == 'DataFrame' else data\n    142     data = [data]\n--> 143   data = [standardize_single_array(x) for x in data]\n    144 \n    145   if len(data) != len(names):\n\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_single_array(x)\n     79   elif tensor_util.is_tensor(x):\n     80     return x\n---> 81   elif x.ndim == 1:\n     82     x = np.expand_dims(x, 1)\n     83   return x\n\nAttributeError: 'tuple' object has no attribute 'ndim'", "body": "Please go to Stack Overflow for help and support:\r\n\r\nhttps://stackoverflow.com/questions/tagged/tensorflow\r\n\r\nIf you open a GitHub issue, here is our policy:\r\n\r\n1. It must be a bug, a feature request, or a significant problem with documentation (for small docs fixes please send a PR instead).\r\n2. The form below must be filled out.\r\n3. It shouldn't be a TensorBoard issue. Those go [here](https://github.com/tensorflow/tensorboard/issues).\r\n\r\n**Here's why we have that policy**: TensorFlow developers respond to issues. We want to focus on work that benefits the whole community, e.g., fixing bugs and adding features. Support only helps individuals. GitHub also notifies thousands of people when issues are filed. We want them to see you communicating an interesting problem, rather than being redirected to Stack Overflow.\r\n\r\n------------------------\r\n\r\n### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: macOS 10.13.5 and Debian GNU/Linux 9 (stretch)\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: v1.9.0-rc2-359-g95cfd8b3d9 1.10.0-dev20180711 also reproduces on v1.9.0\r\n- **Python version**: 3.6.5 and 3.5.3\r\n- **Bazel version (if compiling from source)**:\r\n- **GCC/Compiler version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: None\r\n- **GPU model and memory**: None\r\n- **Exact command to reproduce**: see below\r\n\r\nYou can collect some of this information using our environment capture script:\r\n\r\nhttps://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\r\n\r\nYou can obtain the TensorFlow version with\r\n\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\n### Describe the problem\r\n`tf.keras` multi input models don't work when used together with `tf.data.Dataset` due to input broken validation checks. This problem reproduces both on tf@1.9.0 and the latest nightly.\r\n\r\n@fchollet Do you have any ideas what's going on here, or am I missing something obvious?\r\n\r\n### Source code / logs\r\n\r\n#### Multi input model\r\nConsider the following toy model:\r\n```python\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow import keras\r\n\r\ndata_a = np.array([300, 455, 350, 560, 700, 800, 200, 250], dtype=np.float32)\r\nlabels = np.array([455, 350, 560, 700, 800, 200, 250, 300], dtype=np.float32)\r\ndata_b = np.array([200, 255, 350, 470, 600, 300, 344, 322], dtype=np.float32)\r\ndata_a = np.reshape(data_a, (8, 1, 1))\r\ndata_b = np.reshape(data_b, (8, 1, 1))\r\n\r\nx = keras.layers.Input(shape=(1, 1), name='input_x')\r\ny = keras.layers.Input(shape=(1, 1), name='input_y')\r\nadmi = keras.layers.LSTM(40, return_sequences=False)(x)\r\npla = keras.layers.LSTM(40, return_sequences=False)(y)\r\nout = keras.layers.concatenate([admi, pla], axis=-1)\r\noutput = keras.layers.Dense(1, activation='sigmoid')(out)\r\nmodel = keras.models.Model(inputs=[x, y], outputs=output)\r\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\r\n```\r\n\r\n#### Using `numpy` data\r\nWhen fitting using `numpy` data this works as expected when passing a list or dictionary of inputs:\r\n```python\r\nmodel.fit([data_a, data_b], labels, batch_size=2, epochs=10)\r\nmodel.fit({'input_x': data_a, 'input_y': data_b}, labels, batch_size=2, epochs=10)\r\n```\r\n#### Using `tf.data.Dataset.from_tensor_slices` dictionary\r\nWhen trying the same with a `tf.data.Dataset` the following fails due to incorrect input validation:\r\n```python\r\ndataset = tf.data.Dataset.from_tensor_slices(({'input_x': data_a, 'input_y': data_b}, labels)).batch(2).repeat()\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\r\n````\r\n\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nValueError                                Traceback (most recent call last)\r\n<ipython-input-6-d35bacd274cc> in <module>()\r\n      1 dataset = tf.data.Dataset.from_tensor_slices(({'input_x': data_a, 'input_y': data_b}, labels)).batch(2).repeat()\r\n----> 2 model.fit(dataset, epochs=10, steps_per_epoch=4)\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1276         steps_name='steps_per_epoch',\r\n   1277         steps=steps_per_epoch,\r\n-> 1278         validation_split=validation_split)\r\n   1279 \r\n   1280     # Prepare validation data.\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\r\n    915           feed_output_shapes,\r\n    916           check_batch_axis=False,  # Don't enforce the batch size.\r\n--> 917           exception_prefix='target')\r\n    918 \r\n    919       # Generate sample-wise weight values given the `sample_weight` and\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    180                            ': expected ' + names[i] + ' to have ' +\r\n    181                            str(len(shape)) + ' dimensions, but got array '\r\n--> 182                            'with shape ' + str(data_shape))\r\n    183         if not check_batch_axis:\r\n    184           data_shape = data_shape[1:]\r\n\r\nValueError: Error when checking target: expected dense to have 2 dimensions, but got array with shape (None,)\r\n```\r\n\r\n#### Using `tf.data.Dataset.from_generator` dictionary\r\nHowever using the same network together with `tf.data.Dataset.from_generator` works. Probably because less validation is done:\r\n```python\r\ndef generator():\r\n    while True:\r\n        for i in np.random.permutation(8):\r\n            yield {'input_x': data_a[i], 'input_y': data_b[i]}, labels[i]\r\n\r\ndataset = tf.data.Dataset.from_generator(generator, ({'input_x': tf.float32, 'input_y': tf.float32}, tf.float32)).batch(2)\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\r\n```\r\n\r\n#### Using `tf.data.Dataset` tuple\r\nPassing the multi-input as a tuple to the model both datasets generated with `from_tensor_slices` and `from_generator` fail:\r\n```python\r\ndataset = tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(2).repeat()\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\r\n```\r\n```python\r\ndef generator():\r\n    while True:\r\n        for i in np.random.permutation(8):\r\n            yield (data_a[i], data_b[i]), labels[i]\r\n\r\ndataset = tf.data.Dataset.from_generator(generator, ((tf.float32, tf.float32), tf.float32)).batch(2)\r\nmodel.fit(dataset, epochs=10, steps_per_epoch=4)\r\n```\r\n```python-traceback\r\n---------------------------------------------------------------------------\r\nAttributeError                            Traceback (most recent call last)\r\n<ipython-input-7-512a95f0c2a7> in <module>()\r\n      1 dataset = tf.data.Dataset.from_tensor_slices(((data_a, data_b), labels)).batch(2).repeat()\r\n----> 2 model.fit(dataset, epochs=10, steps_per_epoch=4)\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\r\n   1276         steps_name='steps_per_epoch',\r\n   1277         steps=steps_per_epoch,\r\n-> 1278         validation_split=validation_split)\r\n   1279 \r\n   1280     # Prepare validation data.\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py in _standardize_user_data(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\r\n    876         feed_input_shapes,\r\n    877         check_batch_axis=False,  # Don't enforce the batch size.\r\n--> 878         exception_prefix='input')\r\n    879 \r\n    880     if y is not None:\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_input_data(data, names, shapes, check_batch_axis, exception_prefix)\r\n    141     data = data.values if data.__class__.__name__ == 'DataFrame' else data\r\n    142     data = [data]\r\n--> 143   data = [standardize_single_array(x) for x in data]\r\n    144 \r\n    145   if len(data) != len(names):\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in <listcomp>(.0)\r\n    141     data = data.values if data.__class__.__name__ == 'DataFrame' else data\r\n    142     data = [data]\r\n--> 143   data = [standardize_single_array(x) for x in data]\r\n    144 \r\n    145   if len(data) != len(names):\r\n\r\n/usr/local/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py in standardize_single_array(x)\r\n     79   elif tensor_util.is_tensor(x):\r\n     80     return x\r\n---> 81   elif x.ndim == 1:\r\n     82     x = np.expand_dims(x, 1)\r\n     83   return x\r\n\r\nAttributeError: 'tuple' object has no attribute 'ndim'\r\n```"}