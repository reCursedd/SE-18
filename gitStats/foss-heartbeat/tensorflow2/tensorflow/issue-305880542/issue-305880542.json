{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17763", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17763/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17763/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17763/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/17763", "id": 305880542, "node_id": "MDU6SXNzdWUzMDU4ODA1NDI=", "number": 17763, "title": "TensorFlowInferenceInterface: OutOfMemoryError on Android SDK 16", "user": {"login": "Johnson145", "id": 6339078, "node_id": "MDQ6VXNlcjYzMzkwNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6339078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Johnson145", "html_url": "https://github.com/Johnson145", "followers_url": "https://api.github.com/users/Johnson145/followers", "following_url": "https://api.github.com/users/Johnson145/following{/other_user}", "gists_url": "https://api.github.com/users/Johnson145/gists{/gist_id}", "starred_url": "https://api.github.com/users/Johnson145/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Johnson145/subscriptions", "organizations_url": "https://api.github.com/users/Johnson145/orgs", "repos_url": "https://api.github.com/users/Johnson145/repos", "events_url": "https://api.github.com/users/Johnson145/events{/privacy}", "received_events_url": "https://api.github.com/users/Johnson145/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 5, "created_at": "2018-03-16T10:43:02Z", "updated_at": "2018-08-16T18:24:46Z", "closed_at": "2018-08-16T18:24:46Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: No</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Android SDK 16 (4.1.2)</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: compile 'org.tensorflow:tensorflow-android:+</li>\n<li><strong>TensorFlow version (use command below)</strong>: latest</li>\n<li><strong>Python version</strong>: NA</li>\n<li><strong>Bazel version (if compiling from source)</strong>: NA</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: NA</li>\n<li><strong>CUDA/cuDNN version</strong>: NA</li>\n<li><strong>GPU model and memory</strong>: NA</li>\n<li><strong>Exact command to reproduce</strong>: NA</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I found a strange bug using the <em>TensorFlowInferenceInterface</em> on Android SDK 16. When closing the interface once (using <code>inferenceInterface.close()</code>), the next start (using <code>new TensorFlowInferenceInterface()</code>) will cause an OutOfMemoryError, even though there is enough free RAM available. In contrast, there isn't any problem when keeping the interface alive to reuse it. The latter implies wasting resources though. The only way to prevent the OOM seems to be the manual kill of the surrounding process (<code>android.os.Process.killProcess(android.os.Process.myPid());</code>).</p>\n<p>Just to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.</p>\n<p>I found this behavior using an old Samsung Galaxy S2. Couldn't reproduce it on newer Android versions. As SDK 16 is already quite old, I don't know whether anybody is willing to dig deeper into this bug. However, it costs me a lot of time to isolate the problem and I hope the workaround (using a separate process and killing it manually after closing the interface) may at least save somebody else some time.</p>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): No\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Android SDK 16 (4.1.2)\nTensorFlow installed from (source or binary): compile 'org.tensorflow:tensorflow-android:+\nTensorFlow version (use command below): latest\nPython version: NA\nBazel version (if compiling from source): NA\nGCC/Compiler version (if compiling from source): NA\nCUDA/cuDNN version: NA\nGPU model and memory: NA\nExact command to reproduce: NA\n\nDescribe the problem\nI found a strange bug using the TensorFlowInferenceInterface on Android SDK 16. When closing the interface once (using inferenceInterface.close()), the next start (using new TensorFlowInferenceInterface()) will cause an OutOfMemoryError, even though there is enough free RAM available. In contrast, there isn't any problem when keeping the interface alive to reuse it. The latter implies wasting resources though. The only way to prevent the OOM seems to be the manual kill of the surrounding process (android.os.Process.killProcess(android.os.Process.myPid());).\nJust to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.\nI found this behavior using an old Samsung Galaxy S2. Couldn't reproduce it on newer Android versions. As SDK 16 is already quite old, I don't know whether anybody is willing to dig deeper into this bug. However, it costs me a lot of time to isolate the problem and I hope the workaround (using a separate process and killing it manually after closing the interface) may at least save somebody else some time.", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: No\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Android SDK 16 (4.1.2)\r\n- **TensorFlow installed from (source or binary)**: compile 'org.tensorflow:tensorflow-android:+\r\n- **TensorFlow version (use command below)**: latest\r\n- **Python version**: NA\r\n- **Bazel version (if compiling from source)**: NA\r\n- **GCC/Compiler version (if compiling from source)**: NA\r\n- **CUDA/cuDNN version**: NA\r\n- **GPU model and memory**: NA\r\n- **Exact command to reproduce**: NA\r\n\r\n### Describe the problem\r\nI found a strange bug using the _TensorFlowInferenceInterface_ on Android SDK 16. When closing the interface once (using `inferenceInterface.close()`), the next start (using `new TensorFlowInferenceInterface()`) will cause an OutOfMemoryError, even though there is enough free RAM available. In contrast, there isn't any problem when keeping the interface alive to reuse it. The latter implies wasting resources though. The only way to prevent the OOM seems to be the manual kill of the surrounding process (`android.os.Process.killProcess(android.os.Process.myPid());`).\r\n\r\nJust to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.\r\n\r\nI found this behavior using an old Samsung Galaxy S2. Couldn't reproduce it on newer Android versions. As SDK 16 is already quite old, I don't know whether anybody is willing to dig deeper into this bug. However, it costs me a lot of time to isolate the problem and I hope the workaround (using a separate process and killing it manually after closing the interface) may at least save somebody else some time.\r\n"}