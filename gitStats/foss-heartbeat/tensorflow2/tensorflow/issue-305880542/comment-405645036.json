{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405645036", "html_url": "https://github.com/tensorflow/tensorflow/issues/17763#issuecomment-405645036", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17763", "id": 405645036, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTY0NTAzNg==", "user": {"login": "Johnson145", "id": 6339078, "node_id": "MDQ6VXNlcjYzMzkwNzg=", "avatar_url": "https://avatars0.githubusercontent.com/u/6339078?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Johnson145", "html_url": "https://github.com/Johnson145", "followers_url": "https://api.github.com/users/Johnson145/followers", "following_url": "https://api.github.com/users/Johnson145/following{/other_user}", "gists_url": "https://api.github.com/users/Johnson145/gists{/gist_id}", "starred_url": "https://api.github.com/users/Johnson145/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Johnson145/subscriptions", "organizations_url": "https://api.github.com/users/Johnson145/orgs", "repos_url": "https://api.github.com/users/Johnson145/repos", "events_url": "https://api.github.com/users/Johnson145/events{/privacy}", "received_events_url": "https://api.github.com/users/Johnson145/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-17T16:30:23Z", "updated_at": "2018-07-17T16:30:23Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for your reply.</p>\n<blockquote>\n<p>What is the size of your model?</p>\n</blockquote>\n<p>I tried e.g. the Inception v3 model, which is about 90MB.</p>\n<blockquote>\n<p>while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.</p>\n</blockquote>\n<p>As already stated above: Just to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.</p>\n<blockquote>\n<p>Can you try adding android:largeHeap=\"true\" to your manifest?</p>\n</blockquote>\n<p>I'm already using this option, but the error still occurs from time to time.</p>\n<blockquote>\n<p>In the meantime, we'll see if there's any way for TensorFlowInferenceInterface.close() to be more aggressive about releasing any held resources,</p>\n</blockquote>\n<p>Cool, thanks!</p>\n<blockquote>\n<p>Closing the TensorFlowInferenceInterface isn't guaranteed to immediately free all the used memory from the JVM.  [..] though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.</p>\n</blockquote>\n<p>Garbage collection shouldn't be a problem here, should it? I guess it doesn't matter whether the garbage collection has actually freed the resources before I do anything, as long as they have been \"marked to be removed the next time the garbage collection runs\". Just before the OOM is raised, garbage collection will be triggered (automatically) at least once. Just to be sure, I even tried to trigger it manually. However, it fails to actually free the required resources. Maybe it's about segmentation / fragmentation of the free RAM? Doesn't occur on newer Android versions though.</p>", "body_text": "Thanks for your reply.\n\nWhat is the size of your model?\n\nI tried e.g. the Inception v3 model, which is about 90MB.\n\nwhile your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.\n\nAs already stated above: Just to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.\n\nCan you try adding android:largeHeap=\"true\" to your manifest?\n\nI'm already using this option, but the error still occurs from time to time.\n\nIn the meantime, we'll see if there's any way for TensorFlowInferenceInterface.close() to be more aggressive about releasing any held resources,\n\nCool, thanks!\n\nClosing the TensorFlowInferenceInterface isn't guaranteed to immediately free all the used memory from the JVM.  [..] though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.\n\nGarbage collection shouldn't be a problem here, should it? I guess it doesn't matter whether the garbage collection has actually freed the resources before I do anything, as long as they have been \"marked to be removed the next time the garbage collection runs\". Just before the OOM is raised, garbage collection will be triggered (automatically) at least once. Just to be sure, I even tried to trigger it manually. However, it fails to actually free the required resources. Maybe it's about segmentation / fragmentation of the free RAM? Doesn't occur on newer Android versions though.", "body": "Thanks for your reply.\r\n\r\n> What is the size of your model?\r\n\r\nI tried e.g. the Inception v3 model, which is about 90MB.\r\n\r\n> while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.\r\n\r\nAs already stated above: Just to be clear: there is really enough free RAM available. I'm not only referring to the device's RAM in general, but to the individual Java heap of the Android app. I even outsourced the TensorFlow code to a separate process to ensure that it gets its own heap.\r\n\r\n> Can you try adding android:largeHeap=\"true\" to your manifest?\r\n\r\nI'm already using this option, but the error still occurs from time to time.\r\n\r\n> In the meantime, we'll see if there's any way for TensorFlowInferenceInterface.close() to be more aggressive about releasing any held resources,\r\n\r\nCool, thanks!\r\n\r\n> Closing the TensorFlowInferenceInterface isn't guaranteed to immediately free all the used memory from the JVM.  [..] though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.\r\n\r\nGarbage collection shouldn't be a problem here, should it? I guess it doesn't matter whether the garbage collection has actually freed the resources before I do anything, as long as they have been \"marked to be removed the next time the garbage collection runs\". Just before the OOM is raised, garbage collection will be triggered (automatically) at least once. Just to be sure, I even tried to trigger it manually. However, it fails to actually free the required resources. Maybe it's about segmentation / fragmentation of the free RAM? Doesn't occur on newer Android versions though."}