{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/405628528", "html_url": "https://github.com/tensorflow/tensorflow/issues/17763#issuecomment-405628528", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17763", "id": 405628528, "node_id": "MDEyOklzc3VlQ29tbWVudDQwNTYyODUyOA==", "user": {"login": "jdduke", "id": 479117, "node_id": "MDQ6VXNlcjQ3OTExNw==", "avatar_url": "https://avatars2.githubusercontent.com/u/479117?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jdduke", "html_url": "https://github.com/jdduke", "followers_url": "https://api.github.com/users/jdduke/followers", "following_url": "https://api.github.com/users/jdduke/following{/other_user}", "gists_url": "https://api.github.com/users/jdduke/gists{/gist_id}", "starred_url": "https://api.github.com/users/jdduke/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jdduke/subscriptions", "organizations_url": "https://api.github.com/users/jdduke/orgs", "repos_url": "https://api.github.com/users/jdduke/repos", "events_url": "https://api.github.com/users/jdduke/events{/privacy}", "received_events_url": "https://api.github.com/users/jdduke/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-17T15:41:50Z", "updated_at": "2018-07-17T15:41:50Z", "author_association": "MEMBER", "body_html": "<p>What is the size of your model?</p>\n<p>Closing the <code>TensorFlowInferenceInterface</code> isn't guaranteed to immediately free all the used memory from the JVM. Moreover, while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.</p>\n<p>Can you try adding <code>android:largeHeap=\"true\"</code> to your manifest? In the meantime, we'll see if there's any way for <code>TensorFlowInferenceInterface.close()</code> to be more aggressive about releasing any held resources, though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.</p>", "body_text": "What is the size of your model?\nClosing the TensorFlowInferenceInterface isn't guaranteed to immediately free all the used memory from the JVM. Moreover, while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.\nCan you try adding android:largeHeap=\"true\" to your manifest? In the meantime, we'll see if there's any way for TensorFlowInferenceInterface.close() to be more aggressive about releasing any held resources, though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection.", "body": "What is the size of your model?\r\n\r\nClosing the `TensorFlowInferenceInterface` isn't guaranteed to immediately free all the used memory from the JVM. Moreover, while your device might have free memory available, the JVM heap size is typically much smaller than the device's memory limit.\r\n\r\nCan you try adding `android:largeHeap=\"true\"` to your manifest? In the meantime, we'll see if there's any way for `TensorFlowInferenceInterface.close()` to be more aggressive about releasing any held resources, though again it's still not guaranteed to solve the problem due to the unpredictability of garbage collection."}