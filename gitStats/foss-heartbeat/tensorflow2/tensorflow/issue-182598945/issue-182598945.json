{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4917", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4917/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4917/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4917/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/4917", "id": 182598945, "node_id": "MDU6SXNzdWUxODI1OTg5NDU=", "number": 4917, "title": "QueueRunner deadlock when using all CPUs", "user": {"login": "bb4242", "id": 10068296, "node_id": "MDQ6VXNlcjEwMDY4Mjk2", "avatar_url": "https://avatars1.githubusercontent.com/u/10068296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bb4242", "html_url": "https://github.com/bb4242", "followers_url": "https://api.github.com/users/bb4242/followers", "following_url": "https://api.github.com/users/bb4242/following{/other_user}", "gists_url": "https://api.github.com/users/bb4242/gists{/gist_id}", "starred_url": "https://api.github.com/users/bb4242/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bb4242/subscriptions", "organizations_url": "https://api.github.com/users/bb4242/orgs", "repos_url": "https://api.github.com/users/bb4242/repos", "events_url": "https://api.github.com/users/bb4242/events{/privacy}", "received_events_url": "https://api.github.com/users/bb4242/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-10-12T18:29:30Z", "updated_at": "2016-10-12T21:02:55Z", "closed_at": "2016-10-12T21:02:55Z", "author_association": "NONE", "body_html": "<p>I'm building an input pipeline following the guidelines <a href=\"https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html\" rel=\"nofollow\">here</a>.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using <code>tf.py_func</code>), and return the processed results to an output queue.  I'd like to use <code>QueueRunner</code>'s ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">import</span> numpy <span class=\"pl-k\">as</span> np\n<span class=\"pl-k\">import</span> multiprocessing\n<span class=\"pl-k\">import</span> tensorflow <span class=\"pl-k\">as</span> tf\n\nn_cpus <span class=\"pl-k\">=</span> multiprocessing.cpu_count()\n\nsess <span class=\"pl-k\">=</span> tf.Session()\na <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\nb <span class=\"pl-k\">=</span> tf.placeholder(tf.float32)\nmult <span class=\"pl-k\">=</span> tf.mul(a, b)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">python_op</span>(<span class=\"pl-smi\">x</span>):\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>python_op called with <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(x)\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> In my real function, the np.cos and np.sin calls are replaced by</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> python calculations I can't do in tensorflow</span>\n    y <span class=\"pl-k\">=</span> np.cos(x)\n    z <span class=\"pl-k\">=</span> sess.run(mult, <span class=\"pl-v\">feed_dict</span><span class=\"pl-k\">=</span>{a: y, b: x})\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>intermediate result is <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(z)\n    <span class=\"pl-k\">return</span> np.sin(z)\n\nn_inputs <span class=\"pl-k\">=</span> n_cpus\ninput_queue <span class=\"pl-k\">=</span> tf.FIFOQueue(<span class=\"pl-c1\">10000</span>, [tf.float32], <span class=\"pl-v\">shapes</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>])\nload_input <span class=\"pl-k\">=</span> input_queue.enqueue_many(np.random.random((n_inputs, <span class=\"pl-c1\">1</span>)))\n\noutput_queue <span class=\"pl-k\">=</span> tf.FIFOQueue(<span class=\"pl-c1\">10000</span>, [tf.float32], <span class=\"pl-v\">shapes</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>])\nget_result <span class=\"pl-k\">=</span> output_queue.dequeue_many(n_inputs)\n\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">processing_pipeline</span>():\n    input_value <span class=\"pl-k\">=</span> input_queue.dequeue()\n    <span class=\"pl-k\">return</span> output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], <span class=\"pl-c1\">False</span>))\n\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> Here's the problem: If we use all CPUs here, the program will deadlock.</span>\n<span class=\"pl-c\"><span class=\"pl-c\">#</span> If we change cpus to (cpus-1), it works as expected.</span>\nrunner <span class=\"pl-k\">=</span> tf.train.QueueRunner(output_queue, [processing_pipeline()] <span class=\"pl-k\">*</span> (n_cpus))\n\ncoord <span class=\"pl-k\">=</span> tf.train.Coordinator()\nrunner.create_threads(sess, <span class=\"pl-v\">coord</span><span class=\"pl-k\">=</span>coord, <span class=\"pl-v\">start</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Loading input<span class=\"pl-pds\">\"</span></span>\nsess.run(load_input)\nsess.run(input_queue.close())\n\n<span class=\"pl-k\">try</span>:\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>waiting for result<span class=\"pl-pds\">\"</span></span>\n    result <span class=\"pl-k\">=</span> sess.run(get_result)\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>RESULT: <span class=\"pl-c1\">{}</span><span class=\"pl-pds\">\"</span></span>.format(result)\n<span class=\"pl-k\">except</span> tf.errors.OutOfRangeError:\n    <span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Input exhausted<span class=\"pl-pds\">\"</span></span>\n\ncoord.request_stop()\ncoord.join()\n<span class=\"pl-c1\">print</span> <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Done<span class=\"pl-pds\">\"</span></span></pre></div>\n<p>The program above deadlocks waiting for <code>sess.run</code> to complete in <code>python_op</code>:</p>\n<pre><code>$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n</code></pre>\n<p>This is running on an 8-core machine; you can see that 8 <code>python_op</code>s are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing <code>(n_cpus)</code> to <code>(n_cpus-1)</code> in the line that creates the <code>tf.train.QueueRunner</code>, then the program runs to completion:</p>\n<pre><code>$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n</code></pre>\n<p>The program also completes successfully if we pass in fewer examples than CPUs in the input queue.</p>\n<p>I realize it's somewhat awkward for <code>python_op</code> to call back into the tensorflow session.  However, the <a href=\"https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html#threading-and-queues\" rel=\"nofollow\">threading and queues</a> section of the manual states:</p>\n<p>\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"</p>\n<p>So, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?</p>\n<p>As a side note, one option to work around my problems would be to break <code>python_op</code> into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since <code>python_op</code>'s real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.</p>\n<p>OS: Linux<br>\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)</p>", "body_text": "I'm building an input pipeline following the guidelines here.  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using tf.py_func), and return the processed results to an output queue.  I'd like to use QueueRunner's ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\n\nn_cpus = multiprocessing.cpu_count()\n\nsess = tf.Session()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nmult = tf.mul(a, b)\n\ndef python_op(x):\n    print \"python_op called with {}\".format(x)\n    # In my real function, the np.cos and np.sin calls are replaced by\n    # python calculations I can't do in tensorflow\n    y = np.cos(x)\n    z = sess.run(mult, feed_dict={a: y, b: x})\n    print \"intermediate result is {}\".format(z)\n    return np.sin(z)\n\nn_inputs = n_cpus\ninput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nload_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))\n\noutput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nget_result = output_queue.dequeue_many(n_inputs)\n\ndef processing_pipeline():\n    input_value = input_queue.dequeue()\n    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))\n\n# Here's the problem: If we use all CPUs here, the program will deadlock.\n# If we change cpus to (cpus-1), it works as expected.\nrunner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))\n\ncoord = tf.train.Coordinator()\nrunner.create_threads(sess, coord=coord, start=True)\n\nprint \"Loading input\"\nsess.run(load_input)\nsess.run(input_queue.close())\n\ntry:\n    print \"waiting for result\"\n    result = sess.run(get_result)\n    print \"RESULT: {}\".format(result)\nexcept tf.errors.OutOfRangeError:\n    print \"Input exhausted\"\n\ncoord.request_stop()\ncoord.join()\nprint \"Done\"\nThe program above deadlocks waiting for sess.run to complete in python_op:\n$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n\nThis is running on an 8-core machine; you can see that 8 python_ops are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing (n_cpus) to (n_cpus-1) in the line that creates the tf.train.QueueRunner, then the program runs to completion:\n$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n\nThe program also completes successfully if we pass in fewer examples than CPUs in the input queue.\nI realize it's somewhat awkward for python_op to call back into the tensorflow session.  However, the threading and queues section of the manual states:\n\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"\nSo, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?\nAs a side note, one option to work around my problems would be to break python_op into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since python_op's real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.\nOS: Linux\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)", "body": "I'm building an input pipeline following the guidelines [here](https://www.tensorflow.org/versions/r0.11/how_tos/reading_data/index.html).  The pipeline needs to queue up a bunch of input examples, process each example using a python function (inserted into the tensorflow computation graph using `tf.py_func`), and return the processed results to an output queue.  I'd like to use `QueueRunner`'s ability to process multiple examples in parallel by launching one processing thread per CPU core.  Here's a simplified example of what I'm trying to do:\n\n``` python\nimport numpy as np\nimport multiprocessing\nimport tensorflow as tf\n\nn_cpus = multiprocessing.cpu_count()\n\nsess = tf.Session()\na = tf.placeholder(tf.float32)\nb = tf.placeholder(tf.float32)\nmult = tf.mul(a, b)\n\ndef python_op(x):\n    print \"python_op called with {}\".format(x)\n    # In my real function, the np.cos and np.sin calls are replaced by\n    # python calculations I can't do in tensorflow\n    y = np.cos(x)\n    z = sess.run(mult, feed_dict={a: y, b: x})\n    print \"intermediate result is {}\".format(z)\n    return np.sin(z)\n\nn_inputs = n_cpus\ninput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nload_input = input_queue.enqueue_many(np.random.random((n_inputs, 1)))\n\noutput_queue = tf.FIFOQueue(10000, [tf.float32], shapes=[1])\nget_result = output_queue.dequeue_many(n_inputs)\n\ndef processing_pipeline():\n    input_value = input_queue.dequeue()\n    return output_queue.enqueue(tf.py_func(python_op, [input_value], [tf.float32], False))\n\n# Here's the problem: If we use all CPUs here, the program will deadlock.\n# If we change cpus to (cpus-1), it works as expected.\nrunner = tf.train.QueueRunner(output_queue, [processing_pipeline()] * (n_cpus))\n\ncoord = tf.train.Coordinator()\nrunner.create_threads(sess, coord=coord, start=True)\n\nprint \"Loading input\"\nsess.run(load_input)\nsess.run(input_queue.close())\n\ntry:\n    print \"waiting for result\"\n    result = sess.run(get_result)\n    print \"RESULT: {}\".format(result)\nexcept tf.errors.OutOfRangeError:\n    print \"Input exhausted\"\n\ncoord.request_stop()\ncoord.join()\nprint \"Done\"\n```\n\nThe program above deadlocks waiting for `sess.run` to complete in `python_op`:\n\n```\n$ python queuetest.py                                                                                                                                                                                                                                                  \nLoading input\npython_op called with [ 0.65624136]\n python_op called with [ 0.80651367]\npython_op called with [ 0.31998941]\n python_op called with [ 0.726421]\n python_op called with [ 0.33133706]\npython_op called with [ 0.4912357]\npython_op called with [ 0.27365881]\npython_op called with [ 0.32846987]\n```\n\nThis is running on an 8-core machine; you can see that 8 `python_op`s are currently running but are failing to finish.  If, however, we don't use all CPUs (by changing `(n_cpus)` to `(n_cpus-1)` in the line that creates the `tf.train.QueueRunner`, then the program runs to completion:\n\n```\n$ python queuetest.py                                                                                                                                                                                                                                                        \nLoading input\npython_op called with [ 0.40804103]\npython_op called with [ 0.0182138]\npython_op called with [ 0.17579727]\n python_op called with [ 0.29143187]\npython_op called with [ 0.11612369]\n intermediate result is [ 0.37454084]\npython_op called with [ 0.679506]\npython_op called with [ 0.50754625]\nintermediate result is [ 0.01821078]\nintermediate result is [ 0.52857631]\n intermediate result is [ 0.27914321]\nwaiting for result\npython_op called with [ 0.68288684]\n intermediate result is [ 0.44356483]\nintermediate result is [ 0.11534163]\n intermediate result is [ 0.17308778]\nintermediate result is [ 0.52975237]\nRESULT: [[ 0.36584523]\n [ 0.01820978]\n [ 0.50430447]\n [ 0.42916203]\n [ 0.11508605]\n [ 0.27553213]\n [ 0.1722248 ]\n [ 0.50531965]]\nDone\n```\n\nThe program also completes successfully if we pass in fewer examples than CPUs in the input queue.\n\nI realize it's somewhat awkward for `python_op` to call back into the tensorflow session.  However, the [threading and queues](https://www.tensorflow.org/versions/r0.11/how_tos/threading_and_queues/index.html#threading-and-queues) section of the manual states:\n\n\"The TensorFlow Session object is multithreaded, so multiple threads can easily use the same session and run ops in parallel.\"\n\nSo, I assumed that the use case I've outlined above should be supported, and it seems surprising that it works in some cases (fewer threads than CPUs) and doesn't in others (one thread per CPU).  Is this a bug, or is there some reason I shouldn't expect it to work?\n\nAs a side note, one option to work around my problems would be to break `python_op` into several smaller pieces and chain the intermediate results together in the tensorflow computation graph.  However, in my real pipeline, this isn't straightforward to do, since `python_op`'s real output depends on a number of intermediate, native python data structures that aren't easily converted into tensors that could be spliced into the tensorflow graph.\n\nOS: Linux\nTensorflow versions: 0.10.0, 0.11.0-rc0 (CPU-only version)\n"}