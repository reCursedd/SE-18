{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/253335893", "html_url": "https://github.com/tensorflow/tensorflow/issues/4917#issuecomment-253335893", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4917", "id": 253335893, "node_id": "MDEyOklzc3VlQ29tbWVudDI1MzMzNTg5Mw==", "user": {"login": "bb4242", "id": 10068296, "node_id": "MDQ6VXNlcjEwMDY4Mjk2", "avatar_url": "https://avatars1.githubusercontent.com/u/10068296?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bb4242", "html_url": "https://github.com/bb4242", "followers_url": "https://api.github.com/users/bb4242/followers", "following_url": "https://api.github.com/users/bb4242/following{/other_user}", "gists_url": "https://api.github.com/users/bb4242/gists{/gist_id}", "starred_url": "https://api.github.com/users/bb4242/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bb4242/subscriptions", "organizations_url": "https://api.github.com/users/bb4242/orgs", "repos_url": "https://api.github.com/users/bb4242/repos", "events_url": "https://api.github.com/users/bb4242/events{/privacy}", "received_events_url": "https://api.github.com/users/bb4242/received_events", "type": "User", "site_admin": false}, "created_at": "2016-10-12T20:53:21Z", "updated_at": "2016-10-12T20:53:21Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=23068\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yaroslavvb\">@yaroslavvb</a>: Thanks, your suggestion was very illuminating. :)  I can confirm that setting <code>inter_op_parallelism_threads</code> to a number greater than the number of cores, with <code>intra_op_parallelism_threads=1</code>, solves the deadlock.  I think I understand what's going on better now and why my example deadlocks:</p>\n<ol>\n<li>In the default configuration, tensorflow creates a thread pool with N threads, where N = number of cores.  These threads are shared across all ops that want to execute.</li>\n<li>The pipeline I was attempting to run creates N parallel requests to run <code>python_op</code>.  These requests get scheduled onto the thread pool, so that every worker thread is occupied.</li>\n<li>The <code>session.run</code>s inside of each <code>python_op</code> require at least one free thread in order to execute (the required number might be as large as <code>intra_op_parallelism_threads</code>).  Since there aren't any free threads, deadlock occurs.</li>\n<li>Increasing the size of the thread pool (by setting <code>inter_op_parallelism_threads</code> to a larger value) ensures that there are threads available even after N <code>python_op</code>s are running, thus eliminating the deadlock.</li>\n</ol>\n<p>Does that sound right?  If so, I'd say this is working as expected and I'll close the issue.</p>", "body_text": "@yaroslavvb: Thanks, your suggestion was very illuminating. :)  I can confirm that setting inter_op_parallelism_threads to a number greater than the number of cores, with intra_op_parallelism_threads=1, solves the deadlock.  I think I understand what's going on better now and why my example deadlocks:\n\nIn the default configuration, tensorflow creates a thread pool with N threads, where N = number of cores.  These threads are shared across all ops that want to execute.\nThe pipeline I was attempting to run creates N parallel requests to run python_op.  These requests get scheduled onto the thread pool, so that every worker thread is occupied.\nThe session.runs inside of each python_op require at least one free thread in order to execute (the required number might be as large as intra_op_parallelism_threads).  Since there aren't any free threads, deadlock occurs.\nIncreasing the size of the thread pool (by setting inter_op_parallelism_threads to a larger value) ensures that there are threads available even after N python_ops are running, thus eliminating the deadlock.\n\nDoes that sound right?  If so, I'd say this is working as expected and I'll close the issue.", "body": "@yaroslavvb: Thanks, your suggestion was very illuminating. :)  I can confirm that setting `inter_op_parallelism_threads` to a number greater than the number of cores, with `intra_op_parallelism_threads=1`, solves the deadlock.  I think I understand what's going on better now and why my example deadlocks:\n\n1) In the default configuration, tensorflow creates a thread pool with N threads, where N = number of cores.  These threads are shared across all ops that want to execute.\n2) The pipeline I was attempting to run creates N parallel requests to run `python_op`.  These requests get scheduled onto the thread pool, so that every worker thread is occupied.\n3) The `session.run`s inside of each `python_op` require at least one free thread in order to execute (the required number might be as large as `intra_op_parallelism_threads`).  Since there aren't any free threads, deadlock occurs.\n4) Increasing the size of the thread pool (by setting `inter_op_parallelism_threads` to a larger value) ensures that there are threads available even after N `python_op`s are running, thus eliminating the deadlock.\n\nDoes that sound right?  If so, I'd say this is working as expected and I'll close the issue.\n"}