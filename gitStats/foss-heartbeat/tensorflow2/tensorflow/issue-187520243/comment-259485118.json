{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259485118", "html_url": "https://github.com/tensorflow/tensorflow/issues/5416#issuecomment-259485118", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5416", "id": 259485118, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTQ4NTExOA==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-09T18:18:26Z", "updated_at": "2016-11-09T18:19:01Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> and I had a look at some of these traces this morning.  There definitely seems to be a problem here, and I'm not sure when it crept in.</p>\n<p>First, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=192142\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/mrry\">@mrry</a> is correct that the gpu tracer in the open source tree is hardwired to report <code>/gpu:0</code> - this should be a simple fix, which can probably be done at the same time that GPU tracing is enabled for the distributed runtime.</p>\n<p>Getting back to the bug - there does appear to be some unexpected and undesirable serialization of op dispatch on the GPU devices here (e.g. the dispatch of the Conv3D ops on different GPU devices do not appear to run concurrently!).  I'm not 100% sure what is going on, but I do know that there were some changes to the threadpool usage for GPU op dispatch a while ago (to deliberately avoid using multiple threads to enqueue ops on the same GPU stream).</p>\n<p>Also, in the build you are using, the <code>_SINK</code> NoOp (which is the last op to run on each device) calls <code>StreamExecutor::SynchronizeAllActivity</code> to ensure that all work is done before turning off the GPU tracer.  Looking at the implementation, this does actually appear to a) synchronize the CUDA Context and b) calls <code>BlockOnThreadExecutor(background_threads_.get());</code> on an internal threadpool.</p>\n<p>In later builds the position of this call to SyncAll() had been moved slightly - but I don't think this would affect the behaviour (it would make the Timeline look slightly different - in that the _SINK NoOp would not appear to take as long)</p>\n<p>It might be worth trying to explicitly increase the <code>inter-op-parallalelism</code>, (in your <code>ConfigProto</code>) but I suspect the problem is more subtle than this....</p>\n<p>What concerns me is that one of your traces has <strong>all</strong> the ops on GPU1 executing after all the ops on GPU0 have completed.  In this case there is actually a dependency of the first GPU1 Conv3D on a 'read' of a Variable which is assigned to GPU0.  This will actually require a device to device DMA, and involves multiple GPU streams and some CUDA Event synchronization.  None of this is currently visible in the Timeline (and in fact I don't even see a MEMCPYD2D!)   This worries me a little (and I really think that we really need to change the instrumentation for Send/Recv ops so that they are more visible in the Timeline!)</p>", "body_text": "@mrry and I had a look at some of these traces this morning.  There definitely seems to be a problem here, and I'm not sure when it crept in.\nFirst, @mrry is correct that the gpu tracer in the open source tree is hardwired to report /gpu:0 - this should be a simple fix, which can probably be done at the same time that GPU tracing is enabled for the distributed runtime.\nGetting back to the bug - there does appear to be some unexpected and undesirable serialization of op dispatch on the GPU devices here (e.g. the dispatch of the Conv3D ops on different GPU devices do not appear to run concurrently!).  I'm not 100% sure what is going on, but I do know that there were some changes to the threadpool usage for GPU op dispatch a while ago (to deliberately avoid using multiple threads to enqueue ops on the same GPU stream).\nAlso, in the build you are using, the _SINK NoOp (which is the last op to run on each device) calls StreamExecutor::SynchronizeAllActivity to ensure that all work is done before turning off the GPU tracer.  Looking at the implementation, this does actually appear to a) synchronize the CUDA Context and b) calls BlockOnThreadExecutor(background_threads_.get()); on an internal threadpool.\nIn later builds the position of this call to SyncAll() had been moved slightly - but I don't think this would affect the behaviour (it would make the Timeline look slightly different - in that the _SINK NoOp would not appear to take as long)\nIt might be worth trying to explicitly increase the inter-op-parallalelism, (in your ConfigProto) but I suspect the problem is more subtle than this....\nWhat concerns me is that one of your traces has all the ops on GPU1 executing after all the ops on GPU0 have completed.  In this case there is actually a dependency of the first GPU1 Conv3D on a 'read' of a Variable which is assigned to GPU0.  This will actually require a device to device DMA, and involves multiple GPU streams and some CUDA Event synchronization.  None of this is currently visible in the Timeline (and in fact I don't even see a MEMCPYD2D!)   This worries me a little (and I really think that we really need to change the instrumentation for Send/Recv ops so that they are more visible in the Timeline!)", "body": "@mrry and I had a look at some of these traces this morning.  There definitely seems to be a problem here, and I'm not sure when it crept in.\n\nFirst, @mrry is correct that the gpu tracer in the open source tree is hardwired to report `/gpu:0` - this should be a simple fix, which can probably be done at the same time that GPU tracing is enabled for the distributed runtime.\n\nGetting back to the bug - there does appear to be some unexpected and undesirable serialization of op dispatch on the GPU devices here (e.g. the dispatch of the Conv3D ops on different GPU devices do not appear to run concurrently!).  I'm not 100% sure what is going on, but I do know that there were some changes to the threadpool usage for GPU op dispatch a while ago (to deliberately avoid using multiple threads to enqueue ops on the same GPU stream).  \n\nAlso, in the build you are using, the `_SINK` NoOp (which is the last op to run on each device) calls `StreamExecutor::SynchronizeAllActivity` to ensure that all work is done before turning off the GPU tracer.  Looking at the implementation, this does actually appear to a) synchronize the CUDA Context and b) calls `BlockOnThreadExecutor(background_threads_.get());` on an internal threadpool.\n\nIn later builds the position of this call to SyncAll() had been moved slightly - but I don't think this would affect the behaviour (it would make the Timeline look slightly different - in that the _SINK NoOp would not appear to take as long)\n\nIt might be worth trying to explicitly increase the `inter-op-parallalelism`, (in your `ConfigProto`) but I suspect the problem is more subtle than this....\n\nWhat concerns me is that one of your traces has **all** the ops on GPU1 executing after all the ops on GPU0 have completed.  In this case there is actually a dependency of the first GPU1 Conv3D on a 'read' of a Variable which is assigned to GPU0.  This will actually require a device to device DMA, and involves multiple GPU streams and some CUDA Event synchronization.  None of this is currently visible in the Timeline (and in fact I don't even see a MEMCPYD2D!)   This worries me a little (and I really think that we really need to change the instrumentation for Send/Recv ops so that they are more visible in the Timeline!)\n"}