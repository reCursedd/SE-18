{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259596962", "html_url": "https://github.com/tensorflow/tensorflow/issues/5416#issuecomment-259596962", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5416", "id": 259596962, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTU5Njk2Mg==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T04:05:16Z", "updated_at": "2016-11-10T19:00:47Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>The nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.</p>\n</blockquote>\n<p>Yes - you are right.</p>\n<p>The only way to get nice parallelism is if all of the Variable read and Send ops on GPU 0 and the Recv ops on GPU1 issue before any of the Conv ops start getting enqueued on GPU0.  This is a bit of a lottery currently!    (Alternatively, if you keep the Variables on CPU then you probably won't have this issue, but will incur the cost of a separate DMA to each GPU per step)</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a> and I talked about a possible workaround this morning.... but it would require recording a CUDA event for each of the Send ops on the source GPU so that the dest GPU could wait for the appropriate point in the execution stream.  Previously we've considered this too expensive.</p>", "body_text": "The nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.\n\nYes - you are right.\nThe only way to get nice parallelism is if all of the Variable read and Send ops on GPU 0 and the Recv ops on GPU1 issue before any of the Conv ops start getting enqueued on GPU0.  This is a bit of a lottery currently!    (Alternatively, if you keep the Variables on CPU then you probably won't have this issue, but will incur the cost of a separate DMA to each GPU per step)\n@zheng-xq and I talked about a possible workaround this morning.... but it would require recording a CUDA event for each of the Send ops on the source GPU so that the dest GPU could wait for the appropriate point in the execution stream.  Previously we've considered this too expensive.", "body": "> The nodes I am using have 16 CPUs for 8 GPUs, so I assume this problem will not completely go away but will most likely be minimized for now.\n\nYes - you are right.  \n\nThe only way to get nice parallelism is if all of the Variable read and Send ops on GPU 0 and the Recv ops on GPU1 issue before any of the Conv ops start getting enqueued on GPU0.  This is a bit of a lottery currently!    (Alternatively, if you keep the Variables on CPU then you probably won't have this issue, but will incur the cost of a separate DMA to each GPU per step)\n\n@zheng-xq and I talked about a possible workaround this morning.... but it would require recording a CUDA event for each of the Send ops on the source GPU so that the dest GPU could wait for the appropriate point in the execution stream.  Previously we've considered this too expensive.\n"}