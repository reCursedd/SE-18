{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/258917203", "html_url": "https://github.com/tensorflow/tensorflow/issues/5416#issuecomment-258917203", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5416", "id": 258917203, "node_id": "MDEyOklzc3VlQ29tbWVudDI1ODkxNzIwMw==", "user": {"login": "mrry", "id": 192142, "node_id": "MDQ6VXNlcjE5MjE0Mg==", "avatar_url": "https://avatars1.githubusercontent.com/u/192142?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mrry", "html_url": "https://github.com/mrry", "followers_url": "https://api.github.com/users/mrry/followers", "following_url": "https://api.github.com/users/mrry/following{/other_user}", "gists_url": "https://api.github.com/users/mrry/gists{/gist_id}", "starred_url": "https://api.github.com/users/mrry/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mrry/subscriptions", "organizations_url": "https://api.github.com/users/mrry/orgs", "repos_url": "https://api.github.com/users/mrry/repos", "events_url": "https://api.github.com/users/mrry/events{/privacy}", "received_events_url": "https://api.github.com/users/mrry/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-07T18:19:47Z", "updated_at": "2016-11-07T18:19:47Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Thanks for making such a detailed bug report, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=177576\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/raphtown\">@raphtown</a>!</p>\n<p>The first thing I noticed when looking at the timelines is that all of the GPU kernel profiling events <em>appear</em> to be running on <code>/gpu:0</code>... even if they've been issued by the TensorFlow device <code>/gpu:1</code>. However, this seems to be a red herring, because there's a <a href=\"https://github.com/tensorflow/tensorflow/blob/a6b33c30926b7a11a1ec0a18d01e6f8852561376/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L561\"><code>TODO</code> comment in <code>gpu_tracer.cc</code> that reveals that these prefixes are inaccurate with multiple devices</a>. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=11547801\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/prb12\">@prb12</a>, do you have a sense of how easy it would be to plumb the appropriate information through here? (I'm guessing that we need to add some reverse map from device contexts (or some other object that we get in a CUPTI callback) back to TensorFlow devices....)</p>\n<p>The variability in runtime appears to be caused by (usually) <code>/gpu:1</code> issuing its first kernel a long time after <code>/gpu:0</code> starts issuing kernels... and sometimes <code>/gpu:1</code> doesn't get started until <code>/gpu:0</code> is finished (and has sync'd its stream). This suggests to me that there's some mutex (in the stream executor? the GPU driver?) being held, or a single worker thread is running a blocking operation and prevent the other device from issuing kernels. I'm not sure how likely either of these scenarios is... <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=15736910\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/zheng-xq\">@zheng-xq</a>, do you have any ideas?</p>", "body_text": "Thanks for making such a detailed bug report, @raphtown!\nThe first thing I noticed when looking at the timelines is that all of the GPU kernel profiling events appear to be running on /gpu:0... even if they've been issued by the TensorFlow device /gpu:1. However, this seems to be a red herring, because there's a TODO comment in gpu_tracer.cc that reveals that these prefixes are inaccurate with multiple devices. @prb12, do you have a sense of how easy it would be to plumb the appropriate information through here? (I'm guessing that we need to add some reverse map from device contexts (or some other object that we get in a CUPTI callback) back to TensorFlow devices....)\nThe variability in runtime appears to be caused by (usually) /gpu:1 issuing its first kernel a long time after /gpu:0 starts issuing kernels... and sometimes /gpu:1 doesn't get started until /gpu:0 is finished (and has sync'd its stream). This suggests to me that there's some mutex (in the stream executor? the GPU driver?) being held, or a single worker thread is running a blocking operation and prevent the other device from issuing kernels. I'm not sure how likely either of these scenarios is... @zheng-xq, do you have any ideas?", "body": "Thanks for making such a detailed bug report, @raphtown!\n\nThe first thing I noticed when looking at the timelines is that all of the GPU kernel profiling events _appear_ to be running on `/gpu:0`... even if they've been issued by the TensorFlow device `/gpu:1`. However, this seems to be a red herring, because there's a [`TODO` comment in `gpu_tracer.cc` that reveals that these prefixes are inaccurate with multiple devices](https://github.com/tensorflow/tensorflow/blob/a6b33c30926b7a11a1ec0a18d01e6f8852561376/tensorflow/core/common_runtime/gpu/gpu_tracer.cc#L561). @prb12, do you have a sense of how easy it would be to plumb the appropriate information through here? (I'm guessing that we need to add some reverse map from device contexts (or some other object that we get in a CUPTI callback) back to TensorFlow devices....)\n\nThe variability in runtime appears to be caused by (usually) `/gpu:1` issuing its first kernel a long time after `/gpu:0` starts issuing kernels... and sometimes `/gpu:1` doesn't get started until `/gpu:0` is finished (and has sync'd its stream). This suggests to me that there's some mutex (in the stream executor? the GPU driver?) being held, or a single worker thread is running a blocking operation and prevent the other device from issuing kernels. I'm not sure how likely either of these scenarios is... @zheng-xq, do you have any ideas?\n"}