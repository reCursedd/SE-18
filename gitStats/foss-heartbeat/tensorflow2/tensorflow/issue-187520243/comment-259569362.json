{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/259569362", "html_url": "https://github.com/tensorflow/tensorflow/issues/5416#issuecomment-259569362", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/5416", "id": 259569362, "node_id": "MDEyOklzc3VlQ29tbWVudDI1OTU2OTM2Mg==", "user": {"login": "prb12", "id": 11547801, "node_id": "MDQ6VXNlcjExNTQ3ODAx", "avatar_url": "https://avatars1.githubusercontent.com/u/11547801?v=4", "gravatar_id": "", "url": "https://api.github.com/users/prb12", "html_url": "https://github.com/prb12", "followers_url": "https://api.github.com/users/prb12/followers", "following_url": "https://api.github.com/users/prb12/following{/other_user}", "gists_url": "https://api.github.com/users/prb12/gists{/gist_id}", "starred_url": "https://api.github.com/users/prb12/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/prb12/subscriptions", "organizations_url": "https://api.github.com/users/prb12/orgs", "repos_url": "https://api.github.com/users/prb12/repos", "events_url": "https://api.github.com/users/prb12/events{/privacy}", "received_events_url": "https://api.github.com/users/prb12/received_events", "type": "User", "site_admin": false}, "created_at": "2016-11-10T00:44:58Z", "updated_at": "2016-11-10T00:44:58Z", "author_association": "MEMBER", "body_html": "<blockquote>\n<p>Maybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give). The results I gave in my original bug report were with 2 CPUs available total. If I increase this amount to 4 CPUs, then the issue disappears. If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).</p>\n</blockquote>\n<p>I'm glad you mentioned this.  We were looking at your traces earlier today and very bemused as to why the threadpool dispatching GPU ops appeared to be behaving as though there was only 1-2 threads!  (somebody apparently removed the log message which tells us how many worker threads are configured in each threadpool - but it defaults to the number of \"physical\" cores.)</p>\n<p>I can totally understand how this problem occurs with only a couple of threads.  Your workload is very sensitive to the order of op dispatch (due to the way that GPU to GPU memcpys are currently implemented).  If there are only a tiny number of threads available then the executor for one GPU device manages to get all of its ops enqueued on the GPU stream before the other even gets a look in.  The D2D memcpys actually include a stream synchronization - so the destination device of the transfer ends up waiting for all of the previously dispatched ops on the other GPU.</p>\n<p>The exact behavior is non-deterministic, but it takes very little time to enqueue a handful of ops on GPU0, and if all threadpool workers are busy doing anything else for even a millisecond then it's game over for GPU1!</p>\n<p>I think we probably need to improve the D2D transfer code path, but for now I would recommend using at least 4 or 8 cores in a multi-gpu setting (and probably more!)</p>", "body_text": "Maybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give). The results I gave in my original bug report were with 2 CPUs available total. If I increase this amount to 4 CPUs, then the issue disappears. If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).\n\nI'm glad you mentioned this.  We were looking at your traces earlier today and very bemused as to why the threadpool dispatching GPU ops appeared to be behaving as though there was only 1-2 threads!  (somebody apparently removed the log message which tells us how many worker threads are configured in each threadpool - but it defaults to the number of \"physical\" cores.)\nI can totally understand how this problem occurs with only a couple of threads.  Your workload is very sensitive to the order of op dispatch (due to the way that GPU to GPU memcpys are currently implemented).  If there are only a tiny number of threads available then the executor for one GPU device manages to get all of its ops enqueued on the GPU stream before the other even gets a look in.  The D2D memcpys actually include a stream synchronization - so the destination device of the transfer ends up waiting for all of the previously dispatched ops on the other GPU.\nThe exact behavior is non-deterministic, but it takes very little time to enqueue a handful of ops on GPU0, and if all threadpool workers are busy doing anything else for even a millisecond then it's game over for GPU1!\nI think we probably need to improve the D2D transfer code path, but for now I would recommend using at least 4 or 8 cores in a multi-gpu setting (and probably more!)", "body": "> Maybe this will help you debug the issue, but it seems like the number of CPU cores available has a direct impact on if the GPUs execute sequentially or in parallel (I am using the SLURM allocation system which lets me specify how many CPUs to give). The results I gave in my original bug report were with 2 CPUs available total. If I increase this amount to 4 CPUs, then the issue disappears. If I decrease the amount to 1 CPU, then it seems that all runs execute sequentially (worsening the problem).\n\nI'm glad you mentioned this.  We were looking at your traces earlier today and very bemused as to why the threadpool dispatching GPU ops appeared to be behaving as though there was only 1-2 threads!  (somebody apparently removed the log message which tells us how many worker threads are configured in each threadpool - but it defaults to the number of \"physical\" cores.)\n\nI can totally understand how this problem occurs with only a couple of threads.  Your workload is very sensitive to the order of op dispatch (due to the way that GPU to GPU memcpys are currently implemented).  If there are only a tiny number of threads available then the executor for one GPU device manages to get all of its ops enqueued on the GPU stream before the other even gets a look in.  The D2D memcpys actually include a stream synchronization - so the destination device of the transfer ends up waiting for all of the previously dispatched ops on the other GPU.  \n\nThe exact behavior is non-deterministic, but it takes very little time to enqueue a handful of ops on GPU0, and if all threadpool workers are busy doing anything else for even a millisecond then it's game over for GPU1! \n\nI think we probably need to improve the D2D transfer code path, but for now I would recommend using at least 4 or 8 cores in a multi-gpu setting (and probably more!)\n"}