{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19108", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19108/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19108/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/19108/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/19108", "id": 320522283, "node_id": "MDU6SXNzdWUzMjA1MjIyODM=", "number": 19108, "title": "Set the weights in tf.layers with other variables but not as initializers.", "user": {"login": "siavash-khodadadeh", "id": 9966824, "node_id": "MDQ6VXNlcjk5NjY4MjQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/9966824?v=4", "gravatar_id": "", "url": "https://api.github.com/users/siavash-khodadadeh", "html_url": "https://github.com/siavash-khodadadeh", "followers_url": "https://api.github.com/users/siavash-khodadadeh/followers", "following_url": "https://api.github.com/users/siavash-khodadadeh/following{/other_user}", "gists_url": "https://api.github.com/users/siavash-khodadadeh/gists{/gist_id}", "starred_url": "https://api.github.com/users/siavash-khodadadeh/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/siavash-khodadadeh/subscriptions", "organizations_url": "https://api.github.com/users/siavash-khodadadeh/orgs", "repos_url": "https://api.github.com/users/siavash-khodadadeh/repos", "events_url": "https://api.github.com/users/siavash-khodadadeh/events{/privacy}", "received_events_url": "https://api.github.com/users/siavash-khodadadeh/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, {"login": "fchollet", "id": 710255, "node_id": "MDQ6VXNlcjcxMDI1NQ==", "avatar_url": "https://avatars3.githubusercontent.com/u/710255?v=4", "gravatar_id": "", "url": "https://api.github.com/users/fchollet", "html_url": "https://github.com/fchollet", "followers_url": "https://api.github.com/users/fchollet/followers", "following_url": "https://api.github.com/users/fchollet/following{/other_user}", "gists_url": "https://api.github.com/users/fchollet/gists{/gist_id}", "starred_url": "https://api.github.com/users/fchollet/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/fchollet/subscriptions", "organizations_url": "https://api.github.com/users/fchollet/orgs", "repos_url": "https://api.github.com/users/fchollet/repos", "events_url": "https://api.github.com/users/fchollet/events{/privacy}", "received_events_url": "https://api.github.com/users/fchollet/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 7, "created_at": "2018-05-05T16:27:56Z", "updated_at": "2018-11-20T13:27:48Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code</strong>: N/A (There is no example of meta learning with tf.layers)</li>\n<li><strong>OS Platform and Distribution</strong>: Linux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from</strong>: Installed tf-nightly with pip</li>\n<li><strong>TensorFlow version</strong>: v1.8.0-rc1-934-g291d85be42 1.9.0-dev20180426</li>\n<li><strong>Python version</strong>: 3.5.2</li>\n<li><strong>Bazel version</strong>: N/A</li>\n<li><strong>GCC/Compiler version</strong>: N/A</li>\n<li><strong>CUDA/cuDNN version</strong>: N/A</li>\n<li><strong>GPU model and memory</strong>: N/A</li>\n<li><strong>Exact command to reproduce</strong>: N/A</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Nowadays, with advances in deep learning, the researchers sometimes need to set the weights of their model based on some formula (not initialize them but initialize something which calculates that formula) and then use that to backpropagate through the variables which computed that formula. For example in meta learning, I have a model, let's say it is just one layer tf.layers.dense(). I want to compute gradients and then use that gradients to build updated model graph and compute gradients on that. The backpropagation should tell me how should I update my first model weights to adapt well. Look at the code below:</p>\n<h3>Source code / logs</h3>\n<pre><code>train = tf.placeholder(dtype=tf.float32, shape=(None, 4), name='train')\ntrain_out = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='train_out')\n\nvalidation = tf.placeholder(dtype=tf.float32, shape=(None, 4))\nvalidation_out = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n\nwith tf.variable_scope('model'):\n    model_out = tf.layers.dense(train, 1, activation=tf.nn.relu)\n\nwith tf.variable_scope('loss'):\n    loss = tf.square(model_out - train_out)\n\nwith tf.variable_scope('gradients'):\n    optimizer = tf.train.AdamOptimizer()\n    grads = optimizer.compute_gradients(loss)\n\nwith tf.variable_scope('updated_model'):\n    updated_vars = {\n        grad_info[1].name[6:]: grad_info[1] - 0.1 * grad_info[0] \\\n        for grad_info in grads if grad_info[0] is not None\n        }\n\n    # meta_out = tf.nn.relu(tf.matmul(validation, updated_vars['dense/kernel:0']) + updated_vars['dense/bias:0'])\n    meta_out = tf.layers.dense(validation, 1, activation=tf.nn.relu, \nkernel_initializer=updated_vars['dense/kernel:0'], bias_initializer=updated_vars['dense/bias:0'])\n\n\nwith tf.variable_scope('meta_loss'):\n    meta_loss = tf.square(meta_out - validation_out)\n\nwith tf.variable_scope('meta_optimizer'):\n    model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model')\n    meta_optimizer = tf.train.AdamOptimizer()\n    meta_train_op = meta_optimizer.minimize(meta_loss, var_list=model_vars)\n</code></pre>\n<p>In this code, I have commented one line. In that line, I did not use tf.layers but instead implemented what should happen in that layer myself. I think we need to be able to do that with tf.layers as well.<br>\nPlease notice that we cannot use initializer because initializers should be set when we run tf.global_variables_initializer() and have another meaning(Which is to set the value of a variable). Here we do not want to initial those weights of layers with values but just with variables which we calculated in some other way and be able to backpropagate through them.</p>\n<p>I think one way to solve this is to allow creating tf.layers' models with setting kernel and bias directly instead of using initializers. I would be more than happy if I could help with this issue.</p>", "body_text": "System information\n\nHave I written custom code: N/A (There is no example of meta learning with tf.layers)\nOS Platform and Distribution: Linux Ubuntu 16.04\nTensorFlow installed from: Installed tf-nightly with pip\nTensorFlow version: v1.8.0-rc1-934-g291d85be42 1.9.0-dev20180426\nPython version: 3.5.2\nBazel version: N/A\nGCC/Compiler version: N/A\nCUDA/cuDNN version: N/A\nGPU model and memory: N/A\nExact command to reproduce: N/A\n\nDescribe the problem\nNowadays, with advances in deep learning, the researchers sometimes need to set the weights of their model based on some formula (not initialize them but initialize something which calculates that formula) and then use that to backpropagate through the variables which computed that formula. For example in meta learning, I have a model, let's say it is just one layer tf.layers.dense(). I want to compute gradients and then use that gradients to build updated model graph and compute gradients on that. The backpropagation should tell me how should I update my first model weights to adapt well. Look at the code below:\nSource code / logs\ntrain = tf.placeholder(dtype=tf.float32, shape=(None, 4), name='train')\ntrain_out = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='train_out')\n\nvalidation = tf.placeholder(dtype=tf.float32, shape=(None, 4))\nvalidation_out = tf.placeholder(dtype=tf.float32, shape=(None, 1))\n\nwith tf.variable_scope('model'):\n    model_out = tf.layers.dense(train, 1, activation=tf.nn.relu)\n\nwith tf.variable_scope('loss'):\n    loss = tf.square(model_out - train_out)\n\nwith tf.variable_scope('gradients'):\n    optimizer = tf.train.AdamOptimizer()\n    grads = optimizer.compute_gradients(loss)\n\nwith tf.variable_scope('updated_model'):\n    updated_vars = {\n        grad_info[1].name[6:]: grad_info[1] - 0.1 * grad_info[0] \\\n        for grad_info in grads if grad_info[0] is not None\n        }\n\n    # meta_out = tf.nn.relu(tf.matmul(validation, updated_vars['dense/kernel:0']) + updated_vars['dense/bias:0'])\n    meta_out = tf.layers.dense(validation, 1, activation=tf.nn.relu, \nkernel_initializer=updated_vars['dense/kernel:0'], bias_initializer=updated_vars['dense/bias:0'])\n\n\nwith tf.variable_scope('meta_loss'):\n    meta_loss = tf.square(meta_out - validation_out)\n\nwith tf.variable_scope('meta_optimizer'):\n    model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model')\n    meta_optimizer = tf.train.AdamOptimizer()\n    meta_train_op = meta_optimizer.minimize(meta_loss, var_list=model_vars)\n\nIn this code, I have commented one line. In that line, I did not use tf.layers but instead implemented what should happen in that layer myself. I think we need to be able to do that with tf.layers as well.\nPlease notice that we cannot use initializer because initializers should be set when we run tf.global_variables_initializer() and have another meaning(Which is to set the value of a variable). Here we do not want to initial those weights of layers with values but just with variables which we calculated in some other way and be able to backpropagate through them.\nI think one way to solve this is to allow creating tf.layers' models with setting kernel and bias directly instead of using initializers. I would be more than happy if I could help with this issue.", "body": "### System information\r\n- **Have I written custom code**: N/A (There is no example of meta learning with tf.layers)\r\n- **OS Platform and Distribution**: Linux Ubuntu 16.04\r\n- **TensorFlow installed from**: Installed tf-nightly with pip \r\n- **TensorFlow version**: v1.8.0-rc1-934-g291d85be42 1.9.0-dev20180426\r\n- **Python version**: 3.5.2 \r\n- **Bazel version**: N/A\r\n- **GCC/Compiler version**: N/A\r\n- **CUDA/cuDNN version**: N/A\r\n- **GPU model and memory**: N/A\r\n- **Exact command to reproduce**: N/A\r\n\r\n### Describe the problem\r\nNowadays, with advances in deep learning, the researchers sometimes need to set the weights of their model based on some formula (not initialize them but initialize something which calculates that formula) and then use that to backpropagate through the variables which computed that formula. For example in meta learning, I have a model, let's say it is just one layer tf.layers.dense(). I want to compute gradients and then use that gradients to build updated model graph and compute gradients on that. The backpropagation should tell me how should I update my first model weights to adapt well. Look at the code below:\r\n\r\n### Source code / logs\r\n\r\n    train = tf.placeholder(dtype=tf.float32, shape=(None, 4), name='train')\r\n    train_out = tf.placeholder(dtype=tf.float32, shape=(None, 1), name='train_out')\r\n\r\n    validation = tf.placeholder(dtype=tf.float32, shape=(None, 4))\r\n    validation_out = tf.placeholder(dtype=tf.float32, shape=(None, 1))\r\n\r\n    with tf.variable_scope('model'):\r\n        model_out = tf.layers.dense(train, 1, activation=tf.nn.relu)\r\n\r\n    with tf.variable_scope('loss'):\r\n        loss = tf.square(model_out - train_out)\r\n\r\n    with tf.variable_scope('gradients'):\r\n        optimizer = tf.train.AdamOptimizer()\r\n        grads = optimizer.compute_gradients(loss)\r\n\r\n    with tf.variable_scope('updated_model'):\r\n        updated_vars = {\r\n            grad_info[1].name[6:]: grad_info[1] - 0.1 * grad_info[0] \\\r\n            for grad_info in grads if grad_info[0] is not None\r\n            }\r\n\r\n        # meta_out = tf.nn.relu(tf.matmul(validation, updated_vars['dense/kernel:0']) + updated_vars['dense/bias:0'])\r\n        meta_out = tf.layers.dense(validation, 1, activation=tf.nn.relu, \r\n    kernel_initializer=updated_vars['dense/kernel:0'], bias_initializer=updated_vars['dense/bias:0'])\r\n\r\n\r\n    with tf.variable_scope('meta_loss'):\r\n        meta_loss = tf.square(meta_out - validation_out)\r\n\r\n    with tf.variable_scope('meta_optimizer'):\r\n        model_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='model')\r\n        meta_optimizer = tf.train.AdamOptimizer()\r\n        meta_train_op = meta_optimizer.minimize(meta_loss, var_list=model_vars)\r\n\r\n\r\nIn this code, I have commented one line. In that line, I did not use tf.layers but instead implemented what should happen in that layer myself. I think we need to be able to do that with tf.layers as well. \r\nPlease notice that we cannot use initializer because initializers should be set when we run tf.global_variables_initializer() and have another meaning(Which is to set the value of a variable). Here we do not want to initial those weights of layers with values but just with variables which we calculated in some other way and be able to backpropagate through them.\r\n\r\nI think one way to solve this is to allow creating tf.layers' models with setting kernel and bias directly instead of using initializers. I would be more than happy if I could help with this issue."}