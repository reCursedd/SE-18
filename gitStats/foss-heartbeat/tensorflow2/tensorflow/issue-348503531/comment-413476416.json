{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/413476416", "html_url": "https://github.com/tensorflow/tensorflow/issues/21459#issuecomment-413476416", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21459", "id": 413476416, "node_id": "MDEyOklzc3VlQ29tbWVudDQxMzQ3NjQxNg==", "user": {"login": "jackd", "id": 659115, "node_id": "MDQ6VXNlcjY1OTExNQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/659115?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jackd", "html_url": "https://github.com/jackd", "followers_url": "https://api.github.com/users/jackd/followers", "following_url": "https://api.github.com/users/jackd/following{/other_user}", "gists_url": "https://api.github.com/users/jackd/gists{/gist_id}", "starred_url": "https://api.github.com/users/jackd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jackd/subscriptions", "organizations_url": "https://api.github.com/users/jackd/orgs", "repos_url": "https://api.github.com/users/jackd/repos", "events_url": "https://api.github.com/users/jackd/events{/privacy}", "received_events_url": "https://api.github.com/users/jackd/received_events", "type": "User", "site_admin": false}, "created_at": "2018-08-16T09:02:08Z", "updated_at": "2018-08-16T09:02:08Z", "author_association": "NONE", "body_html": "<p>One more update: I've found lowering the learning rate/switching to a simpler optimizer (momentum, from Adam) resolved my issues. Again I find this incredibly surprising, but it lends support to the idea that these networks might not have convergent activations, even if the final inferences are relatively stable... Alternatively I'm completely wrong and there's a bug somewhere. /shrug</p>", "body_text": "One more update: I've found lowering the learning rate/switching to a simpler optimizer (momentum, from Adam) resolved my issues. Again I find this incredibly surprising, but it lends support to the idea that these networks might not have convergent activations, even if the final inferences are relatively stable... Alternatively I'm completely wrong and there's a bug somewhere. /shrug", "body": "One more update: I've found lowering the learning rate/switching to a simpler optimizer (momentum, from Adam) resolved my issues. Again I find this incredibly surprising, but it lends support to the idea that these networks might not have convergent activations, even if the final inferences are relatively stable... Alternatively I'm completely wrong and there's a bug somewhere. /shrug"}