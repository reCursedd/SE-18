{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21459", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21459/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21459/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21459/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21459", "id": 348503531, "node_id": "MDU6SXNzdWUzNDg1MDM1MzE=", "number": 21459, "title": "Changing optimizer of restored network messes up training output", "user": {"login": "cnsmth", "id": 35533890, "node_id": "MDQ6VXNlcjM1NTMzODkw", "avatar_url": "https://avatars3.githubusercontent.com/u/35533890?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cnsmth", "html_url": "https://github.com/cnsmth", "followers_url": "https://api.github.com/users/cnsmth/followers", "following_url": "https://api.github.com/users/cnsmth/following{/other_user}", "gists_url": "https://api.github.com/users/cnsmth/gists{/gist_id}", "starred_url": "https://api.github.com/users/cnsmth/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cnsmth/subscriptions", "organizations_url": "https://api.github.com/users/cnsmth/orgs", "repos_url": "https://api.github.com/users/cnsmth/repos", "events_url": "https://api.github.com/users/cnsmth/events{/privacy}", "received_events_url": "https://api.github.com/users/cnsmth/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "tatianashp", "id": 986732, "node_id": "MDQ6VXNlcjk4NjczMg==", "avatar_url": "https://avatars2.githubusercontent.com/u/986732?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tatianashp", "html_url": "https://github.com/tatianashp", "followers_url": "https://api.github.com/users/tatianashp/followers", "following_url": "https://api.github.com/users/tatianashp/following{/other_user}", "gists_url": "https://api.github.com/users/tatianashp/gists{/gist_id}", "starred_url": "https://api.github.com/users/tatianashp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tatianashp/subscriptions", "organizations_url": "https://api.github.com/users/tatianashp/orgs", "repos_url": "https://api.github.com/users/tatianashp/repos", "events_url": "https://api.github.com/users/tatianashp/events{/privacy}", "received_events_url": "https://api.github.com/users/tatianashp/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 4, "created_at": "2018-08-07T22:02:38Z", "updated_at": "2018-11-14T19:24:52Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li>\n<p><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</p>\n</li>\n<li>\n<p><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nUbuntu 16.04.2 LTS (GNU/Linux 4.4.0-130-generic x86_64)</p>\n</li>\n<li>\n<p><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>:<br>\nN/A</p>\n</li>\n<li>\n<p><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</p>\n</li>\n<li>\n<p><strong>TensorFlow version (use command below)</strong>:<br>\nv1.4.0-rc1-11-g130a514 1.4.0</p>\n</li>\n<li>\n<p><strong>Python version</strong>:<br>\nPython 3.5.2</p>\n</li>\n<li>\n<p><strong>Bazel version (if compiling from source)</strong>:<br>\nN/A</p>\n</li>\n<li>\n<p><strong>GCC/Compiler version (if compiling from source)</strong>:<br>\nN/A</p>\n</li>\n<li>\n<p><strong>CUDA/cuDNN version</strong>:<br>\nCuda compilation tools, release 8.0, V8.0.61</p>\n</li>\n<li>\n<p><strong>GPU model and memory</strong>:<br>\nGeForce GTX 1080 Ti (11GB)</p>\n</li>\n<li>\n<p><strong>Exact command to reproduce</strong>:</p>\n</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>I am trying to train a model to do face detection, using Resnet_v1_50 as a backbone net and restoring it from the checkpoint provided in tf-slim. I am following the paper \"MB-FCN for Face Detection\" in building my network. Initially trained with gradient descent, the model was converging slowly. When the optimizer was changed from Gradient Descent (which ResNet was trained with) to a different optimizer (momentum, adam) there were wildly different outputs, that were nowhere near correct even though the training and validation losses were lower and care was taken to initialize all weights for new variables (from momentum for example). There seems to perhaps be an issue with the tf Saver class (restoring variables goes wrong) or an issue changing the optimizer of a restored network from the one it was originally trained with. I also had this issue when using a different model for text detection (changing optimizer messing with output). Restoring the weights from the fine-tuned model with another optimizer goes wrong.</p>\n<h3>Source code / logs</h3>\n<p>Code while training from ResNet:<br>\nglobal_step = tf.train.get_or_create_global_step()<br>\nwith slim.arg_scope(resnet_v1.resnet_arg_scope()):<br>\nvars_to_restore = slim.get_variables_to_restore(['resnet_v1_50'])</p>\n<pre><code>optimizer = tf.train.GradientDescentOptimizer(0.001)\noptimizer = optimizer.minimize(loss, global_step=global_step) \n\nconfig = tf.ConfigProto(allow_soft_placement=True)\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\noverall_saver = tf.train.Saver(slim.get_variables_to_restore())\n\nwith tf.Session(config=config) as sess:\n    resnet_model_path = 'backbone_net/resnet_v1_50.ckpt'\n    vars_to_initialize = []\n    all_vars = slim.get_variables_to_restore()\n    restored_vars = set(vars_to_restore)\n    for var in all_vars:\n        if var not in restored_vars:\n            vars_to_initialize.append(var)\n\n    restorer = tf.train.Saver(vars_to_restore)\n    restorer.restore(sess, resnet_model_path)\n    sess.run(tf.variables_initializer(vars_to_initialize))\n</code></pre>\n<p>Line to save weights while training:<br>\noverall_saver.save(sess, 'weights/model.ckpt', global_step=global_step)</p>\n<p>Code to restore weights once fine-tuned:<br>\niterator, init_op = create_dataset('test_imgs.csv', is_training=False)<br>\nb1_cls, b2_cls, b1_reg, b2_reg, ratio_h, ratio_w = mbfcn_model(iterator, is_training=False)</p>\n<pre><code>config = tf.ConfigProto(allow_soft_placement=True)\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\nwith tf.Session(config=config) as sess:\n    chkpt = 'weights/'\n    restorer = tf.train.Saver(slim.get_variables_to_restore())\n    restorer.restore(sess, tf.train.latest_checkpoint(chkpt))\n</code></pre>", "body_text": "System information\n\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\n\n\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nUbuntu 16.04.2 LTS (GNU/Linux 4.4.0-130-generic x86_64)\n\n\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nN/A\n\n\nTensorFlow installed from (source or binary):\nbinary\n\n\nTensorFlow version (use command below):\nv1.4.0-rc1-11-g130a514 1.4.0\n\n\nPython version:\nPython 3.5.2\n\n\nBazel version (if compiling from source):\nN/A\n\n\nGCC/Compiler version (if compiling from source):\nN/A\n\n\nCUDA/cuDNN version:\nCuda compilation tools, release 8.0, V8.0.61\n\n\nGPU model and memory:\nGeForce GTX 1080 Ti (11GB)\n\n\nExact command to reproduce:\n\n\nDescribe the problem\nI am trying to train a model to do face detection, using Resnet_v1_50 as a backbone net and restoring it from the checkpoint provided in tf-slim. I am following the paper \"MB-FCN for Face Detection\" in building my network. Initially trained with gradient descent, the model was converging slowly. When the optimizer was changed from Gradient Descent (which ResNet was trained with) to a different optimizer (momentum, adam) there were wildly different outputs, that were nowhere near correct even though the training and validation losses were lower and care was taken to initialize all weights for new variables (from momentum for example). There seems to perhaps be an issue with the tf Saver class (restoring variables goes wrong) or an issue changing the optimizer of a restored network from the one it was originally trained with. I also had this issue when using a different model for text detection (changing optimizer messing with output). Restoring the weights from the fine-tuned model with another optimizer goes wrong.\nSource code / logs\nCode while training from ResNet:\nglobal_step = tf.train.get_or_create_global_step()\nwith slim.arg_scope(resnet_v1.resnet_arg_scope()):\nvars_to_restore = slim.get_variables_to_restore(['resnet_v1_50'])\noptimizer = tf.train.GradientDescentOptimizer(0.001)\noptimizer = optimizer.minimize(loss, global_step=global_step) \n\nconfig = tf.ConfigProto(allow_soft_placement=True)\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\noverall_saver = tf.train.Saver(slim.get_variables_to_restore())\n\nwith tf.Session(config=config) as sess:\n    resnet_model_path = 'backbone_net/resnet_v1_50.ckpt'\n    vars_to_initialize = []\n    all_vars = slim.get_variables_to_restore()\n    restored_vars = set(vars_to_restore)\n    for var in all_vars:\n        if var not in restored_vars:\n            vars_to_initialize.append(var)\n\n    restorer = tf.train.Saver(vars_to_restore)\n    restorer.restore(sess, resnet_model_path)\n    sess.run(tf.variables_initializer(vars_to_initialize))\n\nLine to save weights while training:\noverall_saver.save(sess, 'weights/model.ckpt', global_step=global_step)\nCode to restore weights once fine-tuned:\niterator, init_op = create_dataset('test_imgs.csv', is_training=False)\nb1_cls, b2_cls, b1_reg, b2_reg, ratio_h, ratio_w = mbfcn_model(iterator, is_training=False)\nconfig = tf.ConfigProto(allow_soft_placement=True)\nconfig.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n\nwith tf.Session(config=config) as sess:\n    chkpt = 'weights/'\n    restorer = tf.train.Saver(slim.get_variables_to_restore())\n    restorer.restore(sess, tf.train.latest_checkpoint(chkpt))", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\n Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-130-generic x86_64)\r\n\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**:\r\nN/A\r\n\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n\r\n- **TensorFlow version (use command below)**:\r\nv1.4.0-rc1-11-g130a514 1.4.0\r\n\r\n- **Python version**:\r\nPython 3.5.2\r\n\r\n- **Bazel version (if compiling from source)**:\r\nN/A\r\n\r\n- **GCC/Compiler version (if compiling from source)**:\r\nN/A\r\n\r\n- **CUDA/cuDNN version**:\r\nCuda compilation tools, release 8.0, V8.0.61\r\n\r\n- **GPU model and memory**:\r\nGeForce GTX 1080 Ti (11GB)\r\n\r\n- **Exact command to reproduce**:\r\n\r\n\r\n### Describe the problem\r\nI am trying to train a model to do face detection, using Resnet_v1_50 as a backbone net and restoring it from the checkpoint provided in tf-slim. I am following the paper \"MB-FCN for Face Detection\" in building my network. Initially trained with gradient descent, the model was converging slowly. When the optimizer was changed from Gradient Descent (which ResNet was trained with) to a different optimizer (momentum, adam) there were wildly different outputs, that were nowhere near correct even though the training and validation losses were lower and care was taken to initialize all weights for new variables (from momentum for example). There seems to perhaps be an issue with the tf Saver class (restoring variables goes wrong) or an issue changing the optimizer of a restored network from the one it was originally trained with. I also had this issue when using a different model for text detection (changing optimizer messing with output). Restoring the weights from the fine-tuned model with another optimizer goes wrong.\r\n\r\n### Source code / logs\r\nCode while training from ResNet:\r\n    global_step = tf.train.get_or_create_global_step()\r\n    with slim.arg_scope(resnet_v1.resnet_arg_scope()):\r\n        vars_to_restore = slim.get_variables_to_restore(['resnet_v1_50'])\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(0.001)\r\n    optimizer = optimizer.minimize(loss, global_step=global_step) \r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n    overall_saver = tf.train.Saver(slim.get_variables_to_restore())\r\n\r\n    with tf.Session(config=config) as sess:\r\n        resnet_model_path = 'backbone_net/resnet_v1_50.ckpt'\r\n        vars_to_initialize = []\r\n        all_vars = slim.get_variables_to_restore()\r\n        restored_vars = set(vars_to_restore)\r\n        for var in all_vars:\r\n            if var not in restored_vars:\r\n                vars_to_initialize.append(var)\r\n\r\n        restorer = tf.train.Saver(vars_to_restore)\r\n        restorer.restore(sess, resnet_model_path)\r\n        sess.run(tf.variables_initializer(vars_to_initialize))\r\n\r\nLine to save weights while training:\r\n        overall_saver.save(sess, 'weights/model.ckpt', global_step=global_step)\r\n\r\nCode to restore weights once fine-tuned:\r\n    iterator, init_op = create_dataset('test_imgs.csv', is_training=False)\r\n    b1_cls, b2_cls, b1_reg, b2_reg, ratio_h, ratio_w = mbfcn_model(iterator, is_training=False)\r\n\r\n    config = tf.ConfigProto(allow_soft_placement=True)\r\n    config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\r\n\r\n    with tf.Session(config=config) as sess:\r\n        chkpt = 'weights/'\r\n        restorer = tf.train.Saver(slim.get_variables_to_restore())\r\n        restorer.restore(sess, tf.train.latest_checkpoint(chkpt))\r\n"}