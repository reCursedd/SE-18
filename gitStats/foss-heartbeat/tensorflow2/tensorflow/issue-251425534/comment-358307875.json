{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/358307875", "html_url": "https://github.com/tensorflow/tensorflow/issues/12414#issuecomment-358307875", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12414", "id": 358307875, "node_id": "MDEyOklzc3VlQ29tbWVudDM1ODMwNzg3NQ==", "user": {"login": "maxfiedler", "id": 4192637, "node_id": "MDQ6VXNlcjQxOTI2Mzc=", "avatar_url": "https://avatars1.githubusercontent.com/u/4192637?v=4", "gravatar_id": "", "url": "https://api.github.com/users/maxfiedler", "html_url": "https://github.com/maxfiedler", "followers_url": "https://api.github.com/users/maxfiedler/followers", "following_url": "https://api.github.com/users/maxfiedler/following{/other_user}", "gists_url": "https://api.github.com/users/maxfiedler/gists{/gist_id}", "starred_url": "https://api.github.com/users/maxfiedler/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/maxfiedler/subscriptions", "organizations_url": "https://api.github.com/users/maxfiedler/orgs", "repos_url": "https://api.github.com/users/maxfiedler/repos", "events_url": "https://api.github.com/users/maxfiedler/events{/privacy}", "received_events_url": "https://api.github.com/users/maxfiedler/received_events", "type": "User", "site_admin": false}, "created_at": "2018-01-17T13:42:39Z", "updated_at": "2018-01-17T13:42:59Z", "author_association": "NONE", "body_html": "<p>A few more things I notice (still on TF 1.4).<br>\nI am also getting this Warning</p>\n<pre><code>2018-01-17 13:35:41.716773: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,90,120,3], [?,10800,21]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n</code></pre>\n<p>Plus a <code>tf.errors.OutOfRangeError</code><br>\nwhen I am using a <code>one_shot_iterator</code> without repeat() that runs over my test_set AND a batch size that IS cleanly divides the number of elements in the test data<br>\n(and I am getting way more than double those warnings when running on 2 GPUs instead of 1 GPU)<br>\nRelevant snippet of my code</p>\n<pre><code>self.test_set = self.create_test_dataset()\n        if self.parse_fn:\n            self.test_set = self.test_set.map(self.parse_fn, num_parallel_calls=self.num_parallel_processes)\n        if self.preprocessing_fn:\n            self.test_set = self.test_set.map(self.preprocessing_fn, num_parallel_calls=self.num_parallel_processes)\n        # self.test_set = self.test_set.repeat(1)  # this line is not needed, but makes the behavior more explicit\n        self.test_set = self.test_set.batch(self.batch_size)\n        iterator = self.test_set.make_one_shot_iterator()\n        features, targets = iterator.get_next()\n</code></pre>\n<p>and</p>\n<pre><code>with self._session as sess:\n                while not self._stop_testing:\n                    # Start the actual calculation\n                    loss, accuracy = sess.run(\n                        [self._net.loss, self._net.perf_metrics[0]], feed_dict={\n                            self._net.net_is_training: False})\n</code></pre>\n<p>Is there a way to signal the while loop that the iterator has reached the end of the dataset instead of throwing an OutOfRangeError?</p>\n<p>I am following the \"load on batch (i.e. one get_next() op) and distribute it over the GPUs via tf.split\" scheme. When the last batch is not cleanly splittable I get understandably an tf.InvalidArgumentError.</p>\n<p>But before I get a ton of the following warnings:</p>\n<pre><code>[[Node: split_batch = Split[T=DT_FLOAT, num_split=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](split_batch_1/split_dim, IteratorGetNext)]]\n2018-01-17 14:35:58.198566: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 10) and num_split 3\n</code></pre>", "body_text": "A few more things I notice (still on TF 1.4).\nI am also getting this Warning\n2018-01-17 13:35:41.716773: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,90,120,3], [?,10800,21]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\n\nPlus a tf.errors.OutOfRangeError\nwhen I am using a one_shot_iterator without repeat() that runs over my test_set AND a batch size that IS cleanly divides the number of elements in the test data\n(and I am getting way more than double those warnings when running on 2 GPUs instead of 1 GPU)\nRelevant snippet of my code\nself.test_set = self.create_test_dataset()\n        if self.parse_fn:\n            self.test_set = self.test_set.map(self.parse_fn, num_parallel_calls=self.num_parallel_processes)\n        if self.preprocessing_fn:\n            self.test_set = self.test_set.map(self.preprocessing_fn, num_parallel_calls=self.num_parallel_processes)\n        # self.test_set = self.test_set.repeat(1)  # this line is not needed, but makes the behavior more explicit\n        self.test_set = self.test_set.batch(self.batch_size)\n        iterator = self.test_set.make_one_shot_iterator()\n        features, targets = iterator.get_next()\n\nand\nwith self._session as sess:\n                while not self._stop_testing:\n                    # Start the actual calculation\n                    loss, accuracy = sess.run(\n                        [self._net.loss, self._net.perf_metrics[0]], feed_dict={\n                            self._net.net_is_training: False})\n\nIs there a way to signal the while loop that the iterator has reached the end of the dataset instead of throwing an OutOfRangeError?\nI am following the \"load on batch (i.e. one get_next() op) and distribute it over the GPUs via tf.split\" scheme. When the last batch is not cleanly splittable I get understandably an tf.InvalidArgumentError.\nBut before I get a ton of the following warnings:\n[[Node: split_batch = Split[T=DT_FLOAT, num_split=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](split_batch_1/split_dim, IteratorGetNext)]]\n2018-01-17 14:35:58.198566: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 10) and num_split 3", "body": "A few more things I notice (still on TF 1.4). \r\nI am also getting this Warning\r\n```\r\n2018-01-17 13:35:41.716773: W tensorflow/core/framework/op_kernel.cc:1192] Out of range: End of sequence\r\n\t [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,90,120,3], [?,10800,21]], output_types=[DT_FLOAT, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](OneShotIterator)]]\r\n```\r\nPlus a `tf.errors.OutOfRangeError`\r\nwhen I am using a `one_shot_iterator` without repeat() that runs over my test_set AND a batch size that IS cleanly divides the number of elements in the test data\r\n(and I am getting way more than double those warnings when running on 2 GPUs instead of 1 GPU)\r\nRelevant snippet of my code\r\n```\r\nself.test_set = self.create_test_dataset()\r\n        if self.parse_fn:\r\n            self.test_set = self.test_set.map(self.parse_fn, num_parallel_calls=self.num_parallel_processes)\r\n        if self.preprocessing_fn:\r\n            self.test_set = self.test_set.map(self.preprocessing_fn, num_parallel_calls=self.num_parallel_processes)\r\n        # self.test_set = self.test_set.repeat(1)  # this line is not needed, but makes the behavior more explicit\r\n        self.test_set = self.test_set.batch(self.batch_size)\r\n        iterator = self.test_set.make_one_shot_iterator()\r\n        features, targets = iterator.get_next()\r\n``` \r\nand \r\n```\r\nwith self._session as sess:\r\n                while not self._stop_testing:\r\n                    # Start the actual calculation\r\n                    loss, accuracy = sess.run(\r\n                        [self._net.loss, self._net.perf_metrics[0]], feed_dict={\r\n                            self._net.net_is_training: False})\r\n```\r\nIs there a way to signal the while loop that the iterator has reached the end of the dataset instead of throwing an OutOfRangeError?\r\n\r\n\r\nI am following the \"load on batch (i.e. one get_next() op) and distribute it over the GPUs via tf.split\" scheme. When the last batch is not cleanly splittable I get understandably an tf.InvalidArgumentError.\r\n\r\nBut before I get a ton of the following warnings:\r\n```\r\n[[Node: split_batch = Split[T=DT_FLOAT, num_split=3, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](split_batch_1/split_dim, IteratorGetNext)]]\r\n2018-01-17 14:35:58.198566: W tensorflow/core/framework/op_kernel.cc:1192] Invalid argument: Number of ways to split should evenly divide the split dimension, but got split_dim 0 (size = 10) and num_split 3\r\n```"}