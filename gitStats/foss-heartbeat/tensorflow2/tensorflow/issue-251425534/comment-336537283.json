{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/336537283", "html_url": "https://github.com/tensorflow/tensorflow/issues/12414#issuecomment-336537283", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12414", "id": 336537283, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNjUzNzI4Mw==", "user": {"login": "ybsave", "id": 26417094, "node_id": "MDQ6VXNlcjI2NDE3MDk0", "avatar_url": "https://avatars0.githubusercontent.com/u/26417094?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ybsave", "html_url": "https://github.com/ybsave", "followers_url": "https://api.github.com/users/ybsave/followers", "following_url": "https://api.github.com/users/ybsave/following{/other_user}", "gists_url": "https://api.github.com/users/ybsave/gists{/gist_id}", "starred_url": "https://api.github.com/users/ybsave/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ybsave/subscriptions", "organizations_url": "https://api.github.com/users/ybsave/orgs", "repos_url": "https://api.github.com/users/ybsave/repos", "events_url": "https://api.github.com/users/ybsave/events{/privacy}", "received_events_url": "https://api.github.com/users/ybsave/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-13T18:49:42Z", "updated_at": "2017-10-13T21:40:20Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6838753\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/fumihwh\">@fumihwh</a> Is the <code>.repeat()</code> function only suitable for training? I suppose it's not suitable for testing, as I only want to evaluate once; will the <code>make_one_shot_iterator()</code> only run the data set once even if my dataset size is NOT divisible by my batch size? My test seems that the evaluation would go infinite loops when using <code>.repeat()</code>.</p>\n<p>I encountered the same error as described by previous users during testing but not in training. During evaluation, there are thousands of \" Out of range: End of sequence\" errors (I guess the number of errors are the same as the number of my evaluation samples). But correct evaluation results are still printed out after those errors, and the program did not crash and can still continue training. Anyone knows the reason and how to fix it? Thank you.</p>\n<p>I used estimator, and the input functions are</p>\n<pre><code>def input_fn(mode):\n  \"\"\"Input function which provides a single batch for train or eval.\"\"\"\n  dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames(mode))\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\tdataset = dataset.repeat()\n\n  dataset = dataset.map(lambda value: dataset_parser(value, mode),\n\t\t\t\t\t\tnum_threads=FLAGS.map_threads,\n\t\t\t\t\t\toutput_buffer_size=FLAGS.batch_size)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n\n  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()\n  images, labels = iterator.get_next()\n  return images, labels\n</code></pre>\n<p>My model function is</p>\n<pre><code>_DEVICE_LIST = ['/gpu:0', '/gpu:1']\ndef imagenet_model_fn(features, labels, mode):\n  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\n  tf.summary.image('images', features, max_outputs=6)\n\n  with tf.device('/cpu:0'):\n\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\n\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\n\t\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  global_step = tf.train.get_or_create_global_step()\n\n\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\n\t  boundaries = [\n\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\n\t  values = [\n\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n\t  learning_rate = tf.train.piecewise_constant(\n\t\ttf.cast(global_step, tf.int32), boundaries, values)\n\n\t  # Create a tensor named learning_rate for logging purposes.\n\t  tf.identity(learning_rate, name='learning_rate')\n\t  tf.summary.scalar('learning_rate', learning_rate)\n\n\t  optimizer = tf.train.MomentumOptimizer(\n\t\tlearning_rate=learning_rate,\n\t\tmomentum=_MOMENTUM)\n\n\ttower_grads = []\n\ttower_cross_entropy = []\n\ttower_reg_loss = []\n\ttower_preds = []\n\n\twith tf.variable_scope(tf.get_variable_scope()):\n\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\n\t\t_DEVICE_LIST, split_batch, split_labels)):\n\t\twith tf.device(device):\n\t\t  with tf.name_scope('device_%d' % dev_idx):\n\t\t\tlogits = network(inputs=device_features,\n\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n\n\t\t\ttf.get_variable_scope().reuse_variables()\n\t  \n\t\t\ttower_pred = {\n\t\t\t  'classes': tf.argmax(logits, axis=1),\n\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n\t\t\t}\n\n\t\t\ttower_preds.append(tower_pred)\n\n\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\n\t\t\t  logits=logits, onehot_labels=device_labels)\n\t\t\ttower_cross_entropy.append(cross_entropy)\n\n\t\t\treg_loss = FLAGS.weight_decay / len(_DEVICE_LIST) * tf.add_n(\n\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n\t\t\ttower_reg_loss.append(reg_loss)            \n\n\t\t\tloss = cross_entropy + reg_loss\n\n\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \n\t\t\t  grads = optimizer.compute_gradients(loss)\n\t\t\t  tower_grads.append(grads)\n\n\tpredictions = {\n\t  'classes': tf.concat([p['classes'] for p in tower_preds], axis=0),\n\t  'probabilities':\n\t\t\ttf.concat([p['probabilities'] for p in tower_preds], axis=0)\n\t}\n\n\tif mode == tf.estimator.ModeKeys.PREDICT:\n\t  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)    \n\n\tcross_entropy = tf.add_n(tower_cross_entropy)\n\ttf.identity(cross_entropy, name='cross_entropy')\n\ttf.summary.scalar('cross_entropy', cross_entropy)\n\n\treg_loss = tf.add_n(tower_reg_loss)\n\ttf.summary.scalar('reg_loss', reg_loss)\n\n\tloss = cross_entropy + reg_loss\n\ttf.summary.scalar('total_loss', loss)\n\n\taccuracy = tf.metrics.accuracy(\n\t\t\t  tf.argmax(labels, axis=1), predictions['classes'])\n\tmetrics = {'accuracy': accuracy}\n\t  \n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  tf.identity(accuracy[1], name='train_accuracy')\n\t  tf.summary.scalar('train_accuracy', accuracy[1])\n\t  \n\t  # Batch norm requires update_ops to be added as a train_op dependency.\n\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\t  with tf.control_dependencies(update_ops):\n\t\tgrads = average_gradients(tower_grads)\n\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\n\telse:\n\t  train_op = None\n\t\n\treturn tf.estimator.EstimatorSpec(\n\t  mode=mode,\n\t  predictions=predictions,\n\t  loss=loss,\n\t  train_op=train_op,\n\t  eval_metric_ops=metrics)\n</code></pre>", "body_text": "@fumihwh Is the .repeat() function only suitable for training? I suppose it's not suitable for testing, as I only want to evaluate once; will the make_one_shot_iterator() only run the data set once even if my dataset size is NOT divisible by my batch size? My test seems that the evaluation would go infinite loops when using .repeat().\nI encountered the same error as described by previous users during testing but not in training. During evaluation, there are thousands of \" Out of range: End of sequence\" errors (I guess the number of errors are the same as the number of my evaluation samples). But correct evaluation results are still printed out after those errors, and the program did not crash and can still continue training. Anyone knows the reason and how to fix it? Thank you.\nI used estimator, and the input functions are\ndef input_fn(mode):\n  \"\"\"Input function which provides a single batch for train or eval.\"\"\"\n  dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames(mode))\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\tdataset = dataset.repeat()\n\n  dataset = dataset.map(lambda value: dataset_parser(value, mode),\n\t\t\t\t\t\tnum_threads=FLAGS.map_threads,\n\t\t\t\t\t\toutput_buffer_size=FLAGS.batch_size)\n\n  if mode == tf.estimator.ModeKeys.TRAIN:\n\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n\n  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()\n  images, labels = iterator.get_next()\n  return images, labels\n\nMy model function is\n_DEVICE_LIST = ['/gpu:0', '/gpu:1']\ndef imagenet_model_fn(features, labels, mode):\n  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\n  tf.summary.image('images', features, max_outputs=6)\n\n  with tf.device('/cpu:0'):\n\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\n\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\n\t\n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  global_step = tf.train.get_or_create_global_step()\n\n\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\n\t  boundaries = [\n\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\n\t  values = [\n\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\n\t  learning_rate = tf.train.piecewise_constant(\n\t\ttf.cast(global_step, tf.int32), boundaries, values)\n\n\t  # Create a tensor named learning_rate for logging purposes.\n\t  tf.identity(learning_rate, name='learning_rate')\n\t  tf.summary.scalar('learning_rate', learning_rate)\n\n\t  optimizer = tf.train.MomentumOptimizer(\n\t\tlearning_rate=learning_rate,\n\t\tmomentum=_MOMENTUM)\n\n\ttower_grads = []\n\ttower_cross_entropy = []\n\ttower_reg_loss = []\n\ttower_preds = []\n\n\twith tf.variable_scope(tf.get_variable_scope()):\n\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\n\t\t_DEVICE_LIST, split_batch, split_labels)):\n\t\twith tf.device(device):\n\t\t  with tf.name_scope('device_%d' % dev_idx):\n\t\t\tlogits = network(inputs=device_features,\n\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\n\n\t\t\ttf.get_variable_scope().reuse_variables()\n\t  \n\t\t\ttower_pred = {\n\t\t\t  'classes': tf.argmax(logits, axis=1),\n\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n\t\t\t}\n\n\t\t\ttower_preds.append(tower_pred)\n\n\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\n\t\t\t  logits=logits, onehot_labels=device_labels)\n\t\t\ttower_cross_entropy.append(cross_entropy)\n\n\t\t\treg_loss = FLAGS.weight_decay / len(_DEVICE_LIST) * tf.add_n(\n\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n\t\t\ttower_reg_loss.append(reg_loss)            \n\n\t\t\tloss = cross_entropy + reg_loss\n\n\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \n\t\t\t  grads = optimizer.compute_gradients(loss)\n\t\t\t  tower_grads.append(grads)\n\n\tpredictions = {\n\t  'classes': tf.concat([p['classes'] for p in tower_preds], axis=0),\n\t  'probabilities':\n\t\t\ttf.concat([p['probabilities'] for p in tower_preds], axis=0)\n\t}\n\n\tif mode == tf.estimator.ModeKeys.PREDICT:\n\t  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)    \n\n\tcross_entropy = tf.add_n(tower_cross_entropy)\n\ttf.identity(cross_entropy, name='cross_entropy')\n\ttf.summary.scalar('cross_entropy', cross_entropy)\n\n\treg_loss = tf.add_n(tower_reg_loss)\n\ttf.summary.scalar('reg_loss', reg_loss)\n\n\tloss = cross_entropy + reg_loss\n\ttf.summary.scalar('total_loss', loss)\n\n\taccuracy = tf.metrics.accuracy(\n\t\t\t  tf.argmax(labels, axis=1), predictions['classes'])\n\tmetrics = {'accuracy': accuracy}\n\t  \n\tif mode == tf.estimator.ModeKeys.TRAIN:\n\t  tf.identity(accuracy[1], name='train_accuracy')\n\t  tf.summary.scalar('train_accuracy', accuracy[1])\n\t  \n\t  # Batch norm requires update_ops to be added as a train_op dependency.\n\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\t  with tf.control_dependencies(update_ops):\n\t\tgrads = average_gradients(tower_grads)\n\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\n\telse:\n\t  train_op = None\n\t\n\treturn tf.estimator.EstimatorSpec(\n\t  mode=mode,\n\t  predictions=predictions,\n\t  loss=loss,\n\t  train_op=train_op,\n\t  eval_metric_ops=metrics)", "body": "@fumihwh Is the `.repeat()` function only suitable for training? I suppose it's not suitable for testing, as I only want to evaluate once; will the `make_one_shot_iterator()` only run the data set once even if my dataset size is NOT divisible by my batch size? My test seems that the evaluation would go infinite loops when using `.repeat()`. \r\n\r\nI encountered the same error as described by previous users during testing but not in training. During evaluation, there are thousands of \" Out of range: End of sequence\" errors (I guess the number of errors are the same as the number of my evaluation samples). But correct evaluation results are still printed out after those errors, and the program did not crash and can still continue training. Anyone knows the reason and how to fix it? Thank you.\r\n\r\nI used estimator, and the input functions are\r\n\r\n\tdef input_fn(mode):\r\n\t  \"\"\"Input function which provides a single batch for train or eval.\"\"\"\r\n\t  dataset = tf.contrib.data.Dataset.from_tensor_slices(filenames(mode))\r\n\t  if mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\r\n\t  dataset = dataset.flat_map(tf.contrib.data.TFRecordDataset)\r\n\r\n\t  if mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdataset = dataset.repeat()\r\n\r\n\t  dataset = dataset.map(lambda value: dataset_parser(value, mode),\r\n\t\t\t\t\t\t\tnum_threads=FLAGS.map_threads,\r\n\t\t\t\t\t\t\toutput_buffer_size=FLAGS.batch_size)\r\n\r\n\t  if mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\tdataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\r\n\r\n\t  iterator = dataset.batch(FLAGS.batch_size).make_one_shot_iterator()\r\n\t  images, labels = iterator.get_next()\r\n\t  return images, labels\r\n\r\nMy model function is\r\n\r\n    _DEVICE_LIST = ['/gpu:0', '/gpu:1']\r\n\tdef imagenet_model_fn(features, labels, mode):\r\n\t  \"\"\" Our model_fn for ResNet to be used with our Estimator.\"\"\"\r\n\t  tf.summary.image('images', features, max_outputs=6)\r\n\r\n\t  with tf.device('/cpu:0'):\r\n\t\tsplit_batch = tf.split(features, len(_DEVICE_LIST))\r\n\t\tsplit_labels = tf.split(labels, len(_DEVICE_LIST))\r\n\t\t\r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  global_step = tf.train.get_or_create_global_step()\r\n\r\n\t\t  # Multiply the learning rate by 0.1 at 30, 60, 120, and 150 epochs.\r\n\t\t  boundaries = [\r\n\t\t\tint(batches_per_epoch * epoch) for epoch in [30, 60, 120, 150]]\r\n\t\t  values = [\r\n\t\t\t_INITIAL_LEARNING_RATE * decay for decay in [1, 0.1, 0.01, 1e-3, 1e-4]]\r\n\t\t  learning_rate = tf.train.piecewise_constant(\r\n\t\t\ttf.cast(global_step, tf.int32), boundaries, values)\r\n\r\n\t\t  # Create a tensor named learning_rate for logging purposes.\r\n\t\t  tf.identity(learning_rate, name='learning_rate')\r\n\t\t  tf.summary.scalar('learning_rate', learning_rate)\r\n\r\n\t\t  optimizer = tf.train.MomentumOptimizer(\r\n\t\t\tlearning_rate=learning_rate,\r\n\t\t\tmomentum=_MOMENTUM)\r\n\r\n\t\ttower_grads = []\r\n\t\ttower_cross_entropy = []\r\n\t\ttower_reg_loss = []\r\n\t\ttower_preds = []\r\n\r\n\t\twith tf.variable_scope(tf.get_variable_scope()):\r\n\t\t  for dev_idx, (device, device_features, device_labels) in enumerate(zip(\r\n\t\t\t_DEVICE_LIST, split_batch, split_labels)):\r\n\t\t\twith tf.device(device):\r\n\t\t\t  with tf.name_scope('device_%d' % dev_idx):\r\n\t\t\t\tlogits = network(inputs=device_features,\r\n\t\t\t\t\t\t\t\t is_training=(mode == tf.estimator.ModeKeys.TRAIN))\r\n\r\n\t\t\t\ttf.get_variable_scope().reuse_variables()\r\n\t\t  \r\n\t\t\t\ttower_pred = {\r\n\t\t\t\t  'classes': tf.argmax(logits, axis=1),\r\n\t\t\t\t  'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\r\n\t\t\t\t}\r\n\r\n\t\t\t\ttower_preds.append(tower_pred)\r\n\r\n\t\t\t\tcross_entropy = tf.losses.softmax_cross_entropy(\r\n\t\t\t\t  logits=logits, onehot_labels=device_labels)\r\n\t\t\t\ttower_cross_entropy.append(cross_entropy)\r\n\r\n\t\t\t\treg_loss = FLAGS.weight_decay / len(_DEVICE_LIST) * tf.add_n(\r\n\t\t\t\t  [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\r\n\t\t\t\ttower_reg_loss.append(reg_loss)            \r\n\r\n\t\t\t\tloss = cross_entropy + reg_loss\r\n\r\n\t\t\t\tif mode == tf.estimator.ModeKeys.TRAIN:          \r\n\t\t\t\t  grads = optimizer.compute_gradients(loss)\r\n\t\t\t\t  tower_grads.append(grads)\r\n\r\n\t\tpredictions = {\r\n\t\t  'classes': tf.concat([p['classes'] for p in tower_preds], axis=0),\r\n\t\t  'probabilities':\r\n\t\t\t\ttf.concat([p['probabilities'] for p in tower_preds], axis=0)\r\n\t\t}\r\n\r\n\t\tif mode == tf.estimator.ModeKeys.PREDICT:\r\n\t\t  return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)    \r\n\r\n\t\tcross_entropy = tf.add_n(tower_cross_entropy)\r\n\t\ttf.identity(cross_entropy, name='cross_entropy')\r\n\t\ttf.summary.scalar('cross_entropy', cross_entropy)\r\n\r\n\t\treg_loss = tf.add_n(tower_reg_loss)\r\n\t\ttf.summary.scalar('reg_loss', reg_loss)\r\n\r\n\t\tloss = cross_entropy + reg_loss\r\n\t\ttf.summary.scalar('total_loss', loss)\r\n\r\n\t\taccuracy = tf.metrics.accuracy(\r\n\t\t\t\t  tf.argmax(labels, axis=1), predictions['classes'])\r\n\t\tmetrics = {'accuracy': accuracy}\r\n\t\t  \r\n\t\tif mode == tf.estimator.ModeKeys.TRAIN:\r\n\t\t  tf.identity(accuracy[1], name='train_accuracy')\r\n\t\t  tf.summary.scalar('train_accuracy', accuracy[1])\r\n\t\t  \r\n\t\t  # Batch norm requires update_ops to be added as a train_op dependency.\r\n\t\t  update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\r\n\t\t  with tf.control_dependencies(update_ops):\r\n\t\t\tgrads = average_gradients(tower_grads)\r\n\t\t\ttrain_op = optimizer.apply_gradients(grads, global_step=global_step)\r\n\t\telse:\r\n\t\t  train_op = None\r\n\t\t\r\n\t\treturn tf.estimator.EstimatorSpec(\r\n\t\t  mode=mode,\r\n\t\t  predictions=predictions,\r\n\t\t  loss=loss,\r\n\t\t  train_op=train_op,\r\n\t\t  eval_metric_ops=metrics)"}