{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7030", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7030/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7030/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/7030/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/7030", "id": 202699926, "node_id": "MDU6SXNzdWUyMDI2OTk5MjY=", "number": 7030, "title": "Feature Request: an op that returns bytes_in_use for its device", "user": {"login": "yaroslavvb", "id": 23068, "node_id": "MDQ6VXNlcjIzMDY4", "avatar_url": "https://avatars3.githubusercontent.com/u/23068?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yaroslavvb", "html_url": "https://github.com/yaroslavvb", "followers_url": "https://api.github.com/users/yaroslavvb/followers", "following_url": "https://api.github.com/users/yaroslavvb/following{/other_user}", "gists_url": "https://api.github.com/users/yaroslavvb/gists{/gist_id}", "starred_url": "https://api.github.com/users/yaroslavvb/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yaroslavvb/subscriptions", "organizations_url": "https://api.github.com/users/yaroslavvb/orgs", "repos_url": "https://api.github.com/users/yaroslavvb/repos", "events_url": "https://api.github.com/users/yaroslavvb/events{/privacy}", "received_events_url": "https://api.github.com/users/yaroslavvb/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 299643928, "node_id": "MDU6TGFiZWwyOTk2NDM5Mjg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:contributions%20welcome", "name": "stat:contributions welcome", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 19, "created_at": "2017-01-24T01:25:10Z", "updated_at": "2018-09-13T18:32:26Z", "closed_at": "2018-09-13T18:07:21Z", "author_association": "CONTRIBUTOR", "body_html": "<p>We are trying to optimize some models to fit into TitanX's 12GB of RAM, and it's hard because of lack of transparency in TF available memory.</p>\n<p>What would help this situation is an op that returns amount of bytes on its device when executed. Something similar to what's done in <a href=\"https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L222\">stack_ops</a> for memory-aware heuristics</p>\n<pre><code>DeviceContext* device_ctxt = ctx-&gt;op_device_context();\nauto device = static_cast&lt;tensorflow::Device*&gt;(ctx-&gt;device());\nAllocator* allocator = device-&gt;GetAllocator(alloc_attrs);\nAllocatorStats stats;\nallocator-&gt;GetStats(&amp;stats)\n//  output stats.bytes_in_use\n\n</code></pre>\n<p>This op can be wedged between other ops using control dependencies and used for memory debugging. This is complementary to request in <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"199382238\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/6716\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/6716/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/6716\">#6716</a> because it would account for memory from parallel run calls, variables, persistent tensors.</p>\n<p>I can take this issue if this op fits into TF framework</p>", "body_text": "We are trying to optimize some models to fit into TitanX's 12GB of RAM, and it's hard because of lack of transparency in TF available memory.\nWhat would help this situation is an op that returns amount of bytes on its device when executed. Something similar to what's done in stack_ops for memory-aware heuristics\nDeviceContext* device_ctxt = ctx->op_device_context();\nauto device = static_cast<tensorflow::Device*>(ctx->device());\nAllocator* allocator = device->GetAllocator(alloc_attrs);\nAllocatorStats stats;\nallocator->GetStats(&stats)\n//  output stats.bytes_in_use\n\n\nThis op can be wedged between other ops using control dependencies and used for memory debugging. This is complementary to request in #6716 because it would account for memory from parallel run calls, variables, persistent tensors.\nI can take this issue if this op fits into TF framework", "body": "We are trying to optimize some models to fit into TitanX's 12GB of RAM, and it's hard because of lack of transparency in TF available memory.\r\n\r\nWhat would help this situation is an op that returns amount of bytes on its device when executed. Something similar to what's done in [stack_ops](https://github.com/tensorflow/tensorflow/blob/64edd34ce69b4a8033af5d217cb8894105297d8a/tensorflow/core/kernels/stack_ops.cc#L222) for memory-aware heuristics\r\n\r\n```\r\nDeviceContext* device_ctxt = ctx->op_device_context();\r\nauto device = static_cast<tensorflow::Device*>(ctx->device());\r\nAllocator* allocator = device->GetAllocator(alloc_attrs);\r\nAllocatorStats stats;\r\nallocator->GetStats(&stats)\r\n//  output stats.bytes_in_use\r\n\r\n```\r\n\r\nThis op can be wedged between other ops using control dependencies and used for memory debugging. This is complementary to request in https://github.com/tensorflow/tensorflow/issues/6716 because it would account for memory from parallel run calls, variables, persistent tensors.\r\n\r\nI can take this issue if this op fits into TF framework"}