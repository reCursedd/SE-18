{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/333429888", "html_url": "https://github.com/tensorflow/tensorflow/issues/13438#issuecomment-333429888", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13438", "id": 333429888, "node_id": "MDEyOklzc3VlQ29tbWVudDMzMzQyOTg4OA==", "user": {"login": "FesianXu", "id": 12878858, "node_id": "MDQ6VXNlcjEyODc4ODU4", "avatar_url": "https://avatars3.githubusercontent.com/u/12878858?v=4", "gravatar_id": "", "url": "https://api.github.com/users/FesianXu", "html_url": "https://github.com/FesianXu", "followers_url": "https://api.github.com/users/FesianXu/followers", "following_url": "https://api.github.com/users/FesianXu/following{/other_user}", "gists_url": "https://api.github.com/users/FesianXu/gists{/gist_id}", "starred_url": "https://api.github.com/users/FesianXu/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/FesianXu/subscriptions", "organizations_url": "https://api.github.com/users/FesianXu/orgs", "repos_url": "https://api.github.com/users/FesianXu/repos", "events_url": "https://api.github.com/users/FesianXu/events{/privacy}", "received_events_url": "https://api.github.com/users/FesianXu/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-02T02:50:38Z", "updated_at": "2017-10-02T02:50:38Z", "author_association": "NONE", "body_html": "<p>The LSTM models key code as follow:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">class</span> <span class=\"pl-en\">LSTMmodel</span>(<span class=\"pl-c1\">object</span>):\n    <span class=\"pl-k\">def</span> <span class=\"pl-c1\">__init__</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">is_train</span>, <span class=\"pl-smi\">batch_size</span>, <span class=\"pl-smi\">num_step</span>):\n        <span class=\"pl-c1\">self</span>.num_step <span class=\"pl-k\">=</span> num_step\n        <span class=\"pl-c1\">self</span>.batch_size <span class=\"pl-k\">=</span> batch_size\n\n        <span class=\"pl-c1\">self</span>.input_data <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (batch_size, num_step, n_input))\n        <span class=\"pl-c1\">self</span>.target <span class=\"pl-k\">=</span> tf.placeholder(tf.float32, (batch_size, n_class))\n\n        lstm_cell_1 <span class=\"pl-k\">=</span> tf.contrib.rnn.BasicLSTMCell(front_d_hidden, <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n        lstm_cell_2 <span class=\"pl-k\">=</span> tf.contrib.rnn.BasicLSTMCell(front_d_hidden, <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n        lstm_cell_3 <span class=\"pl-k\">=</span> tf.contrib.rnn.BasicLSTMCell(front_d_hidden, <span class=\"pl-v\">forget_bias</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n\n        <span class=\"pl-k\">if</span> is_train:\n            lstm_cell_1 <span class=\"pl-k\">=</span> tf.contrib.rnn.DropoutWrapper(lstm_cell_1, <span class=\"pl-v\">output_keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n            lstm_cell_2 <span class=\"pl-k\">=</span> tf.contrib.rnn.DropoutWrapper(lstm_cell_2, <span class=\"pl-v\">output_keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n            lstm_cell_3 <span class=\"pl-k\">=</span> tf.contrib.rnn.DropoutWrapper(lstm_cell_3, <span class=\"pl-v\">output_keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1.0</span>)\n        cell <span class=\"pl-k\">=</span> tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2, lstm_cell_3])\n\n        <span class=\"pl-c1\">self</span>.initiate_state <span class=\"pl-k\">=</span> cell.zero_state(batch_size, tf.float32)\n        <span class=\"pl-k\">if</span> is_train:\n            <span class=\"pl-c1\">self</span>.input_data <span class=\"pl-k\">=</span> tf.nn.dropout(<span class=\"pl-c1\">self</span>.input_data, <span class=\"pl-c1\">KEEP_PROB</span>)\n        outputs <span class=\"pl-k\">=</span> []\n        state <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.initiate_state <span class=\"pl-c\"><span class=\"pl-c\">#</span> state variable</span>\n        <span class=\"pl-k\">with</span> tf.variable_scope(<span class=\"pl-s\"><span class=\"pl-pds\">'</span>RNN<span class=\"pl-pds\">'</span></span>):\n            <span class=\"pl-k\">for</span> time_step <span class=\"pl-k\">in</span> <span class=\"pl-c1\">range</span>(num_step):\n                <span class=\"pl-k\">if</span> time_step <span class=\"pl-k\">&gt;</span> <span class=\"pl-c1\">0</span>:\n                    tf.get_variable_scope().reuse_variables()\n                batch_step <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.skeleInputLayer(<span class=\"pl-c1\">self</span>.input_data[:, time_step, :])\n                cells_output, state <span class=\"pl-k\">=</span> cell(batch_step, state)\n                outputs.append(cells_output)\n        last_output <span class=\"pl-k\">=</span> outputs[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n        <span class=\"pl-c1\">self</span>.logits <span class=\"pl-k\">=</span> <span class=\"pl-c1\">self</span>.softmax_output(last_output)\n        <span class=\"pl-c1\">self</span>.cross_entropy <span class=\"pl-k\">=</span> tf.nn.softmax_cross_entropy_with_logits(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.logits, <span class=\"pl-v\">labels</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.target)\n\n        <span class=\"pl-c1\">self</span>.distribution <span class=\"pl-k\">=</span> tf.nn.softmax(<span class=\"pl-v\">logits</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">self</span>.logits)\n        <span class=\"pl-c1\">self</span>.argmax_target <span class=\"pl-k\">=</span> tf.argmax(<span class=\"pl-c1\">self</span>.distribution, <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">1</span>)\n\n        <span class=\"pl-c1\">self</span>.loss <span class=\"pl-k\">=</span> tf.reduce_sum(<span class=\"pl-c1\">self</span>.cross_entropy, <span class=\"pl-v\">name</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>loss<span class=\"pl-pds\">'</span></span>)<span class=\"pl-k\">/</span>batch_size\n        <span class=\"pl-c1\">self</span>.final_state <span class=\"pl-k\">=</span> state\n\n        <span class=\"pl-k\">if</span> <span class=\"pl-k\">not</span> is_train:\n            <span class=\"pl-k\">return</span>\n\n        trainable_variables <span class=\"pl-k\">=</span> tf.trainable_variables()\n        raw_grad <span class=\"pl-k\">=</span> tf.gradients(<span class=\"pl-c1\">self</span>.loss, trainable_variables)\n        grads, _ <span class=\"pl-k\">=</span> tf.clip_by_global_norm(raw_grad, <span class=\"pl-c1\">10</span>)\n        optimizer <span class=\"pl-k\">=</span> tf.train.GradientDescentOptimizer(<span class=\"pl-v\">learning_rate</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">LEARNING_RATE</span>)\n        <span class=\"pl-c1\">self</span>.train_op <span class=\"pl-k\">=</span> optimizer.apply_gradients(<span class=\"pl-c1\">zip</span>(grads, trainable_variables))</pre></div>", "body_text": "The LSTM models key code as follow:\nclass LSTMmodel(object):\n    def __init__(self, is_train, batch_size, num_step):\n        self.num_step = num_step\n        self.batch_size = batch_size\n\n        self.input_data = tf.placeholder(tf.float32, (batch_size, num_step, n_input))\n        self.target = tf.placeholder(tf.float32, (batch_size, n_class))\n\n        lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\n        lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\n        lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\n\n        if is_train:\n            lstm_cell_1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_1, output_keep_prob=1.0)\n            lstm_cell_2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_2, output_keep_prob=1.0)\n            lstm_cell_3 = tf.contrib.rnn.DropoutWrapper(lstm_cell_3, output_keep_prob=1.0)\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2, lstm_cell_3])\n\n        self.initiate_state = cell.zero_state(batch_size, tf.float32)\n        if is_train:\n            self.input_data = tf.nn.dropout(self.input_data, KEEP_PROB)\n        outputs = []\n        state = self.initiate_state # state variable\n        with tf.variable_scope('RNN'):\n            for time_step in range(num_step):\n                if time_step > 0:\n                    tf.get_variable_scope().reuse_variables()\n                batch_step = self.skeleInputLayer(self.input_data[:, time_step, :])\n                cells_output, state = cell(batch_step, state)\n                outputs.append(cells_output)\n        last_output = outputs[-1]\n\n        self.logits = self.softmax_output(last_output)\n        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.target)\n\n        self.distribution = tf.nn.softmax(logits=self.logits)\n        self.argmax_target = tf.argmax(self.distribution, axis=1)\n\n        self.loss = tf.reduce_sum(self.cross_entropy, name='loss')/batch_size\n        self.final_state = state\n\n        if not is_train:\n            return\n\n        trainable_variables = tf.trainable_variables()\n        raw_grad = tf.gradients(self.loss, trainable_variables)\n        grads, _ = tf.clip_by_global_norm(raw_grad, 10)\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))", "body": "The LSTM models key code as follow:\r\n```python\r\nclass LSTMmodel(object):\r\n    def __init__(self, is_train, batch_size, num_step):\r\n        self.num_step = num_step\r\n        self.batch_size = batch_size\r\n\r\n        self.input_data = tf.placeholder(tf.float32, (batch_size, num_step, n_input))\r\n        self.target = tf.placeholder(tf.float32, (batch_size, n_class))\r\n\r\n        lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\r\n        lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\r\n        lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(front_d_hidden, forget_bias=1.0)\r\n\r\n        if is_train:\r\n            lstm_cell_1 = tf.contrib.rnn.DropoutWrapper(lstm_cell_1, output_keep_prob=1.0)\r\n            lstm_cell_2 = tf.contrib.rnn.DropoutWrapper(lstm_cell_2, output_keep_prob=1.0)\r\n            lstm_cell_3 = tf.contrib.rnn.DropoutWrapper(lstm_cell_3, output_keep_prob=1.0)\r\n        cell = tf.contrib.rnn.MultiRNNCell([lstm_cell_1, lstm_cell_2, lstm_cell_3])\r\n\r\n        self.initiate_state = cell.zero_state(batch_size, tf.float32)\r\n        if is_train:\r\n            self.input_data = tf.nn.dropout(self.input_data, KEEP_PROB)\r\n        outputs = []\r\n        state = self.initiate_state # state variable\r\n        with tf.variable_scope('RNN'):\r\n            for time_step in range(num_step):\r\n                if time_step > 0:\r\n                    tf.get_variable_scope().reuse_variables()\r\n                batch_step = self.skeleInputLayer(self.input_data[:, time_step, :])\r\n                cells_output, state = cell(batch_step, state)\r\n                outputs.append(cells_output)\r\n        last_output = outputs[-1]\r\n\r\n        self.logits = self.softmax_output(last_output)\r\n        self.cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.target)\r\n\r\n        self.distribution = tf.nn.softmax(logits=self.logits)\r\n        self.argmax_target = tf.argmax(self.distribution, axis=1)\r\n\r\n        self.loss = tf.reduce_sum(self.cross_entropy, name='loss')/batch_size\r\n        self.final_state = state\r\n\r\n        if not is_train:\r\n            return\r\n\r\n        trainable_variables = tf.trainable_variables()\r\n        raw_grad = tf.gradients(self.loss, trainable_variables)\r\n        grads, _ = tf.clip_by_global_norm(raw_grad, 10)\r\n        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\r\n        self.train_op = optimizer.apply_gradients(zip(grads, trainable_variables))\r\n```"}