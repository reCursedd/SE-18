{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209714672", "pull_request_review_id": 145782516, "id": 209714672, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTcxNDY3Mg==", "diff_hunk": "@@ -83,24 +103,50 @@ def __call__(self, getter, name, trainable, collections, *args, **kwargs):\n             collections=[ops.GraphKeys.LOCAL_VARIABLES],\n             *args,\n             **kwargs)\n-      global_center_variable = variable_scope.variable(\n+      if kwargs['reuse'] == True:\n+        return local_var\n+      global_center_variable = getter(\n           name='%s/%s' % (GLOBAL_VARIABLE_NAME, name),\n-          initial_value=local_var.initialized_value(),\n           trainable=False,\n-          collections=[ops.GraphKeys.GLOBAL_VARIABLES])\n+          collections=[ops.GraphKeys.GLOBAL_VARIABLES],\n+          *args,\n+          **kwargs)\n \n       with ops.device(self._worker_device):\n-        local_center_variable = variable_scope.variable(\n+        local_center_variable = getter(\n             name='%s/%s' % (LOCAL_VARIABLE_NAME, name),\n-            initial_value=local_var.initialized_value(),\n             trainable=False,\n-            collections=[ops.GraphKeys.LOCAL_VARIABLES])\n-\n-      self._local_map[local_var] = local_center_variable\n-      self._global_map[local_var] = global_center_variable\n+            collections=[ops.GraphKeys.LOCAL_VARIABLES],\n+            *args,\n+            **kwargs)\n+      if kwargs['partitioner'] is None:\n+        self._local_map[local_var] = local_center_variable\n+        self._global_map[local_var] = global_center_variable\n+      else:\n+        v_list = list(local_var)\n+        for i in range(len(v_list)):\n+          self._local_map[v_list[i]] \\\n+              = list(local_center_variable)[i]\n+          self._global_map[v_list[i]] \\\n+              = list(global_center_variable)[i]\n       return local_var\n     else:\n-      return getter(name, trainable, collections, *args, **kwargs)\n+      # 1. default to LOCAL_VARIABLES (instead of GLOBAL_VARIABLES)", "path": "tensorflow/contrib/opt/python/training/elastic_average_optimizer.py", "position": null, "original_position": 117, "commit_id": "7d9a839a26b7b801ffc53eff59688672021d6a43", "original_commit_id": "167487ebf7e50e13779fb344038b2002056e9b81", "user": {"login": "weidankong", "id": 42156564, "node_id": "MDQ6VXNlcjQyMTU2NTY0", "avatar_url": "https://avatars2.githubusercontent.com/u/42156564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weidankong", "html_url": "https://github.com/weidankong", "followers_url": "https://api.github.com/users/weidankong/followers", "following_url": "https://api.github.com/users/weidankong/following{/other_user}", "gists_url": "https://api.github.com/users/weidankong/gists{/gist_id}", "starred_url": "https://api.github.com/users/weidankong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weidankong/subscriptions", "organizations_url": "https://api.github.com/users/weidankong/orgs", "repos_url": "https://api.github.com/users/weidankong/repos", "events_url": "https://api.github.com/users/weidankong/events{/privacy}", "received_events_url": "https://api.github.com/users/weidankong/received_events", "type": "User", "site_admin": false}, "body": "Assuming user code was like this in single GPU version:\r\n```\r\nwith tf.variable_scope('model'):\r\n    model = UserModel(...)\r\n```\r\nWhen using ElasticAverageOptimizer, code will be changed to like following:\r\n```\r\nwith tf.device(tf.train.replica_device_setter(...)), \\\r\n        tf.variable_scope('model', custom_getter=custom_getter):\r\n    model = UserModel(...)\r\n```\r\nWorkers train their own local model and communicate with PS to update the 'global_center_variable' model. On single GPU version, variables defaults to GLOBAL_VARIABLES are fine, however for distributed version, they are logically belonging to local workers, so I move them to LOCAL_VARIABLES.\r\nglobal_step is, of course, an exception, as it is shared across workers.\r\nIn case user does have cases similar to global_step, they can put the variables into the GLOBAL_SHARE_VARS collection to make it 'global'.\r\n\r\n", "created_at": "2018-08-13T18:37:17Z", "updated_at": "2018-08-16T18:44:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21486#discussion_r209714672", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21486", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209714672"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21486#discussion_r209714672"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21486"}}, "body_html": "<p>Assuming user code was like this in single GPU version:</p>\n<pre><code>with tf.variable_scope('model'):\n    model = UserModel(...)\n</code></pre>\n<p>When using ElasticAverageOptimizer, code will be changed to like following:</p>\n<pre><code>with tf.device(tf.train.replica_device_setter(...)), \\\n        tf.variable_scope('model', custom_getter=custom_getter):\n    model = UserModel(...)\n</code></pre>\n<p>Workers train their own local model and communicate with PS to update the 'global_center_variable' model. On single GPU version, variables defaults to GLOBAL_VARIABLES are fine, however for distributed version, they are logically belonging to local workers, so I move them to LOCAL_VARIABLES.<br>\nglobal_step is, of course, an exception, as it is shared across workers.<br>\nIn case user does have cases similar to global_step, they can put the variables into the GLOBAL_SHARE_VARS collection to make it 'global'.</p>", "body_text": "Assuming user code was like this in single GPU version:\nwith tf.variable_scope('model'):\n    model = UserModel(...)\n\nWhen using ElasticAverageOptimizer, code will be changed to like following:\nwith tf.device(tf.train.replica_device_setter(...)), \\\n        tf.variable_scope('model', custom_getter=custom_getter):\n    model = UserModel(...)\n\nWorkers train their own local model and communicate with PS to update the 'global_center_variable' model. On single GPU version, variables defaults to GLOBAL_VARIABLES are fine, however for distributed version, they are logically belonging to local workers, so I move them to LOCAL_VARIABLES.\nglobal_step is, of course, an exception, as it is shared across workers.\nIn case user does have cases similar to global_step, they can put the variables into the GLOBAL_SHARE_VARS collection to make it 'global'.", "in_reply_to_id": 209677282}