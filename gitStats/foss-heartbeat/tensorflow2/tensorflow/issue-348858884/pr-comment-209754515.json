{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209754515", "pull_request_review_id": 145830597, "id": 209754515, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIwOTc1NDUxNQ==", "diff_hunk": "@@ -246,6 +296,25 @@ def apply_gradients(self, grads_and_vars, global_step=None, name=None):\n       local_update = state_ops.assign_add(\n           self._local_step, 1, name='local_step_update').op\n \n+    # this is for place the variables created by optimizer to local collection\n+    # e.g., AdamOptimizer will create beta as global variables\n+    def _adjust_optimizer_variable_collection():", "path": "tensorflow/contrib/opt/python/training/elastic_average_optimizer.py", "position": null, "original_position": 170, "commit_id": "7d9a839a26b7b801ffc53eff59688672021d6a43", "original_commit_id": "167487ebf7e50e13779fb344038b2002056e9b81", "user": {"login": "weidankong", "id": 42156564, "node_id": "MDQ6VXNlcjQyMTU2NTY0", "avatar_url": "https://avatars2.githubusercontent.com/u/42156564?v=4", "gravatar_id": "", "url": "https://api.github.com/users/weidankong", "html_url": "https://github.com/weidankong", "followers_url": "https://api.github.com/users/weidankong/followers", "following_url": "https://api.github.com/users/weidankong/following{/other_user}", "gists_url": "https://api.github.com/users/weidankong/gists{/gist_id}", "starred_url": "https://api.github.com/users/weidankong/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/weidankong/subscriptions", "organizations_url": "https://api.github.com/users/weidankong/orgs", "repos_url": "https://api.github.com/users/weidankong/repos", "events_url": "https://api.github.com/users/weidankong/events{/privacy}", "received_events_url": "https://api.github.com/users/weidankong/received_events", "type": "User", "site_admin": false}, "body": "As it's for the variables created by apply_gradients only, I'm considering only take care of the incremental part, instead of do it on the whole GLOBAL_VARIABLES collection. How does this sound to you?", "created_at": "2018-08-13T20:51:38Z", "updated_at": "2018-08-16T18:44:18Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/21486#discussion_r209754515", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21486", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/209754515"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/21486#discussion_r209754515"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/21486"}}, "body_html": "<p>As it's for the variables created by apply_gradients only, I'm considering only take care of the incremental part, instead of do it on the whole GLOBAL_VARIABLES collection. How does this sound to you?</p>", "body_text": "As it's for the variables created by apply_gradients only, I'm considering only take care of the incremental part, instead of do it on the whole GLOBAL_VARIABLES collection. How does this sound to you?", "in_reply_to_id": 209675014}