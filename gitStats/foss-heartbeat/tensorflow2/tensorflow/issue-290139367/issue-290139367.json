{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16255", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16255/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16255/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/16255/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/16255", "id": 290139367, "node_id": "MDU6SXNzdWUyOTAxMzkzNjc=", "number": 16255, "title": "tf.scatter_update Error ", "user": {"login": "oroojlooy", "id": 20797260, "node_id": "MDQ6VXNlcjIwNzk3MjYw", "avatar_url": "https://avatars0.githubusercontent.com/u/20797260?v=4", "gravatar_id": "", "url": "https://api.github.com/users/oroojlooy", "html_url": "https://github.com/oroojlooy", "followers_url": "https://api.github.com/users/oroojlooy/followers", "following_url": "https://api.github.com/users/oroojlooy/following{/other_user}", "gists_url": "https://api.github.com/users/oroojlooy/gists{/gist_id}", "starred_url": "https://api.github.com/users/oroojlooy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/oroojlooy/subscriptions", "organizations_url": "https://api.github.com/users/oroojlooy/orgs", "repos_url": "https://api.github.com/users/oroojlooy/repos", "events_url": "https://api.github.com/users/oroojlooy/events{/privacy}", "received_events_url": "https://api.github.com/users/oroojlooy/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2018-01-19T23:39:34Z", "updated_at": "2018-01-26T18:31:42Z", "closed_at": "2018-01-26T18:31:42Z", "author_association": "NONE", "body_html": "<p>Hi,</p>\n<p>I use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient.<br>\nBecause of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine.<br>\nHowever, I am getting error:<br>\nLookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate).<br>\nHere are the lines of the code that I update AO and AS that gives the error:</p>\n<p>self.players[k-1].AS = tf.scatter_update(self.players[k-1].AS,<br>\nself.curTime + leadTimeIn,<br>\ntf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )</p>\n<p>self.players[k+1].AO = tf.scatter_update(self.players[k+1].AO,<br>\nself.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime],<br>\nself.players[k].actionValue(self.curTime, self.playType))<br>\n, name='handle_scat_j')</p>\n<p>Since both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here?<br>\nIs it something that you can fix it, or is there any reason behind this behavior?</p>\n<p>BTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory.</p>\n<p>Thanks,<br>\nAfshin</p>", "body_text": "Hi,\nI use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient.\nBecause of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine.\nHowever, I am getting error:\nLookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate).\nHere are the lines of the code that I update AO and AS that gives the error:\nself.players[k-1].AS = tf.scatter_update(self.players[k-1].AS,\nself.curTime + leadTimeIn,\ntf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )\nself.players[k+1].AO = tf.scatter_update(self.players[k+1].AO,\nself.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime],\nself.players[k].actionValue(self.curTime, self.playType))\n, name='handle_scat_j')\nSince both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here?\nIs it something that you can fix it, or is there any reason behind this behavior?\nBTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory.\nThanks,\nAfshin", "body": "Hi, \r\n\r\nI use tf.scatter_update to update non-trainable variables AS and AO in a code. As I found, when one uses scatter_update, the gradient misses, so that there is no gradient. \r\nBecause of that I set both AL and AO as non-trainable variables (actually they are non-trainable), called the optimizer: tf.train.AdamOptimizer(config.actor_lr0,0.9,0.999,1e-8).minimize(actor_loss), and I thought everything should be fine. \r\nHowever, I am getting error:\r\nLookupError: No gradient defined for operation 'actor/encoder/beer_game_flow_8/next_scat_j_2' (op type: ScatterUpdate). \r\nHere are the lines of the code that I update AO and AS that gives the error:\r\n\r\nself.players[k-1].AS = tf.scatter_update(self.players[k-1].AS, \r\n                    self.curTime + leadTimeIn, \r\n                    tf.add(self.players[k-1].AS[self.curTime + leadTimeIn], possible_shipment), name='next_scat_j' )                   \r\n\r\nself.players[k+1].AO = tf.scatter_update(self.players[k+1].AO, \r\n                    self.curTime + leadTime, tf.add(self.players[k+1].AO[self.curTime + leadTime], \r\n                    self.players[k].actionValue(self.curTime, self.playType))\r\n                    , name='handle_scat_j')\r\n\r\nSince both AS and AO are non-trainable, I do not need their gradient, and AS and AO are the only variable in this op. So, I was wondering why TensorFlow want to obtain the gradient, since there is no trainable variable here? \r\nIs it something that you can fix it, or is there any reason behind this behavior? \r\n\r\nBTW, I use python 2.7 with tf 1.4.0 on Debian 8.7 with a K80 with 12GB of memory. \r\n\r\nThanks, \r\nAfshin\r\n"}