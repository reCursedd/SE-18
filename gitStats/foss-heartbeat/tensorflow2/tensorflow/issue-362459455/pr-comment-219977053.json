{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/219977053", "pull_request_review_id": 158280566, "id": 219977053, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIxOTk3NzA1Mw==", "diff_hunk": "@@ -251,8 +283,67 @@ tensorflow::Status TRTOptimizationPass::Optimize(\n   cp.is_dyn_op = is_dynamic_op_;\n   cp.cached_engine_batches = batches_;\n   cp.max_cached_engines = max_cached_batches_;\n+\n+  if (save_input_graph_) {", "path": "tensorflow/contrib/tensorrt/convert/trt_optimization_pass.cc", "position": 51, "original_position": 51, "commit_id": "eaf7de7c21be2c85f07fde7fc69962d1fcf95766", "original_commit_id": "2b2ca341fd600917639aa0a07bf82e812acefff5", "user": {"login": "samikama", "id": 10539540, "node_id": "MDQ6VXNlcjEwNTM5NTQw", "avatar_url": "https://avatars0.githubusercontent.com/u/10539540?v=4", "gravatar_id": "", "url": "https://api.github.com/users/samikama", "html_url": "https://github.com/samikama", "followers_url": "https://api.github.com/users/samikama/followers", "following_url": "https://api.github.com/users/samikama/following{/other_user}", "gists_url": "https://api.github.com/users/samikama/gists{/gist_id}", "starred_url": "https://api.github.com/users/samikama/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/samikama/subscriptions", "organizations_url": "https://api.github.com/users/samikama/orgs", "repos_url": "https://api.github.com/users/samikama/repos", "events_url": "https://api.github.com/users/samikama/events{/privacy}", "received_events_url": "https://api.github.com/users/samikama/received_events", "type": "User", "site_admin": false}, "body": "These are not the same. with trt.create_inference_graph, user has no control over which optimizers can be run. It is always fixed. Even if it is made configurable, above command will potentially return a slightly different graph than that will be executed. With these options, input and output graphs can be retrieved regardless of the position of TRTOptimizer in the optimization chain and closest to be executed.", "created_at": "2018-09-24T20:26:49Z", "updated_at": "2018-09-25T00:07:20Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22435#discussion_r219977053", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22435", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/219977053"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22435#discussion_r219977053"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22435"}}, "body_html": "<p>These are not the same. with trt.create_inference_graph, user has no control over which optimizers can be run. It is always fixed. Even if it is made configurable, above command will potentially return a slightly different graph than that will be executed. With these options, input and output graphs can be retrieved regardless of the position of TRTOptimizer in the optimization chain and closest to be executed.</p>", "body_text": "These are not the same. with trt.create_inference_graph, user has no control over which optimizers can be run. It is always fixed. Even if it is made configurable, above command will potentially return a slightly different graph than that will be executed. With these options, input and output graphs can be retrieved regardless of the position of TRTOptimizer in the optimization chain and closest to be executed.", "in_reply_to_id": 219886687}