{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/437849283", "html_url": "https://github.com/tensorflow/tensorflow/issues/17356#issuecomment-437849283", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17356", "id": 437849283, "node_id": "MDEyOklzc3VlQ29tbWVudDQzNzg0OTI4Mw==", "user": {"login": "seabiscuit08", "id": 5235318, "node_id": "MDQ6VXNlcjUyMzUzMTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/5235318?v=4", "gravatar_id": "", "url": "https://api.github.com/users/seabiscuit08", "html_url": "https://github.com/seabiscuit08", "followers_url": "https://api.github.com/users/seabiscuit08/followers", "following_url": "https://api.github.com/users/seabiscuit08/following{/other_user}", "gists_url": "https://api.github.com/users/seabiscuit08/gists{/gist_id}", "starred_url": "https://api.github.com/users/seabiscuit08/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/seabiscuit08/subscriptions", "organizations_url": "https://api.github.com/users/seabiscuit08/orgs", "repos_url": "https://api.github.com/users/seabiscuit08/repos", "events_url": "https://api.github.com/users/seabiscuit08/events{/privacy}", "received_events_url": "https://api.github.com/users/seabiscuit08/received_events", "type": "User", "site_admin": false}, "created_at": "2018-11-12T11:39:06Z", "updated_at": "2018-11-12T11:39:06Z", "author_association": "NONE", "body_html": "<blockquote>\n<p>Yes, you can indeed. tf.estimator allows to handle it, then you can handle the multiple objectives however you want in your model_fn. See simple example bellow (taken from the TF website and modified for multi-dimension labels):</p>\n<pre><code>import urllib\n\nimport numpy as np\nimport tensorflow as tf\n\n\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n\nIRIS_TEST = \"iris_test.csv\"\nIRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n\n# If the training and test sets aren't stored locally, download them.\nif not os.path.exists(IRIS_TRAINING):\n  raw = urllib.urlopen(IRIS_TRAINING_URL).read()\n  with open(IRIS_TRAINING, \"w\") as f:\n    f.write(raw)\n\nif not os.path.exists(IRIS_TEST):\n  raw = urllib.urlopen(IRIS_TEST_URL).read()\n  with open(IRIS_TEST, \"w\") as f:\n    f.write(raw)\n\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n    filename=IRIS_TRAINING,\n    target_dtype=np.int,\n    features_dtype=np.float32)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n    filename=IRIS_TEST,\n    target_dtype=np.int,\n    features_dtype=np.float32)\n\ndef build_estimator(\n  model_dir,\n  hidden_units,\n  feature_columns,\n  n_classes,\n  output_dim):\n  \n\n  def _model_fn(features, labels, mode, params):\n    \"\"\"Model function for Estimator.\"\"\"\n    \n    # One-hot encoding on targets\n    print 'label shapes', [labels[l].shape for l in labels]\n    \n    labels = labels['y1']\n    targets = tf.one_hot(labels, n_classes)\n\n    # Connect the first hidden layer to input layer\n    # (features[\"x\"]) with relu activation\n    first_hidden_layer = tf.layers.dense(features[\"x\"], 10, activation=tf.nn.relu)\n    # Connect the second hidden layer to first hidden layer with relu\n    second_hidden_layer = tf.layers.dense(\n        first_hidden_layer, 20, activation=tf.nn.relu)\n    \n    # Connect the third hidden layer to second hidden layer with relu\n    third_hidden_layer = tf.layers.dense(\n        second_hidden_layer, 20, activation=tf.nn.relu)\n\n    # Connect the output layer to third hidden layer (no activation fn)\n    output_layer = tf.layers.dense(third_hidden_layer, n_classes)\n\n    # Reshape output layer to 1-dim Tensor to return predictions\n    predictions = tf.argmax(output_layer, axis=1)\n\n    # Provide an estimator spec for `ModeKeys.PREDICT`.\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return tf.estimator.EstimatorSpec(\n          mode=mode,\n          predictions={\"predictions\": predictions})\n\n    # Calculate loss using mean squared error\n    loss = tf.losses.softmax_cross_entropy(targets, output_layer)\n\n    # Calculate root mean squared error as additional eval metric\n    eval_metric_ops = {\n        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions)\n    }\n    \n    optimizer = tf.train.GradientDescentOptimizer(\n        learning_rate=0.1)\n    train_op = optimizer.minimize(\n        loss=loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops)\n\n  return tf.estimator.Estimator(\n    model_fn=_model_fn)\n\n# Specify that all features have real-value data\nfeature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nestimator = build_estimator# (#\nclassifier = estimator(feature_columns=feature_columns,\n                                        hidden_units=[10, 20, 10],\n                                        n_classes=3,\n                                        output_dim=2,\n                                        model_dir=\"/tmp/iris_model\")\n# Define the training inputs\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": np.array(training_set.data)},\n    y={\n        \"y1\": np.array(training_set.target),\n        \"y2\": np.stack(\n            [np.array(training_set.target) for i in xrange(2)],\n            axis=1)\n    },\n    num_epochs=None,\n    shuffle=True)\n\n# Train model.\nclassifier.train(input_fn=train_input_fn, steps=2000)\n</code></pre>\n</blockquote>\n<p>Hello,I see this is for training label for y1,but how can it train y2?Since the EstimatorSpec takes only one loss argument?</p>", "body_text": "Yes, you can indeed. tf.estimator allows to handle it, then you can handle the multiple objectives however you want in your model_fn. See simple example bellow (taken from the TF website and modified for multi-dimension labels):\nimport urllib\n\nimport numpy as np\nimport tensorflow as tf\n\n\n# Data sets\nIRIS_TRAINING = \"iris_training.csv\"\nIRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n\nIRIS_TEST = \"iris_test.csv\"\nIRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n\n# If the training and test sets aren't stored locally, download them.\nif not os.path.exists(IRIS_TRAINING):\n  raw = urllib.urlopen(IRIS_TRAINING_URL).read()\n  with open(IRIS_TRAINING, \"w\") as f:\n    f.write(raw)\n\nif not os.path.exists(IRIS_TEST):\n  raw = urllib.urlopen(IRIS_TEST_URL).read()\n  with open(IRIS_TEST, \"w\") as f:\n    f.write(raw)\n\n# Load datasets.\ntraining_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n    filename=IRIS_TRAINING,\n    target_dtype=np.int,\n    features_dtype=np.float32)\ntest_set = tf.contrib.learn.datasets.base.load_csv_with_header(\n    filename=IRIS_TEST,\n    target_dtype=np.int,\n    features_dtype=np.float32)\n\ndef build_estimator(\n  model_dir,\n  hidden_units,\n  feature_columns,\n  n_classes,\n  output_dim):\n  \n\n  def _model_fn(features, labels, mode, params):\n    \"\"\"Model function for Estimator.\"\"\"\n    \n    # One-hot encoding on targets\n    print 'label shapes', [labels[l].shape for l in labels]\n    \n    labels = labels['y1']\n    targets = tf.one_hot(labels, n_classes)\n\n    # Connect the first hidden layer to input layer\n    # (features[\"x\"]) with relu activation\n    first_hidden_layer = tf.layers.dense(features[\"x\"], 10, activation=tf.nn.relu)\n    # Connect the second hidden layer to first hidden layer with relu\n    second_hidden_layer = tf.layers.dense(\n        first_hidden_layer, 20, activation=tf.nn.relu)\n    \n    # Connect the third hidden layer to second hidden layer with relu\n    third_hidden_layer = tf.layers.dense(\n        second_hidden_layer, 20, activation=tf.nn.relu)\n\n    # Connect the output layer to third hidden layer (no activation fn)\n    output_layer = tf.layers.dense(third_hidden_layer, n_classes)\n\n    # Reshape output layer to 1-dim Tensor to return predictions\n    predictions = tf.argmax(output_layer, axis=1)\n\n    # Provide an estimator spec for `ModeKeys.PREDICT`.\n    if mode == tf.estimator.ModeKeys.PREDICT:\n      return tf.estimator.EstimatorSpec(\n          mode=mode,\n          predictions={\"predictions\": predictions})\n\n    # Calculate loss using mean squared error\n    loss = tf.losses.softmax_cross_entropy(targets, output_layer)\n\n    # Calculate root mean squared error as additional eval metric\n    eval_metric_ops = {\n        \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions)\n    }\n    \n    optimizer = tf.train.GradientDescentOptimizer(\n        learning_rate=0.1)\n    train_op = optimizer.minimize(\n        loss=loss, global_step=tf.train.get_global_step())\n\n    # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n    return tf.estimator.EstimatorSpec(\n        mode=mode,\n        loss=loss,\n        train_op=train_op,\n        eval_metric_ops=eval_metric_ops)\n\n  return tf.estimator.Estimator(\n    model_fn=_model_fn)\n\n# Specify that all features have real-value data\nfeature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\n\n# Build 3 layer DNN with 10, 20, 10 units respectively.\nestimator = build_estimator# (#\nclassifier = estimator(feature_columns=feature_columns,\n                                        hidden_units=[10, 20, 10],\n                                        n_classes=3,\n                                        output_dim=2,\n                                        model_dir=\"/tmp/iris_model\")\n# Define the training inputs\ntrain_input_fn = tf.estimator.inputs.numpy_input_fn(\n    x={\"x\": np.array(training_set.data)},\n    y={\n        \"y1\": np.array(training_set.target),\n        \"y2\": np.stack(\n            [np.array(training_set.target) for i in xrange(2)],\n            axis=1)\n    },\n    num_epochs=None,\n    shuffle=True)\n\n# Train model.\nclassifier.train(input_fn=train_input_fn, steps=2000)\n\n\nHello,I see this is for training label for y1,but how can it train y2?Since the EstimatorSpec takes only one loss argument?", "body": "> Yes, you can indeed. tf.estimator allows to handle it, then you can handle the multiple objectives however you want in your model_fn. See simple example bellow (taken from the TF website and modified for multi-dimension labels):\r\n> \r\n> ```\r\n> import urllib\r\n> \r\n> import numpy as np\r\n> import tensorflow as tf\r\n> \r\n> \r\n> # Data sets\r\n> IRIS_TRAINING = \"iris_training.csv\"\r\n> IRIS_TRAINING_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\r\n> \r\n> IRIS_TEST = \"iris_test.csv\"\r\n> IRIS_TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\r\n> \r\n> # If the training and test sets aren't stored locally, download them.\r\n> if not os.path.exists(IRIS_TRAINING):\r\n>   raw = urllib.urlopen(IRIS_TRAINING_URL).read()\r\n>   with open(IRIS_TRAINING, \"w\") as f:\r\n>     f.write(raw)\r\n> \r\n> if not os.path.exists(IRIS_TEST):\r\n>   raw = urllib.urlopen(IRIS_TEST_URL).read()\r\n>   with open(IRIS_TEST, \"w\") as f:\r\n>     f.write(raw)\r\n> \r\n> # Load datasets.\r\n> training_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n>     filename=IRIS_TRAINING,\r\n>     target_dtype=np.int,\r\n>     features_dtype=np.float32)\r\n> test_set = tf.contrib.learn.datasets.base.load_csv_with_header(\r\n>     filename=IRIS_TEST,\r\n>     target_dtype=np.int,\r\n>     features_dtype=np.float32)\r\n> \r\n> def build_estimator(\r\n>   model_dir,\r\n>   hidden_units,\r\n>   feature_columns,\r\n>   n_classes,\r\n>   output_dim):\r\n>   \r\n> \r\n>   def _model_fn(features, labels, mode, params):\r\n>     \"\"\"Model function for Estimator.\"\"\"\r\n>     \r\n>     # One-hot encoding on targets\r\n>     print 'label shapes', [labels[l].shape for l in labels]\r\n>     \r\n>     labels = labels['y1']\r\n>     targets = tf.one_hot(labels, n_classes)\r\n> \r\n>     # Connect the first hidden layer to input layer\r\n>     # (features[\"x\"]) with relu activation\r\n>     first_hidden_layer = tf.layers.dense(features[\"x\"], 10, activation=tf.nn.relu)\r\n>     # Connect the second hidden layer to first hidden layer with relu\r\n>     second_hidden_layer = tf.layers.dense(\r\n>         first_hidden_layer, 20, activation=tf.nn.relu)\r\n>     \r\n>     # Connect the third hidden layer to second hidden layer with relu\r\n>     third_hidden_layer = tf.layers.dense(\r\n>         second_hidden_layer, 20, activation=tf.nn.relu)\r\n> \r\n>     # Connect the output layer to third hidden layer (no activation fn)\r\n>     output_layer = tf.layers.dense(third_hidden_layer, n_classes)\r\n> \r\n>     # Reshape output layer to 1-dim Tensor to return predictions\r\n>     predictions = tf.argmax(output_layer, axis=1)\r\n> \r\n>     # Provide an estimator spec for `ModeKeys.PREDICT`.\r\n>     if mode == tf.estimator.ModeKeys.PREDICT:\r\n>       return tf.estimator.EstimatorSpec(\r\n>           mode=mode,\r\n>           predictions={\"predictions\": predictions})\r\n> \r\n>     # Calculate loss using mean squared error\r\n>     loss = tf.losses.softmax_cross_entropy(targets, output_layer)\r\n> \r\n>     # Calculate root mean squared error as additional eval metric\r\n>     eval_metric_ops = {\r\n>         \"accuracy\": tf.metrics.accuracy(labels=labels, predictions=predictions)\r\n>     }\r\n>     \r\n>     optimizer = tf.train.GradientDescentOptimizer(\r\n>         learning_rate=0.1)\r\n>     train_op = optimizer.minimize(\r\n>         loss=loss, global_step=tf.train.get_global_step())\r\n> \r\n>     # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\r\n>     return tf.estimator.EstimatorSpec(\r\n>         mode=mode,\r\n>         loss=loss,\r\n>         train_op=train_op,\r\n>         eval_metric_ops=eval_metric_ops)\r\n> \r\n>   return tf.estimator.Estimator(\r\n>     model_fn=_model_fn)\r\n> \r\n> # Specify that all features have real-value data\r\n> feature_columns = [tf.feature_column.numeric_column(\"x\", shape=[4])]\r\n> \r\n> # Build 3 layer DNN with 10, 20, 10 units respectively.\r\n> estimator = build_estimator# (#\r\n> classifier = estimator(feature_columns=feature_columns,\r\n>                                         hidden_units=[10, 20, 10],\r\n>                                         n_classes=3,\r\n>                                         output_dim=2,\r\n>                                         model_dir=\"/tmp/iris_model\")\r\n> # Define the training inputs\r\n> train_input_fn = tf.estimator.inputs.numpy_input_fn(\r\n>     x={\"x\": np.array(training_set.data)},\r\n>     y={\r\n>         \"y1\": np.array(training_set.target),\r\n>         \"y2\": np.stack(\r\n>             [np.array(training_set.target) for i in xrange(2)],\r\n>             axis=1)\r\n>     },\r\n>     num_epochs=None,\r\n>     shuffle=True)\r\n> \r\n> # Train model.\r\n> classifier.train(input_fn=train_input_fn, steps=2000)\r\n> ```\r\n\r\nHello,I see this is for training label for y1,but how can it train y2?Since the EstimatorSpec takes only one loss argument?"}