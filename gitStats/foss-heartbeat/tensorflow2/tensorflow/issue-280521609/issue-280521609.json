{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15215", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15215/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15215/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/15215/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/15215", "id": 280521609, "node_id": "MDU6SXNzdWUyODA1MjE2MDk=", "number": 15215, "title": "Tensorflow - Unable to import frozen graph with batchnorm : uninitialized value batch_normalization/moving_mean", "user": {"login": "cyrilP-qs", "id": 18574755, "node_id": "MDQ6VXNlcjE4NTc0NzU1", "avatar_url": "https://avatars0.githubusercontent.com/u/18574755?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cyrilP-qs", "html_url": "https://github.com/cyrilP-qs", "followers_url": "https://api.github.com/users/cyrilP-qs/followers", "following_url": "https://api.github.com/users/cyrilP-qs/following{/other_user}", "gists_url": "https://api.github.com/users/cyrilP-qs/gists{/gist_id}", "starred_url": "https://api.github.com/users/cyrilP-qs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cyrilP-qs/subscriptions", "organizations_url": "https://api.github.com/users/cyrilP-qs/orgs", "repos_url": "https://api.github.com/users/cyrilP-qs/repos", "events_url": "https://api.github.com/users/cyrilP-qs/events{/privacy}", "received_events_url": "https://api.github.com/users/cyrilP-qs/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 3, "created_at": "2017-12-08T15:19:04Z", "updated_at": "2017-12-11T21:15:08Z", "closed_at": "2017-12-11T21:15:08Z", "author_association": "NONE", "body_html": "<p>I am trying to freeze in a pbtxt file a checkpoint containing batchnorm layers (ubuntu, python 2.7, tf 1.1.0).</p>\n<p>context :<br>\n<strong>Have I written custom code</strong><br>\nYes, see below</p>\n<p><strong>OS Platform and Distribution</strong><br>\nDocker with Ubuntu 14.04</p>\n<p><strong>TensorFlow installed from</strong><br>\npip installer</p>\n<p><strong>TensorFlow version</strong><br>\ntensorflow and tensorflow-gpu 1.1.0</p>\n<p><strong>Bazel version</strong><br>\nN/A</p>\n<p><strong>CUDA/cuDNN version</strong><br>\nCuda 8, CUDNN 5.1</p>\n<p><strong>GPU model and memory</strong><br>\nNvidia titan-x * 2, 12Go Ram each</p>\n<p><strong>Exact command to reproduce</strong></p>\n<p>For this, following these posts and issues :</p>\n<p><a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"207912226\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/davidsandberg/facenet/issues/161\" data-hovercard-type=\"issue\" data-hovercard-url=\"/davidsandberg/facenet/issues/161/hovercard\" href=\"https://github.com/davidsandberg/facenet/issues/161\">davidsandberg/facenet#161</a></p>\n<p><a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/davidsandberg/facenet/commit/0f3ece502550714c91056f3a8630ce8c037f613f/hovercard\" href=\"https://github.com/davidsandberg/facenet/commit/0f3ece502550714c91056f3a8630ce8c037f613f\">davidsandberg/facenet@<tt>0f3ece5</tt></a></p>\n<p>I use this function:</p>\n<pre><code>freeze_and_prune_graph(model_path_and_name, output_file=None):\n\t\"\"\"\n\tfreezes a model trained and saved by the trainer by :\n\t    - extracting the trainable variables between input_node and output_node\n\t    - turning them to constants\n\t    - changing the 1rst dim of input_node to None\n\t    -saving the resulting graph as a single .pb file\n\n\t:param model_path_and_name: must finish by .ckpt, and the checkpoint must be composed of\n\t3+ files : .ckpt.index, .ckpt.meta, and .ckpt.data-0000X-of-0000Y\n\n\t:param model_path_and_name: path to the trained model\n\t:param output_file: file to save to. If None, model_path_and_name.[-ckpt][+pb]\n\t:return: None\n\t\"\"\"\n\tconfig_proto = tf.ConfigProto(allow_soft_placement=True)\n\n\twith tf.Session(config=config_proto) as sess:\n\t    new_saver = tf.train.import_meta_graph(model_path_and_name + '.meta', clear_devices=True)\n\t    tf.get_default_session().run(tf.global_variables_initializer())\n\t    tf.get_default_session().run(tf.local_variables_initializer())\n\t    new_saver.restore(sess, model_path_and_name)\n\n\t    # get graph definition\n\t    gd = sess.graph.as_graph_def()\n\t    # fix batch norm nodes\n\t    for node in gd.node:\n\t        if node.op == 'RefSwitch':\n\t            node.op = 'Switch'\n\t            for index in xrange(len(node.input)):\n\t                if 'moving_' in node.input[index]:\n\t                    node.input[index] = node.input[index] + '/read'\n\t        elif node.op == 'AssignSub':\n\t            node.op = 'Sub'\n\t            if 'use_locking' in node.attr: del node.attr['use_locking']\n\t        elif node.op == 'AssignAdd':\n\t            node.op = 'Add'\n\t            if 'use_locking' in node.attr: del node.attr['use_locking']\n\n\t    # tf.get_collection() returns a list. In this example we only want the\n\t    input_node = sess.graph.get_tensor_by_name('input_node:0')\n\t    new_shape = [None] + input_node.get_shape().as_list()[1:]\n\n\t    trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n\t    new_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, [\"output_node\"],\n\t                                                                 variable_names_whitelist=[t.name[:-2] for t in trainables] + ['output_node'])\n\n\t    for node in new_graph_def.node:\n\t        if node.name == 'input_node':\n\t            node.attr['shape'].CopyFrom(attr_value_pb2.AttrValue(shape=tf.TensorShape(new_shape).as_proto()))\n\t            break\n\n\t    with tf.gfile.GFile(output_file, \"wb\") as f:\n\t        f.write(new_graph_def.SerializeToString())\n\t    print(\"{0} / {1} ops in the final graph.\".format(len(new_graph_def.node), len(sess.graph.as_graph_def().node)))\n</code></pre>\n<p>This goes well and creates the pbtxt file with the following output :</p>\n<blockquote>\n<p>Converted 201 variables to const ops.<br>\n5287 / 41028 ops in the final graph.</p>\n</blockquote>\n<p>I then try to load the pbtxt model using this function :</p>\n<pre><code>def load_frozen_graph(frozen_graph_file):\n    \"\"\"\n    loads a graph frozen via freeze_and_prune_graph and returns the graph, its input placeholder and output tensor\n\n    :param frozen_graph_file: .pb file to load\n    :return: tf.graph, tf.placeholder, tf.tensor\n    \"\"\"\n    # We load the protobuf file from the disk and parse it to retrieve the\n    # unserialized graph_def\n    with tf.gfile.GFile(frozen_graph_file, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we can use again a convenient built-in function to import a graph_def into the\n    # current default Graph\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(\n            graph_def,\n            input_map=None,\n            return_elements=None,\n            name=\"prefix\",\n            op_dict=None,\n            producer_op_list=None\n        )\n\n    input_images_placeholder = graph.get_tensor_by_name('prefix/input_node:0')\n    input_phase_placeholder = None\n    try:\n        input_phase_placeholder = graph.get_tensor_by_name('prefix/phase:0')\n    except KeyError:\n        pass\n    output = graph.get_tensor_by_name('prefix/output_node:0')\n\n    return graph, input_images_placeholder, input_phase_placeholder, output\n</code></pre>\n<p>using the following snippet:</p>\n<pre><code>graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\nsess = tf.Session(config=tf_config, graph=graph)\nfeed_dict = {input_images_placeholder: prepared_input}\nif is_training_placeholder is not None:\n    feed_dict[is_training_placeholder] = False\nret = sess.run([output], feed_dict=feed_dict)\n</code></pre>\n<p>This, however, leads to the following error:</p>\n<blockquote>\n<p>FailedPreconditionError (see above for traceback): Attempting to use uninitialized value prefix/conv0/BatchNorm/batch_normalization/moving_mean<br>\n[[Node: prefix/conv0/BatchNorm/batch_normalization/moving_mean/read = Identity<a href=\"prefix/conv0/BatchNorm/batch_normalization/moving_mean\">T=DT_FLOAT, _class=[\"loc:@prefix/conv0/BatchNorm/batch_normalization/moving_mean\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"</a>]]<br>\n[[Node: prefix/output_node/_381 = _Recv<a href=\"\">client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2447_prefix/output_node\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"</a>]]</p>\n</blockquote>\n<p>Following the question :<br>\n<a href=\"https://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization\" rel=\"nofollow\">https://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization</a><br>\nI tried initializing variables:</p>\n<pre><code>graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\nsess = tf.Session(config=tf_config, graph=graph)\ninit = [tf.global_variables_initializer(), tf.local_variables_initializer()]\nsess.run(init)\n\nfeed_dict = {input_images_placeholder: prepared_input}\nif is_training_placeholder is not None:\n    feed_dict[is_training_placeholder] = False\nret = sess.run([self.output], feed_dict=feed_dict)\n</code></pre>\n<p>This, however, changes the error to:</p>\n<blockquote>\n<p>ValueError: Fetch argument &lt;tf.Operation 'init' type=NoOp&gt; cannot be interpreted as a Tensor.<br>\n(Operation name: \"init\" op: \"NoOp\" is not an element of this graph.)</p>\n</blockquote>\n<p>which seems to show that there is no variable that needs to be initialized.</p>\n<p>What am I missing ? How to I freeze and reload the relevant values of a batch_normalization layer ?</p>\n<p>PS: I do realize that this might better be on stackoverflow, but I posted there first and got no answer in 2 weeks:<br>\n<a href=\"https://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value\" rel=\"nofollow\">https://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value</a></p>", "body_text": "I am trying to freeze in a pbtxt file a checkpoint containing batchnorm layers (ubuntu, python 2.7, tf 1.1.0).\ncontext :\nHave I written custom code\nYes, see below\nOS Platform and Distribution\nDocker with Ubuntu 14.04\nTensorFlow installed from\npip installer\nTensorFlow version\ntensorflow and tensorflow-gpu 1.1.0\nBazel version\nN/A\nCUDA/cuDNN version\nCuda 8, CUDNN 5.1\nGPU model and memory\nNvidia titan-x * 2, 12Go Ram each\nExact command to reproduce\nFor this, following these posts and issues :\ndavidsandberg/facenet#161\ndavidsandberg/facenet@0f3ece5\nI use this function:\nfreeze_and_prune_graph(model_path_and_name, output_file=None):\n\t\"\"\"\n\tfreezes a model trained and saved by the trainer by :\n\t    - extracting the trainable variables between input_node and output_node\n\t    - turning them to constants\n\t    - changing the 1rst dim of input_node to None\n\t    -saving the resulting graph as a single .pb file\n\n\t:param model_path_and_name: must finish by .ckpt, and the checkpoint must be composed of\n\t3+ files : .ckpt.index, .ckpt.meta, and .ckpt.data-0000X-of-0000Y\n\n\t:param model_path_and_name: path to the trained model\n\t:param output_file: file to save to. If None, model_path_and_name.[-ckpt][+pb]\n\t:return: None\n\t\"\"\"\n\tconfig_proto = tf.ConfigProto(allow_soft_placement=True)\n\n\twith tf.Session(config=config_proto) as sess:\n\t    new_saver = tf.train.import_meta_graph(model_path_and_name + '.meta', clear_devices=True)\n\t    tf.get_default_session().run(tf.global_variables_initializer())\n\t    tf.get_default_session().run(tf.local_variables_initializer())\n\t    new_saver.restore(sess, model_path_and_name)\n\n\t    # get graph definition\n\t    gd = sess.graph.as_graph_def()\n\t    # fix batch norm nodes\n\t    for node in gd.node:\n\t        if node.op == 'RefSwitch':\n\t            node.op = 'Switch'\n\t            for index in xrange(len(node.input)):\n\t                if 'moving_' in node.input[index]:\n\t                    node.input[index] = node.input[index] + '/read'\n\t        elif node.op == 'AssignSub':\n\t            node.op = 'Sub'\n\t            if 'use_locking' in node.attr: del node.attr['use_locking']\n\t        elif node.op == 'AssignAdd':\n\t            node.op = 'Add'\n\t            if 'use_locking' in node.attr: del node.attr['use_locking']\n\n\t    # tf.get_collection() returns a list. In this example we only want the\n\t    input_node = sess.graph.get_tensor_by_name('input_node:0')\n\t    new_shape = [None] + input_node.get_shape().as_list()[1:]\n\n\t    trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n\t    new_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, [\"output_node\"],\n\t                                                                 variable_names_whitelist=[t.name[:-2] for t in trainables] + ['output_node'])\n\n\t    for node in new_graph_def.node:\n\t        if node.name == 'input_node':\n\t            node.attr['shape'].CopyFrom(attr_value_pb2.AttrValue(shape=tf.TensorShape(new_shape).as_proto()))\n\t            break\n\n\t    with tf.gfile.GFile(output_file, \"wb\") as f:\n\t        f.write(new_graph_def.SerializeToString())\n\t    print(\"{0} / {1} ops in the final graph.\".format(len(new_graph_def.node), len(sess.graph.as_graph_def().node)))\n\nThis goes well and creates the pbtxt file with the following output :\n\nConverted 201 variables to const ops.\n5287 / 41028 ops in the final graph.\n\nI then try to load the pbtxt model using this function :\ndef load_frozen_graph(frozen_graph_file):\n    \"\"\"\n    loads a graph frozen via freeze_and_prune_graph and returns the graph, its input placeholder and output tensor\n\n    :param frozen_graph_file: .pb file to load\n    :return: tf.graph, tf.placeholder, tf.tensor\n    \"\"\"\n    # We load the protobuf file from the disk and parse it to retrieve the\n    # unserialized graph_def\n    with tf.gfile.GFile(frozen_graph_file, \"rb\") as f:\n        graph_def = tf.GraphDef()\n        graph_def.ParseFromString(f.read())\n\n    # Then, we can use again a convenient built-in function to import a graph_def into the\n    # current default Graph\n    with tf.Graph().as_default() as graph:\n        tf.import_graph_def(\n            graph_def,\n            input_map=None,\n            return_elements=None,\n            name=\"prefix\",\n            op_dict=None,\n            producer_op_list=None\n        )\n\n    input_images_placeholder = graph.get_tensor_by_name('prefix/input_node:0')\n    input_phase_placeholder = None\n    try:\n        input_phase_placeholder = graph.get_tensor_by_name('prefix/phase:0')\n    except KeyError:\n        pass\n    output = graph.get_tensor_by_name('prefix/output_node:0')\n\n    return graph, input_images_placeholder, input_phase_placeholder, output\n\nusing the following snippet:\ngraph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\nsess = tf.Session(config=tf_config, graph=graph)\nfeed_dict = {input_images_placeholder: prepared_input}\nif is_training_placeholder is not None:\n    feed_dict[is_training_placeholder] = False\nret = sess.run([output], feed_dict=feed_dict)\n\nThis, however, leads to the following error:\n\nFailedPreconditionError (see above for traceback): Attempting to use uninitialized value prefix/conv0/BatchNorm/batch_normalization/moving_mean\n[[Node: prefix/conv0/BatchNorm/batch_normalization/moving_mean/read = IdentityT=DT_FLOAT, _class=[\"loc:@prefix/conv0/BatchNorm/batch_normalization/moving_mean\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"]]\n[[Node: prefix/output_node/_381 = _Recvclient_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2447_prefix/output_node\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]\n\nFollowing the question :\nhttps://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization\nI tried initializing variables:\ngraph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\nsess = tf.Session(config=tf_config, graph=graph)\ninit = [tf.global_variables_initializer(), tf.local_variables_initializer()]\nsess.run(init)\n\nfeed_dict = {input_images_placeholder: prepared_input}\nif is_training_placeholder is not None:\n    feed_dict[is_training_placeholder] = False\nret = sess.run([self.output], feed_dict=feed_dict)\n\nThis, however, changes the error to:\n\nValueError: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor.\n(Operation name: \"init\" op: \"NoOp\" is not an element of this graph.)\n\nwhich seems to show that there is no variable that needs to be initialized.\nWhat am I missing ? How to I freeze and reload the relevant values of a batch_normalization layer ?\nPS: I do realize that this might better be on stackoverflow, but I posted there first and got no answer in 2 weeks:\nhttps://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value", "body": "I am trying to freeze in a pbtxt file a checkpoint containing batchnorm layers (ubuntu, python 2.7, tf 1.1.0).\r\n\r\ncontext : \r\n**Have I written custom code**\r\nYes, see below\r\n\r\n**OS Platform and Distribution**\r\nDocker with Ubuntu 14.04\r\n\r\n**TensorFlow installed from**\r\npip installer\r\n\r\n**TensorFlow version**\r\ntensorflow and tensorflow-gpu 1.1.0\r\n\r\n**Bazel version**\r\nN/A\r\n\r\n**CUDA/cuDNN version**\r\nCuda 8, CUDNN 5.1\r\n\r\n**GPU model and memory**\r\nNvidia titan-x * 2, 12Go Ram each\r\n\r\n**Exact command to reproduce**\r\n\r\nFor this, following these posts and issues :\r\n\r\nhttps://github.com/davidsandberg/facenet/issues/161\r\n\r\nhttps://github.com/davidsandberg/facenet/pull/172/commits/0f3ece502550714c91056f3a8630ce8c037f613f\r\n\r\nI use this function:\r\n\r\n    freeze_and_prune_graph(model_path_and_name, output_file=None):\r\n\t\t\"\"\"\r\n\t\tfreezes a model trained and saved by the trainer by :\r\n\t\t    - extracting the trainable variables between input_node and output_node\r\n\t\t    - turning them to constants\r\n\t\t    - changing the 1rst dim of input_node to None\r\n\t\t    -saving the resulting graph as a single .pb file\r\n\r\n\t\t:param model_path_and_name: must finish by .ckpt, and the checkpoint must be composed of\r\n\t\t3+ files : .ckpt.index, .ckpt.meta, and .ckpt.data-0000X-of-0000Y\r\n\r\n\t\t:param model_path_and_name: path to the trained model\r\n\t\t:param output_file: file to save to. If None, model_path_and_name.[-ckpt][+pb]\r\n\t\t:return: None\r\n\t\t\"\"\"\r\n\t\tconfig_proto = tf.ConfigProto(allow_soft_placement=True)\r\n\r\n\t\twith tf.Session(config=config_proto) as sess:\r\n\t\t    new_saver = tf.train.import_meta_graph(model_path_and_name + '.meta', clear_devices=True)\r\n\t\t    tf.get_default_session().run(tf.global_variables_initializer())\r\n\t\t    tf.get_default_session().run(tf.local_variables_initializer())\r\n\t\t    new_saver.restore(sess, model_path_and_name)\r\n\r\n\t\t    # get graph definition\r\n\t\t    gd = sess.graph.as_graph_def()\r\n\t\t    # fix batch norm nodes\r\n\t\t    for node in gd.node:\r\n\t\t        if node.op == 'RefSwitch':\r\n\t\t            node.op = 'Switch'\r\n\t\t            for index in xrange(len(node.input)):\r\n\t\t                if 'moving_' in node.input[index]:\r\n\t\t                    node.input[index] = node.input[index] + '/read'\r\n\t\t        elif node.op == 'AssignSub':\r\n\t\t            node.op = 'Sub'\r\n\t\t            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\t\t        elif node.op == 'AssignAdd':\r\n\t\t            node.op = 'Add'\r\n\t\t            if 'use_locking' in node.attr: del node.attr['use_locking']\r\n\r\n\t\t    # tf.get_collection() returns a list. In this example we only want the\r\n\t\t    input_node = sess.graph.get_tensor_by_name('input_node:0')\r\n\t\t    new_shape = [None] + input_node.get_shape().as_list()[1:]\r\n\r\n\t\t    trainables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\r\n\t\t    new_graph_def = tf.graph_util.convert_variables_to_constants(sess, gd, [\"output_node\"],\r\n\t\t                                                                 variable_names_whitelist=[t.name[:-2] for t in trainables] + ['output_node'])\r\n\r\n\t\t    for node in new_graph_def.node:\r\n\t\t        if node.name == 'input_node':\r\n\t\t            node.attr['shape'].CopyFrom(attr_value_pb2.AttrValue(shape=tf.TensorShape(new_shape).as_proto()))\r\n\t\t            break\r\n\r\n\t\t    with tf.gfile.GFile(output_file, \"wb\") as f:\r\n\t\t        f.write(new_graph_def.SerializeToString())\r\n\t\t    print(\"{0} / {1} ops in the final graph.\".format(len(new_graph_def.node), len(sess.graph.as_graph_def().node)))\r\n\r\nThis goes well and creates the pbtxt file with the following output :\r\n\r\n> Converted 201 variables to const ops.\r\n5287 / 41028 ops in the final graph.\r\n\r\nI then try to load the pbtxt model using this function :\r\n\r\n    def load_frozen_graph(frozen_graph_file):\r\n\t    \"\"\"\r\n\t    loads a graph frozen via freeze_and_prune_graph and returns the graph, its input placeholder and output tensor\r\n\r\n\t    :param frozen_graph_file: .pb file to load\r\n\t    :return: tf.graph, tf.placeholder, tf.tensor\r\n\t    \"\"\"\r\n\t    # We load the protobuf file from the disk and parse it to retrieve the\r\n\t    # unserialized graph_def\r\n\t    with tf.gfile.GFile(frozen_graph_file, \"rb\") as f:\r\n\t        graph_def = tf.GraphDef()\r\n\t        graph_def.ParseFromString(f.read())\r\n\r\n\t    # Then, we can use again a convenient built-in function to import a graph_def into the\r\n\t    # current default Graph\r\n\t    with tf.Graph().as_default() as graph:\r\n\t        tf.import_graph_def(\r\n\t            graph_def,\r\n\t            input_map=None,\r\n\t            return_elements=None,\r\n\t            name=\"prefix\",\r\n\t            op_dict=None,\r\n\t            producer_op_list=None\r\n\t        )\r\n\r\n\t    input_images_placeholder = graph.get_tensor_by_name('prefix/input_node:0')\r\n\t    input_phase_placeholder = None\r\n\t    try:\r\n\t        input_phase_placeholder = graph.get_tensor_by_name('prefix/phase:0')\r\n\t    except KeyError:\r\n\t        pass\r\n\t    output = graph.get_tensor_by_name('prefix/output_node:0')\r\n\r\n\t    return graph, input_images_placeholder, input_phase_placeholder, output\r\n\r\nusing the following snippet:\r\n\r\n    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\r\n\tsess = tf.Session(config=tf_config, graph=graph)\r\n\tfeed_dict = {input_images_placeholder: prepared_input}\r\n\tif is_training_placeholder is not None:\r\n\t    feed_dict[is_training_placeholder] = False\r\n\tret = sess.run([output], feed_dict=feed_dict)\r\n\r\nThis, however, leads to the following error:\r\n\r\n> FailedPreconditionError (see above for traceback): Attempting to use uninitialized value prefix/conv0/BatchNorm/batch_normalization/moving_mean\r\n> [[Node: prefix/conv0/BatchNorm/batch_normalization/moving_mean/read = Identity[T=DT_FLOAT, _class=[\"loc:@prefix/conv0/BatchNorm/batch_normalization/moving_mean\"], _device=\"/job:localhost/replica:0/task:0/gpu:0\"](prefix/conv0/BatchNorm/batch_normalization/moving_mean)]]\r\n\t [[Node: prefix/output_node/_381 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_2447_prefix/output_node\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\r\n\r\nFollowing the question :\r\nhttps://stackoverflow.com/questions/36007883/tensorflow-attempting-to-use-uninitialized-value-in-variable-initialization\r\nI tried initializing variables:\r\n\r\n    graph, input_images_placeholder, is_training_placeholder, output = load_frozen_graph(model_pbtxt)\r\n\tsess = tf.Session(config=tf_config, graph=graph)\r\n\tinit = [tf.global_variables_initializer(), tf.local_variables_initializer()]\r\n\tsess.run(init)\r\n\r\n\tfeed_dict = {input_images_placeholder: prepared_input}\r\n\tif is_training_placeholder is not None:\r\n\t    feed_dict[is_training_placeholder] = False\r\n\tret = sess.run([self.output], feed_dict=feed_dict)\r\n\r\nThis, however, changes the error to:\r\n\r\n> ValueError: Fetch argument <tf.Operation 'init' type=NoOp> cannot be interpreted as a Tensor. \r\n(Operation name: \"init\" op: \"NoOp\" is not an element of this graph.)\r\n\r\nwhich seems to show that there is no variable that needs to be initialized.\r\n\r\nWhat am I missing ? How to I freeze and reload the relevant values of a batch_normalization layer ?\r\n\r\nPS: I do realize that this might better be on stackoverflow, but I posted there first and got no answer in 2 weeks:\r\nhttps://stackoverflow.com/questions/47434139/tensorflow-unable-to-import-frozen-graph-with-batchnorm-uninitialized-value"}