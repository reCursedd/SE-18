{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/312518530", "html_url": "https://github.com/tensorflow/tensorflow/issues/11186#issuecomment-312518530", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/11186", "id": 312518530, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMjUxODUzMA==", "user": {"login": "danijar", "id": 2111293, "node_id": "MDQ6VXNlcjIxMTEyOTM=", "avatar_url": "https://avatars0.githubusercontent.com/u/2111293?v=4", "gravatar_id": "", "url": "https://api.github.com/users/danijar", "html_url": "https://github.com/danijar", "followers_url": "https://api.github.com/users/danijar/followers", "following_url": "https://api.github.com/users/danijar/following{/other_user}", "gists_url": "https://api.github.com/users/danijar/gists{/gist_id}", "starred_url": "https://api.github.com/users/danijar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/danijar/subscriptions", "organizations_url": "https://api.github.com/users/danijar/orgs", "repos_url": "https://api.github.com/users/danijar/repos", "events_url": "https://api.github.com/users/danijar/events{/privacy}", "received_events_url": "https://api.github.com/users/danijar/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-02T21:37:33Z", "updated_at": "2017-07-02T21:37:33Z", "author_association": "MEMBER", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1381301\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ppwwyyxx\">@ppwwyyxx</a> For example, I was trying to feed the transition tuple of state, action, reward, and successor state to a reinforcement learning agent. Here, <code>env.state</code> is a variable holding the current state of the environment that gets updated when calling <code>env.step(action)</code>:</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-c\"><span class=\"pl-c\">#</span> <span class=\"pl-k\">NOTE</span>: old_state actually refers to the new state after stepping the env. Need env.state * 1 instead.</span>\nold_state <span class=\"pl-k\">=</span> tf.identity(env.state)\n<span class=\"pl-k\">with</span> tf.control_dependency([old_state]):\n    action <span class=\"pl-k\">=</span> agent.act(env.state)\n    reward <span class=\"pl-k\">=</span> env.step(action)\n<span class=\"pl-k\">with</span> tf.control_dependency([reward]):\n    experience_op <span class=\"pl-k\">=</span> agent.experience(old_state, action, reward, env.state)</pre></div>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=2138320\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jyegerlehner\">@jyegerlehner</a> That would require creating a second variable to hold the old state of the variable. That's wasteful if the value is only needed inside each <code>sess.run()</code> call separately. Instead, the value should be hold by a tensor.</p>", "body_text": "@ppwwyyxx For example, I was trying to feed the transition tuple of state, action, reward, and successor state to a reinforcement learning agent. Here, env.state is a variable holding the current state of the environment that gets updated when calling env.step(action):\n# NOTE: old_state actually refers to the new state after stepping the env. Need env.state * 1 instead.\nold_state = tf.identity(env.state)\nwith tf.control_dependency([old_state]):\n    action = agent.act(env.state)\n    reward = env.step(action)\nwith tf.control_dependency([reward]):\n    experience_op = agent.experience(old_state, action, reward, env.state)\n@jyegerlehner That would require creating a second variable to hold the old state of the variable. That's wasteful if the value is only needed inside each sess.run() call separately. Instead, the value should be hold by a tensor.", "body": "@ppwwyyxx For example, I was trying to feed the transition tuple of state, action, reward, and successor state to a reinforcement learning agent. Here, `env.state` is a variable holding the current state of the environment that gets updated when calling `env.step(action)`:\r\n\r\n```python\r\n# NOTE: old_state actually refers to the new state after stepping the env. Need env.state * 1 instead.\r\nold_state = tf.identity(env.state)\r\nwith tf.control_dependency([old_state]):\r\n    action = agent.act(env.state)\r\n    reward = env.step(action)\r\nwith tf.control_dependency([reward]):\r\n    experience_op = agent.experience(old_state, action, reward, env.state)\r\n```\r\n\r\n@jyegerlehner That would require creating a second variable to hold the old state of the variable. That's wasteful if the value is only needed inside each `sess.run()` call separately. Instead, the value should be hold by a tensor."}