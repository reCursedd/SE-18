{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12745", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12745/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12745/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12745/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12745", "id": 254554842, "node_id": "MDU6SXNzdWUyNTQ1NTQ4NDI=", "number": 12745, "title": "Distributed tensorflow - Stuck at \"CreateSession still waiting for response from worker\"", "user": {"login": "nolanliou", "id": 30223680, "node_id": "MDQ6VXNlcjMwMjIzNjgw", "avatar_url": "https://avatars3.githubusercontent.com/u/30223680?v=4", "gravatar_id": "", "url": "https://api.github.com/users/nolanliou", "html_url": "https://github.com/nolanliou", "followers_url": "https://api.github.com/users/nolanliou/followers", "following_url": "https://api.github.com/users/nolanliou/following{/other_user}", "gists_url": "https://api.github.com/users/nolanliou/gists{/gist_id}", "starred_url": "https://api.github.com/users/nolanliou/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/nolanliou/subscriptions", "organizations_url": "https://api.github.com/users/nolanliou/orgs", "repos_url": "https://api.github.com/users/nolanliou/repos", "events_url": "https://api.github.com/users/nolanliou/events{/privacy}", "received_events_url": "https://api.github.com/users/nolanliou/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 16, "created_at": "2017-09-01T07:07:56Z", "updated_at": "2018-11-06T00:43:16Z", "closed_at": "2018-01-03T20:36:55Z", "author_association": "CONTRIBUTOR", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:<br>\nYes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>:<br>\nLinux Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>:<br>\nbinary</li>\n<li><strong>TensorFlow version (use command below)</strong>:<br>\n1.0 &amp;&amp; 1.3</li>\n<li><strong>Python version</strong>:<br>\n2.7</li>\n<li><strong>CUDA/cuDNN version</strong>:<br>\nno</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>Run the code on k8s with the spec <code>1 ps + 20 workers</code>. there are some workers print the log all the time.</p>\n<pre><code>I tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:12\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:17\n</code></pre>\n<p>I met two situations:</p>\n<ol>\n<li>worker 1 wait for worker 12 and 17, but worker 12 and 17 could start training without these logs.</li>\n<li>all of other works wait for the worker 12 and 17.</li>\n</ol>\n<p>I could telnet the no-response worker within the docker vm in both situations, so it's wired. Is there a bug?<br>\nSome have also encountered this problem. see <a href=\"https://stackoverflow.com/search?q=CreateSession+still+waiting+for+response+from+worker\" rel=\"nofollow\">Stackoverflow</a>.</p>\n<h3>Source code / logs</h3>\n<pre><code>import datetime\nimport json\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_integer(\"max_epochs\", 10000000, \"Number of steps to run trainer.\")\nflags.DEFINE_string(\"checkpoint_path\", \"./checkpoint/\",\n                    \"The checkpoint directory\")\nflags.DEFINE_string(\"output_path\", \"./tensorboard/\",\n                    \"indicates training output\")\nflags.DEFINE_integer(\"checkpoint_period\", 1,\n                     \"Number of epochs to save checkpoint.\")\nflags.DEFINE_float(\"learning_rate\", 0.01, \"Initial learning rate.\")\nFLAGS = flags.FLAGS\n\n\ndef main():\n    # Create train data\n    train_X = np.linspace(-1, 1, 100)\n    train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10\n    learning_rate = FLAGS.learning_rate\n    start_training_time = datetime.datetime.now()\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    # Exampmle: {\"cluster\": {\"ps\": [\"127.0.0.1:3001\"], \"worker\": [\"127.0.0.1:3002\", \"127.0.0.1:3003\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\n    env = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\"))\n    task_data = env.get(\"task\", None)\n    cluster_spec = env[\"cluster\"]\n    task_type = task_data[\"type\"]\n    task_index = task_data[\"index\"]\n\n    cluster = tf.train.ClusterSpec(cluster_spec)\n    server = tf.train.Server(cluster,\n                             job_name=task_type,\n                             task_index=task_index)\n\n    if task_type == \"ps\":\n        server.join()\n    elif task_type == \"worker\":\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:{}/task:{}\".format(task_type, task_index),\n                cluster=cluster)):\n\n            # Define the model\n            keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])\n            keys = tf.identity(keys_placeholder)\n            X = tf.placeholder(\"float\", shape=[None, 1])\n            Y = tf.placeholder(\"float\", shape=[None, 1])\n            w = tf.Variable(0.0, name=\"weight\")\n            b = tf.Variable(0.0, name=\"bias\")\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n            loss = tf.reduce_sum(tf.square(Y - tf.multiply(X, w) - b))\n            train_op = optimizer.minimize(loss, global_step=global_step)\n            predict_op = tf.multiply(X, w) + b\n            tf.summary.scalar(\"loss\", loss)\n            summary_op = tf.summary.merge_all()\n            init_op = tf.global_variables_initializer()\n            saver = tf.train.Saver()\n            #saver = tf.train.Saver(sharded=True)\n\n\n            sv = tf.train.Supervisor(is_chief=(task_index == 0),\n                                     logdir=FLAGS.checkpoint_path,\n                                     init_op=init_op,\n                                     #summary_op=summary_op,\n                                     summary_op=None,\n                                     saver=saver,\n                                     global_step=global_step,\n                                     save_model_secs=60)\n\n            try:\n                with sv.managed_session(server.target) as sess:\n                    print(\"Save tensorboard files into: {}\".format(FLAGS.output_path))\n                    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\n\n                    print(\"Run training with epoch number: {}\".format(\n                        FLAGS.max_epochs))\n                    for i in range(FLAGS.max_epochs):\n                        for (x, y) in zip(train_X, train_Y):\n                            x = np.array([[x]])\n                            y = np.array([[y]])\n                            sess.run(train_op, feed_dict={X: x, Y: y})\n\n                        if i % FLAGS.checkpoint_period == 0:\n                            x = np.array([[train_X[0]]])\n                            y = np.array([[train_Y[0]]])\n                            summary_value, loss_value, step = sess.run(\n                                [summary_op, loss, global_step],\n                                feed_dict={X: x,\n                                           Y: y})\n                            print(\"Epoch: {}, loss: {}\".format(i, loss_value))\n                            if task_index == 0:\n                                writer.add_summary(summary_value, step)\n\n                    writer.close()\n\n                    end_training_time = datetime.datetime.now()\n                    print(\"[{}] End of distributed training.\".format(\n                        end_training_time - start_training_time))\n\n\n            except Exception as e:\n                print(e)\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nYes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nLinux Ubuntu 16.04\nTensorFlow installed from (source or binary):\nbinary\nTensorFlow version (use command below):\n1.0 && 1.3\nPython version:\n2.7\nCUDA/cuDNN version:\nno\n\nDescribe the problem\nRun the code on k8s with the spec 1 ps + 20 workers. there are some workers print the log all the time.\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:12\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:17\n\nI met two situations:\n\nworker 1 wait for worker 12 and 17, but worker 12 and 17 could start training without these logs.\nall of other works wait for the worker 12 and 17.\n\nI could telnet the no-response worker within the docker vm in both situations, so it's wired. Is there a bug?\nSome have also encountered this problem. see Stackoverflow.\nSource code / logs\nimport datetime\nimport json\nimport numpy as np\nimport os\nimport tensorflow as tf\n\nflags = tf.app.flags\nflags.DEFINE_integer(\"max_epochs\", 10000000, \"Number of steps to run trainer.\")\nflags.DEFINE_string(\"checkpoint_path\", \"./checkpoint/\",\n                    \"The checkpoint directory\")\nflags.DEFINE_string(\"output_path\", \"./tensorboard/\",\n                    \"indicates training output\")\nflags.DEFINE_integer(\"checkpoint_period\", 1,\n                     \"Number of epochs to save checkpoint.\")\nflags.DEFINE_float(\"learning_rate\", 0.01, \"Initial learning rate.\")\nFLAGS = flags.FLAGS\n\n\ndef main():\n    # Create train data\n    train_X = np.linspace(-1, 1, 100)\n    train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10\n    learning_rate = FLAGS.learning_rate\n    start_training_time = datetime.datetime.now()\n\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    # Exampmle: {\"cluster\": {\"ps\": [\"127.0.0.1:3001\"], \"worker\": [\"127.0.0.1:3002\", \"127.0.0.1:3003\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\n    env = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\"))\n    task_data = env.get(\"task\", None)\n    cluster_spec = env[\"cluster\"]\n    task_type = task_data[\"type\"]\n    task_index = task_data[\"index\"]\n\n    cluster = tf.train.ClusterSpec(cluster_spec)\n    server = tf.train.Server(cluster,\n                             job_name=task_type,\n                             task_index=task_index)\n\n    if task_type == \"ps\":\n        server.join()\n    elif task_type == \"worker\":\n        with tf.device(tf.train.replica_device_setter(\n                worker_device=\"/job:{}/task:{}\".format(task_type, task_index),\n                cluster=cluster)):\n\n            # Define the model\n            keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])\n            keys = tf.identity(keys_placeholder)\n            X = tf.placeholder(\"float\", shape=[None, 1])\n            Y = tf.placeholder(\"float\", shape=[None, 1])\n            w = tf.Variable(0.0, name=\"weight\")\n            b = tf.Variable(0.0, name=\"bias\")\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n            loss = tf.reduce_sum(tf.square(Y - tf.multiply(X, w) - b))\n            train_op = optimizer.minimize(loss, global_step=global_step)\n            predict_op = tf.multiply(X, w) + b\n            tf.summary.scalar(\"loss\", loss)\n            summary_op = tf.summary.merge_all()\n            init_op = tf.global_variables_initializer()\n            saver = tf.train.Saver()\n            #saver = tf.train.Saver(sharded=True)\n\n\n            sv = tf.train.Supervisor(is_chief=(task_index == 0),\n                                     logdir=FLAGS.checkpoint_path,\n                                     init_op=init_op,\n                                     #summary_op=summary_op,\n                                     summary_op=None,\n                                     saver=saver,\n                                     global_step=global_step,\n                                     save_model_secs=60)\n\n            try:\n                with sv.managed_session(server.target) as sess:\n                    print(\"Save tensorboard files into: {}\".format(FLAGS.output_path))\n                    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\n\n                    print(\"Run training with epoch number: {}\".format(\n                        FLAGS.max_epochs))\n                    for i in range(FLAGS.max_epochs):\n                        for (x, y) in zip(train_X, train_Y):\n                            x = np.array([[x]])\n                            y = np.array([[y]])\n                            sess.run(train_op, feed_dict={X: x, Y: y})\n\n                        if i % FLAGS.checkpoint_period == 0:\n                            x = np.array([[train_X[0]]])\n                            y = np.array([[train_Y[0]]])\n                            summary_value, loss_value, step = sess.run(\n                                [summary_op, loss, global_step],\n                                feed_dict={X: x,\n                                           Y: y})\n                            print(\"Epoch: {}, loss: {}\".format(i, loss_value))\n                            if task_index == 0:\n                                writer.add_summary(summary_value, step)\n\n                    writer.close()\n\n                    end_training_time = datetime.datetime.now()\n                    print(\"[{}] End of distributed training.\".format(\n                        end_training_time - start_training_time))\n\n\n            except Exception as e:\n                print(e)\n\n\nif __name__ == \"__main__\":\n    main()", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:\r\nYes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**:\r\nLinux Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**:\r\nbinary\r\n- **TensorFlow version (use command below)**:\r\n1.0 && 1.3\r\n- **Python version**: \r\n2.7\r\n- **CUDA/cuDNN version**:\r\nno\r\n\r\n### Describe the problem\r\nRun the code on k8s with the spec `1 ps + 20 workers`. there are some workers print the log all the time.\r\n```\r\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:12\r\nI tensor flow/core/distributed_runtime/master.cc:193] CreateSession still waiting for response from worker: /job:worker/replica:0/task:17\r\n```\r\nI met two situations:\r\n1. worker 1 wait for worker 12 and 17, but worker 12 and 17 could start training without these logs.\r\n2. all of other works wait for the worker 12 and 17.\r\n\r\nI could telnet the no-response worker within the docker vm in both situations, so it's wired. Is there a bug? \r\nSome have also encountered this problem. see [Stackoverflow](https://stackoverflow.com/search?q=CreateSession+still+waiting+for+response+from+worker).\r\n### Source code / logs\r\n```\r\nimport datetime\r\nimport json\r\nimport numpy as np\r\nimport os\r\nimport tensorflow as tf\r\n\r\nflags = tf.app.flags\r\nflags.DEFINE_integer(\"max_epochs\", 10000000, \"Number of steps to run trainer.\")\r\nflags.DEFINE_string(\"checkpoint_path\", \"./checkpoint/\",\r\n                    \"The checkpoint directory\")\r\nflags.DEFINE_string(\"output_path\", \"./tensorboard/\",\r\n                    \"indicates training output\")\r\nflags.DEFINE_integer(\"checkpoint_period\", 1,\r\n                     \"Number of epochs to save checkpoint.\")\r\nflags.DEFINE_float(\"learning_rate\", 0.01, \"Initial learning rate.\")\r\nFLAGS = flags.FLAGS\r\n\r\n\r\ndef main():\r\n    # Create train data\r\n    train_X = np.linspace(-1, 1, 100)\r\n    train_Y = 2 * train_X + np.random.randn(*train_X.shape) * 0.33 + 10\r\n    learning_rate = FLAGS.learning_rate\r\n    start_training_time = datetime.datetime.now()\r\n\r\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\r\n    # Exampmle: {\"cluster\": {\"ps\": [\"127.0.0.1:3001\"], \"worker\": [\"127.0.0.1:3002\", \"127.0.0.1:3003\"]}, \"task\": {\"index\": 0, \"type\": \"worker\"}}\r\n    env = json.loads(os.environ.get(\"TF_CONFIG\", \"{}\"))\r\n    task_data = env.get(\"task\", None)\r\n    cluster_spec = env[\"cluster\"]\r\n    task_type = task_data[\"type\"]\r\n    task_index = task_data[\"index\"]\r\n\r\n    cluster = tf.train.ClusterSpec(cluster_spec)\r\n    server = tf.train.Server(cluster,\r\n                             job_name=task_type,\r\n                             task_index=task_index)\r\n\r\n    if task_type == \"ps\":\r\n        server.join()\r\n    elif task_type == \"worker\":\r\n        with tf.device(tf.train.replica_device_setter(\r\n                worker_device=\"/job:{}/task:{}\".format(task_type, task_index),\r\n                cluster=cluster)):\r\n\r\n            # Define the model\r\n            keys_placeholder = tf.placeholder(tf.int32, shape=[None, 1])\r\n            keys = tf.identity(keys_placeholder)\r\n            X = tf.placeholder(\"float\", shape=[None, 1])\r\n            Y = tf.placeholder(\"float\", shape=[None, 1])\r\n            w = tf.Variable(0.0, name=\"weight\")\r\n            b = tf.Variable(0.0, name=\"bias\")\r\n            global_step = tf.Variable(0, name=\"global_step\", trainable=False)\r\n            loss = tf.reduce_sum(tf.square(Y - tf.multiply(X, w) - b))\r\n            train_op = optimizer.minimize(loss, global_step=global_step)\r\n            predict_op = tf.multiply(X, w) + b\r\n            tf.summary.scalar(\"loss\", loss)\r\n            summary_op = tf.summary.merge_all()\r\n            init_op = tf.global_variables_initializer()\r\n            saver = tf.train.Saver()\r\n            #saver = tf.train.Saver(sharded=True)\r\n\r\n\r\n            sv = tf.train.Supervisor(is_chief=(task_index == 0),\r\n                                     logdir=FLAGS.checkpoint_path,\r\n                                     init_op=init_op,\r\n                                     #summary_op=summary_op,\r\n                                     summary_op=None,\r\n                                     saver=saver,\r\n                                     global_step=global_step,\r\n                                     save_model_secs=60)\r\n\r\n            try:\r\n                with sv.managed_session(server.target) as sess:\r\n                    print(\"Save tensorboard files into: {}\".format(FLAGS.output_path))\r\n                    writer = tf.summary.FileWriter(FLAGS.output_path, sess.graph)\r\n\r\n                    print(\"Run training with epoch number: {}\".format(\r\n                        FLAGS.max_epochs))\r\n                    for i in range(FLAGS.max_epochs):\r\n                        for (x, y) in zip(train_X, train_Y):\r\n                            x = np.array([[x]])\r\n                            y = np.array([[y]])\r\n                            sess.run(train_op, feed_dict={X: x, Y: y})\r\n\r\n                        if i % FLAGS.checkpoint_period == 0:\r\n                            x = np.array([[train_X[0]]])\r\n                            y = np.array([[train_Y[0]]])\r\n                            summary_value, loss_value, step = sess.run(\r\n                                [summary_op, loss, global_step],\r\n                                feed_dict={X: x,\r\n                                           Y: y})\r\n                            print(\"Epoch: {}, loss: {}\".format(i, loss_value))\r\n                            if task_index == 0:\r\n                                writer.add_summary(summary_value, step)\r\n\r\n                    writer.close()\r\n\r\n                    end_training_time = datetime.datetime.now()\r\n                    print(\"[{}] End of distributed training.\".format(\r\n                        end_training_time - start_training_time))\r\n\r\n\r\n            except Exception as e:\r\n                print(e)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n"}