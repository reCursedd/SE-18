{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287671948", "html_url": "https://github.com/tensorflow/tensorflow/issues/8535#issuecomment-287671948", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8535", "id": 287671948, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzY3MTk0OA==", "user": {"login": "zhangyaobit", "id": 1034716, "node_id": "MDQ6VXNlcjEwMzQ3MTY=", "avatar_url": "https://avatars3.githubusercontent.com/u/1034716?v=4", "gravatar_id": "", "url": "https://api.github.com/users/zhangyaobit", "html_url": "https://github.com/zhangyaobit", "followers_url": "https://api.github.com/users/zhangyaobit/followers", "following_url": "https://api.github.com/users/zhangyaobit/following{/other_user}", "gists_url": "https://api.github.com/users/zhangyaobit/gists{/gist_id}", "starred_url": "https://api.github.com/users/zhangyaobit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/zhangyaobit/subscriptions", "organizations_url": "https://api.github.com/users/zhangyaobit/orgs", "repos_url": "https://api.github.com/users/zhangyaobit/repos", "events_url": "https://api.github.com/users/zhangyaobit/events{/privacy}", "received_events_url": "https://api.github.com/users/zhangyaobit/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-20T03:19:44Z", "updated_at": "2017-03-20T03:21:25Z", "author_association": "CONTRIBUTOR", "body_html": "<p>By default, you are using non-fused batch norm, but looks like type conversion from float32 to float16 is not supported for it yet. There is a faster version of batch norm, which could be turned on by setting fused=True. Although its code is templatized by <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L47\">type</a>, we currently only have <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L621\">a float32 instantiation</a>; it would be nice to add support for both float16 and double.</p>\n<p>I marked this as contribution welcome for (1) type conversion for non-fused, default batch norm, and (2) float16 and double support for fused batch norm.</p>", "body_text": "By default, you are using non-fused batch norm, but looks like type conversion from float32 to float16 is not supported for it yet. There is a faster version of batch norm, which could be turned on by setting fused=True. Although its code is templatized by type, we currently only have a float32 instantiation; it would be nice to add support for both float16 and double.\nI marked this as contribution welcome for (1) type conversion for non-fused, default batch norm, and (2) float16 and double support for fused batch norm.", "body": "By default, you are using non-fused batch norm, but looks like type conversion from float32 to float16 is not supported for it yet. There is a faster version of batch norm, which could be turned on by setting fused=True. Although its code is templatized by [type](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L47), we currently only have [a float32 instantiation](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/kernels/fused_batch_norm_op.cc#L621); it would be nice to add support for both float16 and double.\r\n\r\nI marked this as contribution welcome for (1) type conversion for non-fused, default batch norm, and (2) float16 and double support for fused batch norm.\r\n"}