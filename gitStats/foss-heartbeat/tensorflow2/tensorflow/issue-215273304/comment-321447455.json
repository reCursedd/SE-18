{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/321447455", "html_url": "https://github.com/tensorflow/tensorflow/issues/8535#issuecomment-321447455", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8535", "id": 321447455, "node_id": "MDEyOklzc3VlQ29tbWVudDMyMTQ0NzQ1NQ==", "user": {"login": "reedwm", "id": 6510203, "node_id": "MDQ6VXNlcjY1MTAyMDM=", "avatar_url": "https://avatars2.githubusercontent.com/u/6510203?v=4", "gravatar_id": "", "url": "https://api.github.com/users/reedwm", "html_url": "https://github.com/reedwm", "followers_url": "https://api.github.com/users/reedwm/followers", "following_url": "https://api.github.com/users/reedwm/following{/other_user}", "gists_url": "https://api.github.com/users/reedwm/gists{/gist_id}", "starred_url": "https://api.github.com/users/reedwm/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/reedwm/subscriptions", "organizations_url": "https://api.github.com/users/reedwm/orgs", "repos_url": "https://api.github.com/users/reedwm/repos", "events_url": "https://api.github.com/users/reedwm/events{/privacy}", "received_events_url": "https://api.github.com/users/reedwm/received_events", "type": "User", "site_admin": false}, "created_at": "2017-08-10T04:26:41Z", "updated_at": "2017-08-10T04:26:41Z", "author_association": "MEMBER", "body_html": "<p>Float16 support for fused batch norm is being worked on. Then tf.contrib.layers.batch_norm will support tf.float16 if fused=True is passed to it.</p>\n<p>Note the scale and and offset variables will still be stored as float32s, but the inputs and outputs to fused batch norm will have the option of being float16.</p>", "body_text": "Float16 support for fused batch norm is being worked on. Then tf.contrib.layers.batch_norm will support tf.float16 if fused=True is passed to it.\nNote the scale and and offset variables will still be stored as float32s, but the inputs and outputs to fused batch norm will have the option of being float16.", "body": "Float16 support for fused batch norm is being worked on. Then tf.contrib.layers.batch_norm will support tf.float16 if fused=True is passed to it.\r\n\r\nNote the scale and and offset variables will still be stored as float32s, but the inputs and outputs to fused batch norm will have the option of being float16."}