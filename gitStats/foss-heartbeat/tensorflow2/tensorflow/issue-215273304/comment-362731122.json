{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/362731122", "html_url": "https://github.com/tensorflow/tensorflow/issues/8535#issuecomment-362731122", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8535", "id": 362731122, "node_id": "MDEyOklzc3VlQ29tbWVudDM2MjczMTEyMg==", "user": {"login": "aisuni", "id": 35795681, "node_id": "MDQ6VXNlcjM1Nzk1Njgx", "avatar_url": "https://avatars3.githubusercontent.com/u/35795681?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aisuni", "html_url": "https://github.com/aisuni", "followers_url": "https://api.github.com/users/aisuni/followers", "following_url": "https://api.github.com/users/aisuni/following{/other_user}", "gists_url": "https://api.github.com/users/aisuni/gists{/gist_id}", "starred_url": "https://api.github.com/users/aisuni/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aisuni/subscriptions", "organizations_url": "https://api.github.com/users/aisuni/orgs", "repos_url": "https://api.github.com/users/aisuni/repos", "events_url": "https://api.github.com/users/aisuni/events{/privacy}", "received_events_url": "https://api.github.com/users/aisuni/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-02T22:48:02Z", "updated_at": "2018-02-03T08:40:50Z", "author_association": "NONE", "body_html": "<p>Thanks, <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6510203\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/reedwm\">@reedwm</a>  for very useful info.<br>\nI have one follow-up, but may not be specific to this issue.  I have following code which works well with single precision (fp32 or fp16), however, the same will not work with mixed precision.  is it a good idea to cast the DT to fp32 at this stage? Can you pls. provide me some pointers here?<br>\nI'll step out if I am deviating the flow (considering the scope of this issue)...</p>\n<pre><code>            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=net)\n            losfn = tf.reduce_mean(cross_entropy, name=\"celoss\")\n            celoss = lossfn(net, labels)\n            total_loss = None\n            if tf.GraphKeys.REGULARIZATION_LOSSES:\n                losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                if losses:\n                    loss = tf.reduce_sum([tf.reduce_mean(reg_loss)\n                               for loss in losses],\n                               name='reg_loss_mean_sum')\n                    total_loss = celoss + loss\n                else:\n                    total_loss = celoss\n                if not total_loss:\n                    total_loss = celoss\n                optimization = tf.contrib.layers.optimize_loss(total_loss, global_step=tf.train.get_global_step(),\n                        clip_gradients=FLAGS.clip_grad, increment_global_step=True,**train_params)\n</code></pre>", "body_text": "Thanks, @reedwm  for very useful info.\nI have one follow-up, but may not be specific to this issue.  I have following code which works well with single precision (fp32 or fp16), however, the same will not work with mixed precision.  is it a good idea to cast the DT to fp32 at this stage? Can you pls. provide me some pointers here?\nI'll step out if I am deviating the flow (considering the scope of this issue)...\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=net)\n            losfn = tf.reduce_mean(cross_entropy, name=\"celoss\")\n            celoss = lossfn(net, labels)\n            total_loss = None\n            if tf.GraphKeys.REGULARIZATION_LOSSES:\n                losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n                if losses:\n                    loss = tf.reduce_sum([tf.reduce_mean(reg_loss)\n                               for loss in losses],\n                               name='reg_loss_mean_sum')\n                    total_loss = celoss + loss\n                else:\n                    total_loss = celoss\n                if not total_loss:\n                    total_loss = celoss\n                optimization = tf.contrib.layers.optimize_loss(total_loss, global_step=tf.train.get_global_step(),\n                        clip_gradients=FLAGS.clip_grad, increment_global_step=True,**train_params)", "body": "Thanks, @reedwm  for very useful info. \r\nI have one follow-up, but may not be specific to this issue.  I have following code which works well with single precision (fp32 or fp16), however, the same will not work with mixed precision.  is it a good idea to cast the DT to fp32 at this stage? Can you pls. provide me some pointers here?\r\nI'll step out if I am deviating the flow (considering the scope of this issue)... \r\n```\r\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=net)\r\n            losfn = tf.reduce_mean(cross_entropy, name=\"celoss\")\r\n            celoss = lossfn(net, labels)\r\n            total_loss = None\r\n            if tf.GraphKeys.REGULARIZATION_LOSSES:\r\n                losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\r\n                if losses:\r\n                    loss = tf.reduce_sum([tf.reduce_mean(reg_loss)\r\n                               for loss in losses],\r\n                               name='reg_loss_mean_sum')\r\n                    total_loss = celoss + loss\r\n                else:\r\n                    total_loss = celoss\r\n                if not total_loss:\r\n                    total_loss = celoss\r\n                optimization = tf.contrib.layers.optimize_loss(total_loss, global_step=tf.train.get_global_step(),\r\n                        clip_gradients=FLAGS.clip_grad, increment_global_step=True,**train_params)\r\n```"}