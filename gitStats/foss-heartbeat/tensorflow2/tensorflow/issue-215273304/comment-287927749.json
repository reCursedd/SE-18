{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/287927749", "html_url": "https://github.com/tensorflow/tensorflow/issues/8535#issuecomment-287927749", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8535", "id": 287927749, "node_id": "MDEyOklzc3VlQ29tbWVudDI4NzkyNzc0OQ==", "user": {"login": "jakelee8", "id": 19253212, "node_id": "MDQ6VXNlcjE5MjUzMjEy", "avatar_url": "https://avatars3.githubusercontent.com/u/19253212?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jakelee8", "html_url": "https://github.com/jakelee8", "followers_url": "https://api.github.com/users/jakelee8/followers", "following_url": "https://api.github.com/users/jakelee8/following{/other_user}", "gists_url": "https://api.github.com/users/jakelee8/gists{/gist_id}", "starred_url": "https://api.github.com/users/jakelee8/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jakelee8/subscriptions", "organizations_url": "https://api.github.com/users/jakelee8/orgs", "repos_url": "https://api.github.com/users/jakelee8/repos", "events_url": "https://api.github.com/users/jakelee8/events{/privacy}", "received_events_url": "https://api.github.com/users/jakelee8/received_events", "type": "User", "site_admin": false}, "created_at": "2017-03-20T23:16:20Z", "updated_at": "2017-03-20T23:16:20Z", "author_association": "NONE", "body_html": "<p>There seems to be an issue for supporting <code>float16</code> concerning moment calculation. Non-fused BatchNorm uses <code>moments()</code> internally and <code>moments()</code> has this caveat. Except from source code follows.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-c\"><span class=\"pl-c\">#</span> The dynamic range of fp16 is too limited to support the collection of</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> sufficient statistics. As a workaround we simply perform the operations</span>\n    <span class=\"pl-c\"><span class=\"pl-c\">#</span> on 32-bit floats before converting the mean and variance back to fp16</span>\n    y <span class=\"pl-k\">=</span> math_ops.cast(x, dtypes.float32) <span class=\"pl-k\">if</span> x.dtype <span class=\"pl-k\">==</span> dtypes.float16 <span class=\"pl-k\">else</span> x</pre></div>\n<p><a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634</a></p>\n<p>Running <code>layers_test.py</code> with support for <code>float16</code> and <code>float32</code> BatchNorm resulted in the following numerical errors. The <code>float16</code> moving mean values are incorrect.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=20625060\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/narendravardi\">@narendravardi</a> What do you think? Does the fused implementation have the same limitations?</p>\n<pre><code>======================================================================\nFAIL: testTrainMovingVarsNCHW (__main__.BatchNormFloat16Test)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2549, in testTrainMovingVarsNCHW\n    self._testTrainMovingVars(False, data_format='NCHW')\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=0.001, atol=0.001\n\n(mismatch 100.0%)\n x: array([ 0.146973,  0.129639,  0.099976,  0.183716,  0.116333,  0.225464,\n        0.120422,  0.21936 ,  0.140869,  0.147949,  0.140869,  0.156128,\n        0.210205,  0.21936 ,  0.082642,  0.183716,  0.121399,  0.022446,...\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n======================================================================\nFAIL: testTrainMovingVarsNHWC (__main__.BatchNormFloat16Test)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2546, in testTrainMovingVarsNHWC\n    self._testTrainMovingVars(False, data_format='NHWC')\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=0.001, atol=0.001\n\n(mismatch 100.0%)\n x: array([ 0.175537,  0.158203,  0.179565,  0.151001,  0.17041 ,  0.21936 ,\n        0.092834,  0.25415 ,  0.190796,  0.135742,  0.118347,  0.112244,\n        0.074463,  0.165283,  0.198975,  0.093872,  0.233643,  0.165283,...\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n----------------------------------------------------------------------\nRan 462 tests in 8.531s\n\nFAILED (failures=2, skipped=39)\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\nnot close lhs =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\n  0.28051758  0.13879395]\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nnot close dif =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\n  0.28051758  0.13879395]\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001]\ndtype = float16, shape = (32,)\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\nnot close lhs =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\n  0.16638184  0.24182129]\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nnot close dif =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\n  0.16638184  0.24182129]\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001]\ndtype = float16, shape = (32,)\n</code></pre>", "body_text": "There seems to be an issue for supporting float16 concerning moment calculation. Non-fused BatchNorm uses moments() internally and moments() has this caveat. Except from source code follows.\n    # The dynamic range of fp16 is too limited to support the collection of\n    # sufficient statistics. As a workaround we simply perform the operations\n    # on 32-bit floats before converting the mean and variance back to fp16\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\nRunning layers_test.py with support for float16 and float32 BatchNorm resulted in the following numerical errors. The float16 moving mean values are incorrect.\n@narendravardi What do you think? Does the fused implementation have the same limitations?\n======================================================================\nFAIL: testTrainMovingVarsNCHW (__main__.BatchNormFloat16Test)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2549, in testTrainMovingVarsNCHW\n    self._testTrainMovingVars(False, data_format='NCHW')\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=0.001, atol=0.001\n\n(mismatch 100.0%)\n x: array([ 0.146973,  0.129639,  0.099976,  0.183716,  0.116333,  0.225464,\n        0.120422,  0.21936 ,  0.140869,  0.147949,  0.140869,  0.156128,\n        0.210205,  0.21936 ,  0.082642,  0.183716,  0.121399,  0.022446,...\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n======================================================================\nFAIL: testTrainMovingVarsNHWC (__main__.BatchNormFloat16Test)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2546, in testTrainMovingVarsNHWC\n    self._testTrainMovingVars(False, data_format='NHWC')\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\n    verbose=verbose, header=header)\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\n    raise AssertionError(msg)\nAssertionError: \nNot equal to tolerance rtol=0.001, atol=0.001\n\n(mismatch 100.0%)\n x: array([ 0.175537,  0.158203,  0.179565,  0.151001,  0.17041 ,  0.21936 ,\n        0.092834,  0.25415 ,  0.190796,  0.135742,  0.118347,  0.112244,\n        0.074463,  0.165283,  0.198975,  0.093872,  0.233643,  0.165283,...\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\n\n----------------------------------------------------------------------\nRan 462 tests in 8.531s\n\nFAILED (failures=2, skipped=39)\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\nnot close lhs =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\n  0.28051758  0.13879395]\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nnot close dif =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\n  0.28051758  0.13879395]\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001]\ndtype = float16, shape = (32,)\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\nnot close lhs =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\n  0.16638184  0.24182129]\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\nnot close dif =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\n  0.16638184  0.24182129]\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\n  0.001  0.001]\ndtype = float16, shape = (32,)", "body": "There seems to be an issue for supporting `float16` concerning moment calculation. Non-fused BatchNorm uses `moments()` internally and `moments()` has this caveat. Except from source code follows.\r\n\r\n```py\r\n    # The dynamic range of fp16 is too limited to support the collection of\r\n    # sufficient statistics. As a workaround we simply perform the operations\r\n    # on 32-bit floats before converting the mean and variance back to fp16\r\n    y = math_ops.cast(x, dtypes.float32) if x.dtype == dtypes.float16 else x\r\n```\r\n\r\nhttps://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/nn_impl.py#L631-L634\r\n\r\n\r\nRunning `layers_test.py` with support for `float16` and `float32` BatchNorm resulted in the following numerical errors. The `float16` moving mean values are incorrect.\r\n\r\n@narendravardi What do you think? Does the fused implementation have the same limitations?\r\n\r\n```\r\n======================================================================\r\nFAIL: testTrainMovingVarsNCHW (__main__.BatchNormFloat16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2549, in testTrainMovingVarsNCHW\r\n    self._testTrainMovingVars(False, data_format='NCHW')\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\r\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\r\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.146973,  0.129639,  0.099976,  0.183716,  0.116333,  0.225464,\r\n        0.120422,  0.21936 ,  0.140869,  0.147949,  0.140869,  0.156128,\r\n        0.210205,  0.21936 ,  0.082642,  0.183716,  0.121399,  0.022446,...\r\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\r\n\r\n======================================================================\r\nFAIL: testTrainMovingVarsNHWC (__main__.BatchNormFloat16Test)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2546, in testTrainMovingVarsNHWC\r\n    self._testTrainMovingVars(False, data_format='NHWC')\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 2535, in _testTrainMovingVars\r\n    np_output, axis=axis), [0] * channels, rtol=0.001, atol=0.001)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/contrib/layers/python/layers/layers_test.py\", line 1675, in assertAllClose\r\n    return super(BatchNormTest, self).assertAllClose(*args, **kwargs)\r\n  File \"/home/jake/.cache/bazel/_bazel_jake/68a62076e91007a7908bc42a32e4cff9/execroot/tensorflow/bazel-out/local-py3-opt/bin/tensorflow/contrib/layers/layers_test.runfiles/org_tensorflow/tensorflow/python/framework/test_util.py\", line 518, in assertAllClose\r\n    np.testing.assert_allclose(a, b, rtol=rtol, atol=atol)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 1392, in assert_allclose\r\n    verbose=verbose, header=header)\r\n  File \"/opt/anaconda3/lib/python3.6/site-packages/numpy/testing/utils.py\", line 739, in assert_array_compare\r\n    raise AssertionError(msg)\r\nAssertionError: \r\nNot equal to tolerance rtol=0.001, atol=0.001\r\n\r\n(mismatch 100.0%)\r\n x: array([ 0.175537,  0.158203,  0.179565,  0.151001,  0.17041 ,  0.21936 ,\r\n        0.092834,  0.25415 ,  0.190796,  0.135742,  0.118347,  0.112244,\r\n        0.074463,  0.165283,  0.198975,  0.093872,  0.233643,  0.165283,...\r\n y: array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\r\n       0, 0, 0, 0, 0, 0, 0, 0, 0])\r\n\r\n----------------------------------------------------------------------\r\nRan 462 tests in 8.531s\r\n\r\nFAILED (failures=2, skipped=39)\r\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\r\nnot close lhs =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\r\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\r\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\r\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\r\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\r\n  0.28051758  0.13879395]\r\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\r\nnot close dif =  [ 0.14697266  0.12963867  0.09997559  0.18371582  0.11633301  0.22546387\r\n  0.12042236  0.21936035  0.14086914  0.14794922  0.14086914  0.15612793\r\n  0.21020508  0.21936035  0.0826416   0.18371582  0.12139893  0.02244568\r\n  0.13879395  0.23168945  0.17858887  0.15612793  0.12042236  0.20507812\r\n  0.18164062  0.08569336  0.12451172  0.11425781  0.12756348  0.19799805\r\n  0.28051758  0.13879395]\r\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001]\r\ndtype = float16, shape = (32,)\r\nnot close where =  (array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\r\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31]),)\r\nnot close lhs =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\r\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\r\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\r\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\r\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\r\n  0.16638184  0.24182129]\r\nnot close rhs =  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\r\nnot close dif =  [ 0.17553711  0.15820312  0.17956543  0.15100098  0.17041016  0.21936035\r\n  0.09283447  0.25415039  0.1907959   0.13574219  0.11834717  0.11224365\r\n  0.07446289  0.1652832   0.19897461  0.09387207  0.23364258  0.1652832\r\n  0.20922852  0.23266602  0.15820312  0.19995117  0.18371582  0.05307007\r\n  0.17651367  0.11425781  0.19384766  0.12548828  0.23168945  0.06329346\r\n  0.16638184  0.24182129]\r\nnot close tol =  [ 0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001  0.001\r\n  0.001  0.001]\r\ndtype = float16, shape = (32,)\r\n```"}