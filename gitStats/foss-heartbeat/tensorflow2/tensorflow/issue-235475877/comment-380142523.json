{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/380142523", "html_url": "https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-380142523", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10669", "id": 380142523, "node_id": "MDEyOklzc3VlQ29tbWVudDM4MDE0MjUyMw==", "user": {"login": "ydp", "id": 1532805, "node_id": "MDQ6VXNlcjE1MzI4MDU=", "avatar_url": "https://avatars0.githubusercontent.com/u/1532805?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ydp", "html_url": "https://github.com/ydp", "followers_url": "https://api.github.com/users/ydp/followers", "following_url": "https://api.github.com/users/ydp/following{/other_user}", "gists_url": "https://api.github.com/users/ydp/gists{/gist_id}", "starred_url": "https://api.github.com/users/ydp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ydp/subscriptions", "organizations_url": "https://api.github.com/users/ydp/orgs", "repos_url": "https://api.github.com/users/ydp/repos", "events_url": "https://api.github.com/users/ydp/events{/privacy}", "received_events_url": "https://api.github.com/users/ydp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-04-10T15:27:18Z", "updated_at": "2018-04-10T15:27:18Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=26812149\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/July-Morning\">@July-Morning</a>  Hi, I see you finally solved the problem, the reason is variable batch size. how did you solve this in your production environment? Do you mean  the batch size of training and evaluation has to be the same? But in my situation, we often train in large batch size like 4096, but inference with small batch like 20, thus I would be definitely suffer from this problem, is this the situation here?</p>\n<p>Thanks very much.</p>", "body_text": "@July-Morning  Hi, I see you finally solved the problem, the reason is variable batch size. how did you solve this in your production environment? Do you mean  the batch size of training and evaluation has to be the same? But in my situation, we often train in large batch size like 4096, but inference with small batch like 20, thus I would be definitely suffer from this problem, is this the situation here?\nThanks very much.", "body": "@July-Morning  Hi, I see you finally solved the problem, the reason is variable batch size. how did you solve this in your production environment? Do you mean  the batch size of training and evaluation has to be the same? But in my situation, we often train in large batch size like 4096, but inference with small batch like 20, thus I would be definitely suffer from this problem, is this the situation here?\r\n\r\nThanks very much."}