{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/318378089", "html_url": "https://github.com/tensorflow/tensorflow/issues/10669#issuecomment-318378089", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/10669", "id": 318378089, "node_id": "MDEyOklzc3VlQ29tbWVudDMxODM3ODA4OQ==", "user": {"login": "July-Morning", "id": 26812149, "node_id": "MDQ6VXNlcjI2ODEyMTQ5", "avatar_url": "https://avatars1.githubusercontent.com/u/26812149?v=4", "gravatar_id": "", "url": "https://api.github.com/users/July-Morning", "html_url": "https://github.com/July-Morning", "followers_url": "https://api.github.com/users/July-Morning/followers", "following_url": "https://api.github.com/users/July-Morning/following{/other_user}", "gists_url": "https://api.github.com/users/July-Morning/gists{/gist_id}", "starred_url": "https://api.github.com/users/July-Morning/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/July-Morning/subscriptions", "organizations_url": "https://api.github.com/users/July-Morning/orgs", "repos_url": "https://api.github.com/users/July-Morning/repos", "events_url": "https://api.github.com/users/July-Morning/events{/privacy}", "received_events_url": "https://api.github.com/users/July-Morning/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-27T14:25:48Z", "updated_at": "2017-07-27T15:02:59Z", "author_association": "NONE", "body_html": "<p>Well, I managed to save my model in SavedModel format and load it back with Python, but running time is as slow as when using a frozen graph.</p>\n<p>Here is how I save my model:<br>\n`   export_path = './saved_model' + str(datetime.now()) + '/'<br>\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)</p>\n<pre><code>tensor_info_im = tf.saved_model.utils.build_tensor_info(model.image_input)\ntensor_info_pr = tf.saved_model.utils.build_tensor_info(model.keep_prob)\ntensor_info_bb = tf.saved_model.utils.build_tensor_info(model.det_boxes)\ntensor_info_sc = tf.saved_model.utils.build_tensor_info(model.det_probs)\n\nprediction_signature = (\n    tf.saved_model.signature_def_utils.build_signature_def(\n        inputs={'image_input:0': tensor_info_im, 'keep_prob:0':tensor_info_pr},\n        outputs={'bbox/trimming/bbox:0': tensor_info_bb, 'probability/score:0': tensor_info_sc},\n        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\nbuilder.add_meta_graph_and_variables(\n    sess, [tf.saved_model.tag_constants.SERVING],\n    signature_def_map={'predict': prediction_signature}, clear_devices = True)\n\nbuilder.save()\n</code></pre>\n<p>`</p>\n<p>And this is how I load it:<br>\n<code>tf.saved_model.loader.load(sess, [\"serve\"], export_dir)</code></p>\n<p>Is something wrong here?</p>\n<p>Something else that I noticed: when I am running a restored tf session in a loop, first two runs are approximately as slow as with frozen graph or SavedModel. However after that it gets much (2x-3x) faster.</p>", "body_text": "Well, I managed to save my model in SavedModel format and load it back with Python, but running time is as slow as when using a frozen graph.\nHere is how I save my model:\n`   export_path = './saved_model' + str(datetime.now()) + '/'\nbuilder = tf.saved_model.builder.SavedModelBuilder(export_path)\ntensor_info_im = tf.saved_model.utils.build_tensor_info(model.image_input)\ntensor_info_pr = tf.saved_model.utils.build_tensor_info(model.keep_prob)\ntensor_info_bb = tf.saved_model.utils.build_tensor_info(model.det_boxes)\ntensor_info_sc = tf.saved_model.utils.build_tensor_info(model.det_probs)\n\nprediction_signature = (\n    tf.saved_model.signature_def_utils.build_signature_def(\n        inputs={'image_input:0': tensor_info_im, 'keep_prob:0':tensor_info_pr},\n        outputs={'bbox/trimming/bbox:0': tensor_info_bb, 'probability/score:0': tensor_info_sc},\n        method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\nbuilder.add_meta_graph_and_variables(\n    sess, [tf.saved_model.tag_constants.SERVING],\n    signature_def_map={'predict': prediction_signature}, clear_devices = True)\n\nbuilder.save()\n\n`\nAnd this is how I load it:\ntf.saved_model.loader.load(sess, [\"serve\"], export_dir)\nIs something wrong here?\nSomething else that I noticed: when I am running a restored tf session in a loop, first two runs are approximately as slow as with frozen graph or SavedModel. However after that it gets much (2x-3x) faster.", "body": "Well, I managed to save my model in SavedModel format and load it back with Python, but running time is as slow as when using a frozen graph.\r\n\r\nHere is how I save my model:\r\n`   export_path = './saved_model' + str(datetime.now()) + '/'\r\n    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\r\n\r\n    tensor_info_im = tf.saved_model.utils.build_tensor_info(model.image_input)\r\n    tensor_info_pr = tf.saved_model.utils.build_tensor_info(model.keep_prob)\r\n    tensor_info_bb = tf.saved_model.utils.build_tensor_info(model.det_boxes)\r\n    tensor_info_sc = tf.saved_model.utils.build_tensor_info(model.det_probs)\r\n    \r\n    prediction_signature = (\r\n        tf.saved_model.signature_def_utils.build_signature_def(\r\n            inputs={'image_input:0': tensor_info_im, 'keep_prob:0':tensor_info_pr},\r\n            outputs={'bbox/trimming/bbox:0': tensor_info_bb, 'probability/score:0': tensor_info_sc},\r\n            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\r\n    builder.add_meta_graph_and_variables(\r\n        sess, [tf.saved_model.tag_constants.SERVING],\r\n        signature_def_map={'predict': prediction_signature}, clear_devices = True)\r\n\r\n    builder.save()\r\n`\r\n\r\nAnd this is how I load it:\r\n` tf.saved_model.loader.load(sess, [\"serve\"], export_dir)\r\n`\r\n\r\nIs something wrong here?\r\n\r\nSomething else that I noticed: when I am running a restored tf session in a loop, first two runs are approximately as slow as with frozen graph or SavedModel. However after that it gets much (2x-3x) faster. "}