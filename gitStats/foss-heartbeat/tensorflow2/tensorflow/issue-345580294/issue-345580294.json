{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21227", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21227/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21227/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21227/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21227", "id": 345580294, "node_id": "MDU6SXNzdWUzNDU1ODAyOTQ=", "number": 21227, "title": "Confused in Distributed Training in Tensorflow", "user": {"login": "282866787", "id": 26756710, "node_id": "MDQ6VXNlcjI2NzU2NzEw", "avatar_url": "https://avatars2.githubusercontent.com/u/26756710?v=4", "gravatar_id": "", "url": "https://api.github.com/users/282866787", "html_url": "https://github.com/282866787", "followers_url": "https://api.github.com/users/282866787/followers", "following_url": "https://api.github.com/users/282866787/following{/other_user}", "gists_url": "https://api.github.com/users/282866787/gists{/gist_id}", "starred_url": "https://api.github.com/users/282866787/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/282866787/subscriptions", "organizations_url": "https://api.github.com/users/282866787/orgs", "repos_url": "https://api.github.com/users/282866787/repos", "events_url": "https://api.github.com/users/282866787/events{/privacy}", "received_events_url": "https://api.github.com/users/282866787/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 996845227, "node_id": "MDU6TGFiZWw5OTY4NDUyMjc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:dist-strat", "name": "comp:dist-strat", "color": "0052cc", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "yuefengz", "id": 1647833, "node_id": "MDQ6VXNlcjE2NDc4MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1647833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuefengz", "html_url": "https://github.com/yuefengz", "followers_url": "https://api.github.com/users/yuefengz/followers", "following_url": "https://api.github.com/users/yuefengz/following{/other_user}", "gists_url": "https://api.github.com/users/yuefengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuefengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuefengz/subscriptions", "organizations_url": "https://api.github.com/users/yuefengz/orgs", "repos_url": "https://api.github.com/users/yuefengz/repos", "events_url": "https://api.github.com/users/yuefengz/events{/privacy}", "received_events_url": "https://api.github.com/users/yuefengz/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "yuefengz", "id": 1647833, "node_id": "MDQ6VXNlcjE2NDc4MzM=", "avatar_url": "https://avatars0.githubusercontent.com/u/1647833?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yuefengz", "html_url": "https://github.com/yuefengz", "followers_url": "https://api.github.com/users/yuefengz/followers", "following_url": "https://api.github.com/users/yuefengz/following{/other_user}", "gists_url": "https://api.github.com/users/yuefengz/gists{/gist_id}", "starred_url": "https://api.github.com/users/yuefengz/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yuefengz/subscriptions", "organizations_url": "https://api.github.com/users/yuefengz/orgs", "repos_url": "https://api.github.com/users/yuefengz/repos", "events_url": "https://api.github.com/users/yuefengz/events{/privacy}", "received_events_url": "https://api.github.com/users/yuefengz/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-07-30T01:10:54Z", "updated_at": "2018-09-16T04:59:34Z", "closed_at": "2018-09-16T04:59:34Z", "author_association": "NONE", "body_html": "<p>Hi,I have tried several methods to build my distributed training system , but the training speed is fallacious. As I want, several machines with one master and muti ps(3 ps),the training speed depends on the slowest one of the ps. But what I got it is almost 200s every 100steps in En-De training, I'm very confused about this. What's more ,the t2t defines the master and ps , but in conventional tensorflow distributed training tutorial, there should be worker and ps. The ps plays the role of updating variables like master in T2T , and the worker computes the gradients parallel like ps in T2T. This really makes me amazing! I have ask questions in gitter ,but no one answer me . I hope you can give me some advice !</p>", "body_text": "Hi,I have tried several methods to build my distributed training system , but the training speed is fallacious. As I want, several machines with one master and muti ps(3 ps),the training speed depends on the slowest one of the ps. But what I got it is almost 200s every 100steps in En-De training, I'm very confused about this. What's more ,the t2t defines the master and ps , but in conventional tensorflow distributed training tutorial, there should be worker and ps. The ps plays the role of updating variables like master in T2T , and the worker computes the gradients parallel like ps in T2T. This really makes me amazing! I have ask questions in gitter ,but no one answer me . I hope you can give me some advice !", "body": "Hi,I have tried several methods to build my distributed training system , but the training speed is fallacious. As I want, several machines with one master and muti ps(3 ps),the training speed depends on the slowest one of the ps. But what I got it is almost 200s every 100steps in En-De training, I'm very confused about this. What's more ,the t2t defines the master and ps , but in conventional tensorflow distributed training tutorial, there should be worker and ps. The ps plays the role of updating variables like master in T2T , and the worker computes the gradients parallel like ps in T2T. This really makes me amazing! I have ask questions in gitter ,but no one answer me . I hope you can give me some advice !"}