{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9400", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9400/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9400/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9400/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/9400", "id": 223697349, "node_id": "MDU6SXNzdWUyMjM2OTczNDk=", "number": 9400, "title": "Tensorflow is still taking up all GPU memory despite allocation of memory", "user": {"login": "yujulucy", "id": 12662268, "node_id": "MDQ6VXNlcjEyNjYyMjY4", "avatar_url": "https://avatars3.githubusercontent.com/u/12662268?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yujulucy", "html_url": "https://github.com/yujulucy", "followers_url": "https://api.github.com/users/yujulucy/followers", "following_url": "https://api.github.com/users/yujulucy/following{/other_user}", "gists_url": "https://api.github.com/users/yujulucy/gists{/gist_id}", "starred_url": "https://api.github.com/users/yujulucy/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yujulucy/subscriptions", "organizations_url": "https://api.github.com/users/yujulucy/orgs", "repos_url": "https://api.github.com/users/yujulucy/repos", "events_url": "https://api.github.com/users/yujulucy/events{/privacy}", "received_events_url": "https://api.github.com/users/yujulucy/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 386191887, "node_id": "MDU6TGFiZWwzODYxOTE4ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20response", "name": "stat:awaiting response", "color": "f4b400", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 21, "created_at": "2017-04-24T03:44:30Z", "updated_at": "2018-05-15T06:26:11Z", "closed_at": "2017-09-15T12:45:19Z", "author_association": "NONE", "body_html": "<p><strong>System Information:</strong></p>\n<ul>\n<li>OS Platform: Linux Ubuntu 16.04</li>\n<li>TensorFlow installed from binary</li>\n<li>TensorFlow version: 1.0.1</li>\n<li>CUDA version: 8.0</li>\n<li>cuDNN version: 5.1</li>\n</ul>\n<p>output for print(tf.GIT_VERSION, tf.VERSION):<br>\n('v1.0.0-65-g4763edf-dirty', '1.0.1')</p>\n<p><strong>Problem:</strong><br>\nI have three codes running GPU-enabled TensorFlow. Currently, when the GPU is allocated for two of the processes, the GPU does get allocated. However, no matter how much GPU I allocate to the last process, it takes up all the GPU, disallowing other two processes to run simultaneously.</p>\n<p><strong>First process (the one with problem)</strong><br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:<br>\nname: GeForce GTX 1060<br>\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705<br>\npciBusID 0000:01:00.0<br>\nTotal memory: 5.93GiB<br>\nFree memory: 5.68GiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)</p>\n<p><strong>Second process:</strong><br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:<br>\nname: GeForce GTX 1060<br>\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705<br>\npciBusID 0000:01:00.0<br>\nTotal memory: 5.93GiB<br>\nFree memory: 237.00MiB<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y<br>\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.19G (1273049856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.07G (1145744896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 983.40M (1031170560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 885.06M (928053504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 796.55M (835248128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 716.90M (751723264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 645.21M (676550912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 580.69M (608896000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 522.62M (548006400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 470.36M (493205760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 423.32M (443885312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 380.99M (399496960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 342.89M (359547392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 308.60M (323592704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 277.74M (291233536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 249.97M (262110208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 224.97M (235899392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY<br>\nWARNING:apscheduler.scheduler:Execution of job \"session (trigger: interval[0:00:01], next run at: 2017-04-24 11:24:48.702309)\" skipped: maximum number of running instances reached (1)<br>\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR<br>\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM<br>\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream-&gt;parent()-&gt;GetConvolveAlgorithms(&amp;algorithms)</p>\n<p><strong>The code I am using to allocate GPU for the first process:</strong><br>\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)<br>\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))</p>\n<p>I have also looked up stackoverflow but it does not seem to have the same problems raised.<br>\nSome pages I referred to:<br>\n<a href=\"http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\" rel=\"nofollow\">http://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory</a><br>\n<a href=\"https://www.tensorflow.org/tutorials/using_gpu\" rel=\"nofollow\">https://www.tensorflow.org/tutorials/using_gpu</a><br>\n<a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"120118306\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/398\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/398/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/398\">#398</a></p>", "body_text": "System Information:\n\nOS Platform: Linux Ubuntu 16.04\nTensorFlow installed from binary\nTensorFlow version: 1.0.1\nCUDA version: 8.0\ncuDNN version: 5.1\n\noutput for print(tf.GIT_VERSION, tf.VERSION):\n('v1.0.0-65-g4763edf-dirty', '1.0.1')\nProblem:\nI have three codes running GPU-enabled TensorFlow. Currently, when the GPU is allocated for two of the processes, the GPU does get allocated. However, no matter how much GPU I allocate to the last process, it takes up all the GPU, disallowing other two processes to run simultaneously.\nFirst process (the one with problem)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\nname: GeForce GTX 1060\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\npciBusID 0000:01:00.0\nTotal memory: 5.93GiB\nFree memory: 5.68GiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\nSecond process:\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties:\nname: GeForce GTX 1060\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\npciBusID 0000:01:00.0\nTotal memory: 5.93GiB\nFree memory: 237.00MiB\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.19G (1273049856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.07G (1145744896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 983.40M (1031170560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 885.06M (928053504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 796.55M (835248128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 716.90M (751723264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 645.21M (676550912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 580.69M (608896000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 522.62M (548006400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 470.36M (493205760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 423.32M (443885312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 380.99M (399496960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 342.89M (359547392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 308.60M (323592704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 277.74M (291233536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 249.97M (262110208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 224.97M (235899392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\nWARNING:apscheduler.scheduler:Execution of job \"session (trigger: interval[0:00:01], next run at: 2017-04-24 11:24:48.702309)\" skipped: maximum number of running instances reached (1)\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms)\nThe code I am using to allocate GPU for the first process:\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\nI have also looked up stackoverflow but it does not seem to have the same problems raised.\nSome pages I referred to:\nhttp://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory\nhttps://www.tensorflow.org/tutorials/using_gpu\n#398", "body": "**System Information:**\r\n- OS Platform: Linux Ubuntu 16.04\r\n- TensorFlow installed from binary\r\n- TensorFlow version: 1.0.1\r\n- CUDA version: 8.0\r\n- cuDNN version: 5.1\r\n\r\noutput for print(tf.GIT_VERSION, tf.VERSION):\r\n('v1.0.0-65-g4763edf-dirty', '1.0.1')\r\n\r\n**Problem:**\r\nI have three codes running GPU-enabled TensorFlow. Currently, when the GPU is allocated for two of the processes, the GPU does get allocated. However, no matter how much GPU I allocate to the last process, it takes up all the GPU, disallowing other two processes to run simultaneously. \r\n\r\n**First process (the one with problem)**\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 5.68GiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\n\r\n**Second process:**\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:885] Found device 0 with properties: \r\nname: GeForce GTX 1060\r\nmajor: 6 minor: 1 memoryClockRate (GHz) 1.6705\r\npciBusID 0000:01:00.0\r\nTotal memory: 5.93GiB\r\nFree memory: 237.00MiB\r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:906] DMA: 0 \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:916] 0:   Y \r\nI tensorflow/core/common_runtime/gpu/gpu_device.cc:975] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1060, pci bus id: 0000:01:00.0)\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.19G (1273049856 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 1.07G (1145744896 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 983.40M (1031170560 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 885.06M (928053504 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 796.55M (835248128 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 716.90M (751723264 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 645.21M (676550912 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 580.69M (608896000 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 522.62M (548006400 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 470.36M (493205760 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 423.32M (443885312 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 380.99M (399496960 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 342.89M (359547392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 308.60M (323592704 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 277.74M (291233536 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 249.97M (262110208 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nE tensorflow/stream_executor/cuda/cuda_driver.cc:1002] failed to allocate 224.97M (235899392 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY\r\nWARNING:apscheduler.scheduler:Execution of job \"session (trigger: interval[0:00:01], next run at: 2017-04-24 11:24:48.702309)\" skipped: maximum number of running instances reached (1)\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:397] could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\r\nE tensorflow/stream_executor/cuda/cuda_dnn.cc:364] could not destroy cudnn handle: CUDNN_STATUS_BAD_PARAM\r\nF tensorflow/core/kernels/conv_ops.cc:605] Check failed: stream->parent()->GetConvolveAlgorithms(&algorithms) \r\n\r\n**The code I am using to allocate GPU for the first process:**\r\ngpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.3)\r\nsess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options, log_device_placement=False))\r\n\r\nI have also looked up stackoverflow but it does not seem to have the same problems raised.\r\nSome pages I referred to:\r\nhttp://stackoverflow.com/questions/34199233/how-to-prevent-tensorflow-from-allocating-the-totality-of-a-gpu-memory \r\nhttps://www.tensorflow.org/tutorials/using_gpu \r\nhttps://github.com/tensorflow/tensorflow/issues/398 "}