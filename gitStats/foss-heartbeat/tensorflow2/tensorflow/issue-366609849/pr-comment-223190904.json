{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/223190904", "pull_request_review_id": 162266895, "id": 223190904, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDIyMzE5MDkwNA==", "diff_hunk": "@@ -2721,11 +2721,13 @@ def batch_gather(params, indices, name=None):\n       raise ValueError(\"batch_gather does not allow indices with unknown \"\n                        \"shape.\")\n     batch_indices = indices\n+    dtype = indices.dtype\n     accum_dim_value = 1", "path": "tensorflow/python/ops/array_ops.py", "position": 5, "original_position": 5, "commit_id": "1f633aeb1d7f9eb336d659804a19c1452f53f5aa", "original_commit_id": "4dac6317fc2b14c572eeb2203eb39e0e57513853", "user": {"login": "wwt17", "id": 10792281, "node_id": "MDQ6VXNlcjEwNzkyMjgx", "avatar_url": "https://avatars0.githubusercontent.com/u/10792281?v=4", "gravatar_id": "", "url": "https://api.github.com/users/wwt17", "html_url": "https://github.com/wwt17", "followers_url": "https://api.github.com/users/wwt17/followers", "following_url": "https://api.github.com/users/wwt17/following{/other_user}", "gists_url": "https://api.github.com/users/wwt17/gists{/gist_id}", "starred_url": "https://api.github.com/users/wwt17/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/wwt17/subscriptions", "organizations_url": "https://api.github.com/users/wwt17/orgs", "repos_url": "https://api.github.com/users/wwt17/repos", "events_url": "https://api.github.com/users/wwt17/events{/privacy}", "received_events_url": "https://api.github.com/users/wwt17/received_events", "type": "User", "site_admin": false}, "body": "Well, this function do support dynamic shapes, because `params_shape = shape(params)` is a Tensor and `params_shape[dim]` is a slice and all these are dynamic. I experimented dynamic shapes, and it does work. Also, I found a Dimension * Tensor returns a Tensor, and, therefore, your proposal has no effect after first multiplication of `accum_dim_value`. By the way, all these slicing, multiplication and `gen_math_ops._range` inserts nodes in the graph. I believe using `gen_math_ops.cast` is not a bad idea.\r\nIf we really use `Dimension` all the way, `gen_math_ops._range` will anyway convert `Dimension` to `Tensor`. Also, since `batch_indices` is a Tensor, we must convert all these to `Tensor` in the end.", "created_at": "2018-10-06T18:59:49Z", "updated_at": "2018-10-06T19:43:34Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/22714#discussion_r223190904", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22714", "author_association": "NONE", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/223190904"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/22714#discussion_r223190904"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/22714"}}, "body_html": "<p>Well, this function do support dynamic shapes, because <code>params_shape = shape(params)</code> is a Tensor and <code>params_shape[dim]</code> is a slice and all these are dynamic. I experimented dynamic shapes, and it does work. Also, I found a Dimension * Tensor returns a Tensor, and, therefore, your proposal has no effect after first multiplication of <code>accum_dim_value</code>. By the way, all these slicing, multiplication and <code>gen_math_ops._range</code> inserts nodes in the graph. I believe using <code>gen_math_ops.cast</code> is not a bad idea.<br>\nIf we really use <code>Dimension</code> all the way, <code>gen_math_ops._range</code> will anyway convert <code>Dimension</code> to <code>Tensor</code>. Also, since <code>batch_indices</code> is a Tensor, we must convert all these to <code>Tensor</code> in the end.</p>", "body_text": "Well, this function do support dynamic shapes, because params_shape = shape(params) is a Tensor and params_shape[dim] is a slice and all these are dynamic. I experimented dynamic shapes, and it does work. Also, I found a Dimension * Tensor returns a Tensor, and, therefore, your proposal has no effect after first multiplication of accum_dim_value. By the way, all these slicing, multiplication and gen_math_ops._range inserts nodes in the graph. I believe using gen_math_ops.cast is not a bad idea.\nIf we really use Dimension all the way, gen_math_ops._range will anyway convert Dimension to Tensor. Also, since batch_indices is a Tensor, we must convert all these to Tensor in the end.", "in_reply_to_id": 222728491}