{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/199760798", "html_url": "https://github.com/tensorflow/tensorflow/issues/800#issuecomment-199760798", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/800", "id": 199760798, "node_id": "MDEyOklzc3VlQ29tbWVudDE5OTc2MDc5OA==", "user": {"login": "twimnox", "id": 11200412, "node_id": "MDQ6VXNlcjExMjAwNDEy", "avatar_url": "https://avatars0.githubusercontent.com/u/11200412?v=4", "gravatar_id": "", "url": "https://api.github.com/users/twimnox", "html_url": "https://github.com/twimnox", "followers_url": "https://api.github.com/users/twimnox/followers", "following_url": "https://api.github.com/users/twimnox/following{/other_user}", "gists_url": "https://api.github.com/users/twimnox/gists{/gist_id}", "starred_url": "https://api.github.com/users/twimnox/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/twimnox/subscriptions", "organizations_url": "https://api.github.com/users/twimnox/orgs", "repos_url": "https://api.github.com/users/twimnox/repos", "events_url": "https://api.github.com/users/twimnox/events{/privacy}", "received_events_url": "https://api.github.com/users/twimnox/received_events", "type": "User", "site_admin": false}, "created_at": "2016-03-22T11:09:49Z", "updated_at": "2016-03-22T11:09:49Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=7863989\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/HellMood\">@HellMood</a>  that is probably my problem. I'm also using difficult and human labeled data which in some cases might be mislabeled. If I decrease my learning rate from 0.07 to 0.02 the training goes on without loss peaks. I'm also using a small batch size (16 to 32). Using a 32 size will most probably induce my loss to sudden rise, agreeing with what you said about reducing the batch size to not stack errors too high.</p>\n<p>You're telling me that if however a model loss peaks at K steps, you will use the model at K-n steps (step where the loss is still stably converging)?</p>", "body_text": "@HellMood  that is probably my problem. I'm also using difficult and human labeled data which in some cases might be mislabeled. If I decrease my learning rate from 0.07 to 0.02 the training goes on without loss peaks. I'm also using a small batch size (16 to 32). Using a 32 size will most probably induce my loss to sudden rise, agreeing with what you said about reducing the batch size to not stack errors too high.\nYou're telling me that if however a model loss peaks at K steps, you will use the model at K-n steps (step where the loss is still stably converging)?", "body": "@HellMood  that is probably my problem. I'm also using difficult and human labeled data which in some cases might be mislabeled. If I decrease my learning rate from 0.07 to 0.02 the training goes on without loss peaks. I'm also using a small batch size (16 to 32). Using a 32 size will most probably induce my loss to sudden rise, agreeing with what you said about reducing the batch size to not stack errors too high.\n\nYou're telling me that if however a model loss peaks at K steps, you will use the model at K-n steps (step where the loss is still stably converging)?\n"}