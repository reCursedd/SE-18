{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/800", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/800/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/800/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/800/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/800", "id": 127191536, "node_id": "MDU6SXNzdWUxMjcxOTE1MzY=", "number": 800, "title": "Sudden Loss Explosion, followed by total missevaluation", "user": {"login": "HellMood", "id": 7863989, "node_id": "MDQ6VXNlcjc4NjM5ODk=", "avatar_url": "https://avatars2.githubusercontent.com/u/7863989?v=4", "gravatar_id": "", "url": "https://api.github.com/users/HellMood", "html_url": "https://github.com/HellMood", "followers_url": "https://api.github.com/users/HellMood/followers", "following_url": "https://api.github.com/users/HellMood/following{/other_user}", "gists_url": "https://api.github.com/users/HellMood/gists{/gist_id}", "starred_url": "https://api.github.com/users/HellMood/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/HellMood/subscriptions", "organizations_url": "https://api.github.com/users/HellMood/orgs", "repos_url": "https://api.github.com/users/HellMood/repos", "events_url": "https://api.github.com/users/HellMood/events{/privacy}", "received_events_url": "https://api.github.com/users/HellMood/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 9, "created_at": "2016-01-18T09:59:07Z", "updated_at": "2017-02-10T16:48:24Z", "closed_at": "2016-01-18T22:14:16Z", "author_association": "NONE", "body_html": "<p>Hello altogether =)</p>\n<p>I'm testing Tensorflow 0.60 (GPU Version) for Python 3.4 on Ubuntu 15.10. The used GPU is a GT 960m. Testing the CIFAR10 Demo for over two days in a row gave me the ~86.x% everbody gets after evaluation. After transforming my own data into batches (cifar10 demo like) and adjusting the cifar-python files accordingly, i achieved the following results for my binary classification task (always stopped training, then evaluating, then restarting).</p>\n<p>after 240 steps (of 128 size batches)  -- &gt;&gt; 92 %<br>\nafter 420 steps (of 128 size batches)  -- &gt;&gt; 92 %<br>\nafter 1500 steps (of 128 size batches)  -- &gt;&gt; 94 %<br>\nafter 2000 steps (of 128 size batches)  -- &gt;&gt; 95 %</p>\n<p>Over the course of these tests, the loss value - printed on the console and retrieved via tensorboard, went from about 12 to about 3. Over the (last) weekend, i raised the maximum steps to 10 million, and let things just run. Today in the morning i did an evaluation on the network, which arrived at a loss of 0.6 but evaluated with 50.4 % which looks like totally random (binary task). Scrolling through the logs i realized, there was a sudden loss explosion at around step 70000, jumping from like 2 to 2000 (!!), and - after slowly recovering from that, again up to about 2 000 000 (!!) million, and also slowly recovering from that downto said 0.6, but with the said total missevaluation. Also noteworthy, the speed from almost constantly 100 examples per second broke downto 30 when the loss spikes occured. The eventfile (one of them) went up to 9 Gigabytes in size, and re-checking with tensorboard gave strange results (showing sometimes 4 million, and sometimes 900 million learn steps, appeared to be totally broken somehow).</p>\n<p>Appart from just a different batch file, and adjusted image size parameters, the network is exactly the same as in CIFAR 10.</p>\n<p>I'm sorry, i can't provide the logs, but right now i am relearning the whole thing and monitoring if this happens again.</p>\n<p>Did anybody of experience something like this before?</p>", "body_text": "Hello altogether =)\nI'm testing Tensorflow 0.60 (GPU Version) for Python 3.4 on Ubuntu 15.10. The used GPU is a GT 960m. Testing the CIFAR10 Demo for over two days in a row gave me the ~86.x% everbody gets after evaluation. After transforming my own data into batches (cifar10 demo like) and adjusting the cifar-python files accordingly, i achieved the following results for my binary classification task (always stopped training, then evaluating, then restarting).\nafter 240 steps (of 128 size batches)  -- >> 92 %\nafter 420 steps (of 128 size batches)  -- >> 92 %\nafter 1500 steps (of 128 size batches)  -- >> 94 %\nafter 2000 steps (of 128 size batches)  -- >> 95 %\nOver the course of these tests, the loss value - printed on the console and retrieved via tensorboard, went from about 12 to about 3. Over the (last) weekend, i raised the maximum steps to 10 million, and let things just run. Today in the morning i did an evaluation on the network, which arrived at a loss of 0.6 but evaluated with 50.4 % which looks like totally random (binary task). Scrolling through the logs i realized, there was a sudden loss explosion at around step 70000, jumping from like 2 to 2000 (!!), and - after slowly recovering from that, again up to about 2 000 000 (!!) million, and also slowly recovering from that downto said 0.6, but with the said total missevaluation. Also noteworthy, the speed from almost constantly 100 examples per second broke downto 30 when the loss spikes occured. The eventfile (one of them) went up to 9 Gigabytes in size, and re-checking with tensorboard gave strange results (showing sometimes 4 million, and sometimes 900 million learn steps, appeared to be totally broken somehow).\nAppart from just a different batch file, and adjusted image size parameters, the network is exactly the same as in CIFAR 10.\nI'm sorry, i can't provide the logs, but right now i am relearning the whole thing and monitoring if this happens again.\nDid anybody of experience something like this before?", "body": "Hello altogether =)\n\nI'm testing Tensorflow 0.60 (GPU Version) for Python 3.4 on Ubuntu 15.10. The used GPU is a GT 960m. Testing the CIFAR10 Demo for over two days in a row gave me the ~86.x% everbody gets after evaluation. After transforming my own data into batches (cifar10 demo like) and adjusting the cifar-python files accordingly, i achieved the following results for my binary classification task (always stopped training, then evaluating, then restarting).\n\nafter 240 steps (of 128 size batches)  -- >> 92 %\nafter 420 steps (of 128 size batches)  -- >> 92 %\nafter 1500 steps (of 128 size batches)  -- >> 94 %\nafter 2000 steps (of 128 size batches)  -- >> 95 %\n\nOver the course of these tests, the loss value - printed on the console and retrieved via tensorboard, went from about 12 to about 3. Over the (last) weekend, i raised the maximum steps to 10 million, and let things just run. Today in the morning i did an evaluation on the network, which arrived at a loss of 0.6 but evaluated with 50.4 % which looks like totally random (binary task). Scrolling through the logs i realized, there was a sudden loss explosion at around step 70000, jumping from like 2 to 2000 (!!), and - after slowly recovering from that, again up to about 2 000 000 (!!) million, and also slowly recovering from that downto said 0.6, but with the said total missevaluation. Also noteworthy, the speed from almost constantly 100 examples per second broke downto 30 when the loss spikes occured. The eventfile (one of them) went up to 9 Gigabytes in size, and re-checking with tensorboard gave strange results (showing sometimes 4 million, and sometimes 900 million learn steps, appeared to be totally broken somehow).\n\nAppart from just a different batch file, and adjusted image size parameters, the network is exactly the same as in CIFAR 10.\n\nI'm sorry, i can't provide the logs, but right now i am relearning the whole thing and monitoring if this happens again.\n\nDid anybody of experience something like this before?\n"}