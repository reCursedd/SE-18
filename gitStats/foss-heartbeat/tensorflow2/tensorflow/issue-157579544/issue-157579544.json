{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2589", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2589/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2589/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2589/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/2589", "id": 157579544, "node_id": "MDU6SXNzdWUxNTc1Nzk1NDQ=", "number": 2589, "title": "Request for Schedule learning rate at arbitrary step(echo)", "user": {"login": "sun9700", "id": 10956421, "node_id": "MDQ6VXNlcjEwOTU2NDIx", "avatar_url": "https://avatars1.githubusercontent.com/u/10956421?v=4", "gravatar_id": "", "url": "https://api.github.com/users/sun9700", "html_url": "https://github.com/sun9700", "followers_url": "https://api.github.com/users/sun9700/followers", "following_url": "https://api.github.com/users/sun9700/following{/other_user}", "gists_url": "https://api.github.com/users/sun9700/gists{/gist_id}", "starred_url": "https://api.github.com/users/sun9700/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/sun9700/subscriptions", "organizations_url": "https://api.github.com/users/sun9700/orgs", "repos_url": "https://api.github.com/users/sun9700/repos", "events_url": "https://api.github.com/users/sun9700/events{/privacy}", "received_events_url": "https://api.github.com/users/sun9700/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-05-31T02:45:20Z", "updated_at": "2016-05-31T04:35:14Z", "closed_at": "2016-05-31T04:35:14Z", "author_association": "NONE", "body_html": "<p>There is only tf.train.exponential_decay for decaying the learning rate at constant step.</p>\n<p>However in <strong>ResNet-1001</strong>: \"Identity Mappings in Deep Residual Networks\", arXiv:1603.05027, 2016,<br>\n<strong>The learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations.</strong><br>\nFollowing [1], for all CIFAR experiments we warm up then training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that.</p>\n<p>It's hard to implement in tensor flow since tensor doesn\u2019t support python comparison.<br>\nThe learning rate can\u2019t be set in if/else according to global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)</p>", "body_text": "There is only tf.train.exponential_decay for decaying the learning rate at constant step.\nHowever in ResNet-1001: \"Identity Mappings in Deep Residual Networks\", arXiv:1603.05027, 2016,\nThe learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations.\nFollowing [1], for all CIFAR experiments we warm up then training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that.\nIt's hard to implement in tensor flow since tensor doesn\u2019t support python comparison.\nThe learning rate can\u2019t be set in if/else according to global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)", "body": "There is only tf.train.exponential_decay for decaying the learning rate at constant step.\n\nHowever in **ResNet-1001**: \"Identity Mappings in Deep Residual Networks\", arXiv:1603.05027, 2016,\n**The learning rate starts from 0.1, and is divided by 10 at 32k and 48k iterations.**\nFollowing [1], for all CIFAR experiments we warm up then training by using a smaller learning rate of 0.01 at the beginning 400 iterations and go back to 0.1 after that.\n\nIt's hard to implement in tensor flow since tensor doesn\u2019t support python comparison.\nThe learning rate can\u2019t be set in if/else according to global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)\n"}