{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/180236936", "html_url": "https://github.com/tensorflow/tensorflow/issues/976#issuecomment-180236936", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/976", "id": 180236936, "node_id": "MDEyOklzc3VlQ29tbWVudDE4MDIzNjkzNg==", "user": {"login": "psmit", "id": 202014, "node_id": "MDQ6VXNlcjIwMjAxNA==", "avatar_url": "https://avatars2.githubusercontent.com/u/202014?v=4", "gravatar_id": "", "url": "https://api.github.com/users/psmit", "html_url": "https://github.com/psmit", "followers_url": "https://api.github.com/users/psmit/followers", "following_url": "https://api.github.com/users/psmit/following{/other_user}", "gists_url": "https://api.github.com/users/psmit/gists{/gist_id}", "starred_url": "https://api.github.com/users/psmit/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/psmit/subscriptions", "organizations_url": "https://api.github.com/users/psmit/orgs", "repos_url": "https://api.github.com/users/psmit/repos", "events_url": "https://api.github.com/users/psmit/events{/privacy}", "received_events_url": "https://api.github.com/users/psmit/received_events", "type": "User", "site_admin": false}, "created_at": "2016-02-05T07:37:54Z", "updated_at": "2016-02-05T07:39:10Z", "author_association": "NONE", "body_html": "<p>I can't seem to find the ops you are mentioning in the documentation, but I'm sure what you are mentioning is a valid use case.</p>\n<p>My use case is a bit different though. My code looks at the moment something like this:</p>\n<pre><code>FEA_DIM=13\nf = parse_single_example(\n  se,\n  features={\n    'input': tf.VarLenFeature(tf.float32),\n    'output': tf.VarLenFeature(tf.int64),\n  }\n)\n\nx = tf.sparse_tensor_to_dense(features['input'])\ny = tf.sparse_tensor_to_dense(features['output'])\n\nx = tf.reshape(x, [-1,FEA_DIM])\ny = tf.reshape(y, [-1,1])\n\nreturn tf.train.shuffle_batch([x,y], enqueue_many=True)\n</code></pre>\n<p>So I use a single TfRecord to hold multiple x-y pairs.\u00b9 For me it looks wrong that data that is stored densely (as a protobuf float_list), would come to a sparse array first, when I am going to transform it right back to a dense array. Note that I can't use tf.FixedLenSequenceFeature as I have no clue how many examples there will be in one record (not even a maximum).</p>\n<p>\u00b9 In reality this is timeseries (speech) data and I do some striding before the features to the queue. This is why there are multiple pairs in one example, it saves space as I don't have to store the context on disk for every x-y separately.</p>", "body_text": "I can't seem to find the ops you are mentioning in the documentation, but I'm sure what you are mentioning is a valid use case.\nMy use case is a bit different though. My code looks at the moment something like this:\nFEA_DIM=13\nf = parse_single_example(\n  se,\n  features={\n    'input': tf.VarLenFeature(tf.float32),\n    'output': tf.VarLenFeature(tf.int64),\n  }\n)\n\nx = tf.sparse_tensor_to_dense(features['input'])\ny = tf.sparse_tensor_to_dense(features['output'])\n\nx = tf.reshape(x, [-1,FEA_DIM])\ny = tf.reshape(y, [-1,1])\n\nreturn tf.train.shuffle_batch([x,y], enqueue_many=True)\n\nSo I use a single TfRecord to hold multiple x-y pairs.\u00b9 For me it looks wrong that data that is stored densely (as a protobuf float_list), would come to a sparse array first, when I am going to transform it right back to a dense array. Note that I can't use tf.FixedLenSequenceFeature as I have no clue how many examples there will be in one record (not even a maximum).\n\u00b9 In reality this is timeseries (speech) data and I do some striding before the features to the queue. This is why there are multiple pairs in one example, it saves space as I don't have to store the context on disk for every x-y separately.", "body": "I can't seem to find the ops you are mentioning in the documentation, but I'm sure what you are mentioning is a valid use case.\n\nMy use case is a bit different though. My code looks at the moment something like this:\n\n```\nFEA_DIM=13\nf = parse_single_example(\n  se,\n  features={\n    'input': tf.VarLenFeature(tf.float32),\n    'output': tf.VarLenFeature(tf.int64),\n  }\n)\n\nx = tf.sparse_tensor_to_dense(features['input'])\ny = tf.sparse_tensor_to_dense(features['output'])\n\nx = tf.reshape(x, [-1,FEA_DIM])\ny = tf.reshape(y, [-1,1])\n\nreturn tf.train.shuffle_batch([x,y], enqueue_many=True)\n```\n\nSo I use a single TfRecord to hold multiple x-y pairs.\u00b9 For me it looks wrong that data that is stored densely (as a protobuf float_list), would come to a sparse array first, when I am going to transform it right back to a dense array. Note that I can't use tf.FixedLenSequenceFeature as I have no clue how many examples there will be in one record (not even a maximum).\n\n\u00b9 In reality this is timeseries (speech) data and I do some striding before the features to the queue. This is why there are multiple pairs in one example, it saves space as I don't have to store the context on disk for every x-y separately. \n"}