{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/310876759", "html_url": "https://github.com/tensorflow/tensorflow/issues/976#issuecomment-310876759", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/976", "id": 310876759, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMDg3Njc1OQ==", "user": {"login": "345ishaan", "id": 7318028, "node_id": "MDQ6VXNlcjczMTgwMjg=", "avatar_url": "https://avatars3.githubusercontent.com/u/7318028?v=4", "gravatar_id": "", "url": "https://api.github.com/users/345ishaan", "html_url": "https://github.com/345ishaan", "followers_url": "https://api.github.com/users/345ishaan/followers", "following_url": "https://api.github.com/users/345ishaan/following{/other_user}", "gists_url": "https://api.github.com/users/345ishaan/gists{/gist_id}", "starred_url": "https://api.github.com/users/345ishaan/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/345ishaan/subscriptions", "organizations_url": "https://api.github.com/users/345ishaan/orgs", "repos_url": "https://api.github.com/users/345ishaan/repos", "events_url": "https://api.github.com/users/345ishaan/events{/privacy}", "received_events_url": "https://api.github.com/users/345ishaan/received_events", "type": "User", "site_admin": false}, "created_at": "2017-06-25T01:56:36Z", "updated_at": "2017-06-25T01:56:36Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1794715\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/ebrevdo\">@ebrevdo</a>  and <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=202014\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/psmit\">@psmit</a></p>\n<p>I am trying to parse SequenceExamples where my context feature is an image and the features are the bounding box locations for the objects in the image.</p>\n<p>I am not able to dequeue the data.<br>\n<strong>Here is how I create the Example:-</strong></p>\n<p>context = tf.train.Features(feature={'image_raw':self._bytes_feature(image)})<br>\nfeature_lists = tf.train.FeatureLists(feature_list={'locations':self._float_feature_list(list(locs[:,0]))})</p>\n<pre><code>\t\texample = tf.train.SequenceExample(context=context,feature_lists=feature_lists)\n</code></pre>\n<p><strong>Here is how I am enqueing:-</strong></p>\n<p>def load_from_tfRecord(self,filename_queue):</p>\n<pre><code>\treader = tf.TFRecordReader()\n\t_, serialized_example = reader.read(filename_queue)\n\n\tcontext,sequence = tf.parse_single_sequence_example(\n\t\t\t\tserialized_example,\n\t\t\t\tcontext_features={'image_raw':tf.FixedLenFeature([], tf.string)},\n\t\t\t\tsequence_features={'locations':tf.FixedLenSequenceFeature([], dtype=tf.float32)}\n\t\t\t\t)\n\t\n\timage = tf.decode_raw(context['image_raw'], tf.uint8)\n\t#locations = tf.decode_raw(features['locations'], tf.int32)\n\n\timage_shape = tf.stack([1,self.ip_width,self.ip_height,3])\n\timage = tf.cast(tf.reshape(image,image_shape),tf.float32)\n\n\t# loc_shape = tf.stack([n_locs,4])\n\t# locations = tf.reshape(locations,loc_shape)\n\n\tlocations = sequence['locations']\n\t\n\n\n\tbatch_images,batch_locations = tf.train.shuffle_batch([image,locations]\n\t\t,enqueue_many=True,batch_size=self.batch_size,num_threads=1,capacity=1000,min_after_dequeue=500)\n\n\treturn batch_images,batch_locations\n</code></pre>\n<p><strong>Here is how I am dequeuing:-</strong></p>\n<p>print \"Loading Trained Model\"<br>\nself.session.run(tf.local_variables_initializer())</p>\n<pre><code>\t#self.session.run(tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()))\n\n\tcoord = tf.train.Coordinator()\n\tthreads = tf.train.start_queue_runners(sess=self.session,coord=coord)\n\n\ttry:\n\t\twhile not coord.should_stop():\n\t\t\tbatch_imgs,batch_labels = self.session.run([self.images,self.labels])\n\t\t\tprint batch_imgs.shape,batch_labels.shape\n\texcept tf.errors.OutOfRangeError:\n\t\tprint \"Done training -- epoch limit reached\"\n\tfinally:\n\t\tcoord.request_stop()\n\n\tcoord.join(threads)\n</code></pre>", "body_text": "@ebrevdo  and @psmit\nI am trying to parse SequenceExamples where my context feature is an image and the features are the bounding box locations for the objects in the image.\nI am not able to dequeue the data.\nHere is how I create the Example:-\ncontext = tf.train.Features(feature={'image_raw':self._bytes_feature(image)})\nfeature_lists = tf.train.FeatureLists(feature_list={'locations':self._float_feature_list(list(locs[:,0]))})\n\t\texample = tf.train.SequenceExample(context=context,feature_lists=feature_lists)\n\nHere is how I am enqueing:-\ndef load_from_tfRecord(self,filename_queue):\n\treader = tf.TFRecordReader()\n\t_, serialized_example = reader.read(filename_queue)\n\n\tcontext,sequence = tf.parse_single_sequence_example(\n\t\t\t\tserialized_example,\n\t\t\t\tcontext_features={'image_raw':tf.FixedLenFeature([], tf.string)},\n\t\t\t\tsequence_features={'locations':tf.FixedLenSequenceFeature([], dtype=tf.float32)}\n\t\t\t\t)\n\t\n\timage = tf.decode_raw(context['image_raw'], tf.uint8)\n\t#locations = tf.decode_raw(features['locations'], tf.int32)\n\n\timage_shape = tf.stack([1,self.ip_width,self.ip_height,3])\n\timage = tf.cast(tf.reshape(image,image_shape),tf.float32)\n\n\t# loc_shape = tf.stack([n_locs,4])\n\t# locations = tf.reshape(locations,loc_shape)\n\n\tlocations = sequence['locations']\n\t\n\n\n\tbatch_images,batch_locations = tf.train.shuffle_batch([image,locations]\n\t\t,enqueue_many=True,batch_size=self.batch_size,num_threads=1,capacity=1000,min_after_dequeue=500)\n\n\treturn batch_images,batch_locations\n\nHere is how I am dequeuing:-\nprint \"Loading Trained Model\"\nself.session.run(tf.local_variables_initializer())\n\t#self.session.run(tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()))\n\n\tcoord = tf.train.Coordinator()\n\tthreads = tf.train.start_queue_runners(sess=self.session,coord=coord)\n\n\ttry:\n\t\twhile not coord.should_stop():\n\t\t\tbatch_imgs,batch_labels = self.session.run([self.images,self.labels])\n\t\t\tprint batch_imgs.shape,batch_labels.shape\n\texcept tf.errors.OutOfRangeError:\n\t\tprint \"Done training -- epoch limit reached\"\n\tfinally:\n\t\tcoord.request_stop()\n\n\tcoord.join(threads)", "body": "@ebrevdo  and @psmit \r\n\r\nI am trying to parse SequenceExamples where my context feature is an image and the features are the bounding box locations for the objects in the image.\r\n\r\nI am not able to dequeue the data.\r\n**Here is how I create the Example:-**\r\n\r\ncontext = tf.train.Features(feature={'image_raw':self._bytes_feature(image)})\r\n\t\t\tfeature_lists = tf.train.FeatureLists(feature_list={'locations':self._float_feature_list(list(locs[:,0]))})\r\n\r\n\t\t\texample = tf.train.SequenceExample(context=context,feature_lists=feature_lists)\r\n\r\n**Here is how I am enqueing:-**\r\n\r\ndef load_from_tfRecord(self,filename_queue):\r\n\t\t\r\n\t\treader = tf.TFRecordReader()\r\n\t\t_, serialized_example = reader.read(filename_queue)\r\n\r\n\t\tcontext,sequence = tf.parse_single_sequence_example(\r\n\t\t\t\t\tserialized_example,\r\n\t\t\t\t\tcontext_features={'image_raw':tf.FixedLenFeature([], tf.string)},\r\n\t\t\t\t\tsequence_features={'locations':tf.FixedLenSequenceFeature([], dtype=tf.float32)}\r\n\t\t\t\t\t)\r\n\t\t\r\n\t\timage = tf.decode_raw(context['image_raw'], tf.uint8)\r\n\t\t#locations = tf.decode_raw(features['locations'], tf.int32)\r\n\r\n\t\timage_shape = tf.stack([1,self.ip_width,self.ip_height,3])\r\n\t\timage = tf.cast(tf.reshape(image,image_shape),tf.float32)\r\n\r\n\t\t# loc_shape = tf.stack([n_locs,4])\r\n\t\t# locations = tf.reshape(locations,loc_shape)\r\n\r\n\t\tlocations = sequence['locations']\r\n\t\t\r\n\r\n\r\n\t\tbatch_images,batch_locations = tf.train.shuffle_batch([image,locations]\r\n\t\t\t,enqueue_many=True,batch_size=self.batch_size,num_threads=1,capacity=1000,min_after_dequeue=500)\r\n\r\n\t\treturn batch_images,batch_locations\r\n\r\n**Here is how I am dequeuing:-**\r\n\r\nprint \"Loading Trained Model\"\r\n\t\tself.session.run(tf.local_variables_initializer())\r\n\r\n\t\t#self.session.run(tf.group(tf.global_variables_initializer(),tf.local_variables_initializer()))\r\n\r\n\t\tcoord = tf.train.Coordinator()\r\n\t\tthreads = tf.train.start_queue_runners(sess=self.session,coord=coord)\r\n\r\n\t\ttry:\r\n\t\t\twhile not coord.should_stop():\r\n\t\t\t\tbatch_imgs,batch_labels = self.session.run([self.images,self.labels])\r\n\t\t\t\tprint batch_imgs.shape,batch_labels.shape\r\n\t\texcept tf.errors.OutOfRangeError:\r\n\t\t\tprint \"Done training -- epoch limit reached\"\r\n\t\tfinally:\r\n\t\t\tcoord.request_stop()\r\n\r\n\t\tcoord.join(threads)\r\n\r\n\r\n\r\n"}