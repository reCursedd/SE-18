{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12940", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12940/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12940/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/12940/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/12940", "id": 256472287, "node_id": "MDU6SXNzdWUyNTY0NzIyODc=", "number": 12940, "title": "tf.nn.separable_conv2d is slower than conv2d on GPU", "user": {"login": "imaxpayne", "id": 22407456, "node_id": "MDQ6VXNlcjIyNDA3NDU2", "avatar_url": "https://avatars1.githubusercontent.com/u/22407456?v=4", "gravatar_id": "", "url": "https://api.github.com/users/imaxpayne", "html_url": "https://github.com/imaxpayne", "followers_url": "https://api.github.com/users/imaxpayne/followers", "following_url": "https://api.github.com/users/imaxpayne/following{/other_user}", "gists_url": "https://api.github.com/users/imaxpayne/gists{/gist_id}", "starred_url": "https://api.github.com/users/imaxpayne/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/imaxpayne/subscriptions", "organizations_url": "https://api.github.com/users/imaxpayne/orgs", "repos_url": "https://api.github.com/users/imaxpayne/repos", "events_url": "https://api.github.com/users/imaxpayne/events{/privacy}", "received_events_url": "https://api.github.com/users/imaxpayne/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 11, "created_at": "2017-09-09T23:13:16Z", "updated_at": "2018-10-16T02:01:55Z", "closed_at": "2018-01-24T14:53:54Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>:Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Ubuntu 16.04</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: binary</li>\n<li><strong>TensorFlow version (use command below)</strong>: TF 1.3</li>\n<li><strong>Python version</strong>: 3.6</li>\n<li><strong>Bazel version (if compiling from source)</strong>:</li>\n<li><strong>CUDA/cuDNN version</strong>: CUDA8.0 /cuDNN6</li>\n<li><strong>GPU model and memory</strong>: GTX1080ti  11G</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>In theory, <code>separable_conv2d</code> should be more efficient than <code>conv2d</code>, but when I test a simple model on Cifar10, the result shows that <code>nn.separable_conv2d</code> run slower on GPU, but is indeed faster on CPU.</p>\n<p>Here is my test results on GPU:</p>\n<pre><code>training time for normal_conv after 2000 step: 8.18395892999979 sec\ntime for normal_conv after one forward step:  0.003980965999289765 sec\ntraining time for separable_conv after 2000 step: 9.158266903999902 sec\ntime for separable_conv after one forward step:  0.0036441169995669043 sec\n\n</code></pre>\n<h3>Source code / logs</h3>\n<p>Below is a fully self-contained example, I first define a model with two  <code>conv2d</code> , than I define another model with one <code>conv2d</code> followed by one  <code>separable_conv2d</code>, both model have 32 channels for each conv_layer and identical fc_layer.</p>\n<pre><code>import tensorflow as tf\nimport timeit\nimport numpy as np\nfrom tensorflow.contrib.keras.python.keras.datasets.cifar10 import load_data\n\n(x_train, y_train), (x_val, y_val) = load_data()\nlearning_rate = 0.001\nnum_steps = 1000\nn_classes = 10\nbatch_size = 32\n\ndef reformat(labels):\n    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n    labels = (np.arange(n_classes) == labels[:,None]).astype(np.float32)\n    return  labels.reshape(labels.shape[0],10)\ntrain_labels = reformat(y_train)\ntf.reset_default_graph()\nx = tf.placeholder(tf.float32, [None, 32, 32, 3])\ny = tf.placeholder(tf.float32, [None, 10])\nweights1 = {}\nweights2 = {}\ndtype = tf.float32\nwith tf.name_scope('INIT_OP'):\n    conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\n    fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)\n\nk = 3\nkernel = 16\n\n# Define weights for normal ConvNet\nwith tf.name_scope('VARIABLES_1'):\n    weights1['conv1'] = tf.get_variable('conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights1['b1'] = tf.get_variable('b1', initializer=tf.zeros([kernel]))\n    weights1['conv2'] = tf.get_variable('conv2', [k, k, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights1['b2'] = tf.get_variable('b2', initializer=tf.zeros([kernel]))\n\n    weights1['wd1'] = tf.get_variable('wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights1['bd1'] = tf.get_variable('bd1',  initializer=tf.zeros([512]) )\n    weights1['wd2'] = tf.get_variable('wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights1['bd2'] = tf.get_variable('bd2',  initializer=tf.zeros([10]) )\n\n\n#Define weights for separable ConvNet\nwith tf.name_scope('VARIABLES_sep'):\n    weights2['conv1'] = tf.get_variable('2_conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights2['conv_dw2'] = tf.get_variable('conv_dw2', [k, k, kernel, 1], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights2['conv_pw2'] = tf.get_variable('conv_pw2', [1, 1, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n\n    weights2['b1'] = tf.get_variable('2_b1', initializer=tf.zeros([kernel]))\n    weights2['b2'] = tf.get_variable('2_b2', initializer=tf.zeros([kernel]))\n\n    weights2['wd1'] = tf.get_variable('2_wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights2['bd1'] = tf.get_variable('2_bd1',  initializer=tf.zeros([512]) )\n    weights2['wd2'] = tf.get_variable('2_wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights2['bd2'] = tf.get_variable('2_bd2',  initializer=tf.zeros([10]) )\n\ndef forward_conv_sep( inp, weights):\n    hidden = conv_block(inp, weights2['conv1'], weights2['b1'])\n    hidden = maxpool2d(hidden)\n    hidden = conv_block_dw(hidden, weights2['conv_dw2'], weights2['conv_pw2'], weights2['b2'])\n    hidden = maxpool2d(hidden)\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\n    fc1 = tf.matmul(hidden, weights2['wd1']) + weights2['bd1']\n    fc1 = tf.nn.relu(fc1)\n    return tf.matmul(fc1, weights2['wd2']) + weights2['bd2']\n\ndef forward_conv( inp, weights):\n    hidden = conv_block(inp, weights1['conv1'], weights1['b1'])\n    hidden = maxpool2d(hidden)\n    hidden = conv_block(hidden, weights1['conv2'], weights1['b2'])\n    hidden = maxpool2d(hidden)\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\n    fc1 = tf.matmul(hidden, weights1['wd1']) + weights1['bd1']\n    fc1 = tf.nn.relu(fc1)\n    return tf.matmul(fc1, weights1['wd2']) + weights1['bd2']\n\n\ndef conv_block_dw(inp, cweight_w, cweight_p, bweight):\n    no_stride =  [1,1,1,1]\n    conv_output = tf.nn.separable_conv2d(inp, cweight_w, cweight_p, no_stride, 'SAME') + bweight\n    return tf.nn.relu(conv_output)\n\ndef conv_block(inp, cweight, bweight, activation=tf.nn.relu):\n    no_stride =  [1,1,1,1]\n    conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight\n    return tf.nn.relu(conv_output)\n\ndef maxpool2d(inp, k=2):\n    return tf.nn.max_pool(inp, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                          padding='SAME')\n\n#logits for normal ConvNet\nwith tf.name_scope(\"forward_conv\"):\n    pred1 = forward_conv(x, weights1)\n\n#Cost for normal ConvNet\nwith tf.name_scope(\"cost1\"):\n    loss1 = tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)\n    cost1 = tf.reduce_mean(loss1)\n\n#training op for normal ConvNet\nwith tf.name_scope('train_op1'):\n    train_op1 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost1)    \n\n#logits for separable ConvNet\nwith tf.name_scope(\"forward_conv_sep\"):\n    pred2 = forward_conv_sep(x, weights2)\n\n#Cost for separable ConvNet\nwith tf.name_scope(\"cost2\"):\n    loss2 = tf.nn.softmax_cross_entropy_with_logits(logits=pred2, labels=y)\n    cost2 = tf.reduce_mean(loss2)\n\n# training op for separable ConvNet\nwith tf.name_scope('train_op2'):\n    train_op2 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost2)\n\n\nwith tf.name_scope('INIT'):\n    init = tf.global_variables_initializer()\n\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n\n    #train normal ConvNet for 2000 steps\n    start = timeit.default_timer()\n    for step in range(num_steps):\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\n        batch_data = x_train[r]\n        batch_labels = train_labels[r]\n\n        feed_dict = {x : batch_data, y: batch_labels}\n        _ , l = sess.run([train_op1,cost1], feed_dict=feed_dict)\n\n    stop = timeit.default_timer()\n    print ('training time for normal_conv after '+str(num_steps)+' step:',stop - start) \n\n\n    start = timeit.default_timer()\n    feed_dict = {x : batch_data, y: batch_labels}\n    predictions1 = sess.run(pred1, feed_dict=feed_dict)\n    stop = timeit.default_timer()\n    print ('time for normal_conv after one forward step: ',stop - start)\n\n\n\n    # train separable ConvNet for 2000 steps\n    start = timeit.default_timer()\n    for step in range(num_steps):\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\n        batch_data = x_train[r]\n        batch_labels = train_labels[r]\n\n        feed_dict = {x : batch_data, y: batch_labels}\n        _ , l = sess.run([train_op2,cost2], feed_dict=feed_dict)\n\n\n    stop = timeit.default_timer()\n    print ('training time for sep_conv after '+str(num_steps)+' step:',stop - start) \n\n    start = timeit.default_timer()\n    feed_dict = {x : batch_data, y: batch_labels}\n    predictions = sess.run(pred2, feed_dict=feed_dict)\n    stop = timeit.default_timer()\n    print ('time for sep_conv after one forward step: ',stop - start)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Ubuntu 16.04\nTensorFlow installed from (source or binary): binary\nTensorFlow version (use command below): TF 1.3\nPython version: 3.6\nBazel version (if compiling from source):\nCUDA/cuDNN version: CUDA8.0 /cuDNN6\nGPU model and memory: GTX1080ti  11G\n\nDescribe the problem\nIn theory, separable_conv2d should be more efficient than conv2d, but when I test a simple model on Cifar10, the result shows that nn.separable_conv2d run slower on GPU, but is indeed faster on CPU.\nHere is my test results on GPU:\ntraining time for normal_conv after 2000 step: 8.18395892999979 sec\ntime for normal_conv after one forward step:  0.003980965999289765 sec\ntraining time for separable_conv after 2000 step: 9.158266903999902 sec\ntime for separable_conv after one forward step:  0.0036441169995669043 sec\n\n\nSource code / logs\nBelow is a fully self-contained example, I first define a model with two  conv2d , than I define another model with one conv2d followed by one  separable_conv2d, both model have 32 channels for each conv_layer and identical fc_layer.\nimport tensorflow as tf\nimport timeit\nimport numpy as np\nfrom tensorflow.contrib.keras.python.keras.datasets.cifar10 import load_data\n\n(x_train, y_train), (x_val, y_val) = load_data()\nlearning_rate = 0.001\nnum_steps = 1000\nn_classes = 10\nbatch_size = 32\n\ndef reformat(labels):\n    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\n    labels = (np.arange(n_classes) == labels[:,None]).astype(np.float32)\n    return  labels.reshape(labels.shape[0],10)\ntrain_labels = reformat(y_train)\ntf.reset_default_graph()\nx = tf.placeholder(tf.float32, [None, 32, 32, 3])\ny = tf.placeholder(tf.float32, [None, 10])\nweights1 = {}\nweights2 = {}\ndtype = tf.float32\nwith tf.name_scope('INIT_OP'):\n    conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\n    fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)\n\nk = 3\nkernel = 16\n\n# Define weights for normal ConvNet\nwith tf.name_scope('VARIABLES_1'):\n    weights1['conv1'] = tf.get_variable('conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights1['b1'] = tf.get_variable('b1', initializer=tf.zeros([kernel]))\n    weights1['conv2'] = tf.get_variable('conv2', [k, k, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights1['b2'] = tf.get_variable('b2', initializer=tf.zeros([kernel]))\n\n    weights1['wd1'] = tf.get_variable('wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights1['bd1'] = tf.get_variable('bd1',  initializer=tf.zeros([512]) )\n    weights1['wd2'] = tf.get_variable('wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights1['bd2'] = tf.get_variable('bd2',  initializer=tf.zeros([10]) )\n\n\n#Define weights for separable ConvNet\nwith tf.name_scope('VARIABLES_sep'):\n    weights2['conv1'] = tf.get_variable('2_conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights2['conv_dw2'] = tf.get_variable('conv_dw2', [k, k, kernel, 1], initializer=conv_initializer, dtype=dtype, trainable=True)\n    weights2['conv_pw2'] = tf.get_variable('conv_pw2', [1, 1, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\n\n    weights2['b1'] = tf.get_variable('2_b1', initializer=tf.zeros([kernel]))\n    weights2['b2'] = tf.get_variable('2_b2', initializer=tf.zeros([kernel]))\n\n    weights2['wd1'] = tf.get_variable('2_wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights2['bd1'] = tf.get_variable('2_bd1',  initializer=tf.zeros([512]) )\n    weights2['wd2'] = tf.get_variable('2_wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\n    weights2['bd2'] = tf.get_variable('2_bd2',  initializer=tf.zeros([10]) )\n\ndef forward_conv_sep( inp, weights):\n    hidden = conv_block(inp, weights2['conv1'], weights2['b1'])\n    hidden = maxpool2d(hidden)\n    hidden = conv_block_dw(hidden, weights2['conv_dw2'], weights2['conv_pw2'], weights2['b2'])\n    hidden = maxpool2d(hidden)\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\n    fc1 = tf.matmul(hidden, weights2['wd1']) + weights2['bd1']\n    fc1 = tf.nn.relu(fc1)\n    return tf.matmul(fc1, weights2['wd2']) + weights2['bd2']\n\ndef forward_conv( inp, weights):\n    hidden = conv_block(inp, weights1['conv1'], weights1['b1'])\n    hidden = maxpool2d(hidden)\n    hidden = conv_block(hidden, weights1['conv2'], weights1['b2'])\n    hidden = maxpool2d(hidden)\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\n    fc1 = tf.matmul(hidden, weights1['wd1']) + weights1['bd1']\n    fc1 = tf.nn.relu(fc1)\n    return tf.matmul(fc1, weights1['wd2']) + weights1['bd2']\n\n\ndef conv_block_dw(inp, cweight_w, cweight_p, bweight):\n    no_stride =  [1,1,1,1]\n    conv_output = tf.nn.separable_conv2d(inp, cweight_w, cweight_p, no_stride, 'SAME') + bweight\n    return tf.nn.relu(conv_output)\n\ndef conv_block(inp, cweight, bweight, activation=tf.nn.relu):\n    no_stride =  [1,1,1,1]\n    conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight\n    return tf.nn.relu(conv_output)\n\ndef maxpool2d(inp, k=2):\n    return tf.nn.max_pool(inp, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n                          padding='SAME')\n\n#logits for normal ConvNet\nwith tf.name_scope(\"forward_conv\"):\n    pred1 = forward_conv(x, weights1)\n\n#Cost for normal ConvNet\nwith tf.name_scope(\"cost1\"):\n    loss1 = tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)\n    cost1 = tf.reduce_mean(loss1)\n\n#training op for normal ConvNet\nwith tf.name_scope('train_op1'):\n    train_op1 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost1)    \n\n#logits for separable ConvNet\nwith tf.name_scope(\"forward_conv_sep\"):\n    pred2 = forward_conv_sep(x, weights2)\n\n#Cost for separable ConvNet\nwith tf.name_scope(\"cost2\"):\n    loss2 = tf.nn.softmax_cross_entropy_with_logits(logits=pred2, labels=y)\n    cost2 = tf.reduce_mean(loss2)\n\n# training op for separable ConvNet\nwith tf.name_scope('train_op2'):\n    train_op2 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost2)\n\n\nwith tf.name_scope('INIT'):\n    init = tf.global_variables_initializer()\n\n\nwith tf.Session() as sess:\n\n    sess.run(init)\n\n    #train normal ConvNet for 2000 steps\n    start = timeit.default_timer()\n    for step in range(num_steps):\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\n        batch_data = x_train[r]\n        batch_labels = train_labels[r]\n\n        feed_dict = {x : batch_data, y: batch_labels}\n        _ , l = sess.run([train_op1,cost1], feed_dict=feed_dict)\n\n    stop = timeit.default_timer()\n    print ('training time for normal_conv after '+str(num_steps)+' step:',stop - start) \n\n\n    start = timeit.default_timer()\n    feed_dict = {x : batch_data, y: batch_labels}\n    predictions1 = sess.run(pred1, feed_dict=feed_dict)\n    stop = timeit.default_timer()\n    print ('time for normal_conv after one forward step: ',stop - start)\n\n\n\n    # train separable ConvNet for 2000 steps\n    start = timeit.default_timer()\n    for step in range(num_steps):\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\n        batch_data = x_train[r]\n        batch_labels = train_labels[r]\n\n        feed_dict = {x : batch_data, y: batch_labels}\n        _ , l = sess.run([train_op2,cost2], feed_dict=feed_dict)\n\n\n    stop = timeit.default_timer()\n    print ('training time for sep_conv after '+str(num_steps)+' step:',stop - start) \n\n    start = timeit.default_timer()\n    feed_dict = {x : batch_data, y: batch_labels}\n    predictions = sess.run(pred2, feed_dict=feed_dict)\n    stop = timeit.default_timer()\n    print ('time for sep_conv after one forward step: ',stop - start)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**:Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Ubuntu 16.04\r\n- **TensorFlow installed from (source or binary)**: binary\r\n- **TensorFlow version (use command below)**: TF 1.3\r\n- **Python version**: 3.6\r\n- **Bazel version (if compiling from source)**:\r\n- **CUDA/cuDNN version**: CUDA8.0 /cuDNN6\r\n- **GPU model and memory**: GTX1080ti  11G\r\n\r\n\r\n### Describe the problem\r\nIn theory, `separable_conv2d` should be more efficient than `conv2d`, but when I test a simple model on Cifar10, the result shows that `nn.separable_conv2d` run slower on GPU, but is indeed faster on CPU. \r\n\r\nHere is my test results on GPU: \r\n```\r\ntraining time for normal_conv after 2000 step: 8.18395892999979 sec\r\ntime for normal_conv after one forward step:  0.003980965999289765 sec\r\ntraining time for separable_conv after 2000 step: 9.158266903999902 sec\r\ntime for separable_conv after one forward step:  0.0036441169995669043 sec\r\n\r\n```\r\n\r\n### Source code / logs\r\n\r\nBelow is a fully self-contained example, I first define a model with two  `conv2d` , than I define another model with one `conv2d` followed by one  `separable_conv2d`, both model have 32 channels for each conv_layer and identical fc_layer.\r\n\r\n\r\n\r\n```\r\nimport tensorflow as tf\r\nimport timeit\r\nimport numpy as np\r\nfrom tensorflow.contrib.keras.python.keras.datasets.cifar10 import load_data\r\n\r\n(x_train, y_train), (x_val, y_val) = load_data()\r\nlearning_rate = 0.001\r\nnum_steps = 1000\r\nn_classes = 10\r\nbatch_size = 32\r\n\r\ndef reformat(labels):\r\n    # Map 0 to [1.0, 0.0, 0.0 ...], 1 to [0.0, 1.0, 0.0 ...]\r\n    labels = (np.arange(n_classes) == labels[:,None]).astype(np.float32)\r\n    return  labels.reshape(labels.shape[0],10)\r\ntrain_labels = reformat(y_train)\r\ntf.reset_default_graph()\r\nx = tf.placeholder(tf.float32, [None, 32, 32, 3])\r\ny = tf.placeholder(tf.float32, [None, 10])\r\nweights1 = {}\r\nweights2 = {}\r\ndtype = tf.float32\r\nwith tf.name_scope('INIT_OP'):\r\n    conv_initializer =  tf.contrib.layers.xavier_initializer_conv2d(dtype=dtype)\r\n    fc_initializer =  tf.contrib.layers.xavier_initializer(dtype=dtype)\r\n\r\nk = 3\r\nkernel = 16\r\n\r\n# Define weights for normal ConvNet\r\nwith tf.name_scope('VARIABLES_1'):\r\n    weights1['conv1'] = tf.get_variable('conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights1['b1'] = tf.get_variable('b1', initializer=tf.zeros([kernel]))\r\n    weights1['conv2'] = tf.get_variable('conv2', [k, k, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights1['b2'] = tf.get_variable('b2', initializer=tf.zeros([kernel]))\r\n\r\n    weights1['wd1'] = tf.get_variable('wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights1['bd1'] = tf.get_variable('bd1',  initializer=tf.zeros([512]) )\r\n    weights1['wd2'] = tf.get_variable('wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights1['bd2'] = tf.get_variable('bd2',  initializer=tf.zeros([10]) )\r\n\r\n\r\n#Define weights for separable ConvNet\r\nwith tf.name_scope('VARIABLES_sep'):\r\n    weights2['conv1'] = tf.get_variable('2_conv1', [k, k, 3, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights2['conv_dw2'] = tf.get_variable('conv_dw2', [k, k, kernel, 1], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n    weights2['conv_pw2'] = tf.get_variable('conv_pw2', [1, 1, kernel, kernel], initializer=conv_initializer, dtype=dtype, trainable=True)\r\n\r\n    weights2['b1'] = tf.get_variable('2_b1', initializer=tf.zeros([kernel]))\r\n    weights2['b2'] = tf.get_variable('2_b2', initializer=tf.zeros([kernel]))\r\n\r\n    weights2['wd1'] = tf.get_variable('2_wd1', [8*8*kernel, 512], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights2['bd1'] = tf.get_variable('2_bd1',  initializer=tf.zeros([512]) )\r\n    weights2['wd2'] = tf.get_variable('2_wd2', [512, 10], initializer=fc_initializer, dtype=dtype, trainable=True)\r\n    weights2['bd2'] = tf.get_variable('2_bd2',  initializer=tf.zeros([10]) )\r\n\r\ndef forward_conv_sep( inp, weights):\r\n    hidden = conv_block(inp, weights2['conv1'], weights2['b1'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = conv_block_dw(hidden, weights2['conv_dw2'], weights2['conv_pw2'], weights2['b2'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\r\n    fc1 = tf.matmul(hidden, weights2['wd1']) + weights2['bd1']\r\n    fc1 = tf.nn.relu(fc1)\r\n    return tf.matmul(fc1, weights2['wd2']) + weights2['bd2']\r\n\r\ndef forward_conv( inp, weights):\r\n    hidden = conv_block(inp, weights1['conv1'], weights1['b1'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = conv_block(hidden, weights1['conv2'], weights1['b2'])\r\n    hidden = maxpool2d(hidden)\r\n    hidden = tf.reshape( hidden, [-1, np.prod([int(dim) for dim in hidden.get_shape()[1:]])] )\r\n    fc1 = tf.matmul(hidden, weights1['wd1']) + weights1['bd1']\r\n    fc1 = tf.nn.relu(fc1)\r\n    return tf.matmul(fc1, weights1['wd2']) + weights1['bd2']\r\n\r\n\r\ndef conv_block_dw(inp, cweight_w, cweight_p, bweight):\r\n    no_stride =  [1,1,1,1]\r\n    conv_output = tf.nn.separable_conv2d(inp, cweight_w, cweight_p, no_stride, 'SAME') + bweight\r\n    return tf.nn.relu(conv_output)\r\n\r\ndef conv_block(inp, cweight, bweight, activation=tf.nn.relu):\r\n    no_stride =  [1,1,1,1]\r\n    conv_output = tf.nn.conv2d(inp, cweight, no_stride, 'SAME') + bweight\r\n    return tf.nn.relu(conv_output)\r\n\r\ndef maxpool2d(inp, k=2):\r\n    return tf.nn.max_pool(inp, ksize=[1, k, k, 1], strides=[1, k, k, 1],\r\n                          padding='SAME')\r\n\r\n#logits for normal ConvNet\r\nwith tf.name_scope(\"forward_conv\"):\r\n    pred1 = forward_conv(x, weights1)\r\n\r\n#Cost for normal ConvNet\r\nwith tf.name_scope(\"cost1\"):\r\n    loss1 = tf.nn.softmax_cross_entropy_with_logits(logits=pred1, labels=y)\r\n    cost1 = tf.reduce_mean(loss1)\r\n\r\n#training op for normal ConvNet\r\nwith tf.name_scope('train_op1'):\r\n    train_op1 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost1)    \r\n\r\n#logits for separable ConvNet\r\nwith tf.name_scope(\"forward_conv_sep\"):\r\n    pred2 = forward_conv_sep(x, weights2)\r\n\r\n#Cost for separable ConvNet\r\nwith tf.name_scope(\"cost2\"):\r\n    loss2 = tf.nn.softmax_cross_entropy_with_logits(logits=pred2, labels=y)\r\n    cost2 = tf.reduce_mean(loss2)\r\n\r\n# training op for separable ConvNet\r\nwith tf.name_scope('train_op2'):\r\n    train_op2 = tf.train.RMSPropOptimizer(learning_rate, 0.9).minimize(cost2)\r\n\r\n\r\nwith tf.name_scope('INIT'):\r\n    init = tf.global_variables_initializer()\r\n\r\n\r\nwith tf.Session() as sess:\r\n\r\n    sess.run(init)\r\n\r\n    #train normal ConvNet for 2000 steps\r\n    start = timeit.default_timer()\r\n    for step in range(num_steps):\r\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\r\n        batch_data = x_train[r]\r\n        batch_labels = train_labels[r]\r\n\r\n        feed_dict = {x : batch_data, y: batch_labels}\r\n        _ , l = sess.run([train_op1,cost1], feed_dict=feed_dict)\r\n\r\n    stop = timeit.default_timer()\r\n    print ('training time for normal_conv after '+str(num_steps)+' step:',stop - start) \r\n\r\n\r\n    start = timeit.default_timer()\r\n    feed_dict = {x : batch_data, y: batch_labels}\r\n    predictions1 = sess.run(pred1, feed_dict=feed_dict)\r\n    stop = timeit.default_timer()\r\n    print ('time for normal_conv after one forward step: ',stop - start)\r\n\r\n\r\n\r\n    # train separable ConvNet for 2000 steps\r\n    start = timeit.default_timer()\r\n    for step in range(num_steps):\r\n        r = np.random.choice(y_train.shape[0], batch_size, replace=False)\r\n        batch_data = x_train[r]\r\n        batch_labels = train_labels[r]\r\n\r\n        feed_dict = {x : batch_data, y: batch_labels}\r\n        _ , l = sess.run([train_op2,cost2], feed_dict=feed_dict)\r\n\r\n\r\n    stop = timeit.default_timer()\r\n    print ('training time for sep_conv after '+str(num_steps)+' step:',stop - start) \r\n\r\n    start = timeit.default_timer()\r\n    feed_dict = {x : batch_data, y: batch_labels}\r\n    predictions = sess.run(pred2, feed_dict=feed_dict)\r\n    stop = timeit.default_timer()\r\n    print ('time for sep_conv after one forward step: ',stop - start)\r\n```"}