{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4185", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4185/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4185/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/4185/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/4185", "id": 174847483, "node_id": "MDExOlB1bGxSZXF1ZXN0ODM4NjQyNjc=", "number": 4185, "title": "Use valid GPU for allocating CUDA host memory", "user": {"login": "gibiansky", "id": 1865411, "node_id": "MDQ6VXNlcjE4NjU0MTE=", "avatar_url": "https://avatars3.githubusercontent.com/u/1865411?v=4", "gravatar_id": "", "url": "https://api.github.com/users/gibiansky", "html_url": "https://github.com/gibiansky", "followers_url": "https://api.github.com/users/gibiansky/followers", "following_url": "https://api.github.com/users/gibiansky/following{/other_user}", "gists_url": "https://api.github.com/users/gibiansky/gists{/gist_id}", "starred_url": "https://api.github.com/users/gibiansky/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/gibiansky/subscriptions", "organizations_url": "https://api.github.com/users/gibiansky/orgs", "repos_url": "https://api.github.com/users/gibiansky/repos", "events_url": "https://api.github.com/users/gibiansky/events{/privacy}", "received_events_url": "https://api.github.com/users/gibiansky/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2016-09-02T21:04:31Z", "updated_at": "2016-09-02T21:39:08Z", "closed_at": "2016-09-02T21:39:08Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/4185", "html_url": "https://github.com/tensorflow/tensorflow/pull/4185", "diff_url": "https://github.com/tensorflow/tensorflow/pull/4185.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/4185.patch"}, "body_html": "<p>This PR fixes (again!) issue <a class=\"issue-link js-issue-link\" data-error-text=\"Failed to load issue title\" data-id=\"147834996\" data-permission-text=\"Issue title is private\" data-url=\"https://github.com/tensorflow/tensorflow/issues/1888\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/1888/hovercard\" href=\"https://github.com/tensorflow/tensorflow/issues/1888\">#1888</a>.</p>\n<p>Before this commit, when allocating CUDA host memory, device 0 would<br>\nalways be used to allocate the memory (because the memory is allocated<br>\non the host with DMA enabled in the same way for all devices; it does<br>\nnot matter which stream executor allocates the host memory). This<br>\nworks, but if device 0 is not a valid device in this session (e.g. it<br>\nis turned off using GPUOptions.visible_device_list), doing so would<br>\nallocate a context on GPU 0, even when it was not being used. This<br>\nchange fixes this, so that a visible device is found by looking at<br>\nwhich GPU allocators exist, and then using that device to allocate host<br>\nmemory.</p>\n<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=463737\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/vrv\">@vrv</a></p>", "body_text": "This PR fixes (again!) issue #1888.\nBefore this commit, when allocating CUDA host memory, device 0 would\nalways be used to allocate the memory (because the memory is allocated\non the host with DMA enabled in the same way for all devices; it does\nnot matter which stream executor allocates the host memory). This\nworks, but if device 0 is not a valid device in this session (e.g. it\nis turned off using GPUOptions.visible_device_list), doing so would\nallocate a context on GPU 0, even when it was not being used. This\nchange fixes this, so that a visible device is found by looking at\nwhich GPU allocators exist, and then using that device to allocate host\nmemory.\n@vrv", "body": "This PR fixes (again!) issue #1888.\n\nBefore this commit, when allocating CUDA host memory, device 0 would\nalways be used to allocate the memory (because the memory is allocated\non the host with DMA enabled in the same way for all devices; it does\nnot matter which stream executor allocates the host memory). This\nworks, but if device 0 is not a valid device in this session (e.g. it\nis turned off using GPUOptions.visible_device_list), doing so would\nallocate a context on GPU 0, even when it was not being used. This\nchange fixes this, so that a visible device is found by looking at\nwhich GPU allocators exist, and then using that device to allocate host\nmemory.\n\n@vrv \n"}