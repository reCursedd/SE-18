{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21663", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21663/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21663/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21663/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21663", "id": 351341419, "node_id": "MDU6SXNzdWUzNTEzNDE0MTk=", "number": 21663, "title": "how to train the network with mini-batches in eager execution?", "user": {"login": "alinamadchian", "id": 42450224, "node_id": "MDQ6VXNlcjQyNDUwMjI0", "avatar_url": "https://avatars1.githubusercontent.com/u/42450224?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alinamadchian", "html_url": "https://github.com/alinamadchian", "followers_url": "https://api.github.com/users/alinamadchian/followers", "following_url": "https://api.github.com/users/alinamadchian/following{/other_user}", "gists_url": "https://api.github.com/users/alinamadchian/gists{/gist_id}", "starred_url": "https://api.github.com/users/alinamadchian/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alinamadchian/subscriptions", "organizations_url": "https://api.github.com/users/alinamadchian/orgs", "repos_url": "https://api.github.com/users/alinamadchian/repos", "events_url": "https://api.github.com/users/alinamadchian/events{/privacy}", "received_events_url": "https://api.github.com/users/alinamadchian/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2018-08-16T19:40:50Z", "updated_at": "2018-08-17T04:45:52Z", "closed_at": "2018-08-17T04:45:51Z", "author_association": "NONE", "body_html": "<h3>Describe the problem</h3>\n<p>I find a simple code from tensorflow documents to train a simple neural network by eager execution.<br>\nI need to know that how should I modify the code to train the network with mini-batches?</p>\n<h3>Source code / logs</h3>\n<p>from <strong>future</strong> import absolute_import, division, print_function<br>\nimport tensorflow as tf<br>\nimport numpy as np<br>\ntfe = tf.contrib.eager<br>\nNUM_EXAMPLES = 1000<br>\ntraining_inputs = tf.random_normal([NUM_EXAMPLES])<br>\nnoise = tf.random_normal([NUM_EXAMPLES])<br>\ntraining_outputs = training_inputs * 3 + 2</p>\n<p>def prediction(input, weight, bias):<br>\nreturn input * weight + bias</p>\n<p>def loss(weights, biases):<br>\nerror = prediction(training_inputs, weights, biases) - training_outputs<br>\nreturn tf.reduce_sum(tf.square(error))</p>\n<p>def grad(weights, biases):<br>\nwith tf.GradientTape() as tape:<br>\nloss_value = loss(weights, biases)<br>\nreturn tape.gradient(loss_value, [weights, biases])</p>\n<p>train_steps = 20000<br>\nlearning_rate = 0.01<br>\nW = tfe.Variable(5.)<br>\nB = tfe.Variable(10.)<br>\noptimizer=tf.train.AdamOptimizer(0.1)</p>\n<p>for i in range(train_steps):<br>\ngrads=grad(W,B)<br>\noptimizer.apply_gradients(zip(grads,[W,B]),global_step=tf.train.get_or_create_global_step())<br>\nif i % 20 == 0:<br>\nprint(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))</p>", "body_text": "Describe the problem\nI find a simple code from tensorflow documents to train a simple neural network by eager execution.\nI need to know that how should I modify the code to train the network with mini-batches?\nSource code / logs\nfrom future import absolute_import, division, print_function\nimport tensorflow as tf\nimport numpy as np\ntfe = tf.contrib.eager\nNUM_EXAMPLES = 1000\ntraining_inputs = tf.random_normal([NUM_EXAMPLES])\nnoise = tf.random_normal([NUM_EXAMPLES])\ntraining_outputs = training_inputs * 3 + 2\ndef prediction(input, weight, bias):\nreturn input * weight + bias\ndef loss(weights, biases):\nerror = prediction(training_inputs, weights, biases) - training_outputs\nreturn tf.reduce_sum(tf.square(error))\ndef grad(weights, biases):\nwith tf.GradientTape() as tape:\nloss_value = loss(weights, biases)\nreturn tape.gradient(loss_value, [weights, biases])\ntrain_steps = 20000\nlearning_rate = 0.01\nW = tfe.Variable(5.)\nB = tfe.Variable(10.)\noptimizer=tf.train.AdamOptimizer(0.1)\nfor i in range(train_steps):\ngrads=grad(W,B)\noptimizer.apply_gradients(zip(grads,[W,B]),global_step=tf.train.get_or_create_global_step())\nif i % 20 == 0:\nprint(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))", "body": "\r\n### Describe the problem\r\nI find a simple code from tensorflow documents to train a simple neural network by eager execution.\r\nI need to know that how should I modify the code to train the network with mini-batches?\r\n### Source code / logs\r\nfrom __future__ import absolute_import, division, print_function\r\nimport tensorflow as tf\r\nimport numpy as np\r\ntfe = tf.contrib.eager\r\nNUM_EXAMPLES = 1000\r\ntraining_inputs = tf.random_normal([NUM_EXAMPLES])\r\nnoise = tf.random_normal([NUM_EXAMPLES])\r\ntraining_outputs = training_inputs * 3 + 2 \r\n\r\ndef prediction(input, weight, bias):\r\n  return input * weight + bias\r\n\r\ndef loss(weights, biases):\r\n  error = prediction(training_inputs, weights, biases) - training_outputs\r\n  return tf.reduce_sum(tf.square(error))\r\n\r\ndef grad(weights, biases):\r\n  with tf.GradientTape() as tape:\r\n    loss_value = loss(weights, biases)\r\n  return tape.gradient(loss_value, [weights, biases])\r\n\r\ntrain_steps = 20000\r\nlearning_rate = 0.01\r\nW = tfe.Variable(5.)\r\nB = tfe.Variable(10.)\r\noptimizer=tf.train.AdamOptimizer(0.1)\r\n\r\nfor i in range(train_steps):\r\n    grads=grad(W,B)\r\n    optimizer.apply_gradients(zip(grads,[W,B]),global_step=tf.train.get_or_create_global_step())\r\n    if i % 20 == 0:\r\n     print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(W, B)))\r\n"}