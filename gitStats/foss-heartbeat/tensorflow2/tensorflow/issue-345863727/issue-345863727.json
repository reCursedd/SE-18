{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21248", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21248/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21248/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21248/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21248", "id": 345863727, "node_id": "MDU6SXNzdWUzNDU4NjM3Mjc=", "number": 21248, "title": "[Bug] TensorRT conversion error with conv2d_transpose", "user": {"login": "mike199515", "id": 6950999, "node_id": "MDQ6VXNlcjY5NTA5OTk=", "avatar_url": "https://avatars3.githubusercontent.com/u/6950999?v=4", "gravatar_id": "", "url": "https://api.github.com/users/mike199515", "html_url": "https://github.com/mike199515", "followers_url": "https://api.github.com/users/mike199515/followers", "following_url": "https://api.github.com/users/mike199515/following{/other_user}", "gists_url": "https://api.github.com/users/mike199515/gists{/gist_id}", "starred_url": "https://api.github.com/users/mike199515/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/mike199515/subscriptions", "organizations_url": "https://api.github.com/users/mike199515/orgs", "repos_url": "https://api.github.com/users/mike199515/repos", "events_url": "https://api.github.com/users/mike199515/events{/privacy}", "received_events_url": "https://api.github.com/users/mike199515/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": {"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "aaroey", "id": 31743510, "node_id": "MDQ6VXNlcjMxNzQzNTEw", "avatar_url": "https://avatars0.githubusercontent.com/u/31743510?v=4", "gravatar_id": "", "url": "https://api.github.com/users/aaroey", "html_url": "https://github.com/aaroey", "followers_url": "https://api.github.com/users/aaroey/followers", "following_url": "https://api.github.com/users/aaroey/following{/other_user}", "gists_url": "https://api.github.com/users/aaroey/gists{/gist_id}", "starred_url": "https://api.github.com/users/aaroey/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/aaroey/subscriptions", "organizations_url": "https://api.github.com/users/aaroey/orgs", "repos_url": "https://api.github.com/users/aaroey/repos", "events_url": "https://api.github.com/users/aaroey/events{/privacy}", "received_events_url": "https://api.github.com/users/aaroey/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 10, "created_at": "2018-07-30T17:49:59Z", "updated_at": "2018-09-25T21:04:36Z", "closed_at": "2018-09-21T18:16:11Z", "author_association": "NONE", "body_html": "<h3>System information</h3>\n<ul>\n<li><strong>Have I written custom code (as opposed to using a stock example script provided in TensorFlow)</strong>: Yes</li>\n<li><strong>OS Platform and Distribution (e.g., Linux Ubuntu 16.04)</strong>: Linux Ubuntu 16.04</li>\n<li><strong>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device</strong>: No</li>\n<li><strong>TensorFlow installed from (source or binary)</strong>: Source</li>\n<li><strong>TensorFlow version (use command below)</strong>: 1.9.0rc0</li>\n<li><strong>Python version</strong>: 2.7</li>\n<li><strong>Bazel version (if compiling from source)</strong>: 0.14.1</li>\n<li><strong>GCC/Compiler version (if compiling from source)</strong>: 5.4.0</li>\n<li><strong>CUDA/cuDNN version</strong>: 9.0.176/7.0.5</li>\n<li><strong>GPU model and memory</strong>: GTX 1080 with 8GiB memory</li>\n<li><strong>Exact command to reproduce</strong>: python ./minimal_graph.py</li>\n</ul>\n<h3>Describe the problem</h3>\n<p>When I use Tensorflow's TensorRT integration from tensorflow.contrib to transform my model into tensorRT compatible graph, I notice that if I have conv2d_transpose layer (which is not supported for conversion yet) following some convolution layer (or other layer that can be optimized), the latter would be transformed into a tensorRT operation during graph optimization and eventually cause Cudnn to Segfault.</p>\n<p>Below is a minimal example I figured out to trigger this bug, which describe the process to transform a simple randomly initialized network into tensorRT inference graph, and then execute it with some dummy input.</p>\n<p>The Tensorflow version I use comes from dockerhub's nightly build (tensorflow/tensorflow:nightly-devel-gpu) with build configuration changed to enable tensorRT. I also notice that previous tensorflow version (like 1.8.0) has the same issue. On 1.9.0 there's a bug that prevents tensorRT to run correctly so I use 1.9.0rc0 instead.</p>\n<p>The tensorRT version I use is 3.0.4. I notice another issue from <a href=\"https://github.com/tensorflow/tensorflow/issues/20157\" data-hovercard-type=\"issue\" data-hovercard-url=\"/tensorflow/tensorflow/issues/20157/hovercard\">this link</a> claiming conv2d_transpose is not supported, but here the problem is that the model won't correctly executed instead of not transformed.</p>\n<h3>Source code / logs</h3>\n<h4>Code</h4>\n<p>minimal_graph.py</p>\n<pre><code>import numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import tensorrt as trt\n\n\ndef build_graph_from_def(graph_def, input_nodes, output_nodes):\n    \"\"\"\n    build the actual graph from definition\n    \"\"\"\n    tf.reset_default_graph()\n    graph = tf.Graph()\n    with graph.as_default():\n        return_tensors = [operation_name + \":0\" for operation_name in input_nodes + output_nodes]\n        tensors = tf.import_graph_def(graph_def=graph_def, name=\"\",\n                                      return_elements=return_tensors)\n        input_tensor_list = tensors[:len(input_nodes)]\n        output_tensor_list = tensors[len(input_nodes):]\n\n    return graph, input_tensor_list, output_tensor_list\n\n\ndef main():\n    with tf.variable_scope(\"Net\"):\n        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=\"input_image\")\n        deconv1 = tf.layers.conv2d_transpose(inp, filters=8, kernel_size=(3, 3), strides=(2, 2))\n        output = tf.layers.conv2d(deconv1, filters=8, kernel_size=(3, 3), name=\"output\")\n    input_nodes = [\"Net/input_image\"]\n    output_nodes = [\"Net/output/BiasAdd\"]\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        const_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess, sess.graph.as_graph_def(), output_nodes)\n\n    optimized_graph_def = trt.create_inference_graph(\n        input_graph_def=const_graph_def,\n        outputs=output_nodes,\n        max_batch_size=1,\n        max_workspace_size_bytes=1 &lt;&lt; 25)\n    graph, input_tensors, output_tensors = build_graph_from_def(\n        optimized_graph_def, input_nodes, output_nodes)\n    with tf.Session(graph=graph) as sess:\n        sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 28, 28, 3))})\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>\n<h4>Log</h4>\n<pre lang=\"$\" data-meta=\"python ./minimal_graph.py\"><code>2018-07-30 10:30:11.355872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-07-30 10:30:11.356407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 6.07GiB\n2018-07-30 10:30:11.356418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\n2018-07-30 10:30:11.604399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-30 10:30:11.604422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \n2018-07-30 10:30:11.604427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \n2018-07-30 10:30:11.604573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-07-30 10:30:11.604697: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.\n2018-07-30 10:30:11.669166: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count &gt;= 8): 1\n2018-07-30 10:30:11.671202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'Net/', converted to graph\n2018-07-30 10:30:11.674008: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n2018-07-30 10:30:12.200384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\n2018-07-30 10:30:12.200405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-30 10:30:12.200411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \n2018-07-30 10:30:12.200414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \n2018-07-30 10:30:12.200490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-07-30 10:30:12.212534: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: cudnnEngine.cpp::enqueue::140, condition: bindings[x] != nullptr\n2018-07-30 10:30:12.212549: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:401] Failed to enqueue batch for TRT engine: Net/my_trt_op_0\n[1]    18050 segmentation fault (core dumped)\n</code></pre>", "body_text": "System information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow): Yes\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04): Linux Ubuntu 16.04\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device: No\nTensorFlow installed from (source or binary): Source\nTensorFlow version (use command below): 1.9.0rc0\nPython version: 2.7\nBazel version (if compiling from source): 0.14.1\nGCC/Compiler version (if compiling from source): 5.4.0\nCUDA/cuDNN version: 9.0.176/7.0.5\nGPU model and memory: GTX 1080 with 8GiB memory\nExact command to reproduce: python ./minimal_graph.py\n\nDescribe the problem\nWhen I use Tensorflow's TensorRT integration from tensorflow.contrib to transform my model into tensorRT compatible graph, I notice that if I have conv2d_transpose layer (which is not supported for conversion yet) following some convolution layer (or other layer that can be optimized), the latter would be transformed into a tensorRT operation during graph optimization and eventually cause Cudnn to Segfault.\nBelow is a minimal example I figured out to trigger this bug, which describe the process to transform a simple randomly initialized network into tensorRT inference graph, and then execute it with some dummy input.\nThe Tensorflow version I use comes from dockerhub's nightly build (tensorflow/tensorflow:nightly-devel-gpu) with build configuration changed to enable tensorRT. I also notice that previous tensorflow version (like 1.8.0) has the same issue. On 1.9.0 there's a bug that prevents tensorRT to run correctly so I use 1.9.0rc0 instead.\nThe tensorRT version I use is 3.0.4. I notice another issue from this link claiming conv2d_transpose is not supported, but here the problem is that the model won't correctly executed instead of not transformed.\nSource code / logs\nCode\nminimal_graph.py\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.contrib import tensorrt as trt\n\n\ndef build_graph_from_def(graph_def, input_nodes, output_nodes):\n    \"\"\"\n    build the actual graph from definition\n    \"\"\"\n    tf.reset_default_graph()\n    graph = tf.Graph()\n    with graph.as_default():\n        return_tensors = [operation_name + \":0\" for operation_name in input_nodes + output_nodes]\n        tensors = tf.import_graph_def(graph_def=graph_def, name=\"\",\n                                      return_elements=return_tensors)\n        input_tensor_list = tensors[:len(input_nodes)]\n        output_tensor_list = tensors[len(input_nodes):]\n\n    return graph, input_tensor_list, output_tensor_list\n\n\ndef main():\n    with tf.variable_scope(\"Net\"):\n        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=\"input_image\")\n        deconv1 = tf.layers.conv2d_transpose(inp, filters=8, kernel_size=(3, 3), strides=(2, 2))\n        output = tf.layers.conv2d(deconv1, filters=8, kernel_size=(3, 3), name=\"output\")\n    input_nodes = [\"Net/input_image\"]\n    output_nodes = [\"Net/output/BiasAdd\"]\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        const_graph_def = tf.graph_util.convert_variables_to_constants(\n            sess, sess.graph.as_graph_def(), output_nodes)\n\n    optimized_graph_def = trt.create_inference_graph(\n        input_graph_def=const_graph_def,\n        outputs=output_nodes,\n        max_batch_size=1,\n        max_workspace_size_bytes=1 << 25)\n    graph, input_tensors, output_tensors = build_graph_from_def(\n        optimized_graph_def, input_nodes, output_nodes)\n    with tf.Session(graph=graph) as sess:\n        sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 28, 28, 3))})\n\nif __name__ == \"__main__\":\n    main()\n\nLog\n2018-07-30 10:30:11.355872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2018-07-30 10:30:11.356407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: \nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\npciBusID: 0000:01:00.0\ntotalMemory: 7.92GiB freeMemory: 6.07GiB\n2018-07-30 10:30:11.356418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\n2018-07-30 10:30:11.604399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-30 10:30:11.604422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \n2018-07-30 10:30:11.604427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \n2018-07-30 10:30:11.604573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-07-30 10:30:11.604697: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.\n2018-07-30 10:30:11.669166: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\n2018-07-30 10:30:11.671202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'Net/', converted to graph\n2018-07-30 10:30:11.674008: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\n2018-07-30 10:30:12.200384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\n2018-07-30 10:30:12.200405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\n2018-07-30 10:30:12.200411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \n2018-07-30 10:30:12.200414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \n2018-07-30 10:30:12.200490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\n2018-07-30 10:30:12.212534: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: cudnnEngine.cpp::enqueue::140, condition: bindings[x] != nullptr\n2018-07-30 10:30:12.212549: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:401] Failed to enqueue batch for TRT engine: Net/my_trt_op_0\n[1]    18050 segmentation fault (core dumped)", "body": "### System information\r\n- **Have I written custom code (as opposed to using a stock example script provided in TensorFlow)**: Yes\r\n- **OS Platform and Distribution (e.g., Linux Ubuntu 16.04)**: Linux Ubuntu 16.04\r\n- **Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device**: No\r\n- **TensorFlow installed from (source or binary)**: Source\r\n- **TensorFlow version (use command below)**: 1.9.0rc0\r\n- **Python version**: 2.7\r\n- **Bazel version (if compiling from source)**: 0.14.1\r\n- **GCC/Compiler version (if compiling from source)**: 5.4.0\r\n- **CUDA/cuDNN version**: 9.0.176/7.0.5\r\n- **GPU model and memory**: GTX 1080 with 8GiB memory\r\n- **Exact command to reproduce**: python ./minimal_graph.py\r\n\r\n### Describe the problem\r\nWhen I use Tensorflow's TensorRT integration from tensorflow.contrib to transform my model into tensorRT compatible graph, I notice that if I have conv2d_transpose layer (which is not supported for conversion yet) following some convolution layer (or other layer that can be optimized), the latter would be transformed into a tensorRT operation during graph optimization and eventually cause Cudnn to Segfault. \r\n\r\nBelow is a minimal example I figured out to trigger this bug, which describe the process to transform a simple randomly initialized network into tensorRT inference graph, and then execute it with some dummy input.\r\n\r\nThe Tensorflow version I use comes from dockerhub's nightly build (tensorflow/tensorflow:nightly-devel-gpu) with build configuration changed to enable tensorRT. I also notice that previous tensorflow version (like 1.8.0) has the same issue. On 1.9.0 there's a bug that prevents tensorRT to run correctly so I use 1.9.0rc0 instead.\r\n\r\nThe tensorRT version I use is 3.0.4. I notice another issue from [this link](https://github.com/tensorflow/tensorflow/issues/20157) claiming conv2d_transpose is not supported, but here the problem is that the model won't correctly executed instead of not transformed.\r\n\r\n\r\n### Source code / logs\r\n\r\n#### Code\r\nminimal_graph.py\r\n```\r\nimport numpy as np\r\nimport tensorflow as tf\r\nfrom tensorflow.contrib import tensorrt as trt\r\n\r\n\r\ndef build_graph_from_def(graph_def, input_nodes, output_nodes):\r\n    \"\"\"\r\n    build the actual graph from definition\r\n    \"\"\"\r\n    tf.reset_default_graph()\r\n    graph = tf.Graph()\r\n    with graph.as_default():\r\n        return_tensors = [operation_name + \":0\" for operation_name in input_nodes + output_nodes]\r\n        tensors = tf.import_graph_def(graph_def=graph_def, name=\"\",\r\n                                      return_elements=return_tensors)\r\n        input_tensor_list = tensors[:len(input_nodes)]\r\n        output_tensor_list = tensors[len(input_nodes):]\r\n\r\n    return graph, input_tensor_list, output_tensor_list\r\n\r\n\r\ndef main():\r\n    with tf.variable_scope(\"Net\"):\r\n        inp = tf.placeholder(tf.float32, shape=(1, 28, 28, 3), name=\"input_image\")\r\n        deconv1 = tf.layers.conv2d_transpose(inp, filters=8, kernel_size=(3, 3), strides=(2, 2))\r\n        output = tf.layers.conv2d(deconv1, filters=8, kernel_size=(3, 3), name=\"output\")\r\n    input_nodes = [\"Net/input_image\"]\r\n    output_nodes = [\"Net/output/BiasAdd\"]\r\n    with tf.Session() as sess:\r\n        sess.run(tf.global_variables_initializer())\r\n        const_graph_def = tf.graph_util.convert_variables_to_constants(\r\n            sess, sess.graph.as_graph_def(), output_nodes)\r\n\r\n    optimized_graph_def = trt.create_inference_graph(\r\n        input_graph_def=const_graph_def,\r\n        outputs=output_nodes,\r\n        max_batch_size=1,\r\n        max_workspace_size_bytes=1 << 25)\r\n    graph, input_tensors, output_tensors = build_graph_from_def(\r\n        optimized_graph_def, input_nodes, output_nodes)\r\n    with tf.Session(graph=graph) as sess:\r\n        sess.run(output_tensors[0], feed_dict={input_tensors[0]: np.zeros((1, 28, 28, 3))})\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n#### Log\r\n```$ python ./minimal_graph.py\r\n2018-07-30 10:30:11.355872: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:897] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\r\n2018-07-30 10:30:11.356407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1404] Found device 0 with properties: \r\nname: GeForce GTX 1080 major: 6 minor: 1 memoryClockRate(GHz): 1.7335\r\npciBusID: 0000:01:00.0\r\ntotalMemory: 7.92GiB freeMemory: 6.07GiB\r\n2018-07-30 10:30:11.356418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\r\n2018-07-30 10:30:11.604399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 10:30:11.604422: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \r\n2018-07-30 10:30:11.604427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \r\n2018-07-30 10:30:11.604573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-07-30 10:30:11.604697: E tensorflow/core/common_runtime/gpu/gpu_device.cc:228] Illegal GPUOptions.experimental.num_dev_to_dev_copy_streams=0 set to 1 instead.\r\n2018-07-30 10:30:11.669166: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count >= 8): 1\r\n2018-07-30 10:30:11.671202: I tensorflow/contrib/tensorrt/convert/convert_nodes.cc:2853] Segment @scope 'Net/', converted to graph\r\n2018-07-30 10:30:11.674008: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:724] Can't determine the device, constructing an allocator at device 0\r\n2018-07-30 10:30:12.200384: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1483] Adding visible gpu devices: 0\r\n2018-07-30 10:30:12.200405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:964] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n2018-07-30 10:30:12.200411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:970]      0 \r\n2018-07-30 10:30:12.200414: I tensorflow/core/common_runtime/gpu/gpu_device.cc:983] 0:   N \r\n2018-07-30 10:30:12.200490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1096] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5853 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1)\r\n2018-07-30 10:30:12.212534: E tensorflow/contrib/tensorrt/log/trt_logger.cc:38] DefaultLogger Parameter check failed at: cudnnEngine.cpp::enqueue::140, condition: bindings[x] != nullptr\r\n2018-07-30 10:30:12.212549: E tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:401] Failed to enqueue batch for TRT engine: Net/my_trt_op_0\r\n[1]    18050 segmentation fault (core dumped)\r\n```\r\n"}