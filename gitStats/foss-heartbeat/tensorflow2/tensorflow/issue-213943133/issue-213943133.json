{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8375", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8375/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8375/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8375/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/8375", "id": 213943133, "node_id": "MDU6SXNzdWUyMTM5NDMxMzM=", "number": 8375, "title": "Streaming accuracy and recall aren't working as expected", "user": {"login": "ihsangunay", "id": 8973723, "node_id": "MDQ6VXNlcjg5NzM3MjM=", "avatar_url": "https://avatars3.githubusercontent.com/u/8973723?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ihsangunay", "html_url": "https://github.com/ihsangunay", "followers_url": "https://api.github.com/users/ihsangunay/followers", "following_url": "https://api.github.com/users/ihsangunay/following{/other_user}", "gists_url": "https://api.github.com/users/ihsangunay/gists{/gist_id}", "starred_url": "https://api.github.com/users/ihsangunay/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ihsangunay/subscriptions", "organizations_url": "https://api.github.com/users/ihsangunay/orgs", "repos_url": "https://api.github.com/users/ihsangunay/repos", "events_url": "https://api.github.com/users/ihsangunay/events{/privacy}", "received_events_url": "https://api.github.com/users/ihsangunay/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 473184161, "node_id": "MDU6TGFiZWw0NzMxODQxNjE=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:support", "name": "type:support", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 5, "created_at": "2017-03-14T01:23:27Z", "updated_at": "2018-03-06T13:04:00Z", "closed_at": "2017-03-16T00:00:46Z", "author_association": "NONE", "body_html": "<h3>Environment info</h3>\n<p>Ubuntu 16.04<br>\nCuda 8.0<br>\nCudnn 5.1</p>\n<ol>\n<li>The commit hash (<code>git rev-parse HEAD</code>)<br>\n<a class=\"commit-link\" data-hovercard-type=\"commit\" data-hovercard-url=\"https://github.com/tensorflow/tensorflow/commit/c56c873fbaf976d26d487ad57c8efbc87f05331c/hovercard\" href=\"https://github.com/tensorflow/tensorflow/commit/c56c873fbaf976d26d487ad57c8efbc87f05331c\"><tt>c56c873</tt></a></li>\n<li>The output of <code>bazel version</code><br>\nBuild label: 0.4.4<br>\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar<br>\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)<br>\nBuild timestamp: 1485975261<br>\nBuild timestamp as int: 1485975261</li>\n</ol>\n<p>Here is my code:</p>\n<pre><code>weights = {'first': tf.Variable(tf.random_normal([1, 3, 1, 10])),\n           'iterated': tf.Variable(tf.random_normal([1, 3, 10, 10])),\n           'out': tf.Variable(tf.random_normal([embedding_dim*10, n_classes]))}\n\nbiases = {'first': tf.Variable(tf.random_normal([10])),\n          'iterated': tf.Variable(tf.random_normal([10])),\n          'out': tf.Variable(tf.random_normal([n_classes]))}\n\npreds, cost = model(x, y, weights, biases, dropout, depth_tensor)\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\naccuracy, update_accuracy = streaming_accuracy(y, preds)\nrecall, update_recall = streaming_recall(y, preds)\n\ninit = tf.global_variables_initializer()\ninit2 = tf.local_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    i = 1\n    \n    for batch_x, batch_y in data_processor(data, train_inds, embedding, label_processor, n_iter):\n        sess.run(optimizer, \n                 feed_dict={x: batch_x, y: batch_y, \n                            dropout: dropout_prob})\n        \n        if i % display_step == 0:\n            loss = sess.run(cost, \n                            feed_dict={x: batch_x, y: batch_y, dropout: dropout_prob})\n            \n            print(\"Iter:{}, Minibatch Loss:{:.6f}\".format(i,loss))\n        i += 1\n    \n    sess.run(init2)\n    for batch_x, batch_y in data_processor(data, val_inds, embedding, label_processor, n_iter):\n        recall, accuracy = sess.run([update_recall, update_accuracy], \n                                    feed_dict={x:batch_x, y: batch_y, dropout: 1})\n        \n        f1 = 2 * recall * accuracy / (recall + accuracy)\n    \n    print(\"Testing Accuracy:\", accuracy,\"Testing Recall:\", recall, \"Testing F1 Score:\", f1) \n</code></pre>\n<p>Output:</p>\n<pre><code>Iter:100, Minibatch Loss:18038.144531\nIter:200, Minibatch Loss:11628.046875\nIter:300, Minibatch Loss:9288.974609\nIter:400, Minibatch Loss:4583.474121\nIter:500, Minibatch Loss:6600.524902\n...\nIter:11700, Minibatch Loss:4.203137\nIter:11800, Minibatch Loss:3.623320\nIter:11900, Minibatch Loss:4.883300\nIter:12000, Minibatch Loss:3.045975\nTesting Accuracy: 0.0 Testing Recall: 0.00211863 Testing F1 Score: 0.0\n</code></pre>", "body_text": "Environment info\nUbuntu 16.04\nCuda 8.0\nCudnn 5.1\n\nThe commit hash (git rev-parse HEAD)\nc56c873\nThe output of bazel version\nBuild label: 0.4.4\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\nBuild timestamp: 1485975261\nBuild timestamp as int: 1485975261\n\nHere is my code:\nweights = {'first': tf.Variable(tf.random_normal([1, 3, 1, 10])),\n           'iterated': tf.Variable(tf.random_normal([1, 3, 10, 10])),\n           'out': tf.Variable(tf.random_normal([embedding_dim*10, n_classes]))}\n\nbiases = {'first': tf.Variable(tf.random_normal([10])),\n          'iterated': tf.Variable(tf.random_normal([10])),\n          'out': tf.Variable(tf.random_normal([n_classes]))}\n\npreds, cost = model(x, y, weights, biases, dropout, depth_tensor)\n\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n\naccuracy, update_accuracy = streaming_accuracy(y, preds)\nrecall, update_recall = streaming_recall(y, preds)\n\ninit = tf.global_variables_initializer()\ninit2 = tf.local_variables_initializer()\n\nwith tf.Session() as sess:\n    sess.run(init)\n    i = 1\n    \n    for batch_x, batch_y in data_processor(data, train_inds, embedding, label_processor, n_iter):\n        sess.run(optimizer, \n                 feed_dict={x: batch_x, y: batch_y, \n                            dropout: dropout_prob})\n        \n        if i % display_step == 0:\n            loss = sess.run(cost, \n                            feed_dict={x: batch_x, y: batch_y, dropout: dropout_prob})\n            \n            print(\"Iter:{}, Minibatch Loss:{:.6f}\".format(i,loss))\n        i += 1\n    \n    sess.run(init2)\n    for batch_x, batch_y in data_processor(data, val_inds, embedding, label_processor, n_iter):\n        recall, accuracy = sess.run([update_recall, update_accuracy], \n                                    feed_dict={x:batch_x, y: batch_y, dropout: 1})\n        \n        f1 = 2 * recall * accuracy / (recall + accuracy)\n    \n    print(\"Testing Accuracy:\", accuracy,\"Testing Recall:\", recall, \"Testing F1 Score:\", f1) \n\nOutput:\nIter:100, Minibatch Loss:18038.144531\nIter:200, Minibatch Loss:11628.046875\nIter:300, Minibatch Loss:9288.974609\nIter:400, Minibatch Loss:4583.474121\nIter:500, Minibatch Loss:6600.524902\n...\nIter:11700, Minibatch Loss:4.203137\nIter:11800, Minibatch Loss:3.623320\nIter:11900, Minibatch Loss:4.883300\nIter:12000, Minibatch Loss:3.045975\nTesting Accuracy: 0.0 Testing Recall: 0.00211863 Testing F1 Score: 0.0", "body": "### Environment info\r\nUbuntu 16.04\r\nCuda 8.0\r\nCudnn 5.1\r\n\r\n1. The commit hash (`git rev-parse HEAD`)\r\nc56c873fbaf976d26d487ad57c8efbc87f05331c\r\n2. The output of `bazel version`\r\nBuild label: 0.4.4\r\nBuild target: bazel-out/local-fastbuild/bin/src/main/java/com/google/devtools/build/lib/bazel/BazelServer_deploy.jar\r\nBuild time: Wed Feb 1 18:54:21 2017 (1485975261)\r\nBuild timestamp: 1485975261\r\nBuild timestamp as int: 1485975261\r\n\r\nHere is my code:\r\n```\r\nweights = {'first': tf.Variable(tf.random_normal([1, 3, 1, 10])),\r\n           'iterated': tf.Variable(tf.random_normal([1, 3, 10, 10])),\r\n           'out': tf.Variable(tf.random_normal([embedding_dim*10, n_classes]))}\r\n\r\nbiases = {'first': tf.Variable(tf.random_normal([10])),\r\n          'iterated': tf.Variable(tf.random_normal([10])),\r\n          'out': tf.Variable(tf.random_normal([n_classes]))}\r\n\r\npreds, cost = model(x, y, weights, biases, dropout, depth_tensor)\r\n\r\noptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\r\n\r\naccuracy, update_accuracy = streaming_accuracy(y, preds)\r\nrecall, update_recall = streaming_recall(y, preds)\r\n\r\ninit = tf.global_variables_initializer()\r\ninit2 = tf.local_variables_initializer()\r\n\r\nwith tf.Session() as sess:\r\n    sess.run(init)\r\n    i = 1\r\n    \r\n    for batch_x, batch_y in data_processor(data, train_inds, embedding, label_processor, n_iter):\r\n        sess.run(optimizer, \r\n                 feed_dict={x: batch_x, y: batch_y, \r\n                            dropout: dropout_prob})\r\n        \r\n        if i % display_step == 0:\r\n            loss = sess.run(cost, \r\n                            feed_dict={x: batch_x, y: batch_y, dropout: dropout_prob})\r\n            \r\n            print(\"Iter:{}, Minibatch Loss:{:.6f}\".format(i,loss))\r\n        i += 1\r\n    \r\n    sess.run(init2)\r\n    for batch_x, batch_y in data_processor(data, val_inds, embedding, label_processor, n_iter):\r\n        recall, accuracy = sess.run([update_recall, update_accuracy], \r\n                                    feed_dict={x:batch_x, y: batch_y, dropout: 1})\r\n        \r\n        f1 = 2 * recall * accuracy / (recall + accuracy)\r\n    \r\n    print(\"Testing Accuracy:\", accuracy,\"Testing Recall:\", recall, \"Testing F1 Score:\", f1) \r\n```\r\n\r\nOutput:\r\n```\r\nIter:100, Minibatch Loss:18038.144531\r\nIter:200, Minibatch Loss:11628.046875\r\nIter:300, Minibatch Loss:9288.974609\r\nIter:400, Minibatch Loss:4583.474121\r\nIter:500, Minibatch Loss:6600.524902\r\n...\r\nIter:11700, Minibatch Loss:4.203137\r\nIter:11800, Minibatch Loss:3.623320\r\nIter:11900, Minibatch Loss:4.883300\r\nIter:12000, Minibatch Loss:3.045975\r\nTesting Accuracy: 0.0 Testing Recall: 0.00211863 Testing F1 Score: 0.0\r\n```\r\n"}