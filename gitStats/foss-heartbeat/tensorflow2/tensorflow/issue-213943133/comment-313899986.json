{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/313899986", "html_url": "https://github.com/tensorflow/tensorflow/issues/8375#issuecomment-313899986", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/8375", "id": 313899986, "node_id": "MDEyOklzc3VlQ29tbWVudDMxMzg5OTk4Ng==", "user": {"login": "randomrandom", "id": 1579822, "node_id": "MDQ6VXNlcjE1Nzk4MjI=", "avatar_url": "https://avatars1.githubusercontent.com/u/1579822?v=4", "gravatar_id": "", "url": "https://api.github.com/users/randomrandom", "html_url": "https://github.com/randomrandom", "followers_url": "https://api.github.com/users/randomrandom/followers", "following_url": "https://api.github.com/users/randomrandom/following{/other_user}", "gists_url": "https://api.github.com/users/randomrandom/gists{/gist_id}", "starred_url": "https://api.github.com/users/randomrandom/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/randomrandom/subscriptions", "organizations_url": "https://api.github.com/users/randomrandom/orgs", "repos_url": "https://api.github.com/users/randomrandom/repos", "events_url": "https://api.github.com/users/randomrandom/events{/privacy}", "received_events_url": "https://api.github.com/users/randomrandom/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-09T05:29:09Z", "updated_at": "2017-07-09T19:17:46Z", "author_association": "NONE", "body_html": "<p>I also get strangely small recall for multi-class problem. Here's a comparison between the metrics I get via scikit-learn and via Tensorflow:<br>\n<strong>Scikit-learn code</strong></p>\n<pre><code>        all_true = []\n        all_predicted = []\n        for i in tqdm(range(0, EPOCHS * test.data_size // BATCH_SIZE)):\n            entities_sample, predictions_sample, _, __ = sess.run([test.entities, predictions, precision_op, recall_op])\n\n            all_true.extend(entities_sample.flatten())\n            all_predicted.extend(predictions_sample.flatten())\n\n        s_prec = metrics.precision_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\n        s_rec = metrics.recall_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\n        s_f1 = metrics.f1_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\n        s_confusion = metrics.confusion_matrix(all_true, all_predicted)\n\n        print(s_prec)\n        print(s_rec)\n        print(s_f1)\n</code></pre>\n<p><strong>Scikit-learn output</strong></p>\n<pre><code>0.875761847506\n0.875743692863\n0.87575277009\n</code></pre>\n<p><strong>Tensorflow code</strong></p>\n<pre><code>        for i in tqdm(range(0, EPOCHS * test.data_size // BATCH_SIZE)):\n            entities_sample, predictions_sample, _, __ = sess.run([test.entities, predictions, precision_op, recall_op])\n\n            all_true.extend(entities_sample.flatten())\n            all_predicted.extend(predictions_sample.flatten())\n\n\n        f1_score = (2 * (precision * recall)) / (precision + recall)\n\n        final_precision, final_recall, final_f1 = sess.run([precision, recall, f1_score])\n        print('Precision:{}'.format(final_precision))\n        print('Recall:{}'.format(final_recall))\n        print('f-1 score:{}'.format(final_f1))\n</code></pre>\n<p><strong>Tensorflow output</strong></p>\n<pre><code>Precision:0.8595223781587512\nRecall:0.4297611890793756\nf-1 score:0.5730149187725008\n</code></pre>\n<p>As you can see the difference between scikit's recall and tensorflow is quite drastic. Not sure if this is a bug or a difference in the implementation.</p>", "body_text": "I also get strangely small recall for multi-class problem. Here's a comparison between the metrics I get via scikit-learn and via Tensorflow:\nScikit-learn code\n        all_true = []\n        all_predicted = []\n        for i in tqdm(range(0, EPOCHS * test.data_size // BATCH_SIZE)):\n            entities_sample, predictions_sample, _, __ = sess.run([test.entities, predictions, precision_op, recall_op])\n\n            all_true.extend(entities_sample.flatten())\n            all_predicted.extend(predictions_sample.flatten())\n\n        s_prec = metrics.precision_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\n        s_rec = metrics.recall_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\n        s_f1 = metrics.f1_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\n        s_confusion = metrics.confusion_matrix(all_true, all_predicted)\n\n        print(s_prec)\n        print(s_rec)\n        print(s_f1)\n\nScikit-learn output\n0.875761847506\n0.875743692863\n0.87575277009\n\nTensorflow code\n        for i in tqdm(range(0, EPOCHS * test.data_size // BATCH_SIZE)):\n            entities_sample, predictions_sample, _, __ = sess.run([test.entities, predictions, precision_op, recall_op])\n\n            all_true.extend(entities_sample.flatten())\n            all_predicted.extend(predictions_sample.flatten())\n\n\n        f1_score = (2 * (precision * recall)) / (precision + recall)\n\n        final_precision, final_recall, final_f1 = sess.run([precision, recall, f1_score])\n        print('Precision:{}'.format(final_precision))\n        print('Recall:{}'.format(final_recall))\n        print('f-1 score:{}'.format(final_f1))\n\nTensorflow output\nPrecision:0.8595223781587512\nRecall:0.4297611890793756\nf-1 score:0.5730149187725008\n\nAs you can see the difference between scikit's recall and tensorflow is quite drastic. Not sure if this is a bug or a difference in the implementation.", "body": "I also get strangely small recall for multi-class problem. Here's a comparison between the metrics I get via scikit-learn and via Tensorflow:\r\n**Scikit-learn code**\r\n```\r\n        all_true = []\r\n        all_predicted = []\r\n        for i in tqdm(range(0, EPOCHS * test.data_size // BATCH_SIZE)):\r\n            entities_sample, predictions_sample, _, __ = sess.run([test.entities, predictions, precision_op, recall_op])\r\n\r\n            all_true.extend(entities_sample.flatten())\r\n            all_predicted.extend(predictions_sample.flatten())\r\n\r\n        s_prec = metrics.precision_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\r\n        s_rec = metrics.recall_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\r\n        s_f1 = metrics.f1_score(all_true, all_predicted, labels=[1, 2, 3, 4, 5, 6, 7, 8, 9], average='micro')\r\n        s_confusion = metrics.confusion_matrix(all_true, all_predicted)\r\n\r\n        print(s_prec)\r\n        print(s_rec)\r\n        print(s_f1)\r\n```\r\n\r\n**Scikit-learn output**\r\n\r\n```\r\n0.875761847506\r\n0.875743692863\r\n0.87575277009\r\n```\r\n\r\n**Tensorflow code**\r\n\r\n```\r\n        for i in tqdm(range(0, EPOCHS * test.data_size // BATCH_SIZE)):\r\n            entities_sample, predictions_sample, _, __ = sess.run([test.entities, predictions, precision_op, recall_op])\r\n\r\n            all_true.extend(entities_sample.flatten())\r\n            all_predicted.extend(predictions_sample.flatten())\r\n\r\n\r\n        f1_score = (2 * (precision * recall)) / (precision + recall)\r\n\r\n        final_precision, final_recall, final_f1 = sess.run([precision, recall, f1_score])\r\n        print('Precision:{}'.format(final_precision))\r\n        print('Recall:{}'.format(final_recall))\r\n        print('f-1 score:{}'.format(final_f1))\r\n```\r\n\r\n**Tensorflow output**\r\n\r\n```\r\nPrecision:0.8595223781587512\r\nRecall:0.4297611890793756\r\nf-1 score:0.5730149187725008\r\n```\r\n\r\nAs you can see the difference between scikit's recall and tensorflow is quite drastic. Not sure if this is a bug or a difference in the implementation. "}