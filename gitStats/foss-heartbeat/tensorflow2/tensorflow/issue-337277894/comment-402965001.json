{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/402965001", "html_url": "https://github.com/tensorflow/tensorflow/issues/20451#issuecomment-402965001", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/20451", "id": 402965001, "node_id": "MDEyOklzc3VlQ29tbWVudDQwMjk2NTAwMQ==", "user": {"login": "Canxes", "id": 18511496, "node_id": "MDQ6VXNlcjE4NTExNDk2", "avatar_url": "https://avatars0.githubusercontent.com/u/18511496?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Canxes", "html_url": "https://github.com/Canxes", "followers_url": "https://api.github.com/users/Canxes/followers", "following_url": "https://api.github.com/users/Canxes/following{/other_user}", "gists_url": "https://api.github.com/users/Canxes/gists{/gist_id}", "starred_url": "https://api.github.com/users/Canxes/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Canxes/subscriptions", "organizations_url": "https://api.github.com/users/Canxes/orgs", "repos_url": "https://api.github.com/users/Canxes/repos", "events_url": "https://api.github.com/users/Canxes/events{/privacy}", "received_events_url": "https://api.github.com/users/Canxes/received_events", "type": "User", "site_admin": false}, "created_at": "2018-07-06T08:27:58Z", "updated_at": "2018-07-06T08:27:58Z", "author_association": "NONE", "body_html": "<p>Same situation with MobilenetV1, that is used as a backbone for separate model. Model was trained and then freezed with tf.contrib.quantize.create_training_graph and tf.contrib.quantize.create_eval_graph. Backbone then was separated and converted with flag --allow_nudging_weights_to_use_fast_gemm_kernel. My tensorflow version is 1.9.0-rc2.<br>\nMy thoughts so far: in the source code for TFLite there is 2 quantization routins QuantizeMultiplierGreaterThanOne and QuantizeMultiplierSmallerThanOneExp defined here. May be one can just add logic in tensorflow/contrib/lite/kernels/conv.cc to handle various real_multiplier? If i won't be able to find out anything better, then i will probably give it a try and report here.</p>", "body_text": "Same situation with MobilenetV1, that is used as a backbone for separate model. Model was trained and then freezed with tf.contrib.quantize.create_training_graph and tf.contrib.quantize.create_eval_graph. Backbone then was separated and converted with flag --allow_nudging_weights_to_use_fast_gemm_kernel. My tensorflow version is 1.9.0-rc2.\nMy thoughts so far: in the source code for TFLite there is 2 quantization routins QuantizeMultiplierGreaterThanOne and QuantizeMultiplierSmallerThanOneExp defined here. May be one can just add logic in tensorflow/contrib/lite/kernels/conv.cc to handle various real_multiplier? If i won't be able to find out anything better, then i will probably give it a try and report here.", "body": "Same situation with MobilenetV1, that is used as a backbone for separate model. Model was trained and then freezed with tf.contrib.quantize.create_training_graph and tf.contrib.quantize.create_eval_graph. Backbone then was separated and converted with flag --allow_nudging_weights_to_use_fast_gemm_kernel. My tensorflow version is 1.9.0-rc2.\r\nMy thoughts so far: in the source code for TFLite there is 2 quantization routins QuantizeMultiplierGreaterThanOne and QuantizeMultiplierSmallerThanOneExp defined here. May be one can just add logic in tensorflow/contrib/lite/kernels/conv.cc to handle various real_multiplier? If i won't be able to find out anything better, then i will probably give it a try and report here."}