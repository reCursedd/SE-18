{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21772", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21772/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21772/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21772/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21772", "id": 352705989, "node_id": "MDU6SXNzdWUzNTI3MDU5ODk=", "number": 21772, "title": "dataloss error tf.records at random times", "user": {"login": "Suryabhan90", "id": 8082518, "node_id": "MDQ6VXNlcjgwODI1MTg=", "avatar_url": "https://avatars0.githubusercontent.com/u/8082518?v=4", "gravatar_id": "", "url": "https://api.github.com/users/Suryabhan90", "html_url": "https://github.com/Suryabhan90", "followers_url": "https://api.github.com/users/Suryabhan90/followers", "following_url": "https://api.github.com/users/Suryabhan90/following{/other_user}", "gists_url": "https://api.github.com/users/Suryabhan90/gists{/gist_id}", "starred_url": "https://api.github.com/users/Suryabhan90/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/Suryabhan90/subscriptions", "organizations_url": "https://api.github.com/users/Suryabhan90/orgs", "repos_url": "https://api.github.com/users/Suryabhan90/repos", "events_url": "https://api.github.com/users/Suryabhan90/events{/privacy}", "received_events_url": "https://api.github.com/users/Suryabhan90/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "open", "locked": false, "assignee": {"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "karmel", "id": 667809, "node_id": "MDQ6VXNlcjY2NzgwOQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/667809?v=4", "gravatar_id": "", "url": "https://api.github.com/users/karmel", "html_url": "https://github.com/karmel", "followers_url": "https://api.github.com/users/karmel/followers", "following_url": "https://api.github.com/users/karmel/following{/other_user}", "gists_url": "https://api.github.com/users/karmel/gists{/gist_id}", "starred_url": "https://api.github.com/users/karmel/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/karmel/subscriptions", "organizations_url": "https://api.github.com/users/karmel/orgs", "repos_url": "https://api.github.com/users/karmel/repos", "events_url": "https://api.github.com/users/karmel/events{/privacy}", "received_events_url": "https://api.github.com/users/karmel/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 11, "created_at": "2018-08-21T20:53:38Z", "updated_at": "2018-11-15T19:04:08Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>OS Platform and Distribution --&gt; Ubuntu - 18.04.1<br>\nTensorFlow installed from --&gt; using pip<br>\nTensorFlow version --&gt; 1.8.0<br>\nBazel version --&gt; don't know<br>\nCUDA/cuDNN version --&gt; cuda 9<br>\nGPU model and memory --&gt; nvidia Titan XP 12GB</p>\n<p>I am stuck at a very strange issue for a long time. This is my problem -<br>\nI have a tfrecords file (name = \"Input.tfrecords\") from which I read data and then I do some modifcation to the data and store it to another tfrecords file (name = \"Output.tfrecods\") . Below is the code snippet --</p>\n<pre><code>tf.reset_default_graph()\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _str_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n\n\ndef _float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef som_function(FLAGS):\n    with tf.Graph().as_default() as g:\n\n        tfr_writer = tf.python_io.TFRecordWriter(FLAGS.Output_tfrdatafile)\n\n        dataset = tf.data.TFRecordDataset(FLAGS.Input_tfrdatafile)\n\n        dataset = dataset.map(lambda x: reader.initial_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\n\n        dataset = dataset.batch(FLAGS.BATCH_SIZE)\n        iterator = dataset.make_one_shot_iterator()\n\n        images, original_ig, img_name = iterator.get_next()\n\n\n        org_batch = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]), trainable=False)\n        initial = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]))\n        org_batch_assign_op = org_batch.assign(original_ig)\n\n        initial_assign_op = initial.assign(images)\n\n        total_loss = #someloss function\n\n\n\n        train_op = tf.train.MomentumOptimizer(FLAGS.LEARNING_RATE, momentum=0.95, use_nesterov=True,\n                                              name=\"non_paraopt_SGD\").minimize(total_loss,\n                                                                               global_step=global_step)\n\n\n        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            start_time = time.time()\n            batches_count = 0\n            while True:\n                try:\n                    _, _, image_names = sess.run([initial_assign_op,org_batch_assign_op,  img_name])\n\n                    //some code that updates initial variable\n\n                    org_batch = tf.cast(org_batch, tf.uint8)\n                    image_t, org_image_t = sess.run([initial, org_batch])\n\n                    if not FLAGS.addNetworklose:\n                        lambda_val = np.zeros(image_t.shape).astype(np.float32)\n\n                    for i in range(image_t.shape[0]):\n                        filename = str(image_names[i], 'utf-8')\n\n                        example = tf.train.Example(features=tf.train.Features(feature={\n                                'file_name': _str_feature(filename),\n                                'float_image': _float_feature(image_t[i] + reader.mean_pixel),\n                                'image_raw': _bytes_feature(org_image_t[i].tostring()),\n                                'lambda_image': _float_feature(lambda_val[i])\n                            }))\n                        tfr_writer.write(example.SerializeToString())\n\n                    batches_count = batches_count + 1\n                except tf.errors.OutOfRangeError:\n                    print(\"final time elspased\", (time.time() - start_time))\n                    print('Done doing non paramteric part')\n                    break\n\n            tfr_writer.close()\n</code></pre>\n<p>I always succesfully creats the  \"Output.tfrecods\".But whenever I read the file  \"Output.tfrecods\"file, I randomly get the <strong>Dataloss Error</strong>. This is where I am trying to read the Output.tfrecods file.</p>\n<pre><code>def start_training(FLAGS):\n    tf.reset_default_graph()\n    run_id = FLAGS.MODEL_NAME if FLAGS.MODEL_NAME else str(uuid.uuid4())\n\n    if not os.path.exists(FLAGS.summary_path):\n        os.makedirs(FLAGS.summary_path)\n\n    model_path = '%s/%s' % (FLAGS.MODEL_DIR, run_id)\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n\n    training_dataset = tf.data.TFRecordDataset(FLAGS.Training_tfrdatafile)\n    training_dataset = training_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\n\n    training_dataset = training_dataset.batch(FLAGS.BATCH_SIZE)\n    min_queue_examples = int(FLAGS.EPOCHS * 0.4)\n    training_dataset = training_dataset.shuffle(buffer_size=min_queue_examples + 3 * FLAGS.BATCH_SIZE)\n\n    validation_dataset = tf.data.TFRecordDataset(FLAGS.Validation_tfrdatafile)\n    validation_dataset = validation_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\n    validation_dataset = validation_dataset.batch(FLAGS.VAL_BATCH_SIZE)\n    \n    iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n                                               training_dataset.output_shapes)\n\n    training_init_op = iterator.make_initializer(training_dataset)\n    validation_init_op = iterator.make_initializer(validation_dataset)\n\n    target_ig, images, img_name, _ = iterator.get_next()\n\n    ae_inputs = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),\n                               name='auto_input')  # input to the network (MNIST images)\n    target = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),\n                            name='target')\n\n    ae_output = model.net(ae_inputs, training=True)\n\n    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n    img_loss = tf.nn.l2_loss((target - ae_output), name='img_l2loss') / tf.to_float(size2)\n\n    tv_loss = total_variation_loss(ae_output, FLAGS.HEIGHT, FLAGS.WIDTH)\n    loss = img_loss + 200.0 * tv_loss\n\n    global_step = tf.Variable(FLAGS.gs_val, name=\"p_global_step\", trainable=False)\n\n    \n    train_op = tf.train.AdamOptimizer(learning_rate, name=\"p_trainopt\").minimize(loss, global_step=global_step)\n    \n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    return_lr = FLAGS.LEARNING_RATE\n\n    with tf.Session(config=config) as sess:\n        \n        saver = tf.train.Saver(tf.trainable_variables())\n        if FLAGS.ModelFromName:\n            file = FLAGS.MODEL_FILENAME\n        else:\n            file = tf.train.latest_checkpoint(model_path)\n        # file = model_path + '/' + FLAGS.MODEL_FILENAME\n\n        sess.run(init_op)\n        if file:\n            print('Restoring model from {}'.format(file))\n            saver.restore(sess, file)\n\n        start_time = time.time()\n        start_ti = time.time()\n        acc_loss = []\n        vgg_pl_loss = []\n        img_pl_loss = []\n        cl_pl_loss = []\n        sl_pl_loss = []\n        fl_pl_loss = []\n\n        cont_los_FF = 0\n        stl_los_FF = 0\n\n        total_batches = FLAGS.TOTALINPUT // FLAGS.BATCH_SIZE\n        val_batches = FLAGS.TOTALVAL // FLAGS.VAL_BATCH_SIZE\n        print('the value of total batches is: ', total_batches)\n        for ep_count in range(FLAGS.EPOCHS):\n            sess.run(training_init_op)\n            count = 1\n            st_t = time.time()\n            # if ep_count+1 &lt;= FLAGS.EPOCHS  or ep_count == 0:\n            while True:\n                try:\n                    ig_b, tg_b = sess.run([images, target_ig])\n\n                    _, loss_t,img_l2_t, step = sess.run([train_op, loss, img_loss],feed_dict={ae_inputs: ig_b,\n                                                                                           target: tg_b,\n                                                                                           learning_rate: FLAGS.LEARNING_RATE})\n                                                                                \n                    count = count + 1\n                except tf.errors.OutOfRangeError:\n                    print(\"final time elspased\", (time.time() - st_t))\n                    #  print('Done doing non paramteric part')\n                    break\n\n            print('Number of epochs done= ', (ep_count + 1))\n            if (ep_count + 1) % FLAGS.chanelr == 0:\n                print('the value of count is: ', count)\n                FLAGS.LEARNING_RATE = FLAGS.LEARNING_RATE / FLAGS.div\n                print('learning rate now: ', FLAGS.LEARNING_RATE)\n                return_lr = FLAGS.LEARNING_RATE\n\n        print(step, loss_t, elapsed_time)\n        saver.save(sess, model_path + '/style-model', global_step=step)\n        nameofmodel = model_path + '/style-model-' + str(step)\n        print(\"final time elspased\", (time.time() - start_ti))\n        print('Done training -- epoch limit reached')\n\n    return step, return_lr, nameofmodel\n</code></pre>\n<p>I have to restart my system and re-run the above code for 5-6 times and then it works fine. And when I run the same code on a different linux machine its work fine all the time. I really don't know what is the issue here.</p>\n<p>Thanks in advance. Please comment if need more explanation from my side.</p>\n<p>Here is the stack trace</p>\n<pre><code>\nTraceback (most recent call last):\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n    return fn(*args)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n    options, feed_dict, fetch_list, target_list, run_metadata)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"Maincreateall.py\", line 240, in &lt;module&gt;\n    main()\n  File \"Maincreateall.py\", line 194, in main\n    opts_para[\"gs_val\"], nameofmodel = tm.train_model(opts_para)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 465, in train_model\n    gs,_,nameofmodel = start_training(FLAGS)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 277, in start_training\n    ig_b, tg_b = sess.run([images, target_ig])\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\n    run_metadata_ptr)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\n    run_metadata)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"Maincreateall.py\", line 240, in &lt;module&gt;\n    main()\n  File \"Maincreateall.py\", line 194, in main\n    opts_para[\"gs_val\"], nameofmodel = tm.train_model(opts_para)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 465, in train_model\n    gs,_,nameofmodel = start_training(FLAGS)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 137, in start_training\n    target_ig, images, img_name,_ = iterator.get_next()\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 370, in get_next\n    name=name)), self._output_types,\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1466, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): corrupted record at 775087002\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n</code></pre>", "body_text": "OS Platform and Distribution --> Ubuntu - 18.04.1\nTensorFlow installed from --> using pip\nTensorFlow version --> 1.8.0\nBazel version --> don't know\nCUDA/cuDNN version --> cuda 9\nGPU model and memory --> nvidia Titan XP 12GB\nI am stuck at a very strange issue for a long time. This is my problem -\nI have a tfrecords file (name = \"Input.tfrecords\") from which I read data and then I do some modifcation to the data and store it to another tfrecords file (name = \"Output.tfrecods\") . Below is the code snippet --\ntf.reset_default_graph()\nconfig = tf.ConfigProto()\nconfig.gpu_options.allow_growth = True\n\n\ndef _bytes_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef _str_feature(value):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\n\n\ndef _float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\n\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef som_function(FLAGS):\n    with tf.Graph().as_default() as g:\n\n        tfr_writer = tf.python_io.TFRecordWriter(FLAGS.Output_tfrdatafile)\n\n        dataset = tf.data.TFRecordDataset(FLAGS.Input_tfrdatafile)\n\n        dataset = dataset.map(lambda x: reader.initial_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\n\n        dataset = dataset.batch(FLAGS.BATCH_SIZE)\n        iterator = dataset.make_one_shot_iterator()\n\n        images, original_ig, img_name = iterator.get_next()\n\n\n        org_batch = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]), trainable=False)\n        initial = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]))\n        org_batch_assign_op = org_batch.assign(original_ig)\n\n        initial_assign_op = initial.assign(images)\n\n        total_loss = #someloss function\n\n\n\n        train_op = tf.train.MomentumOptimizer(FLAGS.LEARNING_RATE, momentum=0.95, use_nesterov=True,\n                                              name=\"non_paraopt_SGD\").minimize(total_loss,\n                                                                               global_step=global_step)\n\n\n        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n\n        with tf.Session(config=config) as sess:\n            sess.run(init_op)\n            start_time = time.time()\n            batches_count = 0\n            while True:\n                try:\n                    _, _, image_names = sess.run([initial_assign_op,org_batch_assign_op,  img_name])\n\n                    //some code that updates initial variable\n\n                    org_batch = tf.cast(org_batch, tf.uint8)\n                    image_t, org_image_t = sess.run([initial, org_batch])\n\n                    if not FLAGS.addNetworklose:\n                        lambda_val = np.zeros(image_t.shape).astype(np.float32)\n\n                    for i in range(image_t.shape[0]):\n                        filename = str(image_names[i], 'utf-8')\n\n                        example = tf.train.Example(features=tf.train.Features(feature={\n                                'file_name': _str_feature(filename),\n                                'float_image': _float_feature(image_t[i] + reader.mean_pixel),\n                                'image_raw': _bytes_feature(org_image_t[i].tostring()),\n                                'lambda_image': _float_feature(lambda_val[i])\n                            }))\n                        tfr_writer.write(example.SerializeToString())\n\n                    batches_count = batches_count + 1\n                except tf.errors.OutOfRangeError:\n                    print(\"final time elspased\", (time.time() - start_time))\n                    print('Done doing non paramteric part')\n                    break\n\n            tfr_writer.close()\n\nI always succesfully creats the  \"Output.tfrecods\".But whenever I read the file  \"Output.tfrecods\"file, I randomly get the Dataloss Error. This is where I am trying to read the Output.tfrecods file.\ndef start_training(FLAGS):\n    tf.reset_default_graph()\n    run_id = FLAGS.MODEL_NAME if FLAGS.MODEL_NAME else str(uuid.uuid4())\n\n    if not os.path.exists(FLAGS.summary_path):\n        os.makedirs(FLAGS.summary_path)\n\n    model_path = '%s/%s' % (FLAGS.MODEL_DIR, run_id)\n    if not os.path.exists(model_path):\n        os.makedirs(model_path)\n\n    training_dataset = tf.data.TFRecordDataset(FLAGS.Training_tfrdatafile)\n    training_dataset = training_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\n\n    training_dataset = training_dataset.batch(FLAGS.BATCH_SIZE)\n    min_queue_examples = int(FLAGS.EPOCHS * 0.4)\n    training_dataset = training_dataset.shuffle(buffer_size=min_queue_examples + 3 * FLAGS.BATCH_SIZE)\n\n    validation_dataset = tf.data.TFRecordDataset(FLAGS.Validation_tfrdatafile)\n    validation_dataset = validation_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\n    validation_dataset = validation_dataset.batch(FLAGS.VAL_BATCH_SIZE)\n    \n    iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\n                                               training_dataset.output_shapes)\n\n    training_init_op = iterator.make_initializer(training_dataset)\n    validation_init_op = iterator.make_initializer(validation_dataset)\n\n    target_ig, images, img_name, _ = iterator.get_next()\n\n    ae_inputs = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),\n                               name='auto_input')  # input to the network (MNIST images)\n    target = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),\n                            name='target')\n\n    ae_output = model.net(ae_inputs, training=True)\n\n    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n    img_loss = tf.nn.l2_loss((target - ae_output), name='img_l2loss') / tf.to_float(size2)\n\n    tv_loss = total_variation_loss(ae_output, FLAGS.HEIGHT, FLAGS.WIDTH)\n    loss = img_loss + 200.0 * tv_loss\n\n    global_step = tf.Variable(FLAGS.gs_val, name=\"p_global_step\", trainable=False)\n\n    \n    train_op = tf.train.AdamOptimizer(learning_rate, name=\"p_trainopt\").minimize(loss, global_step=global_step)\n    \n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n    return_lr = FLAGS.LEARNING_RATE\n\n    with tf.Session(config=config) as sess:\n        \n        saver = tf.train.Saver(tf.trainable_variables())\n        if FLAGS.ModelFromName:\n            file = FLAGS.MODEL_FILENAME\n        else:\n            file = tf.train.latest_checkpoint(model_path)\n        # file = model_path + '/' + FLAGS.MODEL_FILENAME\n\n        sess.run(init_op)\n        if file:\n            print('Restoring model from {}'.format(file))\n            saver.restore(sess, file)\n\n        start_time = time.time()\n        start_ti = time.time()\n        acc_loss = []\n        vgg_pl_loss = []\n        img_pl_loss = []\n        cl_pl_loss = []\n        sl_pl_loss = []\n        fl_pl_loss = []\n\n        cont_los_FF = 0\n        stl_los_FF = 0\n\n        total_batches = FLAGS.TOTALINPUT // FLAGS.BATCH_SIZE\n        val_batches = FLAGS.TOTALVAL // FLAGS.VAL_BATCH_SIZE\n        print('the value of total batches is: ', total_batches)\n        for ep_count in range(FLAGS.EPOCHS):\n            sess.run(training_init_op)\n            count = 1\n            st_t = time.time()\n            # if ep_count+1 <= FLAGS.EPOCHS  or ep_count == 0:\n            while True:\n                try:\n                    ig_b, tg_b = sess.run([images, target_ig])\n\n                    _, loss_t,img_l2_t, step = sess.run([train_op, loss, img_loss],feed_dict={ae_inputs: ig_b,\n                                                                                           target: tg_b,\n                                                                                           learning_rate: FLAGS.LEARNING_RATE})\n                                                                                \n                    count = count + 1\n                except tf.errors.OutOfRangeError:\n                    print(\"final time elspased\", (time.time() - st_t))\n                    #  print('Done doing non paramteric part')\n                    break\n\n            print('Number of epochs done= ', (ep_count + 1))\n            if (ep_count + 1) % FLAGS.chanelr == 0:\n                print('the value of count is: ', count)\n                FLAGS.LEARNING_RATE = FLAGS.LEARNING_RATE / FLAGS.div\n                print('learning rate now: ', FLAGS.LEARNING_RATE)\n                return_lr = FLAGS.LEARNING_RATE\n\n        print(step, loss_t, elapsed_time)\n        saver.save(sess, model_path + '/style-model', global_step=step)\n        nameofmodel = model_path + '/style-model-' + str(step)\n        print(\"final time elspased\", (time.time() - start_ti))\n        print('Done training -- epoch limit reached')\n\n    return step, return_lr, nameofmodel\n\nI have to restart my system and re-run the above code for 5-6 times and then it works fine. And when I run the same code on a different linux machine its work fine all the time. I really don't know what is the issue here.\nThanks in advance. Please comment if need more explanation from my side.\nHere is the stack trace\n\nTraceback (most recent call last):\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\n    return fn(*args)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\n    options, feed_dict, fetch_list, target_list, run_metadata)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\n    run_metadata)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"Maincreateall.py\", line 240, in <module>\n    main()\n  File \"Maincreateall.py\", line 194, in main\n    opts_para[\"gs_val\"], nameofmodel = tm.train_model(opts_para)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 465, in train_model\n    gs,_,nameofmodel = start_training(FLAGS)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 277, in start_training\n    ig_b, tg_b = sess.run([images, target_ig])\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\n    run_metadata_ptr)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\n    feed_dict_tensor, options, run_metadata)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\n    run_metadata)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\n    raise type(e)(node_def, op, message)\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\n\nCaused by op 'IteratorGetNext', defined at:\n  File \"Maincreateall.py\", line 240, in <module>\n    main()\n  File \"Maincreateall.py\", line 194, in main\n    opts_para[\"gs_val\"], nameofmodel = tm.train_model(opts_para)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 465, in train_model\n    gs,_,nameofmodel = start_training(FLAGS)\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 137, in start_training\n    target_ig, images, img_name,_ = iterator.get_next()\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 370, in get_next\n    name=name)), self._output_types,\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1466, in iterator_get_next\n    output_shapes=output_shapes, name=name)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nDataLossError (see above for traceback): corrupted record at 775087002\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]", "body": "OS Platform and Distribution --> Ubuntu - 18.04.1\r\nTensorFlow installed from --> using pip\r\nTensorFlow version --> 1.8.0\r\nBazel version --> don't know \r\nCUDA/cuDNN version --> cuda 9\r\nGPU model and memory --> nvidia Titan XP 12GB\r\n\r\n\r\nI am stuck at a very strange issue for a long time. This is my problem - \r\nI have a tfrecords file (name = \"Input.tfrecords\") from which I read data and then I do some modifcation to the data and store it to another tfrecords file (name = \"Output.tfrecods\") . Below is the code snippet --\r\n\r\n    tf.reset_default_graph()\r\n    config = tf.ConfigProto()\r\n    config.gpu_options.allow_growth = True\r\n    \r\n    \r\n    def _bytes_feature(value):\r\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\r\n    \r\n    \r\n    def _str_feature(value):\r\n        return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode('utf-8')]))\r\n    \r\n    \r\n    def _float_feature(value):\r\n        return tf.train.Feature(float_list=tf.train.FloatList(value=value.reshape(-1)))\r\n    \r\n    \r\n    def _int64_feature(value):\r\n        return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\r\n    \r\n    \r\n    def som_function(FLAGS):\r\n        with tf.Graph().as_default() as g:\r\n    \r\n            tfr_writer = tf.python_io.TFRecordWriter(FLAGS.Output_tfrdatafile)\r\n    \r\n            dataset = tf.data.TFRecordDataset(FLAGS.Input_tfrdatafile)\r\n    \r\n            dataset = dataset.map(lambda x: reader.initial_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\r\n    \r\n            dataset = dataset.batch(FLAGS.BATCH_SIZE)\r\n            iterator = dataset.make_one_shot_iterator()\r\n    \r\n            images, original_ig, img_name = iterator.get_next()\r\n    \r\n    \r\n            org_batch = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]), trainable=False)\r\n            initial = tf.Variable(tf.random_normal([FLAGS.BATCH_SIZE, FLAGS.HEIGHT, FLAGS.WIDTH, 3]))\r\n            org_batch_assign_op = org_batch.assign(original_ig)\r\n    \r\n            initial_assign_op = initial.assign(images)\r\n    \r\n            total_loss = #someloss function\r\n    \r\n    \r\n    \r\n            train_op = tf.train.MomentumOptimizer(FLAGS.LEARNING_RATE, momentum=0.95, use_nesterov=True,\r\n                                                  name=\"non_paraopt_SGD\").minimize(total_loss,\r\n                                                                                   global_step=global_step)\r\n    \r\n    \r\n            init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    \r\n            with tf.Session(config=config) as sess:\r\n                sess.run(init_op)\r\n                start_time = time.time()\r\n                batches_count = 0\r\n                while True:\r\n                    try:\r\n                        _, _, image_names = sess.run([initial_assign_op,org_batch_assign_op,  img_name])\r\n    \r\n                        //some code that updates initial variable\r\n    \r\n                        org_batch = tf.cast(org_batch, tf.uint8)\r\n                        image_t, org_image_t = sess.run([initial, org_batch])\r\n    \r\n                        if not FLAGS.addNetworklose:\r\n                            lambda_val = np.zeros(image_t.shape).astype(np.float32)\r\n    \r\n                        for i in range(image_t.shape[0]):\r\n                            filename = str(image_names[i], 'utf-8')\r\n    \r\n                            example = tf.train.Example(features=tf.train.Features(feature={\r\n                                    'file_name': _str_feature(filename),\r\n                                    'float_image': _float_feature(image_t[i] + reader.mean_pixel),\r\n                                    'image_raw': _bytes_feature(org_image_t[i].tostring()),\r\n                                    'lambda_image': _float_feature(lambda_val[i])\r\n                                }))\r\n                            tfr_writer.write(example.SerializeToString())\r\n    \r\n                        batches_count = batches_count + 1\r\n                    except tf.errors.OutOfRangeError:\r\n                        print(\"final time elspased\", (time.time() - start_time))\r\n                        print('Done doing non paramteric part')\r\n                        break\r\n    \r\n                tfr_writer.close()\r\n\r\n\r\nI always succesfully creats the  \"Output.tfrecods\".But whenever I read the file  \"Output.tfrecods\"file, I randomly get the **Dataloss Error**. This is where I am trying to read the Output.tfrecods file.\r\n```\r\ndef start_training(FLAGS):\r\n    tf.reset_default_graph()\r\n    run_id = FLAGS.MODEL_NAME if FLAGS.MODEL_NAME else str(uuid.uuid4())\r\n\r\n    if not os.path.exists(FLAGS.summary_path):\r\n        os.makedirs(FLAGS.summary_path)\r\n\r\n    model_path = '%s/%s' % (FLAGS.MODEL_DIR, run_id)\r\n    if not os.path.exists(model_path):\r\n        os.makedirs(model_path)\r\n\r\n    training_dataset = tf.data.TFRecordDataset(FLAGS.Training_tfrdatafile)\r\n    training_dataset = training_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\r\n\r\n    training_dataset = training_dataset.batch(FLAGS.BATCH_SIZE)\r\n    min_queue_examples = int(FLAGS.EPOCHS * 0.4)\r\n    training_dataset = training_dataset.shuffle(buffer_size=min_queue_examples + 3 * FLAGS.BATCH_SIZE)\r\n\r\n    validation_dataset = tf.data.TFRecordDataset(FLAGS.Validation_tfrdatafile)\r\n    validation_dataset = validation_dataset.map(lambda x: reader.lambda_parser(x, FLAGS.HEIGHT, FLAGS.WIDTH))\r\n    validation_dataset = validation_dataset.batch(FLAGS.VAL_BATCH_SIZE)\r\n    \r\n    iterator = tf.data.Iterator.from_structure(training_dataset.output_types,\r\n                                               training_dataset.output_shapes)\r\n\r\n    training_init_op = iterator.make_initializer(training_dataset)\r\n    validation_init_op = iterator.make_initializer(validation_dataset)\r\n\r\n    target_ig, images, img_name, _ = iterator.get_next()\r\n\r\n    ae_inputs = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),\r\n                               name='auto_input')  # input to the network (MNIST images)\r\n    target = tf.placeholder(tf.float32, (None, FLAGS.HEIGHT, FLAGS.WIDTH, 3),\r\n                            name='target')\r\n\r\n    ae_output = model.net(ae_inputs, training=True)\r\n\r\n    learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\r\n    img_loss = tf.nn.l2_loss((target - ae_output), name='img_l2loss') / tf.to_float(size2)\r\n\r\n    tv_loss = total_variation_loss(ae_output, FLAGS.HEIGHT, FLAGS.WIDTH)\r\n    loss = img_loss + 200.0 * tv_loss\r\n\r\n    global_step = tf.Variable(FLAGS.gs_val, name=\"p_global_step\", trainable=False)\r\n\r\n    \r\n    train_op = tf.train.AdamOptimizer(learning_rate, name=\"p_trainopt\").minimize(loss, global_step=global_step)\r\n    \r\n    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\r\n    return_lr = FLAGS.LEARNING_RATE\r\n\r\n    with tf.Session(config=config) as sess:\r\n        \r\n        saver = tf.train.Saver(tf.trainable_variables())\r\n        if FLAGS.ModelFromName:\r\n            file = FLAGS.MODEL_FILENAME\r\n        else:\r\n            file = tf.train.latest_checkpoint(model_path)\r\n        # file = model_path + '/' + FLAGS.MODEL_FILENAME\r\n\r\n        sess.run(init_op)\r\n        if file:\r\n            print('Restoring model from {}'.format(file))\r\n            saver.restore(sess, file)\r\n\r\n        start_time = time.time()\r\n        start_ti = time.time()\r\n        acc_loss = []\r\n        vgg_pl_loss = []\r\n        img_pl_loss = []\r\n        cl_pl_loss = []\r\n        sl_pl_loss = []\r\n        fl_pl_loss = []\r\n\r\n        cont_los_FF = 0\r\n        stl_los_FF = 0\r\n\r\n        total_batches = FLAGS.TOTALINPUT // FLAGS.BATCH_SIZE\r\n        val_batches = FLAGS.TOTALVAL // FLAGS.VAL_BATCH_SIZE\r\n        print('the value of total batches is: ', total_batches)\r\n        for ep_count in range(FLAGS.EPOCHS):\r\n            sess.run(training_init_op)\r\n            count = 1\r\n            st_t = time.time()\r\n            # if ep_count+1 <= FLAGS.EPOCHS  or ep_count == 0:\r\n            while True:\r\n                try:\r\n                    ig_b, tg_b = sess.run([images, target_ig])\r\n\r\n                    _, loss_t,img_l2_t, step = sess.run([train_op, loss, img_loss],feed_dict={ae_inputs: ig_b,\r\n                                                                                           target: tg_b,\r\n                                                                                           learning_rate: FLAGS.LEARNING_RATE})\r\n                                                                                \r\n                    count = count + 1\r\n                except tf.errors.OutOfRangeError:\r\n                    print(\"final time elspased\", (time.time() - st_t))\r\n                    #  print('Done doing non paramteric part')\r\n                    break\r\n\r\n            print('Number of epochs done= ', (ep_count + 1))\r\n            if (ep_count + 1) % FLAGS.chanelr == 0:\r\n                print('the value of count is: ', count)\r\n                FLAGS.LEARNING_RATE = FLAGS.LEARNING_RATE / FLAGS.div\r\n                print('learning rate now: ', FLAGS.LEARNING_RATE)\r\n                return_lr = FLAGS.LEARNING_RATE\r\n\r\n        print(step, loss_t, elapsed_time)\r\n        saver.save(sess, model_path + '/style-model', global_step=step)\r\n        nameofmodel = model_path + '/style-model-' + str(step)\r\n        print(\"final time elspased\", (time.time() - start_ti))\r\n        print('Done training -- epoch limit reached')\r\n\r\n    return step, return_lr, nameofmodel\r\n```\r\n\r\n\r\n\r\n\r\nI have to restart my system and re-run the above code for 5-6 times and then it works fine. And when I run the same code on a different linux machine its work fine all the time. I really don't know what is the issue here.\r\n\r\nThanks in advance. Please comment if need more explanation from my side. \r\n\r\nHere is the stack trace\r\n```\r\n\r\nTraceback (most recent call last):\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1322, in _do_call\r\n    return fn(*args)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1307, in _run_fn\r\n    options, feed_dict, fetch_list, target_list, run_metadata)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1409, in _call_tf_sessionrun\r\n    run_metadata)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\r\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"Maincreateall.py\", line 240, in <module>\r\n    main()\r\n  File \"Maincreateall.py\", line 194, in main\r\n    opts_para[\"gs_val\"], nameofmodel = tm.train_model(opts_para)\r\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 465, in train_model\r\n    gs,_,nameofmodel = start_training(FLAGS)\r\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 277, in start_training\r\n    ig_b, tg_b = sess.run([images, target_ig])\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 900, in run\r\n    run_metadata_ptr)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1135, in _run\r\n    feed_dict_tensor, options, run_metadata)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1316, in _do_run\r\n    run_metadata)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/client/session.py\", line 1335, in _do_call\r\n    raise type(e)(node_def, op, message)\r\ntensorflow.python.framework.errors_impl.DataLossError: corrupted record at 775087002\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\r\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n\r\nCaused by op 'IteratorGetNext', defined at:\r\n  File \"Maincreateall.py\", line 240, in <module>\r\n    main()\r\n  File \"Maincreateall.py\", line 194, in main\r\n    opts_para[\"gs_val\"], nameofmodel = tm.train_model(opts_para)\r\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 465, in train_model\r\n    gs,_,nameofmodel = start_training(FLAGS)\r\n  File \"/home/suryabhan/Desktop/New_NST_MAC/teststuff.py\", line 137, in start_training\r\n    target_ig, images, img_name,_ = iterator.get_next()\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\", line 370, in get_next\r\n    name=name)), self._output_types,\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\", line 1466, in iterator_get_next\r\n    output_shapes=output_shapes, name=name)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\r\n    op_def=op_def)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3392, in create_op\r\n    op_def=op_def)\r\n  File \"/home/suryabhan/anaconda3/envs/nn_env/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1718, in __init__\r\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\r\n\r\nDataLossError (see above for traceback): corrupted record at 775087002\r\n         [[Node: IteratorGetNext = IteratorGetNext[output_shapes=[[?,256,256,3], [?,256,256,3], [?], [?,256,256,3]], output_types=[DT_FLOAT, DT_FLOAT, DT_STRI\r\nNG, DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Iterator)]]\r\n```\r\n\r\n\r\n"}