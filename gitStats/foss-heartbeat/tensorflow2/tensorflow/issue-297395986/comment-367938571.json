{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/367938571", "html_url": "https://github.com/tensorflow/tensorflow/issues/17037#issuecomment-367938571", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/17037", "id": 367938571, "node_id": "MDEyOklzc3VlQ29tbWVudDM2NzkzODU3MQ==", "user": {"login": "bignamehyp", "id": 3474655, "node_id": "MDQ6VXNlcjM0NzQ2NTU=", "avatar_url": "https://avatars2.githubusercontent.com/u/3474655?v=4", "gravatar_id": "", "url": "https://api.github.com/users/bignamehyp", "html_url": "https://github.com/bignamehyp", "followers_url": "https://api.github.com/users/bignamehyp/followers", "following_url": "https://api.github.com/users/bignamehyp/following{/other_user}", "gists_url": "https://api.github.com/users/bignamehyp/gists{/gist_id}", "starred_url": "https://api.github.com/users/bignamehyp/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/bignamehyp/subscriptions", "organizations_url": "https://api.github.com/users/bignamehyp/orgs", "repos_url": "https://api.github.com/users/bignamehyp/repos", "events_url": "https://api.github.com/users/bignamehyp/events{/privacy}", "received_events_url": "https://api.github.com/users/bignamehyp/received_events", "type": "User", "site_admin": false}, "created_at": "2018-02-23T08:08:10Z", "updated_at": "2018-02-23T08:08:10Z", "author_association": "MEMBER", "body_html": "<p>GPU memory allocator still holds the memory. It didn't released back to OS. You can verify by checking the number of training params in your training.</p>\n<p>self.num_trainable_params = np.sum([<br>\nnp.prod(var.get_shape().as_list()) for var in tf.trainable_variables()<br>\n])<br>\ntf.logging.info(<br>\n'number of trainable params: {}'.format(self.num_trainable_params))</p>", "body_text": "GPU memory allocator still holds the memory. It didn't released back to OS. You can verify by checking the number of training params in your training.\nself.num_trainable_params = np.sum([\nnp.prod(var.get_shape().as_list()) for var in tf.trainable_variables()\n])\ntf.logging.info(\n'number of trainable params: {}'.format(self.num_trainable_params))", "body": "GPU memory allocator still holds the memory. It didn't released back to OS. You can verify by checking the number of training params in your training. \r\n\r\nself.num_trainable_params = np.sum([\r\n        np.prod(var.get_shape().as_list()) for var in tf.trainable_variables()\r\n    ])\r\n    tf.logging.info(\r\n        'number of trainable params: {}'.format(self.num_trainable_params))"}