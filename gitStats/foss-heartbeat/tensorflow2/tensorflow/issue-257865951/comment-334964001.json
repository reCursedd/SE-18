{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/334964001", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#issuecomment-334964001", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049", "id": 334964001, "node_id": "MDEyOklzc3VlQ29tbWVudDMzNDk2NDAwMQ==", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-07T20:34:15Z", "updated_at": "2017-10-09T18:05:15Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Ping <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> , <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a>,<br>\nUnfortunately I think we've run into a conflict. By examining the code <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> submitted, I think we have very similar ideas but the implementation <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> and I worked out here forms a super set of what <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> has in his implementation.</p>\n<p>edit: I take the above statement back. <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> seems to have batched sub-tiles within a single kernel launch. It is useful to learn the motivation behind such design.</p>\n<p>Notably from his comment:</p>\n<pre><code>// Each thread block operates on a single rectangle tile, where its width is\n// kTileLength (we currently set it to 64) and its height is small_dim,\n// We set the thread block's X dimension to be tile_num_per_block, and its Y\n// and Z to be one.\n</code></pre>\n<p>and</p>\n<pre><code>// When only one of the dimensions is smaller than kMinDimensionToUseTiles,\n// we use one block to process a rectangle region with the size of\n// kTileLength * small_dim. We found that when set kTileLength to 64 on\n// TitanX Maxwell GPU, it achieves the best performance.\n</code></pre>\n<p>A few things I would like to point out:</p>\n<ul>\n<li>this PR specializes the kernel launch parameters for each type.</li>\n<li>this PR also does not have a fixed long side len (in fact we have choices of {32, 64, 128...1024}).</li>\n<li>this PR also has a very nicely designed kernel dispatcher (thanks to <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=150663\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/jlebar\">@jlebar</a> ) that works to select among multiple pre-compiled kernels.</li>\n<li>this PR has dynamically determined share memory sizes and therefore will be more shared-memory-efficient.</li>\n<li><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> seems to have written another kernel but I simply generalized the SwapDimension1And2InTensor3UsingTiles kernel so it seems like my approach produces less maintenance overhead.</li>\n<li>Similarly, can you explain why do you need two <code>__syncthreads()</code>? I think you are operating on a 1D sliding window in a tile to transpose the matrix. I suspect that this could hurt parallelism; also to achieve performance portability, I think we should make this into multiple blocks and let the hardware scheduler make the decision of how to schedule sub-tiles.</li>\n<li>this PR performs exhaustive experiments on 2 server-grade GPUs including P100 and K40 and thus the performance results will be more reliable and trust-worthy for performance-critical scenarios.</li>\n</ul>\n<p>So I think if I get these facts right, I would like to propose the following to resolve the conflicts:</p>\n<ul>\n<li>Absorb suggestions from <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a>. Maybe there are reasons for having another kernel SwapDimension1And2InTensor3UsingTiles.</li>\n<li>Acknowledge in any ways we see fit the contribution of <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a>.</li>\n<li>Unfortunately, if the reasons I listed above are factually correct, I think the logical thing to do is to base everything in <code>conv_ops_gpu_3.cu.cc</code> on this PR and acknowledge <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> 's contribution by applying his suggestions here and make explicit attributions in comment.</li>\n<li><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=1002405\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/yzhwang\">@yzhwang</a> seems to have a more comprehensive test suite that should be used instead of mine.</li>\n<li>I hope there could be a mechanism for people inside and outside Google to know what each other is working on... at least for the open source part of Tensorflow.</li>\n</ul>\n<p><strong>Most importantly</strong>, I hope that this conflict can be resolved in a way that pays acknowledgement to every one involved in a fair way. It is genuinely awful see anyone's effort made in vain and I hope we can work towards preventing that from happening.</p>", "body_text": "Ping @jlebar , @yzhwang,\nUnfortunately I think we've run into a conflict. By examining the code @yzhwang submitted, I think we have very similar ideas but the implementation @jlebar and I worked out here forms a super set of what @yzhwang has in his implementation.\nedit: I take the above statement back. @yzhwang seems to have batched sub-tiles within a single kernel launch. It is useful to learn the motivation behind such design.\nNotably from his comment:\n// Each thread block operates on a single rectangle tile, where its width is\n// kTileLength (we currently set it to 64) and its height is small_dim,\n// We set the thread block's X dimension to be tile_num_per_block, and its Y\n// and Z to be one.\n\nand\n// When only one of the dimensions is smaller than kMinDimensionToUseTiles,\n// we use one block to process a rectangle region with the size of\n// kTileLength * small_dim. We found that when set kTileLength to 64 on\n// TitanX Maxwell GPU, it achieves the best performance.\n\nA few things I would like to point out:\n\nthis PR specializes the kernel launch parameters for each type.\nthis PR also does not have a fixed long side len (in fact we have choices of {32, 64, 128...1024}).\nthis PR also has a very nicely designed kernel dispatcher (thanks to @jlebar ) that works to select among multiple pre-compiled kernels.\nthis PR has dynamically determined share memory sizes and therefore will be more shared-memory-efficient.\n@yzhwang seems to have written another kernel but I simply generalized the SwapDimension1And2InTensor3UsingTiles kernel so it seems like my approach produces less maintenance overhead.\nSimilarly, can you explain why do you need two __syncthreads()? I think you are operating on a 1D sliding window in a tile to transpose the matrix. I suspect that this could hurt parallelism; also to achieve performance portability, I think we should make this into multiple blocks and let the hardware scheduler make the decision of how to schedule sub-tiles.\nthis PR performs exhaustive experiments on 2 server-grade GPUs including P100 and K40 and thus the performance results will be more reliable and trust-worthy for performance-critical scenarios.\n\nSo I think if I get these facts right, I would like to propose the following to resolve the conflicts:\n\nAbsorb suggestions from @yzhwang. Maybe there are reasons for having another kernel SwapDimension1And2InTensor3UsingTiles.\nAcknowledge in any ways we see fit the contribution of @yzhwang.\nUnfortunately, if the reasons I listed above are factually correct, I think the logical thing to do is to base everything in conv_ops_gpu_3.cu.cc on this PR and acknowledge @yzhwang 's contribution by applying his suggestions here and make explicit attributions in comment.\n@yzhwang seems to have a more comprehensive test suite that should be used instead of mine.\nI hope there could be a mechanism for people inside and outside Google to know what each other is working on... at least for the open source part of Tensorflow.\n\nMost importantly, I hope that this conflict can be resolved in a way that pays acknowledgement to every one involved in a fair way. It is genuinely awful see anyone's effort made in vain and I hope we can work towards preventing that from happening.", "body": "Ping @jlebar , @yzhwang,\r\nUnfortunately I think we've run into a conflict. By examining the code @yzhwang submitted, I think we have very similar ideas but the implementation @jlebar and I worked out here forms a super set of what @yzhwang has in his implementation. \r\n\r\nedit: I take the above statement back. @yzhwang seems to have batched sub-tiles within a single kernel launch. It is useful to learn the motivation behind such design.\r\n\r\nNotably from his comment:\r\n```\r\n// Each thread block operates on a single rectangle tile, where its width is\r\n// kTileLength (we currently set it to 64) and its height is small_dim,\r\n// We set the thread block's X dimension to be tile_num_per_block, and its Y\r\n// and Z to be one.\r\n```\r\nand \r\n```\r\n// When only one of the dimensions is smaller than kMinDimensionToUseTiles,\r\n// we use one block to process a rectangle region with the size of\r\n// kTileLength * small_dim. We found that when set kTileLength to 64 on\r\n// TitanX Maxwell GPU, it achieves the best performance.\r\n```\r\nA few things I would like to point out:\r\n- this PR specializes the kernel launch parameters for each type.\r\n- this PR also does not have a fixed long side len (in fact we have choices of {32, 64, 128...1024}).\r\n- this PR also has a very nicely designed kernel dispatcher (thanks to @jlebar ) that works to select among multiple pre-compiled kernels.\r\n- this PR has dynamically determined share memory sizes and therefore will be more shared-memory-efficient.\r\n- @yzhwang seems to have written another kernel but I simply generalized the SwapDimension1And2InTensor3UsingTiles kernel so it seems like my approach produces less maintenance overhead.\r\n- Similarly, can you explain why do you need two `__syncthreads()`? I think you are operating on a 1D sliding window in a tile to transpose the matrix. I suspect that this could hurt parallelism; also to achieve performance portability, I think we should make this into multiple blocks and let the hardware scheduler make the decision of how to schedule sub-tiles.\r\n- this PR performs exhaustive experiments on 2 server-grade GPUs including P100 and K40 and thus the performance results will be more reliable and trust-worthy for performance-critical scenarios.\r\n\r\nSo I think if I get these facts right, I would like to propose the following to resolve the conflicts:\r\n- Absorb suggestions from @yzhwang. Maybe there are reasons for having another kernel SwapDimension1And2InTensor3UsingTiles.\r\n- Acknowledge in any ways we see fit the contribution of @yzhwang.\r\n- Unfortunately, if the reasons I listed above are factually correct, I think the logical thing to do is to base everything in `conv_ops_gpu_3.cu.cc` on this PR and acknowledge @yzhwang 's contribution by applying his suggestions here and make explicit attributions in comment. \r\n- @yzhwang seems to have a more comprehensive test suite that should be used instead of mine.\r\n- I hope there could be a mechanism for people inside and outside Google to know what each other is working on... at least for the open source part of Tensorflow.\r\n\r\n**Most importantly**, I hope that this conflict can be resolved in a way that pays acknowledgement to every one involved in a fair way. It is genuinely awful see anyone's effort made in vain and I hope we can work towards preventing that from happening."}