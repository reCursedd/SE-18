{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/142577882", "pull_request_review_id": 66893436, "id": 142577882, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MjU3Nzg4Mg==", "diff_hunk": "@@ -413,6 +429,275 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// TPF stands for Tile size Posibility Frontier. It denotes the tile size\n+// combinations that consume the most computational resources constrained by\n+// number of threads per SM limit, shared memory limit and overall degree of\n+// parallelism.\n+// Tile size combinations lying on this frontier would achieve the maximum\n+// utilization of available resources and combinations lying outside this\n+// frontier is not performance-wise profitable or simply not possible.\n+#define TPF_CHECKER(OP)                                                   \\\n+  size_of_t == 16 &&    (TileLongSide == 32   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 64   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 128  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 2)) || \\\n+      size_of_t == 8 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 8)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 512  && (TileShortSide OP 2)) || \\\n+      size_of_t == 4 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 2 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 1 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2))\n+\n+constexpr bool TileSizePossibilityFrontierCheck(int TileLongSide,\n+                                                int TileShortSide,\n+                                                int size_of_t, int mode) {\n+  // When mode is 0, this function checks whether the tile size combination lies\n+  // on the tile size possibility frontier. When mode is 1, it checks whether\n+  // the combination lies outside the frontier.\n+  return mode == 0 ? TPF_CHECKER(==) : TPF_CHECKER(>);\n+}\n+\n+#undef TPF_CHECKER\n+\n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide, typename dummy = void>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+    bool request_satisfied = max(tile_size_i, tile_size_j) <= TileLongSide &&\n+                             min(tile_size_i, tile_size_j) <= TileShortSide;\n+\n+    if (request_satisfied) {\n+      const int NumThread = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileLongSide,\n+            TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileShortSide,\n+            TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      return;\n+    }\n+\n+    // Kernel is not launched, meaning the launch configuration is not\n+    // satisfied.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    // Increase launch parameters and try again.\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide * 2, TileShortSide>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide, TileShortSide + 1>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    }\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T), 0),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+\n+    const int NumThread = TileLongSide;\n+    if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileLongSide,\n+          TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileShortSide,\n+          TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    return;\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T),\n+                                /* mode = outside */ 1),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+    assert(false &&\n+           \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"\n+           \"launch configuration. \");\n+  }\n+};\n+\n+// A helper function to make RunSwapDimension1And2InTensor3 concise. This\n+// helper function looks at the data type and input matrix sizes and decides\n+// the thread numbers and tile sizes to use.\n+template <typename T>\n+void SwapDimension1And2InTensor3WithNarrowMatrices(\n+    const GPUDevice& d, const T* input, const Dimension<3>& input_dims,\n+    T* output, const int kMinDimensionToUseTiles) {\n+  // Define available tile sizes here for each size of data type supported:\n+  std::vector<std::pair<int, int>> tile_spec =\n+      []() -> std::vector<std::pair<int, int>> {\n+    switch (sizeof(T)) {\n+      case 1:\n+        return {{32, 15}, {64, 15}, {128, 15}, {256, 15}, {512, 4}, {1024, 2}};\n+      case 2:\n+        return {{32, 15}, {64, 15}, {128, 15}, {256, 15}, {512, 4}, {1024, 2}};\n+      case 4:\n+        return {{32, 15}, {64, 15}, {128, 15}, {256, 15}, {512, 4}, {1024, 2}};\n+      case 8:\n+        return {{32, 15}, {64, 15}, {128, 8}, {256, 4}, {512, 2}};\n+      case 16:\n+        return {{32, 4},  {64, 4},  {128, 4}, {256, 2}};\n+    }\n+    return {};\n+  }();\n+\n+  int tile_long_side_len = 0;\n+  int tile_short_side_len = 0;\n+  float lowest_cost = std::numeric_limits<float>::max();\n+  int data_long_side = max(input_dims[1], input_dims[2]);\n+\n+  for (auto tile_size_pair : tile_spec) {\n+    int proposed_tile_long_side_len = tile_size_pair.first;\n+\n+    // Threads that will not be doing anything useful when reading the matrix\n+    // because the thread block size is bigger than the data block size.\n+    int wasted_threads = (data_long_side -\n+                          MathUtil::FloorOfRatio<int>(\n+                              data_long_side, proposed_tile_long_side_len) *\n+                              proposed_tile_long_side_len);", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 362, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "372d4111b99467a9eb4b4866ae8b53ecba505512", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "body": "Sorry I made the mistake of making some final changes after running clang-format.", "created_at": "2017-10-04T04:14:38Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r142577882", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/142577882"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r142577882"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>Sorry I made the mistake of making some final changes after running clang-format.</p>", "body_text": "Sorry I made the mistake of making some final changes after running clang-format.", "in_reply_to_id": 141919128}