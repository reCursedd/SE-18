{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141926643", "pull_request_review_id": 66213548, "id": 141926643, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTkyNjY0Mw==", "diff_hunk": "@@ -413,6 +429,275 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// TPF stands for Tile size Posibility Frontier. It denotes the tile size\n+// combinations that consume the most computational resources constrained by\n+// number of threads per SM limit, shared memory limit and overall degree of\n+// parallelism.\n+// Tile size combinations lying on this frontier would achieve the maximum\n+// utilization of available resources and combinations lying outside this\n+// frontier is not performance-wise profitable or simply not possible.\n+#define TPF_CHECKER(OP)                                                   \\\n+  size_of_t == 16 &&    (TileLongSide == 32   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 64   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 128  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 2)) || \\\n+      size_of_t == 8 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 8)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 512  && (TileShortSide OP 2)) || \\\n+      size_of_t == 4 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 2 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 1 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2))\n+\n+constexpr bool TileSizePossibilityFrontierCheck(int TileLongSide,\n+                                                int TileShortSide,\n+                                                int size_of_t, int mode) {\n+  // When mode is 0, this function checks whether the tile size combination lies\n+  // on the tile size possibility frontier. When mode is 1, it checks whether\n+  // the combination lies outside the frontier.\n+  return mode == 0 ? TPF_CHECKER(==) : TPF_CHECKER(>);\n+}\n+\n+#undef TPF_CHECKER\n+\n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide, typename dummy = void>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+    bool request_satisfied = max(tile_size_i, tile_size_j) <= TileLongSide &&\n+                             min(tile_size_i, tile_size_j) <= TileShortSide;\n+\n+    if (request_satisfied) {\n+      const int NumThread = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileLongSide,\n+            TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileShortSide,\n+            TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      return;\n+    }\n+\n+    // Kernel is not launched, meaning the launch configuration is not\n+    // satisfied.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    // Increase launch parameters and try again.\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide * 2, TileShortSide>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide, TileShortSide + 1>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    }\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T), 0),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+\n+    const int NumThread = TileLongSide;\n+    if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileLongSide,\n+          TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileShortSide,\n+          TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    return;\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T),\n+                                /* mode = outside */ 1),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+    assert(false &&\n+           \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"\n+           \"launch configuration. \");\n+  }\n+};\n+\n+// A helper function to make RunSwapDimension1And2InTensor3 concise. This\n+// helper function looks at the data type and input matrix sizes and decides\n+// the thread numbers and tile sizes to use.\n+template <typename T>\n+void SwapDimension1And2InTensor3WithNarrowMatrices(\n+    const GPUDevice& d, const T* input, const Dimension<3>& input_dims,\n+    T* output, const int kMinDimensionToUseTiles) {\n+  // Define available tile sizes here for each size of data type supported:", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 331, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "372d4111b99467a9eb4b4866ae8b53ecba505512", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "If this needs to be kept in sync with something else, we should probably say so...\r\n\r\nIn fact it would be really nice if we could autogenerate it from the frontier function?  It seems like this might actually be feasible, since the search space is small.  One only needs to compute it once; you can cache the result in a function-local static variable.\r\n\r\nThe trick is that the function-local static variable must be a POD, according to our style rules.  So a vector is not OK, but a vector* is fine.  For example:\r\n\r\n```\r\n// (Some meaningful comment...)\r\ntemplate <int SizeOfT>\r\nconst std::vector<std::pair<int, int>>& GetTileSizes() {\r\n  static_assert(SizeOfT <= 16);\r\n  static auto* sizes = [] {\r\n    auto* sizes = new std::vector<pair<int, int>>();\r\n    // Expensive work to populate sizes, lazily run in a thread-safe manner the first time GetTileSizes<N> is called.\r\n    return sizes;\r\n  }();\r\n  return *sizes;\r\n}\r\n```", "created_at": "2017-09-29T17:45:15Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141926643", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141926643"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141926643"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>If this needs to be kept in sync with something else, we should probably say so...</p>\n<p>In fact it would be really nice if we could autogenerate it from the frontier function?  It seems like this might actually be feasible, since the search space is small.  One only needs to compute it once; you can cache the result in a function-local static variable.</p>\n<p>The trick is that the function-local static variable must be a POD, according to our style rules.  So a vector is not OK, but a vector* is fine.  For example:</p>\n<pre><code>// (Some meaningful comment...)\ntemplate &lt;int SizeOfT&gt;\nconst std::vector&lt;std::pair&lt;int, int&gt;&gt;&amp; GetTileSizes() {\n  static_assert(SizeOfT &lt;= 16);\n  static auto* sizes = [] {\n    auto* sizes = new std::vector&lt;pair&lt;int, int&gt;&gt;();\n    // Expensive work to populate sizes, lazily run in a thread-safe manner the first time GetTileSizes&lt;N&gt; is called.\n    return sizes;\n  }();\n  return *sizes;\n}\n</code></pre>", "body_text": "If this needs to be kept in sync with something else, we should probably say so...\nIn fact it would be really nice if we could autogenerate it from the frontier function?  It seems like this might actually be feasible, since the search space is small.  One only needs to compute it once; you can cache the result in a function-local static variable.\nThe trick is that the function-local static variable must be a POD, according to our style rules.  So a vector is not OK, but a vector* is fine.  For example:\n// (Some meaningful comment...)\ntemplate <int SizeOfT>\nconst std::vector<std::pair<int, int>>& GetTileSizes() {\n  static_assert(SizeOfT <= 16);\n  static auto* sizes = [] {\n    auto* sizes = new std::vector<pair<int, int>>();\n    // Expensive work to populate sizes, lazily run in a thread-safe manner the first time GetTileSizes<N> is called.\n    return sizes;\n  }();\n  return *sizes;\n}"}