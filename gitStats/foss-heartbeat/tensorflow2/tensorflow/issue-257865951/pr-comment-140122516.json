{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140122516", "pull_request_review_id": 64150115, "id": 140122516, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDEyMjUxNg==", "diff_hunk": "@@ -413,6 +432,160 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoBatchNarrowMatrixTranspose(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+    bool request_satisfied = (max(tile_size_i, tile_size_j) <= TileLongSide) &&\n+                             (min(tile_size_i, tile_size_j) <= TileShortSide);\n+\n+    if (request_satisfied) {\n+      const int ThreadNum = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide)\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, ThreadNum, TileLongSide,\n+            TileShortSide><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide)\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, ThreadNum, TileShortSide,\n+            TileLongSide><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      return;\n+    }\n+\n+    // Kernel is not launched, meaning the launch configuration is not\n+    // satisfied.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    // Increase launch parameters and try again.\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide * 2, TileShortSide>::\n+          DoBatchNarrowMatrixTranspose(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide, TileShortSide + 1>::\n+          DoBatchNarrowMatrixTranspose(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    }\n+  }\n+};\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, LONG_SIDE,           \\\n+                                                    SHORT_SIDE)                \\\n+  template <int TileSizeI>                                                     \\\n+  struct BatchNarrowMatrixTransposeDispatcher<TYPE, TileSizeI, SHORT_SIDE> {   \\\n+    static void DoBatchNarrowMatrixTranspose(const GPUDevice& d,               \\\n+                                             int tile_size_i, int tile_size_j, \\\n+                                             int total_tiles_count,            \\\n+                                             const TYPE* input,                \\\n+                                             const Dimension<3>& input_dims,   \\\n+                                             TYPE* output) {                   \\\n+      assert(                                                                  \\\n+          false &&                                                             \\\n+          \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"  \\\n+          \"launch configuration. \");                                           \\\n+    }                                                                          \\\n+  };                                                                           \\\n+  template <int TileSizeJ>                                                     \\\n+  struct BatchNarrowMatrixTransposeDispatcher<TYPE, LONG_SIDE, TileSizeJ> {    \\\n+    static void DoBatchNarrowMatrixTranspose(const GPUDevice& d,               \\\n+                                             int tile_size_i, int tile_size_j, \\\n+                                             int total_tiles_count,            \\\n+                                             const TYPE* input,                \\\n+                                             const Dimension<3>& input_dims,   \\\n+                                             TYPE* output) {                   \\\n+      assert(                                                                  \\\n+          false &&                                                             \\\n+          \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"  \\\n+          \"launch configuration. \");                                           \\\n+    }                                                                          \\\n+  };\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, LONG_SIDE, SHORT_SIDE)       \\\n+  template <>                                                                  \\\n+  struct BatchNarrowMatrixTransposeDispatcher<TYPE, LONG_SIDE, SHORT_SIDE> {   \\\n+    static void DoBatchNarrowMatrixTranspose(const GPUDevice& d,               \\\n+                                             int tile_size_i, int tile_size_j, \\\n+                                             int total_tiles_count,            \\\n+                                             const TYPE* input,                \\\n+                                             const Dimension<3>& input_dims,   \\\n+                                             TYPE* output) {                   \\\n+      const int ThreadNum = LONG_SIDE;                                         \\\n+      if (tile_size_i <= LONG_SIDE && tile_size_j <= SHORT_SIDE)               \\\n+        SwapDimension1And2InTensor3UsingTiles<                                 \\\n+            TYPE, ThreadNum, LONG_SIDE,                                        \\\n+            SHORT_SIDE><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(      \\\n+            input, input_dims, output);                                        \\\n+      else if (tile_size_j <= LONG_SIDE && tile_size_i <= SHORT_SIDE)          \\\n+        SwapDimension1And2InTensor3UsingTiles<                                 \\\n+            TYPE, ThreadNum, SHORT_SIDE,                                       \\\n+            LONG_SIDE><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(       \\\n+            input, input_dims, output);                                        \\\n+      return;                                                                  \\\n+    }                                                                          \\\n+  };\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_128(TYPE)               \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, 256, 16); \\", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 275, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "4c36dee945836ac2b83f058ee6107d7cc876d484", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "body": "Hi, I think the point is to prevent tile size combinations like (256, 3) or (256, 4) to materialize as legitimate launch parameters. The compiler may not be aware that I always try doubling the first param (long side length) before incrementing the second (short side length) so it may attempt to jump from tile size combinations like (128, 3) to (256, 3) which may never occur at runtime and compile a dispatcher that launches tile size combinations like (256, 3) which would be a nuisance because this breaks the shared memory limits and may result in a compile time error.\r\n\r\nAlso, I'm not sure if this is part of your struggle: by template partial ordering, kernel<256, 2> is considered more specialized than kernel<256, anything> thus the implementation of kernel<256, 2> will override that of kernel<256, anything> thus a dispatcher for tile size combinations <256, 2> will materialize.", "created_at": "2017-09-21T00:05:37Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140122516", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140122516"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140122516"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>Hi, I think the point is to prevent tile size combinations like (256, 3) or (256, 4) to materialize as legitimate launch parameters. The compiler may not be aware that I always try doubling the first param (long side length) before incrementing the second (short side length) so it may attempt to jump from tile size combinations like (128, 3) to (256, 3) which may never occur at runtime and compile a dispatcher that launches tile size combinations like (256, 3) which would be a nuisance because this breaks the shared memory limits and may result in a compile time error.</p>\n<p>Also, I'm not sure if this is part of your struggle: by template partial ordering, kernel&lt;256, 2&gt; is considered more specialized than kernel&lt;256, anything&gt; thus the implementation of kernel&lt;256, 2&gt; will override that of kernel&lt;256, anything&gt; thus a dispatcher for tile size combinations &lt;256, 2&gt; will materialize.</p>", "body_text": "Hi, I think the point is to prevent tile size combinations like (256, 3) or (256, 4) to materialize as legitimate launch parameters. The compiler may not be aware that I always try doubling the first param (long side length) before incrementing the second (short side length) so it may attempt to jump from tile size combinations like (128, 3) to (256, 3) which may never occur at runtime and compile a dispatcher that launches tile size combinations like (256, 3) which would be a nuisance because this breaks the shared memory limits and may result in a compile time error.\nAlso, I'm not sure if this is part of your struggle: by template partial ordering, kernel<256, 2> is considered more specialized than kernel<256, anything> thus the implementation of kernel<256, 2> will override that of kernel<256, anything> thus a dispatcher for tile size combinations <256, 2> will materialize.", "in_reply_to_id": 140108897}