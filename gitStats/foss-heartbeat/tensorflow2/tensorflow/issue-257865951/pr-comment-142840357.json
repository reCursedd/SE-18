{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/142840357", "pull_request_review_id": 67260553, "id": 142840357, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0Mjg0MDM1Nw==", "diff_hunk": "@@ -413,6 +429,313 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// We want std::equal_to and std::greater, but they're not constexpr until\n+// C++14.\n+struct EqualTo {\n+  constexpr bool operator()(int a, int b) const { return a == b; }\n+};\n+\n+struct GreaterThan {\n+  constexpr bool operator()(int a, int b) const { return a > b; }\n+};\n+\n+// Tile size posibility frontier denotes the tile size combinations that consume\n+// the most computational resources constrained by\n+// - number of threads per SM limit,\n+// - shared memory limit and\n+// - some experimentally determined, type-specific constraint on the product of\n+// two side lengths to increase grid-level parallelism.\n+//\n+// Tile size combinations lying on this frontier would achieve the maximum\n+// utilization of available resources, and combinations lying outside this\n+// frontier are either not possible, or are slower than the alternatives.\n+\n+template <typename Op>\n+constexpr bool TileSizePossibilityFrontierCheck(int TileLongSide,\n+                                                int TileShortSide,\n+                                                int size_of_t, Op op) {\n+  // clang-format off\n+  return size_of_t == 16 && (TileLongSide == 32   && op(TileShortSide, 4)  ||\n+                             TileLongSide == 64   && op(TileShortSide, 4)  ||\n+                             TileLongSide == 128  && op(TileShortSide, 4)  ||\n+                             TileLongSide == 256  && op(TileShortSide, 2)) ||\n+          size_of_t == 8 && (TileLongSide == 32   && op(TileShortSide, 15) ||\n+                             TileLongSide == 64   && op(TileShortSide, 15) ||\n+                             TileLongSide == 128  && op(TileShortSide, 8)  ||\n+                             TileLongSide == 256  && op(TileShortSide, 4)  ||\n+                             TileLongSide == 512  && op(TileShortSide, 2)) ||\n+          size_of_t == 4 && (TileLongSide == 32   && op(TileShortSide, 15) ||\n+                             TileLongSide == 64   && op(TileShortSide, 15) ||\n+                             TileLongSide == 128  && op(TileShortSide, 15) ||\n+                             TileLongSide == 256  && op(TileShortSide, 8)  ||\n+                             TileLongSide == 512  && op(TileShortSide, 4)  ||\n+                             TileLongSide == 1024 && op(TileShortSide, 2)) ||\n+          size_of_t == 2 && (TileLongSide == 32   && op(TileShortSide, 15) ||\n+                             TileLongSide == 64   && op(TileShortSide, 15) ||\n+                             TileLongSide == 128  && op(TileShortSide, 15) ||\n+                             TileLongSide == 256  && op(TileShortSide, 8)  ||\n+                             TileLongSide == 512  && op(TileShortSide, 4)  ||\n+                             TileLongSide == 1024 && op(TileShortSide, 2)) ||\n+          size_of_t == 1 && (TileLongSide == 32   && op(TileShortSide, 15) ||\n+                             TileLongSide == 64   && op(TileShortSide, 15) ||\n+                             TileLongSide == 128  && op(TileShortSide, 15) ||\n+                             TileLongSide == 256  && op(TileShortSide, 8)  ||\n+                             TileLongSide == 512  && op(TileShortSide, 4)  ||\n+                             TileLongSide == 1024 && op(TileShortSide, 2));\n+  // clang-format on\n+}\n+\n+constexpr bool TileSizeOnFrontier(int TileLongSide, int TileShortSide,\n+                                  int size_of_t) {\n+  return TileSizePossibilityFrontierCheck(TileLongSide, TileShortSide,\n+                                          size_of_t, EqualTo());\n+}\n+constexpr bool TileSizePastFrontier(int TileLongSide, int TileShortSide,\n+                                    int size_of_t) {\n+  return TileSizePossibilityFrontierCheck(TileLongSide, TileShortSide,\n+                                          size_of_t, GreaterThan());\n+}\n+\n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide,\n+          typename dummy = void>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoIt(const GPUDevice& d, int tile_size_i, int tile_size_j,\n+                   int total_tiles_count, const T* input,\n+                   const Dimension<3>& input_dims, T* output) {\n+    static_assert(\n+        (TileLongSide & (TileLongSide - 1)) == 0,\n+        \"The length of the longer side of the tile is always a power of 2.\");\n+    bool request_satisfied = max(tile_size_i, tile_size_j) <= TileLongSide &&\n+                             min(tile_size_i, tile_size_j) <= TileShortSide;\n+\n+    if (request_satisfied) {\n+      constexpr int NumThreads = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<T, NumThreads, TileLongSide,\n+                                              TileShortSide>\n+            <<<total_tiles_count, NumThreads, 0, d.stream()>>>(\n+                input, input_dims, output);\n+      } else {\n+        SwapDimension1And2InTensor3UsingTiles<T, NumThreads, TileShortSide,\n+                                              TileLongSide>\n+            <<<total_tiles_count, NumThreads, 0, d.stream()>>>(\n+                input, input_dims, output);\n+      }\n+    }\n+\n+    // If the execution reaches here, then the kernel was not launched; we then\n+    // determine whether it is the long side or the short side that falls short\n+    // of the request and increase that parameter accordingly.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<\n+          T, TileLongSide * 2, TileShortSide>::DoIt(d, tile_size_i, tile_size_j,\n+                                                    total_tiles_count, input,\n+                                                    input_dims, output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<\n+          T, TileLongSide, TileShortSide + 1>::DoIt(d, tile_size_i, tile_size_j,\n+                                                    total_tiles_count, input,\n+                                                    input_dims, output);\n+    }\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizeOnFrontier(TileLongSide, TileShortSide,\n+                                               sizeof(T)),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i, int tile_size_j,\n+                   int total_tiles_count, const T* input,\n+                   const Dimension<3>& input_dims, T* output) {\n+    static_assert(\n+        (TileLongSide & (TileLongSide - 1)) == 0,\n+        \"The length of the longer side of the tile is always a power of 2.\");\n+\n+    constexpr int NumThreads = TileLongSide;\n+    if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide) {\n+      SwapDimension1And2InTensor3UsingTiles<T, NumThreads, TileLongSide,\n+                                            TileShortSide>\n+          <<<total_tiles_count, NumThreads, 0, d.stream()>>>(input, input_dims,\n+                                                             output);\n+    } else {\n+      SwapDimension1And2InTensor3UsingTiles<T, NumThreads, TileShortSide,\n+                                            TileLongSide>\n+          <<<total_tiles_count, NumThreads, 0, d.stream()>>>(input, input_dims,\n+                                                             output);\n+    }\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePastFrontier(TileLongSide, TileShortSide,\n+                                                 sizeof(T)),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i, int tile_size_j,\n+                   int total_tiles_count, const T* input,\n+                   const Dimension<3>& input_dims, T* output) {\n+    assert(false &&\n+           \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"\n+           \"launch configuration. \");\n+  }\n+};\n+\n+// This function tries to recover, in a brute force way, the frontier defined in\n+// TileSizePossibilityFrontierCheck as a vector of tile size combinations.\n+template <int SizeOfT>\n+const std::vector<std::pair<int, int>>& GetTileSizesFrontier() {\n+  static_assert(\n+      SizeOfT <= 16,\n+      \"Currently, only data types of sizes 16 bytes or less are supported.\");\n+  static_assert((SizeOfT & (SizeOfT - 1)) == 0,\n+                \"Data types must have sizes that are powers of 2.\");\n+\n+  // Expensive work to populate sizes, lazily run in a thread-safe\n+  // manner the first time GetTileSizesFrontier<N> is called.\n+  static auto* frontier = [] {\n+    auto* frontier = new std::vector<std::pair<int, int>>();\n+    const int kMaxLongSideLen = 1024;\n+    const int kMaxShortSideLen = 15;\n+    for (int long_side = 32; long_side <= kMaxLongSideLen; long_side *= 2) {\n+      for (int short_side = 2; short_side <= kMaxShortSideLen;\n+           short_side += 1) {\n+        if (TileSizeOnFrontier(long_side, short_side, SizeOfT)) {\n+          // The current combination lies on the frontier, thus we\n+          // add it to the frontier definition.\n+          frontier->push_back(std::make_pair(long_side, short_side));\n+\n+          // The long side length is the largest one allowed iff its\n+          // corresponding short side length is 2.\n+          if (short_side == 2) return frontier;\n+\n+          // We have exhausted all the possibilities in the frontier\n+          // with the given long side length.\n+          break;\n+        }\n+      }\n+    }\n+\n+    assert(false &&\n+           \"The corresponding short side length of the largest long side \"\n+           \"length has to be 2.\");\n+    return frontier;\n+  }();\n+  return *frontier;\n+}\n+\n+template <int ElemBytes>\n+struct TransposeElemType;\n+template <>\n+struct TransposeElemType<1> {\n+  using type = uint8;\n+};\n+template <>\n+struct TransposeElemType<2> {\n+  using type = uint16;\n+};\n+template <>\n+struct TransposeElemType<4> {\n+  using type = uint32;\n+};\n+template <>\n+struct TransposeElemType<8> {\n+  using type = uint64;\n+};\n+template <>\n+struct TransposeElemType<16> {\n+  using type = float4;\n+};\n+\n+// A helper function to make RunSwapDimension1And2InTensor3 concise. This\n+// helper function looks at the data type and input matrix sizes and decides\n+// the thread numbers and tile sizes to use.\n+template <typename T>\n+void SwapDimension1And2InTensor3WithNarrowMatrices(\n+    const GPUDevice& d, const T* input, const Dimension<3>& input_dims,\n+    T* output, const int kMinDimensionToUseTiles) {\n+  // Define available tile sizes here for each size of data type supported:\n+  auto tile_spec = GetTileSizesFrontier<sizeof(T)>();", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 407, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "f4a033d59e2478a5f3ea7597b9a5d55d030fa887", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "body": "You are right.", "created_at": "2017-10-05T03:15:23Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r142840357", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/142840357"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r142840357"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>You are right.</p>", "body_text": "You are right.", "in_reply_to_id": 142775025}