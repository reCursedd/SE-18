{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141924950", "pull_request_review_id": 66213548, "id": 141924950, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTkyNDk1MA==", "diff_hunk": "@@ -413,6 +429,275 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// TPF stands for Tile size Posibility Frontier. It denotes the tile size\n+// combinations that consume the most computational resources constrained by\n+// number of threads per SM limit, shared memory limit and overall degree of\n+// parallelism.\n+// Tile size combinations lying on this frontier would achieve the maximum\n+// utilization of available resources and combinations lying outside this\n+// frontier is not performance-wise profitable or simply not possible.\n+#define TPF_CHECKER(OP)                                                   \\\n+  size_of_t == 16 &&    (TileLongSide == 32   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 64   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 128  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 2)) || \\\n+      size_of_t == 8 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 8)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 512  && (TileShortSide OP 2)) || \\\n+      size_of_t == 4 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 2 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 1 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2))\n+\n+constexpr bool TileSizePossibilityFrontierCheck(int TileLongSide,\n+                                                int TileShortSide,\n+                                                int size_of_t, int mode) {\n+  // When mode is 0, this function checks whether the tile size combination lies\n+  // on the tile size possibility frontier. When mode is 1, it checks whether\n+  // the combination lies outside the frontier.\n+  return mode == 0 ? TPF_CHECKER(==) : TPF_CHECKER(>);\n+}\n+\n+#undef TPF_CHECKER\n+\n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide, typename dummy = void>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+    bool request_satisfied = max(tile_size_i, tile_size_j) <= TileLongSide &&\n+                             min(tile_size_i, tile_size_j) <= TileShortSide;\n+\n+    if (request_satisfied) {\n+      const int NumThread = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileLongSide,\n+            TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileShortSide,\n+            TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      return;\n+    }\n+\n+    // Kernel is not launched, meaning the launch configuration is not\n+    // satisfied.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    // Increase launch parameters and try again.\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide * 2, TileShortSide>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide, TileShortSide + 1>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    }\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T), 0),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+\n+    const int NumThread = TileLongSide;\n+    if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileLongSide,\n+          TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileShortSide,\n+          TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    return;\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T),\n+                                /* mode = outside */ 1),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+    assert(false &&\n+           \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"\n+           \"launch configuration. \");\n+  }\n+};\n+\n+// A helper function to make RunSwapDimension1And2InTensor3 concise. This\n+// helper function looks at the data type and input matrix sizes and decides\n+// the thread numbers and tile sizes to use.\n+template <typename T>\n+void SwapDimension1And2InTensor3WithNarrowMatrices(\n+    const GPUDevice& d, const T* input, const Dimension<3>& input_dims,\n+    T* output, const int kMinDimensionToUseTiles) {\n+  // Define available tile sizes here for each size of data type supported:\n+  std::vector<std::pair<int, int>> tile_spec =", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 332, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "372d4111b99467a9eb4b4866ae8b53ecba505512", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "nit: `auto tile_spec`, since we can see the type of the lambda.", "created_at": "2017-09-29T17:36:28Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141924950", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141924950"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141924950"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>nit: <code>auto tile_spec</code>, since we can see the type of the lambda.</p>", "body_text": "nit: auto tile_spec, since we can see the type of the lambda."}