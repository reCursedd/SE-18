{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049/events", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049", "id": 257865951, "node_id": "MDExOlB1bGxSZXF1ZXN0MTQxMTg0NDMy", "number": 13049, "title": "Optimize batch matrix transposition for narrow matrices.", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 419840263, "node_id": "MDU6TGFiZWw0MTk4NDAyNjM=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/awaiting%20testing%20(then%20merge)", "name": "awaiting testing (then merge)", "color": "c2e0c6", "default": false}, {"id": 300136587, "node_id": "MDU6TGFiZWwzMDAxMzY1ODc=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/cla:%20yes", "name": "cla: yes", "color": "009800", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 40, "created_at": "2017-09-14T21:44:18Z", "updated_at": "2017-12-31T06:39:34Z", "closed_at": "2017-12-31T06:39:34Z", "author_association": "CONTRIBUTOR", "pull_request": {"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049", "diff_url": "https://github.com/tensorflow/tensorflow/pull/13049.diff", "patch_url": "https://github.com/tensorflow/tensorflow/pull/13049.patch"}, "body_html": "<p>This is an improved version of matrix transposition that specializes in the case when the matrices to be transposed are narrow. This is a preliminary implementation and I hope to get some feedback before further optimizing this implementation as it would likely entail great deal of efforts and patience.</p>\n<ul>\n<li>This implementation modifies the general matrix transposition kernel in a minimal way. Essentially this implementation enables the rectangular tile shapes to be used for transposition. Before, only square tiles are allowed.</li>\n<li>This implementation has a couple of specialized tile sizes and a very simple cost function is used to select among them during runtime. The logic behind this cost function is simple and will likely to be effective on a variety of platforms.</li>\n<li>We tested this implementation on problem sizes of {32, 64, 128, 256, 512, 1024} X range(16, 2048, 32) X range(2, 16) as well as {32, 64, 128, 256, 512, 1024} X range(2, 16)  X range(16, 2048, 32) where these problem size dimensions can be interpreted as batch_size, matrix_height, matrix_width correspondingly.  The average speedup is 16.7%. This experiment includes 10752 data points and is plotted using excel to indicate for what sub-space of all problem size space do we see speedups over baseline/existing implementation. See <a href=\"https://imgur.com/a/MyVOf\" rel=\"nofollow\">picture</a>. In the picture, problem sizes where we do see speedups are colored red and otherwise white. The three dimensional problem size space is collapsed to two by serializing the batch size dimension and matrix height dimension.</li>\n<li>As you can see from the current performance results, existing implementation is very good at dealing with large batches of tiny matrices. This is due to the fact that the baseline implementation uses a brute force (and nonetheless effective) way of dividing up workloads evenly which outperforms this commits. I may consider modify the cost function to use the baseline in these cases in the future.</li>\n<li>Again, this is only a preliminary implementation, I'm happy to make significant changes.</li>\n</ul>", "body_text": "This is an improved version of matrix transposition that specializes in the case when the matrices to be transposed are narrow. This is a preliminary implementation and I hope to get some feedback before further optimizing this implementation as it would likely entail great deal of efforts and patience.\n\nThis implementation modifies the general matrix transposition kernel in a minimal way. Essentially this implementation enables the rectangular tile shapes to be used for transposition. Before, only square tiles are allowed.\nThis implementation has a couple of specialized tile sizes and a very simple cost function is used to select among them during runtime. The logic behind this cost function is simple and will likely to be effective on a variety of platforms.\nWe tested this implementation on problem sizes of {32, 64, 128, 256, 512, 1024} X range(16, 2048, 32) X range(2, 16) as well as {32, 64, 128, 256, 512, 1024} X range(2, 16)  X range(16, 2048, 32) where these problem size dimensions can be interpreted as batch_size, matrix_height, matrix_width correspondingly.  The average speedup is 16.7%. This experiment includes 10752 data points and is plotted using excel to indicate for what sub-space of all problem size space do we see speedups over baseline/existing implementation. See picture. In the picture, problem sizes where we do see speedups are colored red and otherwise white. The three dimensional problem size space is collapsed to two by serializing the batch size dimension and matrix height dimension.\nAs you can see from the current performance results, existing implementation is very good at dealing with large batches of tiny matrices. This is due to the fact that the baseline implementation uses a brute force (and nonetheless effective) way of dividing up workloads evenly which outperforms this commits. I may consider modify the cost function to use the baseline in these cases in the future.\nAgain, this is only a preliminary implementation, I'm happy to make significant changes.", "body": "This is an improved version of matrix transposition that specializes in the case when the matrices to be transposed are narrow. This is a preliminary implementation and I hope to get some feedback before further optimizing this implementation as it would likely entail great deal of efforts and patience.\r\n- This implementation modifies the general matrix transposition kernel in a minimal way. Essentially this implementation enables the rectangular tile shapes to be used for transposition. Before, only square tiles are allowed.\r\n- This implementation has a couple of specialized tile sizes and a very simple cost function is used to select among them during runtime. The logic behind this cost function is simple and will likely to be effective on a variety of platforms.\r\n- We tested this implementation on problem sizes of {32, 64, 128, 256, 512, 1024} X range(16, 2048, 32) X range(2, 16) as well as {32, 64, 128, 256, 512, 1024} X range(2, 16)  X range(16, 2048, 32) where these problem size dimensions can be interpreted as batch_size, matrix_height, matrix_width correspondingly.  The average speedup is 16.7%. This experiment includes 10752 data points and is plotted using excel to indicate for what sub-space of all problem size space do we see speedups over baseline/existing implementation. See [picture](https://imgur.com/a/MyVOf). In the picture, problem sizes where we do see speedups are colored red and otherwise white. The three dimensional problem size space is collapsed to two by serializing the batch size dimension and matrix height dimension.\r\n- As you can see from the current performance results, existing implementation is very good at dealing with large batches of tiny matrices. This is due to the fact that the baseline implementation uses a brute force (and nonetheless effective) way of dividing up workloads evenly which outperforms this commits. I may consider modify the cost function to use the baseline in these cases in the future.\r\n- Again, this is only a preliminary implementation, I'm happy to make significant changes."}