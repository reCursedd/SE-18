{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/338724001", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#issuecomment-338724001", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049", "id": 338724001, "node_id": "MDEyOklzc3VlQ29tbWVudDMzODcyNDAwMQ==", "user": {"login": "yzhwang", "id": 1002405, "node_id": "MDQ6VXNlcjEwMDI0MDU=", "avatar_url": "https://avatars1.githubusercontent.com/u/1002405?v=4", "gravatar_id": "", "url": "https://api.github.com/users/yzhwang", "html_url": "https://github.com/yzhwang", "followers_url": "https://api.github.com/users/yzhwang/followers", "following_url": "https://api.github.com/users/yzhwang/following{/other_user}", "gists_url": "https://api.github.com/users/yzhwang/gists{/gist_id}", "starred_url": "https://api.github.com/users/yzhwang/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/yzhwang/subscriptions", "organizations_url": "https://api.github.com/users/yzhwang/orgs", "repos_url": "https://api.github.com/users/yzhwang/repos", "events_url": "https://api.github.com/users/yzhwang/events{/privacy}", "received_events_url": "https://api.github.com/users/yzhwang/received_events", "type": "User", "site_admin": false}, "created_at": "2017-10-23T16:52:38Z", "updated_at": "2017-10-23T16:52:38Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Hi <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=6410074\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/tjingrant\">@tjingrant</a> Sorry for the late reply. I spent some time understanding your implementation. I really like how it provides a unified version of doing transpose with tiling and shared memory. Thanks a lot for the work! After some internal discussion with my colleagues, we would like to integrate your implementation.</p>\n<p>So I did some benchmarks using: <a href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/transpose_benchmark.py\">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/transpose_benchmark.py</a> and we found that comparing with our current implementation, the performance of your implementation is not always better. For float64, float32, int8, and float16, they all have some input shapes that showed regression with your implementation. I did the benchmark on a GTX 1080 card. That being said, it is likely due to that you are not tuning the consts in TileSizePossibilityFrontierCheck() for that specific architecture. I will try to benchmark this on more architectures this week and let you know.</p>\n<p>Since checking in your code as it is would cause some performance regression, going forward, we should work together to make sure that this unified tiling transpose implementation could provide the best performance on every architecture (or most architectures) we care about. I hope you could keep working on this with us. So my plan is to first figure out a better tuning strategy so that it could work better on both Pascal and K40/K80. Then we can improve it further for Volta in another PR. I know it might be hard for you to work on this given the hardware constraint, I will try my best to assist you with the benchmarks. Right now, it would be really nice if you could share some of your thoughts on tuning for different architectures. Thank you!</p>", "body_text": "Hi @tjingrant Sorry for the late reply. I spent some time understanding your implementation. I really like how it provides a unified version of doing transpose with tiling and shared memory. Thanks a lot for the work! After some internal discussion with my colleagues, we would like to integrate your implementation.\nSo I did some benchmarks using: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/transpose_benchmark.py and we found that comparing with our current implementation, the performance of your implementation is not always better. For float64, float32, int8, and float16, they all have some input shapes that showed regression with your implementation. I did the benchmark on a GTX 1080 card. That being said, it is likely due to that you are not tuning the consts in TileSizePossibilityFrontierCheck() for that specific architecture. I will try to benchmark this on more architectures this week and let you know.\nSince checking in your code as it is would cause some performance regression, going forward, we should work together to make sure that this unified tiling transpose implementation could provide the best performance on every architecture (or most architectures) we care about. I hope you could keep working on this with us. So my plan is to first figure out a better tuning strategy so that it could work better on both Pascal and K40/K80. Then we can improve it further for Volta in another PR. I know it might be hard for you to work on this given the hardware constraint, I will try my best to assist you with the benchmarks. Right now, it would be really nice if you could share some of your thoughts on tuning for different architectures. Thank you!", "body": "Hi @tjingrant Sorry for the late reply. I spent some time understanding your implementation. I really like how it provides a unified version of doing transpose with tiling and shared memory. Thanks a lot for the work! After some internal discussion with my colleagues, we would like to integrate your implementation.\r\n\r\nSo I did some benchmarks using: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/transpose_benchmark.py and we found that comparing with our current implementation, the performance of your implementation is not always better. For float64, float32, int8, and float16, they all have some input shapes that showed regression with your implementation. I did the benchmark on a GTX 1080 card. That being said, it is likely due to that you are not tuning the consts in TileSizePossibilityFrontierCheck() for that specific architecture. I will try to benchmark this on more architectures this week and let you know.\r\n\r\nSince checking in your code as it is would cause some performance regression, going forward, we should work together to make sure that this unified tiling transpose implementation could provide the best performance on every architecture (or most architectures) we care about. I hope you could keep working on this with us. So my plan is to first figure out a better tuning strategy so that it could work better on both Pascal and K40/K80. Then we can improve it further for Volta in another PR. I know it might be hard for you to work on this given the hardware constraint, I will try my best to assist you with the benchmarks. Right now, it would be really nice if you could share some of your thoughts on tuning for different architectures. Thank you!"}