{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141919694", "pull_request_review_id": 66213548, "id": 141919694, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTkxOTY5NA==", "diff_hunk": "@@ -175,34 +177,31 @@ __global__ void SwapDimension1And2InTensor3Simple(int nthreads, const T* input,\n   }\n }\n \n+\n // Use shared memory tiles to swap dimension-1 and dimension-2 of a 3D tensor,\n // where dimensions are zero-based: output[i][j][k] = input[i][k][j].\n //\n-// Each thread block operates on a single tile, a square of dimensions TileSize\n-// x TileSize.  We require that the thread block's X dimension equals TileSize,\n-// and its Y dimension equals NumSubTiles.\n+// Each thread block operates on a single tile, a rectangle of dimensions\n+// TileSizeI x TileSizeJ.\n //\n-// For best performance, you should probably set TileSize equal to the number of\n-// threads in a warp (32 in nvidia GPUs).  With a TileSize of 32, NumSubTiles ==\n-// 4 or 8 seems to get the best performance on K40 GPUs.\n-template <typename T, int TileSize, int NumSubTiles>\n-__global__ void SwapDimension1And2InTensor3UsingTiles(const T* input,\n-                                                      Dimension<3> input_dims,\n-                                                      T* output) {\n-  // One extra line in the inner dimension to avoid share memory bank conflict.\n-  __shared__ T shared_memory_tile[TileSize][TileSize + 1];\n-\n-  static_assert(TileSize % NumSubTiles == 0,\n-                \"TileSize must be divisible by NumSubTiles\");\n-  eigen_assert(blockDim.x == TileSize);\n-  eigen_assert(blockDim.y == NumSubTiles);\n+// In general, for best performance, you should probably set TileSizeI,\n+// TileSizeJ equal to the number of threads in a warp (32 in nvidia GPUs).\n+// With a TileSizeI, TileSizeJ of 32, NumThread of 128 or 256 seems to get\n+// the best performance on K40 GPUs.\n+template <typename T, int NumThread, int TileSizeI, int TileSizeJ>", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 47, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "372d4111b99467a9eb4b4866ae8b53ecba505512", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "Nit, can we call it NumThreads?  `NumThread` sounds to me a lot like `ThreadNum`, which is a different thing.", "created_at": "2017-09-29T17:11:34Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141919694", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141919694"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141919694"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>Nit, can we call it NumThreads?  <code>NumThread</code> sounds to me a lot like <code>ThreadNum</code>, which is a different thing.</p>", "body_text": "Nit, can we call it NumThreads?  NumThread sounds to me a lot like ThreadNum, which is a different thing."}