{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140107128", "pull_request_review_id": 64123689, "id": 140107128, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDEwNzEyOA==", "diff_hunk": "@@ -413,6 +432,160 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoBatchNarrowMatrixTranspose(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+    bool request_satisfied = (max(tile_size_i, tile_size_j) <= TileLongSide) &&\n+                             (min(tile_size_i, tile_size_j) <= TileShortSide);\n+\n+    if (request_satisfied) {\n+      const int ThreadNum = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide)\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, ThreadNum, TileLongSide,\n+            TileShortSide><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide)\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, ThreadNum, TileShortSide,\n+            TileLongSide><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      return;\n+    }\n+\n+    // Kernel is not launched, meaning the launch configuration is not\n+    // satisfied.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    // Increase launch parameters and try again.\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide * 2, TileShortSide>::\n+          DoBatchNarrowMatrixTranspose(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide, TileShortSide + 1>::\n+          DoBatchNarrowMatrixTranspose(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    }\n+  }\n+};\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, LONG_SIDE,           \\\n+                                                    SHORT_SIDE)                \\\n+  template <int TileSizeI>                                                     \\\n+  struct BatchNarrowMatrixTransposeDispatcher<TYPE, TileSizeI, SHORT_SIDE> {   \\\n+    static void DoBatchNarrowMatrixTranspose(const GPUDevice& d,               \\\n+                                             int tile_size_i, int tile_size_j, \\\n+                                             int total_tiles_count,            \\\n+                                             const TYPE* input,                \\\n+                                             const Dimension<3>& input_dims,   \\\n+                                             TYPE* output) {                   \\\n+      assert(                                                                  \\\n+          false &&                                                             \\\n+          \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"  \\\n+          \"launch configuration. \");                                           \\\n+    }                                                                          \\\n+  };                                                                           \\\n+  template <int TileSizeJ>                                                     \\\n+  struct BatchNarrowMatrixTransposeDispatcher<TYPE, LONG_SIDE, TileSizeJ> {    \\\n+    static void DoBatchNarrowMatrixTranspose(const GPUDevice& d,               \\\n+                                             int tile_size_i, int tile_size_j, \\\n+                                             int total_tiles_count,            \\\n+                                             const TYPE* input,                \\\n+                                             const Dimension<3>& input_dims,   \\\n+                                             TYPE* output) {                   \\\n+      assert(                                                                  \\\n+          false &&                                                             \\\n+          \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"  \\\n+          \"launch configuration. \");                                           \\\n+    }                                                                          \\\n+  };\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, LONG_SIDE, SHORT_SIDE)       \\\n+  template <>                                                                  \\\n+  struct BatchNarrowMatrixTransposeDispatcher<TYPE, LONG_SIDE, SHORT_SIDE> {   \\\n+    static void DoBatchNarrowMatrixTranspose(const GPUDevice& d,               \\\n+                                             int tile_size_i, int tile_size_j, \\\n+                                             int total_tiles_count,            \\\n+                                             const TYPE* input,                \\\n+                                             const Dimension<3>& input_dims,   \\\n+                                             TYPE* output) {                   \\\n+      const int ThreadNum = LONG_SIDE;                                         \\\n+      if (tile_size_i <= LONG_SIDE && tile_size_j <= SHORT_SIDE)               \\\n+        SwapDimension1And2InTensor3UsingTiles<                                 \\\n+            TYPE, ThreadNum, LONG_SIDE,                                        \\\n+            SHORT_SIDE><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(      \\\n+            input, input_dims, output);                                        \\\n+      else if (tile_size_j <= LONG_SIDE && tile_size_i <= SHORT_SIDE)          \\\n+        SwapDimension1And2InTensor3UsingTiles<                                 \\\n+            TYPE, ThreadNum, SHORT_SIDE,                                       \\\n+            LONG_SIDE><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(       \\\n+            input, input_dims, output);                                        \\\n+      return;                                                                  \\\n+    }                                                                          \\\n+  };\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_128(TYPE)               \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, 256, 16); \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 32, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 64, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 128, 15);         \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 256, 2);\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_64(TYPE)                \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, 512, 16); \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 32, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 64, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 128, 15);         \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 256, 8);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 512, 2);\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_32(TYPE)                 \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, 1024, 16); \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 32, 15);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 64, 15);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 128, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 256, 10);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 512, 4);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 1024, 2);\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_16(TYPE)                 \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, 1024, 16); \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 32, 15);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 64, 15);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 128, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 256, 10);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 512, 4);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 1024, 2);\n+\n+#define BATCH_NARROW_MATRIX_TRANSPOSE_8(TYPE)                  \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_OVERALL(TYPE, 1024, 16); \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 32, 15);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 64, 15);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 128, 15);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 256, 10);          \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 512, 4);           \\\n+  BATCH_NARROW_MATRIX_TRANSPOSE_LIMIT_PER_LONG_SIDE_LEN(TYPE, 1024, 2);\n+\n+BATCH_NARROW_MATRIX_TRANSPOSE_128(float4);\n+BATCH_NARROW_MATRIX_TRANSPOSE_64(double);\n+BATCH_NARROW_MATRIX_TRANSPOSE_64(uint64);\n+BATCH_NARROW_MATRIX_TRANSPOSE_32(float);\n+BATCH_NARROW_MATRIX_TRANSPOSE_32(uint32);\n+BATCH_NARROW_MATRIX_TRANSPOSE_16(Eigen::half);\n+BATCH_NARROW_MATRIX_TRANSPOSE_16(uint16);\n+BATCH_NARROW_MATRIX_TRANSPOSE_8(uint8);", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 323, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "4c36dee945836ac2b83f058ee6107d7cc876d484", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "Is it really necessary to instantiate with all of these types?  Would it be insufficient to do uint8, uint16, uint32, uint64, and uint128/float4?  (Assuming that all of the types have the same alignment -- which I think they do, and which we could assert?)  I presume this would decrease cimple time significantly.", "created_at": "2017-09-20T22:15:38Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140107128", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140107128"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140107128"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>Is it really necessary to instantiate with all of these types?  Would it be insufficient to do uint8, uint16, uint32, uint64, and uint128/float4?  (Assuming that all of the types have the same alignment -- which I think they do, and which we could assert?)  I presume this would decrease cimple time significantly.</p>", "body_text": "Is it really necessary to instantiate with all of these types?  Would it be insufficient to do uint8, uint16, uint32, uint64, and uint128/float4?  (Assuming that all of the types have the same alignment -- which I think they do, and which we could assert?)  I presume this would decrease cimple time significantly."}