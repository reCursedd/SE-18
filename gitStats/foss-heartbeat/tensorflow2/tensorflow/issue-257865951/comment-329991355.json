{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/329991355", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#issuecomment-329991355", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/13049", "id": 329991355, "node_id": "MDEyOklzc3VlQ29tbWVudDMyOTk5MTM1NQ==", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "created_at": "2017-09-16T19:57:30Z", "updated_at": "2017-09-16T22:22:33Z", "author_association": "CONTRIBUTOR", "body_html": "<p>Sorry for this late reply. I was organizing my benchmarks into a presentable form.</p>\n<ul>\n<li>\n<p>I'm looking for comments that address whether this PR clears or can be improved to clear all the basic design rationales behind Tensorflow (my last point of the PR message euphemistically states that if this PR fails to clear such criteria then I'm willing to let it go). Some of my concerns are:</p>\n<ul>\n<li>Method invocation is very very ugly.</li>\n<li>Maybe TF dev team values maintainability well over performance.<br>\nAnd your comments are very helpful as they showed me what metrics you hope to evaluate this PR with apart from performance (compilation time, binary size).</li>\n</ul>\n</li>\n<li>\n<p>I have uploaded my standalone benchmark <a href=\"https://github.com/tjingrant/batch_matrix_transpose_benchmark\">here</a> which includes a README for anyone interested in evaluating. Also a detailed performance evaluation report is included in the repo for data collected on K40 machine and P100 machine. Basically based on our observation, this PR on average gives 12% performance increase on K40 machine and 39% on P100. This benchmark also serves as a test suite as all results are cross-checked.</p>\n</li>\n<li>\n<p>I will eventually integrate the standalone benchmark to TF but for now the standalone version seems better suited for evaluating compilation times and binary size. I will add the performance figures to the commit message once implementation stabilizes.</p>\n</li>\n<li>\n<p>Compilation time is 5.6 seconds for float. Assuming this holds more or less the same for half and double this amounts to 16.8 seconds on our older 32 core Power8 machine with nvcc 8.0.61. Binary size is 2MB. I hope these figures provide reassurance.</p>\n</li>\n</ul>", "body_text": "Sorry for this late reply. I was organizing my benchmarks into a presentable form.\n\n\nI'm looking for comments that address whether this PR clears or can be improved to clear all the basic design rationales behind Tensorflow (my last point of the PR message euphemistically states that if this PR fails to clear such criteria then I'm willing to let it go). Some of my concerns are:\n\nMethod invocation is very very ugly.\nMaybe TF dev team values maintainability well over performance.\nAnd your comments are very helpful as they showed me what metrics you hope to evaluate this PR with apart from performance (compilation time, binary size).\n\n\n\nI have uploaded my standalone benchmark here which includes a README for anyone interested in evaluating. Also a detailed performance evaluation report is included in the repo for data collected on K40 machine and P100 machine. Basically based on our observation, this PR on average gives 12% performance increase on K40 machine and 39% on P100. This benchmark also serves as a test suite as all results are cross-checked.\n\n\nI will eventually integrate the standalone benchmark to TF but for now the standalone version seems better suited for evaluating compilation times and binary size. I will add the performance figures to the commit message once implementation stabilizes.\n\n\nCompilation time is 5.6 seconds for float. Assuming this holds more or less the same for half and double this amounts to 16.8 seconds on our older 32 core Power8 machine with nvcc 8.0.61. Binary size is 2MB. I hope these figures provide reassurance.", "body": "Sorry for this late reply. I was organizing my benchmarks into a presentable form. \r\n- I'm looking for comments that address whether this PR clears or can be improved to clear all the basic design rationales behind Tensorflow (my last point of the PR message euphemistically states that if this PR fails to clear such criteria then I'm willing to let it go). Some of my concerns are:\r\n    - Method invocation is very very ugly.\r\n    - Maybe TF dev team values maintainability well over performance.\r\nAnd your comments are very helpful as they showed me what metrics you hope to evaluate this PR with apart from performance (compilation time, binary size).\r\n\r\n- I have uploaded my standalone benchmark [here](https://github.com/tjingrant/batch_matrix_transpose_benchmark) which includes a README for anyone interested in evaluating. Also a detailed performance evaluation report is included in the repo for data collected on K40 machine and P100 machine. Basically based on our observation, this PR on average gives 12% performance increase on K40 machine and 39% on P100. This benchmark also serves as a test suite as all results are cross-checked.\r\n- I will eventually integrate the standalone benchmark to TF but for now the standalone version seems better suited for evaluating compilation times and binary size. I will add the performance figures to the commit message once implementation stabilizes.\r\n- Compilation time is 5.6 seconds for float. Assuming this holds more or less the same for half and double this amounts to 16.8 seconds on our older 32 core Power8 machine with nvcc 8.0.61. Binary size is 2MB. I hope these figures provide reassurance."}