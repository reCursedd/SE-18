{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140115995", "pull_request_review_id": 64123689, "id": 140115995, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDExNTk5NQ==", "diff_hunk": "@@ -422,29 +595,131 @@ void RunSwapDimension1And2InTensor3(const GPUDevice& d, const T* input,\n   // If both dimensions are not trivial, use tiles for the actual swapping.\n   // Otherwise, the trivial swapping relying on the ldg cache is more efficient.\n   static const int kMinDimensionToUseTiles = 16;\n-  bool use_tiles = (input_dims[1] >= kMinDimensionToUseTiles &&\n-                    input_dims[2] >= kMinDimensionToUseTiles);\n-  if (use_tiles) {\n+  static const int kMinDimensionToUseRectTiles = 96;\n+\n+  bool large_matrix = (input_dims[1] >= kMinDimensionToUseTiles &&\n+                       input_dims[2] >= kMinDimensionToUseTiles);\n+  bool narrow_matrix = (input_dims[1] >= kMinDimensionToUseRectTiles ||\n+                      input_dims[2] >= kMinDimensionToUseRectTiles);\n+  if (large_matrix) {\n     // We get best performance when TileSize is the number of threads in a warp\n     // (32 on our GPUs) and NumSubTiles is 8, so our block size is 8 * 32 = 256\n     // threads.\n     static const int TileSize = 32;\n-    static const int NumSubTiles = 8;\n+    static const int ThreadNum = 256;\n+\n     Dimension<3> input_dims_in_tiles = {\n         input_dims[0], (input_dims[1] + TileSize - 1) / TileSize,\n         (input_dims[2] + TileSize - 1) / TileSize,\n     };\n+\n+    int total_tiles_count = input_dims_in_tiles[0] * input_dims_in_tiles[1] *\n+                            input_dims_in_tiles[2];\n+    SwapDimension1And2InTensor3UsingTiles<\n+        T, ThreadNum, TileSize,\n+        TileSize><<<total_tiles_count, ThreadNum, 0, d.stream()>>>(\n+        input, input_dims, output);\n+\n+  } else if (narrow_matrix) {\n+    // Define available tile sizes here for each size of data type supported:\n+    std::map<int, int> tile_spec_128 = {{32, 15}, {64, 15}, {128, 15},\n+                                        {256, 2}};\n+    std::map<int, int> tile_spec_64  = {{32, 15},  {64, 15}, {128, 15},\n+                                        {256, 8},  {512, 2}};\n+    std::map<int, int> tile_spec_32  = {{32, 15},  {64, 15}, {128, 15},\n+                                        {256, 10}, {512, 4}, {1024, 2}};\n+    std::map<int, int> tile_spec_16  = {{32, 15},  {64, 15}, {128, 15},\n+                                        {256, 10}, {512, 4}, {1024, 2}};\n+    std::map<int, int> tile_spec_8   = {{32, 15},  {64, 15}, {128, 15},\n+                                        {256, 10}, {512, 4}, {1024, 2}};\n+\n+    // Organize these tile size specifications into a map that maps from data\n+    // type sizes to their specifications.\n+    std::map<int, std::map<int, int>> tile_spec_map = {{128, tile_spec_128},\n+                                                       {64, tile_spec_64},\n+                                                       {32, tile_spec_32},\n+                                                       {16, tile_spec_16},\n+                                                       {8, tile_spec_8}};\n+\n+    std::map<int, int> tile_spec = tile_spec_map[8 * sizeof(T)];\n+\n+    int tile_long_side_len = 0;\n+    int tile_short_side_len = 0;\n+    float lowest_cost = std::numeric_limits<float>::max();\n+    int data_long_side = max(input_dims[1], input_dims[2]);\n+\n+    for (std::map<int, int>::iterator it = tile_spec.begin();\n+         it != tile_spec.end(); ++it) {\n+      int proposed_tile_long_side_len = it->first;\n+\n+      // Threads that will not be doing anything useful when reading the matrix\n+      // because the thread block size is bigger than the data block size.\n+      float wasted_threads = (data_long_side -\n+                              data_long_side / proposed_tile_long_side_len *\n+                                  proposed_tile_long_side_len);\n+      int num_full_tiles = data_long_side / proposed_tile_long_side_len;\n+\n+      float cost = 0;\n+\n+      // However, if we can execute two or more full tiles, then we gladly\n+      // accept any number of wasted thread and ignore its cost.\n+      if (num_full_tiles <= 1) cost = wasted_threads;\n+\n+      // Using less and equal here because given the same cost, we would like to\n+      // launch as many threads\n+      // as possible.\n+      if (cost <= lowest_cost) {\n+        tile_long_side_len = proposed_tile_long_side_len;\n+        tile_short_side_len = it->second;\n+        lowest_cost = cost;\n+      }\n+    }\n+\n+    // Request tile sizes such that the longer side of threadblock align with\n+    // the longer side of input data block to maximize read throughput.\n+    // The ideal tile shape to request is one with its length of the shorter", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 418, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "4c36dee945836ac2b83f058ee6107d7cc876d484", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "with the length", "created_at": "2017-09-20T23:15:29Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140115995", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140115995"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140115995"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>with the length</p>", "body_text": "with the length"}