{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140130259", "pull_request_review_id": 64158227, "id": 140130259, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDEzMDI1OQ==", "diff_hunk": "@@ -422,29 +595,131 @@ void RunSwapDimension1And2InTensor3(const GPUDevice& d, const T* input,\n   // If both dimensions are not trivial, use tiles for the actual swapping.\n   // Otherwise, the trivial swapping relying on the ldg cache is more efficient.\n   static const int kMinDimensionToUseTiles = 16;\n-  bool use_tiles = (input_dims[1] >= kMinDimensionToUseTiles &&\n-                    input_dims[2] >= kMinDimensionToUseTiles);\n-  if (use_tiles) {\n+  static const int kMinDimensionToUseRectTiles = 96;\n+\n+  bool large_matrix = (input_dims[1] >= kMinDimensionToUseTiles &&\n+                       input_dims[2] >= kMinDimensionToUseTiles);\n+  bool narrow_matrix = (input_dims[1] >= kMinDimensionToUseRectTiles ||\n+                      input_dims[2] >= kMinDimensionToUseRectTiles);\n+  if (large_matrix) {\n     // We get best performance when TileSize is the number of threads in a warp\n     // (32 on our GPUs) and NumSubTiles is 8, so our block size is 8 * 32 = 256\n     // threads.\n     static const int TileSize = 32;\n-    static const int NumSubTiles = 8;\n+    static const int ThreadNum = 256;", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 347, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "4c36dee945836ac2b83f058ee6107d7cc876d484", "user": {"login": "tjingrant", "id": 6410074, "node_id": "MDQ6VXNlcjY0MTAwNzQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/6410074?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tjingrant", "html_url": "https://github.com/tjingrant", "followers_url": "https://api.github.com/users/tjingrant/followers", "following_url": "https://api.github.com/users/tjingrant/following{/other_user}", "gists_url": "https://api.github.com/users/tjingrant/gists{/gist_id}", "starred_url": "https://api.github.com/users/tjingrant/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tjingrant/subscriptions", "organizations_url": "https://api.github.com/users/tjingrant/orgs", "repos_url": "https://api.github.com/users/tjingrant/repos", "events_url": "https://api.github.com/users/tjingrant/events{/privacy}", "received_events_url": "https://api.github.com/users/tjingrant/received_events", "type": "User", "site_admin": false}, "body": "Because in general the imaginary 2-dimensional thread blocks can change shape for reading and outputting matrices and therefore do not have a fixed size (essentially the `effective_thread_num` is the size of the imaginary block size, which can be different in the same kernel execution depending on whether you are reading or writing). I'm afraid that calling it `BlockSize` may induce some incorrect intuition.", "created_at": "2017-09-21T01:15:01Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140130259", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "CONTRIBUTOR", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140130259"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140130259"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>Because in general the imaginary 2-dimensional thread blocks can change shape for reading and outputting matrices and therefore do not have a fixed size (essentially the <code>effective_thread_num</code> is the size of the imaginary block size, which can be different in the same kernel execution depending on whether you are reading or writing). I'm afraid that calling it <code>BlockSize</code> may induce some incorrect intuition.</p>", "body_text": "Because in general the imaginary 2-dimensional thread blocks can change shape for reading and outputting matrices and therefore do not have a fixed size (essentially the effective_thread_num is the size of the imaginary block size, which can be different in the same kernel execution depending on whether you are reading or writing). I'm afraid that calling it BlockSize may induce some incorrect intuition.", "in_reply_to_id": 140114235}