{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141927407", "pull_request_review_id": 66213548, "id": 141927407, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MTkyNzQwNw==", "diff_hunk": "@@ -413,6 +429,275 @@ struct PadInput<GPUDevice, T, int, NDIMS> {\n   }\n };\n \n+// TPF stands for Tile size Posibility Frontier. It denotes the tile size\n+// combinations that consume the most computational resources constrained by\n+// number of threads per SM limit, shared memory limit and overall degree of\n+// parallelism.\n+// Tile size combinations lying on this frontier would achieve the maximum\n+// utilization of available resources and combinations lying outside this\n+// frontier is not performance-wise profitable or simply not possible.\n+#define TPF_CHECKER(OP)                                                   \\\n+  size_of_t == 16 &&    (TileLongSide == 32   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 64   && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 128  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 2)) || \\\n+      size_of_t == 8 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 8)  || \\\n+                         TileLongSide == 256  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 512  && (TileShortSide OP 2)) || \\\n+      size_of_t == 4 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 2 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2)) || \\\n+      size_of_t == 1 && (TileLongSide == 32   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 64   && (TileShortSide OP 15) || \\\n+                         TileLongSide == 128  && (TileShortSide OP 15) || \\\n+                         TileLongSide == 256  && (TileShortSide OP 10) || \\\n+                         TileLongSide == 512  && (TileShortSide OP 4)  || \\\n+                         TileLongSide == 1024 && (TileShortSide OP 2))\n+\n+constexpr bool TileSizePossibilityFrontierCheck(int TileLongSide,\n+                                                int TileShortSide,\n+                                                int size_of_t, int mode) {\n+  // When mode is 0, this function checks whether the tile size combination lies\n+  // on the tile size possibility frontier. When mode is 1, it checks whether\n+  // the combination lies outside the frontier.\n+  return mode == 0 ? TPF_CHECKER(==) : TPF_CHECKER(>);\n+}\n+\n+#undef TPF_CHECKER\n+\n+// Recursive template function to search for the minimum tile size configuration\n+// satisfying the requested tile side lengths.\n+template <typename T, int TileLongSide, int TileShortSide, typename dummy = void>\n+struct BatchNarrowMatrixTransposeDispatcher {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+    bool request_satisfied = max(tile_size_i, tile_size_j) <= TileLongSide &&\n+                             min(tile_size_i, tile_size_j) <= TileShortSide;\n+\n+    if (request_satisfied) {\n+      const int NumThread = TileLongSide;\n+      if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileLongSide,\n+            TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide) {\n+        SwapDimension1And2InTensor3UsingTiles<\n+            T, NumThread, TileShortSide,\n+            TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+            input, input_dims, output);\n+      }\n+      return;\n+    }\n+\n+    // Kernel is not launched, meaning the launch configuration is not\n+    // satisfied.\n+    const bool long_side_request_not_satisfied =\n+        max(tile_size_i, tile_size_j) > TileLongSide;\n+\n+    // Increase launch parameters and try again.\n+    if (long_side_request_not_satisfied) {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide * 2, TileShortSide>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    } else {\n+      BatchNarrowMatrixTransposeDispatcher<T, TileLongSide, TileShortSide + 1>::\n+          DoIt(d, tile_size_i, tile_size_j,\n+                                       total_tiles_count, input, input_dims,\n+                                       output);\n+    }\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T), 0),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+\n+    static_assert((TileLongSide & (TileLongSide - 1)) == 0,\n+      \"The length of the longer side of the tile is always a power of 2.\");\n+\n+    const int NumThread = TileLongSide;\n+    if (tile_size_i <= TileLongSide && tile_size_j <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileLongSide,\n+          TileShortSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    else if (tile_size_j <= TileLongSide && tile_size_i <= TileShortSide)\n+      SwapDimension1And2InTensor3UsingTiles<\n+          T, NumThread, TileShortSide,\n+          TileLongSide><<<total_tiles_count, NumThread, 0, d.stream()>>>(\n+          input, input_dims, output);\n+    return;\n+  }\n+};\n+\n+template <typename T, int TileLongSide, int TileShortSide>\n+struct BatchNarrowMatrixTransposeDispatcher<\n+    T, TileLongSide, TileShortSide,\n+    typename std::enable_if<TileSizePossibilityFrontierCheck(\n+                                TileLongSide, TileShortSide, sizeof(T),\n+                                /* mode = outside */ 1),\n+                            void>::type> {\n+  static void DoIt(const GPUDevice& d, int tile_size_i,\n+                                           int tile_size_j,\n+                                           int total_tiles_count,\n+                                           const T* input,\n+                                           const Dimension<3>& input_dims,\n+                                           T* output) {\n+    assert(false &&\n+           \"BatchNarrowMatrixTransposeDispatcher has requested an unexpected \"\n+           \"launch configuration. \");\n+  }\n+};\n+\n+// A helper function to make RunSwapDimension1And2InTensor3 concise. This\n+// helper function looks at the data type and input matrix sizes and decides\n+// the thread numbers and tile sizes to use.\n+template <typename T>\n+void SwapDimension1And2InTensor3WithNarrowMatrices(\n+    const GPUDevice& d, const T* input, const Dimension<3>& input_dims,\n+    T* output, const int kMinDimensionToUseTiles) {\n+  // Define available tile sizes here for each size of data type supported:\n+  std::vector<std::pair<int, int>> tile_spec =\n+      []() -> std::vector<std::pair<int, int>> {\n+    switch (sizeof(T)) {\n+      case 1:\n+        return {{32, 15}, {64, 15}, {128, 15}, {256, 15}, {512, 4}, {1024, 2}};\n+      case 2:\n+        return {{32, 15}, {64, 15}, {128, 15}, {256, 15}, {512, 4}, {1024, 2}};\n+      case 4:\n+        return {{32, 15}, {64, 15}, {128, 15}, {256, 15}, {512, 4}, {1024, 2}};\n+      case 8:\n+        return {{32, 15}, {64, 15}, {128, 8}, {256, 4}, {512, 2}};\n+      case 16:\n+        return {{32, 4},  {64, 4},  {128, 4}, {256, 2}};\n+    }\n+    return {};\n+  }();\n+\n+  int tile_long_side_len = 0;\n+  int tile_short_side_len = 0;\n+  float lowest_cost = std::numeric_limits<float>::max();\n+  int data_long_side = max(input_dims[1], input_dims[2]);\n+\n+  for (auto tile_size_pair : tile_spec) {\n+    int proposed_tile_long_side_len = tile_size_pair.first;\n+\n+    // Threads that will not be doing anything useful when reading the matrix\n+    // because the thread block size is bigger than the data block size.\n+    int wasted_threads = (data_long_side -\n+                          MathUtil::FloorOfRatio<int>(\n+                              data_long_side, proposed_tile_long_side_len) *\n+                              proposed_tile_long_side_len);\n+\n+    int num_full_tiles = MathUtil::FloorOfRatio<int>(\n+        data_long_side, proposed_tile_long_side_len);\n+\n+    float cost = 0;\n+\n+    // However, if we can execute two or more full tiles, then we gladly\n+    // accept any number of wasted thread and ignore its cost.\n+    if (num_full_tiles <= 1) cost = wasted_threads;\n+\n+    // Using less and equal here because given the same cost, we would like to\n+    // launch as many threads as possible.\n+    if (cost <= lowest_cost) {\n+      tile_long_side_len = proposed_tile_long_side_len;\n+      tile_short_side_len = tile_size_pair.second;\n+      lowest_cost = cost;\n+    }\n+  }\n+\n+  // Request tile sizes such that the longer side of threadblock aligns with\n+  // the longer side of input data block to maximize read throughput.\n+  // The ideal tile shape to request is one with its length of the shorter\n+  // side of the tile being equal to the length of the shorter side of the\n+  // input matrix.\n+  int requested_tile_size_i = input_dims[1] >= kMinDimensionToUseTiles\n+                                  ? tile_long_side_len\n+                                  : input_dims[1];\n+  int requested_tile_size_j = input_dims[1] >= kMinDimensionToUseTiles\n+                                  ? input_dims[2]\n+                                  : tile_long_side_len;\n+\n+  // Truncate the shorter size requested according to the manual limit set in\n+  // tile_spec to make sure that we do not launch configurations violating\n+  // hardware limits.\n+  requested_tile_size_i = requested_tile_size_i == tile_long_side_len\n+                              ? tile_long_side_len\n+                              : min(requested_tile_size_i, tile_short_side_len);\n+  requested_tile_size_j = requested_tile_size_j == tile_long_side_len\n+                              ? tile_long_side_len\n+                              : min(requested_tile_size_j, tile_short_side_len);\n+\n+  Dimension<3> input_dims_in_tiles = {\n+      input_dims[0],\n+      (input_dims[1] + requested_tile_size_i - 1) / requested_tile_size_i,\n+      (input_dims[2] + requested_tile_size_j - 1) / requested_tile_size_j,", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 407, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "372d4111b99467a9eb4b4866ae8b53ecba505512", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "CeilOfRatio?", "created_at": "2017-09-29T17:49:05Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141927407", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/141927407"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r141927407"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>CeilOfRatio?</p>", "body_text": "CeilOfRatio?"}