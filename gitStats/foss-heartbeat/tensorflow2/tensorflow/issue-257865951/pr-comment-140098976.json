{"url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140098976", "pull_request_review_id": 64123689, "id": 140098976, "node_id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDE0MDA5ODk3Ng==", "diff_hunk": "@@ -178,31 +179,35 @@ __global__ void SwapDimension1And2InTensor3Simple(int nthreads, const T* input,\n // Use shared memory tiles to swap dimension-1 and dimension-2 of a 3D tensor,\n // where dimensions are zero-based: output[i][j][k] = input[i][k][j].\n //\n-// Each thread block operates on a single tile, a square of dimensions TileSize\n-// x TileSize.  We require that the thread block's X dimension equals TileSize,\n-// and its Y dimension equals NumSubTiles.\n+// Each thread block operates on a single tile, a rectangle of dimensions TileSizeI\n+// x TileSizeJ.\n //\n-// For best performance, you should probably set TileSize equal to the number of\n-// threads in a warp (32 in nvidia GPUs).  With a TileSize of 32, NumSubTiles ==\n-// 4 or 8 seems to get the best performance on K40 GPUs.\n-template <typename T, int TileSize, int NumSubTiles>\n-__global__ void SwapDimension1And2InTensor3UsingTiles(const T* input,\n-                                                      Dimension<3> input_dims,\n-                                                      T* output) {\n-  // One extra line in the inner dimension to avoid share memory bank conflict.\n-  __shared__ T shared_memory_tile[TileSize][TileSize + 1];\n-\n-  static_assert(TileSize % NumSubTiles == 0,\n-                \"TileSize must be divisible by NumSubTiles\");\n-  eigen_assert(blockDim.x == TileSize);\n-  eigen_assert(blockDim.y == NumSubTiles);\n+// In general, for best performance, you should probably set TileSizeI,\n+// TileSizeJ equal to the number of threads in a warp (32 in nvidia GPUs).\n+// With a TileSizeI, TileSizeJ of 32, ThreadNum of 128 or 256 seems to get\n+// the best performance on K40 GPUs.\n+template <typename T, int ThreadNum, int TileSizeI, int TileSizeJ>\n+__global__ void SwapDimension1And2InTensor3UsingTiles(\n+    const T* __restrict__ input, Dimension<3> input_dims,\n+    T* __restrict__ output) {\n+\n+  eigen_assert(blockDim.x == ThreadNum);\n+  eigen_assert(blockDim.y == 1);\n   eigen_assert(blockDim.z == 1);\n   eigen_assert(gridDim.y == 1);\n   eigen_assert(gridDim.z == 1);\n \n-  // We break down the tile into NumSubTiles groups, so each thread processes\n-  // kSubTileSize elements (except at the edges of the input).\n-  const int kSubTileSize = TileSize / NumSubTiles;\n+  const int ReadRowPerPass = (ThreadNum / TileSizeJ);\n+  const int WriteRowPerPass = (ThreadNum / TileSizeI);\n+  // One extra line in the inner dimension to avoid share memory bank conflict.\n+  __shared__ T shared_memory_tile[TileSizeI][TileSizeJ + 1];\n+\n+// Memory access macros:\n+#define SHARED(i, j) shared_memory_tile[i][j]\n+\n+#define INPUT(i, j) input[input_origin_flat_index + (i)*input_dims[2] + (j)]\n+\n+#define OUTPUT(i, j) output[output_origin_flat_index + (i)*output_dims[2] + (j)]", "path": "tensorflow/core/kernels/conv_ops_gpu_3.cu.cc", "position": null, "original_position": 60, "commit_id": "63d7a082d37c7db42ce52410cf240efda92eaa74", "original_commit_id": "4c36dee945836ac2b83f058ee6107d7cc876d484", "user": {"login": "jlebar", "id": 150663, "node_id": "MDQ6VXNlcjE1MDY2Mw==", "avatar_url": "https://avatars1.githubusercontent.com/u/150663?v=4", "gravatar_id": "", "url": "https://api.github.com/users/jlebar", "html_url": "https://github.com/jlebar", "followers_url": "https://api.github.com/users/jlebar/followers", "following_url": "https://api.github.com/users/jlebar/following{/other_user}", "gists_url": "https://api.github.com/users/jlebar/gists{/gist_id}", "starred_url": "https://api.github.com/users/jlebar/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/jlebar/subscriptions", "organizations_url": "https://api.github.com/users/jlebar/orgs", "repos_url": "https://api.github.com/users/jlebar/repos", "events_url": "https://api.github.com/users/jlebar/events{/privacy}", "received_events_url": "https://api.github.com/users/jlebar/received_events", "type": "User", "site_admin": false}, "body": "Since the nontrivial INPUT and OUTPUT macros are only used once, would prefer simply not to have them.  And SHARED is a trivial macro and only used twice, probably also better not to have it as well.", "created_at": "2017-09-20T21:33:16Z", "updated_at": "2017-12-28T20:26:00Z", "html_url": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140098976", "pull_request_url": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049", "author_association": "MEMBER", "_links": {"self": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/comments/140098976"}, "html": {"href": "https://github.com/tensorflow/tensorflow/pull/13049#discussion_r140098976"}, "pull_request": {"href": "https://api.github.com/repos/tensorflow/tensorflow/pulls/13049"}}, "body_html": "<p>Since the nontrivial INPUT and OUTPUT macros are only used once, would prefer simply not to have them.  And SHARED is a trivial macro and only used twice, probably also better not to have it as well.</p>", "body_text": "Since the nontrivial INPUT and OUTPUT macros are only used once, would prefer simply not to have them.  And SHARED is a trivial macro and only used twice, probably also better not to have it as well."}