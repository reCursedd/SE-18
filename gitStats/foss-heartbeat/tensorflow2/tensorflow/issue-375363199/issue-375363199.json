{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23369", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23369/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23369/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23369/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23369", "id": 375363199, "node_id": "MDU6SXNzdWUzNzUzNjMxOTk=", "number": 23369, "title": "KeyError: 'BlockLSTM' when using tf.train.import_meta_graph()", "user": {"login": "dsindex", "id": 8259057, "node_id": "MDQ6VXNlcjgyNTkwNTc=", "avatar_url": "https://avatars0.githubusercontent.com/u/8259057?v=4", "gravatar_id": "", "url": "https://api.github.com/users/dsindex", "html_url": "https://github.com/dsindex", "followers_url": "https://api.github.com/users/dsindex/followers", "following_url": "https://api.github.com/users/dsindex/following{/other_user}", "gists_url": "https://api.github.com/users/dsindex/gists{/gist_id}", "starred_url": "https://api.github.com/users/dsindex/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/dsindex/subscriptions", "organizations_url": "https://api.github.com/users/dsindex/orgs", "repos_url": "https://api.github.com/users/dsindex/repos", "events_url": "https://api.github.com/users/dsindex/events{/privacy}", "received_events_url": "https://api.github.com/users/dsindex/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 404586594, "node_id": "MDU6TGFiZWw0MDQ1ODY1OTQ=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/stat:awaiting%20tensorflower", "name": "stat:awaiting tensorflower", "color": "f4b400", "default": false}, {"id": 473172988, "node_id": "MDU6TGFiZWw0NzMxNzI5ODg=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:bug/performance", "name": "type:bug/performance", "color": "159b2e", "default": false}], "state": "closed", "locked": false, "assignee": {"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "drpngx", "id": 20959853, "node_id": "MDQ6VXNlcjIwOTU5ODUz", "avatar_url": "https://avatars1.githubusercontent.com/u/20959853?v=4", "gravatar_id": "", "url": "https://api.github.com/users/drpngx", "html_url": "https://github.com/drpngx", "followers_url": "https://api.github.com/users/drpngx/followers", "following_url": "https://api.github.com/users/drpngx/following{/other_user}", "gists_url": "https://api.github.com/users/drpngx/gists{/gist_id}", "starred_url": "https://api.github.com/users/drpngx/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/drpngx/subscriptions", "organizations_url": "https://api.github.com/users/drpngx/orgs", "repos_url": "https://api.github.com/users/drpngx/repos", "events_url": "https://api.github.com/users/drpngx/events{/privacy}", "received_events_url": "https://api.github.com/users/drpngx/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 1, "created_at": "2018-10-30T08:00:29Z", "updated_at": "2018-10-30T17:51:44Z", "closed_at": "2018-10-30T17:51:43Z", "author_association": "NONE", "body_html": "<p><em>Please make sure that this is a bug. As per our <a href=\"https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md\">GitHub Policy</a>, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em></p>\n<p><strong>System information</strong></p>\n<ul>\n<li>Have I written custom code (as opposed to using a stock example script provided in TensorFlow):</li>\n<li>OS Platform and Distribution (e.g., Linux Ubuntu 16.04):<br>\nCentOS 7</li>\n<li>Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:</li>\n<li>TensorFlow installed from (source or binary):<br>\npip install tensorflow-gpu</li>\n<li>TensorFlow version (use command below):<br>\n1.11</li>\n<li>Python version:<br>\npython 3.6</li>\n<li>Bazel version (if compiling from source):</li>\n<li>GCC/Compiler version (if compiling from source):</li>\n<li>CUDA/cuDNN version:<br>\nCUDA 9.0, cuDNN 7.31</li>\n<li>GPU model and memory:<br>\nTITAN X (Pascal), 12G</li>\n</ul>\n<p>You can collect some of this information using our environment capture <a href=\"https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh\">script</a><br>\nYou can also obtain the TensorFlow version with<br>\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"</p>\n<p>v1.11.0-0-gc19e29306c 1.11.0</p>\n<p><strong>Describe the current behavior</strong></p>\n<p>I'm using two kinds of LSTM operations like below.</p>\n<div class=\"highlight highlight-source-python\"><pre>    <span class=\"pl-k\">def</span> <span class=\"pl-en\">__bi_lstm</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">lengths</span>, <span class=\"pl-smi\">rnn_size</span>, <span class=\"pl-smi\">keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bi-lstm<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Apply bi-directional LSTM</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n            cell_fw <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMCell(rnn_size)\n            cell_bw <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMCell(rnn_size)\n            (output_fw, output_bw), _ <span class=\"pl-k\">=</span> tf.nn.bidirectional_dynamic_rnn(cell_fw,\n                                                                        cell_bw,\n                                                                        inputs,\n                                                                        <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>lengths,\n                                                                        <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32)\n            outputs <span class=\"pl-k\">=</span> tf.concat([output_fw, output_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n            <span class=\"pl-k\">return</span> tf.nn.dropout(outputs, keep_prob)\n\n    <span class=\"pl-k\">def</span> <span class=\"pl-en\">__bi_lstm_fused</span>(<span class=\"pl-smi\"><span class=\"pl-smi\">self</span></span>, <span class=\"pl-smi\">inputs</span>, <span class=\"pl-smi\">lengths</span>, <span class=\"pl-smi\">rnn_size</span>, <span class=\"pl-smi\">keep_prob</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">0.5</span>, <span class=\"pl-smi\">scope</span><span class=\"pl-k\">=</span><span class=\"pl-s\"><span class=\"pl-pds\">'</span>bi-lstm-fused<span class=\"pl-pds\">'</span></span>):\n        <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Apply bi-directional LSTM block fused</span>\n<span class=\"pl-s\">        <span class=\"pl-pds\">\"\"\"</span></span>\n        <span class=\"pl-k\">with</span> tf.variable_scope(scope):\n            t <span class=\"pl-k\">=</span> tf.transpose(inputs, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])  <span class=\"pl-c\"><span class=\"pl-c\">#</span> Need time-major</span>\n            lstm_cell_fw <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\n            lstm_cell_bw <span class=\"pl-k\">=</span> tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\n            lstm_cell_bw <span class=\"pl-k\">=</span> tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n            output_fw, _ <span class=\"pl-k\">=</span> lstm_cell_fw(t, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>lengths)\n            output_bw, _ <span class=\"pl-k\">=</span> lstm_cell_bw(t, <span class=\"pl-v\">dtype</span><span class=\"pl-k\">=</span>tf.float32, <span class=\"pl-v\">sequence_length</span><span class=\"pl-k\">=</span>lengths)\n            outputs <span class=\"pl-k\">=</span> tf.concat([output_fw, output_bw], <span class=\"pl-v\">axis</span><span class=\"pl-k\">=</span><span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>)\n            outputs <span class=\"pl-k\">=</span> tf.transpose(outputs, <span class=\"pl-v\">perm</span><span class=\"pl-k\">=</span>[<span class=\"pl-c1\">1</span>, <span class=\"pl-c1\">0</span>, <span class=\"pl-c1\">2</span>])\n            <span class=\"pl-k\">return</span> tf.nn.dropout(outputs, keep_prob)</pre></div>\n<p>i trained a model and save it via saver.save().<br>\nand then, tried to import_meta_data</p>\n<div class=\"highlight highlight-source-python\"><pre>loader <span class=\"pl-k\">=</span> tf.train.import_meta_graph(meta_file, <span class=\"pl-v\">clear_devices</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">True</span>)\n<span class=\"pl-c1\">...</span>.</pre></div>\n<p>in case tf.contrib.rnn.LSTMCell(), everything goes fine.<br>\nbut, when it comes to use tf.contrib.rnn.LSTMBlockFusedCell(),<br>\nthere is an key error.</p>\n<div class=\"highlight highlight-source-shell\"><pre> Traceback (most recent call last):\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>export.py<span class=\"pl-pds\">\"</span></span>, line 55, <span class=\"pl-k\">in</span> <span class=\"pl-k\">&lt;</span>module<span class=\"pl-k\">&gt;</span>\n    export(args)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>export.py<span class=\"pl-pds\">\"</span></span>, line 13, <span class=\"pl-k\">in</span> <span class=\"pl-k\">export</span>\n    loader = tf.train.import_meta_graph(meta_file, clear_devices=True)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py<span class=\"pl-pds\">\"</span></span>, line 1666, <span class=\"pl-k\">in</span> import_meta_graph\n    meta_graph_or_file, clear_devices, import_scope, <span class=\"pl-k\">**</span>kwargs)[0]\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py<span class=\"pl-pds\">\"</span></span>, line 1688, <span class=\"pl-k\">in</span> _import_meta_graph_with_return_elements\n    <span class=\"pl-k\">**</span>kwargs))\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py<span class=\"pl-pds\">\"</span></span>, line 806, <span class=\"pl-k\">in</span> import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py<span class=\"pl-pds\">\"</span></span>, line 488, <span class=\"pl-k\">in</span> new_func\n    <span class=\"pl-k\">return</span> func(<span class=\"pl-k\">*</span>args, <span class=\"pl-k\">**</span>kwargs)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py<span class=\"pl-pds\">\"</span></span>, line 391, <span class=\"pl-k\">in</span> import_graph_def\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\n  File <span class=\"pl-s\"><span class=\"pl-pds\">\"</span>~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py<span class=\"pl-pds\">\"</span></span>, line 158, <span class=\"pl-k\">in</span> _RemoveDefaultAttrs\n    op_def = op_dict[node.op]\nKeyError: <span class=\"pl-s\"><span class=\"pl-pds\">'</span>BlockLSTM<span class=\"pl-pds\">'</span></span></pre></div>\n<p><strong>Describe the expected behavior</strong></p>\n<p><strong>Code to reproduce the issue</strong><br>\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.</p>\n<p><strong>Other info / logs</strong><br>\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.</p>", "body_text": "Please make sure that this is a bug. As per our GitHub Policy, we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template\nSystem information\n\nHave I written custom code (as opposed to using a stock example script provided in TensorFlow):\nOS Platform and Distribution (e.g., Linux Ubuntu 16.04):\nCentOS 7\nMobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\nTensorFlow installed from (source or binary):\npip install tensorflow-gpu\nTensorFlow version (use command below):\n1.11\nPython version:\npython 3.6\nBazel version (if compiling from source):\nGCC/Compiler version (if compiling from source):\nCUDA/cuDNN version:\nCUDA 9.0, cuDNN 7.31\nGPU model and memory:\nTITAN X (Pascal), 12G\n\nYou can collect some of this information using our environment capture script\nYou can also obtain the TensorFlow version with\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\nv1.11.0-0-gc19e29306c 1.11.0\nDescribe the current behavior\nI'm using two kinds of LSTM operations like below.\n    def __bi_lstm(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm'):\n        \"\"\"Apply bi-directional LSTM\n        \"\"\"\n        with tf.variable_scope(scope):\n            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n                                                                        cell_bw,\n                                                                        inputs,\n                                                                        sequence_length=lengths,\n                                                                        dtype=tf.float32)\n            outputs = tf.concat([output_fw, output_bw], axis=-1)\n            return tf.nn.dropout(outputs, keep_prob)\n\n    def __bi_lstm_fused(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm-fused'):\n        \"\"\"Apply bi-directional LSTM block fused\n        \"\"\"\n        with tf.variable_scope(scope):\n            t = tf.transpose(inputs, perm=[1, 0, 2])  # Need time-major\n            lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\n            lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\n            lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\n            output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=lengths)\n            output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=lengths)\n            outputs = tf.concat([output_fw, output_bw], axis=-1)\n            outputs = tf.transpose(outputs, perm=[1, 0, 2])\n            return tf.nn.dropout(outputs, keep_prob)\ni trained a model and save it via saver.save().\nand then, tried to import_meta_data\nloader = tf.train.import_meta_graph(meta_file, clear_devices=True)\n....\nin case tf.contrib.rnn.LSTMCell(), everything goes fine.\nbut, when it comes to use tf.contrib.rnn.LSTMBlockFusedCell(),\nthere is an key error.\n Traceback (most recent call last):\n  File \"export.py\", line 55, in <module>\n    export(args)\n  File \"export.py\", line 13, in export\n    loader = tf.train.import_meta_graph(meta_file, clear_devices=True)\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1666, in import_meta_graph\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1688, in _import_meta_graph_with_return_elements\n    **kwargs))\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\n    return_elements=return_elements)\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 391, in import_graph_def\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 158, in _RemoveDefaultAttrs\n    op_def = op_dict[node.op]\nKeyError: 'BlockLSTM'\nDescribe the expected behavior\nCode to reproduce the issue\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\nOther info / logs\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.", "body": "<em>Please make sure that this is a bug. As per our [GitHub Policy](https://github.com/tensorflow/tensorflow/blob/master/ISSUES.md), we only address code/doc bugs, performance issues, feature requests and build/installation issues on GitHub. tag:bug_template</em>\r\n\r\n**System information**\r\n- Have I written custom code (as opposed to using a stock example script provided in TensorFlow):\r\n- OS Platform and Distribution (e.g., Linux Ubuntu 16.04):\r\nCentOS 7\r\n- Mobile device (e.g. iPhone 8, Pixel 2, Samsung Galaxy) if the issue happens on mobile device:\r\n- TensorFlow installed from (source or binary):\r\npip install tensorflow-gpu\r\n- TensorFlow version (use command below):\r\n1.11\r\n- Python version:\r\npython 3.6\r\n- Bazel version (if compiling from source):\r\n- GCC/Compiler version (if compiling from source):\r\n- CUDA/cuDNN version:\r\nCUDA 9.0, cuDNN 7.31\r\n- GPU model and memory:\r\nTITAN X (Pascal), 12G\r\n\r\nYou can collect some of this information using our environment capture [script](https://github.com/tensorflow/tensorflow/tree/master/tools/tf_env_collect.sh)\r\nYou can also obtain the TensorFlow version with\r\npython -c \"import tensorflow as tf; print(tf.GIT_VERSION, tf.VERSION)\"\r\n\r\nv1.11.0-0-gc19e29306c 1.11.0\r\n\r\n**Describe the current behavior**\r\n\r\nI'm using two kinds of LSTM operations like below.\r\n```python\r\n    def __bi_lstm(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm'):\r\n        \"\"\"Apply bi-directional LSTM\r\n        \"\"\"\r\n        with tf.variable_scope(scope):\r\n            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\r\n            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\r\n            (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\r\n                                                                        cell_bw,\r\n                                                                        inputs,\r\n                                                                        sequence_length=lengths,\r\n                                                                        dtype=tf.float32)\r\n            outputs = tf.concat([output_fw, output_bw], axis=-1)\r\n            return tf.nn.dropout(outputs, keep_prob)\r\n\r\n    def __bi_lstm_fused(self, inputs, lengths, rnn_size, keep_prob=0.5, scope='bi-lstm-fused'):\r\n        \"\"\"Apply bi-directional LSTM block fused\r\n        \"\"\"\r\n        with tf.variable_scope(scope):\r\n            t = tf.transpose(inputs, perm=[1, 0, 2])  # Need time-major\r\n            lstm_cell_fw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\r\n            lstm_cell_bw = tf.contrib.rnn.LSTMBlockFusedCell(rnn_size)\r\n            lstm_cell_bw = tf.contrib.rnn.TimeReversedFusedRNN(lstm_cell_bw)\r\n            output_fw, _ = lstm_cell_fw(t, dtype=tf.float32, sequence_length=lengths)\r\n            output_bw, _ = lstm_cell_bw(t, dtype=tf.float32, sequence_length=lengths)\r\n            outputs = tf.concat([output_fw, output_bw], axis=-1)\r\n            outputs = tf.transpose(outputs, perm=[1, 0, 2])\r\n            return tf.nn.dropout(outputs, keep_prob)\r\n```\r\ni trained a model and save it via saver.save().\r\nand then, tried to import_meta_data\r\n\r\n```python\r\nloader = tf.train.import_meta_graph(meta_file, clear_devices=True)\r\n....\r\n```\r\n\r\nin case tf.contrib.rnn.LSTMCell(), everything goes fine. \r\nbut, when it comes to use tf.contrib.rnn.LSTMBlockFusedCell(), \r\nthere is an key error.\r\n\r\n```bash\r\n Traceback (most recent call last):\r\n  File \"export.py\", line 55, in <module>\r\n    export(args)\r\n  File \"export.py\", line 13, in export\r\n    loader = tf.train.import_meta_graph(meta_file, clear_devices=True)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1666, in import_meta_graph\r\n    meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1688, in _import_meta_graph_with_return_elements\r\n    **kwargs))\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/meta_graph.py\", line 806, in import_scoped_meta_graph_with_return_elements\r\n    return_elements=return_elements)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py\", line 488, in new_func\r\n    return func(*args, **kwargs)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 391, in import_graph_def\r\n    _RemoveDefaultAttrs(op_dict, producer_op_list, graph_def)\r\n  File \"~/python3.6_venv/lib/python3.6/site-packages/tensorflow/python/framework/importer.py\", line 158, in _RemoveDefaultAttrs\r\n    op_def = op_dict[node.op]\r\nKeyError: 'BlockLSTM'\r\n```\r\n\r\n**Describe the expected behavior**\r\n\r\n**Code to reproduce the issue**\r\nProvide a reproducible test case that is the bare minimum necessary to generate the problem.\r\n\r\n**Other info / logs**\r\nInclude any logs or source code that would be helpful to diagnose the problem. If including tracebacks, please include the full traceback. Large logs and files should be attached.\r\n"}