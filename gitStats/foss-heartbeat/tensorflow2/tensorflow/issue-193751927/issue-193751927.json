{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6118", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6118/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6118/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6118/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/6118", "id": 193751927, "node_id": "MDU6SXNzdWUxOTM3NTE5Mjc=", "number": 6118, "title": "Export Model For Serving But Tensor Type Dismatch", "user": {"login": "ericyue", "id": 918889, "node_id": "MDQ6VXNlcjkxODg4OQ==", "avatar_url": "https://avatars0.githubusercontent.com/u/918889?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ericyue", "html_url": "https://github.com/ericyue", "followers_url": "https://api.github.com/users/ericyue/followers", "following_url": "https://api.github.com/users/ericyue/following{/other_user}", "gists_url": "https://api.github.com/users/ericyue/gists{/gist_id}", "starred_url": "https://api.github.com/users/ericyue/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ericyue/subscriptions", "organizations_url": "https://api.github.com/users/ericyue/orgs", "repos_url": "https://api.github.com/users/ericyue/repos", "events_url": "https://api.github.com/users/ericyue/events{/privacy}", "received_events_url": "https://api.github.com/users/ericyue/received_events", "type": "User", "site_admin": false}, "labels": [], "state": "closed", "locked": false, "assignee": null, "assignees": [], "milestone": null, "comments": 1, "created_at": "2016-12-06T11:45:57Z", "updated_at": "2016-12-07T03:33:21Z", "closed_at": "2016-12-07T03:33:21Z", "author_association": "NONE", "body_html": "<h3>What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?</h3>\n<p>I am trying to export the model for serving , but it's report type error about <em>inputs</em> tensor.<br>\nbut in the export and predict part , the <strong>inputs</strong> are the same type.</p>\n<h3>If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)</h3>\n<p>here is a sample code for my exporting</p>\n<pre><code>            named_graph_signature = {\n                'inputs': exporter.generic_signature({\n                              'sparse_index': tf.placeholder(tf.int64, name=\"feature_index\")\n                              'sparse_ids': tf.placeholder(tf.int64,name = \"feature_ids\"),\n                              'sparse_values':tf.placeholder(tf.int64, name =\"feature_values\"),\n                              'sparse_shape':tf.placeholder(tf.int64, name=\"feature_shape\")\n                }),\n                'outputs': exporter.generic_signature({\n                    'prob': inference_softmax\n                })}\n            model_exporter.init(\n                sess.graph.as_graph_def(),\n                #default_graph_signature=named_graph_signature,\n                named_graph_signatures=named_graph_signature,\n                init_op=init_op)\n            model_exporter.export(export_path, tf.constant(export_version), sess)\n            print('Done exporting!')\n</code></pre>\n<p>here is my code for predicting</p>\n<pre><code>  ins = \"0 142635:1 250810:1 335229:1 375278:1 392970:1 506983:1 554566:1 631968:1 647823:1 658803:1 733446:1 856305:1 868202:1\"\n  FEATURE_SIZE = 1000000\n  tokens = ins.split(\" \")\n  feature_num = 0\n  feature_ids = []\n  feature_values = []\n  feature_index = []\n\n  for feature in tokens[1:]:\n      feature_id, feature_value = feature.split(\":\")\n      feature_ids.append(int(feature_id))\n      feature_values.append(float(feature_value))\n      feature_index.append([1, feature_num])\n      feature_num += 1\n\n  feature_shape = [1, FEATURE_SIZE]\n\n  sparse_index = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_index), dtype=tf.int64)\n  sparse_ids = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_ids), dtype=tf.int64)\n  sparse_values = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_values), dtype=tf.float32)\n  sparse_shape= tf.contrib.util.make_tensor_proto(numpy.asarray(feature_shape), dtype=tf.int64)\n\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  request.model_spec.version.value = model_version\n  print model_name,model_version\n\n  request.inputs['sparse_index'].CopyFrom(sparse_index)\n  request.inputs['sparse_ids'].CopyFrom(sparse_ids)\n  request.inputs['sparse_values'].CopyFrom(sparse_values)\n  request.inputs['sparse_shape'].CopyFrom(sparse_shape)\n  # Send request\n\n  result = stub.Predict(request, request_timeout)\n</code></pre>\n<h3>Logs or other output that would be helpful</h3>\n<p>(If logs are large, please upload as attachment or provide link).</p>\n<pre><code>Traceback (most recent call last):\n  File \"run.py\", line 63, in &lt;module&gt;\n    main()\n  File \"run.py\", line 59, in main\n    result = stub.Predict(request, request_timeout)\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\n    self._request_serializer, self._response_deserializer)\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\n    raise _abortion_error(rpc_error_call)\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input size does not match signature\")\n</code></pre>", "body_text": "What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\nI am trying to export the model for serving , but it's report type error about inputs tensor.\nbut in the export and predict part , the inputs are the same type.\nIf possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\nhere is a sample code for my exporting\n            named_graph_signature = {\n                'inputs': exporter.generic_signature({\n                              'sparse_index': tf.placeholder(tf.int64, name=\"feature_index\")\n                              'sparse_ids': tf.placeholder(tf.int64,name = \"feature_ids\"),\n                              'sparse_values':tf.placeholder(tf.int64, name =\"feature_values\"),\n                              'sparse_shape':tf.placeholder(tf.int64, name=\"feature_shape\")\n                }),\n                'outputs': exporter.generic_signature({\n                    'prob': inference_softmax\n                })}\n            model_exporter.init(\n                sess.graph.as_graph_def(),\n                #default_graph_signature=named_graph_signature,\n                named_graph_signatures=named_graph_signature,\n                init_op=init_op)\n            model_exporter.export(export_path, tf.constant(export_version), sess)\n            print('Done exporting!')\n\nhere is my code for predicting\n  ins = \"0 142635:1 250810:1 335229:1 375278:1 392970:1 506983:1 554566:1 631968:1 647823:1 658803:1 733446:1 856305:1 868202:1\"\n  FEATURE_SIZE = 1000000\n  tokens = ins.split(\" \")\n  feature_num = 0\n  feature_ids = []\n  feature_values = []\n  feature_index = []\n\n  for feature in tokens[1:]:\n      feature_id, feature_value = feature.split(\":\")\n      feature_ids.append(int(feature_id))\n      feature_values.append(float(feature_value))\n      feature_index.append([1, feature_num])\n      feature_num += 1\n\n  feature_shape = [1, FEATURE_SIZE]\n\n  sparse_index = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_index), dtype=tf.int64)\n  sparse_ids = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_ids), dtype=tf.int64)\n  sparse_values = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_values), dtype=tf.float32)\n  sparse_shape= tf.contrib.util.make_tensor_proto(numpy.asarray(feature_shape), dtype=tf.int64)\n\n  channel = implementations.insecure_channel(host, port)\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n  request = predict_pb2.PredictRequest()\n  request.model_spec.name = model_name\n  request.model_spec.version.value = model_version\n  print model_name,model_version\n\n  request.inputs['sparse_index'].CopyFrom(sparse_index)\n  request.inputs['sparse_ids'].CopyFrom(sparse_ids)\n  request.inputs['sparse_values'].CopyFrom(sparse_values)\n  request.inputs['sparse_shape'].CopyFrom(sparse_shape)\n  # Send request\n\n  result = stub.Predict(request, request_timeout)\n\nLogs or other output that would be helpful\n(If logs are large, please upload as attachment or provide link).\nTraceback (most recent call last):\n  File \"run.py\", line 63, in <module>\n    main()\n  File \"run.py\", line 59, in main\n    result = stub.Predict(request, request_timeout)\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\n    self._request_serializer, self._response_deserializer)\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\n    raise _abortion_error(rpc_error_call)\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input size does not match signature\")", "body": "### What related GitHub issues or StackOverflow threads have you found by searching the web for your problem?\r\nI am trying to export the model for serving , but it's report type error about *inputs* tensor.\r\nbut in the export and predict part , the **inputs** are the same type.\r\n\r\n### If possible, provide a minimal reproducible example (We usually don't have time to read hundreds of lines of your code)\r\nhere is a sample code for my exporting\r\n```\r\n            named_graph_signature = {\r\n                'inputs': exporter.generic_signature({\r\n                              'sparse_index': tf.placeholder(tf.int64, name=\"feature_index\")\r\n                              'sparse_ids': tf.placeholder(tf.int64,name = \"feature_ids\"),\r\n                              'sparse_values':tf.placeholder(tf.int64, name =\"feature_values\"),\r\n                              'sparse_shape':tf.placeholder(tf.int64, name=\"feature_shape\")\r\n                }),\r\n                'outputs': exporter.generic_signature({\r\n                    'prob': inference_softmax\r\n                })}\r\n            model_exporter.init(\r\n                sess.graph.as_graph_def(),\r\n                #default_graph_signature=named_graph_signature,\r\n                named_graph_signatures=named_graph_signature,\r\n                init_op=init_op)\r\n            model_exporter.export(export_path, tf.constant(export_version), sess)\r\n            print('Done exporting!')\r\n```\r\nhere is my code for predicting\r\n```\r\n  ins = \"0 142635:1 250810:1 335229:1 375278:1 392970:1 506983:1 554566:1 631968:1 647823:1 658803:1 733446:1 856305:1 868202:1\"\r\n  FEATURE_SIZE = 1000000\r\n  tokens = ins.split(\" \")\r\n  feature_num = 0\r\n  feature_ids = []\r\n  feature_values = []\r\n  feature_index = []\r\n\r\n  for feature in tokens[1:]:\r\n      feature_id, feature_value = feature.split(\":\")\r\n      feature_ids.append(int(feature_id))\r\n      feature_values.append(float(feature_value))\r\n      feature_index.append([1, feature_num])\r\n      feature_num += 1\r\n\r\n  feature_shape = [1, FEATURE_SIZE]\r\n\r\n  sparse_index = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_index), dtype=tf.int64)\r\n  sparse_ids = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_ids), dtype=tf.int64)\r\n  sparse_values = tf.contrib.util.make_tensor_proto(numpy.asarray(feature_values), dtype=tf.float32)\r\n  sparse_shape= tf.contrib.util.make_tensor_proto(numpy.asarray(feature_shape), dtype=tf.int64)\r\n\r\n  channel = implementations.insecure_channel(host, port)\r\n  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\r\n  request = predict_pb2.PredictRequest()\r\n  request.model_spec.name = model_name\r\n  request.model_spec.version.value = model_version\r\n  print model_name,model_version\r\n\r\n  request.inputs['sparse_index'].CopyFrom(sparse_index)\r\n  request.inputs['sparse_ids'].CopyFrom(sparse_ids)\r\n  request.inputs['sparse_values'].CopyFrom(sparse_values)\r\n  request.inputs['sparse_shape'].CopyFrom(sparse_shape)\r\n  # Send request\r\n\r\n  result = stub.Predict(request, request_timeout)\r\n```\r\n\r\n### Logs or other output that would be helpful\r\n(If logs are large, please upload as attachment or provide link).\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"run.py\", line 63, in <module>\r\n    main()\r\n  File \"run.py\", line 59, in main\r\n    result = stub.Predict(request, request_timeout)\r\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 305, in __call__\r\n    self._request_serializer, self._response_deserializer)\r\n  File \"/home/serving/.local/lib/python2.7/site-packages/grpc/beta/_client_adaptations.py\", line 203, in _blocking_unary_unary\r\n    raise _abortion_error(rpc_error_call)\r\ngrpc.framework.interfaces.face.face.AbortionError: AbortionError(code=StatusCode.INVALID_ARGUMENT, details=\"input size does not match signature\")\r\n```"}