{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/316892961", "html_url": "https://github.com/tensorflow/tensorflow/issues/6504#issuecomment-316892961", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/6504", "id": 316892961, "node_id": "MDEyOklzc3VlQ29tbWVudDMxNjg5Mjk2MQ==", "user": {"login": "guivenca", "id": 10676786, "node_id": "MDQ6VXNlcjEwNjc2Nzg2", "avatar_url": "https://avatars1.githubusercontent.com/u/10676786?v=4", "gravatar_id": "", "url": "https://api.github.com/users/guivenca", "html_url": "https://github.com/guivenca", "followers_url": "https://api.github.com/users/guivenca/followers", "following_url": "https://api.github.com/users/guivenca/following{/other_user}", "gists_url": "https://api.github.com/users/guivenca/gists{/gist_id}", "starred_url": "https://api.github.com/users/guivenca/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/guivenca/subscriptions", "organizations_url": "https://api.github.com/users/guivenca/orgs", "repos_url": "https://api.github.com/users/guivenca/repos", "events_url": "https://api.github.com/users/guivenca/events{/privacy}", "received_events_url": "https://api.github.com/users/guivenca/received_events", "type": "User", "site_admin": false}, "created_at": "2017-07-21T03:31:29Z", "updated_at": "2017-07-21T03:31:29Z", "author_association": "NONE", "body_html": "<p>I tried a simple implementation of  the gradient expression based on an article and it seemed to match the numerical gradients for the QR decomposition of tall and square random matrices. For fat ones (width&gt;height) the results weren't as good, which might be associated with some assumption in the reference, numerical difficulties (since it requires the computation of a pseudo inverse instead of an exact one) or a mistake in the code.</p>\n<p>Any suggestions?</p>\n<div class=\"highlight highlight-source-python\"><pre><span class=\"pl-k\">from</span> tensorflow.python.framework <span class=\"pl-k\">import</span> ops\n<span class=\"pl-en\">@ops.RegisterGradient</span>(<span class=\"pl-s\"><span class=\"pl-pds\">\"</span>Qr<span class=\"pl-pds\">\"</span></span>)\n<span class=\"pl-k\">def</span> <span class=\"pl-en\">_qr_grad</span>(<span class=\"pl-smi\">op</span>, <span class=\"pl-smi\">Qb</span>,<span class=\"pl-smi\">Rb</span>):\n  <span class=\"pl-s\"><span class=\"pl-pds\">\"\"\"</span>Computes and returns the gradient of a QR decomposition</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  computes the expression (in Matlab terms)</span>\n<span class=\"pl-s\">  Q*(Rb+( Pl.*(R*Rb'-Rb*R'+Q'*Qb-Qb'*Q))pinv(R)')</span>\n<span class=\"pl-s\">  where Pl is 1 below the diagonal and 0 elsewhere</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  reference in http://dx.doi.org/10.1080/10556788.2011.610454 eq (13)</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Args:</span>\n<span class=\"pl-s\">    op: operation</span>\n<span class=\"pl-s\">    Qb: gradient in Q</span>\n<span class=\"pl-s\">    Rb: gradient in R</span>\n<span class=\"pl-s\"></span>\n<span class=\"pl-s\">  Returns:</span>\n<span class=\"pl-s\">    Ab: gradient in Q*R</span>\n<span class=\"pl-s\">  <span class=\"pl-pds\">\"\"\"</span></span>\n\n  Q <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">0</span>]\n  R <span class=\"pl-k\">=</span> op.outputs[<span class=\"pl-c1\">1</span>]\n  batch_shape <span class=\"pl-k\">=</span> Q.shape[<span class=\"pl-c1\">0</span>:<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>]\n  tr_ord <span class=\"pl-k\">=</span> <span class=\"pl-c1\">list</span>(<span class=\"pl-c1\">range</span>(<span class=\"pl-c1\">len</span>(Q.shape)))\n  tr_ord[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>],tr_ord[<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>] <span class=\"pl-k\">=</span> tr_ord[<span class=\"pl-k\">-</span><span class=\"pl-c1\">2</span>],tr_ord[<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>]\n\n  M1 <span class=\"pl-k\">=</span>    tf.matmul(R,tf.transpose(Rb,tr_ord))\\\n        <span class=\"pl-k\">-</span> tf.matmul(Rb,tf.transpose(R,tr_ord))\\\n        <span class=\"pl-k\">+</span> tf.matmul(tf.transpose(Q,tr_ord),Qb)\\\n        <span class=\"pl-k\">-</span> tf.matmul(tf.transpose(Qb,tr_ord),Q)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> lower triangular part without the diagonal</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> equivalent to Haddamard product with Pl</span>\n  M1_low <span class=\"pl-k\">=</span> tf.matrix_band_part(M1,<span class=\"pl-k\">-</span><span class=\"pl-c1\">1</span>,<span class=\"pl-c1\">0</span>)<span class=\"pl-k\">-</span>tf.matrix_band_part(M1,<span class=\"pl-c1\">0</span>,<span class=\"pl-c1\">0</span>)\n\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> equivalent to explicitly computing the pseudo inverse</span>\n  <span class=\"pl-c\"><span class=\"pl-c\">#</span> ideally we should use the fact that R is triangular</span>\n  M2_tr <span class=\"pl-k\">=</span> tf.matrix_solve_ls(r,tf.transpose(M1_low,tr_ord),<span class=\"pl-v\">fast</span><span class=\"pl-k\">=</span><span class=\"pl-c1\">False</span>)\n\n\n  M2 <span class=\"pl-k\">=</span> tf.transpose(M2_tr,tr_ord)\n  Ab <span class=\"pl-k\">=</span> tf.matmul(Q,Rb<span class=\"pl-k\">+</span>M2)\n  <span class=\"pl-k\">return</span> Ab</pre></div>", "body_text": "I tried a simple implementation of  the gradient expression based on an article and it seemed to match the numerical gradients for the QR decomposition of tall and square random matrices. For fat ones (width>height) the results weren't as good, which might be associated with some assumption in the reference, numerical difficulties (since it requires the computation of a pseudo inverse instead of an exact one) or a mistake in the code.\nAny suggestions?\nfrom tensorflow.python.framework import ops\n@ops.RegisterGradient(\"Qr\")\ndef _qr_grad(op, Qb,Rb):\n  \"\"\"Computes and returns the gradient of a QR decomposition\n\n  computes the expression (in Matlab terms)\n  Q*(Rb+( Pl.*(R*Rb'-Rb*R'+Q'*Qb-Qb'*Q))pinv(R)')\n  where Pl is 1 below the diagonal and 0 elsewhere\n\n  reference in http://dx.doi.org/10.1080/10556788.2011.610454 eq (13)\n\n  Args:\n    op: operation\n    Qb: gradient in Q\n    Rb: gradient in R\n\n  Returns:\n    Ab: gradient in Q*R\n  \"\"\"\n\n  Q = op.outputs[0]\n  R = op.outputs[1]\n  batch_shape = Q.shape[0:-2]\n  tr_ord = list(range(len(Q.shape)))\n  tr_ord[-1],tr_ord[-2] = tr_ord[-2],tr_ord[-1]\n\n  M1 =    tf.matmul(R,tf.transpose(Rb,tr_ord))\\\n        - tf.matmul(Rb,tf.transpose(R,tr_ord))\\\n        + tf.matmul(tf.transpose(Q,tr_ord),Qb)\\\n        - tf.matmul(tf.transpose(Qb,tr_ord),Q)\n\n  # lower triangular part without the diagonal\n  # equivalent to Haddamard product with Pl\n  M1_low = tf.matrix_band_part(M1,-1,0)-tf.matrix_band_part(M1,0,0)\n\n  # equivalent to explicitly computing the pseudo inverse\n  # ideally we should use the fact that R is triangular\n  M2_tr = tf.matrix_solve_ls(r,tf.transpose(M1_low,tr_ord),fast=False)\n\n\n  M2 = tf.transpose(M2_tr,tr_ord)\n  Ab = tf.matmul(Q,Rb+M2)\n  return Ab", "body": "I tried a simple implementation of  the gradient expression based on an article and it seemed to match the numerical gradients for the QR decomposition of tall and square random matrices. For fat ones (width>height) the results weren't as good, which might be associated with some assumption in the reference, numerical difficulties (since it requires the computation of a pseudo inverse instead of an exact one) or a mistake in the code.\r\n\r\nAny suggestions?\r\n\r\n```python\r\nfrom tensorflow.python.framework import ops\r\n@ops.RegisterGradient(\"Qr\")\r\ndef _qr_grad(op, Qb,Rb):\r\n  \"\"\"Computes and returns the gradient of a QR decomposition\r\n\r\n  computes the expression (in Matlab terms)\r\n  Q*(Rb+( Pl.*(R*Rb'-Rb*R'+Q'*Qb-Qb'*Q))pinv(R)')\r\n  where Pl is 1 below the diagonal and 0 elsewhere\r\n\r\n  reference in http://dx.doi.org/10.1080/10556788.2011.610454 eq (13)\r\n\r\n  Args:\r\n    op: operation\r\n    Qb: gradient in Q\r\n    Rb: gradient in R\r\n\r\n  Returns:\r\n    Ab: gradient in Q*R\r\n  \"\"\"\r\n\r\n  Q = op.outputs[0]\r\n  R = op.outputs[1]\r\n  batch_shape = Q.shape[0:-2]\r\n  tr_ord = list(range(len(Q.shape)))\r\n  tr_ord[-1],tr_ord[-2] = tr_ord[-2],tr_ord[-1]\r\n\r\n  M1 =    tf.matmul(R,tf.transpose(Rb,tr_ord))\\\r\n        - tf.matmul(Rb,tf.transpose(R,tr_ord))\\\r\n        + tf.matmul(tf.transpose(Q,tr_ord),Qb)\\\r\n        - tf.matmul(tf.transpose(Qb,tr_ord),Q)\r\n\r\n  # lower triangular part without the diagonal\r\n  # equivalent to Haddamard product with Pl\r\n  M1_low = tf.matrix_band_part(M1,-1,0)-tf.matrix_band_part(M1,0,0)\r\n\r\n  # equivalent to explicitly computing the pseudo inverse\r\n  # ideally we should use the fact that R is triangular\r\n  M2_tr = tf.matrix_solve_ls(r,tf.transpose(M1_low,tr_ord),fast=False)\r\n\r\n\r\n  M2 = tf.transpose(M2_tr,tr_ord)\r\n  Ab = tf.matmul(Q,Rb+M2)\r\n  return Ab\r\n```"}