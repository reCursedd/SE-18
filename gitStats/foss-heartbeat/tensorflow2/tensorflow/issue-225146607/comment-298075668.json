{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/298075668", "html_url": "https://github.com/tensorflow/tensorflow/issues/9517#issuecomment-298075668", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9517", "id": 298075668, "node_id": "MDEyOklzc3VlQ29tbWVudDI5ODA3NTY2OA==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-04-28T18:40:42Z", "updated_at": "2017-04-28T18:40:42Z", "author_association": "MEMBER", "body_html": "<p>I meant to ask for this.  <a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=577277\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/martinwicke\">@martinwicke</a> and I spoke about it but I never formally asked.  This would be a good add to be able to state where you want the variables to go.  Just this type of change should be a 10-12% speedup using the current stock slim models as tested with resnet-50 a couple months ago.  Putting the variables on cpu has been optimal for many models, e.g. inception and resnet, regardless of GPU.  It even works great for Tesla P100s on systems with high speed GPU links.  Putting shared variables on GPU:0 has only been effective in a limited number of situations and architectures that I have tested, e.g. AlexNet on AWS K80s, and VGG16 again only on AWS K80s.</p>", "body_text": "I meant to ask for this.  @martinwicke and I spoke about it but I never formally asked.  This would be a good add to be able to state where you want the variables to go.  Just this type of change should be a 10-12% speedup using the current stock slim models as tested with resnet-50 a couple months ago.  Putting the variables on cpu has been optimal for many models, e.g. inception and resnet, regardless of GPU.  It even works great for Tesla P100s on systems with high speed GPU links.  Putting shared variables on GPU:0 has only been effective in a limited number of situations and architectures that I have tested, e.g. AlexNet on AWS K80s, and VGG16 again only on AWS K80s.", "body": "I meant to ask for this.  @martinwicke and I spoke about it but I never formally asked.  This would be a good add to be able to state where you want the variables to go.  Just this type of change should be a 10-12% speedup using the current stock slim models as tested with resnet-50 a couple months ago.  Putting the variables on cpu has been optimal for many models, e.g. inception and resnet, regardless of GPU.  It even works great for Tesla P100s on systems with high speed GPU links.  Putting shared variables on GPU:0 has only been effective in a limited number of situations and architectures that I have tested, e.g. AlexNet on AWS K80s, and VGG16 again only on AWS K80s.  "}