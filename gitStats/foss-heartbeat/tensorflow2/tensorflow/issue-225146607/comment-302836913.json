{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/302836913", "html_url": "https://github.com/tensorflow/tensorflow/issues/9517#issuecomment-302836913", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/9517", "id": 302836913, "node_id": "MDEyOklzc3VlQ29tbWVudDMwMjgzNjkxMw==", "user": {"login": "tfboyd", "id": 23486130, "node_id": "MDQ6VXNlcjIzNDg2MTMw", "avatar_url": "https://avatars1.githubusercontent.com/u/23486130?v=4", "gravatar_id": "", "url": "https://api.github.com/users/tfboyd", "html_url": "https://github.com/tfboyd", "followers_url": "https://api.github.com/users/tfboyd/followers", "following_url": "https://api.github.com/users/tfboyd/following{/other_user}", "gists_url": "https://api.github.com/users/tfboyd/gists{/gist_id}", "starred_url": "https://api.github.com/users/tfboyd/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/tfboyd/subscriptions", "organizations_url": "https://api.github.com/users/tfboyd/orgs", "repos_url": "https://api.github.com/users/tfboyd/repos", "events_url": "https://api.github.com/users/tfboyd/events{/privacy}", "received_events_url": "https://api.github.com/users/tfboyd/received_events", "type": "User", "site_admin": false}, "created_at": "2017-05-20T00:12:35Z", "updated_at": "2017-05-20T00:12:35Z", "author_association": "MEMBER", "body_html": "<p>I learned and noticed something new today.  In the tf_cnn_benchmark code when we place the variables on the GPUs we spread them out even in the basic parameter server mode.  Here is a <a href=\"https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/variable_mgr.py#L81\">link to the code</a> (long term the line number may change but for now the link goes to the correct function). This will get rolled into a much nicer example and I hope utility methods in the next 60 days or so.</p>\n<p>Personally, I am curious of the gain.  I hope to give this a test on a model I know works better with GPU as a parameter server and test GPU:0 vs. this method.  I hope this helps.  There are so many ways to approach these problems and variables that determine which is the best choice for each situation.</p>", "body_text": "I learned and noticed something new today.  In the tf_cnn_benchmark code when we place the variables on the GPUs we spread them out even in the basic parameter server mode.  Here is a link to the code (long term the line number may change but for now the link goes to the correct function). This will get rolled into a much nicer example and I hope utility methods in the next 60 days or so.\nPersonally, I am curious of the gain.  I hope to give this a test on a model I know works better with GPU as a parameter server and test GPU:0 vs. this method.  I hope this helps.  There are so many ways to approach these problems and variables that determine which is the best choice for each situation.", "body": "I learned and noticed something new today.  In the tf_cnn_benchmark code when we place the variables on the GPUs we spread them out even in the basic parameter server mode.  Here is a [link to the code](https://github.com/tensorflow/benchmarks/blob/master/scripts/tf_cnn_benchmarks/variable_mgr.py#L81) (long term the line number may change but for now the link goes to the correct function). This will get rolled into a much nicer example and I hope utility methods in the next 60 days or so.  \r\n\r\nPersonally, I am curious of the gain.  I hope to give this a test on a model I know works better with GPU as a parameter server and test GPU:0 vs. this method.  I hope this helps.  There are so many ways to approach these problems and variables that determine which is the best choice for each situation.  \r\n"}