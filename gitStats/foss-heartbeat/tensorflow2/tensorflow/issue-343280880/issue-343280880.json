{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21008", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21008/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21008/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/21008/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/21008", "id": 343280880, "node_id": "MDU6SXNzdWUzNDMyODA4ODA=", "number": 21008, "title": "Controlling the Quantization Graph-Rewriter", "user": {"login": "smcgregor", "id": 64780, "node_id": "MDQ6VXNlcjY0Nzgw", "avatar_url": "https://avatars1.githubusercontent.com/u/64780?v=4", "gravatar_id": "", "url": "https://api.github.com/users/smcgregor", "html_url": "https://github.com/smcgregor", "followers_url": "https://api.github.com/users/smcgregor/followers", "following_url": "https://api.github.com/users/smcgregor/following{/other_user}", "gists_url": "https://api.github.com/users/smcgregor/gists{/gist_id}", "starred_url": "https://api.github.com/users/smcgregor/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/smcgregor/subscriptions", "organizations_url": "https://api.github.com/users/smcgregor/orgs", "repos_url": "https://api.github.com/users/smcgregor/repos", "events_url": "https://api.github.com/users/smcgregor/events{/privacy}", "received_events_url": "https://api.github.com/users/smcgregor/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 750616506, "node_id": "MDU6TGFiZWw3NTA2MTY1MDY=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:lite", "name": "comp:lite", "color": "0052cc", "default": false}, {"id": 473173272, "node_id": "MDU6TGFiZWw0NzMxNzMyNzI=", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/type:feature", "name": "type:feature", "color": "159b2e", "default": false}], "state": "open", "locked": false, "assignee": {"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "suharshs", "id": 1450614, "node_id": "MDQ6VXNlcjE0NTA2MTQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/1450614?v=4", "gravatar_id": "", "url": "https://api.github.com/users/suharshs", "html_url": "https://github.com/suharshs", "followers_url": "https://api.github.com/users/suharshs/followers", "following_url": "https://api.github.com/users/suharshs/following{/other_user}", "gists_url": "https://api.github.com/users/suharshs/gists{/gist_id}", "starred_url": "https://api.github.com/users/suharshs/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/suharshs/subscriptions", "organizations_url": "https://api.github.com/users/suharshs/orgs", "repos_url": "https://api.github.com/users/suharshs/repos", "events_url": "https://api.github.com/users/suharshs/events{/privacy}", "received_events_url": "https://api.github.com/users/suharshs/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 3, "created_at": "2018-07-21T00:27:37Z", "updated_at": "2018-11-21T19:00:25Z", "closed_at": null, "author_association": "NONE", "body_html": "<h3>System information</h3>\n<p>Not applicable to this feature request.</p>\n<h3>Describe the problem</h3>\n<p>Our (Syntiant Corp's) neural network inference chips support a continuous range of parameter and activation quantization levels for reducing power consumption. Consequently, we aggressively tune our quantization levels for each application. Based on the research literature and product datasheets we are seeing, it is highly likely there are other chip makers with similar requirements. TF's current graph re-writer approach finds matching blocks in the graph and wraps them in fake quantization operations. This approach poorly serves our use cases for the following reasons:</p>\n<ol>\n<li>Different layers can have different quantizations. The graph re-writing approach is global to the graph.</li>\n<li>The graph re-writer attempts to heuristically match the properties of operations that should be re-written. This will generally work for traditional stored-program architectures, but when you are meddling with layers to match silicon you need to drop into TensorBoard to figure out whether the re-writer picked up the unit. If the unit is not picked up, then you are better off not using the re-writer.</li>\n<li>We have little transparency into changes in the TF codebase on these features. With more explicit specification of layer quantization it is possible to know when the quantization assumptions change and we can track the latest releases of TF.</li>\n</ol>\n<p>Our request: We would like to work with an API in which the quantization operations are more explicitly specified at the layer (Keras) or op level. We could then plug the API into our specification of neural network layers built to explicitly match the low-level operations implemented in silicon.</p>\n<p>Thank you for open sourcing TF and your efforts in supporting the community. :)</p>\n<p>For reference:</p>\n<ul>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/experimental_create_training_graph\" rel=\"nofollow\">tf.contrib.quantize.experimental_create_training_graph</a></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_eval_graph\" rel=\"nofollow\">tf.contrib.quantize.create_eval_graph</a></li>\n<li><a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_training_graph\" rel=\"nofollow\">tf.contrib.quantize.create_training_graph</a></li>\n</ul>\n<h3>Source code / logs</h3>\n<p>Not applicable to this feature request.</p>", "body_text": "System information\nNot applicable to this feature request.\nDescribe the problem\nOur (Syntiant Corp's) neural network inference chips support a continuous range of parameter and activation quantization levels for reducing power consumption. Consequently, we aggressively tune our quantization levels for each application. Based on the research literature and product datasheets we are seeing, it is highly likely there are other chip makers with similar requirements. TF's current graph re-writer approach finds matching blocks in the graph and wraps them in fake quantization operations. This approach poorly serves our use cases for the following reasons:\n\nDifferent layers can have different quantizations. The graph re-writing approach is global to the graph.\nThe graph re-writer attempts to heuristically match the properties of operations that should be re-written. This will generally work for traditional stored-program architectures, but when you are meddling with layers to match silicon you need to drop into TensorBoard to figure out whether the re-writer picked up the unit. If the unit is not picked up, then you are better off not using the re-writer.\nWe have little transparency into changes in the TF codebase on these features. With more explicit specification of layer quantization it is possible to know when the quantization assumptions change and we can track the latest releases of TF.\n\nOur request: We would like to work with an API in which the quantization operations are more explicitly specified at the layer (Keras) or op level. We could then plug the API into our specification of neural network layers built to explicitly match the low-level operations implemented in silicon.\nThank you for open sourcing TF and your efforts in supporting the community. :)\nFor reference:\n\ntf.contrib.quantize.experimental_create_training_graph\ntf.contrib.quantize.create_eval_graph\ntf.contrib.quantize.create_training_graph\n\nSource code / logs\nNot applicable to this feature request.", "body": "### System information\r\nNot applicable to this feature request.\r\n\r\n### Describe the problem\r\n\r\nOur (Syntiant Corp's) neural network inference chips support a continuous range of parameter and activation quantization levels for reducing power consumption. Consequently, we aggressively tune our quantization levels for each application. Based on the research literature and product datasheets we are seeing, it is highly likely there are other chip makers with similar requirements. TF's current graph re-writer approach finds matching blocks in the graph and wraps them in fake quantization operations. This approach poorly serves our use cases for the following reasons:\r\n\r\n1. Different layers can have different quantizations. The graph re-writing approach is global to the graph.\r\n2. The graph re-writer attempts to heuristically match the properties of operations that should be re-written. This will generally work for traditional stored-program architectures, but when you are meddling with layers to match silicon you need to drop into TensorBoard to figure out whether the re-writer picked up the unit. If the unit is not picked up, then you are better off not using the re-writer.\r\n3. We have little transparency into changes in the TF codebase on these features. With more explicit specification of layer quantization it is possible to know when the quantization assumptions change and we can track the latest releases of TF.\r\n\r\nOur request: We would like to work with an API in which the quantization operations are more explicitly specified at the layer (Keras) or op level. We could then plug the API into our specification of neural network layers built to explicitly match the low-level operations implemented in silicon.\r\n\r\nThank you for open sourcing TF and your efforts in supporting the community. :)\r\n\r\nFor reference:\r\n\r\n* [tf.contrib.quantize.experimental_create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/experimental_create_training_graph)\r\n* [tf.contrib.quantize.create_eval_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_eval_graph)\r\n* [tf.contrib.quantize.create_training_graph](https://www.tensorflow.org/api_docs/python/tf/contrib/quantize/create_training_graph)\r\n\r\n### Source code / logs\r\nNot applicable to this feature request.\r\n"}