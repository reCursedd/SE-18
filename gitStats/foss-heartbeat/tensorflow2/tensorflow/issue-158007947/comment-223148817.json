{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/223148817", "html_url": "https://github.com/tensorflow/tensorflow/issues/2610#issuecomment-223148817", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/2610", "id": 223148817, "node_id": "MDEyOklzc3VlQ29tbWVudDIyMzE0ODgxNw==", "user": {"login": "alquraishi", "id": 5205204, "node_id": "MDQ6VXNlcjUyMDUyMDQ=", "avatar_url": "https://avatars1.githubusercontent.com/u/5205204?v=4", "gravatar_id": "", "url": "https://api.github.com/users/alquraishi", "html_url": "https://github.com/alquraishi", "followers_url": "https://api.github.com/users/alquraishi/followers", "following_url": "https://api.github.com/users/alquraishi/following{/other_user}", "gists_url": "https://api.github.com/users/alquraishi/gists{/gist_id}", "starred_url": "https://api.github.com/users/alquraishi/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/alquraishi/subscriptions", "organizations_url": "https://api.github.com/users/alquraishi/orgs", "repos_url": "https://api.github.com/users/alquraishi/repos", "events_url": "https://api.github.com/users/alquraishi/events{/privacy}", "received_events_url": "https://api.github.com/users/alquraishi/received_events", "type": "User", "site_admin": false}, "created_at": "2016-06-01T22:53:13Z", "updated_at": "2016-06-01T22:53:13Z", "author_association": "NONE", "body_html": "<p>Hmmm, it may not be memory allocation <em>per se</em> that's changing. I haven't logged those values yet but I tried running a smaller model and it too ran out of memory. This new one consumes about half as much memory and so it should have no trouble running. The only other thing I changed is to set allow_growth=False, but that is the default anyway, isn't it? And along with per_process_gpu_memory_fraction=1.0, it should have no effect, no?</p>\n<p>I have no problem running much smaller models with per_process_gpu_memory_fraction set as low as 0.33, so I have gotten it to work, but it's the largish models that are giving me errors.</p>\n<p>I can log the variables you mentioned, but there's no way to do this other than changing the C++ code and recompiling correct? I can do that but it'll take me a little bit of time because I have jobs currently running.</p>", "body_text": "Hmmm, it may not be memory allocation per se that's changing. I haven't logged those values yet but I tried running a smaller model and it too ran out of memory. This new one consumes about half as much memory and so it should have no trouble running. The only other thing I changed is to set allow_growth=False, but that is the default anyway, isn't it? And along with per_process_gpu_memory_fraction=1.0, it should have no effect, no?\nI have no problem running much smaller models with per_process_gpu_memory_fraction set as low as 0.33, so I have gotten it to work, but it's the largish models that are giving me errors.\nI can log the variables you mentioned, but there's no way to do this other than changing the C++ code and recompiling correct? I can do that but it'll take me a little bit of time because I have jobs currently running.", "body": "Hmmm, it may not be memory allocation _per se_ that's changing. I haven't logged those values yet but I tried running a smaller model and it too ran out of memory. This new one consumes about half as much memory and so it should have no trouble running. The only other thing I changed is to set allow_growth=False, but that is the default anyway, isn't it? And along with per_process_gpu_memory_fraction=1.0, it should have no effect, no?\n\nI have no problem running much smaller models with per_process_gpu_memory_fraction set as low as 0.33, so I have gotten it to work, but it's the largish models that are giving me errors.\n\nI can log the variables you mentioned, but there's no way to do this other than changing the C++ code and recompiling correct? I can do that but it'll take me a little bit of time because I have jobs currently running.\n"}