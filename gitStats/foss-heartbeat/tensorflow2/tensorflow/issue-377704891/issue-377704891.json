{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23545", "repository_url": "https://api.github.com/repos/tensorflow/tensorflow", "labels_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23545/labels{/name}", "comments_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23545/comments", "events_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/23545/events", "html_url": "https://github.com/tensorflow/tensorflow/issues/23545", "id": 377704891, "node_id": "MDU6SXNzdWUzNzc3MDQ4OTE=", "number": 23545, "title": "Unacceptable framework overhead for huge networks", "user": {"login": "IFeelBloated", "id": 10070053, "node_id": "MDQ6VXNlcjEwMDcwMDUz", "avatar_url": "https://avatars1.githubusercontent.com/u/10070053?v=4", "gravatar_id": "", "url": "https://api.github.com/users/IFeelBloated", "html_url": "https://github.com/IFeelBloated", "followers_url": "https://api.github.com/users/IFeelBloated/followers", "following_url": "https://api.github.com/users/IFeelBloated/following{/other_user}", "gists_url": "https://api.github.com/users/IFeelBloated/gists{/gist_id}", "starred_url": "https://api.github.com/users/IFeelBloated/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/IFeelBloated/subscriptions", "organizations_url": "https://api.github.com/users/IFeelBloated/orgs", "repos_url": "https://api.github.com/users/IFeelBloated/repos", "events_url": "https://api.github.com/users/IFeelBloated/events{/privacy}", "received_events_url": "https://api.github.com/users/IFeelBloated/received_events", "type": "User", "site_admin": false}, "labels": [{"id": 1097543484, "node_id": "MDU6TGFiZWwxMDk3NTQzNDg0", "url": "https://api.github.com/repos/tensorflow/tensorflow/labels/comp:runtime", "name": "comp:runtime", "color": "0052cc", "default": false}], "state": "open", "locked": false, "assignee": {"login": "ezhulenev", "id": 1174378, "node_id": "MDQ6VXNlcjExNzQzNzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1174378?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezhulenev", "html_url": "https://github.com/ezhulenev", "followers_url": "https://api.github.com/users/ezhulenev/followers", "following_url": "https://api.github.com/users/ezhulenev/following{/other_user}", "gists_url": "https://api.github.com/users/ezhulenev/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezhulenev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezhulenev/subscriptions", "organizations_url": "https://api.github.com/users/ezhulenev/orgs", "repos_url": "https://api.github.com/users/ezhulenev/repos", "events_url": "https://api.github.com/users/ezhulenev/events{/privacy}", "received_events_url": "https://api.github.com/users/ezhulenev/received_events", "type": "User", "site_admin": false}, "assignees": [{"login": "ezhulenev", "id": 1174378, "node_id": "MDQ6VXNlcjExNzQzNzg=", "avatar_url": "https://avatars2.githubusercontent.com/u/1174378?v=4", "gravatar_id": "", "url": "https://api.github.com/users/ezhulenev", "html_url": "https://github.com/ezhulenev", "followers_url": "https://api.github.com/users/ezhulenev/followers", "following_url": "https://api.github.com/users/ezhulenev/following{/other_user}", "gists_url": "https://api.github.com/users/ezhulenev/gists{/gist_id}", "starred_url": "https://api.github.com/users/ezhulenev/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/ezhulenev/subscriptions", "organizations_url": "https://api.github.com/users/ezhulenev/orgs", "repos_url": "https://api.github.com/users/ezhulenev/repos", "events_url": "https://api.github.com/users/ezhulenev/events{/privacy}", "received_events_url": "https://api.github.com/users/ezhulenev/received_events", "type": "User", "site_admin": false}], "milestone": null, "comments": 0, "created_at": "2018-11-06T06:06:58Z", "updated_at": "2018-11-09T22:41:08Z", "closed_at": null, "author_association": "NONE", "body_html": "<p>I posted the same issue for Keras <a href=\"https://github.com/keras-team/keras/issues/11592\" data-hovercard-type=\"issue\" data-hovercard-url=\"/keras-team/keras/issues/11592/hovercard\">here</a>, but I guess it's more of a tensorflow problem in general.<br>\nso for a large network with thousands or even more layers (the network might not be that large mathematically since each layer could contain just a small number of parameters, but u get the idea), the overhead of basically anything messing with some graph stuff becomes unacceptable, things like model compilation, XLA optimization, gradient checkpoint, and also the keras \"save_weights\" functions are of ABYSMAL performance and could take literally DAYS!! even longer than the actual training process on GPU, I noticed that every time something like the aforementioned stuff was being executed, the python script just got stuck, one CPU core skyrocketed to 100% usage and the rest 39 cores were just almost 0% usage, then it would take hours and hours before that thing was done and then we could finally move on to the actual training.<br>\nIt's honestly just frustrating and unacceptable, any idea to solve this?<br>\nsample script to reproduce the performance problem: <a href=\"https://gist.github.com/IFeelBloated/6e85b251f66941b7eb50f987c95ce69b\">https://gist.github.com/IFeelBloated/6e85b251f66941b7eb50f987c95ce69b</a></p>", "body_text": "I posted the same issue for Keras here, but I guess it's more of a tensorflow problem in general.\nso for a large network with thousands or even more layers (the network might not be that large mathematically since each layer could contain just a small number of parameters, but u get the idea), the overhead of basically anything messing with some graph stuff becomes unacceptable, things like model compilation, XLA optimization, gradient checkpoint, and also the keras \"save_weights\" functions are of ABYSMAL performance and could take literally DAYS!! even longer than the actual training process on GPU, I noticed that every time something like the aforementioned stuff was being executed, the python script just got stuck, one CPU core skyrocketed to 100% usage and the rest 39 cores were just almost 0% usage, then it would take hours and hours before that thing was done and then we could finally move on to the actual training.\nIt's honestly just frustrating and unacceptable, any idea to solve this?\nsample script to reproduce the performance problem: https://gist.github.com/IFeelBloated/6e85b251f66941b7eb50f987c95ce69b", "body": "I posted the same issue for Keras [here](https://github.com/keras-team/keras/issues/11592), but I guess it's more of a tensorflow problem in general.\r\nso for a large network with thousands or even more layers (the network might not be that large mathematically since each layer could contain just a small number of parameters, but u get the idea), the overhead of basically anything messing with some graph stuff becomes unacceptable, things like model compilation, XLA optimization, gradient checkpoint, and also the keras \"save_weights\" functions are of ABYSMAL performance and could take literally DAYS!! even longer than the actual training process on GPU, I noticed that every time something like the aforementioned stuff was being executed, the python script just got stuck, one CPU core skyrocketed to 100% usage and the rest 39 cores were just almost 0% usage, then it would take hours and hours before that thing was done and then we could finally move on to the actual training.\r\nIt's honestly just frustrating and unacceptable, any idea to solve this?\r\nsample script to reproduce the performance problem: https://gist.github.com/IFeelBloated/6e85b251f66941b7eb50f987c95ce69b"}