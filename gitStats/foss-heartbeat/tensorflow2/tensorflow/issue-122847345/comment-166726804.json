{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/166726804", "html_url": "https://github.com/tensorflow/tensorflow/issues/537#issuecomment-166726804", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/537", "id": 166726804, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NjcyNjgwNA==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-22T20:52:27Z", "updated_at": "2015-12-22T20:52:27Z", "author_association": "NONE", "body_html": "<p><a class=\"user-mention\" data-hovercard-type=\"user\" data-hovercard-url=\"/hovercards?user_id=684901\" data-octo-click=\"hovercard-link-click\" data-octo-dimensions=\"link_type:self\" href=\"https://github.com/lukaszkaiser\">@lukaszkaiser</a> , I got this to work by declaring all of the attention modules on every run so there is no under-sharing. The gradients are then the sum of the gradients of each module. At step time, I only feed the losses for the current module into the output. This is inefficient, but it seems to work. I'm going to dig into the graph that TensorFlow made just to make sure that it's right and then work on making it more efficient.</p>\n<p>It does seem though that it would be a lot easier to build this graph if there was a way to turn off reuse in a sub-scope.</p>", "body_text": "@lukaszkaiser , I got this to work by declaring all of the attention modules on every run so there is no under-sharing. The gradients are then the sum of the gradients of each module. At step time, I only feed the losses for the current module into the output. This is inefficient, but it seems to work. I'm going to dig into the graph that TensorFlow made just to make sure that it's right and then work on making it more efficient.\nIt does seem though that it would be a lot easier to build this graph if there was a way to turn off reuse in a sub-scope.", "body": "@lukaszkaiser , I got this to work by declaring all of the attention modules on every run so there is no under-sharing. The gradients are then the sum of the gradients of each module. At step time, I only feed the losses for the current module into the output. This is inefficient, but it seems to work. I'm going to dig into the graph that TensorFlow made just to make sure that it's right and then work on making it more efficient.\n\nIt does seem though that it would be a lot easier to build this graph if there was a way to turn off reuse in a sub-scope.\n"}