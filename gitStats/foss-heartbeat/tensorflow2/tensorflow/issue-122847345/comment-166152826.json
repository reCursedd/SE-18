{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/166152826", "html_url": "https://github.com/tensorflow/tensorflow/issues/537#issuecomment-166152826", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/537", "id": 166152826, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NjE1MjgyNg==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-20T20:44:02Z", "updated_at": "2015-12-20T20:44:02Z", "author_association": "NONE", "body_html": "<p>I tried a few things to solve this including changing the <code>reuse_variables()</code> in rnn.rnn to similarly be a with block. They didn't work, so I moved towards making the attention module modular.</p>\n<p>That's complicated by the fact that the attention_states aren't calculated until embedding_attention_seq2seq. I moved it to its own function like this:</p>\n<pre><code>def attention_module(attention_states, module=None, num_heads=1):\n    ...\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.                                                                                                                            \n    ...                                                                                                                              \n    for a in xrange(num_heads):\n      k_scope = 'AttnW_%d' % a\n      v_scope = 'AttnV_%d' % a\n      if module:\n        k_scope += '_%s' % module\n        v_scope += '_%s' % module\n\n      k = vs.get_variable(k_scope, [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n      v.append(vs.get_variable(v_scope, [attention_vec_size]))\n\n    def attention(query):\n      \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n      attn_scope = 'Attention'\n      if module:\n        attn_scope += '_%s' % module\n\n      for a in xrange(num_heads):\n        with vs.variable_scope(\"%s_%d\" % (attn_scope, a)):\n          ...\n      return ds\n\n    return attention\n</code></pre>\n<p>And then instantiated it in embedding_attention_seq2seq:</p>\n<pre><code>...\ntop_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\nattention_states = array_ops.concat(1, top_states)\nattention_func = attention_module(attention_states, module, num_heads) \n...\n</code></pre>\n<p>But there is still this Under-Sharing problem. This time, it's on the AttnW_0_b. And it happens because to instantiate the modules with the overall model, I am doing this:</p>\n<pre><code>for num, module in enumerate(modules):\n  with vs.variable_scope('modules', reuse=True if num &gt; 0 else None):\n    _outputs, _losses = seq2seq.model_with_buckets(..., \n                    lambda x, y: seq2seq.embedding_attention_seq2seq(x, y, module, False), \n                    ...)\n  ...\n</code></pre>\n<p>If I remove the reuse=True on that, then the error is again over-sharing, this time because <code>modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding</code> already exists, which I'm pretty sure is a side-effect of rnn.py.</p>\n<p>Is there something else I could do to instantiate the graph like I'm describing? Thanks.</p>", "body_text": "I tried a few things to solve this including changing the reuse_variables() in rnn.rnn to similarly be a with block. They didn't work, so I moved towards making the attention module modular.\nThat's complicated by the fact that the attention_states aren't calculated until embedding_attention_seq2seq. I moved it to its own function like this:\ndef attention_module(attention_states, module=None, num_heads=1):\n    ...\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.                                                                                                                            \n    ...                                                                                                                              \n    for a in xrange(num_heads):\n      k_scope = 'AttnW_%d' % a\n      v_scope = 'AttnV_%d' % a\n      if module:\n        k_scope += '_%s' % module\n        v_scope += '_%s' % module\n\n      k = vs.get_variable(k_scope, [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n      v.append(vs.get_variable(v_scope, [attention_vec_size]))\n\n    def attention(query):\n      \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n      attn_scope = 'Attention'\n      if module:\n        attn_scope += '_%s' % module\n\n      for a in xrange(num_heads):\n        with vs.variable_scope(\"%s_%d\" % (attn_scope, a)):\n          ...\n      return ds\n\n    return attention\n\nAnd then instantiated it in embedding_attention_seq2seq:\n...\ntop_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\nattention_states = array_ops.concat(1, top_states)\nattention_func = attention_module(attention_states, module, num_heads) \n...\n\nBut there is still this Under-Sharing problem. This time, it's on the AttnW_0_b. And it happens because to instantiate the modules with the overall model, I am doing this:\nfor num, module in enumerate(modules):\n  with vs.variable_scope('modules', reuse=True if num > 0 else None):\n    _outputs, _losses = seq2seq.model_with_buckets(..., \n                    lambda x, y: seq2seq.embedding_attention_seq2seq(x, y, module, False), \n                    ...)\n  ...\n\nIf I remove the reuse=True on that, then the error is again over-sharing, this time because modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding already exists, which I'm pretty sure is a side-effect of rnn.py.\nIs there something else I could do to instantiate the graph like I'm describing? Thanks.", "body": "I tried a few things to solve this including changing the `reuse_variables()` in rnn.rnn to similarly be a with block. They didn't work, so I moved towards making the attention module modular. \n\nThat's complicated by the fact that the attention_states aren't calculated until embedding_attention_seq2seq. I moved it to its own function like this:\n\n```\ndef attention_module(attention_states, module=None, num_heads=1):\n    ...\n    # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.                                                                                                                            \n    ...                                                                                                                              \n    for a in xrange(num_heads):\n      k_scope = 'AttnW_%d' % a\n      v_scope = 'AttnV_%d' % a\n      if module:\n        k_scope += '_%s' % module\n        v_scope += '_%s' % module\n\n      k = vs.get_variable(k_scope, [1, 1, attn_size, attention_vec_size])\n      hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n      v.append(vs.get_variable(v_scope, [attention_vec_size]))\n\n    def attention(query):\n      \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n      attn_scope = 'Attention'\n      if module:\n        attn_scope += '_%s' % module\n\n      for a in xrange(num_heads):\n        with vs.variable_scope(\"%s_%d\" % (attn_scope, a)):\n          ...\n      return ds\n\n    return attention\n```\n\nAnd then instantiated it in embedding_attention_seq2seq:\n\n```\n...\ntop_states = [array_ops.reshape(e, [-1, 1, cell.output_size]) for e in encoder_outputs]\nattention_states = array_ops.concat(1, top_states)\nattention_func = attention_module(attention_states, module, num_heads) \n...\n```\n\nBut there is still this Under-Sharing problem. This time, it's on the AttnW_0_b. And it happens because to instantiate the modules with the overall model, I am doing this:\n\n```\nfor num, module in enumerate(modules):\n  with vs.variable_scope('modules', reuse=True if num > 0 else None):\n    _outputs, _losses = seq2seq.model_with_buckets(..., \n                    lambda x, y: seq2seq.embedding_attention_seq2seq(x, y, module, False), \n                    ...)\n  ...\n```\n\nIf I remove the reuse=True on that, then the error is again over-sharing, this time because `modules/embedding_attention_seq2seq/RNN/cell_output/EmbeddingWrapper/embedding` already exists, which I'm pretty sure is a side-effect of rnn.py.\n\nIs there something else I could do to instantiate the graph like I'm describing? Thanks.\n"}