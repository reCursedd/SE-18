{"url": "https://api.github.com/repos/tensorflow/tensorflow/issues/comments/167355746", "html_url": "https://github.com/tensorflow/tensorflow/issues/537#issuecomment-167355746", "issue_url": "https://api.github.com/repos/tensorflow/tensorflow/issues/537", "id": 167355746, "node_id": "MDEyOklzc3VlQ29tbWVudDE2NzM1NTc0Ng==", "user": {"login": "cinjon", "id": 615351, "node_id": "MDQ6VXNlcjYxNTM1MQ==", "avatar_url": "https://avatars1.githubusercontent.com/u/615351?v=4", "gravatar_id": "", "url": "https://api.github.com/users/cinjon", "html_url": "https://github.com/cinjon", "followers_url": "https://api.github.com/users/cinjon/followers", "following_url": "https://api.github.com/users/cinjon/following{/other_user}", "gists_url": "https://api.github.com/users/cinjon/gists{/gist_id}", "starred_url": "https://api.github.com/users/cinjon/starred{/owner}{/repo}", "subscriptions_url": "https://api.github.com/users/cinjon/subscriptions", "organizations_url": "https://api.github.com/users/cinjon/orgs", "repos_url": "https://api.github.com/users/cinjon/repos", "events_url": "https://api.github.com/users/cinjon/events{/privacy}", "received_events_url": "https://api.github.com/users/cinjon/received_events", "type": "User", "site_admin": false}, "created_at": "2015-12-26T19:28:16Z", "updated_at": "2015-12-27T04:28:50Z", "author_association": "NONE", "body_html": "<p>It looks like if I change _compute_sampled_logits in ops/nn.py to cast all_b into a float32, then I get past this error and can build the graph, but later run into another error at step time where the params are not 1-dimensional.</p>\n<pre><code>...\nall_b = embedding_ops.embedding_lookup(biases, all_ids)\nall_b = math_ops.cast(all_b, dtypes.float32)\n...\n</code></pre>\n<pre><code>[[Node: modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_INT32, \n_device=\"/job:localhost/replica:0/task:0/cpu:0\"](modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1/params_0, \nmodules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/concat)]]\n\n  File \"../tensorflow/python/ops/nn.py\", line 840, in sampled_softmax_loss\n    name=name)\n  File \"../tensorflow/python/ops/nn.py\", line 633, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \".../tensorflow/python/ops/embedding_ops.py\", line 82, in embedding_lookup\n    return array_ops.gather(params[0], ids, name=name)\n  File \".../tensorflow/python/ops/gen_array_ops.py\", line 301, in gather\n    name=name)\n  File \".../tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1850, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1049, in __init__\n    self._traceback = _extract_stack()\n</code></pre>\n<p>EDIT: I'm going to dig more into this tomorrow, but in the meantime, I forgot to mention an important part and that's that the above error comes after we use control_flow_ops.cond to choose a different attention weight/bias (via the def attention_module above) based on which module we're using. This happens in the attention_decoder, where instead of <code>attns = attention(new_state)</code>, we have a recursive build up of <code>attns = control_flow_ops.cond(module == m, lambda: attention_func(new_state), lambda: attns)</code>.</p>", "body_text": "It looks like if I change _compute_sampled_logits in ops/nn.py to cast all_b into a float32, then I get past this error and can build the graph, but later run into another error at step time where the params are not 1-dimensional.\n...\nall_b = embedding_ops.embedding_lookup(biases, all_ids)\nall_b = math_ops.cast(all_b, dtypes.float32)\n...\n\n[[Node: modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_INT32, \n_device=\"/job:localhost/replica:0/task:0/cpu:0\"](modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1/params_0, \nmodules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/concat)]]\n\n  File \"../tensorflow/python/ops/nn.py\", line 840, in sampled_softmax_loss\n    name=name)\n  File \"../tensorflow/python/ops/nn.py\", line 633, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \".../tensorflow/python/ops/embedding_ops.py\", line 82, in embedding_lookup\n    return array_ops.gather(params[0], ids, name=name)\n  File \".../tensorflow/python/ops/gen_array_ops.py\", line 301, in gather\n    name=name)\n  File \".../tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1850, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1049, in __init__\n    self._traceback = _extract_stack()\n\nEDIT: I'm going to dig more into this tomorrow, but in the meantime, I forgot to mention an important part and that's that the above error comes after we use control_flow_ops.cond to choose a different attention weight/bias (via the def attention_module above) based on which module we're using. This happens in the attention_decoder, where instead of attns = attention(new_state), we have a recursive build up of attns = control_flow_ops.cond(module == m, lambda: attention_func(new_state), lambda: attns).", "body": "It looks like if I change _compute_sampled_logits in ops/nn.py to cast all_b into a float32, then I get past this error and can build the graph, but later run into another error at step time where the params are not 1-dimensional.\n\n```\n...\nall_b = embedding_ops.embedding_lookup(biases, all_ids)\nall_b = math_ops.cast(all_b, dtypes.float32)\n...\n```\n\n```\n[[Node: modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1 = Gather[Tindices=DT_INT64, Tparams=DT_INT32, \n_device=\"/job:localhost/replica:0/task:0/cpu:0\"](modules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/embedding_lookup_1/params_0, \nmodules_1/model_with_buckets/modules/sequence_loss/sequence_loss_by_example/sampled_softmax_loss/concat)]]\n\n  File \"../tensorflow/python/ops/nn.py\", line 840, in sampled_softmax_loss\n    name=name)\n  File \"../tensorflow/python/ops/nn.py\", line 633, in _compute_sampled_logits\n    all_b = embedding_ops.embedding_lookup(biases, all_ids)\n  File \".../tensorflow/python/ops/embedding_ops.py\", line 82, in embedding_lookup\n    return array_ops.gather(params[0], ids, name=name)\n  File \".../tensorflow/python/ops/gen_array_ops.py\", line 301, in gather\n    name=name)\n  File \".../tensorflow/python/ops/op_def_library.py\", line 664, in apply_op\n    op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1850, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \".../tensorflow/python/framework/ops.py\", line 1049, in __init__\n    self._traceback = _extract_stack()\n```\n\nEDIT: I'm going to dig more into this tomorrow, but in the meantime, I forgot to mention an important part and that's that the above error comes after we use control_flow_ops.cond to choose a different attention weight/bias (via the def attention_module above) based on which module we're using. This happens in the attention_decoder, where instead of `attns = attention(new_state)`, we have a recursive build up of `attns = control_flow_ops.cond(module == m, lambda: attention_func(new_state), lambda: attns)`. \n"}